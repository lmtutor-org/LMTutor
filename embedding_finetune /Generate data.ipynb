{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63df71b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.text_splitter import TextSplitter, RecursiveCharacterTextSplitter,CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain import LLMChain\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import uuid\n",
    "from uuid import uuid4\n",
    "from langchain.chat_models import ChatAnthropic\n",
    "from langchain.schema import HumanMessage\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b191e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8622197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_path=\"/Users/lichenghu/Desktop/DSC-291-temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07f21055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(doc_path):\n",
    "    docs = DirectoryLoader(doc_path, glob=\"*.txt\", show_progress=True, use_multithreading=True,max_concurrency=16).load()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"hkunlp/instructor-large\", max_length=512,truncation=True)\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "                                        tokenizer,\n",
    "                                        chunk_size=300, \n",
    "                                        chunk_overlap=0, \n",
    "                                        separators=[\". \"],\n",
    "                                        keep_separator=False) ### hyperparams  \n",
    "    splitted_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "    content={}\n",
    "    for doc in splitted_documents:\n",
    "        content[str(uuid.uuid4())]= doc.page_content\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58639bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 39/39 [00:13<00:00,  2.85it/s]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "document=load_docs(doc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "166247d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d427e4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "serie = pd.Series(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31c8f093",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data , test_data  = [i.to_dict() for i in train_test_split(serie, train_size=0.7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1623669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3455, 1482)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data),len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb7161fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'8990730e-f6b0-4831-8ac6-6b5510ee0f93': 'There Section 10.1.4 is, however, one subtlety that needs to be addressed.\\n\\nFor any given setting of the parameters in a Gaussian mixture model (except for speciﬁc degenerate settings), there will exist other parameter settings for which the density over the observed variables will be identical. These parameter values differ only through a re-labelling of the components. For instance, consider a mixture of two Gaussians and a single observed variable x, in which the parameters have the values π1 = a, π2 = b, µ1 = c, µ2 = d, σ1 = e, σ2 = f. Then the parameter values π1 = b, π2 = a, µ1 = d, µ2 = c, σ1 = f, σ2 = e, in which the two components have been exchanged, will by symmetry give rise to the same value of p(x). If we have a mixture model comprising K components, then each parameter setting will be a member of a family of K! equivalent settings',\n",
       " '7c6c931f-1736-4ca8-b94f-b0cc2c037656': 'Chapter 19  Approximate Inference  Many probabilistic models are difficult to train because it is difficult to perform inference in them. In the context of deep learning, we usually have a set of visible variables v and a set of latent variables h. The challenge of inference usually refers to the difficult problem of computing p(h | v) or taking expectations with respect to it. Such operations are often necessary for tasks like maximum likelihood learning. Many simple graphical models with only one hidden layer, such as restricted Boltzmann machines and probabilistic PCA, are defined in a way that makes inference operations like computing p(h | v), or taking expectations with respect o it, simple. Unfortunately, most graphical models with multiple layers of hidden variables have intractable posterior distributions. Exact inference requires an exponential amount of time in these models. Even some models with only a single ayer, such as sparse coding, have this problem. In this chapter, we introduce several of the techniques for confronting these intractable inference problems. In chapter 20, we describe how to use these echniques to train probabilistic models that would otherwise be intractable, such as deep belief networks and deep Boltzmann machines',\n",
       " '594b4184-e5da-465e-8a56-88cb10db4c90': 'This algorithm is the most data eﬃcient form of linear TD(0), but it is also more expensive computationally.\\n\\nRecall that semi-gradient TD(0) requires memory and perstep computation that is only O(d). How complex is LSTD? As it is written above the complexity seems to increase with t, but the two approximations in (9.20) could be implemented incrementally using the techniques we have covered earlier (e.g., in Chapter 2) so that they can be done in constant time per step. Even so, the update for bAt would involve an outer product (a column vector times a row vector) and thus would be a matrix update; its computational complexity would be O(d2), and of course the memory required to hold the bAt matrix would be O(d2). A potentially greater problem is that our ﬁnal computation (9.21) uses the inverse of bAt, and the computational complexity of a general inverse computation is O(d3). Fortunately, an inverse of a matrix of our special form—a sum of outer products—can also be updated incrementally with only O(d2) computations, as for t > 0, with bA0 .= \"I',\n",
       " 'ce5d5992-7e9b-4d75-8f58-04e727e74318': 'The next value of z is obtained by considering a region zmin ⩽ z ⩽ zmax that contains z(τ). It is in the choice of this region that the adaptation to the characteristic length scales of the distribution takes place. We want the region to encompass as much of the slice as possible so as to allow large moves in z space while having as little as possible of this region lying outside the slice, because this makes the sampling less efﬁcient.\\n\\nOne approach to the choice of region involves starting with a region containing z(τ) having some width w and then testing each of the end points to see if they lie within the slice. If either end point does not, then the region is extended in that direction by increments of value w until the end point lies outside the region. A candidate value z′ is then chosen uniformly from this region, and if it lies within the slice, then it forms z(τ+1). If it lies outside the slice, then the region is shrunk such that z′ forms an end point and such that the region still contains z(τ). Then another candidate point is drawn uniformly from this reduced region and so on, until a value of z is found that lies within the slice',\n",
       " 'dfa1df4a-2ed3-4538-a6bc-11b5cbd6bc43': 'MACHINE LEARNING BASICS  affine functions means that the plot of the model’s predictions still looks like a line, but it need not pass through the origin. Instead of adding the bias parameter b, one can continue to use the model with only weights but augment a with an extra entry that is always set to 1. The weight corresponding to the extra 1 entry plays the role of the bias parameter. We frequently use the term “linear” when referring to affine functions throughout this book. The intercept term 6 is often called the bias parameter of the affine transfor- mation. This terminology derives from the point of view that the output of the transformation is biased toward being b in the absence of any input. This term is different from the idea of a statistical bias, in which a statistical estimation algorithm’s expected estimate of a quantity is not equal to the true quantity. Linear regression is of course an extremely simple and limited learning algorithm, but it provides an example of how a learning algorithm can work.\\n\\nIn subsequent sections we describe some of the basic principles underlying learning algorithm design and demonstrate how these principles can be used to build more complicated learning algorithms',\n",
       " 'fb2989af-f20e-4e03-bd2d-2d5241464c91': 'Finally, show that without loss of generality, the set of eigenvectors can be chosen to be orthonormal, so that they satisfy (2.46), even if some of the eigenvalues are zero. 2.19 (⋆ ⋆) Show that a real, symmetric matrix Σ having the eigenvector equation (2.45) can be expressed as an expansion in the eigenvectors, with coefﬁcients given by the eigenvalues, of the form (2.48). Similarly, show that the inverse matrix Σ−1 has a representation of the form (2.49). is positive for any real value of the vector a.\\n\\nShow that a necessary and sufﬁcient condition for Σ to be positive deﬁnite is that all of the eigenvalues λi of Σ, deﬁned by (2.45), are positive. 2.23 (⋆ ⋆) By diagonalizing the coordinate system using the eigenvector expansion (2.45), show that the volume contained within the hyperellipsoid corresponding to a constant where VD is the volume of the unit sphere in D dimensions, and the Mahalanobis distance is deﬁned by (2.44). and making use of the deﬁnition (2.77)',\n",
       " '1526fa80-cb7f-40a3-aa7b-7d7963af4176': 'The network’s weights were updated to make the network’s policy output p more closely match the policy returned by MCTS, and to make its value output, v, more closely match the probability that the current best policy wins from the board position represented by the network’s input. The DeepMind team trained AlphaGo Zero over 4.9 million games of self-play, which took about 3 days. Each move of each game was selected by running MCTS for 1,600 iterations, taking approximately 0.4 second per move. Network weights were updated over 700,000 batches each consisting of 2,048 board conﬁgurations. They then ran tournaments with the trained AlphaGo Zero playing against the version of AlphaGo that defeated Fan Hui by 5 games to 0, and against the version that defeated Lee Sedol by 4 games to 1. They used the Elo rating system to evaluate the relative performances of the programs. The di↵erence between two Elo ratings is meant to predict the outcome of games between the players.\\n\\nThe Elo ratings of AlphaGo Zero, the version of AlphaGo that played against Fan Hui, and the version that played against Lee Sedol were respectively 4,308, 3,144, and 3,739',\n",
       " '6f37ae3d-e603-46b0-8773-ddce08b6dc82': 'One advantage of SVMs is that, although the training involves nonlinear optimization, the objective function is convex, and so the solution of the optimization problem is relatively straightforward. The number of basis functions in the resulting models is generally much smaller than the number of training points, although it is often still relatively large and typically increases with the size of the training set. The relevance vector machine, discussed in Section 7.2, also chooses a subset from a ﬁxed set of basis functions and typically results in much sparser models. Unlike the SVM it also produces probabilistic outputs, although this is at the expense of a nonconvex optimization during training. An alternative approach is to ﬁx the number of basis functions in advance but allow them to be adaptive, in other words to use parametric forms for the basis functions in which the parameter values are adapted during training. The most successful model of this type in the context of pattern recognition is the feed-forward neural network, also known as the multilayer perceptron, discussed in this chapter.\\n\\nIn fact, ‘multilayer perceptron’ is really a misnomer, because the model comprises multiple layers of logistic regression models (with continuous nonlinearities) rather than multiple perceptrons (with discontinuous nonlinearities)',\n",
       " '45d89ed2-9a97-4a9c-b545-083447c40058': 'DEEP GENERATIVE MODELS  For a deep Boltzmann machine with two hidden layers, £ is given by >» WED + Loh WOW PAL —log Z(0) + H(Q). (20.35) fa  This expression still contains the log partition function, log Z(@). Because a deep Boltzmann machine contains restricted Boltzmann machines as components, the hardness results for computing the partition function and sampling that apply to restricted Boltzmann machines also apply to deep Boltzmann machines.\\n\\nThis means that evaluating the probability mass function of a Boltzmann machine requires approximate methods such as annealed importance sampling. Likewise, training the model requires approximations to the gradient of the log partition function. See chapter 18 for a general description of these methods. DBMs are typically trained using stochastic maximum likelihood. Many of the other techniques described in  chapter 18 are not applicable. Techniques such as pseudolikelihood require the ability to evaluate the unnormalized probabilities, rather than merely obtain a  variational lower bound on them',\n",
       " 'f53f67e7-041f-48fe-a7d6-8e39e5d419ba': 'Only if the target and behavior policies are related, if they visit similar states and take similar actions, should one be able to make signiﬁcant progress in o↵-policy training. On the other hand, any policy has many neighbors, many similar policies with considerable overlap in states visited and actions chosen, and yet which are not identical. The raison d’ˆetre of o↵-policy learning is to enable generalization to this vast number of related-but-not-identical policies. The problem remains of how to make the best use of the experience. Now that we have some methods that are stable in expected value (if the step sizes are set right), attention naturally turns to reducing the variance of the estimates.\\n\\nThere are many possible ideas, and we can just touch on a few of them in this introductory text. Why is controlling variance especially critical in o↵-policy methods based on importance sampling? As we have seen, importance sampling often involves products of policy ratios. The ratios are always one in expectation (5.13), but their actual values may be very high or as low as zero. Successive ratios are uncorrelated, so their products are also always one in expected value, but they can be of very high variance',\n",
       " 'f7fdb36d-64d9-4365-87f3-fd9ad9f49088': 'The constraint that the n outputs must sum to 1 means that only 1 parameters are necessary; the probability of the n-th value may be obtained, by subtracting the  https://www.deeplearningbook.org/contents/mlp.html    first 2 — 1 probabilities from 1. We can thus impose a requirement that one element of 2 be fixed. For example, we can require that, 2” = 0. Indeed, this is exactly what the sigmoid unit does. Defining Ply — =l|a)= = (2) i is equivalent to defining  P(y=1 | x) = softmax(z), with a two- ‘hamensiova zand z% =0. Both the n— 1 argument and the n argument approaches to the softmax can describe the same set of probability distributions but have different learning dynamics. In practice, there is rarely much difference between using the overparametrized version or the restricted version, and it is simpler to implement the overparametrized version',\n",
       " 'f947c650-ea88-48ff-a1e4-e46dc20fb662': 'The di↵erential return (10.9) is not well deﬁned for this case as the limit does not exist. To repair this, one could alternately deﬁne the value of a state as Exercise 10.7 Consider a Markov reward process consisting of a ring of three states A, B, and C, with state transitions going deterministically around the ring. A reward of +1 is received upon arrival in A and otherwise the reward is 0. What are the di↵erential values of the three states using (10.13)? ⇤ Exercise 10.8 The pseudocode in the box on page 251 updates ¯Rt using δt as an error rather than simply Rt+1 − ¯Rt. Both errors work, but using δt is better. To see why, consider the ring MRP of three states from Exercise 10.7. The estimate of the average reward should tend towards its true value of 1 held stuck there',\n",
       " '2e0ab83f-dfcf-4bc6-bc18-3d249c254422': 'Eﬃcient implementation relies on the fact that the k-step λ-return can be written exactly as Exercise 12.5 Several times in this book (often in exercises) we have established that returns can be written as sums of TD errors if the value function is held constant. Why is (12.10) another instance of this? Prove (12.10). ⇤ Choosing the truncation parameter n in truncated TD(λ) involves a tradeo↵. n should be large so that the method closely approximates the o✏ine λ-return algorithm, but it should also be small so that the updates can be made sooner and can inﬂuence behavior sooner. Can we get the best of both? Well, yes, in principle we can, albeit at the cost of computational complexity. The idea is that, on each time step as you gather a new increment of data, you go back and redo all the updates since the beginning of the current episode.\\n\\nThe new updates will be better than the ones you previously made because now they can take into account the time step’s new data. That is, the updates are always towards an n-step truncated λ-return target, but they always use the latest horizon',\n",
       " 'c37d7a90-0a63-438e-b9a4-945dab201d98': 'Cross Entropy and Kullback–Leibler (KL) Divergence.\\n\\nThe diverse algorithms discussed in Section 4 have all been based on the cross entropy as the divergence function in SE, namely, A nice advantage of using the cross entropy is the close-form solution of q in the teacher-student procedure, as shown in Equation 3.3, which makes the optimization and analysis easier. In the case where the uncertainty function is the Shannon entropy H(q) = −Eq (as is commonly assumed in this article), one could alternatively see the above algorithms as using the KL divergence for D, by noticing that KL(q, pθ) = Eq−Eq. That is, given speciﬁc balancing weights (α0, β0), the divergence and uncertainty terms in SE can be rearranged as: where the KL divergence term corresponds to D in SE if we see α = α0 − β0 and β = β0. 2(q+pθ) is the mean distribution',\n",
       " '5d2727e1-c21b-42c2-9ebe-2aeba2fb8ad2': 'For example, the US generally must begin after the onset of a neutral stimulus for conditioning to occur, with the rate and e↵ectiveness of learning depending on the inter-stimulus interval, or ISI, the interval between the onsets of the CS and the US. When CRs appear, they generally begin before the appearance of the US and their temporal proﬁles change during learning.\\n\\nIn conditioning with compound CSs, the component stimuli of the compound CSs may not all begin and end at the same time, sometimes forming what is called a serial compound in which the component stimuli occur in a sequence over time. Timing considerations like these make it important to consider how stimuli are represented, how these representations unfold over time during and between trials, and how they interact with discounting and eligibility traces. the behavior of the TD model: the complete serial compound (CSC), the microstimulus (MS), and the presence representations . These representations di↵er in the degree to which they force generalization among nearby time points during which a stimulus is present. The simplest of the representations shown in Figure 14.1 is the presence representation in the ﬁgure’s right column',\n",
       " '4473d162-63ee-4cbc-8a82-193a86ed92c4': 'For K classes, the DAGSVM has a total of K(K − 1)/2 classiﬁers, and to classify a new test point only K − 1 pairwise classiﬁers need to be evaluated, with the particular classiﬁers used depending on which path through the graph is traversed.\\n\\nA different approach to multiclass classiﬁcation, based on error-correcting output codes, was developed by Dietterich and Bakiri  and applied to support vector machines by Allwein et al. This can be viewed as a generalization of the voting scheme of the one-versus-one approach in which more general partitions of the classes are used to train the individual classiﬁers. The K classes themselves are represented as particular sets of responses from the two-class classiﬁers chosen, and together with a suitable decoding scheme, this gives robustness to errors and to ambiguity in the outputs of the individual classiﬁers. Although the application of SVMs to multiclass classiﬁcation problems remains an open issue, in practice the one-versus-the-rest approach is the most widely used in spite of its ad-hoc formulation and its practical limitations. There are also single-class support vector machines, which solve an unsupervised learning problem related to probability density estimation',\n",
       " '917e5fba-5285-4f2a-aaf9-548be0b5fc6c': 'Because the Gibbs sampling is performed in a deep graphical model, this similarity is based more on semantic than raw visual features, but it is still difficult for the Gibbs chain to transition from one mode of the distribution to another, for example, by changing the digit identity. (Right)Consecutive ancestral samples from a generative adversarial network. Because ancestral sampling  https://www.deeplearningbook.org/contents/monte_carlo.html   generates each sample independently from the others, there is no mixing problem. 600  CHAPTER 17. MONTE CARLO METHODS  deterministic. When the temperature rises to infinity, and 6 falls to zero, the distribution (for discrete x) becomes uniform. Typically, a model is trained to be evaluated at 6 = 1. However, we can make use of other temperatures, particularly those where 3 < 1. Tempering is a general strategy of mixing between modes of p, rapidly by drawing samples with 8 < 1. Markov chains based on tempered transitions  temporarily sample from higher-temperature distributions to mix to different modes, then resume sampling from the unit temperature distribution',\n",
       " 'c18cf274-53ef-4b38-ae19-032ee1f1693f': 'So far in this book almost all the methods have been action-value methods; they learned the values of actions and then selected actions based on their estimated action values1; their policies would not even exist without the action-value estimates. In this chapter we consider methods that instead learn a parameterized policy that can select actions without consulting a value function.\\n\\nA value function may still be used to learn the policy parameter, but is not required for action selection. We use the notation ✓ 2 Rd0 for the policy’s parameter vector. Thus we write ⇡(a|s, ✓) = Pr{At =a | St =s, ✓t =✓} for the probability that action a is taken at time t given that the environment is in state s at time t with parameter ✓. If a method uses a learned value function as well, then the value function’s weight vector is denoted w 2 Rd In this chapter we consider methods for learning the policy parameter based on the gradient of some scalar performance measure J(✓) with respect to the policy parameter. These methods seek to maximize performance, so their updates approximate gradient ascent in J: of the performance measure with respect to its argument ✓t',\n",
       " 'af8f35e0-d60c-46f6-af02-b9cdf80edc4e': 'Next, we present several of the concrete challenges that make optimization of neural networks difficult.\\n\\nWe then define several practical algorithms, including both optimization algorithms themselves and strategies for initializing the parameters. More advanced algorithms adapt their learning rates during training or leverage information contained in  https://www.deeplearningbook.org/contents/optimization.html    271  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  the second derivatives of the cost function. Finally, we conclude with a review of several optimization strategies that are formed by combining simple optimization algorithms into higher-level procedures. 8.1 How Learning Differs from Pure Optimization  Optimization algorithms used for training of deep models differ from traditional optimization algorithms in several ways. Machine learning usually acts indirectly. In most machine learning scenarios, we care about some performance measure P, that is defined with respect to the test set and may also be intractable. We therefore optimize P only indirectly. We reduce a different cost function J(@) in the hope that doing so will improve P. This is in contrast to pure optimization, where minimizing J is a goal in and of itself',\n",
       " '8db1bb19-760a-4e1f-be8d-ef295a1502f9': 'A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., Hassabis, D. Humanlevel control through deep reinforcement learning. Nature, 518:529–533. Modayil, J., Sutton, R. S. Prediction driven behavior: Learning predictions that drive Modayil, J., White, A., Sutton, R. S. .\\n\\nMulti-timescale nexting in a reinforcement learning Monahan, G. E. State of the art—a survey of partially observable Markov decision processes: theory, models, and algorithms. Management Science, 28(1):1–16. Montague, P. R., Dayan, P., Nowlan, S',\n",
       " '00003785-54ce-4b39-888e-454bdb4388b1': 'The normalization condition for the coefficients ai is obtained by requiring that the eigenvectors in feature space be normalized. Using (12.76) and (12.80), we have Having solved the eigenvector problem, the resulting principal component projections can then also be cast in terms of the kernel function so that, using (12.76), the projection of a point x onto eigenvector i is given by and so again is expressed in terms of the kernel function. In the original D-dimensional x space there are D orthogonal eigenvectors and hence we can find at most D linear principal components.\\n\\nThe dimensionality M of the feature space, however, can be much larger than D (even infinite), and thus we can find a number of nonlinear principal components that can exceed D. Note, however, that the number of nonzero eigenvalues cannot exceed the number N of data points, because (even if M > N) the covariance matrix in feature space has rank at most equal to N. This is reflected in the fact that kernel PCA involves the eigenvector expansion of the N x N matrix K',\n",
       " '45362b31-92f0-4841-a0d0-b58281712c4c': 'It enables us to draw from the latest RL advances, such as path consistency learning, to combine the best of on-/off-policy updates, and learn effectively from sparse reward. We apply the approach to a wide range of novel text generation tasks, including learning from noisy/negative examples, adversarial attacks, and prompt generation.\\n\\nExperiments show our approach consistently outperforms both task-specialized algorithms and the previous RL methods.1 Recent natural language generation systems have made remarkable progress in producing wellformed text, especially with massive pretrained language models. Those models are typically trained using maximum likelihood estimation (MLE) with a large amount of data supervisions. Despite its successes, the standard training method suffers from limited applicability to many emerging text generation problems, where little or no supervised data is available. Prominent examples of such low-data problems include generating prompts to control the massive LMs , learning text generation from noisy or even negative data, generating adversarial text attacks for robustness study , and others (Figure 1, right). Due to the failure of standard MLE, people have had to devise specialized algorithms for those problems respectively',\n",
       " 'd2e471f0-71ff-4f09-8fd6-88bc21306600': 'For our GLUE submission, we always predicted the ma14Note that we only report single-task ﬁne-tuning results in this paper. A multitask ﬁne-tuning approach could potentially push the performance even further. For example, we did observe substantial improvements on RTE from multitask training with MNLI. 15https://gluebenchmark.com/faq 1. Question: Does BERT really need such a large amount of pre-training (128,000 words/batch * 1,000,000 steps) to achieve high ﬁne-tuning accuracy? Answer: Yes, BERTBASE achieves almost 1.0% additional accuracy on MNLI when trained on 1M steps compared to 500k steps. 2. Question: Does MLM pre-training converge slower than LTR pre-training, since only 15% of words are predicted in each batch rather than every word? Answer: The MLM model does converge slightly slower than the LTR model. However, in terms of absolute accuracy the MLM model begins to outperform the LTR model almost immediately. In Section 3.1, we mention that BERT uses a mixed strategy for masking the target tokens when pre-training with the masked language model (MLM) objective',\n",
       " '3ad0dab1-e21b-4112-b0bf-367ed361ebc6': 'Chapter 3  Probability and Information Theory  In this chapter, we describe probability theory and information theory. Probability theory is a mathematical framework for representing uncertain statements. It provides a means of quantifying uncertainty as well as axioms for deriving new uncertain statements. In artificial intelligence applications, we use probability theory in two major ways. First, the laws of probability tell us how AI systems should reason, so we design our algorithms to compute or approximate various expressions derived using probability theory. Second, we can use probability and statistics to theoretically analyze the behavior of proposed AI systems. Probability theory is a fundamental tool of many disciplines of science and engineering. We provide this chapter to ensure that readers whose background is primarily in software engineering, with limited exposure to probability theory, can understand the material in this book. While probability theory allows us to make uncertain statements and to reason in the presence of uncertainty, information theory enables us to quantify the amount of uncertainty in a probability distribution',\n",
       " 'f8e80582-a95b-49ac-ae79-182aa95b77e0': 'A fundamental difficulty with such local nonparametric approaches to manifold learning is raised in Bengio and Monperrus : if the manifolds are not very smooth (they have many peaks and troughs and twists), one may need a very large number of training examples to cover each one of these variations, with no chance to generalize to unseen variations. Indeed, these methods can only generalize the shape of the manifold by interpolating between neighboring examples. Unfortunately, the manifolds involved in AI problems can have very complicated structures that can be difficult to capture from only local interpolation.\\n\\nConsider for example the manifold resulting from translation shown in figure 14.6. If we watch just one coordinate within the input vector, 2;, as the image is translated, we will observe that one coordinate encounters a peak or a trough in its value once for every peak or trough in brightness in the image. In other words, the complexity of the patterns of brightness in an underlying image template drives the complexity of the manifolds that are generated by performing simple image  https://www.deeplearningbook.org/contents/autoencoders.html    transformations. This motivates the use of distributed representations and deep learning for capturing manifold structure',\n",
       " '69d4a253-f0ea-4eb7-8932-26e33c2b2de5': \"Among tested transfer tasks,  https://lilianweng.github.io/posts/2021-05-31-contrastive/     Contrastive Representation Learning | Lil'Log   CLIP struggles with very fine-grained classification, as well as abstract or systematic tasks such as counting the number of objects. The transfer performance of CLIP models is smoothly correlated with the amount of model compute. Supervised Contrastive Learning  There are several known issues with cross entropy loss, such as the lack of robustness to noisy labels and the possibility of poor margins. Existing improvement for cross entropy loss involves the curation of better training data, such as label smoothing and data augmentation\",\n",
       " '416ce8f9-77cd-41d8-9624-8caf65cf326a': 'Such a point must have the property that the vector ∇f(x) is also orthogonal to the constraint surface, as illustrated in Figure E.1, because otherwise we could increase the value of f(x) by moving a short distance along the constraint surface. Thus ∇f and ∇g are parallel (or anti-parallel) vectors, and so there must exist a parameter λ such that where λ ̸= 0 is known as a Lagrange multiplier. Note that λ can have either sign. At this point, it is convenient to introduce the Lagrangian function deﬁned by The constrained stationarity condition (E.3) is obtained by setting ∇xL = 0. Furthermore, the condition ∂L/∂λ = 0 leads to the constraint equation g(x) = 0.\\n\\nThus to ﬁnd the maximum of a function f(x) subject to the constraint g(x) = 0, we deﬁne the Lagrangian function given by (E.4) and we then ﬁnd the stationary point of L(x, λ) with respect to both x and λ. For a D-dimensional vector x, this gives D +1 equations that determine both the stationary point x⋆ and the value of λ',\n",
       " '6906bacf-5c2c-404f-8255-35c65b9b9cd2': 'Both kinds of graphical models use a graph G in which each node in the graph corresponds to a random variable, and an edge connecting two random variables means that the probability distribution is able to represent direct interactions between those two random variables. Directed models use graphs with directed edges, and they represent fac- torizations into conditional probability distributions, as in the example above.\\n\\nSpecifically, a directed model contains one factor for every random variable x; in the distribution, and that factor consists of the conditional distribution over x; given the parents of x;, denoted Pag(x,):  p(x) = oc | Pag(x:)) - (3.53)  See figure 3.7 for an example of a directed graph and the factorization of probability distributions it represents. Undirected models use graphs with undirected edges, and they represent factorizations into a set of functions; unlike in the directed case, these functions are usually not probability distributions of any kind. Any set of nodes that are all connected to each other in G is called a clique. Each clique C® in an undirected model is associated with a factor ¢(C)',\n",
       " 'd481f4b9-9a79-4591-a6c9-d5cbe7a95026': \"In each training episode, the truth label y; is presented with one step offset, (Xt41, Yt): it is the true label for the input at the previous time step t, but presented as part of the input at time step t+1. https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Log   External Memory External Memory  Class Prediction 2 | c be ---—> 68) seen > | fe > : > Backpropagated  * 4 Shuffle: * 4 * * Signal (Xi, Yer)(Kev1,y) Labels (1,0) (x2,%) Xi Xp Xpand Classes  i=l where M; is the memory matrix at time t and M;(2) is the i-th row in this matrix.\\n\\n» How to write into memory? The addressing mechanism for writing newly received information into memory operates a lot like the cache replacement policy\",\n",
       " '5e44c072-f37f-4905-9af8-3992df7b0e8e': 'This can be a difficult and subtle task. Often, we cannot actually evaluate the log-probability of the data under the model, but can evaluate only an approximation. In these cases, it is important to think and communicate clearly about what exactly is being measured. For example, suppose we can evaluate a stochastic estimate of the log-likelihood for model A, and a deterministic lower bound on the log-likelihood for model B. If model A gets a higher score than model B, which is better? If we care about determining which model has a better internal representation of the distribution, we actually cannot tell, unless we have some way of determining how  loose the bound for model B is. However, if we care about how well we can use he model in practice, for example to perform anomaly detection, then it is fair to say that a model is preferable based on a criterion specific to the practical task of interest, for example, based on ranking test examples and ranking criteria such as precision and recall. Another subtlety of evaluating generative models is that the evaluation metrics are often hard research problems in and of themselves',\n",
       " 'a592bd5c-615f-45d3-8d34-d8e5f86a1329': 'In Chapter 7, however, we distinguished n-step Expected Sarsa from n-step Tree Backup, where the latter retained the property of not using importance sampling. It remains then to present the eligibility trace version of Tree Backup, which we will call Tree-Backup(λ), or TB(λ) for short. This is arguably the true successor to Q-learning because it retains its appealing absence of importance sampling even though it can be applied to o↵-policy data. The concept of TB(λ) is straightforward. As shown in its backup diagram in Figure 12.13, the tree-backup updates of each length (from Section 7.5) are weighted in the usual way dependent on the bootstrapping parameter λ',\n",
       " '7c4e53b1-a351-4537-89c7-db3142562d9b': 'The conjugate prior for µ is the Gaussian, the conjugate prior for Λ is the Wishart, and the conjugate prior for (µ, Λ) is the Gaussian-Wishart.\\n\\nIf we have a marginal Gaussian distribution for x and a conditional Gaussian distribution for y given x in the form then the marginal distribution of y, and the conditional distribution of x given y, are given by If we have a joint Gaussian distribution N(x|µ, Σ) with Λ ≡ Σ−1 and we deﬁne the following partitions then the conditional distribution p(xa|xb) is given by and the marginal distribution p(xa) is given by This is the conjugate prior distribution for a univariate Gaussian N(x|µ, λ−1) in which the mean µ and the precision λ are both unknown and is also called the normal-gamma distribution. It comprises the product of a Gaussian distribution for µ, whose precision is proportional to λ, and a gamma distribution over λ',\n",
       " '0796c277-469d-4bf9-9a63-8664cb19d970': 'The decision surface, shown in red, is perpendicular to w, and its displacement from the origin is controlled by the bias parameter w0. Also, the signed orthogonal distance of a general point x from the decision surface is given by y(x)/∥w∥. an arbitrary point x and let x⊥ be its orthogonal projection onto the decision surface, so that Multiplying both sides of this result by wT and adding w0, and making use of y(x) = wTx + w0 and y(x⊥) = wTx⊥ + w0 = 0, we have This result is illustrated in Figure 4.1. As with the linear regression models in Chapter 3, it is sometimes convenient to use a more compact notation in which we introduce an additional dummy ‘input’ value x0 = 1 and then deﬁne �w = (w0, w) and �x = (x0, x) so that In this case, the decision surfaces are D-dimensional hyperplanes passing through the origin of the D + 1-dimensional expanded input space. Now consider the extension of linear discriminants to K > 2 classes',\n",
       " '494d81a3-6574-4cd4-9eb6-9a170e37f928': 'F. Skinner called shaping in which reward contingencies are progressively altered to train an animal to successively approximate a desired behavior. Shaping is not only indispensable for animal training, it is also an e↵ective tool for training reinforcement learning agents.\\n\\nThere is also a connection to the idea of an animal’s motivational state, which inﬂuences what an animal will approach or avoid and what events are rewarding or punishing for the animal. The reinforcement learning algorithms presented in this book include two basic mechanisms for addressing the problem of delayed reinforcement: eligibility traces and value functions learned via TD algorithms. Both mechanisms have antecedents in theories of animal learning. Eligibility traces are similar to stimulus traces of early theories, and value functions correspond to the role of secondary reinforcement in providing nearly immediate evaluative feedback. The next correspondence the chapter addressed is that between reinforcement learning’s environment models and what psychologists call cognitive maps. Experiments conducted in the mid 20th century purported to demonstrate the ability of animals to learn cognitive maps as alternatives to, or as additions to, state–action associations, and later use them to guide behavior, especially when the environment changes unexpectedly',\n",
       " '7eb1e5b0-9030-482c-a69f-98fc3f97d903': 'Other views of learning under delayed reinforcement invoke roles for awareness and working memory .\\n\\n14.5 Thistlethwaite  provides an extensive review of latent learning experiments up to the time of its publication. Ljung  provides an overview of model learning, or system identiﬁcation, techniques in engineering. Gopnik, Glymour, Sobel, Schulz, Kushnir, and Danks  present a Bayesian theory about how children learn models. 14.6 Connections between habitual and goal-directed behavior and model-free and model-based reinforcement learning were ﬁrst proposed by Daw, Niv, and Dayan . The hypothetical maze task used to explain habitual and goal-directed behavioral control is based on the explanation of Niv, Joel, and Dayan . Dolan and Dayan  review four generations of experimental research related to this issue and discuss how it can move forward on the basis of reinforcement learning’s model-free/model-based distinction. Dickinson  and Dickinson and Balleine  discuss experimental evidence related to this distinction. Donahoe and Burgos  alternatively argue that model-free processes can account for the results of outcome-devaluation experiments. Dayan and Berridge  argue that classical conditioning involves model-based processes',\n",
       " '6621c53d-f7d7-4ede-9b0e-058ef66310aa': 'Often, we consider functionals deﬁned by integrals whose integrands take the form G(y, x) and that do not depend on the derivatives of y(x). In this case, stationarity simply requires that ∂G/∂y(x) = 0 for all values of x. If we are optimizing a functional with respect to a probability distribution, then we need to maintain the normalization constraint on the probabilities. This is often most conveniently done using a Lagrange multiplier, which then allows an unconAppendix E strained optimization to be performed. The extension of the above results to a multidimensional variable x is straightforward. For a more comprehensive discussion of the calculus of variations, see Sagan .\\n\\nLagrange multipliers, also sometimes called undetermined multipliers, are used to ﬁnd the stationary points of a function of several variables subject to one or more constraints',\n",
       " '4c2a767d-71d4-4dc1-8070-1ed2d5718376': 'This is plausible given that many—though not all— drugs of abuse increase levels of dopamine either directly or indirectly in regions around terminals of dopamine neuron axons in the striatum, a brain structure ﬁrmly implicated in normal reward-based learning (Section 15.7). But the self-destructive behavior associated with drug addiction is not characteristic of normal learning. What is di↵erent about dopamine-mediated learning when the reward is the result of an addictive drug? Is addiction the result of normal learning in response to substances that were largely unavailable throughout our evolutionary history, so that evolution could not select against their damaging e↵ects? Or do addictive substances somehow interfere with normal dopamine-mediated learning? The reward prediction error hypothesis of dopamine neuron activity and its connection to TD learning are the basis of a model due to Redish  of some—but certainly not all—features of addiction.\\n\\nThe model is based on the observation that administration of cocaine and some other addictive drugs produces a transient increase in dopamine. In the model, this dopamine surge is assumed to increase the TD error, δ, in a way that cannot be cancelled out by changes in the value function',\n",
       " '95c81d0a-78b6-4502-af35-aee17316cb0a': \"Authors’ contributions  CS performed the primary literature review and analysis for this work, and also drafted the manuscript. TMK, JLL, RAB, RZ, KW, NS, and RK worked with CS to develop the article's framework and focus. TMK introduced this topic to CS, and helped to complete and finalize this work. All authors read and approved the final manuscript. Funding Not applicable. Availability of data and materials Not applicable. Competing interests The authors declare that they have no competing interests. Consent for publication Not applicable. Ethics approval and consent to participate Not applicable. Received: 9 January 2019 Accepted: 22 April 2019 Published online: 06 July 2019  References 1. Krizhevsky A, Sutskever |, Hinton GE. ImageNet classification with deep convolutional neural networks. Adv Neural Inf Process Syst. 2012;25:1 106-14. 2. Karen S, Andrew Z. Very deep convolutional networks for large-scale image recognition. arXiv e-prints. 2014. 3\",\n",
       " '21641d4d-c8a5-41d6-9e3a-6857d1053f8c': 'An important limitation of these early works is that they did not treat the o↵-policy case with function approximation. Intra-option learning in general requires o↵-policy learning, which could not be done reliably with function approximation at that time. Although now we have a variety of stable o↵-policy learning methods using function approximation, their combination with option ideas had not been significantly explored at the time of publication of this book. Barto and Mahadevan  and Hengst  review the options formalism and other approaches to temporal abstraction.\\n\\nUsing GVFs to implement option models has not previously been described. Our presentation uses the trick introduced by Modayil, White and Sutton  for predicting signals at the termination of policies. The extension of options and option models to the average-reward setting has not yet been developed in the literature. 17.3 A good presentation of the POMDP approach is given by Monahan . PSRs and tests were introduced by Littman, Sutton and Singh . OOMs were introduced by Jaeger . Sequential Systems, which unify PSRs, OOMs, and many other works, were introduced in the PhD thesis of Michael Thon',\n",
       " 'd152b01c-0c8e-479a-982a-de9363c4ecc9': \"Initialize the time step t = 1 While T <= T_MAX:  Reset gradient: d@ = 0 and dw = 0.\\n\\nSynchronize thread-specific parameters with global ones: 8' = 8 and w' = w.  tstart = t and get s;. While (s¢ #4 TERMINAL) and (¢ — tstart <= tax):  Pick the action ay ~ 7(a;|sz; 6’) and receive a new reward r; and a new state $4, ,. Update t=t+1andT=T+1. Initialize the variable that holds the return estimation  R={0 if s,is TERMINAL V(s,;w’) otherwise  Fori =t—1,..-,tstart! R«vr, + 7R; here R is a MC measure of G;. Accumulate gradients w.r.t. 6': dO — dé + Vo log n(a;\\\\s;; 0’)(R — V(s;; w’)); Accumulate gradients w.r.t\",\n",
       " '93079a0d-018b-463e-90e7-9952a8565258': 'However, we see from Figure 5.21(c) that the model is able to produce a conditional density that is unimodal for some values of x and trimodal for other values by modulating the amplitudes of the mixing components πk(x). Once a mixture density network has been trained, it can predict the conditional density function of the target data for any given value of the input vector. This conditional density represents a complete description of the generator of the data, so far as the problem of predicting the value of the output vector is concerned.\\n\\nFrom this density function we can calculate more speciﬁc quantities that may be of interest in different applications. One of the simplest of these is the mean, corresponding to the conditional average of the target data, and is given by where we have used (5.148). Because a standard network trained by least squares is approximating the conditional mean, we see that a mixture density network can reproduce the conventional least-squares result as a special case. Of course, as we have already noted, for a multimodal distribution the conditional mean is of limited value. We can similarly evaluate the variance of the density function about the conditional average, to give Exercise 5.37 where we have used (5.148) and (5.158)',\n",
       " 'd608d9a5-bf51-4e5a-a139-09142a1816dc': 'maximize similarity between augmented and unaugmented copies of the same image, we apply data augmentation symmetrically to both branches of our framework (Figure 2). We also apply a nonlinear projection on the output of base feature network, and use the representation before projection network, whereas Ye et al. use the linearly projected ﬁnal hidden vector as the representation. When training with large batch sizes using multiple accelerators, we use global BN to avoid shortcuts that can greatly decrease representation quality.',\n",
       " '1667019a-203f-4110-a966-e6c761618978': 'Note that the effect of marginalization is to spread out the contours and to make the predictions less conﬁdent, so that at each input point x, the posterior probabilities are shifted towards 0.5, while the y = 0.5 contour itself is unaffected. The convolution of a Gaussian with a logistic sigmoid is intractable. We therefore apply the approximation (4.153) to (5.189) giving where κ(·) is deﬁned by (4.154). Recall that both σ2 ﬁcation data set described in Appendix A.\\n\\n5.1 (⋆ ⋆) Consider a two-layer network function of the form (5.7) in which the hiddenunit nonlinear activation functions g(·) are given by logistic sigmoid functions of the form Show that there exists an equivalent network, which computes exactly the same function, but with hidden unit activation functions given by tanh(a) where the tanh function is deﬁned by (5.59). Hint: ﬁrst ﬁnd the relation between σ(a) and tanh(a), and then show that the parameters of the two networks differ by linear transformations',\n",
       " '05a11d38-b49b-4641-9043-ce8b3a3ca139': \"Ahhough the data space comprises 12 measuremenlS, a data set of points will lie close to a Iwo-dimensional manifold embedded within this space. In this case, the manifold comprises scveral distinct segments corresponding to different flow regimes. each such segment being a (noisy) continuous two-dimensional manifold. If our goal is data compression. or density modelling, then there can be benefits in exploiling this manifold struclUre. In praclice. the data points will not be confined precisely to a smooth lowdimensional manifold, and we can interpret the departures of data points from the manifold as ·noise'.\\n\\nThis leads naturally to a generative view of such models in which we first select a poinl within the manifold according to some latent variable distribution and then generate an observed data point by :ldding noise, drawn from some conditional distribution of the data varillbles given the latent varillbles\",\n",
       " '3aa3d0bb-7edd-495e-a988-187119f09530': 'By basing a regression model on a heavy-tailed distribution such as a t-distribution, we obtain a more robust model.\\n\\nIf we go back to (2.158) and substitute the alternative parameters ν = 2a, λ = a/b, and η = τb/a, we see that the t-distribution can be written in the form We can then generalize this to a multivariate Gaussian N(x|µ, Λ) to obtain the corresponding multivariate Student’s t-distribution in the form with corresponding results for the univariate case. Although Gaussian distributions are of great practical signiﬁcance, both in their own right and as building blocks for more complex probabilistic models, there are situations in which they are inappropriate as density models for continuous variables. One important case, which arises in practical applications, is that of periodic variables. An example of a periodic variable would be the wind direction at a particular geographical location. We might, for instance, measure values of wind direction on a number of days and wish to summarize this using a parametric distribution. Another example is calendar time, where we may be interested in modelling quantities that are believed to be periodic over 24 hours or over an annual cycle',\n",
       " '3a171e47-94de-49f4-a3e4-14c393a6d9ec': 'A major drawback to kernel machines is that the cost of evaluating the decision function is linear in the number of training examples, because the i-th example contributes a term a;k(a, al)) to the decision function. Support vector machines are able to mitigate this by learning an @ vector that contains mostly zeros. Classifying a new example then requires evaluating the kernel function only for the training examples that have nonzero a;. These training examples are known as support vectors. Kernel machines also suffer from a high computational cost of training when the dataset is large. We revisit this idea in section 5.9. Kernel machines with  140  CHAPTER 5. MACHINE LEARNING BASICS  generic kernels struggle to generalize well. We explain why in section 5.11. The modern incarnation of deep learning was designed to overcome these limitations of kernel machines.\\n\\nThe current deep learning renaissance began when Hinton e¢ al. demonstrated that a neural network could outperform the RBF kernel SVM on the MNIST benchmark',\n",
       " 'dfd676a4-2fa9-4133-9240-1860b9ae0f72': 'In particular, model innovations like the LSTM, rectified linear units and maxout units have all moved toward using more linear functions than previous models like deep networks based on sigmoidal units. These models have nice properties that make optimization easier. The gradient flows through many layers provided that the Jacobian of the linear transformation has reasonable singular values. Moreover, linear functions consistently increase in a single direction, so even if the model’s output is very far from correct, it is clear simply from computing the gradient which direction its output should move to reduce the loss function.\\n\\nIn other words, modern neural nets have been designed so that their local gradient information corresponds reasonably well to moving toward a distant solution. Other model design strategies can help to make optimization easier. For example, linear paths or skip connections between layers reduce the length of he shortest path from the lower layer’s parameters to the output, and thus mitigate the vanishing gradient problem . A related idea o skip connections is adding extra copies of the output that are attached to the intermediate hidden layers of the network, as in GoogLeNet  and deeply supervised nets . These “auxiliary heads” are trained o perform the same task as the primary output at the top of the network to ensure that the lower layers receive a large gradient',\n",
       " 'bd774cb6-75dc-4d57-83d4-3663ab91705c': 'Clearly this is artiﬁcial. A more general way to think of an earliest reward-predicting state is that it is an unpredicted predictor of reward, and there can be many such states. In an animal’s life, many di↵erent states may precede an earliest reward-predicting state. However, because these states are more often followed by other states that do not predict reward, their reward-predicting powers, that is, their values, remain low. A TD algorithm, if operating throughout the animal’s life, would update the values of these states too, but the updates would not consistently accumulate because, by assumption, none of these states reliably precedes an earliest reward-predicting state. If any of them did, they would be reward-predicting states as well. This might explain why with overtraining, dopamine responses decrease to even the earliest reward-predicting stimulus in a trial',\n",
       " '8f687df8-879b-4f5a-a1cf-10daf07bf999': 'Thus x1 is a good predictor of t, x2 is a more noisy predictor of t, and x3 has only chance correlations with t. The marginal likelihood for a Gaussian process with ARD parameters η1, η2, η3 is optimized using the scaled conjugate gradients algorithm. We see from Figure 6.10 that η1 converges to a relatively large value, η2 converges to a much smaller value, and η3 becomes very small indicating that x3 is irrelevant for predicting t. The ARD framework is easily incorporated into the exponential-quadratic kernel (6.63) to give the following form of kernel function, which has been found useful for applications of Gaussian processes to a range of regression problems where D is the dimensionality of the input space. In a probabilistic approach to classiﬁcation, our goal is to model the posterior probabilities of the target variable for a new input vector, given a set of training data. These probabilities must lie in the interval (0, 1), whereas a Gaussian process model makes predictions that lie on the entire real axis',\n",
       " '4d178e19-9602-460c-aa0c-3c9066819e29': 'These represent error signals δ for each pattern and for each output unit, and can be backpropagated to the hidden units and the error function derivatives evaluated in the usual way. Because the error function (5.153) is composed of a sum of terms, one for each training data point, we can consider the derivatives for a particular pattern n and then ﬁnd the derivatives of E by summing over all patterns.\\n\\nBecause we are dealing with mixture distributions, it is convenient to view the mixing coefﬁcients πk(x) as x-dependent prior probabilities and to introduce the corresponding posterior probabilities given by Similarly, the derivatives with respect to the output activations controlling the component means are given by Exercise 5.35 Finally, the derivatives with respect to the output activations controlling the component variances are given by Exercise 5.36 We illustrate the use of a mixture density network by returning to the toy example of an inverse problem shown in Figure 5.19. Plots of the mixing coefﬁcients πk(x), the means µk(x), and the conditional density contours corresponding to p(t|x), are shown in Figure 5.21. The outputs of the neural network, and hence the parameters in the mixture model, are necessarily continuous single-valued functions of the input variables',\n",
       " 'b8e5a54e-fc0f-4446-be70-0a3441164e19': 'For example, we may have a predictor f(x;@) that we wish to employ to predict the mean of y. If we use a sufficiently powerful neural network, we can think of the neural network as being able to represent any function f from a wide class of functions, with this class being limited only by features such as continuity and boundedness  175  https://www.deeplearningbook.org/contents/mlp.html    CHAPTER 6 DEEP FEEDFORWARD NETWORKS  rather than by having a specific parametric form. From this point of view, we can view the cost function as being a functional rather than just a function. A functional is a mapping from functions to real numbers. We can thus think of earning as choosing a function rather than merely choosing a set of parameters.\\n\\nWe can design our cost functional to have its minimum occur at some specific function we desire. For example, we can design the cost functional to have its minimum lie on the function that maps a to the expected value of y given a. Solving an optimization problem with respect to a function requires a mathematical ool called calculus of variations, described in section 19.4.2',\n",
       " 'adb7a7bc-2198-49e6-a338-4d7239cdf221': 'So if c = (0, 0)>, the feature is constant over both dimensions; if c = (c1, 0)> the feature is constant over the second dimension and varies over the ﬁrst with frequency depending on c1; and similarly, for c = (0, c2)>. When c = (c1, c2)> with neither cj = 0, the feature varies along both dimensions and represents an interaction between the two state variables. The values of c1 and c2 determine the frequency along each dimension, and their ratio gives the direction of the interaction. When using Fourier cosine features with a learning algorithm such as (9.7), semigradient TD(0), or semi-gradient Sarsa, it may be helpful to use a di↵erent step-size parameter for each feature.\\n\\nIf ↵ is the basic step-size parameter, then Konidaris, Osentoski, and Thomas  suggest setting the step-size parameter for feature xi to ↵i = ↵/ Fourier cosine features with Sarsa can produce good performance compared to several other collections of basis functions, including polynomial and radial basis functions',\n",
       " 'e94d44e6-33e6-4c15-9024-c3852be4559b': 'A deep RNN  has state variables from several layers at each time step, giving the unfolded graph two kinds of depth: ordinary depth due to a stack of layers, and depth due to time unfolding. This work brought the phoneme error rate on TIMIT to a record low of 17.7 percent. See Pascanu ef al. and Chung ef al. for other variants of deep RNNs, applied in other settings. Another contemporary step toward end-to-end deep learning ASR is to let the system learn how to “align” the acoustic-level information with the phonetic-level information . 455  CHAPTER 12. APPLICATIONS  12.4 Natural Language Processing  Natural language processing (NLP) is the use of human languages, such as English or French, by a computer. Computer programs typically read and emit specialized languages designed to allow efficient and unambiguous parsing by simple programs.\\n\\nMore naturally occurring languages are often ambiguous and defy formal description. Natural language processing includes applications such as machine translation, in which the learner must read a sentence in one human language and emit an equivalent sentence in another human language',\n",
       " '79462613-9454-49b4-9fa1-351245cd517c': 'AAAI Conference on Artiﬁcial Intelligence, 8, 1433–1438.',\n",
       " '7eb449e2-2a26-433f-828f-a4775fdf55d6': 'Both types of multimodality were encountered in Chapter 9 in the context of Gaussian mixtures, where they manifested themselves as multiple maxima in the likelihood function, and a variational treatment based on the minimization of KL(q∥p) will tend to ﬁnd one of these modes.\\n\\nBy contrast, if we were to minimize KL(p∥q), the resulting approximations would average across all of the modes and, in the context of the mixture model, would lead to poor predictive distributions (because the average of two good parameter values is typically itself not a good parameter value). It is possible to make use of KL(p∥q) to deﬁne a useful inference procedure, but this requires a rather different approach to the one discussed here, and will be considered in detail when we discuss expectation propagation. Section 10.7 The two forms of Kullback-Leibler divergence are members of the alpha family of divergences  deﬁned by where −∞ < α < ∞ is a continuous parameter. The Kullback-Leibler divergence KL(p∥q) corresponds to the limit α → 1, whereas KL(q∥p) corresponds to the limit α → −1',\n",
       " '30047d89-5f0e-4c5f-a3f1-dd1b7e6a9dc8': 'The backward view of the λ-return algorithm is also straightforward, using separate eligibility traces for the actor and critic, each after the patterns in Chapter 12. Pseudocode for the complete algorithm is given in the box below',\n",
       " '92ca6458-43e1-4ab3-adc3-b85a8789a996': '(8.25)  i=1  J(9) = Exy~ toate (wy) lE(F (as 8), 9] =  The methods we discuss here extend readily, however, to more general objective functions, such as those that include parameter regularization terms, as discussed in chapter 7. 8.6.1 Newton’s Method  In section 4.3, we introduced second-order gradient methods. In contrast to first- order methods, second-order methods make use of second derivatives to improve optimization. The most widely used second-order method is Newton’s method. We now describe Newton’s method in more detail, with emphasis on its application to neural network training. Newton’s method is an optimization scheme based on using a second-order Tay- lor series expansion to approximate J(@) near some point 00, ignoring derivatives of higher order:  J(0) ~ (00) + (9 ~ 80)\" Vo (8) + 5(0~ 00)\" H(O— 0), (8.26)  where H is the Hessian of J with respect to @ evaluated at 09. If we then solve for the critical point of this function, we obtain the Newton parameter update rule:  0* = 0) — HV J(0,)',\n",
       " 'a851044c-524a-4c7a-a79a-e460c4b0dc5a': 'In this case, however, these policies are not just better, but optimal, proceeding to the terminal states in the minimum number of steps. In this example, policy iteration would ﬁnd the optimal policy after just one iteration. Exercise 4.4 The policy iteration algorithm on page 80 has a subtle bug in that it may never terminate if the policy continually switches between two or more policies that are equally good. This is ok for pedagogy, but not for actual use. Modify the pseudocode so that convergence is guaranteed. ⇤ Exercise 4.5 How would policy iteration be deﬁned for action values? Give a complete algorithm for computing q⇤, analogous to that on page 80 for computing v⇤. Please pay special attention to this exercise, because the ideas involved will be used throughout the rest of the book. ⇤ meaning that the probability of selecting each action in each state, s, is at least \"/|A(s)|. Describe qualitatively the changes that would be required in each of the steps 3, 2, and 1, in that order, of the policy iteration algorithm for v⇤ on page 80',\n",
       " '63728cad-bbae-4c12-b9c8-b95509402a8a': '5.38 (⋆) Using the general result (2.115), derive the predictive distribution (5.172) for the Laplace approximation to the Bayesian neural network model. 5.39 (⋆) www Make use of the Laplace approximation result (4.135) to show that the evidence function for the hyperparameters α and β in the Bayesian neural network model can be approximated by (5.175). 5.40 (⋆) www Outline the modiﬁcations needed to the framework for Bayesian neural networks, discussed in Section 5.7.3, to handle multiclass problems using networks having softmax output-unit activation functions.\\n\\n5.41 (⋆ ⋆) By following analogous steps to those given in Section 5.7.1 for regression networks, derive the result (5.183) for the marginal likelihood in the case of a network having a cross-entropy error function and logistic-sigmoid output-unit activation function. In Chapters 3 and 4, we considered linear parametric models for regression and classiﬁcation in which the form of the mapping y(x, w) from input x to output y is governed by a vector w of adaptive parameters',\n",
       " '157e2d0b-261d-4825-8bff-aeef4deb70ed': 'In some cases when the auxiliary q is assumed to have a certain form (e.g., a deep network), the approximate E-step in Equation 2.12 may still be too complex to be tractable, or the gradient estimator (w.r.t.\\n\\nthe parameters of q) can suﬀer from high variance . To tackle the challenge, more approximations are introduced. The wake-sleep algorithm  is one of such methods. In the E-step w.r.t. q, rather than minimizing KL(q(y)∥pθ(y|x∗)) (Equation 2.9) as in EM and variational EM, the wakesleep algorithm makes an approximation by minimizing the Kullback–Leibler (KL) divergence in which can be optimized eﬃciently with gradient descent when q is parameterized. Besides wakesleep, one can also use other methods for low-variance gradient estimation in Equation 2.12, such as reparameterization gradient  and score gradient . In sum, the entropy maximization perspective has formulated unsupervised MLE as an optimizationtheoretic framework that permits simple alternating minimization solvers',\n",
       " 'ce114ad2-e737-4c29-8f7e-9b37cc965986': 'Obtaining this goal was rewarding to them and reinforced the actions allowing them to escape. It is diﬃcult to link the concept of motivation, which has many dimensions, in a precise way to reinforcement learning’s computational perspective, but there are clear links with some of its dimensions. In one sense, a reinforcement learning agent’s reward signal is at the base of its motivation: the agent is motivated to maximize the total reward it receives over the long run. A key facet of motivation, then, is what makes an agent’s experience rewarding. In reinforcement learning, reward signals depend on the state of the reinforcement learning agent’s environment and the agent’s actions. Further, as pointed out in Chapter 1, the state of the agent’s environment not only includes information about what is external to the machine, like an organism or a robot, that houses the agent, but also what is internal to this machine. Some internal state components correspond to what psychologists call an animal’s motivational state, which inﬂuences what is rewarding to the animal. For example, an animal will be more rewarded by eating when it is hungry than when it has just ﬁnished a satisfying meal',\n",
       " 'a2d04503-8d83-4cf3-a782-a5e5a86b6824': 'The distribution µ in the VE (9.1) is then deﬁned as the distribution of states encountered while following the target policy, weighted by the interest. Second, we introduce another non-negative scalar random variable, the emphasis Mt. This scalar multiplies the learning update and thus emphasizes or de-emphasizes the learning done at time t. The general n-step learning rule, replacing (9.15), is with Mt .= 0, for all t < 0. These equations are taken to include the Monte Carlo case, for which Gt:t+n = Gt, all the updates are made at end of the episode, n = T − t, and Mt = It. Example 9.4 illustrates how interest and emphasis can result in more accurate value To see the potential beneﬁts of using interest and emphasis, consider the four-state Markov reward process shown below: Episodes start in the leftmost state, then transition one state to the right, with a reward of +1, on each step until the terminal state is reached.\\n\\nThe true value of the ﬁrst state is thus 4, of the second state 3, and so on as shown below each state',\n",
       " '55e7ff70-bd43-42e6-8788-f1995a39dfa4': 'These two error functions are compared in Figure 14.4. There are various simple, but widely used, models that work by partitioning the input space into cuboid regions, whose edges are aligned with the axes, and then assigning a simple model (for example, a constant) to each region.\\n\\nThey can be viewed as a model combination method in which only one model is responsible for making predictions at any given point in input space. The process of selecting a speciﬁc model, given a new input x, can be described by a sequential decision making process corresponding to the traversal of a binary tree (one that splits into two branches at each node). Here we focus on a particular tree-based framework called classiﬁcation and regression trees, or CART , although there are many other variants going by such names as ID3 and C4.5 . space, along with the corresponding tree structure. In this example, the ﬁrst step divides the whole of the input space into two regions according to whether x1 ⩽ θ1 or x1 > θ1 where θ1 is a parameter of the model. This creates two subregions, each of which can then be subdivided independently',\n",
       " '10d4c4f6-7f1d-43a1-b7cb-e69069e05dab': 'In its original form, this applies to problems for which the conditional distributions are Gaussian, which represents a more general class of distributions than the multivariate Gaussian because, for example, the non-Gaussian distribution p(z, y) ∝ exp(−z2y2) has Gaussian conditional distributions. At each step of the Gibbs sampling algorithm, the conditional distribution for a particular component zi has some mean µi and some variance σ2 work, the value of zi is replaced with where ν is a Gaussian random variable with zero mean and unit variance, and α is a parameter such that −1 < α < 1.\\n\\nFor α = 0, the method is equivalent to standard Gibbs sampling, and for α < 0 the step is biased to the opposite side of the mean. This step leaves the desired distribution invariant because if zi has mean µi and variance σ2 i. The effect of over-relaxation is to encourage directed motion through state space when the variables are highly correlated. The framework of ordered over-relaxation  generalizes this approach to nonGaussian distributions',\n",
       " '7326fb57-9f53-4da3-882d-452fe2bdaa4d': 'Suppose, however, that it is impractical to sample directly from p(z) but that we can evaluate p(z) easily for any given value of z.\\n\\nOne simplistic strategy for evaluating expectations would be to discretize z-space into a uniform grid and to evaluate the integrand as a sum of the form An obvious problem with this approach is that the number of terms in the summation grows exponentially with the dimensionality of z. Furthermore, as we have already noted, the kinds of probability distributions of interest will often have much of their mass conﬁned to relatively small regions of z space and so uniform sampling will be very inefﬁcient because in high-dimensional problems, only a very small proportion of the samples will make a signiﬁcant contribution to the sum. We would really like to choose the sample points to fall in regions where p(z) is large, or ideally where the product p(z)f(z) is large. As in the case of rejection sampling, importance sampling is based on the use of a proposal distribution q(z) from which it is easy to draw samples, as illustrated in Figure 11.8',\n",
       " '38e40825-d112-4157-a66c-579a13ddf523': 'Dauphin et al. introduced a saddle-free Newton method for second-order optimization and showed that it improves significantly  https://www.deeplearningbook.org/contents/optimization.html   over the traditional version. Second-order methods remain difficult to scale to large  neural networks, but this saddle-free approach holds promise if it can be scaled. 284  CHAPTER 8.\\n\\nOPTIMIZATION FOR TRAINING DEEP MODELS  J(w,b)  Figure 8.3: The objective function for highly nonlinear deep neural networks or for recurrent neural networks often contains sharp nonlinearities in parameter space resulting from the multiplication of several parameters. These nonlinearities give rise to very high derivatives in some places. When the parameters get close to such a cliff region, a gradient descent update can catapult the parameters very far, possibly losing most of the optimization work that has been done. Figure adapted with permission from Pascanu  et al. There are other kinds of points with zero gradient besides minima and saddle points. Maxima are much like saddle points from the perspective of optimization— many algorithms are not attracted to them, but unmodified Newton’s method is',\n",
       " '794e70ed-0944-46d7-9555-191a7a67024e': 'With various teams at the Stanford School of Medicine, we have worked to extend the cross-modal radiology application described in Sect. 4 to a range of other similar cross-modal medical problems, which has involved building robust interfaces for various multi-modal clinical data and formats . In collaboration with several teams at Google, we recently developed a new version of Snorkel, Snorkel DryBell, to interface with Google’s organizational weak supervision resources and compute infrastructure, and enable weak supervision at industrial scale . Another focus has been extending Snorkel to handle richly-formatted data, deﬁned in  as data with multi-modal, semi-structured components such as PDF forms, tables, and HTML pages. Supportforthisrichbutchallengingdatatypehasbeenimplemented in a system built on top of Snorkel, Fonduer , which has been applied to domains such as anti-human trafﬁcking efforts via DARPA’s MEMEX project and extraction of genome-wide association (GWA) studies from the scientiﬁc literature . The goal of Snorkel is to enable users to program the modern machine learning stack, by labeling training data with labeling functions rather than manual annotation',\n",
       " 'ca79fe00-146d-4f59-9d52-a7a796490944': 'We have seen that the joint probability of two independent events is given by the product of the marginal probabilities for each event separately.\\n\\nBecause our data set x is i.i.d., we can therefore write the probability of the data set, given µ and σ2, in the form When viewed as a function of µ and σ2, this is the likelihood function for the Gaussian and is interpreted diagrammatically in Figure 1.14. One common criterion for determining the parameters in a probability distribution using an observed data set is to ﬁnd the parameter values that maximize the likelihood function. This might seem like a strange criterion because, from our foregoing discussion of probability theory, it would seem more natural to maximize the probability of the parameters given the data, not the probability of the data given the parameters. In fact, these two criteria are related, as we shall discuss in the context of curve ﬁtting. Section 1.2.5 For the moment, however, we shall determine values for the unknown parameters µ and σ2 in the Gaussian by maximizing the likelihood function (1.53). In practice, it is more convenient to maximize the log of the likelihood function',\n",
       " '36f04b7f-ec42-473a-8631-ba3703a8c625': 'They applied this successfully to model sequences (protein secondary structure) and introduced a (one-dimensional) convolutional structure in the transition operator of the Markov chain. It is important to remember that, for each step of the Markov chain, one generates a new sequence for each layer, and that sequence is the input for computing other layer values (say the one below and the one above) at the next time step.\\n\\nHence, the Markov chain is really over the output variable (and associated higher-level hidden layers), and the input sequence only serves to condition that chain, with back-propagation enabling it to learn how the input sequence can condition the output distribution implicitly represented by the Markov chain. It is therefore a case of using the GSN in the context of structured outputs. Zohrer and Pernkopf  introduced a hybrid model that combines a su- pervised objective (as in the above work) and an unsupervised objective (as in the original GSN work) by simply adding (with a different weight) the supervised and unsupervised costs, that is, the reconstruction log-probabilities of y and x  711  CHAPTER 20. DEEP GENERATIVE MODELS  respectively',\n",
       " '027772b8-6947-4e7d-847b-3274819c58d3': 'lhe .ill\"\"\"\"\"l\"\\'\" equalions lell\\' U, thaI Y, !-ali,fies.\" s L <bC\".) {<b(x.lTv,} - )\"Y, .., Substituting this expansion back into the eigenvector equation, we obtain The key step is now to express this in terms of the kernel function k(xn , xm ) = ¢(Xn)T¢(xm ), which we do by multiplying both sides by ¢(xZ)T to give This can be written in matrix notation as where ai is an N-dimensional column vector with elements ani for n = 1, ... ,N. We can find solutions for ai by solving the following eigenvalue problem in which we have removed a factor of K from both sides of (12.79). Note that the solutions of (12.79) and (12.80) differ only by eigenvectors of K having zero eigenvalues that do not affect the principal components projection',\n",
       " '8f631beb-7d6d-4846-88fb-cf963d134379': 'We train our model with Cloud TPUs, using 32 to 128 cores depending on the batch size.2 Global BN. Standard ResNets use batch normalization . In distributed training with data parallelism, the BN mean and variance are typically aggregated locally per device. In our contrastive learning, as positive pairs are computed in the same device, the model can exploit the local information leakage to improve prediction accuracy without improving representations. We address this issue by aggregating BN mean and variance over all devices during the training. Other approaches include shufﬂing data examples across devices , or replacing BN with layer norm . 2With 128 TPU v3 cores, it takes ∼1.5 hours to train our ResNet-50 with a batch size of 4096 for 100 epochs. Here we lay out the protocol for our empirical studies, which aim to understand different design choices in our framework. Dataset and Metrics. Most of our study for unsupervised pretraining (learning encoder network f without labels) is done using the ImageNet ILSVRC-2012 dataset .\\n\\nSome additional pretraining experiments on CIFAR-10  can be found in Appendix B.9',\n",
       " '5b9aeb38-ad73-4408-abaa-12e14dddc788': 'In certain important special cases, this computation is tractable and leads directly to optimal solutions, although it does require complete knowledge of the prior distribution of possible problems, which we generally assume is not available.\\n\\nIn addition, neither the theory nor the computational tractability of this approach appear to generalize to the full reinforcement learning problem that we consider in the rest of the book. The Gittins-index approach is an instance of Bayesian methods, which assume a known initial distribution over the action values and then update the distribution exactly after each step (assuming that the true action values are stationary). In general, the update computations can be very complex, but for certain special distributions (called conjugate priors) they are easy. One possibility is to then select actions at each step according to their posterior probability of being the best action. This method, sometimes called posterior sampling or Thompson sampling, often performs similarly to the best of the distribution-free methods we have presented in this chapter. In the Bayesian setting it is even conceivable to compute the optimal balance between exploration and exploitation. One can compute for any possible action the probability of each possible immediate reward and the resultant posterior distributions over action values. This evolving distribution becomes the information state of the problem',\n",
       " 'ae883bcb-f08c-4399-ae67-fd88b87c70c3': 'Following , we aim to control the generation to have one of 7 topics (e.g., “science”); the generated prompt is prepended to one of 20 input sentences (Figure 5) for the pretrained LM to generate continuation sentences. There is no direct supervision data available for training the prompt generator. We randomly create some noisy text as the training data for MLE baselines below and for off-policy updates for our algorithm.\\n\\nSpeciﬁcally, the noisy text is created by sampling keywords and topics from the list used in  and a paraphrase generation model. A.3.1 Comparison with MLE Objective It is interesting to take a closer look at the above objective and compare with the common MLE training. Speciﬁcally, we notice the relations between the optimal Q∗, V ∗, and A∗ functions: A∗ (st, at) = Q∗(st, at) − V ∗(st) = rt + γV ∗(st+1) − V ∗ (st), where the ﬁrst equation is the deﬁnition of A∗ (see Eq.5) and the second equation is due to Eqs. (10) and (5)',\n",
       " 'b00ca20e-4ed4-426b-afdc-7a4646051ba1': 'Q-values as Generation Model Logits. We show the connection of the Q-values with the logits, i.e., outputs right before the softmax layer. Concretely, with the SQL objective, the following relationship between optimal policy π∗ and action-value Q∗ holds : This form is highly reminiscent of the softmax layer of the generation model in Eq.(1). The con3WLOG, we can assume α=1, as it can be folded into the reward function by scaling the latter with 1/α. nection suggests that we can naturally parameterize the Q-function in SQL as the generation model logit function, i.e., Qθ(s, a) ≡ fθ(a|s). In other words, the model output fθ(a|s), originally interpretted as the “logit” of token a given the preceding tokens s, is now re-interpretted as the Q-value of action a in state s. When achieving optimality, fθ∗(a|s), namely Q∗(s, a), represents the best possible future reward achievable by generating token a in state s',\n",
       " '490af8bb-2fea-4162-9f44-349fd8ccf91d': 'We now explore three approaches to learning the parameters of linear discriminant functions, based on least squares, Fisher’s linear discriminant, and the perceptron algorithm. In Chapter 3, we considered models that were linear functions of the parameters, and we saw that the minimization of a sum-of-squares error function led to a simple closed-form solution for the parameter values. It is therefore tempting to see if we can apply the same formalism to classiﬁcation problems. Consider a general classiﬁcation problem with K classes, with a 1-of-K binary coding scheme for the target vector t. One justiﬁcation for using least squares in such a context is that it approximates the conditional expectation E of the target values given the input vector. For the binary coding scheme, this conditional expectation is given by the vector of posterior class probabilities. Unfortunately, however, these probabilities are typically approximated rather poorly, indeed the approximations can have values outside the range (0, 1), due to the limited ﬂexibility of a linear model as we shall see shortly. Each class Ck is described by its own linear model so that where k = 1, . .\\n\\n, K',\n",
       " 'cd26b833-52f2-46fa-9df1-e4d8e2242184': 'In the same paper, the authors propose deep versions of the architecture, but unfortunately that immediately makes computation as expensive as in the original neural auto-regressive network .\\n\\nThe first layer and the output layer can still be computed in O(nh) multiply-add operations, as in the regular NADE, where h is the number of hidden units (the size of the groups h;, in figures 20.10 and 20.9), whereas it is O(n?h) in Bengio and Bengio . For the other hidden layers, however, the computation is O(n? h?) if every “previous” group at layer | participates in predicting the “next” group at layer ]+ 1, assuming n groups of h, hidden units at each layer. Making the 7-th group at layer /+ 1 only depend on the i-th group, as in Uria et al. , at layer | reduces it to O(nh? ),  https://www.deeplearningbook.org/contents/generative_models.html    which 1s still A times worse than the regular NADE. 20.11 Drawing Samples from Autoencoders  In chapter 14, we saw that many kinds of autoencoders learn the data distribution',\n",
       " 'e2247ab8-605e-4987-b253-23f1c917193a': 'The reduction in variance is greater if the variance in the posterior mean is greater. Note, however, that this result only holds on average, and that for a particular observed data set it is possible for the posterior variance to be larger than the prior variance. Binary variables can be used to describe quantities that can take one of two possible values. Often, however, we encounter discrete variables that can take on one of K possible mutually exclusive states.\\n\\nAlthough there are various alternative ways to express such variables, we shall see shortly that a particularly convenient representation is the 1-of-K scheme in which the variable is represented by a K-dimensional vector x in which one of the elements xk equals 1, and all remaining elements equal 0. So, for instance if we have a variable that can take K = 6 states and a particular observation of the variable happens to correspond to the state where x3 = 1, then x will be represented by x = (0, 0, 1, 0, 0, 0)T. (2.25) Note that such vectors satisfy �K k=1 xk = 1. If we denote the probability of xk = 1 by the parameter µk, then the distribution of x is given where µ = (µ1,',\n",
       " 'edf7c46e-98e0-486d-9ac1-682a0418aa8b': \" https://lilianweng.github.io/posts/2021-05-31-contrastive/  Contrastive Representation Learning | Lil'Log  St tt tt tt te He  image_encoder - ResNet or Vision Transformer text_encoder - CBOW or Text Transformer  I - minibatch of aligned images T - learned proj of image to embed W_t - learned proj of text to embed t - learned temperature parameter  xtract feature representations of each modality  # e I_f = image_encoder(I) # T_f = text_encoder(T) #  # joint multimodal embedding  np.linalg.norm(np.dot(I_f, W_i), Wt),  axis=1) axis=1)  # scaled pairwise cosine similarities   logits = np.dot(I_e, T_e.T) * np.exp(t)  # symmetric loss function  labels = np.arange(n)  loss_i = cross_entropy_loss(logits, labels, axis=@) loss_t = cross_entropy_loss(logits, labels, axis=1) loss = (loss_i + loss_t)/2  Compared to other methods above for learning good visual representation, what makes CLIP really special is “the appreciation of using natural language as a training signal”\",\n",
       " 'a684a92f-04d3-468a-adc5-ea72e0b8448a': 'They therefore have the same  702  CHAPTER 20.\\n\\nDEEP GENERATIVE MODELS  Figure 20.8: A fully visible belief network predicts the i-th variable from the i — 1 previous ones. (Top)The directed graphical model for an FVBN. (Bottom)Corresponding computational graph for the logistic FVBN, where each prediction is made by a linear predictor. advantages and disadvantages as linear classifiers. Like linear classifiers, they may be trained with convex loss functions and sometimes admit closed form solutions (as in the Gaussian case). Like linear classifiers, the model itself does not offer a way of increasing its capacity, so capacity must be raised using techniques like basis expansions of the input or the kernel trick',\n",
       " '6cd5b572-4870-43af-a417-8a78eb8082d3': 'The geometric transformations stud- ied were flipping, — 30° to 30° rotations, and cropping. The color space transformations studied were color jittering, (random color manipulation), edge enhancement, and PCA. They tested these augmentations with 4-fold cross-validation on the Caltech101 dataset filtered to 8421 images of size 256 x 256 (Table 1). Kernel filters  Kernel filters are a very popular technique in image processing to sharpen and blur images. These filters work by sliding an n x n matrix across an image with either a Gauss- ian blur filter, which will result in a blurrier image, or a high contrast vertical or hori- zontal edge filter which will result in a sharper image along edges.\\n\\nIntuitively, blurring images for Data Augmentation could lead to higher resistance to motion blur during testing. Additionally, sharpening images for Data Augmentation could result in encapsu- lating more details about objects of interest. Sharpening and blurring are some of the classical ways of applying kernel filters to images. Kang et al',\n",
       " 'ed934a3c-dc06-4223-a145-51c8c2cd2e6f': 'Triplet loss learns to minimize the distance between the anchor x and positive x* and maximize the distance between the anchor x and negative x~ at the same time with the following equation:  Leripler(x,*, x) = SP max (0, || f(x) — f&*)I3 — F() — FOO )II3 + €)  xer  where the margin parameter € is configured as the minimum offset between distances of similar vs dissimilar pairs. It is crucial to select challenging x~ to truly improve the model',\n",
       " 'a614b423-ceba-4dda-8ffa-87fb3cf70a0c': 'This type of policy can result in more clicks by a user over repeated visits to the site, and if the policy is suitably designed, more eventual sales. Working at Adobe Systems Incorporated, Theocharous et al. conducted experiments to see if policies designed to maximize clicks over the long term could in fact improve over short-term greedy policies. The Adobe Marketing Cloud, a set of tools that many companies use to run digital marketing campaigns, provides infrastructure for automating user-targeted advertising and fund-raising campaigns. Actually deploying novel policies using these tools entails signiﬁcant risk because a new policy may end up performing poorly. For this reason, the research team needed to assess what a policy’s performance would be if it were to be actually deployed, but to do so on the basis of data collected under the execution of other policies. A critical aspect of this research, then, was o↵-policy evaluation. Further, the team wanted to do this with high conﬁdence to reduce the risk of deploying a new policy. Although high conﬁdence o↵-policy evaluation was a central component of this research , here we focus only on the algorithms and their results.\\n\\nTheocharous et al',\n",
       " '0bc648b0-78ef-4b39-8f49-0f7ddd0de857': 'For a given ﬁnite data set, it is possible for the Bayes factor to be larger for the incorrect model.\\n\\nHowever, if we average the Bayes factor over the distribution of data sets, we obtain the expected Bayes factor in the form where the average has been taken with respect to the true distribution of the data. This quantity is an example of the Kullback-Leibler divergence and satisﬁes the propSection 1.6.1 erty of always being positive unless the two distributions are equal in which case it is zero. Thus on average the Bayes factor will always favour the correct model. We have seen that the Bayesian framework avoids the problem of over-ﬁtting and allows models to be compared on the basis of the training data alone. However, a Bayesian approach, like any approach to pattern recognition, needs to make assumptions about the form of the model, and if these are invalid then the results can be misleading. In particular, we see from Figure 3.12 that the model evidence can be sensitive to many aspects of the prior, such as the behaviour in the tails',\n",
       " 'c14b0ab4-6c74-4f18-a6cc-ca3a2624dfaf': 'As proved on  we know that MMD is a proper metric and not only a pseudometric when the kernel is universal.\\n\\nIn the speciﬁc case where H = L2(X, m) for m the normalized Lebesgue measure on X, we know that {f ∈ Cb(X), ∥f∥∞ ≤ 1} will be contained in F, and therefore dF(Pr, Pθ) ≤ δ(Pr, Pθ) so the regularity of the MMD distance as a loss function will be at least as bad as the one of the total variation. Nevertheless this is a very extreme case, since we would need a very powerful kernel to approximate the whole L2. However, even Gaussian kernels are able to detect tiny noise patterns as recently evidenced by . This points to the fact that especially with low bandwidth kernels, the distance might be close to a saturating regime similar as with total variation or the JS. This obviously doesn’t need to be the case for every kernel, and ﬁguring out how and which diﬀerent MMDs are closer to Wasserstein or total variation distances is an interesting topic of research',\n",
       " '09637815-5c2c-4e24-8a5b-38088213bc89': 'The states might be represented only by feature vectors, which may do little to distinguish the states from each other. As a special case, all of the feature vectors may be the same. Thus one really has only the reward sequence (and the actions), and performance has to be assessed purely from these. How could it be done? One way is by averaging the rewards over a long interval—this is the idea of the average-reward setting. How could discounting be used? Well, for each time step we could measure the discounted return. Some returns would be small and some big, so again we would have to average them over a suﬃciently large time interval. In the continuing setting there are no starts and ends, and no special time steps, so there is nothing else that could be done. However, if you do this, it turns out that the average of the discounted returns is proportional to the average reward. In fact, for policy ⇡, the average of the discounted returns is always r(⇡)/(1 − γ), that is, it is essentially the average reward, r(⇡).\\n\\nIn particular, the ordering of all policies in the average discounted return setting would be exactly the same as in the average-reward setting',\n",
       " 'aa070f83-bf3b-4c83-b3e6-ab7e10e626d3': 'The eigenvalues λi measure the curvature of the likelihood function, and so in Figure 3.15 the eigenvalue λ1 is small compared with λ2 (because a smaller curvature corresponds to a greater elongation of the contours of the likelihood function). Because βΦTΦ is a positive deﬁnite matrix, it will have positive eigenvalues, and so the ratio λi/(λi + α) will lie between 0 and 1. Consequently, the quantity γ deﬁned by (3.91) will lie in the range 0 ⩽ γ ⩽ M. For directions in which λi ≫ α, the corresponding parameter wi will be close to its maximum likelihood value, and the ratio λi/(λi + α) will be close to 1. Such parameters are called well determined because their values are tightly constrained by the data. Conversely, for directions in which λi ≪ α, the corresponding parameters wi will be close to zero, as will the ratios λi/(λi +α). These are directions in which the likelihood function is relatively insensitive to the parameter value and so the parameter has been set to a small value by the prior',\n",
       " '1b41f4e5-1592-4236-be33-ef7a0cfd7449': 'The only difference is how the numbers are ar- ranged in a grid to form a tensor.\\n\\nWe could imagine flattening each tensor into a vector before we run back-propagation, computing a vector-valued gradient, and then reshaping the gradient back into a tensor. In this rearranged view, back-propagation is still just multiplying Jacobians by gradients. To denote the gradient of a value z with respect to a tensor X, we write V xz, just as if X were a vector. The indices into X now have multiple coordinates—for example, a 3-D tensor is indexed by three coordinates. We can abstract this away by using a single variable i to represent the complete tuple of indices. For all possible index tuples i, (Vx z); gives e- This is exactly the same as how for all  possible integer indices 7 into a vector, (Vzz); gives ae Using this notation, we can write the chain rule as it applies to tensors',\n",
       " '2d12186f-7c01-49d7-a114-16c864fc0892': '1.21 (⋆ ⋆) Consider two nonnegative numbers a and b, and show that, if a ⩽ b, then a ⩽ (ab)1/2. Use this result to show that, if the decision regions of a two-class classiﬁcation problem are chosen to minimize the probability of misclassiﬁcation, this probability will satisfy 1.22 (⋆) www Given a loss matrix with elements Lkj, the expected risk is minimized if, for each x, we choose the class that minimizes (1.81). Verify that, when the loss matrix is given by Lkj = 1 − Ikj, where Ikj are the elements of the identity matrix, this reduces to the criterion of choosing the class having the largest posterior probability. What is the interpretation of this form of loss matrix? 1.24 (⋆ ⋆) www Consider a classiﬁcation problem in which the loss incurred when an input vector from class Ck is classiﬁed as belonging to class Cj is given by the loss matrix Lkj, and for which the loss incurred in selecting the reject option is λ. Find the decision criterion that will give the minimum expected loss',\n",
       " 'c74584a5-79e9-48c4-9950-1ea529f26f2e': 'DBNs only need to use MCMC sampling in their top pair of layers. The other layers are used only at the end of the sampling process, in one efficient ancestral sampling pass. To generate a sample from a DBM, it is necessary to use MCMC across all layers, with every layer of the model participating in every Markov chain transition. 20.4.2 DBM Mean Field Inference  The conditional distribution over one DBM layer given the neighboring layers is factorial. In the example of the DBM with two hidden layers, these distributions are Pv | AY), P(A | v,h@), and P(h@) | Ah). The distribution over ail hidden layers generally does not factorize because of interactions between layers.\\n\\nIn he example with two hidden layers, P(AY), hn?) | v) does not factorize because of che interaction weights W®) between Ah“ and h@), which render these variables mutually dependent. As was the case with the DBN, we are left to seek out methods to approximate he DBM posterior distribution',\n",
       " 'f6913ad0-4997-44f6-8fb2-736033af06ed': \"The factorial nature of the conditional P(h | v) follows immediately from our ability to write the joint probability over the vector h as the product of (unnormalized) distributions over the individual elements, h;. It is now a simple matter of normalizing the distributions over the individual binary h,. P(hj = P(h; =1\\\\v) == (hj = 1] ») (20.12) P(hj =0|v)+ P(h; =1|v) = exp 9 +015 (20.13) exp {0} + exp {ej + 0! W. 5} , =o c+uew.;. — f (20.14)  We can now express the full condifional over tHe hidden layer as the factorial  https://www.deeplearningbook.org/contents/generative_models.html    distribution: nh  P(h| v) = TJ o(@h- 0 (e+W'e)),\",\n",
       " '98841e6d-b5c3-4687-9ce2-0808f4648d26': 'Dynamic programming requires a distribution model because it uses expected updates, which involve computing expectations over all the possible next states and rewards. A sample model, on the other hand, is what is needed to simulate interacting with the environment during which sample updates, like those used by many reinforcement learning algorithms, can be used. Sample models are generally much easier to obtain than distribution models.\\n\\nWe have presented a perspective emphasizing the surprisingly close relationships between planning optimal behavior and learning optimal behavior. Both involve estimating the same value functions, and in both cases it is natural to update the estimates incrementally, in a long series of small backing-up operations. This makes it straightforward to integrate learning and planning processes simply by allowing both to update the same estimated value function. In addition, any of the learning methods can be converted into planning methods simply by applying them to simulated (model-generated) experience rather than to real experience. In this case learning and planning become even more similar; they are possibly identical algorithms operating on two di↵erent sources of experience. It is straightforward to integrate incremental planning methods with acting and modelthe diagram on page 162), each producing what the other needs to improve; no other interaction among them is either required or prohibited. The most natural approach is for all processes to proceed asynchronously and in parallel',\n",
       " '23a3e893-7c29-4b76-b6f6-046e3a22c6d1': 'This demonstrates the data-specific design of augmen- tations and the challenge of developing generalizable augmentation policies. This is an  important consideration with respect to the geometric augmentations listed below. Flipping  Horizontal axis flipping is much more common than flipping the vertical axis. This aug- mentation is one of the easiest to implement and has proven useful on datasets such as CIFAR-10 and ImageNet. On datasets involving text recognition such as MNIST or  SVHN, this is not a label-preserving transformation. Shorten and Khoshgoftaar J Big Data  6:60   Color space  Digital image data is usually encoded as a tensor of the dimension (height x width x color channels). Performing augmentations in the color channels space is another strategy that is very practical to implement. Very simple color augmentations include isolating a single color channel such as R, G, or B',\n",
       " '4b6437c3-110b-4dd7-9fbe-326613bdc364': 'S., Wheeler, R. M. An N-player sequential stochastic game with identical payo↵s.\\n\\nIEEE Transactions on Systems, Man, and Cybernetics, 6:1154–1158. Narendra, K. S., Wheeler, R. M. Decentralized learning in ﬁnite Markov chains. IEEE Nedi´c, A., Bertsekas, D. P. Least squares policy evaluation algorithms with linear function approximation. Discrete Event Dynamic Systems, 13(1-2):79–110. Ng, A. Y. Shaping and Policy Search in Reinforcement Learning. Ph.D. thesis, University Theory and application to reward shaping. In I. Bratko and S. Dzeroski (Eds. ), Proceedings of the 16th International Conference on Machine Learning , pp. 278–287. Ng, A. Y., Russell, S. J. Algorithms for inverse reinforcement learning',\n",
       " '2bcc5ad9-2116-4f2e-9af4-59ed47b71d57': 'The distinction between model-free and model-based algorithms is proving to be useful for this research. One can examine the computational implications of these types of algorithms in abstract settings that expose basic advantages and limitations of each type.\\n\\nThis serves both to suggest and to sharpen questions that guide the design of experiments necessary for increasing psychologists’ understanding of habitual and goal-directed behavioral control. Our goal in this chapter has been to discuss correspondences between reinforcement learning and the experimental study of animal learning in psychology. We emphasized at the outset that reinforcement learning as described in this book is not intended to model details of animal behavior. It is an abstract computational framework that explores idealized situations from the perspective of artiﬁcial intelligence and engineering. But many of the basic reinforcement learning algorithms were inspired by psychological theories, and in some cases, these algorithms have contributed to the development of new animal learning models. This chapter described the most conspicuous of these correspondences. The distinction in reinforcement learning between algorithms for prediction and algorithms for control parallels animal learning theory’s distinction between classical, or Pavlovian, conditioning and instrumental conditioning',\n",
       " '79d8511e-6f77-45c9-9430-6c2e4c98b1a5': 'Figure 4 shows the topic accuracy of the controlled LM outputs averaged across the 7 topics, and Table 1 shows the respective language quality results. More detailed topic accuracy results and samples are provided in the appendix (§A.1.3) (where GeDi obtained low accuracy on 2 of the 7 topics, possibly because the topic tokens are tokenized into two subwords for which the model released by the authors was not speciﬁcally trained). 5https://huggingface.co/distilgpt2 6Note that the language quality emphasis is on the generated sentences. Prompts themselves do not necessarily have to be human-readable . We can see that the prompts generated by our SQL cause the LM to generate sentences with high topic accuracy while maintaining low perplexity in most settings. Increasing the prompt length positively impacts the topic accuracy, which makes sense because longer prompts give more ﬂexible for steering the LM',\n",
       " '028d4bdf-baa7-4d12-947d-4d79f3c4e3eb': 'Other models, presented later, apply these principles to learning stochastic mappings, functions with feedback, and probability distributions over a single vector. We begin this chapter with a simple example of a feedforward network. Next, we address each of the design decisions needed to deploy a feedforward network. First, training a feedforward network requires making many of the same design decisions as are necessary for a linear model: choosing the optimizer, the cost function, and the form of the output units. We review these basics of gradient-based learning, then proceed to confront some of the design decisions that are unique to feedforward networks.\\n\\nFeedforward networks have introduced the concept of a hidden layer, and this requires us to choose the activation functions that will be used to compute the hidden layer values. We must also design the architecture of the network, including how many layers the network should contain, how these layers should be connected to each other, and how many units should be in each layer. Learning in deep neural networks requires computing the gradients of complicated functions. We present the back-propagation algorithm and its modern generalizations, which can be used to efficiently compute these gradients. Finally, we close with some historical perspective',\n",
       " '2c00295a-203e-4ede-9699-ffcec6610948': 'Kuleshov, V., Hancock, B., Ratner, A., Ré C, Batzaglou, S., Snyder, M.: A machine-compiled database of genome-wide association studies. NIPS ML4H Workshop  30.\\n\\nLehmann, J., Isele, R., Jakob, M., Jentzsch, A., Kontokostas, D., Mendes,P.,Hellmann,S.,Morsey,M.,vanKleef,P.,Auer,S.,Bizer, C.: DBpedia–A large-scale, multilingual knowledge base extracted from Wikipedia. Semantic Web Journal  31. Li, H., Yu, B., Zhou, D.: Error rate analysis of labeling by crowdsourcing. In: ICML Workshop: Machine Learning Meets Crowdsourcing. Atalanta, Georgia, USA  32. Li, Y., Gao, J., Meng, C., Li, Q., Su, L., Zhao, B., Fan, W., Han, J.: A survey on truth discovery. SIGKDD Explor. Newsl',\n",
       " '9b7cde84-5c7a-4716-ab63-a9d532905bfc': 'For example, suppose one wants to o↵er v0 : S ! R as an initial guess at the true optimal value function v⇤, and that one is using linear function approximation with features x : S ! Rd. Then one would deﬁne the initial value function approximation to be and update the weights w as usual. If the initial weight vector is 0, then the initial value function will be v0, but the asymptotic solution quality will be determined by the feature vectors as usual. This initialization can be done for arbitrary nonlinear approximators and arbitrary forms of v0, though it is not guaranteed to always accelerate learning. A particularly e↵ective approach to the sparse reward problem is the shaping technique introduced by the psychologist B. F. Skinner and described in Section 14.3',\n",
       " '46b37fbc-03db-46c4-a471-ea4da9e969b1': '3.5 (⋆) www Using the technique of Lagrange multipliers, discussed in Appendix E, show that minimization of the regularized error function (3.29) is equivalent to minimizing the unregularized sum-of-squares error (3.12) subject to the constraint (3.30).\\n\\nDiscuss the relationship between the parameters η and λ. together with a training data set comprising input basis vectors φ(xn) and corresponding target vectors tn, with n = 1, . , N. Show that the maximum likelihood solution WML for the parameter matrix W has the property that each column is given by an expression of the form (3.15), which was the solution for an isotropic noise distribution. Note that this is independent of the covariance matrix Σ. Show that the maximum likelihood solution for Σ is given by 3.7 (⋆) By using the technique of completing the square, verify the result (3.49) for the posterior distribution of the parameters w in the linear basis function model in which mN and SN are deﬁned by (3.50) and (3.51) respectively',\n",
       " '17f0a051-235b-4570-b1b8-ac611a3323bf': \"https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Log   The training process in Matching Networks is designed to match inference at test time, see the details in the earlier section. It is worthy of mentioning that the Matching Networks paper refined the idea that training and testing conditions should match. o* = arg max Eycc|Estcp,pt-p| S Pa(yl|x, $”)]]  (x,y)EBL  Relation Network  Relation Network (RN)  is similar to siamese network but with a few differences:  The relationship is not captured by a simple L1 distance in the feature space, but predicted by a CNN classifier gy.\\n\\nThe relation score between a pair of inputs, x; and x,, isr;; = go() where  is concatenation\",\n",
       " '60c86c7a-2fe6-4c52-ad56-5e2fa1656474': 'Optimization of this objective is equivalent to approximate MAP estimation, where the likelihood gradient is approximated by the gradient of the lower bound. We compared performance of AEVB to the wake-sleep algorithm . We employed the same encoder (also called recognition model) for the wake-sleep algorithm and the variational autoencoder. All parameters, both variational and generative, were initialized by random sampling from N(0, 0.01), and were jointly stochastically optimized using the MAP criterion. Stepsizes were adapted with Adagrad ; the Adagrad global stepsize parameters were chosen from {0.01, 0.02, 0.1} based on performance on the training set in the ﬁrst few iterations. Minibatches of size M = 100 were used, with L = 1 samples per datapoint. Likelihood lower bound We trained generative models (decoders) and corresponding encoders (a.k.a.\\n\\nrecognition models) having 500 hidden units in case of MNIST, and 200 hidden units in case of the Frey Face dataset (to prevent overﬁtting, since it is a considerably smaller dataset)',\n",
       " '3ce9e25b-b8c9-4cc2-a66e-5000b3a1f70b': 'We decouple the prediction task and the encoder architecture, so we do not require a context aggregation network, and our encoder can look at the images of wider spectrum of resolutions.\\n\\nIn addition, we use the NT-Xent loss function, which leverages normalization and temperature, whereas they use an unnormalized cross-entropy-based objective. We use simpler data augmentation. • InstDisc, MoCo, PIRL  generalize the Exemplar approach originally proposed by Dosovitskiy et al. and leverage an explicit memory bank. We do not use a memory bank; we ﬁnd that, with a larger batch size, in-batch negative example sampling sufﬁces. We also utilize a nonlinear projection head, and use the representation before the projection head. Although we use similar types of augmentations (e.g., random crop and color distortion), we expect speciﬁc parameters may be different. • CMC  uses a separated network for each view, while we simply use a single network shared for all randomly augmented views. The data augmentation, projection head and loss function are also different. We use larger batch size instead of a memory bank. • Whereas Ye et al',\n",
       " '89e13e26-6efc-4f97-a17d-56a2ab533f6f': 'Instead, there is a general equation for the mean field fixed-point updates.\\n\\nIf we make the mean field approximation  a(h |v) =T] ahs |»), (19.55)  and fix g(hj | v) for all 7 A i, then the optimal q(h; | v) may be obtained by normalizing the unnormalized distribution G(hi |v) = exp Ep_jvq(n_,|vylogp(v,h) , (19.56)  as long as p does not assign 0 probability to any joint configuration of variables. Carrying out the expectation insidd the equation will yield dhe correct functional form of g(h; __). Deriving functional forms of q directly using calculus of variations is only necepgary if one wishes to develop a new form of variational learning;  https://www.deeplearningbook.org/contents/inference.html    equation 19.56 yields the mean field approximation for any probabilistic model. Equation 19.56 is a fixed-point equation, designed to be iteratively applied for each value of 7 repeatedly until convergence. However, it also tells us more than  that',\n",
       " '4a3e88e3-ce54-4ff2-aa06-28d0d844dba5': 'For example, we can add weight decay to the linear regression cost function  https://www.deeplearningbook.org/contents/ml.html   LO ODLaLLL J(w, b) = AI|w||2 — Ex.v~paata log pmodel(y | x), (5.101) This still allows closed form optimization. If we change the model to be nonlinear, then most cost functions can no longer be optimized in closed form. This requires us to choose an iterative numerical optimization procedure, such as gradient descent. The recipe for constructing a learning algorithm by combining models, costs, and optimization algorithms supports both supervised and unsupervised learning.\\n\\nThe linear regression example shows how to support supervised learning. Unsupervised learning can be supported by defining a dataset that contains only X and providing an appropriate unsupervised cost and model. For example, we can obtain the first PCA vector by specifying that our loss function is  J(w) =E \\\\|x — r(w; w)||3 (5.102)  X~Paata  while our model is defined to have w with norm one and reconstruction function  r(a) = wlaew. 151  CHAPTER 5',\n",
       " 'cf1f714d-eae1-4291-9648-5b1a880b5eea': 'Similarly, the optimal solution for the factor qτ(τ) is given by and hence qτ(τ) is a gamma distribution Gam(τ|aN, bN) with parameters It should be emphasized that we did not assume these speciﬁc functional forms for the optimal distributions qµ(µ) and qτ(τ).\\n\\nThey arose naturally from the structure of the likelihood function and the corresponding conjugate priors. Section 10.4.1 Thus we have expressions for the optimal distributions qµ(µ) and qτ(τ) each of which depends on moments evaluated with respect to the other distribution. One approach to ﬁnding a solution is therefore to make an initial guess for, say, the moment E and use this to re-compute the distribution qµ(µ). Given this revised distribution we can then extract the required moments E and E, and use these to recompute the distribution qτ(τ), and so on. Since the space of hidden variables for this example is only two dimensional, we can illustrate the variational approximation to the posterior distribution by plotting contours of both the true posterior and the factorized approximation, as illustrated in Figure 10.4. tion',\n",
       " '379287ca-8b21-46f9-b902-ca1fc7c86df3': 'N000141210041 and N000141310129, the Moore Foundation, the Okawa Research Grant, American Family Insurance, Accenture, Toshiba, the Stanford Interdisciplinary Graduate and Bio-X fellowships, and members of the Stanford DAWN project: Intel, Microsoft, Teradata, and VMware.\\n\\nThe US Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, ﬁndings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reﬂect the views, policies, or endorsements, either expressed or implied, of DARPA, DOE, NIH, ONR, or the US Government. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecomm ons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. 1',\n",
       " 'bc012ce0-e56c-4af2-b73f-508a6ed17d6e': 'The linear and quadratic decision boundaries are illustrated in Figure 4.11. distribution, coloured red, green, and blue, in which the red and green classes have the same covariance matrix. The right-hand plot shows the corresponding posterior probabilities, in which the RGB colour vector represents the posterior probabilities for the respective three classes. The decision boundaries are also shown. Notice that the boundary between the red and green classes, which have the same covariance matrix, is linear, whereas those between the other pairs of classes are quadratic. Once we have speciﬁed a parametric functional form for the class-conditional densities p(x|Ck), we can then determine the values of the parameters, together with the prior class probabilities p(Ck), using maximum likelihood. This requires a data set comprising observations of x along with their corresponding class labels. Consider ﬁrst the case of two classes, each having a Gaussian class-conditional density with a shared covariance matrix, and suppose we have a data set {xn, tn} where n = 1, . , N. Here tn = 1 denotes class C1 and tn = 0 denotes class C2',\n",
       " '12984491-fdb8-4749-be81-db71c794c2f2': 'Thus, if the kernel function k(xn, xm) depends only on the distance ∥xn − xm∥, then we obtain an expansion in radial basis functions. The results (6.66) and (6.67) deﬁne the predictive distribution for Gaussian process regression with an arbitrary kernel function k(xn, xm). In the particular case in which the kernel function k(x, x′) is deﬁned in terms of a ﬁnite set of basis functions, we can derive the results obtained previously in Section 3.3.2 for linear regression starting from the Gaussian process viewpoint. Exercise 6.21 For such models, we can therefore obtain the predictive distribution either by taking a parameter space viewpoint and using the linear regression result or by taking a function space viewpoint and using the Gaussian process result. The central computational operation in using Gaussian processes will involve the inversion of a matrix of size N × N, for which standard methods require O(N 3) computations. By contrast, in the basis function model we have to invert a matrix SN of size M × M, which has O(M 3) computational complexity',\n",
       " '0bc7106a-e97d-4983-9977-78f151cc37ea': 'We did not reach for the highest possible level of mathematical abstraction and did not rely on a theorem–proof format.\\n\\nWe tried to choose a level of mathematical detail that points the mathematically inclined in the right directions without distracting from the simplicity and potential generality of the underlying ideas. of people to thank. First, we thank those who have personally helped us develop the overall view presented in this book: Harry Klopf, for helping us recognize that reinforcement learning needed to be revived; Chris Watkins, Dimitri Bertsekas, John Tsitsiklis, and Paul Werbos, for helping us see the value of the relationships to dynamic programming; John Moore and Jim Kehoe, for insights and inspirations from animal learning theory; Oliver Selfridge, for emphasizing the breadth and importance of adaptation; and, more generally, our colleagues and students who have contributed in countless ways: Ron Williams, Charles Anderson, Satinder Singh, Sridhar Mahadevan, Steve Bradtke, Bob Crites, Peter Dayan, and Leemon Baird',\n",
       " 'f9c2498d-dbaa-413e-917a-a32fe789632b': 'In principle, we could also treat 6 as a parameter of the model and learn it. Our presentation here has discarded some terms that do not depend on h but do depend on 8. To learn 3, these terms must be included, or ( will collapse to 0. Not all approaches to sparse coding explicitly build a p(h) and a p(a | h). Often we are just interested in learning a dictionary of features with activation values that will often be zero when extracted using this inference procedure. If we sample h from a Laplace prior, it is in fact a zero probability event for an element of h to actually be zero. The generative model itself is not especially sparse; only the feature extractor is.\\n\\nGoodfellow et al. describe approximate inference in a different model family, the spike and slab sparse coding model, for which samples from the prior usually contain true zeros. 493  CHAPTER 13. LINEAR FACTOR MODELS  The sparse coding approach combined with the use of the nonparametric encoder can in principle minimize the combination of reconstruction error and log-prior better than any specific parametric encoder',\n",
       " 'daca351c-25ff-4795-b7ca-75e1d16efd67': 'In the context of models with latent variables, which define a joint distribution Pmodel(x, h), we often draw samples of x by alternating between sampling from Pmodel(& | A) and sampling from pmodei(h | x). From the point of view of mixing rapidly, we would like pmode( | #) to have high entropy. From the point of view of learning a useful representation of h, we would like h to encode enough information about x to reconstruct it well, which implies that h and a should have high mutual information. These two goals are at odds with each other. We often learn generative models that very precisely encode a into h but are not able to mix very well. This situation arises frequently with Boltzmann machines—the sharper the distribution a Boltzmann machine learns, the harder it is for a Markov chain sampling from the model distribution to mix well. This problem is illustrated in figure 17.2.\\n\\nAll this could make MCMC methods less useful when the distribution of interest has a manifold structure with a separate manifold for each class: the distribution is concentrated around many modes, and these modes are separated by vast regions of high energy',\n",
       " '30f4d581-c211-4dd1-a105-68bade3fbf55': 'Linked importance sampling Both AIS and bridge sampling have their ad- vantages. If Dxy(pollpi) is not too large (because po and py, are sufficiently close), bridge sampling can be a more effective means of estimating the ratio of partition functions than AIS. If, however, the two distributions are too far apart for a single distribution p, to bridge the gap, then one can at least use AIS with potentially many intermediate distributions to span the distance between pg and p,. Neal  showed how his linked importance sampling method leveraged the power of the bridge sampling strategy to bridge the intermediate distributions used in AIS and significantly improve the overall partition function estimates. 627  https://www.deeplearningbook.org/contents/partition.html    CHAPTER 18. CONFRONTING THE PARTITION FUNCTION  Estimating the partition function while training While AIS has become accepted as the standard method for estimating the partition function for many undirected models, it is sufficiently computationally intensive that it remains infeasible to use during training.\\n\\nAlternative strategies have been explored to maintain an estimate of the partition function throughout training',\n",
       " '4cf5b680-ca58-45ce-840d-483a150ce0dc': 'The Experience, E  Machine learning algorithms can be broadly categorized as unsupervised or supervised by what kind of experience they are allowed to have during the learning process. Most of the learning algorithms in this book can be understood as being allowed  102  CHAPTER 5. MACHINE LEARNING BASICS  to experience an entire dataset. A dataset is a collection of many examples, as defined in section 5.1.1. Sometimes we call examples data points.\\n\\nOne of the oldest datasets studied by statisticians and machine learning re- searchers is the Iris dataset . It is a collection of measurements of different parts of 150 iris plants. Each individual plant corresponds to one example. The features within each example are the measurements of each part of the plant: the sepal length, sepal width, petal length and petal width. The dataset also records which species each plant belonged to. Three different species are represented in the dataset. Unsupervised learning algorithms experience a dataset containing many features, then learn useful properties of the structure of this dataset. In the context of deep learning, we usually want to learn the entire probability distribution that generated a dataset, whether explicitly, as in density estimation, or implicitly, for tasks like synthesis or denoising',\n",
       " '0fbee514-c110-4443-b3c2-a7fb8cae26aa': 'For the moment, Section 10.1 however, we simply note that in applying maximum likelihood to Gaussian mixture models we must take steps to avoid ﬁnding such pathological solutions and instead seek local maxima of the likelihood function that are well behaved.\\n\\nWe can hope to avoid the singularities by using suitable heuristics, for instance by detecting when a Gaussian component is collapsing and resetting its mean to a randomly chosen value while also resetting its covariance to some large value, and then continuing with the optimization. A further issue in ﬁnding maximum likelihood solutions arises from the fact that for any given maximum likelihood solution, a K-component mixture will have a total of K! equivalent solutions corresponding to the K! ways of assigning K sets of parameters to K components. In other words, for any given (nondegenerate) point in the space of parameter values there will be a further K!−1 additional points all of which give rise to exactly the same distribution. This problem is known as identiﬁability  and is an important issue when we wish to interpret the parameter values discovered by a model. Identiﬁability will also arise when we discuss models having continuous latent variables in Chapter 12',\n",
       " '0a71defe-2c69-4300-acf5-34e4a2d03cb6': '(Right)With weight decay approaching zero (i.e., using the Moore-Penrose pseudoinverse to solve the underdetermined problem with minimal regularization), the degree-9 polynomial overfits significantly, as we saw in figure 5.2. 118  CHAPTER 5. MACHINE LEARNING BASICS  Sometimes a setting is chosen to be a hyperparameter that the learning algo- rithm does not learn because the setting is difficult to optimize. More frequently,  https://www.deeplearningbook.org/contents/ml.html    the setting must be a hyperparameter because it is not appropriate to learn that hyperparameter on the training set. This applies to all -hyperparameters that control model capacity.\\n\\nIf learned on the training set, such hyperparameters would always choose the maximum possible model capacity, resulting in overfitting (refer to figure 5.3). For example, we can always fit the training set better with a higher-degree polynomial and a weight decay setting of \\\\ = 0 than we could with a lower-degree polynomial and a positive weight decay setting. To solve this problem, we need a validation set of examples that the training algorithm does not observe',\n",
       " '2c9594a6-79be-4668-b1ce-fece29ed44f6': 'The convergence point of gradient descent depends on the initial values of the parameters.\\n\\nIn practice, gradient descent would usually not find clean, easily understood, integer-valued solutions like the one we presented here. 6.2 Gradient-Based Learning  Designing and training a neural network is not much different from training any other machine learning model with gradient descent. In section 5.10, we described how to build a machine learning algorithm by specifying an optimization procedure, a cost function, and a model family. 172  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  The largest difference between the linear models we have seen so far and neural networks is that the nonlinearity of a neural network causes most interesting loss functions to become nonconvex. This means that neural networks are usually trained by using iterative, gradient-based optimizers that merely drive the cost function to a very low value, rather than the linear equation solvers used to train linear regression models or the convex optimization algorithms with global conver- gence guarantees used to train logistic regression or SVMs',\n",
       " 'a5c0e321-b4b2-438b-b32f-9919a88b5626': 'A successful example of such a method is the Variational Auto-Encoder (VAE),  https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/  Self-supervised learning: The dark matter of intelligence   in which the latent variable is made “fuzzy”, which limits its capacity. But VAE have not yet been shown to produce good representations for downstream visual tasks. Another successful example is sparse modeling, but its use has been limited to simple architectures. No perfect recipe seems to exist to limit the capacity of latent variables. The challenge of the next few years may be to devise non-contrastive methods for latent-variable energy- based model that successfully produce good representations of image, video, speech, and other signals and yield top performance in downstream supervised tasks without requiring large amounts of labeled data',\n",
       " 'dc17bf29-b83a-4370-bdec-6f2eed469e71': 'If we deﬁne p(t = 1|y) = σ(y) where y(x) is given by (7.1), show that the negative log likelihood, with the addition of a quadratic regularization term, takes the form (7.47). 7.7 (⋆) Consider the Lagrangian (7.56) for the regression support vector machine. By setting the derivatives of the Lagrangian with respect to w, b, ξn, and �ξn to zero and then back substituting to eliminate the corresponding variables, show that the dual Lagrangian is given by (7.61).\\n\\n7.8 (⋆) www For the regression support vector machine considered in Section 7.1.4, show that all training data points for which ξn > 0 will have an = C, and similarly all points for which �ξn > 0 will have �an = C. 7.9 (⋆) Verify the results (7.82) and (7.83) for the mean and covariance of the posterior distribution over weights in the regression RVM. 7.10 (⋆ ⋆) www Derive the result (7.85) for the marginal likelihood function in the regression RVM, by performing the Gaussian integral over w in (7.84) using the technique of completing the square in the exponential',\n",
       " '83d5635a-82f0-4d2b-bfb9-48b0f1298e78': '(19.61)  Because of the presence of the terms multiplying h; and hg together, we can see that the true posterior does not factorize over h, and hy. Applying equation 19.56, we find that  G(hy | v) (19.62) =exp Ey, cg(ho|v) log B(v, h) (19.63) 1 =exp 5 Ey ~q(ho|v) hi + fp +y2+ hiwi + h3ws (19.64) (s« owolt Ihiwihowy) . (19.65) —2v — 2vh  https://www.deeplearningbook.org/contents/inference.html    From t ¥i8, we can see that. there arg Bictectivelyy nly two values we need to obtain from 4\\\\ h2 | v): Eho~g(h|v)  and Eho~q(h|v) s',\n",
       " '400d5878-35b8-4081-ab84-10c673f62b3a': 'An immediate practical advantage of tile coding is that, because it works with partitions, the overall number of features that are active at one time is the same for any state. Exactly one feature is present in each tiling, so the total number of features present is always the same as the number of tilings. This allows the step-size parameter, ↵, to be set in an easy, intuitive way. For example, choosing ↵ = 1 of tilings, results in exact one-trial learning. If the example s 7!\\n\\nv is trained on, then whatever the prior estimate, ˆv(s,wt), the new estimate will be ˆv(s,wt+1) = v. Usually one wishes to change more slowly than this, to allow for generalization and stochastic variation in target outputs. For example, one might choose ↵ = 1 estimate for the trained state would move one-tenth of the way to the target in one update, and neighboring states will be moved less, proportional to the number of tiles they have in common. Tile coding also gains computational advantages from its use of binary feature vectors',\n",
       " '254b503c-9706-48d5-b8b3-756325a5dbfe': 'We want to establish bench- marks for different ensembles of test-time augmentations and investigate the solution algorithms used.\\n\\nCurrently, majority voting seems to be the dominant solution algo- rithm for test-time augmentation. It seems highly likely that test-time augmentation can be further improved if the weight of each augmented images prediction is further parameterized and learned. Additionally, we will explore the effectiveness of test-time augmentation on object detection, comparing color space augmentations and the Neural Style Transfer algorithm. Meta-learning GAN architectures is another exciting area of interest. Using Reinforce- ment Learning algorithms such as NAS on the generator and discriminator architectures seem very promising. Another interesting area of further research is to use an evolu- tionary approach to speed up the training of GANs through parallelization and cluster computing. Another important area of future work for practical integration of Data Augmentation into Deep Learning workflows is the development of software tools. Similar to how the Tensorflow  system automates the back-end processes of gradient-descent learn- ing, Data Augmentation libraries will automate preprocessing functions',\n",
       " '050f0e69-bd61-4f45-b04d-ed2bf4254c13': 'Klopf recognized that essential aspects of adaptive behavior were being lost as learning researchers came to focus almost exclusively on supervised learning. What was missing, according to Klopf, were the hedonic aspects of behavior, the drive to achieve some result from the environment, to control the environment toward desired ends and away from undesired ends (see Section 15.9). This is the essential idea of trial-and-error learning. Klopf’s ideas were especially inﬂuential on the authors because our assessment of them  led to our appreciation of the distinction between supervised and reinforcement learning, and to our eventual focus on reinforcement learning. Much of the early work that we and colleagues accomplished was directed toward showing that reinforcement learning and supervised learning were indeed di↵erent .\\n\\nOther studies showed how reinforcement learning could address important problems in artiﬁcial neural network learning, in particular, how it could produce learning algorithms for multilayer networks (Barto, Anderson, and Sutton, 1982; Barto and Anderson, 1985; Barto, 1985, 1986; Barto and Jordan, 1987; see Section 15.10). We turn now to the third thread to the history of reinforcement learning, that concerning temporal-di↵erence learning',\n",
       " '57cfa45d-c6cf-470a-9e4d-92e60bc3fbd3': '8.1 The overall view of planning and learning presented here has developed gradually over a number of years, in part by the authors ; it has been strongly inﬂuenced by Agre and Chapman , Bertsekas and Tsitsiklis , Singh , and others. The authors were also strongly inﬂuenced by psychological studies of latent learning  and by psychological views of the nature of thought . In Part III of the book, Section 14.6 relates model-based and model-free methods to psychological theories of learning and behavior, and Section 15.11 discusses ideas about how the brain might implement these types of methods.\\n\\n8.2 The terms direct and indirect, which we use to describe di↵erent kinds of reinforcement learning, are from the adaptive control literature , where they are used to make the same kind of distinction. The term system identiﬁcation is used in adaptive control for what we call modellearning . The Dyna architecture is due to Sutton , and the results in this and the next section are based on results reported there. Barto and Singh  consider some of the issues in comparing direct and indirect reinforcement learning methods',\n",
       " 'ecddb9ef-5ad1-4793-ad3a-6144d6a736de': 'In our discussion of inference in graphical models, we have assumed that the structure of the graph is known and ﬁxed. However, there is also interest in going beyond the inference problem and learning the graph structure itself from data . This requires that we deﬁne a space of possible structures as well as a measure that can be used to score each structure. From a Bayesian viewpoint, we would ideally like to compute a posterior distribution over graph structures and to make predictions by averaging with respect to this distribution. If we have a prior p(m) over graphs indexed by m, then the posterior distribution is given by where D is the observed data set. The model evidence p(D|m) then provides the score for each model. However, evaluation of the evidence involves marginalization over the latent variables and presents a challenging computational problem for many models. Exploring the space of structures can also be problematic. Because the number of different graph structures grows exponentially with the number of nodes, it is often necessary to resort to heuristics to ﬁnd good candidates',\n",
       " '59ff2495-a6a7-48b8-93fd-2d397c119097': '⇤Exercise 5.14 Modify the algorithm for o↵-policy Monte Carlo control (page 111) to use the idea of the truncated weighted-average estimator (5.10). Note that you will ﬁrst need to convert this equation to action values. ⇤ The Monte Carlo methods presented in this chapter learn value functions and optimal policies from experience in the form of sample episodes. This gives them at least three kinds of advantages over DP methods. First, they can be used to learn optimal behavior directly from interaction with the environment, with no model of the environment’s dynamics. Second, they can be used with simulation or sample models. For surprisingly many applications it is easy to simulate sample episodes even though it is diﬃcult to construct the kind of explicit model of transition probabilities required by DP methods. Third, it is easy and eﬃcient to focus Monte Carlo methods on a small subset of the states.\\n\\nA region of special interest can be accurately evaluated without going to the expense of accurately evaluating the rest of the state set (we explore this further in Chapter 8). A fourth advantage of Monte Carlo methods, which we discuss later in the book, is that they may be less harmed by violations of the Markov property',\n",
       " '4b424c0e-a5f9-40bb-b9ec-ab4496390129': 'It is natural to ask which of these methods is best. Although this is a diﬃcult question to answer in general, we can certainly run them all on the 10-armed testbed that we have used throughout this chapter and compare their performances. A complication is that they all have a parameter; to get a meaningful comparison we have to consider their performance as a function of their parameter. Our graphs so far have shown the course of learning over time for each algorithm and parameter setting, to produce a learning curve for that algorithm and parameter setting. If we plotted learning curves for all algorithms and all parameter settings, then the graph would be too complex and crowded to make clear comparisons. Instead we summarize a complete learning curve by its average value over the 1000 steps; this value is proportional to the area under the learning curve. Figure 2.6 shows this measure for the various bandit algorithms from this chapter, each as a function of its own parameter shown on a single scale on the x-axis. This kind of graph is called a parameter study. Note that the parameter values are varied by factors of two and presented on a log scale',\n",
       " '2ecc803f-ffa0-4a22-ab67-6c9c13ba87a1': 'Residual algorithms: Reinforcement learning with function approximation. In Proceedings of the 12th International Conference on Machine Learning , pp. 30–37. Morgan Kaufmann. Baird, L. C. Reinforcement Learning through Gradient Descent. Ph.D. thesis, Carnegie Baird, L. C., Klopf, A. H. Reinforcement learning with high-dimensional, continuous actions. Wright Laboratory, Wright-Patterson Air Force Base, Tech. Rep. WL-TR-93-1147. Baldassarre, G., Mirolli, M. (Eds.) . Intrinsically Motivated Learning in Natural and Artiﬁcial Systems. Springer-Verlag, Berlin Heidelberg. Balke, A., Pearl, J. Counterfactual probabilities: Computational methods, bounds and applications. In Proceedings of the Tenth International Conference on Uncertainty in Artiﬁcial Intelligence (UAI-1994, pp. 46–54. Morgan Kaufmann',\n",
       " '13b7f95b-680c-4a2e-ae51-52ae512945c1': 'But like the RBM, within each layer, each of the variables are mutually independent, conditioned on the variables in  https://www.deeplearningbook.org/contents/generative_models.html    he neighboring layers.\\n\\nSee figure 20.2 for the graph structure. Deep Boltzmann machines have been applied to a variety of tasks, including document modeling Srivastava et al., 3013). Like RBMs and DBNs, DBMs typically contain only binary units—as we assume for simplicity of our presentation of the model—but it is straightforward (o include real-valued visible units. A DBM is an energy-based model, meaning that the joint probability distribu- ion over the model variables is parametrized by an energy function EF. In the case of a deep Boltzmann machine with one visible layer, v, and three hidden layers, AQ), A), and A), the joint probability is given by  P (vA, n,n) _ Fo  To simplify our presentation, we omit the bias parameters below. The DBM energy function is then defined as follows:  exp (—B(v,h\\\\,h®,W;6))',\n",
       " '0c8578a4-6c04-4b12-8873-b5dffae0cf94': 'We would also like to thank CIFAR, and Canada Research Chairs for funding, and Compute Canada, and Calcul Qu´ebec for providing computational resources. Ian Goodfellow is supported by the 2013 Google Fellowship in Deep Learning. Finally, we would like to thank Les Trois Brasseurs for stimulating our creativity. Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I. J., Bergeron, A., Bouchard, N., and Bengio, Y. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop. Bengio, Y. Learning deep architectures for AI. Now Publishers. Bengio, Y., Mesnil, G., Dauphin, Y., and Rifai, S. Better mixing via deep representations. In ICML’13. Bengio, Y., Thibodeau-Laufer, E., and Yosinski, J. Deep generative stochastic networks trainable by backprop. In ICML’14',\n",
       " '59dca559-9201-469b-abb7-9f3d4ca72b3b': 'Then by construction this distribution will be periodic, although it will not be normalized.\\n\\nWe can determine the form of this distribution by transforming from Cartesian coordinates (x1, x2) to polar coordinates (r, θ) so that We also map the mean µ into polar coordinates by writing Next we substitute these transformations into the two-dimensional Gaussian distribution (2.173), and then condition on the unit circle r = 1, noting that we are interested only in the dependence on θ. Focussing on the exponent in the Gaussian distribution we have (r cos θ − r0 cos θ0)2 + (r sin θ − r0 sin θ0)2� 0 − 2r0 cos θ cos θ0 − 2r0 sin θ sin θ0 on the left and as the corresponding polar plot on the right. where ‘const’ denotes terms independent of θ, and we have made use of the following trigonometrical identities Exercise 2.51 If we now deﬁne m = r0/σ2, we obtain our ﬁnal expression for the distribution of p(θ) along the unit circle r = 1 in the form which is called the von Mises distribution, or the circular normal',\n",
       " '9613ac93-2cbd-4ea3-92a8-4f4bdb5fd909': 'DEEP GENERATIVE MODELS  a tensor of zeros, then copies each value from spatial coordinate i of the input to spatial coordinate i x k of the output. The integer value k defines the size of the pooling region. Even though the assumptions motivating the definition of the un-pooling operator are unrealistic, the subsequent layers are able to learn to compensate for its unusual output, so the samples generated by the model as a whole are visually pleasing. 20.10.7 Auto-Regressive Networks  Auto-regressive networks are directed probabilistic models with no latent random variables. The conditional probability distributions in these models are represented by neural networks (sometimes extremely simple neural networks, such as logistic regression). The graph structure of these models is the complete graph. They decompose a joint probability over the observed variables using the chain rule of probability to obtain a product of conditionals of the form P(aq | a—1,.--,%1).\\n\\nSuch models have been called fully-visible Bayes networks (FVBNs) and used successfully in many forms, first with logistic regression for each conditional distribution , and then with neural networks with hidden units',\n",
       " '52de38b7-f048-4633-9dc3-22085a5899df': 'Then show that minimizing J with respect to Zn, where W is kept fixed, gives rise to the PCA Estep (12.58), and that minimizing J with respect to W, where {zn} is kept fixed, gives rise to the PCA M step (12.59). 12.20 (**) By considering second derivatives, show that the only stationary point of the log likelihood function for the factor analysis model discussed in Section 12.2.4 with respect to the parameter J1 is given by the sample mean defined by (12.1). Furthermore, show that this stationary point is a maximum. 12.21 (**) Derive the formulae (12.66) and (12.67) for the E step of the EM algorithm for factor analysis. Note that from the result of Exercise 12.20, the parameter J1 can be replaced by the sample mean x.\\n\\n12.22 (* *) Write down an expression for the expected complete-data log likelihood function for the factor analysis model, and hence derive the corresponding M step equations (12.69) and (12.70)',\n",
       " '95558677-8820-4bfb-9c13-67aa0b2d8677': 'On the other hand, inactivating the dorsomedial striatum (DMS) impairs goal-directed processes, requiring the animal to rely more on habit learning. Results like these support the view that the DLS in rodents is more involved in model-free processes, whereas their DMS is more involved in model-based processes. Results of studies with human subjects in similar experiments using functional neuroimaging, and with non-human primates, support the view that the analogous structures in the primate brain are di↵erentially involved in habitual and goal-directed modes of behavior.\\n\\nOther studies identify activity associated with model-based processes in the prefrontal cortex of the human brain, the front-most part of the frontal cortex implicated in executive function, including planning and decision making. Speciﬁcally implicated is the orbitofrontal cortex (OFC), the part of the prefrontal cortex immediately above the eyes. Functional neuroimaging in humans, and also recordings of the activities of single neurons in monkeys, reveals strong activity in the OFC related to the subjective reward value of biologically signiﬁcant stimuli, as well as activity related to the reward expected as a consequence of actions. Although not free of controversy, these results suggest signiﬁcant involvement of the OFC in goal-directed choice',\n",
       " '4a2d1836-5a31-4f45-957c-01c282b42081': 'Sample a corrupted version @ from C(x | x = x). 3. Use (a, %) as a training example for estimating the autoencoder reconstruction distribution preconstruct(& | &) = Paecoder(& | A) with h the output of encoder f(€) and paecoder typically defined by a decoder g(h). Typically we can simply perform gradient-based approximate minimization (such as minibatch gradient descent) on the negative log-likelihood — log p decoder(& | h). As long as the encoder is deterministic, the denoising autoencoder is a feedforward network and may be trained with exactly the same techniques as any other feedforward network. We can therefore view the DAE as performing stochastic gradient descent on the following expectation:  _ Exn Bata (x) xe C(%| ar) 108 Pdecoder (x | h= f(&)), (14.14) where Pgata(X) is the training distribution. 508  CHAPTER 14. AUTOENCODERS  14.5.1 Estimating the Score  Score matching  is an alternative to maximum likelihood',\n",
       " '3970bb5b-8ac6-42cb-9020-b5fe6b67bb51': \"Similar to other metric-based models, the classifier output is defined as a sum of labels of support samples weighted by attention kernel a(x, x;) - which should be proportional to the similarity between x and x;. cs(x) = P(y|x, S) = So a(x, x;)yi, where S = {(xi5 yi) a  i=1  The attention kernel depends on two embedding functions, f and g, for encoding the test sample and the support set samples respectively. The attention weight between two data points is the  https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Log  cosine similarity, cosine(. ), between their embedding vectors, normalized by softmax:  exp(cosine( f(x), 9(x:)) Doj-1 exp(cosine( f(x), 9(x,;))  a(x, x;) =  Simple Embedding  In the simple version, an embedding function is a neural network with a single data sample as input\",\n",
       " 'f6be9335-db4f-4eab-8b35-e24c09307177': 'Such feature vectors enable approximations as arbitrary quadratic functions of the state numbers—even though the approximation is still linear in the weights that have to be learned.\\n\\nGeneralizing this example from two to k numbers, we can represent highly-complex interactions among a problem’s state dimensions: Suppose each state s corresponds to k numbers, s1, s2, ..., sk, with each si 2 R. For this k-dimensional state space, each order-n polynomial-basis feature xi can be written as where each ci,j is an integer in the set {0, 1, . , n} for an integer n ≥ 0. These features make up the order-n polynomial basis for dimension k, which contains (n + 1)k di↵erent features. Higher-order polynomial bases allow for more accurate approximations of more complicated functions. But because the number of features in an order-n polynomial basis grows exponentially with the dimension k of the natural state space (if n>0), it is generally necessary to select a subset of them for function approximation',\n",
       " 'f780e724-48cb-463e-976a-895de2086876': 'On extremely small datasets, such as the alternative splicing dataset, Bayesian methods outperform methods based on unsupervised pretraining . For these reasons, the popularity of unsupervised pretraining has declined. Nevertheless, unsupervised pretraining remains an important milestone in the history of deep learning research  533  CHAPTER 15. REPRESENTATION LEARNING  and continues to influence contemporary approaches. The idea of pretraining has been generalized to supervised pretraining, discussed in section 8.7.4, as a very common approach for transfer learning.\\n\\nSupervised pretraining for transfer learning is popular  for use with convolutional  https://www.deeplearningbook.org/contents/representation.html    networks pretrained on the ImageNet dataset. Practitioners publish the parameters of these trained networks for this purpose, just as pretrained word vectors are published for natural language tasks . 15.2 Transfer Learning and Domain Adaptation  Transfer learning and domain adaptation refer to the situation where what has been learned in one setting (e.g., distribution P;) is exploited to improve generalization in another setting (say, distribution P)). This generalizes the idea presented in the previous section, where we transferred representations between an unsupervised learning task and a supervised learning task',\n",
       " '72a65a16-f816-4482-ba85-c16c96aaae4f': 'Clearly, each example conveys less information than in the supervised case, where the true label y is directly accessible, so more examples are necessary.\\n\\nWorse, if we are not careful, we could end up with a system that continues picking the wrong decisions even as more and more data is collected, because the correct decision initially had a very low probability: until the learner picks that correct decision, it does not learn about the correct decision. This is similar to the situation in reinforcement learning where only the reward for the selected action is observed. In general, reinforcement learning can involve a sequence of many actions and many rewards. The bandits scenario is a special case of reinforcement learning, in which the learner takes only a single action and receives a single reward. The bandit problem is easier in the sense that the learner knows which reward is associated with which action. In the general reinforcement learning scenario, a high reward or a low reward might have been caused by a recent action or by an action in the distant past. The term contextual bandits refers to the case where the action is taken in the context of some input variable that can inform the decision. For example, we at least know the user identity, and we want to pick an item',\n",
       " 'd5f83a24-e1e3-4147-8b1d-c8ccbd22700b': 'The entropy is given by (p — 1) log(1 — p) — plogp. When p is near 0, the distribution is nearly deterministic, because the random variable is nearly always 0. Whenp is near 1, the distribution is nearly deterministic, because the random variable is nearly always 1. Whenp = 0.5, the entropy is maximal, because the distribution is uniform over the two outcomes. A quantity that is closely related to the KL divergence is the cross-entropy H(P,Q) = H(P)+ Dgr(P||Q), which is similar to the KL divergence but lacking the term on the left:  H(P,Q) = —Exxp log Q(2). (3.51) Minimizing the cross-entropy with respect to Q is equivalent to minimizing the KL divergence, because @ does not participate in the omitted term. When computing many of these quantities, it is common to encounter expres- sions of the form 0 log0',\n",
       " '50db6259-4ec7-431d-b9e1-61fbeb4c4833': 'For example, when processing images, it is useful to detect edges in the first layer of a convolutional network. The same edges appear more or less everywhere in the image, so it is practical to share parameters across the entire image. In some cases, we may not wish to share parameters across the entire image. For example, if we are processing images that are cropped to be centered on an individual’s face, we probably want to extract different features at different locations—the part of the network processing the top of the face needs to look for eyebrows, while the part of the network processing the bottom of the face needs to look for a chin. Convolution is not naturally equivariant to some other transformations, such as changes in the scale or rotation of an image. Other mechanisms are necessary for handling these kinds of transformations. Finally, some kinds of data cannot be processed by neural networks defined by matrix multiplication with a fixed-shape matrix. Convolution enables processing of some of these kinds of data. We discuss this further in section 9.7. 9.3 Pooling  A typical layer of a convolutional network consists of three stages (see figure 9.7)',\n",
       " '330a28ac-b725-4e84-b9c8-4fd60a73a8a1': 'From a neuroscientific point of view, it is interesting to think of the softmax as a way to create a form of competition between the units that participate in it: the softmax outputs always sum to | so an increase in the value of one unit necessarily corresponds to a decrease in the value of others.\\n\\nThis is analogous to the lateral inhibition that is believed to exist between nearby neurons in the cortex. At the extreme (when the difference between the maximal qa; and the others is large in magnitude) it becomes a form of winner-take-all (one of the outputs is nearly 1, and the others are nearly 0). The name “softmax” can be somewhat confusing. The function is more closely related to the arg max function than the max function. The term “soft” derives from the fact that the softmax function is continuous and differentiable. The arg max function, with its result represented as a one-hot vector, is not continuous or differentiable. The softmax function thus provides a “softened” version of the arg max. The corresponding soft version of the maximum function is softmax(z) ! z. 183  CHAPTER 6',\n",
       " '91595849-8387-4eca-aa72-bc10bc7583e6': \"None 0.705 Traditional 0.775 GANs 0.720 Neural + no loss 0.765 Neural + content loss 0.770 Neural + style 0.740 Control 0.710  MNIST 0's and 8’s  Augmentation Val. acc. None 0.972 Neural + no loss 0.975 Neural + content loss 0.968  backpropagated to update Network-A. Additionally another loss function is incorpo- rated into Network-A to ensure that its outputs are similar to others within the class. Network-A uses a series of convolutional layers to produce the augmented image. The conceptual framework of Network-A can be expanded to use several Networks trained in parallel. Multiple Network-As could be very useful for learning class-specific aug- mentations via meta-learning (Fig. 30). Smart Augmentation is similar to SamplePairing  or mixed-examples in the sense that a combination of existing examples produces new ones. However, the mechanism of Smart Augmentation is much more sophisticated, using an adaptive CNN to derive new images rather than averaging pixels or hand-engineered image combinations. The Smart Augmentation technique was tested on the task of gender recognition\",\n",
       " '8d568886-ab3d-46fb-86f0-d9de19cff818': 'Ioffe and Szegedy  recommend  316  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  the latter. More specifically, XW + 6 should be replaced by a normalized version of XW. The bias term should be omitted because it becomes redundant with the @ parameter applied by the batch normalization reparametrization. The input to a layer is usually the output of a nonlinear activation function such as the rectified linear function in a previous layer. The statistics of the input are thus more non-Gaussian and less amenable to standardization by linear operations.\\n\\nIn convolutional networks, described in chapter 9, it is important to apply the same normalizing y, and o at every spatial location within a feature map, so that the statistics of the feature map remain the same regardless of spatial location. https://www.deeplearningbook.org/contents/optimization.html    8.7.2 Coordinate Descent  In some cases, it may be possible to solve an optimization problem quickly by breaking it into separate pieces',\n",
       " '96bf8f9a-f107-4680-9390-c0b9edf56891': 'The following general procedure for solving such problems was given by and we shall also, without loss of generality, consider the case where f(θ) > 0 for θ > θ⋆ and f(θ) < 0 for θ < θ⋆, as is the case in Figure 2.10. The Robbins-Monro procedure then deﬁnes a sequence of successive estimates of the root θ⋆ given by where z(θ(N)) is an observed value of z when θ takes the value θ(N). The coefﬁcients {aN} represent a sequence of positive numbers that satisfy the conditions It can then be shown  that the sequence of estimates given by (2.129) does indeed converge to the root with probability one. Note that the ﬁrst condition (2.130) ensures that the successive corrections decrease in magnitude so that the process can converge to a limiting value.\\n\\nThe second condition (2.131) is required to ensure that the algorithm does not converge short of the root, and the third condition (2.132) is needed to ensure that the accumulated noise has ﬁnite variance and hence does not spoil convergence. Now let us consider how a general maximum likelihood problem can be solved sequentially using the Robbins-Monro algorithm',\n",
       " '83c020d6-57d4-4730-962f-5b542fa2d56c': 'However, adapting weighted importance sampling to function approximation is challenging and can probably only be done approximately with O(d) complexity . The Tree Backup algorithm (Section 7.5) shows that it is possible to perform some o↵-policy learning without using importance sampling. This idea has been extended to the o↵-policy case to produce stable and more eﬃcient methods by Munos, Stepleton, Harutyunyan, and Bellemare  and by Mahmood, Yu and Sutton . Another, complementary strategy is to allow the target policy to be determined in part by the behavior policy, in such a way that it never can be so di↵erent from it to create large importance sampling ratios. For example, the target policy can be deﬁned by reference to the behavior policy, as in the “recognizers” proposed by Precup et al. O↵-policy learning is a tempting challenge, testing our ingenuity in designing stable and eﬃcient learning algorithms. Tabular Q-learning makes o↵-policy learning seem easy, and it has natural generalizations to Expected Sarsa and to the Tree Backup algorithm',\n",
       " 'd477723f-a850-4cc9-b555-fc21f1c2118e': 'T., Glanz, F. H. UNH CMAC verison 2.1: The University of New Hampshire Implementation of the Cerebellar Model Arithmetic Computer - CMAC. Robotics Laboratory Technical Report, University of New Hampshire, Durham. Miller, S., Williams, R. J. Learning to control a bioreactor using a neural net Dyna-Q system. In Proceedings of the Seventh Yale Workshop on Adaptive and Learning Systems, pp. 167–172. Center for Systems Science, Dunham Laboratory, Yale University, New Haven.\\n\\nMiller, W. T., Scalera, S. M., Kim, A. Neural network control of dynamic balance for a biped walking robot. In Proceedings of the Eighth Yale Workshop on Adaptive and Learning Systems, pp. 156–161. Center for Systems Science, Dunham Laboratory, Yale University, New Haven. Minton, S. Quantitative results concerning the utility of explanation-based learning. Minsky, M. L',\n",
       " '4b297e9e-b1e4-48f8-9dc2-59894aa7d1c5': \"In this book, a generative model either represents p(a) or can draw samples from it. Many variants of ICA only know how to transform between x and h but do not have any way of representing p(h), and thus do not impose a distribution over p(a). For example, many ICA variants aim to increase the sample kurtosis of h = W'z, because high kurtosis indicates that p(h) is non-Gaussian, but this is accomplished without explicitly representing p(h). This is because ICA is more often used as an analysis tool for separating signals, rather than for generating data or estimating its density. Just as PCA can be generalized to the nonlinear autoencoders described in chapter 14, ICA can be generalized to a nonlinear generative model, in which we use a nonlinear function f to generate the observed data. See Hyvarinen and Pajunen  for the initial work on nonlinear ICA and its successful use with ensemble learning by Roberts and Everson  and Lappalainen ef al\",\n",
       " '75e7c740-1844-47c6-9605-54a55d3e4d2d': 'The approach for setting the biases must be coordinated with the approach for setting the weights. Setting the biases to zero is compatible with most weight initialization schemes. There are a few situations where we may set some biases to nonzero values:  https://www.deeplearningbook.org/contents/optimization.html  e Ifa bias is for an output unit, then it is often beneficial to initialize the bias to obtain the right marginal statistics of the output. To do this, we assume that the initial weights are small enough that the output of the unit is determined  only by the bias. This justifies setting the bias to the inverse of the activation function applied to the marginal statistics of the output in the training set. For example, if the output is a distribution over classes, and this distribution is a highly skewed distribution with the marginal probability of class 7 given by element c; of some vector c, then we can set the bias vector b by solving the equation softmax(b) = c',\n",
       " '7a46707b-56d9-442b-9017-79467b855826': 'So far, we have considered various approximation schemes for evaluating the Hessian matrix or its inverse.\\n\\nThe Hessian can also be evaluated exactly, for a network of arbitrary feed-forward topology, using extension of the technique of backpropagation used to evaluate ﬁrst derivatives, which shares many of its desirable features including computational efﬁciency . It can be applied to any differentiable error function that can be expressed as a function of the network outputs and to networks having arbitrary differentiable activation functions. The number of computational steps needed to evaluate the Hessian scales like O(W 2). Similar algorithms have also been considered by Buntine and Weigend . Here we consider the speciﬁc case of a network having two layers of weights, for which the required equations are easily derived. We shall use indices i and i′ Exercise 5.22 to denote inputs, indices j and j′ to denoted hidden units, and indices k and k′ to denote outputs. We ﬁrst deﬁne where En is the contribution to the error from data point n. The Hessian matrix for this network can then be considered in three separate blocks as follows',\n",
       " 'da6ab905-bb6b-404c-b335-21f8be836737': 'One way to reduce the cost of convolutional network training is to use features that are not trained in a supervised fashion. There are three basic strategies for obtaining convolution kernels without supervised training. One is to simply initialize them randomly. Another is to design them by hand, for example, by setting each kernel to detect edges at a certain orientation or scale. Finally, one can learn the kernels with an unsupervised criterion. For example, Coates et al. apply k-means clustering to small image patches, then use each learned centroid as a convolution kernel.\\n\\nIn Part III we describe many more unsupervised learning approaches. Learning the features with an unsupervised criterion allows them to be determined separately from the classifier layer at the top of the architecture. One can then extract the features for the entire training set just once, essentially constructing a new training set for the last layer. Learning the last layer is then typically a convex optimization problem, assuming the last layer is something like logistic regression or an SVM. Random filters often work surprisingly well in convolutional networks . Saxe et al',\n",
       " 'f66d0ba6-754f-4c3f-bd39-793784a39b3b': 'If the transition occurs again, then it will be from a state of estimated value ⇡11 to a state of estimated value ⇡22, for a TD error of ⇡11—larger, not smaller, than before.\\n\\nIt will look even more like the ﬁrst state is undervalued, and its value will be increased again, this time to ⇡12.1. This looks bad, and in fact with further updates w will diverge to inﬁnity. To see this deﬁnitively we have to look more carefully at the sequence of updates. The δt = Rt+1 + γˆv(St+1,wt) − ˆv(St,wt) = 0 + γ2wt − wt = (2γ − 1)wt, and the o↵-policy semi-gradient TD(0) update (from (11.2)) is wt+1 = wt + ↵⇢tδtrˆv(St,wt) = wt + ↵ · 1 · (2γ − 1)wt · 1 = Note that the importance sampling ratio, ⇢t, is 1 on this transition because there is only one action available from the ﬁrst state, so its probabilities of being taken under the target and behavior policies must both be 1',\n",
       " '0674fc6a-a85d-4e38-bbde-25b465c5755d': 'In practice, a numerical  235  https://www.deeplearningbook.org/contents/regularization.html    CHAPTER 7 REGULARIZATION FOR DEEP LEARNING  implementation of gradient descent will eventually reach sufficiently large weights to cause numerical overflow, at which point its behavior will depend on how the programmer has decided to handle values that are not real numbers. Most forms of regularization are able to guarantee the convergence of iterative methods applied to underdetermined problems. For example, weight decay will cause gradient descent to quit increasing the magnitude of the weights when the slope of the likelihood is equal to the weight decay coefficient. The idea of using regularization to solve underdetermined problems extends beyond machine learning. The same idea is useful for several basic linear algebra problems. As we saw in section 2.9, we can solve underdetermined linear equations using the Moore-Penrose pseudoinverse. Recall that one definition of the pseudoinverse X* of a matrix X is  Xt= lim (XX tol) tx, (7.29)  We can now recognize equation 7.29 as performing linear regression with weight decay. Specifically, equation 7.29 is the limit of equation 7.17 as the regularization coefficient shrinks to zero',\n",
       " 'e48f7302-5ced-44e3-b087-9e7d13209634': 'Again, we can identify two approaches. In the ﬁrst, we simply set the required derivatives of the marginal likelihood to zero and obtain the following re-estimation equations Exercise 7.12 where mi is the ith component of the posterior mean m deﬁned by (7.82). The quantity γi measures how well the corresponding parameter wi is determined by the data and is deﬁned by Section 3.5.3 in which Σii is the ith diagonal component of the posterior covariance Σ given by (7.83).\\n\\nLearning therefore proceeds by choosing initial values for α and β, evaluating the mean and covariance of the posterior using (7.82) and (7.83), respectively, and then alternately re-estimating the hyperparameters, using (7.87) and (7.88), and re-estimating the posterior mean and covariance, using (7.82) and (7.83), until a suitable convergence criterion is satisﬁed. The second approach is to use the EM algorithm, and is discussed in Section 9.3.4. These two approaches to ﬁnding the values of the hyperparameters that maximize the evidence are formally equivalent. Numerically, however, it is found Exercise 9.23 that the direct optimization approach corresponding to (7.87) and (7.88) gives somewhat faster convergence',\n",
       " 'dc2a8e6d-beba-4ce4-8aca-72346fdde224': 'In some forms of auto- regressive networks, such as NADE , described in section 20.10.10, we can introduce a form of parameter sharing that brings both a statistical advantage (fewer unique parameters) and a computational advantage (less computation). This is one more instance of the recurring deep learning motif of reuse of features. 20.10.8 Linear Auto-Regressive Networks  The simplest form of auto-regressive network has no hidden units and no sharing of parameters or features. Each P(x; | 2j_1,..., 21) is parametrized as a linear model (linear regression for real-valued data, logistic regression for binary data, softmax regression for discrete data). This model was introduced by Frey  and has O(d?) parameters when there are d variables to model. It is illustrated in figure 20.8. If the variables are continuous, a linear auto-regressive model is merely another way to formulate a multivariate Gaussian distribution, capturing linear pairwise interactions between the observed variables. https://www.deeplearningbook.org/contents/generative_models.html    Linear auto-regressive networks are essentially the generalization of linear classification methods to generative modeling',\n",
       " '428720bf-5b67-4396-b9e8-87cf73f7156d': 'We ﬁnd that BERTLARGE signiﬁcantly outperforms BERTBASE across all tasks, especially those with very little training data. The effect of model size is explored more thoroughly in Section 5.2. 9The GLUE data set distribution does not include the Test labels, and we only made a single GLUE evaluation server submission for each of BERTBASE and BERTLARGE. 10https://gluebenchmark.com/leaderboard Wikipedia containing the answer, the task is to predict the answer text span in the passage. As shown in Figure 1, in the question answering task, we represent the input question and passage as a single packed sequence, with the question using the A embedding and the passage using the B embedding. We only introduce a start vector S ∈ RH and an end vector E ∈ RH during ﬁne-tuning.\\n\\nThe probability of word i being the start of the answer span is computed as a dot product between Ti and S followed by a softmax over all of the words in the paragraph: Pi = eS·Ti � j eS·Tj . The analogous formula is used for the end of the answer span',\n",
       " '6b6ac2e5-05b7-4bfa-b213-979290ba8f34': 'We saw that in going from a directed to an undirected representation we had to discard some conditional independence properties from the graph. Of course, we could always trivially convert any distribution over a directed graph into one over an undirected graph by simply using a fully connected undirected graph. This would, however, discard all conditional independence properties and so would be vacuous. The process of moralization adds the fewest extra links and so retains the maximum number of independence properties.\\n\\nWe have seen that the procedure for determining the conditional independence properties is different between directed and undirected graphs. It turns out that the two types of graph can express different conditional independence properties, and it is worth exploring this issue in more detail. To do so, we return to the view of a speciﬁc (directed or undirected) graph as a ﬁlter, so that the set of all possible Section 8.2 distributions over the given variables could be reduced to a subset that respects the conditional independencies implied by the graph. A graph is said to be a D map (for ‘dependency map’) of a distribution if every conditional independence statement satisﬁed by the distribution is reﬂected in the graph',\n",
       " 'eb473eb2-24e5-4720-810a-884f1e642e9b': 'Its input at time t is a feature vector x(St); its output, At, is a random variable having two values, 0 and 1, with Pr{At = 1} = Pt and Pr{At = 0} = 1 − Pt (the Bernoulli distribution). Let h(s, 0, ✓) and h(s, 1, ✓) be the preferences in state s for the unit’s two actions given policy parameter ✓',\n",
       " '5a15a49f-04e0-4e5a-b5f2-d23f1a305365': 'Using this synthetically generated data, a convolutional network is able to learn to map z descriptions of the content of an image to # approximations of rendered images. This suggests that contemporary differentiable generator networks have sufficient model capacity to be good generative models, and that contemporary optimization algorithms have the ability to fit them.\\n\\nThe difficulty lies in determining how to train generator networks when the value of z for each x is not fixed and known ahead of each time. 692  CHAPTER 20. DEEP GENERATIVE MODELS  The following sections describe several approaches to training differentiable generator nets given only training samples of a. 20.10.3. Variational Autoencoders  The variational autoencoder, or VAE , is a directed model that uses learned approximate inference and can be trained purely with gradient-based methods. To generate a sample from the model, the VAE first draws a sample z from the code distribution pmodei (2). The sample is then run through a differentiable generator network g(z). Finally, w is sampled from a distribution Pmodei(x; g(z)) = Pmodel(x | z)',\n",
       " '497987a1-9669-485d-8292-c61e3e998be4': 'Given an observed data set {xn}, where n = 1, . , N, derive the E and M step equations of the EM algorithm for optimizing the mixing coefﬁcients πk and the component parameters µkij of this distribution by maximum likelihood. 9.20 (⋆) www Show that maximization of the expected complete-data log likelihood function (9.62) for the Bayesian linear regression model leads to the M step reestimation result (9.63) for α. 9.21 (⋆ ⋆) Using the evidence framework of Section 3.5, derive the M-step re-estimation equations for the parameter β in the Bayesian linear regression model, analogous to the result (9.63) for α. 9.22 (⋆ ⋆) By maximization of the expected complete-data log likelihood deﬁned by (9.66), derive the M step equations (9.67) and (9.68) for re-estimating the hyperparameters of the relevance vector machine for regression',\n",
       " 'a7f93cf1-2c2e-48e6-9dfe-cf64197ea5a9': 'This is unrealistic as a model of the monkey’s situation because the monkey would likely learn these predictions at the same time that it is learning to act correctly (as would a reinforcement learning algorithm that learns policies as well as value functions, such as an actor–critic algorithm), but this scenario is simpler to describe than one in which a policy and a value function are learned simultaneously.\\n\\nNow imagine that the agent’s experience divides into multiple trials, in each of which the same sequence of states repeats, with a distinct state occurring on each time step during the trial. Further imagine that the return being predicted is limited to the return over a trial, which makes a trial analogous to a reinforcement learning episode as we have deﬁned it. In reality, of course, the returns being predicted are not conﬁned to single trials, and the time interval between trials is an important factor in determining what an animal learns. This is true for TD learning as well, but here we assume that returns do not accumulate over multiple trials. Given this, then, a trial in experiments like those conducted by Schultz and colleagues is equivalent to an episode of reinforcement learning',\n",
       " '38ca7a30-5160-4403-b9c6-bf1b465d683e': 'This is much like the classical problem of tomographic reconstruction, used in medical imaging for example, in which a two-dimensional distribution is to be reconstructed from an number of one-dimensional averages.\\n\\nHere there are far fewer line measurements than in a typical tomography application. On the other hand the range of geometrical conﬁgurations is much more limited, and so the conﬁguration, as well as the phase fractions, can be predicted with reasonable accuracy from the densitometer data. For safety reasons, the intensity of the gamma beams is kept relatively weak and so to obtain an accurate measurement of the attenuation, the measured beam intensity is integrated over a speciﬁc time interval. For a ﬁnite integration time, there are random ﬂuctuations in the measured intensity due to the fact that the gamma beams comprise discrete packets of energy called photons. In practice, the integration time is chosen as a compromise between reducing the noise level (which requires a long integration time) and detecting temporal variations in the ﬂow (which requires a short integration time)',\n",
       " 'd9684eda-fb8e-48b8-99a2-a1be50735812': 'More generally, it is straightforward to obtain the required expectations for any member of the exponential family, provided it can be normalized, because the expected statistics can be related to the derivatives of the normalization coefﬁcient, as given by (2.226). The EP approximation is illustrated in Figure 10.14. From (10.193), we see that the revised factor �fj(θ) can be found by taking qnew(θ) and dividing out the remaining factors so that where we have used (10.195). The coefﬁcient K is determined by multiplying both sides of (10.199) by q\\\\i(θ) and integrating to give Combining this with (10.197), we then see that K = Zj and so can be found by evaluating the integral in (10.197).\\n\\nIn practice, several passes are made through the set of factors, revising each factor in turn. The posterior distribution p(θ|D) is then approximated using (10.191), and the model evidence p(D) can be approximated by using (10.190) with the factors fi(θ) replaced by their approximations �fi(θ)',\n",
       " '7db1a833-846f-4565-9b2e-3b2bc082d77b': 'Bridge sampling estimates the ratio Z,/Z as the ratio of the expected impor- tance weights between po and pe and between a and px:  Px a} P(x ?) ay? is S . (18.62) (k) ta Bole ; kai Pi(@j) If the bridge distribution p, is chosen carefully to have a large overlap of support with both pp and p,, then bridge sampling can allow the distance between two distributions (or more formally, Dx, (po||p1)) to be much larger than with standard  importance sampling. It can be shown that the optimal bridging distribution is given by ph?) (x) ine ay).\\n\\nwhere r = Z/Z. At first, this appears to be an unworkable solution as it would seem to require the very quantity we are trying to estimate, Z,/Z. However, it is possible to start with a coarse estimate of r and use the resulting bridge distribution to refine our estimate iteratively . That is, we  iteratively reestimate the ratio and use each iteration to update the value of r',\n",
       " '535d4e64-eb40-41fe-8b32-7faa111703a4': 'Now, we can see Ez∼p(z) − Ez∼p(z) − ⟨(θ − θ0), Ez∼p(z)⟩ �f(gθ(z)) − f(gθ0(z)) − ⟨(θ − θ0), ∇θf(gθ(z))|θ0⟩ By diﬀerentiability, the term inside the integral converges p(z)-a.e. to 0 as θ → θ0. Furthermore, ∥f(gθ(z)) − f(gθ0(z)) − ⟨(θ − θ0), ∇θf(gθ(z))|θ0⟩ and since Ez∼p(z) < +∞ by assumption 1, we get by dominated convergence that Equation 6 converges to 0 as θ → θ0 so for almost every θ, and in particular when the right hand side is well deﬁned. Note that the mere existance of the left hand side (meaning the diﬀerentiability a.e. of Ez∼p(z)) had to be proven, which we just did',\n",
       " '70b28632-18b1-43e4-a4cd-6179b5231e1f': 'found that using Exercise 5.4 the cross-entropy error function instead of the sum-of-squares for a classiﬁcation problem leads to faster training as well as improved generalization. If we have K separate binary classiﬁcations to perform, then we can use a network having K outputs each of which has a logistic sigmoid activation function. Associated with each output is a binary class label tk ∈ {0, 1}, where k = 1, . .\\n\\n, K. If we assume that the class labels are independent, given the input vector, then the conditional distribution of the targets is Taking the negative logarithm of the corresponding likelihood function then gives the following error function Exercise 5.5 where ynk denotes yk(xn, w). Again, the derivative of the error function with respect to the activation for a particular output unit takes the form (5.18) just as in the Exercise 5.6 regression case. It is interesting to contrast the neural network solution to this problem with the corresponding approach based on a linear classiﬁcation model of the kind discussed in Chapter 4. Suppose that we are using a standard two-layer network of the kind shown in Figure 5.1',\n",
       " '654b1c43-4ec1-42eb-9bf1-17f44a3e0d2a': 'With slight modification, this approach can also work using an extra output value in the neural language model’s softmax layer, rather than a separate sigmoid unit.\\n\\nAn obvious disadvantage of the short list approach is that the potential gener- alization advantage of the neural language models is limited to the most frequent words, where, arguably, it is the least useful. This disadvantage has stimulated the exploration of alternative methods to deal with high-dimensional outputs, described below. 461  CHAPTER 12. APPLICATIONS  https://www.deeplearningbook.org/contents/applications.html    12.4.3.2 Hierarchical Softmax  A classical approach  to reducing the computational burden of high-dimensional output layers over large vocabulary sets V is to decompose probabilities hierarchically. Instead of necessitating a number of computations proportional to |Y| (and also proportional to the number of hidden units, n;), the |V| factor can be reduced to as low as log |V|. Bengio  and Morin and Bengio  introduced this factorized approach to the context of neural language models. One can think of this hierarchy as building categories of words, then categories of categories of words, then categories of categories of categories of words, and so on',\n",
       " '7d70ede0-f81b-4ddd-9e6c-702ab8cc9f8f': 'If this random walk is tuned to preserve norms, then feedforward networks can mostly avoid the vanishing and exploding gradients problem that arises when the same weight matrix is used at each step, as described in section 8.2.5. Unfortunately, these optimal criteria for initial weights often do not lead to optimal performance. This may be for three different reasons. First, we may be using the wrong criteria—it may not actually be beneficial to preserve the norm of a signal throughout the entire network. Second, the properties imposed at initialization may not persist after learning has begun to proceed. Third, the criteria might succeed at improving the speed of optimization but inadvertently increase generalization error.\\n\\nIn practice, we usually need to treat the scale of the weights as a hyperparameter whose optimal value lies somewhere roughly near but not exactly equal to the theoretical predictions. One drawback to scaling rules that set all the initial weights to have the same  standard deviation, such as +, is that every individual weight becomes extremely  Vv  ae fAn4nN +  https://www.deeplearningbook.org/contents/optimization.html    small when the layers become large',\n",
       " '02b3c520-4f61-4ba1-b7ed-8677c689a8d4': '(We can assume the right MRP starts in one of two states at random with equal probability.) Thus, even given even an inﬁnite amount of data, it would not be possible to tell which of these two MRPs was generating it. In particular, we could not tell if the MRP has one state or two, is stochastic or deterministic. These things are not learnable. This pair of MRPs also illustrates that the VE objective (9.1) is not learnable. If γ = 0, then the true values of the three states (in both MRPs), left to right, are 1, 0, and 2. Suppose w = 1. Then the VE is 0 for the left MRP and 1 for the right MRP. Because the VE is di↵erent in the two problems, yet the data generated has the same distribution, the VE cannot be learned. The VE is not a unique function of the data distribution. And if it cannot be learned, then how could the VE possibly be useful as an objective for learning?\\n\\nIf an objective cannot be learned, it does indeed draw its utility into question. In the case of the VE, however, there is a way out',\n",
       " 'ae904337-d5ee-487d-bc53-a88d468bf6ea': 'use reinforcement learning techniques (policy gradient) to learn a form of conditional dropout on blocks of hidden units and get an actual reduction in computational cost without negatively affecting the quality of the approximation. Another kind of dynamic structure is a switch, where a hidden unit can receive input from different units depending on the context. This dynamic routing approach can be interpreted as an attention mechanism . So far, the use of a hard switch has not proven effective on large-scale applications. Contemporary approaches instead use a weighted average over many possible inputs, and thus do not achieve all the possible computational benefits of dynamic structure. Contemporary attention mechanisms are described in section 12.4.5.1. One major obstacle to using dynamically structured systems is the decreased degree of parallelism that results from the system following different code branches for different inputs. This means that few operations in the network can be described as matrix multiplication or batch convolution on a minibatch of examples. We can write more specialized subroutines that convolve each example with different kernels or multiply each row of a design matrix by a different set of columns of weights. Unfortunately, these more specialized subroutines are difficult to implement efficiently',\n",
       " '7196072f-0520-4df4-b60e-a2d64ad7ad68': 'These results demonstrate that the discriminative model effectively learns from the additional signal contained in Snorkel’s probabilistic training labels over simpler modeling strategies. One of the most exciting potential advantages of using a programmatic supervision approach as in Snorkel is the ability to incorporate additional unlabeled data, which is often cheaply available. Recently, proposed theory characterizing the data programming approach used predicts that discriminative model generalization risk (i.e., predictive performance on the held-out test set) should improve with additional unlabeled data, at the same asymptotic rate as in traditional supervised methods with respect to labeled data . That is, with a ﬁxed amount of effort writing labeling functions, we could then get improved discriminative model performance simply by adding more unlabeled data. Wevalidatethistheoreticalpredictionempiricallyonthree of our datasets (Fig. 11). We see that by adding additional unlabeled data—in these datasets, candidates from addiFig.\\n\\n11 The increase in end model performance (measured in F1 score) for different amounts of unlabeled data, measured in the number of candidates',\n",
       " '537b113a-b928-4f01-b7a0-1f27f34dfd9f': 'We begin by considering the binomial and multinomial distributions for discrete random variables and the Gaussian distribution for continuous random variables. These are speciﬁc examples of parametric distributions, so-called because they are governed by a small number of adaptive parameters, such as the mean and variance in the case of a Gaussian for example. To apply such models to the problem of density estimation, we need a procedure for determining suitable values for the parameters, given an observed data set. In a frequentist treatment, we choose speciﬁc values for the parameters by optimizing some criterion, such as the likelihood function. By contrast, in a Bayesian treatment we introduce prior distributions over the parameters and then use Bayes’ theorem to compute the corresponding posterior distribution given the observed data.\\n\\nWe shall see that an important role is played by conjugate priors, that lead to posterior distributions having the same functional form as the prior, and that therefore lead to a greatly simpliﬁed Bayesian analysis. For example, the conjugate prior for the parameters of the multinomial distribution is called the Dirichlet distribution, while the conjugate prior for the mean of a Gaussian is another Gaussian',\n",
       " 'a99b501a-94a7-427d-8a65-d0168ef4742a': '12.9 I\\\\n ~I\"\\'tfat\"\" oIlt>e II\"\"\"fative vi&w oI1t>e p<ot>abi! ;st\", PeA modeIfof\" two-dimensiooal <!ala space and a on&-<lirnent.ionallat/l<1t space, An Ob&erved <!ala point x Is generated by first drawing a value i fof 1t>e Iat&n1 vafiatlle /f(lm ~s prior dist,~t\"\" P(~) and Itlen drawing a val\"\" fof x lrom an iSO/fopK: Gaussian distr~t\"\" (iijust,al&(l by the red cir<:ie\\'s) having mean wi+\" and coY8r1.once ,,\\'1 The l/f&er\\\\ ellips.&$ show l!le density \"\"\"toors!of the marg\\'\\'\\'\\'\\'1 dis1r1bulion PIx)',\n",
       " 'd84e1f0e-61f2-4758-8fd9-4e02607cb632': '1llt.1f \" .If \"\\'\"\"\"* CO\\\\-.ullCC mMfU ,n (~ .If*\\'e ,~l\"\" by , and ,l~ \",,,,n\\'\"MOl\" opan,ion i\\' «lined by ; = 1... ,. M. Our goal is 10 soh\\'\" lhis eigen\"lIlue problem WilhoUl ha\"inlllO work . \"plici,ly in ,he f.lIture \\'pace.\\n\\nFrom !he definilion of C',\n",
       " 'ca13775c-bed9-4ee8-b035-d89bea11c9e6': 'They observed without proof that these results extend to deeper networks without nonlinearities. The output of such networks is a linear function of their input, but they are useful o study as a model of nonlinear neural networks because their loss function is a nonconvex function of their parameters. Such networks are essentially just multiple matrices composed together. Saxe et al. provided exact solutions  showed experimentally that real neural networks also have loss functions that contain very many high-cost saddle points. Choromanska et al. provided additional theoretical arguments, showing that another class of high-dimensional random functions related to neural networks does so as well. What are the implications of the proliferation of saddle points for training algorithms? For first-order optimization, algorithms that use only gradient infor- mation, the situation is unclear.\\n\\nThe gradient can often become very small near a saddle point. On the other hand, gradient descent empirically seems able to escape saddle points in many cases. Goodfellow e¢ al. provided visualizations of several learning trajectories of state-of-the-art neural networks, with an example given in figure 8.2',\n",
       " '6a0494fe-7818-43e6-851f-4837ce1deda1': 'This work may not be translated or copied in whole or in part without the written permission of the publisher (Springer Science+Business Media, LLC, 233 Spring Street, New York, NY 10013, USA), except for brief excerpts in connection with reviews or scholarly analysis. Use in connection with any form of information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed is forbidden.\\n\\nThe use in this publication of trade names, trademarks, service marks, and similar terms, even if they are not identified as such, is not to be taken as an expression of opinion as to whether or not they are subject to proprietary rights. Total eclipse of the sun, Antalya, Turkey, 29 March 2006. Pattern recognition has its origins in engineering, whereas machine learning grew out of computer science. However, these activities can be viewed as two facets of the same ﬁeld, and together they have undergone substantial development over the past ten years. In particular, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic models',\n",
       " '0bbfffcc-528b-4228-ab89-e1e2c91bd038': 'After warming up with the supervised MLE for n iterations, the approach then changes the experience function to the data augmentation-based one fτ=n = fdata-aug deﬁned in Equation 4.8, which introduces noise and task-speciﬁc evaluation information into the training. As revealed in Section 4.1.4, this stage in effect corresponds to the reward-augmented maximum likelihood (RAML) algorithm . The approach proceeds by further annealing the balancing weights, speciﬁcally by gradually increasing βτ from ϵ to 1 as τ increases. The increase of β eﬀectively gets the learning closer to a reinforcement-style learning (notice that the policy gradient algorithm has α = β = 1 as described in Section 4.3.1). Intuitively, the target model pθ(t), as part of the q(τ+1) solution in the teacher step weighted by the increasing weight β (see Equation 3.3), serves to produce more data samples. Those samples are weighted by the experience function and used for updating the target model further, simulating the policy gradients.\\n\\nTherefore, the whole learning procedure spans multiple paradigms of learning (from supervised MLE, RAML, to reinforcement learning) that increasingly introduces more noise and exploration for improved robustness',\n",
       " 'd5283fc1-bb5f-4582-bb7c-982ced7ae888': 'Stochastic learning automata were foreshadowed by earlier work in psychology, beginning with William Estes’  e↵ort toward a statistical theory of learning and further developed by others . The statistical learning theories developed in psychology were adopted by researchers in economics, leading to a thread of research in that ﬁeld devoted to reinforcement learning. This work began in 1973 with the application of Bush and Mosteller’s learning theory to a collection of classical economic models .\\n\\nOne goal of this research was to study artiﬁcial agents that act more like real people than do traditional idealized economic agents . This approach expanded to the study of reinforcement learning in the context of game theory. Reinforcement learning in economics developed largely independently of the early work in reinforcement learning in artiﬁcial intelligence, though game theory remains a topic of interest in both ﬁelds (beyond the scope of this book). Camerer  discusses the reinforcement learning tradition in economics, and Now´e, Vrancx, and De Hauwere  provide an overview of the subject from the point of view of multi-agent extensions to the approach that we introduce in this book',\n",
       " '0db457fc-f9fd-4732-9ea7-226b74c47f10': 'Context- specific independences are independences that are present dependent on the value of some variables in the network.\\n\\nFor example, consider a model of three binary variables: a, b and c. Suppose that when a is 0, b and c are independent, but when a is 1, b is deterministically equal to c. Encoding the behavior when a = 1 requires an edge connecting b and c. The graph then fails to indicate that b and c are independent when a = 0. In general, a graph will never imply that an independence exists when it does  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    not. However, a graph may fail to encode an independence. 16.2.6 Converting between Undirected and Directed Graphs  We often refer to a specific machine learning model as being undirected or directed. For example, we typically refer to RBMs as undirected and sparse coding as directed. This choice of wording can be somewhat misleading, because no probabilistic model is inherently directed or undirected',\n",
       " 'a7a027e0-630c-47aa-b03b-0351f15342ea': 'In general, we can think of a sigmoid belief network as having a vector of binary states s, with each element of the state influenced by its ancestors:  p(si) =o | S > Wyisj +h: | - (20.70)  The most common structure of sigmoid belief network is one that is divided into many layers, with ancestral sampling proceeding through a series of many hidden layers and then ultimately generating the visible layer. This structure is very similar to the deep belief network, except that the units at the beginning of the sampling process are independent from each other, rather than sampled from a restricted Boltzmann machine. Such a structure is interesting for a variety of reasons.\\n\\nOne is that the structure is a universal approximator of probability distributions over the visible units, in the sense that it can approximate any probability distribution over binary variables arbitrarily well, given enough depth, even if the width of the individual layers is restricted to the dimensionality of the visible layer . While generating a sample of the visible units is very efficient in a sigmoid belief network, most other operations are not. Inference over the hidden units given the visible units is intractable',\n",
       " '822b1803-e49f-49a6-b62b-aa6dad67f2bd': 'In this approach the sigmoid i  trained to maximize the log-probability of the correct prediction as to whether the  sequence ends or continues at each time step. Another way to determine the sequence length 7 is to add an extra output to the model that predicts the integer 7 itself. The model can sample a value of 7 and then sample 7 steps worth of data. This approach requires adding an extra input to the recurrent update at each time step so that the recurrent update is aware of whether it is near the end of the generated sequence. This extra input can either consist of the value of + or can consist of + — t, the number of remaining time steps. Without this extra input, the RNN might generate sequences that end abruptly, such as a sentence that ends before it is complete.\\n\\nThis approach is based on the decomposition  P(a™,...,a) = P(r)P(a™,...,2 | 7). (10.34)  384  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  The strategy of predicting 7 directly is used, for example, by Goodfellow e# al',\n",
       " '79421cdc-eeda-451f-adcf-f12e3ffc4f61': 'Another very popular idea is to form a mixture model containing higher-order and lower-order n-gram models, with the higher-order models providing more capacity and the lower-order models being more likely to avoid counts of zero. Back-off methods look up the lower-order n-grams if the frequency of the context x~1,...,2+~-n+41 is too small to use the higher-order model. More formally, they estimate the distribution over x; by using contexts Y4-n+k,---;@+-1, for increasing k, until a sufficiently reliable estimate is found. Classical n-gram models are particularly vulnerable to the curse of dimension- ality. There are |V|\" possible n-grams and |V| is often very large. Even with a massive training set and modest n, most n-grams will not occur in the training set. One way to view a classical n-gram model is that it is performing nearest neighbor  yo4 T ad ton 1',\n",
       " '3e16f078-851e-4aed-be1f-60ec856014fe': 'Shorten and Khoshgoftaar J Big Data  6:60   “deconv” conv project & |, “deconv” = conv reshape “deconv” Leesa om Real | I} a axs xe oS : 100 8x8 16x16 @si2 e104 @s12 mone 32x32 @128 ene @2s6 32X32 64X64X1 @128 @6a lesion ie J J Y T Generator Discriminator Fig. 18 Complete DCGAN architecture used by Frid-Adar et al. to generate liver lesion images  Frid-Adar et al. tested the effectiveness of using DCGANs to generate liver lesion medical images. They use the architecture pictured above to generate 64 x 64 x 1 size images of liver lesion CT scans',\n",
       " '881298e6-bd12-4d25-aef2-ac22bdee79be': 'The PCL-based training updates Q-values of all tokens at once through a connection between the value function and the induced policy. More speciﬁcally, Nachum et al. showed that the optimal policy π∗ (Eq.3) and the optimal state value function V ∗ (Eq.5) in SQL must satisfy the following consistency property for all states and actions: V ∗ (st) − γV ∗ (st+1) = rt − log π∗ (at|st) , ∀st, at. (6) Accordingly, the PCL-based training attempts to encourage the satisfaction of the consistency with the following regression objective LSQL, PCL(θ): (7) where πθ is the induced policy deﬁned in Eq. (4); V¯θ is deﬁned similarly as in Eq.\\n\\n(5) but depends on the target Q¯θ network (i.e., a slow copy of the Qθ to be learned), and recall that π′ is an arbitrary behavior policy (e.g., data distribution). Please see Figure 2 (left) for an illustration',\n",
       " '960b75e8-01bf-445d-8ee0-4953f2342e5e': '(Right)Regularized GCN, with \\\\ > 0, draws examples toward the sphere but does not completely discard the variation in their norm. We leave s and € the same as before.\\n\\nmotivates introducing a small positive regularization parameter to bias the estimate of the standard deviation. Alternately, one can constrain the denominator to be at least «. Given an input image X, GCN produces an output image X’, defined such that  Xi j,k —X  7  ijk — 5 — max {6 At Bre Lint Dj Lok (Xi,j,k— X) \\\\  (12.3)  Datasets consisting of large images cropped to interesting objects are unlikely to contain any images with nearly constant intensity. In these cases, it is safe to practically ignore the small denominator problem by setting \\\\ = 0 and avoid division by 0 in extremely rare cases by setting € to an extremely low value like 10-8. This is the approach used by Goodfellow ef al. on the CIFAR-10 dataset. Small images cropped randomly are more likely to have nearly constant intensity, making aggressive regularization more useful. Coates ef al',\n",
       " '6f28efaa-0dbf-49b7-87d2-095a5a9cbf83': 'Constructing d-dimensional feature vectors to represent states is the same as selecting a set of d basis functions.\\n\\nFeatures may be deﬁned in many di↵erent ways; we cover a few possibilities in the next sections. It is natural to use SGD updates with linear function approximation. The gradient of the approximate value function with respect to w in this case is Because it is so simple, the linear SGD case is one of the most favorable for mathematical analysis. Almost all useful convergence results for learning systems of all kinds are for linear (or simpler) function approximation methods. In particular, in the linear case there is only one optimum (or, in degenerate cases, one set of equally good optima), and thus any method that is guaranteed to converge to or near a local optimum is automatically guaranteed to converge to or near the global optimum. For example, the gradient Monte Carlo algorithm presented in the previous section converges to the global optimum of the VE under linear function approximation if ↵ is reduced over time according to the usual conditions',\n",
       " 'd4c1b974-8b3e-4276-b426-c3684076caef': 'Thus x will always appear in the set of conditioning variables, and so from now on we will drop the explicit x from expressions such as p(t|x, w, β) in order to keep the notation uncluttered.\\n\\nTaking the logarithm of the likelihood function, and making use of the standard form (1.46) for the univariate Gaussian, we have where the sum-of-squares error function is deﬁned by Having written down the likelihood function, we can use maximum likelihood to determine w and β. Consider ﬁrst the maximization with respect to w. As observed already in Section 1.2.5, we see that maximization of the likelihood function under a conditional Gaussian noise distribution for a linear model is equivalent to minimizing a sum-of-squares error function given by ED(w). The gradient of the log likelihood function (3.11) takes the form which are known as the normal equations for the least squares problem. Here Φ is an N×M matrix, called the design matrix, whose elements are given by Φnj = φj(xn), so that is known as the Moore-Penrose pseudo-inverse of the matrix Φ',\n",
       " 'dcc9b6bd-5857-431c-9540-221f65595a91': 'In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3606– 3613. IEEE, 2014. Cubuk, E. D., Zoph, B., Mane, D., Vasudevan, V., and Le, Q. V. Autoaugment: Learning augmentation strategies from data. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 113–123, 2019. Doersch, C. and Zisserman, A. Multi-task self-supervised visual learning. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2051–2060, 2017. Doersch, C., Gupta, A., and Efros, A. A. Unsupervised visual representation learning by context prediction. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1422–1430, 2015. Donahue, J. and Simonyan, K. Large scale adversarial representation learning. In Advances in Neural Information Processing Systems, pp',\n",
       " 'c13cab25-a3f1-4b08-8a04-73db83427cfd': 'The ﬁrst stage in applying the Laplace framework to this model is to initialize the hyperparameter α, and then to determine the parameter vector w by maximizing the log posterior distribution. This is equivalent to minimizing the regularized error function E(w) = − ln p(D|w) + α and can be achieved using error backpropagation combined with standard optimization algorithms, as discussed in Section 5.3. Having found a solution wMAP for the weight vector, the next step is to evaluate the Hessian matrix H comprising the second derivatives of the negative log likelihood function. This can be done, for instance, using the exact method of Section 5.4.5, or using the outer product approximation given by (5.85). The second derivatives of the negative log posterior can again be written in the form (5.166), and the Gaussian approximation to the posterior is then given by (5.167).\\n\\nTo optimize the hyperparameter α, we again maximize the marginal likelihood, which is easily shown to take the form Exercise 5.41 where the regularized error function is deﬁned by in which yn ≡ y(xn, wMAP)',\n",
       " '2c245e88-17b6-4ada-b21c-ac9c2c0fef0d': '2th image => —> | |-— fh ——> J triage 128D 128D Be 1 th image ae 2048D Se \\\\-th image — EE  fo(x) 128D Unit Sphere  Let v = f(x) be an embedding function to learn and the vector is normalized to have |v| = 1.A non-parametric classifier predicts the probability of a sample v belonging to class 2 with a temperature parameter T:  P(C =i|\\\\v) =  Instead of computing the representations for all the samples every time, they implement an Memory Bank for storing sample representation in the database from past iterations. Let  V = {v,} be the memory bank and f; = f(x;) be the feature generated by forwarding the network. We can use the representation from the memory bank v; instead of the feature forwarded from the network f£; when comparing pairwise similarity. The denominator theoretically requires access to the representations of all the samples, but that is too expensive in practice. Instead we can estimate it via Monte Carlo approximation using a random subset of M indices {j,',\n",
       " '9b84d343-dfb2-4742-be76-575ee9deddb5': 'Would the variance of the estimator still be inﬁnite? Why or why not? ⇤ Monte Carlo prediction methods can be implemented incrementally, on an episode-byepisode basis, using extensions of the techniques described in Chapter 2 (Section 2.4). Whereas in Chapter 2 we averaged rewards, in Monte Carlo methods we average returns. In all other respects exactly the same methods as used in Chapter 2 can be used for onpolicy Monte Carlo methods. For o↵-policy Monte Carlo methods, we need to separately consider those that use ordinary importance sampling and those that use weighted importance sampling. In ordinary importance sampling, the returns are scaled by the importance sampling ratio ⇢t:T (t)−1 (5.3), then simply averaged, as in (5.5).\\n\\nFor these methods we can again use the incremental methods of Chapter 2, but using the scaled returns in place of the rewards of that chapter. This leaves the case of o↵-policy methods using weighted importance sampling. Here we have to form a weighted average of the returns, and a slightly di↵erent incremental algorithm is required. Suppose we have a sequence of returns G1, G2,',\n",
       " '3f50b4c4-70c8-4730-b21c-f4a9898b3436': 'Chapter 16  Structured Probabilistic Models for Deep Learning  Deep learning draws on many modeling formalisms that researchers can use to guide their design efforts and describe their algorithms. One of these formalisms is the idea of structured probabilistic models. We discuss structured probabilistic models briefly in section 3.14. That brief presentation is sufficient to understand how to use structured probabilistic models as a language to describe some of the algorithms in part II. Now, in part III, structured probabilistic models are a key ingredient of many of the most important research topics in deep learning. To prepare to discuss these research ideas, in this chapter, we describe structured probabilistic models in much greater detail. This chapter is intended to be self- contained; the reader does not need to review the earlier introduction before continuing with this chapter. A structured probabilistic model is a way of describing a probability distribution, using a graph to describe which random variables in the probability distribution interact with each other directly. Here we use “graph” in the graph theory sense—a set of vertices connected to one another by a set of edges. Because the structure of the model is defined by a graph, these models are often also referred to as graphical models',\n",
       " '254353a9-0dec-43e0-99d7-a2e4b5555dda': 'As with the k-nearest neighbors algorithm, each input is represented with multiple values, but  https://www.deeplearningbook.org/contents/representation.html    those values cannot readily be controlled separately from each other. e Kernel machines with a Gaussian kernel (or other similarly local kernel): although the degree of activation of each “support vector” or template example is now continuous-valued, the same issue arises as with Gaussian mixtures. e Language or translation models based on n-grams: the set of contexts (sequences of symbols) is partitioned according to a tree structure of suffixes. A leaf may correspond to the last two words being w, and wy, for example.\\n\\nSeparate parameters are estimated for each leaf of the tree (with some sharing being possible). For some of these nondistributed algorithms, the output is not constant by parts but instead interpolates between neighboring regions. The relationship between the number of parameters (or examples) and the number of regions they can define remains linear. An important related concept that distinguishes a distributed representation from a symbolic one is that generalization arises due to shared attributes between different concepts',\n",
       " 'a44b1a3b-72d1-4d50-b579-4a58afd25fb8': 'A signiﬁcant di↵erence between AlphaGo Zero and AlphaGo is that AlphaGo Zero used MCTS to select moves throughout self-play reinforcement learning, whereas AlphaGo used MCTS for live play after—but not during—learning. Other di↵erences besides not using any human data or human-crafted features are that AlphaGo Zero used only one deep convolutional ANN and used a simpler version of MCTS. AlphaGo Zero’s MCTS was simpler than the version used by AlphaGo in that it did not include rollouts of complete games, and therefore did not need a rollout policy.\\n\\nEach iteration of AlphaGo Zero’s MCTS ran a simulation that ended at a leaf node of the current search tree instead of at the terminal position of a complete game simulation. But as in AlphaGo, each iteration of MCTS in AlphaGo Zero was guided by the output of a deep convolutional network, labeled f✓ in Figure 16.7, where ✓ is the network’s weight vector',\n",
       " '945047e0-0b9e-4ae1-ae9d-267438be4d1b': '1 1 soa od sad wd 1  https://www.deeplearningbook.org/contents/convnets.html    * ine numan visual sysven IS lvegraved WIL Waly OlWer senses, SUC as hearing, and factors like our moods and thoughts.\\n\\nConvolutional networks so far are purely visual. e The human visual system does much more than just recognize objects. It is able to understand entire scenes, including many objects and relationships between objects, and it processes rich 3-D geometric information needed for our bodies to interface with the world. Convolutional networks have been applied to some of these problems, but these applications are in their infancy. e Even simple brain areas like V1 are heavily affected by feedback from higher levels. Feedback has been explored extensively in neural network models but has not yet been shown to offer a compelling improvement. e While feedforward IT firing rates capture much of the same information as convolutional network features, it is not clear how similar the intermediate computations are. The brain probably uses very different activation and pooling functions. An individual neuron’s activation probably is not well characterized by a single linear filter response',\n",
       " '3551bdc7-e1f0-4abd-8d4c-42964335fd72': 'Hashing frees us from the curse of dimensionality in the sense that memory requirements need not be exponential in the number of dimensions, but need merely match the real demands of the task. Open-source implementations of tile Exercise 9.4 Suppose we believe that one of two state dimensions is more likely to have an e↵ect on the value function than is the other, that generalization should be primarily across this dimension rather than along it.\\n\\nWhat kind of tilings could be used to take advantage of this prior knowledge? ⇤ Radial basis functions (RBFs) are the natural generalization of coarse coding to continuousvalued features. Rather than each feature being either 0 or 1, it can be anything in the interval , reﬂecting various degrees to which the feature is present. A typical RBF feature, xi, has a Gaussian (bell-shaped) response xi(s) dependent only on the distance between the state, s, and the feature’s prototypical or center state, ci, and relative to the feature’s width, σi: The norm or distance metric of course can be chosen in whatever way seems most appropriate to the states and task at hand. The ﬁgure below shows a one-dimensional example with a Euclidean distance metric',\n",
       " '6f9cc1e3-74f6-4eea-ba46-9e1b7f9f9d09': 'Because there is one basis function associated with every data point, the corresponding model can be computationally costly to evaluate when making predictions for new data points. Models have therefore been proposed , which retain the expansion in radial basis functions but where the number M of basis functions is smaller than the number N of data points. Typically, the number of basis functions, and the locations µi of their centres, are determined based on the input data {xn} alone. The basis functions are then kept ﬁxed and the coefﬁcients {wi} are determined by least squares by solving the usual set of linear equations, as discussed in Section 3.1.1. One of the simplest ways of choosing basis function centres is to use a randomly chosen subset of the data points. A more systematic approach is called orthogonal least squares .\\n\\nThis is a sequential selection process in which at each step the next data point to be chosen as a basis function centre corresponds to the one that gives the greatest reduction in the sum-of-squares error. Values for the expansion coefﬁcients are determined as part of the algorithm. Clustering algorithms such as K-means have also been used, which give a set of basis function centres that Section 9.1 no longer coincide with training data points',\n",
       " '916d2d9f-591f-4c7c-83c0-59628f0c5d08': 'Evaluating or maximizing the log-likelihood requires confronting not just the problem of intractable inference to marginalize out the latent variables, but also  the problem of an intractable partition function within the undirected model of the top two layers.\\n\\nTo train a deep belief network, one begins by training an RBM to maximize Ev~paata log p(v) using contrastive divergence or stochastic maximum likelihood. The parameters of the RBM then define the parameters of the first layer of the DBN. Next, a second RBM is trained to approximately maximize  Sy Paaa np) (nv) log p (A), (20.21)  where p) is the probability distribution represented by the first RBM, and p() is the probability distribution represented by the second RBM. In other words, the second RBM is trained to model the distribution defined by sampling the hidden units of the first RBM, when the first RBM is driven by the data. This procedure can be repeated indefinitely, to add as many layers to the DBN as desired, with each new RBM modeling the samples of the previous one. Each RBM defines another layer of the DBN',\n",
       " '531ee2ef-f005-43eb-a789-4a4afb03beed': 'Timing in simple conditioning and occasion setting: A neural network approach. Behavioural Processes, 45(1):33–57. Bu¸soniu, L., Lazaric, A., Ghavamzadeh, M., Munos, R., Babu˘ska, R., De Schutter, B. Reinforcement Learning: State-of-the-Art, pp. 75–109. Springer-Verlag Berlin Heidelberg. Bush, R. R., Mosteller, F. Stochastic Models for Learning. Wiley, New York. Byrne, J. H., Gingrich, K. J., Baxter, D. A. Computational capabilities of single neurons: Relationship to simple forms of associative and nonassociative learning in aplysia. In R. D. Hawkins and G. H. Bower (Eds. ), Computational Models of Learning, pp. 31–63',\n",
       " '36b10b7c-7df2-4787-8462-7669a0f5bbe0': 'If ¢(a) is of high enough dimension, we can always have enough capacity to fit the training set, but generalization to the test set often remains poor.\\n\\nVery generic feature mappings are usually based only on the principle of local smoothness and do not encode enough prior information to solve advanced problems. 2. Another option is to manually engineer ¢. Until the advent of deep learning, this was the dominant approach. It requires decades of human effort for each separate task, with practitioners specializing in different domains, such as speech recognition or computer vision, and with little transfer between domains. 3. The strategy of deep learning is to learn ¢. In this approach, we have a model y = f(x; 0, w) = 6(@; 0)\" w. We now have parameters @ that we use to learn ¢ from a broad class of functions, and parameters w that map from ¢(a) to the desired output. This is an example of a deep feedforward network, with ¢ defining a hidden layer. This approach is the only one of the three that gives up on the convexity of the training problem, but the benefits outweigh the harms',\n",
       " 'c4b9e57c-0050-41e1-8250-3e9bb0cfc24b': 'Correct application of an algorithm depends on mastering some fairly simple methodology.\\n\\nMany of the recommendations in this chapter are adapted from Ng . We recommend the following practical design process:  e Determine your goals—what error metric to use, and your target value for this error metric. These goals and error metrics should be driven by the problem that the application is intended to solve. https://www.deeplearningbook.org/contents/guidelines.html    °® Hstablish a working end-to-end pipeline as soon as possible, including the 416  CHAPTER 11. PRACTICAL METHODOLOGY  estimation of the appropriate performance metrics. e Instrument the system well to determine bottlenecks in performance. Diag- nose which components are performing worse than expected and whether poor performance is due to overfitting, underfitting, or a defect in the data or software. e Repeatedly make incremental changes such as gathering new data, adjusting hyperparameters, or changing algorithms, based on specific findings from your instrumentation. As a running example, we will use the Street View address number transcription system . The purpose of this application is to add buildings to Google Maps',\n",
       " '66330527-b680-47cd-be13-c021044d9919': 'We now take the various parameters of the mixture model, namely the mixing coefﬁcients πk(x), the means µk(x), and the variances σ2 the outputs of a conventional neural network that takes x as its input. The structure of this mixture density network is illustrated in Figure 5.20. The mixture density network is closely related to the mixture of experts discussed in Section 14.5.3.\\n\\nThe principle difference is that in the mixture density network the same function is used to predict the parameters of all of the component densities as well as the mixing coefﬁcients, and so the nonlinear hidden units are shared amongst the input-dependent functions. The neural network in Figure 5.20 can, for example, be a two-layer network having sigmoidal (‘tanh’) hidden units. If there are L components in the mixture model (5.148), and if t has K components, then the network will have L output unit activations denoted by aπ k that determine the mixing coefﬁcients πk(x), K outputs k that determine the kernel widths σk(x), and L × K outputs denoted kj that determine the components µkj(x) of the kernel centres µk(x)',\n",
       " '3cdcbf2b-b64f-4d31-9003-619f162649d2': 'When y = f(a), the model assigns an input described by vector x to a category identified by numeric code y. There are other variants of the classification task, for example, where f outputs a probability distribution over classes. An example of a classification task is object recognition, where the input is an image (usually described as a set of pixel brightness values), and the output is a numeric code identifying the object in the image.\\n\\nFor example, the Willow Garage PR2 robot is able to act as a waiter that can recognize different kinds of drinks and deliver them to people on command . Modern object recognition is best accomplished with deep learning . Object recognition is the same basic technology that enables computers to recognize faces , which can be used to automatically tag people in photo collections and for computers to interact more naturally with their users. e Classification with missing inputs: Classification becomes more chal- lenging if the computer program is not guaranteed that every measurement in its input vector will always be provided. To solve the classification task, the learning algorithm only has to define a single function mapping from a vector input to a categorical output',\n",
       " '706a50e8-5dff-48c1-a950-34fc987f3e81': 'We have discussed the importance of probability distributions that are members of the exponential family, and we have seen that this family includes many wellSection 2.4 known distributions as particular cases.\\n\\nAlthough such distributions are relatively simple, they form useful building blocks for constructing more complex probability distributions, and the framework of graphical models is very useful in expressing the way in which these building blocks are linked together. Such models have particularly nice properties if we choose the relationship between each parent-child pair in a directed graph to be conjugate, and we shall explore several examples of this shortly. Two cases are particularly worthy of note, namely when the parent and child node each correspond to discrete variables and when they each correspond to Gaussian variables, because in these two cases the relationship can be extended hierarchically to construct arbitrarily complex directed acyclic graphs. We begin by examining the discrete case. The probability distribution p(x|µ) for a single discrete variable x having K possible states (using the 1-of-K representation) is given by k µk = 1, only K − 1 values for µk need to be speciﬁed in order to deﬁne the Now suppose that we have two discrete variables, x1 and x2, each of which has K states, and we wish to model their joint distribution',\n",
       " '89dcee4a-fdf2-4110-82e4-5630d4feb601': 'For instance, the region x1 ⩽ θ1 is further subdivided according to whether x2 ⩽ θ2 or x2 > θ2, giving rise to the regions denoted A and B. The recursive subdivision can be described by the traversal of the binary tree shown in Figure 14.6. For any new input x, we determine which region it falls into by starting at the top of the tree at the root node and following a path down to a speciﬁc leaf node according to the decision criteria at each node. Note that such decision trees are not probabilistic graphical models. Within each region, there is a separate model to predict the target variable.\\n\\nFor instance, in regression we might simply predict a constant over each region, or in classiﬁcation we might assign each region to a speciﬁc class. A key property of treebased models, which makes them popular in ﬁelds such as medical diagnosis, for example, is that they are readily interpretable by humans because they correspond to a sequence of binary decisions applied to the individual input variables. For instance, to predict a patient’s disease, we might ﬁrst ask “is their temperature greater than some threshold?”. If the answer is yes, then we might next ask “is their blood pressure less than some threshold?”',\n",
       " 'fa82c282-2ddc-47c5-9520-502072e56827': 'Like some variants of ICA, SFA is not quite a generative model per se, in the sense that it defines a linear map between input space and feature space but does not define a prior over feature space and thus does not impose a distribution p(a) on input space. The SFA algorithm  consists of defining f(a; 0) to be a linear transformation, then solving the optimization problem  min E,(f(al*), — f(al),)? (13.8) subject to the constraints Bef (a); =0 (13.9) and  = 1. (13.10)  The constraint that the learned feature have zero mean is necessary to make the problem have a unique solution; otherwise we could add a constant to all feature  490  CHAPTER 13. LINEAR FACTOR MODELS  values and obtain a different solution with equal value of the slowness objective.\\n\\nThe constraint that the features have unit variance is necessary to prevent the pathological solution where all features collapse to 0. Like PCA, the SFA features are ordered, with the first feature being the slowest. To learn multiple features, we must also add the constraint  Vi <j, Elf(@™) f(e);] = 0',\n",
       " 'bf183d0a-7ca7-48b6-a8a8-24fc979388fc': 'Restricted Boltzmann Machines  Invented under the name harmonium , restricted Boltzmann machines are some of the most common building blocks of deep probabilistic models. We briefly describe RBMs in section 16.7.1. Here we review the previous information and go into more detail. RBMs are undirected probabilistic graphical  https://www.deeplearningbook.org/contents/generative_models.html    models containing a layer of observable variables and a single layer of latent variables. RBMs may be stacked (one on top of the other) to form deeper models. See figure 20.1 for some examples. In particular, figure 20.la shows the graph  structure of the RBM itself. It is a bipartite graph, with no connections permitted between any variables in the observed layer or between any units in the latent layer. We begin with the binary version of the restricted Boltzmann machine, but as  653  CHAPTER 20. DEEP GENERATIVE MODELS  Figure 20.1: Examples of models that may be built with restricted Boltzmann machines',\n",
       " 'bff9d4b7-5a8f-415b-bd17-5086b8d63772': 'Since the objective is to maximize L(θ), we wish to compute an updated estimate θ such that, L(θ) > L(θn) (8) Equivalently we want to maximize the diﬀerence, So far, we have not considered any unobserved or missing variables. In problems where such data exist, the EM algorithm provides a natural framework for their inclusion. Alternately, hidden variables may be introduced purely as an artiﬁce for making the maximum likelihood estimation of θ tractable. In this case, it is assumed that knowledge of the hidden variables will make the maximization of the likelihood function easier. Either way, denote the hidden random vector by Z and a given realization by z. The total probability P(X|θ) may be written in terms of the hidden variables z as, We may then rewrite Equation (9) as, Notice that this expression involves the logarithm of a sum. In Section (2) using Jensen’s inequality, it was shown that, for constants λi ≥ 0 with �n i=1 λi = 1.\\n\\nThis result may be applied to Equation (11) which involves the logarithm of a sum provided that the constants λi can be identiﬁed',\n",
       " '83ab9bbe-a859-4a79-bf29-05e45186c828': 'Whereas rewards determine the immediate, intrinsic desirability of environmental states, values indicate the long-term desirability of states after taking into account the states that are likely to follow and the rewards available in those states. For example, a state might always yield a low immediate reward but still have a high value because it is regularly followed by other states that yield high rewards.\\n\\nOr the reverse could be true. To make a human analogy, rewards are somewhat like pleasure (if high) and pain (if low), whereas values correspond to a more reﬁned and farsighted judgment of how pleased or displeased we are that our environment is in a particular state. Rewards are in a sense primary, whereas values, as predictions of rewards, are secondary. Without rewards there could be no values, and the only purpose of estimating values is to achieve more reward. Nevertheless, it is values with which we are most concerned when making and evaluating decisions. Action choices are made based on value judgments. We seek actions that bring about states of highest value, not highest reward, because these actions obtain the greatest amount of reward for us over the long run. Unfortunately, it is much harder to determine values than it is to determine rewards',\n",
       " '04af7524-ccee-45b6-ad81-ef64602f8301': 'The marginal likelihood function p(t|α, β) is obtained by integrating over the weight parameters w, so that One way to evaluate this integral is to make use once again of the result (2.115) for the conditional distribution in a linear-Gaussian model. Here we shall evaluate Exercise 3.16 the integral instead by completing the square in the exponent and making use of the standard form for the normalization coefﬁcient of a Gaussian. From (3.11), (3.12), and (3.52), we can write the evidence function in the form Exercise 3.17 where M is the dimensionality of w, and we have deﬁned We recognize (3.79) as being equal, up to a constant of proportionality, to the regularized sum-of-squares error function (3.27).\\n\\nWe now complete the square over w Exercise 3.18 Note that A corresponds to the matrix of second derivatives of the error function and is known as the Hessian matrix. Here we have also deﬁned mN given by Using (3.54), we see that A = S−1 N , and hence (3.84) is equivalent to the previous deﬁnition (3.53), and therefore represents the mean of the posterior distribution',\n",
       " 'f622f62a-5f46-48a9-bb25-59b74cbd1361': 'The general solution to the minimization of J for arbitrary D and arbitrary M < D is obtained by choosing the {Ui} to be eigenvectors of the covariance matrix given by SUi = AiUi (12.17) where i = 1, ... ,D, and as usual the eigenvectors {Ui} are chosen to be orthonormal. The corresponding value of the distortion measure is then given by which is simply the sum of the eigenvalues of those eigenvectors that are orthogonal to the principal subspace. We therefore obtain the minimum value of J by selecting these eigenvectors to be those having the D - M smallest eigenvalues, and hence the eigenvectors defining the principal subspace are those corresponding to the M largest eigenvalues. Although we have considered M < D, the PCA analysis still holds if M = D, in which case there is no dimensionality reduction but simply a rotation of the coordinate axes to align with principal components. Finally, it is worth noting that there exists a closely related linear dimensionality reduction technique called canonical correlation analysis, or CCA',\n",
       " '2d758864-86df-4436-8961-b2a398c81e4e': ', xM)T is an (M + 1)-dimensional vector of parent states augmented with an additional variable x0 whose value is clamped to 1, and w = (w0, w1, . , wM)T is a vector of M + 1 parameters. This is a more restricted form of conditional distribution than the general case but is now governed by a number of parameters that grows linearly with M. In this sense, it is analogous to the choice of a restrictive form of covariance matrix (for example, a diagonal matrix) in a multivariate Gaussian distribution. The motivation for the logistic sigmoid representation was discussed in Section 4.2. In the previous section, we saw how to construct joint probability distributions over a set of discrete variables by expressing the variables as nodes in a directed acyclic graph. Here we show how a multivariate Gaussian can be expressed as a directed graph corresponding to a linear-Gaussian model over the component variables.\\n\\nThis allows us to impose interesting structure on the distribution, with the general Gaussian and the diagonal covariance Gaussian representing opposite extremes',\n",
       " '137e410b-8a73-4604-9a72-58e512823cb2': 'We denote the probability of observing both x1k = 1 and x2l = 1 by the parameter µkl, where x1k denotes the kth component of x1, and similarly for x2l. The joint distribution can be written Because the parameters µkl are subject to the constraint � bution is governed by K2 − 1 parameters.\\n\\nIt is easily seen that the total number of parameters that must be speciﬁed for an arbitrary joint distribution over M variables is KM − 1 and therefore grows exponentially with the number M of variables. Using the product rule, we can factor the joint distribution p(x1, x2) in the form p(x2|x1)p(x1), which corresponds to a two-node graph with a link going from the x1 node to the x2 node as shown in Figure 8.9(a). The marginal distribution p(x1) is governed by K − 1 parameters, as before, Similarly, the conditional distribution p(x2|x1) requires the speciﬁcation of K − 1 parameters for each of the K possible values of x1',\n",
       " 'd21cca0e-fc74-4b8d-a3a0-7cff5e8c670e': \"Textmix < My © I, + (1 — Mp) © In, where My € {0, 1} is a binary mask and © is element-wise multiplication. It is equivalent to filling the cutout  region with the same region from another image. MoCHi : Given a query q, MoCHi maintains a queue of K negative features Q = {n, sey nx} and sorts these negative features by similarity to the query, q'n, in descending order. The first VV items in the queue are considered as the hardest negatives, Qn. Then synthetic hard examples can be generated by  h = h/|h| where h = an, + (1 — @)n, and a € (0, 1). Even harder examples can be created by mixing with the query feature, h’ = h’/|h’|y where h’ = Bq + (1 — B)n; and 6 € (0, 0.5)\",\n",
       " '430f1a33-6a00-4191-8209-90f416b32d30': 'If n = |S| is the number of states, then just forming the maximum-likelihood estimate of the process may require on the order of n2 memory, and computing the corresponding value function requires on the order of n3 computational steps if done conventionally. In these terms it is indeed striking that TD methods can approximate the same solution using memory no more than order n and repeated computations over the training set. On tasks with large state spaces, TD methods may be the only feasible way of approximating the certainty-equivalence solution. ⇤Exercise 6.7 Design an o↵-policy version of the TD(0) update that can be used with We turn now to the use of TD prediction methods for the control problem. As usual, we follow the pattern of generalized policy iteration (GPI), only this time using TD methods for the evaluation or prediction part.\\n\\nAs with Monte Carlo methods, we face the need to trade o↵ exploration and exploitation, and again approaches fall into two main classes: on-policy and o↵-policy. In this section we present an on-policy TD control method. The ﬁrst step is to learn an action-value function rather than a state-value function',\n",
       " '19688d58-1e49-485c-a074-dbb451c4d3b6': 'Maximum entropy rl (provably) solves some robust rl problems. arXiv preprint arXiv:2103.06257. Roy Fox, Ari Pakman, and Naftali Tishby. 2016. Taming the noise in reinforcement learning via soft updates. In Proceedings of the Thirty-Second Conference on Uncertainty in Artiﬁcial Intelligence, pages 202–211. Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, and Jun Wang. 2018. Long text generation via adversarial training with leaked information. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence. Tatsunori Hashimoto, Hugh Zhang, and Percy Liang. 2019. Unifying human and statistical evaluation for natural language generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1689–1701.\\n\\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019',\n",
       " 'b12c0345-ed68-49af-a1a7-eba4716aefbe': 'In the E step of the EM algorithm, we then use these parameter values to ﬁnd the posterior distribution over w, which is given by (10.156). In the M step, we then maximize the expected complete-data log likelihood which is given by where the expectation is taken with respect to the posterior distribution q(w) evaluated using ξold. Noting that p(w) does not depend on ξ, and substituting for h(w, ξ) we obtain (10.161) where ‘const’ denotes terms that are independent of ξ. We now set the derivative with respect to ξn equal to zero.\\n\\nA few lines of algebra, making use of the deﬁnitions of σ(ξ) and λ(ξ), then gives We now note that λ′(ξ) is a monotonic function of ξ for ξ ⩾ 0, and that we can restrict attention to nonnegative values of ξ without loss of generality due to the symmetry of the bound around ξ = 0. Thus λ′(ξ) ̸= 0, and hence we obtain the following re-estimation equations Exercise 10.33 where we have used (10.156). Let us summarize the EM algorithm for ﬁnding the variational posterior distribution',\n",
       " 'ad9a0905-b809-47e1-8e9a-1edcff747707': 'The use of a reward signal to formalize the idea of a goal is one of the most distinctive features of reinforcement learning. Although formulating goals in terms of reward signals might at ﬁrst appear limiting, in practice it has proved to be ﬂexible and widely applicable. The best way to see this is to consider examples of how it has been, or could be, used. For example, to make a robot learn to walk, researchers have provided reward on each time step proportional to the robot’s forward motion. In making a robot learn how to escape from a maze, the reward is often −1 for every time step that passes prior to escape; this encourages the agent to escape as quickly as possible. To make a robot learn to ﬁnd and collect empty soda cans for recycling, one might give it a reward of zero most of the time, and then a reward of +1 for each can collected. One might also want to give the robot negative rewards when it bumps into things or when somebody yells at it.\\n\\nFor an agent to learn to play checkers or chess, the natural rewards are +1 for winning, −1 for losing, and 0 for drawing and for all nonterminal positions. You can see what is happening in all of these examples',\n",
       " '287f6f3d-d574-4be3-ad31-0bc1316bb335': 'The knowledge the agent brings to the task at the start—either from previous experience with related tasks or built into it by design or evolution—inﬂuences what is useful or easy to learn, but interaction with the environment is essential for adjusting behavior to exploit speciﬁc features of the task. Beyond the agent and the environment, one can identify four main subelements of a reinforcement learning system: a policy, a reward signal, a value function, and, optionally, a model of the environment. A policy deﬁnes the learning agent’s way of behaving at a given time. Roughly speaking, a policy is a mapping from perceived states of the environment to actions to be taken when in those states. It corresponds to what in psychology would be called a set of stimulus–response rules or associations. In some cases the policy may be a simple function or lookup table, whereas in others it may involve extensive computation such as a search process. The policy is the core of a reinforcement learning agent in the sense that it alone is suﬃcient to determine behavior. In general, policies may be stochastic, specifying probabilities for each action',\n",
       " 'b71f7681-dad1-4e1d-9578-246102013581': 'Any value of x in the interval from x = a to x = b can be written in the form λa + (1 − λ)b where 0 ⩽ λ ⩽ 1. The corresponding point on the chord is given by λf(a) + (1 − λ)f(b), ory. This paper introduced the word ‘bit’, and his concept that information could be sent as a stream of 1s and 0s paved the way for the communications revolution. It is said that von Neumann recommended to Shannon that he use the term entropy, not only because of its similarity to the quantity used in physics, but also because “nobody knows what entropy really is, so in any discussion you will always have an advantage”. and the corresponding value of the function is f (λa + (1 − λ)b). Convexity then implies This is equivalent to the requirement that the second derivative of the function be everywhere positive. Examples of convex functions are x ln x (for x > 0) and x2',\n",
       " 'f1b57013-51f3-48df-a6c1-0f524be7f88b': 'They simply save training examples in memory as they arrive (or at least save a subset of the examples) without updating any parameters. Then, whenever a query state’s value estimate is needed, a set of examples is retrieved from memory and used to compute a value estimate for the query state. This approach is sometimes called lazy learning because processing training examples is postponed until the system is queried to provide an output. Memory-based function approximation methods are prime examples of nonparametric methods. Unlike parametric methods, the approximating function’s form is not limited to a ﬁxed parameterized class of functions, such as linear functions or polynomials, but is instead determined by the training examples themselves, together with some means for combining them to output estimated values for query states. As more training examples accumulate in memory, one expects nonparametric methods to produce increasingly accurate approximations of any target function. There are many di↵erent memory-based methods depending on how the stored training examples are selected and how they are used to respond to a query.\\n\\nHere, we focus on local-learning methods that approximate a value function only locally in the neighborhood of the current query state',\n",
       " '835a360c-ca36-4681-9e85-60d45396f776': 'lan JG, David W-F, Mehdi M, Aaron C, Yoshua B. Maxout networks. arXiv preprint. 2013. Shuangtao L, Yuanke C, Yanlin P, Lin B. Learning more robust features with adversarial training. ArXiv preprints. 2018. Lingxi X, Jingdong W, Zhen W, Meng W, QiT. DisturbLabel: regularizing CNN on the loss layer. arXiv preprint. 2016. Christopher B, Liang C, Ricardo GPB, Roger G, Alexander H, David AD, Maria VH, Joanna W, Daniel R. GAN augmen- tation: augmenting training data using generative adversarial networks. arXiv preprint. 2018. Doersch C. Tutorial on Variational Autoencoders. ArXiv e-prints. 2016. Laurens M, Geoffrey H. Visualizing data using t-SNE',\n",
       " '3b5e3cd9-fea8-4783-b124-3ba68081be88': 'Also show that the minimum expected Lq loss for q → 0 is given by the conditional mode, i.e., by the function y(x) equal to the value of t that maximizes p(t|x) for each x. 1.28 (⋆) In Section 1.6, we introduced the idea of entropy h(x) as the information gained on observing the value of a random variable x having distribution p(x). We saw that, for independent variables x and y for which p(x, y) = p(x)p(y), the entropy functions are additive, so that h(x, y) = h(x) + h(y). In this exercise, we derive the relation between h and p in the form of a function h(p).\\n\\nFirst show that h(p2) = 2h(p), and hence by induction that h(pn) = nh(p) where n is a positive integer. Hence show that h(pn/m) = (n/m)h(p) where m is also a positive integer',\n",
       " '6d5c94a6-c625-452d-9db1-719ca3769f8c': 'When the dataset has hundreds of thousands of examples or more, this is not a serious issue. When the dataset is too small, alternative procedures enable one to use all the examples in the estimation of the mean test error, at the price of increased computational cost. These procedures are based on the idea of repeating the training and testing computation on different randomly chosen subsets or splits of the original dataset. The most common of these is the k-fold cross-validation procedure, shown in algorithm 5.1, in which a partition of the dataset is formed by splitting it into k nonoverlapping subsets. The test error may then be estimated by taking the average test error across k trials. On trial 7, the i-th subset of the data is used as the test set, and the rest of the data is used as the training set. One problem is that no unbiased estimators of the variance of such average error estimators exist , but approximations are typically used. 5.4 Estimators, Bias and Variance  The field of statistics gives us many tools to achieve the machine learning goal of solving a task not only on the training set but also to generalize',\n",
       " 'f64afd8a-6dc8-4f80-8677-7921c31a4fed': 'Adaptive Filtering Prediction and Control. Prentice-Hall, Gopnik, A., Glymour, C., Sobel, D., Schulz, L. E., Kushnir, T., Danks, D. A theory of causal learning in children: Causal maps and Bayes nets. Psychological Review, 111(1):3–32. Gordon, G. J. Stable function approximation in dynamic programming. In A. Prieditis and S. Russell (Eds. ), Proceedings of the 12th International Conference on Machine Learning , pp. 261–268. Morgan Kaufmann. An expanded version was published as Technical Report CMU-CS-95-103. Carnegie Mellon University, Pittsburgh, PA, 1995. Gordon, G. J. Chattering in SARSA(λ). CMU learning lab internal report. Gordon, G. J. Stable ﬁtted reinforcement learning.\\n\\nIn Advances in Neural Information Processing Systems 8 , pp',\n",
       " 'ccea5816-ce10-404c-8c2e-10f2f4c4ac71': 'used € = 0 and A = 10 on small, randomly selected patches drawn from CIFAR-10. The scale parameter s can usually be set to 1, as done by Coates et al. , or chosen to make each individual pixel have standard deviation across examples close to 1, as done by Goodfellow et al. The standard deviation in equation 12.3 is just a rescaling of the L? norm  450  https://www.deeplearningbook.org/contents/applications.html    CHAPTER 12. APPLICATIONS  of the image (assuming the mean of the image has already been removed). It is preferable to define GCN in terms of standard deviation rather than L?\\n\\nnorm because the standard deviation includes division by the number of pixels, so GCN based on standard deviation allows the same s to be used regardless of image size. However, the observation that the L? norm is proportional to the standard deviation can help build a useful intuition. One can understand GCN as mapping examples to a spherical shell. See figure 12.1 for an illustration. This can be a useful property because neural networks are often better at responding to directions in space rather than to exact locations',\n",
       " '1ef863f0-3809-4cac-a396-5435f41e5083': 'More generally, we can consider priors in which the weights are divided into any number of groups Wk so that As a special case of this prior, if we choose the groups to correspond to the sets of weights associated with each of the input units, and we optimize the marginal likelihood with respect to the corresponding parameters αk, we obtain automatic relevance determination as discussed in Section 7.2.2. An alternative to regularization as a way of controlling the effective complexity of a network is the procedure of early stopping. The training of nonlinear network models corresponds to an iterative reduction of the error function deﬁned with respect to a set of training data. For many of the optimization algorithms used for network training, such as conjugate gradients, the error is a nonincreasing function of the iteration index. However, the error measured with respect to independent data, generally called a validation set, often shows a decrease at ﬁrst, followed by an increase as the network starts to over-ﬁt',\n",
       " 'ba186160-c0da-4d3a-a08f-1b5fedfe93ff': 'Text data augmentation has also achieved impressive success, such as contextual augmentation , back-translation , and manual approaches . In addition to perturbing the input text as in classiﬁcation tasks, text generation problems expose opportunities to adding noise also in the output text, such as .\\n\\nRecent work  shows output nosing in sequence generation can be treated as an intermediate approach in between supervised learning and reinforcement learning, and developed a new sequence learning algorithm that interpolates between the spectrum of existing algorithms. We instantiate our approach for text contextual augmentation as in , but enhance the previous work by additionally ﬁne-tuning the augmentation network jointly with the target model. Data weighting has been used in various algorithms, such as AdaBoost , self-paced learning , hard-example mining , and others . These algorithms largely deﬁne sample weights based on training loss. Recent work  learns a separate network to predict sample weights. Of particular relevance to our work is  which induces sample weights using a validation set. The data weighting mechanism instantiated by our framework has a key difference in that samples weights are treated as parameters that are updated iteratively, instead of re-estimated from scratch at each step. We show improved performance of our approach',\n",
       " 'b84bc755-a62b-405f-958e-dbaa0b83e1b5': 'Several different performance metrics may be used to measure the effectiveness of a complete application that includes machine learning components. These performance metrics are usually different from the cost function used to train the model. As described in section 5.1.2, it is common to measure the accuracy, or equivalently, the error rate, of a system. However, many applications require more advanced metrics. Sometimes it is much more costly to make one kind of a mistake than another. For example, an e-mail spam detection system can make two kinds of mistakes: incorrectly classifying a legitimate message as spam, and incorrectly allowing a spam message to appear in the inbox.\\n\\nIt is much worse to block a legitimate message than to allow a questionable message to pass through. Rather than measuring the error rate of a spam classifier, we may wish to measure some form of total cost, where the cost of blocking legitimate messages is higher than the cost of allowing spam messages. Sometimes we wish to train a binary classifier that is intended to detect some rare event. For example, we might design a medical test for a rare disease. Suppose that only one in every million people has this disease',\n",
       " '97c88710-c7e5-4970-9e01-31b8a3f61230': 'Hull hypothesized that an animal’s actions leave internal stimuli whose traces decay exponentially as functions of time since an action was taken. Looking at the animal learning data available at the time, he hypothesized that the traces e↵ectively reach zero after 30 to 40 seconds. The eligibility traces used in the algorithms described in this book are like Hull’s traces: they are decaying traces of past state visitations, or of past state–action pairs.\\n\\nEligibility traces were introduced by Klopf  in his neuronal theory in which they are temporally-extended traces of past activity at synapses, the connections between neurons. Klopf’s traces are more complex than the exponentially-decaying traces our algorithms use, and we discuss this more when we take up his theory in Section 15.9. To account for goal gradients that extend over longer time periods than spanned by stimulus traces, Hull  proposed that longer gradients result from conditioned reinforcement passing backwards from the goal, a process acting in conjunction with his molar stimulus traces',\n",
       " '727203c6-3bf8-4fac-a246-49647f8907cb': '+ won + wan  https://www.deeplearningbook.org/contents/partition.html  gradient ot ee pwhen experienc! ng real events while awake and follows the negative radient of log p to minimize log Z while sleeping and experiencing events sampled rom the current model. This view explains much of the language used to describe algorithms with a positive and a negative phase, but it has not been proved to be  correct with neuroscientific experiments.\\n\\nIn machine learning models, it is usually necessary to use the positive and negative phase simultaneously, rather than in separate periods of wakefulness and REM sleep. As we will see in section 19.5, other machine learning algorithms draw samples from the model distribution for other purposes, and such algorithms could also provide an account for the function of dream sleep. Given this understanding of the role of the positive and the negative phase of learning, we can attempt to design a less expensive alternative to algorithm 18.1. The main cost of the naive MCMC algorithm is the cost of burning in the Markov chains from a random initialization at each step. A natural solution is to initialize  607  CHAPTER 18',\n",
       " '1aa7f387-51a5-4524-951e-16985a565bce': 'The development of sampling methods, such as Markov chain Monte Carlo (discussed in Chapter 11) along with dramatic improvements in the speed and memory capacity of computers, opened the door to the practical use of Bayesian techniques in an impressive range of problem domains. Monte Carlo methods are very ﬂexible and can be applied to a wide range of models.\\n\\nHowever, they are computationally intensive and have mainly been used for small-scale problems. More recently, highly efﬁcient deterministic approximation schemes such as variational Bayes and expectation propagation (discussed in Chapter 10) have been developed. These offer a complementary alternative to sampling methods and have allowed Bayesian techniques to be used in large-scale applications . We shall devote the whole of Chapter 2 to a study of various probability distributions and their key properties. It is convenient, however, to introduce here one of the most important probability distributions for continuous variables, called the normal or Gaussian distribution. We shall make extensive use of this distribution in the remainder of this chapter and indeed throughout much of the book. For the case of a single real-valued variable x, the Gaussian distribution is deﬁned by which is governed by two parameters: µ, called the mean, and σ2, called the variance',\n",
       " '9de2bcdf-9963-44d7-a45c-6cf7a0bb450d': 'One key benefit to the MCMC-based methods described in this section is that hey provide an estimate of the gradient of log Z, and thus we can essentially decompose the problem into the logp contribution and the log Z contribution.\\n\\nWe can then use any other method to tackle log p(x) and just add our negative phase gradient onto the other method’s gradient. In particular, this means that  612  CHAPTER 18. CONFRONTING THE PARTITION FUNCTION  our positive phase can make use of methods that provide only a lower bound on p. Most of the other methods of dealing with log Z presented in this chapter are incompatible with bound-based positive phase methods. https://www.deeplearningbook.org/contents/partition.html    18.3. Pseudolikelihood  Monte Carlo approximations to the partition function and its gradient directly confront the partition function. Other approaches sidestep the issue, by training the model without computing the partition function. Most of these approaches are based on the observation that it is easy to compute ratios of probabilities in an undirected probabilistic model',\n",
       " '100d35e7-df06-4aeb-9856-277be8cd6379': 'The key advantage is that samples from other sources, e.g., human-written text, can be used, making them more data efﬁcient than on-policy metharXiv:2106.07704v4    22 Oct 2022 Prompt Generation for Controlling Pretrained LMs ods. Previous work has used either importance weighted PG  or Q-learning based algorithms . However, off-policy methods have been considered to be less stable. For example, the Q-learning performance relies heavily on how accurate the learned Q-function assesses the quality of intermediate subsequences – a challenging task due to the sparse reward signals. In this paper, we develop a new RL formulation for text generation that tackles the above issues (Figure 1, left).\\n\\nWe reframe the text generation problem from the soft Q-learning perspective originally developed in robotics . The resulting connection allows us to seamlessly take advantage of the latest successful techniques from the RL literature',\n",
       " 'b7a1a9b5-1548-4569-a791-303c04918014': 'Yang, R. Salakhutdinov, X. Liang, L. Qin, H. Dong, and E. Xing. Deep generative models with learnable knowledge constraints. In NIPS, 2018. Z. Hu, Z. Yang, R. Salakhutdinov, and E. P. Xing. On unifying deep generative models. In ICLR, 2018. E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016. L. Jiang, Z. Zhou, T. Leung, L.-J. Li, and L. Fei-Fei. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In ICML, 2018. A. Katharopoulos and F',\n",
       " '5c6678f0-009e-4e05-aa8d-da85323e3f21': 'conservatively, based on prior experience with similar experiments, to make sure hat the optimal value is likely to be in the selected range. Typically, a grid search involves picking values approximately on a logarithmic scale, e.g., a learning rate aken within the set {0.1,0.01, 1073, 1074, 107°}, or a number of hidden units aken with the set {50, 100, 200, 500, 1000, 2000}. Grid search usually performs best when it is performed repeatedly. For example, suppose that we ran a grid search over a hyperparameter a using values of {—1, 0, 1}. If the best value found is 1, then we underestimated the range in which the best a lies and should shift the grid and run another search with a in, for example, {1,2,3}. If we find that the best value of a is 0, then we may wish to refine our  428  https://www.deeplearningbook.org/contents/guidelines.html    CHAPTER 11',\n",
       " '6f11822a-2dc0-4bea-8ec6-61f1ab242c30': 'Typically these can be debugged by testing each of their guarantees. Some guarantees that some optimization algorithms offer include that the objective function will never increase after one step of the algorithm, that the gradient with respect to some subset of variables will be zero after each step of the algorithm, and that the gradient with respect to all variables will be zero at convergence. Usually due to rounding error, these conditions will not hold exactly in a digital computer, so the debugging test should include some tolerance parameter.\\n\\n11.6 Example: Multi-Digit Number Recognition  https://www.deeplearningbook.org/contents/guidelines.html    To provide an end-to-end description of how to apply our design methodology in practice, we present a brief account of the Street View transcription system, from the point of view of designing the deep learning components. Obviously, many other components of the complete system, such as the Street View cars, the  database infrastructure, and so on, were of paramount importance. From the point of view of the machine learning task, the process began with data collection. The cars collected the raw data, and human operators provided labels. The transcription task was preceded by a significant amount of dataset curation, including using other machine learning techniques to detect the house numbers prior to transcribing them',\n",
       " 'cda893a7-1be6-4115-af40-6e60e1dadd6c': 'When neural networks are applied to regression problems, it is common to use a sum-of-squares error function of the form where we have considered the case of a single output in order to keep the notation simple (the extension to several outputs is straightforward). We can then write the Exercise 5.16 If the network has been trained on the data set, and its outputs yn happen to be very close to the target values tn, then the second term in (5.83) will be small and can be neglected. More generally, however, it may be appropriate to neglect this term by the following argument. Recall from Section 1.5.5 that the optimal function that minimizes a sum-of-squares loss is the conditional average of the target data. The quantity (yn − tn) is then a random variable with zero mean.\\n\\nIf we assume that its value is uncorrelated with the value of the second derivative term on the right-hand side of (5.83), then the whole term will average to zero in the summation over n',\n",
       " '65949621-88be-4403-975b-ca67692d2798': '“AutoAugment: Learning augmentation policies from data.\" arXiv preprint arXiv:1805.09501 . 11] Daniel Ho et al. “Population Based Augmentation: Efficient Learning of Augmentation Policy Schedules.\" ICML 2019. 12] Ekin D. Cubuk & Barret Zoph et al. “RandAugment: Practical automated data augmentation with a reduced search space.\" arXiv preprint arXiv:1909.13719 . 13] Hongyi Zhang et al. “mixup: Beyond Empirical Risk Minimization.\" ICLR 2017. 14] Sangdoo Yun et al. “CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features.\" ICCV 2019. 15] Yannis Kalantidis et al. “Mixing of Contrastive Hard Negatives” NeuriPS 2020. 16] Ashish Jaiswal et al',\n",
       " 'c0c9a3cc-b100-4a45-a2a2-70877150fac1': 'In ll1e E \\'tel\\', we keep the nxI hed and allow the attachment point\\' tn ,Iide up and <I<,wn ll1e nxI \\'\" a, to minimize ll1e e\",,\\'llY, This cau\",. each attachment point (independently) 10 position Itself at the orthogonal pmjeclion of the c~sponding data point onto the nxI. In the M \\'tel\\'. we keep the attachment poiOl\\' fil<ed and then release tile nxI and allow it to m\\'>,\\'e 10 tile minimum energy posilion. 11Ie E and M \\'teps are then repeated until a ,uitable c\"\"vergence cri.eri\"\" is ..,isfled. a. is illuSlrated in Figure 12.12. S<J far in OIlr di\",\"\"ioo of PeA',\n",
       " '51a7dcdd-dd5b-41e9-ae20-f974e64ba668': 'The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The applica- tion of augmentation methods based on GANSs are heavily covered in this survey.\\n\\nIn addition to augmentation techniques, this paper will briefly discuss other character- istics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data. Keywords: Data Augmentation, Big data, Image data, Deep Learning, GANs  Introduction  Deep Learning models have made incredible progress in discriminative tasks. This has been fueled by the advancement of deep network architectures, powerful computation, and access to big data',\n",
       " '1dcb8a0f-4ca8-4e10-aa8a-3ace90285a55': 'The practical applicability of Gibbs sampling depends on the ease with which samples can be drawn from the conditional distributions p(zk|z\\\\k). In the case of probability distributions speciﬁed using graphical models, the conditional distributions for individual nodes depend only on the variables in the corresponding Markov blankets, as illustrated in Figure 11.12. For directed graphs, a wide choice of conditional distributions for the individual nodes conditioned on their parents will lead to conditional distributions for Gibbs sampling that are log concave. The adaptive rejection sampling methods discussed in Section 11.1.3 therefore provide a framework for Monte Carlo sampling from directed graphs with broad applicability.\\n\\nIf the graph is constructed using distributions from the exponential family, and if the parent-child relationships preserve conjugacy, then the full conditional distributions arising in Gibbs sampling will have the same functional form as the original conditional distributions (conditioned on the parents) deﬁning each node, and so standard sampling techniques can be employed. In general, the full conditional distributions will be of a complex form that does not permit the use of standard sampling algorithms',\n",
       " '5ca32ddc-8b43-47d6-b5e1-71116e634fd9': '4.4. Model-Based Experience. A model may also learn from other models of the same or related tasks. For example, one can learn a small-size target model by mimicking the outputs of a larger pretrained model, or an ensemble of multiple models, that is more powerful but often too expensive to deploy. Thus, the large model serves as the experience, or the source of information about the task at hand.\\n\\nBy seeing that the large source model is eﬀectively providing ‘pseudo-labels’ on the observed inputs D = {x∗}, we can readily write down the corresponding experience function, as a variant of the standard supervised data experience function in Equation 4.2: Another way of model-based experience is that the pretrained model directly measures the score of a given conﬁguration (x, y) with its (log-)likelihood: As a concrete example, consider learning a text generation model that aims to generate text y conditioning on a given sentiment label x (either positive or negative)',\n",
       " 'e96f3c55-7917-4b5d-9dbe-494fa5964229': 'LS Aner eet ne tha TAL nn 2k nafs Ate aelee Ta tn  https://www.deeplearningbook.org/contents/generative_models.html    yuavlow 4U.UU LOLS ULL LLC ASS ULLLVLLOLL ULLAL ¥ UVES LOL LELELELICe w UICC.\\n\\niu ib the derivative tule for the logarithm ee ae iia a ation 20.60 gives an unbiased Monte Carlo estimator of the gradient. Anywhere we write p(y) in this section, one could equally write p(y | x). This is because p(y) is parametrized by w, and w contains both @ and a, if x is present. One issue with the simple REINFORCE estimator is that it has a very high variance, so that many samples of y need to be drawn to obtain a good estimator of the gradient, or equivalently, if only one sample is drawn, SGD will converge very slowly and will require a smaller learning rate',\n",
       " '1085af77-6757-4481-afa7-a60b11c31a13': 'First of all, we note that the mean of the posterior distribution given by (2.141) is a compromise between the prior mean µ0 and the maximum likelihood solution µML. If the number of observed data points N = 0, then (2.141) reduces to the prior mean as expected. For N → ∞, the posterior mean is given by the maximum likelihood solution. Similarly, consider the result (2.142) for the variance of the posterior distribution. We see that this is most naturally expressed in terms of the inverse variance, which is called the precision. Furthermore, the precisions are additive, so that the precision of the posterior is given by the precision of the prior plus one contribution of the data precision from each of the observed data points. As we increase the number of observed data points, the precision steadily increases, corresponding to a posterior distribution with steadily decreasing variance',\n",
       " '12878fd0-2a85-4ea7-acd0-6fabda19438d': 'If f is Markov, then St = f(Ht) is a state as we have used the term in this book. Let us henceforth call it a Markov state to distinguish it from states that are summaries of the history but fall short of the Markov property (which we will consider shortly). A Markov state is a good basis for predicting the next observation (17.5) but, more importantly, it is also a good basis for predicting or controlling anything. For example, let a test be any speciﬁc sequence of alternating actions and observations that might occur in the future. For example, a three-step test is denoted ⌧ = a1o1a2o2a3o3.\\n\\nThe probability of this test given a speciﬁc history h is deﬁned as p(⌧|h) .= Pr{Ot+1 =o1, Ot+2 =o2, Ot+3 =o3 | Ht =h, At =a1, At+1 =a2, At+2 =a3}',\n",
       " '179a5bf7-8e9c-4385-ae81-3499f8caa294': 'If we substitute the factorized expression (8.49) for the joint distribution into (8.50), then we can rearrange the order of the summations and the multiplications to allow the required marginal to be evaluated much more efﬁciently. Consider for instance the summation over xN. The potential ψN−1,N(xN−1, xN) is the only one that depends on xN, and so we can perform the summation � ﬁrst to give a function of xN−1. We can then use this to perform the summation over xN−1, which will involve only this new function together with the potential ψN−2,N−1(xN−2, xN−1), because this is the only other place that xN−1 appears.\\n\\nSimilarly, the summation over x1 involves only the potential ψ1,2(x1, x2) and so can be performed separately to give a function of x2, and so on. Because each summation effectively removes a variable from the distribution, this can be viewed as the removal of a node from the graph',\n",
       " '63d57d44-a5c9-4a4f-b6d3-3cedc92802e5': 'Fleuret. Not all samples are created equal: Deep learning with importance sampling. In ICML, 2018. D. P. Kingma and M. Welling. Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114, 2013. S. Kobayashi. Contextual augmentation: Data augmentation by words with paradigmatic relations. In NAACL, 2018.\\n\\nA. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105, 2012. M. P. Kumar, B. Packer, and D. Koller. Self-paced learning for latent variable models. In NeurIPS, pages 1189–1197, 2010. J. Lemley, S. Bazrafkan, and P. Corcoran',\n",
       " '7cd66f48-9f6d-4f9a-beb9-ad494d4c41e7': 'Initialize three matrices, V, HY, and H®) each with m rows set to random values (e.g., from Bernoulli distributions, possibly with marginals matched to the model’s marginals).\\n\\nwhile not converged (learning loop) do Sample a minibatch of m examples from the training data and arrange them as the rows of a design matrix V. Initialize matrices H and H), possibly to the model’s marginals. while not converged (mean field inference loop) do  HY eg (vw) 4 HOW?)\"). H® eo (Ow). end while  Aw) — ivlA®  Aw) — <A TA)  for 1 = 1 to k (Gibbs sampling) do Gibbs block 1:  ~ ~ ~ T Vi, j, Vij sampled from P(Vi; = 1) =o wi) Ht - . p(2) Fy(2) 4) ap(L) qr (2) Vi, j,H;; sampled from P(H;/ = 1) “( (Wi',\n",
       " 'fc1e8b91-e01f-47ed-b889-fd829deda9bb': 'Proceedings of the Nineteenth conference on Uncertainty in Artiﬁcial Intelligence, 583–591. Yang, Z., Hu, Z., Dyer, C., Xing, E. P., & Berg-Kirkpatrick, T. Unsupervised text style transfer using language models as discriminators. Advances in Neural Information Processing Systems, 31. Yu, J., Yang, M.-S., & Lee, E. S. Sample-weighted clustering methods. Computers & mathematics with applications, 62(5), 2200–2208. Zheng, S., Jayasumana, S., Romera-Paredes, B., Vineet, V., Su, Z., Du, D., Huang, C., & Torr, P. H. Conditional random ﬁelds as recurrent neural networks. Proceedings of the IEEE International Conference on Computer Vision, 1529–1537. Zheng, Z., Oh, J., & Singh, S. On learning intrinsic rewards for policy gradient methods. Advances in Neural Information Processing Systems, 31',\n",
       " '9ec758c0-825a-45e8-a59d-7fe49d353527': 'We divide these values of weight decay by the learning rate.\\n\\nTraining from Random Initialization We trained the network from random initialization using the same procedure as for ﬁne-tuning, but for longer, and with an altered hyperparameter grid. We chose hyperparameters from a grid of 7 logarithmically spaced learning rates between 0.001 and 1.0 and 8 logarithmically spaced values of weight decay between 10−5 and 10−1.5. Importantly, our random initialization baselines are trained for 40,000 steps, which is sufﬁciently long to achieve near-maximal accuracy, as demonstrated in Figure 8 of Kornblith et al. On Birdsnap, there are no statistically signiﬁcant differences among methods, and on Food-101, Stanford Cars, and FGVC Aircraft datasets, ﬁne-tuning provides only a small advantage over training from random initialization. However, on the remaining 8 datasets, pretraining has clear advantages. Supervised Baselines We compare against architecturally identical ResNet models trained on ImageNet with standard cross-entropy loss. These models are trained with the same data augmentation as our self-supervised models (crops, strong color augmentation, and blur) and are also trained for 1000 epochs',\n",
       " '4049e2be-8063-4c6e-a984-e4a29dcdf6d8': 'PROBABILITY AND INFORMATION THEORY  3.13 Information Theory  Information theory is a branch of applied mathematics that revolves around quantifying how much information is present in a signal. It was originally invented to study sending messages from discrete alphabets over a noisy channel, such as communication via radio transmission. In this context, information theory tells how to design optimal codes and calculate the expected length of messages sampled from specific probability distributions using various encoding schemes. In the context of machine learning, we can also apply information theory to continuous variables where some of these message length interpretations do not apply. This field is fundamental to many areas of electrical engineering and computer science.\\n\\nIn this textbook, we mostly use a few key ideas from information theory to characterize probability distributions or to quantify similarity between probability distributions. https://www.deeplearningbook.org/contents/prob.html       For more detail on information theory, see Cover and Thomas  or MacKay  The basic intuition behind information theory is that learning that an unlikely event has occurred is more informative than learning that a likely event has occurred. A message saying “the sun rose this morning” is so uninformative as to be unnecessary to send, but a message saying “there was a solar eclipse this morning” is very informative',\n",
       " 'c6b9b8ef-a8e1-4875-b7c4-52c3d756a6e2': 'The e↵ect of the introduction of reward upon the maze performance of rats. University of California Publications in Psychology, 4:113–134. Boakes, R. A., Costa, D. S. J. Temporal contiguity in associative learning: Iinterference Booker, L. B. Intelligent Behavior as an Adaptation to the Task Environment. Ph.D. theBostrom, N. Superintelligence: Paths, Dangers, Strategies. Oxford University Press. Bottou, L., Vapnik, V. Local learning algorithms. Neural Computation, 4(6):888–900. Boyan, J. A. Least-squares temporal di↵erence learning. In Proceedings of the 16th International Conference on Machine Learning , pp. 49–56. Boyan, J. A. Technical update: Least-squares temporal di↵erence learning. Machine Boyan, J. A., Moore, A. W',\n",
       " '0b568d9b-e946-4242-afc1-370002e2ae97': '(b) Supervised SimCSE  (a) Unsupervised SimCSE  A man surfing on the sea. Different dropout masks in two forward passes  There are animals outdoors. label=entailment  The pets are sitting on a couch. ‘label=contradiction  +! There is a man *-label=entailment-~  I The man wears a business suit.\\n\\n| ----label=contradiction -------  1A kid is skateboarding. |  Akid is ona ‘+ \\\\---- label=entailment ----  skateboard. |  | — Positive instance  —~ Negative instance :  J A kit is inside the house. \\\\~--~ label= contradiction -- ~~  They ran experiments on 7 STS (Semantic Text Similarity) datasets and computed cosine similarity between sentence embeddings. They also tried out an optional MLM auxiliary objective loss to help avoid catastrophic forgetting of token-level knowledge. This aux loss was found to help improve performance on transfer tasks, but a consistent drop on the main STS tasks',\n",
       " '12ac1dd2-a897-4aa3-93f6-9fa07582e9fe': 'Shorten and Khoshgoftaar J Big Data  6:60   Table 3 Performance results of the experiment with feature vs. input space extrapolation on MNIST and CIFAR-10   Model MNIST CIFAR-10 Baseline 1.093 + 0.057 30.65 £0.27 Baseline + input space affine transformations 1.477 £ 0.068 -  Baseline + input space extrapolation 1.0104 0.065 -  Baseline + feature space extrapolation 0.950 + 0.036 29.24+40.27  The italic value denote high performance according to the comparative metrics  The use of auto-encoders is especially useful for performing feature space augmen- tations on data. Autoencoders work by having one half of the network, the encoder, map images into low-dimensional vector representations such that the other half of the network, the decoder, can reconstruct these vectors back into the original image. This encoded representation is used for feature space augmentation',\n",
       " '928302a3-4c38-491e-be57-bd8fd6b3e050': 'The\" wake-sleep\" algorithm for unsupervised neural networks. Science, 268, 1158. Houthooft, R., Chen, X., Duan, Y., Schulman, J., De Turck, F., & Abbeel, P. VIME: Variational information maximizing exploration. Proceedings of the 30th International Conference on Neural Information Processing Systems, 1117–1125. Hu, Z., Ma, X., Liu, Z., Hovy, E., & Xing, E. Harnessing deep neural networks with logic rules. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2410–2420. Hu, Z., Tan, B., Salakhutdinov, R., Mitchell, T., & Xing, E. P. Learning data manipulation for augmentation and weighting. Proceedings of the 33rd International Conference on Neural Information Processing Systems, 15764–15775.\\n\\nHu, Z., Wilson, A',\n",
       " '18b7f056-d419-4a77-a61a-513417d1c79f': 'For these reasons, designing a reward signal is a critical part of any application of reinforcement learning. By designing a reward signal we mean designing the part of an agent’s environment that is responsible for computing each scalar reward Rt and sending it to the agent at each time t. In our discussion of terminology at the end of Chapter 14, we said that Rt is more like a signal generated inside an animal’s brain than it is like an object or event in the animal’s external environment. The parts of our brains that generate these signals for us evolved over millions of years to be well suited to the challenges our ancestors had to face in their struggles to propagate their genes to future generations.\\n\\nWe should therefore not think that designing a good reward signal is always an easy thing to do! One challenge is to design a reward signal so that as an agent learns, its behavior approaches, and ideally eventually achieves, what the application’s designer actually desires. This can be easy if the designer’s goal is simple and easy to identify, such as ﬁnding the solution to a well-deﬁned problem or earning a high score in a well-deﬁned game. In cases like these, it is usual to reward the agent according to its success in solving the problem or its success in improving its score',\n",
       " '7ecef9bf-705a-49a6-8ac9-1dee98212573': 'By using the technique of Lagrange multipliers, verify that minimization of the Kullback-Leibler divergence KL(p∥q) with respect to one of the factors qi(Zi), keeping all other factors ﬁxed, leads to the solution (10.17). 10.4 (⋆ ⋆) Suppose that p(x) is some ﬁxed distribution and that we wish to approximate it using a Gaussian distribution q(x) = N(x|µ, Σ). By writing down the form of the KL divergence KL(p∥q) for a Gaussian q(x) and then differentiating, show that minimization of KL(p∥q) with respect to µ and Σ leads to the result that µ is given by the expectation of x under p(x) and that Σ is given by the covariance. 10.5 (⋆ ⋆) www Consider a model in which the set of all hidden stochastic variables, denoted collectively by Z, comprises some latent variables z together with some model parameters θ',\n",
       " '342f05c7-ae19-472f-b2f4-54c897c4d31e': 'By contrast, traditional graphical models usually contain mostly variables that are at least occasionally observed, even if many of the variables are missing at random from some training examples. Traditional models mostly use higher-order terms and structure learning to capture complicated nonlinear interactions between variables. If there are latent variables, they are usually few in number. The way that latent variables are designed also differs in deep learning. The deep learning practitioner typically does not intend for the latent variables to take on any specific semantics ahead of time—the training algorithm is free to invent the concepts it needs to model a particular dataset. The latent variables are usually not very easy for a human to interpret after the fact, though visualization techniques may allow some rough characterization of what they represent',\n",
       " 'efa7e1a5-a588-4e4b-8f92-eacb01b7feed': 'These skip connections make it easier for the gradient to flow from output layers to layers nearer the input. Another key consideration of architecture design is exactly how to connect a pair of layers to each other. In the default neural network layer described by a linear transformation via a matrix W, every input unit is connected to every output unit. Many specialized networks in the chapters ahead have fewer connections, so that each unit in the input layer is connected to only a small subset of units in the output layer. These strategies for decreasing the number of connections reduce the number of parameters and the amount of computation required to evaluate the network but are often highly problem dependent. For example, convolutional  198  CHAPTER 6.\\n\\nDEEP FEEDFORWARD NETWORKS  https://www.deeplearningbook.org/contents/mlp.html    e—e 3, convolutional +— 8, fully connected VV 11, convolutional  Test accuracy (percent)  0.0 0.2 0.4 0.6 0.8 1.0  Number of parameters x 108  Figure 6.7: Effect of number of parameters. Deeper models tend to perform better. This is not merely because the model is larger. This experiment from Goodfellow ef al',\n",
       " '90323231-bcc3-4ffc-8b02-664f8f26a82c': 'Advances in Neural Information Processing Systems 27 (NIPS 2014 ), pp. 3014–3022. Curran Associates, Inc. Marbach, P., Tsitsiklis, J. N. Simulation-based optimization of Markov reward processes. Marbach, P., Tsitsiklis, J. N. Simulation-based optimization of Markov reward processes.\\n\\nIEEE Transactions on Automatic Control, 46(2):191–209. Markram, H., L¨ubke, J., Frotscher, M., Sakmann, B. Regulation of synaptic eﬃcacy by coincidence of postsynaptic APs and EPSPs. Science, 275:213–215. Mart´ınez, J. F., ˙Ipek, E. Dynamic multicore resource management: A machine learning Matsuda, W., Furuta, T., Nakamura, K. C., Hioki, H., Fujiyama, F., Arai, R., Kaneko, T',\n",
       " '3c93a787-c6c8-4302-9b9d-cf1eb1324ada': 'As we have seen already, for the output units, we have provided we are using the canonical link as the output-unit activation function. To evaluate the δ’s for hidden units, we again make use of the chain rule for partial derivatives, where the sum runs over all units k to which unit j sends connections. The arrangement of units and weights is illustrated in Figure 5.7. Note that the units labelled k could include other hidden units and/or output units. In writing down (5.55), we are making use of the fact that variations in aj give rise to variations in the error function only through variations in the variables ak.\\n\\nIf we now substitute the deﬁnition of δ given by (5.51) into (5.55), and make use of (5.48) and (5.49), we obtain the following backpropagation formula which tells us that the value of δ for a particular hidden unit can be obtained by propagating the δ’s backwards from units higher up in the network, as illustrated in Figure 5.7. Note that the summation in (5.56) is taken over the ﬁrst index on wkj (corresponding to backward propagation of information through the network), whereas in the forward propagation equation (5.10) it is taken over the second index',\n",
       " '966030ba-d61e-4c0e-9312-59d573dad74a': 'More generally, a broad set of sophisticated algorithms, such as the policy gradient for reinforcement learning and the generative adversarial learning, can also be easily derived by plugging in speciﬁc designs of the experience function f and divergence D. Table 1 summarizes various speciﬁcations of the SE components that recover a range of existing well-known algorithms from diﬀerent paradigms. As shown in more detail in the subsequent sections, the standard equation (Equation 3.1 and 3.2) oﬀers a uniﬁed and universal paradigm for model training under many scenarios based on many types of experience, potentiating a turnkey implementation and a more generalizable theoretical characterization. The experience function f(t) in the standard equation can be instantiated to encode vastly distinct types of experience.\\n\\nDiﬀerent choices of f(t) result in learning algorithms applied to diﬀerent problems. With particular choices, the standard equation rediscovers a wide array of well-known algorithms. The resulting common treatment of the previously disparate algorithms is appealing as it oﬀers new holistic insights into the commonalities and diﬀerences of those algorithms. Table 1 shows examples of extant algorithms that are recovered by the standard equation. 4.1. Data Instance Experience',\n",
       " '29b9461a-0e34-4b06-be16-f7e1c06f04fc': 'The quantity γ deﬁned by (3.91) therefore measures the effective total number of well determined parameters. We can obtain some insight into the result (3.95) for re-estimating β by comparing it with the corresponding maximum likelihood result given by (3.21). Both of these formulae express the variance (the inverse precision) as an average of the squared differences between the targets and the model predictions.\\n\\nHowever, they differ in that the number of data points N in the denominator of the maximum likelihood result is replaced by N − γ in the Bayesian result. We recall from (1.56) that the maximum likelihood estimate of the variance for a Gaussian distribution over a and that this estimate is biased because the maximum likelihood solution µML for the mean has ﬁtted some of the noise on the data. In effect, this has used up one degree of freedom in the model. The corresponding unbiased estimate is given by (1.59) and takes the form We shall see in Section 10.1.3 that this result can be obtained from a Bayesian treatment in which we marginalize over the unknown mean. The factor of N − 1 in the denominator of the Bayesian result takes account of the fact that one degree of freedom has been used in ﬁtting the mean and removes the bias of maximum likelihood',\n",
       " '7b547039-9761-444e-9298-124a457459e0': 'Consider a functional that is deﬁned by an integral over a function G(y, y′, x) that depends on both y(x) and its derivative y′(x) as well as having a direct dependence on x where the value of y(x) is assumed to be ﬁxed at the boundary of the region of integration (which might be at inﬁnity).\\n\\nIf we now consider variations in the function y(x), we obtain We now have to cast this in the form (D.3). To do so, we integrate the second term by parts and make use of the fact that η(x) must vanish at the boundary of the integral (because y(x) is ﬁxed at the boundary). This gives from which we can read off the functional derivative by comparison with (D.3). Requiring that the functional derivative vanishes then gives which are known as the Euler-Lagrange equations. For example, if then the Euler-Lagrange equations take the form This second order differential equation can be solved for y(x) by making use of the boundary conditions on y(x)',\n",
       " '9427302c-9712-42ec-998b-f63514a18ba9': 'The text and image encoders are jointly trained to maximize the similarity between N correct pairs of (image, text) associations while minimizing the similarity for N(N — 1) incorrect pairs via a symmetric cross entropy loss over the  dense matrix. See the numy-like pseudo code for CLIP in Fig.\\n\\n17',\n",
       " 'a5aa7129-0998-4eaa-9241-33aeb905b83b': 'arge weights may also result in extreme values that cause the activation function  to saturate, causing complete loss of gradient through saturated units. These competing factors determine the ideal initial scale of the weights. The perspectives of regularization and optimization can give very different  298  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  insights into how we should initialize a network. The optimization perspective suggests that the weights should be large enough to propagate information suc- cessfully, but some regularization concerns encourage making them smaller.\\n\\nThe use of an optimization algorithm, such as stochastic gradient descent, that makes small incremental changes to the weights and tends to halt in areas that are nearer to the initial parameters (whether due to getting stuck in a region of low gradient, or due to triggering some early stopping criterion based on overfitting) expresses a prior that the final parameters should be close to the initial parameters. Recall from section 7.8 that gradient descent with early stopping is equivalent to weight decay for some models. In the general case, gradient descent with early stopping is not the same as weight decay, but it does provide a loose analogy for thinking about the effect of initialization',\n",
       " 'ad0bda5d-41ab-40bc-b426-b495b79e3651': 'The most important  (t) i units described in the previous section. Here, however, the self-loop weight (or the  component is the state unit s>’, which has a linear self-loop similar to the leaky associated time constant) is controlled by a forget gate unit fo (for time step t and cell i), which sets this weight to a value between 0 and 1 via a sigmoid unit:  A =o (of +Culal + Owen). aoa)  j j where x) is the current input vector and h® is the current hidden layer vector, containing the outputs of all the LSTM cells, and b*, Uf, Wf are respectively biases, input weights, and recurrent weights for the forget gates.\\n\\nThe LSTM cell internal state is thus updated as follows, but with a conditional self-loop weight  : Pie => FOS) + go b; + S- Uz + ia , (10.41) j j  where 6, U and W respectively denote the biases, input weights, and recurrent weights into the LSTM cell',\n",
       " '187751cc-c6b4-441f-a379-737a0fb9b796': 'Each dataset has the same number of examples as the original dataset, but each dataset is constructed by sampling with replacement from the original dataset. This means that, with high probability, each dataset is missing some of the examples from the  253  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  Original dataset  First resampled dataset First ensemble member  Second resampled dataset  DDO >  Figure 7.5: A cartoon depiction of how bagging works. Suppose we train an 8 detector on the dataset depicted above, containing an 8, a 6 and a 9. Suppose we make two different resampled datasets. The bagging training procedure is to construct each of these datasets by sampling with replacement. The first dataset omits the 9 and repeats the 8. On this dataset, the detector learns that a loop on top of the digit corresponds to an 8. On the second dataset, we repeat the 9 and omit the 6',\n",
       " '4c70035b-d63c-43d9-b753-5f213dfdfb5e': '(You may wish to review how this kind of scaling works, first explained in figure 2.3).\\n\\nAlong the directions where the eigenvalues of H are relatively large, for example, where \\\\; >> a, the effect of regularization is relatively small. Yet components with Ai < @ will be shrunk to have nearly zero magnitude. This effect is illustrated in figure 7.1. 228  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  https://www.deeplearningbook.org/contents/regularization.html    Figure 7.1: An illustration of the effect of L (or weight decay) regularization on the value of the optimal W. The solid ellipses represent contours of equal value of the unregularized objective. The dotted circles represent contours of equal value of theL regularizer. At the point w, these competing objectives reach an equilibrium. In the first dimension, the  eigenvalue of the Hessian of J is small. The objective function does not increase much when moving horizontally away from w*. Because the objective function does not express a strong preference along this direction, the regularizer has a strong effect on this axis',\n",
       " '0fd4af99-41d8-4a55-a738-aa829d7a3713': 'The comparison between MLE and SQL(off) shows that the off-policy component of SQL is better than standard MLE training, as it incorporates reward signals instead of just blindly following the (noisy) data.\\n\\nNext, comparing with the previous steered decoding such as PPLM and GeDi, we can see the prompt-based control trained with RL achieves better trade-off between topic accuracy and language quality. Moreover, once a prompt is produced, we can use the pretrained LM to generate text of desired topics efﬁciently, with the same time cost as standard non-controlled decoding. In comparison, the dedicated steered decoding is often orders-ofmagnitude slower, as shown in Table 2. Standard RL algorithms can sometimes be oversensitive to the randomness in the environment. Recent works have considered maximum-entropy RL extensions, such as the soft Q-learning (SQL) , that maximize the entropy of policy besides the rewards, and demonstrated substantial improvement in robotic and game control . Our work is the ﬁrst to adapt SQL and its advanced variants ) to the challenging text generation problem and show signiﬁcant results on diverse applications. Applying RL for text generation has been discussed in alleviating the exposure bias problem and optimizing task metrics',\n",
       " 'e46d95c3-14d8-49ff-a3e7-ebbebb8e334b': 'Synthesis lectures on artificial intelligence and machine learning 4.1 : 1-103. If you notice mistakes and errors in this post, please don’t hesitate to contact me at  and | would be super happy to correct them right away!\\n\\nes es ee  Policy Gradient Algorithms The Multi-Armed Bandit Problem and Its  Solutions  https://lilianweng.github.io/posts/2018-02-19-rl-overview/',\n",
       " 'efa922d2-2240-4289-a6d9-bdb780dcb7f4': 'Each step involves a series of eye movements to obtain information and to guide reaching and locomotion. Rapid judgments are continually made about how to carry the objects or whether it is better to ferry some of them to the dining table before obtaining others. Each step is guided by goals, such as grasping a spoon or getting to the refrigerator, and is in service of other goals, such as having the spoon to eat with once the cereal is prepared and ultimately obtaining nourishment. Whether he is aware of it or not, Phil is accessing information about the state of his body that determines his nutritional needs, level of hunger, and food preferences.\\n\\nThese examples share features that are so basic that they are easy to overlook. All involve interaction between an active decision-making agent and its environment, within which the agent seeks to achieve a goal despite uncertainty about its environment. The agent’s actions are permitted to a↵ect the future state of the environment (e.g., the next chess position, the level of reservoirs of the reﬁnery, the robot’s next location and the future charge level of its battery), thereby a↵ecting the actions and opportunities available to the agent at later times',\n",
       " 'f3fc690d-9560-43e1-a580-0ab8c90dad21': 'For example, multiplying the input by 0.1 will artificially increase likelihood by a factor of 10. Issues with preprocessing commonly arise when benchmarking generative models on the MNIST dataset, one of the more popular generative modeling benchmarks. MNIST consists of grayscale images. Some models treat MNIST images as points in a real vector space, while others treat them as binary.\\n\\nYet others treat the grayscale values as probabilities for binary samples. It is essential to compare real-valued models only to other real-valued models and binary-valued models only o other binary-valued models. Otherwise the likelihoods measured are not on the same space. For binary-valued models, the log-likelihood can be at most zero, while for real-valued models, it can be arbitrarily high, since it is the measurement of a density. Among binary models, it is important to compare models using exactly he same kind of binarization. For example, we might binarize a gray pixel to 0 or 1 by thresholding at 0.5, or by drawing a random sample whose probability of being is given by the gray pixel intensity',\n",
       " 'edd4c3ad-947a-4637-9496-7bc6e8eeac9d': \"For example, a more complicated_penalty term can be derived by using a mixture of Gaussians, rather than a single Gaussian distribution, as the prior . 5.7 Supervised Learning Algorithms  Recall from section 5.1.3 that supervised learning algorithms are, roughly speaking, learning algorithms that learn to associate some input with some output, given a training set of examples of inputs x and outputs y. In many cases the outputs y may be difficult to collect automatically and must be provided by a human  137  CHAPTER 5. MACHINE LEARNING BASICS  “supervisor,” but the term still applies even when the training set targets were collected automatically. 5.7.1 Probabilistic Supervised Learning  Most supervised learning algorithms in this book are based on estimating a probability distribution p(y | #). We can do this simply by using maximum likelihood estimation to find the best parameter vector @ for a parametric family of distributions p(y | x; 0).\\n\\nWe have already seen that linear regression corresponds to the family Ply | @;0) =N(y;0'@, 1). (5.80)  We can generalize linear regression to the classification scenario by defining a different family of probability distributions\",\n",
       " '46e9c35b-3d28-47f1-acf2-4f473079cb9a': '(4) Also note that we may rewrite z = λy + (1 − λ)x in the form Finally, combining the above we have, Proposition 1 − ln(x) is strictly convex on (0, ∞). x2 > 0 for x ∈ (0, ∞). By Theorem (1), − ln(x) is strictly convex on (0, ∞). Also, by Deﬁnition (2) ln(x) is strictly concave on (0, ∞). The notion of convexity can be extended to apply to n points. This result is known as Jensen’s inequality. Theorem 2 (Jensen’s inequality) Let f be a convex function deﬁned on an interval I. If x1, x2, . , xn ∈ I and λ1, λ2, . , λn ≥ 0 with �n i=1 λi = 1, Proof: For n = 1 this is trivial. The case n = 2 corresponds to the deﬁnition of convexity',\n",
       " '26741a61-2524-4d7a-9a3b-be1f273ba4e2': 'First, it makes use of  528  CHAPTER 15.\\n\\nREPRESENTATION LEARNING  the idea that the choice of initial parameters for a deep neural network can have a significant regularizing effect on the model (and, to a lesser extent, that it can improve optimization). Second, it makes use of the more general idea that learning about the input distribution can help with learning about the mapping from inputs to outputs. Both ideas involve many complicated interactions between several parts of the machine learning algorithm that are not entirely understood. The first idea, that the choice of initial parameters for a deep neural network can have a strong regularizing effect on its performance, is the least understood. At the time that pretraining became popular, it was understood as initializing the model in a location that would cause it to approach one local minimum rather than another. Today, local minima are no longer considered to be a serious problem for neural network optimization. We now know that our standard neural network training procedures usually do not arrive at a critical point of any kind',\n",
       " '6a906160-aae9-42e1-b1e9-1bfc7e7ef4c9': 'The results above suggest a simple way of achieving this, namely by taking the available data and partitioning it into a training set, used to determine the coefﬁcients w, and a separate validation set, also called a hold-out set, used to optimize the model complexity (either M or λ). In many cases, however, this will prove to be too wasteful of valuable training data, and we have to seek more sophisticated approaches. Section 1.3 So far our discussion of polynomial curve ﬁtting has appealed largely to intuition. We now seek a more principled approach to solving problems in pattern recognition by turning to a discussion of probability theory. As well as providing the foundation for nearly all of the subsequent developments in this book, it will also give us some important insights into the concepts we have introduced in the context of polynomial curve ﬁtting and will allow us to extend these to more complex situations. A key concept in the ﬁeld of pattern recognition is that of uncertainty. It arises both through noise on measurements, as well as through the ﬁnite size of data sets.\\n\\nProbability theory provides a consistent framework for the quantiﬁcation and manipulation of uncertainty and forms one of the central foundations for pattern recognition',\n",
       " '2c11a40d-c3fb-4796-a31b-f120c8e20404': 'Figure 14.3 shows the behavior of the TD model (again with the presence representation) in a higher-order conditioning experiment—in this case it is second-order conditioning. In the ﬁrst phase (not shown in the ﬁgure), CSB is trained to predict a US so that its associative strength increases, here to 1.65. In the second phase, CSA is paired with CSB in the absence of the US, in the sequential arrangement shown at the top of the ﬁgure. CSA acquires associative strength even though it is never paired with the US. With continued training, CSA’s associative strength reaches a peak and then decreases because the associative strength of CSB, the secondary reinforcer, decreases so that it loses its ability to provide secondary reinforcement. CSB’s associative can di↵er from ˆv(St,wt), making δt non-zero (a temporal di↵erence). This di↵erence has the same status as Rt+1 in (14.5), implying that as far as learning is concerned there is no di↵erence between a temporal di↵erence and the occurrence of a US',\n",
       " 'c5e730d3-e6fd-48c9-b448-2f7143f3e793': ', xN}. We immediately see that the situation is now much more complex than with a single Gaussian, due to the presence of the summation over k inside the logarithm. As a result, the maximum likelihood solution for the parameters no longer has a closed-form analytical solution. One approach to maximizing the likelihood function is to use iterative numerical optimization techniques . Alternatively we can employ a powerful framework called expectation maximization, which will be discussed at length in Chapter 9. The probability distributions that we have studied so far in this chapter (with the exception of the Gaussian mixture) are speciﬁc examples of a broad class of distributions called the exponential family . Members of the exponential family have many important properties in common, and it is illuminating to discuss these properties in some generality. The exponential family of distributions over x, given parameters η, is deﬁned to be the set of distributions of the form where x may be scalar or vector, and may be discrete or continuous',\n",
       " '4a0db538-ee43-4b4c-8142-ced74a420394': 'As illustrated in Figure 8.10, each iteration traverses the tree by simulating actions guided by statistics associated with the tree’s edges.\\n\\nIn its basic version, when a simulation reaches a leaf node of the search tree, MCTS expands the tree by adding some, or all, of the leaf node’s children to the tree. From the leaf node, or one of its newly added child nodes, a rollout is executed: a simulation that typically proceeds all the way to a terminal state, with actions selected by a rollout policy. When the rollout completes, the statistics associated with the search tree’s edges that were traversed in this iteration are updated by backing up the return produced by the rollout. MCTS continues this process, starting each time at the search tree’s root at the current state, for as many iterations as possible given the time constraints. Then, ﬁnally, an action from the root node (which still represents the current environment state) is selected according to statistics accumulated in the root node’s outgoing edges. This is the action the agent takes. After the environment transitions to its next state, MCTS is executed again with the root node set to represent the new current state',\n",
       " '310e5b5a-00fe-421e-847c-42b08a689964': 'The VAE framework has been extended to maximize not just the traditional vari- ational lower bound, but also the importance-weighted autoencoder  objective:  oO 20)  Pt Lz Ly(@, 9) = Ea), alk) wq(z|e) De aoe (20.79)  This new objective is equivalent to the traditional lower bound £ when k = 1. How- ever, it may also be interpreted as forming an estimate of the true log pPyoqe] (2) using importance sampling of z from proposal distribution q(z | x). The importance- weighted autoencoder objective is also a lower bound on log Pmodei (#) and becomes tighter as & increases. Variational autoencoders have some interesting connections to the MP-DBM and other approaches that involve back-propagation through the approximate inference graph .\\n\\nhttps://www.deeplearningbook.org/contents/generative_models.html    These previous approaches required an inference procedure such as mean field fixed- oint equations to provide the computational graph',\n",
       " '6329971d-7d48-4ad3-b189-1681fd48225e': 'The Keras  library provides an ImageDataGenerator class that greatly facilitates the implementa- tion of geometric augmentations. Buslaev et al. presented another augmentation tool they called Albumentations .\\n\\nThe development of Neural Style Transfer, adversar- ial training, GANs, and meta-learning APIs will help engineers utilize the performance  power of advanced Data Augmentation techniques much faster and more easily. Conclusion  This survey presents a series of Data Augmentation solutions to the problem of overfit- ting in Deep Learning models due to limited data. Deep Learning models rely on big data to avoid overfitting. Artificially inflating datasets using the methods discussed in this survey achieves the benefit of big data in the limited data domain. Data Augmen- tation is a very useful technique for constructing better datasets. Many augmentations have been proposed which can generally be classified as either a data warping or over- sampling technique. The future of Data Augmentation is very bright. The use of search algorithms com- bining data warping and oversampling methods has enormous potential. The layered architecture of deep neural networks presents many opportunities for Data Augmen- tation',\n",
       " '82751659-4f10-4476-acd7-7e29a78727f8': 'Consider the three-state episodic MRP shown to the right.\\n\\nEpisodes begin in state A and then ‘split’ stochastically, half the time going to B (and then invariably going on to terminate with a reward of 1) and half the time going to state C (and then invariably terminating with a reward of zero). Reward for the ﬁrst transition, out of A, is always zero whichever way the episode goes. As this is an episodic problem, we can take γ to be 1. We also assume on-policy training, so that ⇢t is always 1, and tabular function approximation, so that the learning algorithms are free to give arbitrary, independent values to all three states. Thus, this should be an easy problem. What should the values be? From A, half the time the return is 1, and half the value should be 1, and similarly from C the return is always 0, so its value should be 0. These are the true values and, as this is a tabular problem, all the methods presented previously converge to them exactly. However, the naive residual-gradient algorithm ﬁnds di↵erent values for B and 2). These are in fact the values that minimize the TDE',\n",
       " 'ee5dddbc-37fd-448e-8b3b-f139dc3c46f1': 'Each leaf of the tree is then associated with a speciﬁc diagnosis. In order to learn such a model from a training set, we have to determine the structure of the tree, including which input variable is chosen at each node to form the split criterion as well as the value of the threshold parameter θi for the split. We also have to determine the values of the predictive variable within each region. Consider ﬁrst a regression problem in which the goal is to predict a single target variable t from a D-dimensional vector x = (x1, . , xD)T of input variables. The training data consists of input vectors {x1, . , xN} along with the corresponding continuous labels {t1, . , tN}.\\n\\nIf the partitioning of the input space is given, and we minimize the sum-of-squares error function, then the optimal value of the predictive variable within any given region is just given by the average of the values of tn for those data points that fall in that region. Exercise 14.10 Now consider how to determine the structure of the decision tree',\n",
       " '91f5e89c-d1b1-4931-9fbe-c2683be6d636': 'The optimization-based formulation of Bayesian inference in Equation 2.15 oﬀers important additional ﬂexibility in learning by allowing rich constraints on machine learning models to be imposed to regularize the outcome. For example, in Equation 2.15 we have seen the standard normality constraint of a probability distribution being imposed on the posterior q. It is natural to consider other types of constraints that encode richer problem structures and domain knowledge, which can regularize the model to learn desired behaviors.\\n\\nThe idea has led to posterior regularization  or regularized Bayes , which augments the Bayesian inference objective with additional constraints: where we have rearranged the terms and dropped any constant factors in Equation 2.15, and added constraints with ξ being a vector of slack variables, U(ξ) a penalty function (e.g., ℓ1 norm of ξ), and Q(ξ) a subset of valid distributions over θ that satisfy the constraints determined by ξ. The optimization problem is generally easy to solve when the penalty/constraints are convex and deﬁned w.r.t. a linear operator (e.g., expectation) of the posterior q',\n",
       " 'f9ad31c8-93f0-46e3-b35a-f50fbb577428': 'A. Comparing policy-gradient algorithms. Sutton, R. S., Szepesv´ari, Cs., Geramifard, A., Bowling, M., . Dyna-style planning with linear function approximation and prioritized sweeping. In Proceedings of the 24th Conference on Uncertainty in Artiﬁcial Intelligence, pp. 528–536. Szepesv´ari, Cs. Algorithms for reinforcement learning. In Synthesis Lectures on Artiﬁcial Intelligence and Machine Learning, 4(1):1–103. Morgan and Claypool. Szita, I. Reinforcement learning in games. In M. Wiering and M. van Otterlo (Eds. ), Reinforcement Learning: State-of-the-Art, pp. 539–577. Springer-Verlag Berlin Heidelberg. undiscounted average reward. Technical Report 94-30-01. Oregon State University, Computer Science Department, Corvallis',\n",
       " '513ecc14-56ca-4f12-bc64-41d89ba64b00': 'These techniques have been applied to models such as RBMs . Another approach is to use parallel tempering , in which the Markov chain simulates many different states in parallel, at different temperatures.\\n\\nThe highest temperature states mix slowly, while the lowest temperature states, at temperature 1, provide accurate samples from the model. The transition operator includes stochastically swapping states between two different temperature levels, so that a sufficiently high- probability sample from a high-temperature slot can jump into a lower temperature slot. This approach has also been applied to RBMs . Although tempering is a promising approach, at this point it has not allowed researchers to make a strong advance in solving the challenge of sampling from complex EBMs. One possible reason is that there are critical temperatures around which the temperature transition must be very slow (as the temperature is gradually reduced) for tempering to be effective. 17.5.2 Depth May Help Mixing  When drawing samples from a latent variable model p(h, a), we have seen that if p(h | x) encodes x too well, then sampling from p(a | h) will not change « very much, and mixing will be poor',\n",
       " 'b6a63125-2ea7-4485-ae9a-97b84e23f68d': 'Representational regularization is accomplished by the same sorts of mechanisms that we have used in parameter regularization. Norm penalty regularization of representations is performed by adding to the loss function J a norm penalty on the representation. This penalty is denoted Q(h). As before, we denote the regularized loss function by J:  J(0;X,y) = J(O;X,y) + aQ(h), (7.48)  where a € [0,00) weights the relative contribution of the norm penalty term, with larger values of @ corresponding to more regularization. Just as an L! penalty on the parameters induces parameter sparsity, an L! penalty on the elements of the representation induces representational sparsity: QO(h) = ||Al|: = 30; |Ri|. Of course, the L+ penalty is only one choice of penalty that can result in a sparse representation.\\n\\nOthers include the penalty derived from a Student ¢ prior on the representation  and KL divergence penalties , which are especially useful for representations with elements constrained to lie on the unit interval. Lee et al. and Goodfellow et al',\n",
       " '41c626c5-2c42-4d37-968b-df8ae977afd7': 'Uti- lizing evolutionary and random search algorithms is an interesting area of future work, but the meta-learning schemes reviewed in this survey are all neural-network, gradient-based. The history of Deep Learning advancement from feature engineering such as SIFT  and HOG  to architecture design such as AlexNet , VGGNet , and Inception-V3 , suggest that meta-architecture design is the next paradigm shift. NAS takes a novel approach to meta-learning architectures by using a recurrent net- work trained with Reinforcement Learning to design architectures that result in the best accuracy. On the CIFAR-10 dataset, this achieved an error rate of 3.65 (Fig. 28). This section will introduce three experiments using meta-learning for Data Aug- mentation. These methods use a prepended neural network to learn Data Augmenta-  tions via mixing images, Neural Style Transfer, and geometric transformations.\\n\\nNeural augmentation The Neural Style Transfer algorithm requires two parameters for the weights of the style and content loss. Perez and Wang  presented an algo- rithm to meta-learn a Neural Style Transfer strategy called Neural Augmentation. The Neural Augmentation approach takes in two random images from the same class',\n",
       " '2c6122a6-93c2-46e2-a592-5bb665d4c4cb': 'All of these distributions are examples of the exponential family of distributions, which possess a number of important properties, and which will be discussed in some detail. One limitation of the parametric approach is that it assumes a speciﬁc functional form for the distribution, which may turn out to be inappropriate for a particular application. An alternative approach is given by nonparametric density estimation methods in which the form of the distribution typically depends on the size of the data set. Such models still contain parameters, but these control the model complexity rather than the form of the distribution. We end this chapter by considering three nonparametric methods based respectively on histograms, nearest-neighbours, and kernels. We begin by considering a single binary random variable x ∈ {0, 1}. For example, x might describe the outcome of ﬂipping a coin, with x = 1 representing ‘heads’, and x = 0 representing ‘tails’.\\n\\nWe can imagine that this is a damaged coin so that the probability of landing heads is not necessarily the same as that of landing tails',\n",
       " '2f0ad765-b562-4acd-98c7-5af585ed213a': 'This arises because any ﬁnite value for α will always assign a lower probability to the data, thereby decreasing the value of the density at t, provided that β is set to its optimal value.\\n\\nWe see that any ﬁnite value for α would cause the distribution to be elongated in a direction away from the data, thereby increasing the probability mass in regions away from the observed data and hence reducing the value of the density at the target data vector itself. For the more general case of M basis vectors ϕ1, . , ϕM a similar intuition holds, namely that if a particular basis vector is poorly aligned with the data vector t, then it is likely to be pruned from the model. We now investigate the mechanism for sparsity from a more mathematical perspective, for a general case involving M basis functions. To motivate this analysis we ﬁrst note that, in the result (7.87) for re-estimating the parameter αi, the terms on the right-hand side are themselves also functions of αi. These results therefore represent implicit solutions, and iteration would be required even to determine a single αi with all other αj for j ̸= i ﬁxed',\n",
       " '64d78e44-421a-4caf-8c6d-bbc8d2a46a6c': 'Even for a ﬁxed number of nodes in the tree, the problem of determining the optimal structure (including choice of input variable for each split as well as the corresponding thresholds) to minimize the sum-of-squares error is usually computationally infeasible due to the combinatorially large number of possible solutions. Instead, a greedy optimization is generally done by starting with a single root node, corresponding to the whole input space, and then growing the tree by adding nodes one at a time. At each step there will be some number of candidate regions in input space that can be split, corresponding to the addition of a pair of leaf nodes to the existing tree. For each of these, there is a choice of which of the D input variables to split, as well as the value of the threshold. The joint optimization of the choice of region to split, and the choice of input variable and threshold, can be done efﬁciently by exhaustive search noting that, for a given choice of split variable and threshold, the optimal choice of predictive variable is given by the local average of the data, as noted earlier.\\n\\nThis is repeated for all possible choices of variable to be split, and the one that gives the smallest residual sum-of-squares error is retained',\n",
       " 'fe9ef1ef-c6c3-4135-b862-00e18ceb5446': 'We can determine the mean and covariance of the joint distribution recursively as follows.\\n\\nEach variable xi has (conditional on the states of its parents) a Gaussian distribution of the form (8.11) and so where ϵi is a zero mean, unit variance Gaussian random variable satisfying E = 0 and E = Iij, where Iij is the i, j element of the identity matrix. Taking the expectation of (8.14), we have Thus we can ﬁnd the components of E = (E, . , E)T by starting at the lowest numbered node and working recursively through the graph (here we again assume that the nodes are numbered such that each node has a higher number than its parents). Similarly, we can use (8.14) and (8.15) to obtain the i, j element of the covariance matrix for p(x) in the form of a recursion relation and so the covariance can similarly be evaluated recursively starting from the lowest numbered node. Let us consider two extreme cases. First of all, suppose that there are no links in the graph, which therefore comprises D isolated nodes',\n",
       " '170c3304-c769-4fcd-8040-9016ececa305': 'Either of the two SGVB estimators in section 2.3 can be used. We use settings M = 100 and L = 1 in experiments. XM ← Random minibatch of M datapoints (drawn from full dataset) ϵ ← Random samples from noise distribution p(ϵ) g ← ∇θ,φ �LM(θ, φ; XM, ϵ) (Gradients of minibatch estimator (8)) θ, φ ← Update parameters using gradients g (e.g. SGD or Adagrad ) Often, the KL-divergence DKL(qφ(z|x(i))||pθ(z)) of eq. (3) can be integrated analytically (see appendix B), such that only the expected reconstruction error Eqφ(z|x(i)) � log pθ(x(i)|z) � requires estimation by sampling',\n",
       " '3fc94e67-b3ab-48be-960a-a3f766912fce': \"Recall that the eigendecomposition involves analyzing a matrix A to discover a matrix V of eigenvectors and a vector of eigenvalues A such that we can rewrite Aas A=Vdiag(A)V~!. (2.42)  The singular value decomposition is similar, except this time we will write A as a product of three matrices:  A=UDV'. (2.43)  Suppose that A is an m xn matrix. Then U is defined to be an m x m matrix, D to be an m X n matrix, and V to be an n x n matrix. Each of these matrices is defined to have a special structure. The matrices U and V are both defined to be orthogonal matrices. The matrix D is defined to be a diagonal matrix. Note that D is not necessarily square. The elements along the diagonal of D are known as the singular values of the matrix A. The columns of U are known as the left-singular vectors. The columns of V are known as as the right-singular vectors\",\n",
       " 'e8753007-e1ae-4716-bf36-fb00d8411459': 'The dealer hits or sticks according to a ﬁxed strategy without choice: he sticks on any sum of 17 or greater, and hits otherwise. If the dealer goes bust, then the player wins; otherwise, the outcome—win, lose, or draw—is determined by whose ﬁnal sum is closer to 21. Playing blackjack is naturally formulated as an episodic ﬁnite MDP. Each game of blackjack is an episode. Rewards of +1, −1, and 0 are given for winning, losing, and drawing, respectively. All rewards within a game are zero, and we do not discount (γ = 1); therefore these terminal rewards are also the returns. The player’s actions are to hit or to stick. The states depend on the player’s cards and the dealer’s showing card. We assume that cards are dealt from an inﬁnite deck (i.e., with replacement) so that there is no advantage to keeping track of the cards already dealt',\n",
       " 'c0f7f31a-9ac8-4de0-967f-89d1279dd28f': ', M − 1} This of course can be represented as a directed graph in the form of a chain, an example of which is shown in Figure 8.38. We can then specify the Markov chain by giving the probability distribution for the initial variable p(z(0)) together with the conditional probabilities for subsequent variables in the form of transition probabilities Tm(z(m), z(m+1)) ≡ p(z(m+1)|z(m)). A Markov chain is called homogeneous if the transition probabilities are the same for all m. The marginal probability for a particular variable can be expressed in terms of the marginal probability for the previous variable in the chain in the form A distribution is said to be invariant, or stationary, with respect to a Markov chain if each step in the chain leaves that distribution invariant. Thus, for a homogeneous Markov chain with transition probabilities T(z′, z), the distribution p⋆(z) is invariant if p⋆(z) = � Note that a given Markov chain may have more than one invariant distribution. For instance, if the transition probabilities are given by the identity transformation, then any distribution will be invariant',\n",
       " 'd3717640-3281-4ce0-9944-80ecce8edb22': 'Likewise, undirected models can include substructures that no directed model can represent perfectly.\\n\\nSpecifically, a directed graph D cannot capture all the conditional independences implied by an undirected graph U if U contains a loop of length greater than three, unless that loop also contains a chord. A loop is a sequence of variables connected by undirected edges, with the last variable in the sequence connected back to the first variable in the sequence. A chord is a connection between any two nonconsecutive variables in the sequence defining a loop. If has loops of length four or greater and does not have chords for these loops, we must add the chords before we can convert it to a directed model. Adding these chords discards some of the independence information that was encoded in UW. The graph formed by adding chords to UY is known as a chordal, or triangulated, graph, because all the loops can now be described in terms of smaller, triangular loops. To build a directed graph D from the chordal graph, we need to also assign directions to the edges. When doing so, we must not create a directed cycle in D, or the result will not define a valid directed probabilistic model',\n",
       " '1967dcb0-27e2-4563-8580-6c569684edc8': 'But the concept of one person being a space is also very confusing, and can be very difﬁcult to obtain.\\\\n So, politics: the primary referendum is In summary, the outcome will be a referendum on the EU membership for the ﬁrst time of its kind for EU citizens, and the full extent of the beneﬁts of a single market and a ﬂexible single EU state.\" computers: macintoshintoshintoshintosh In summary, it appears that the company and IBM products are currently in need of upgrading the computer. This can be seen in a detailed review of the Macintosh version of Windows Vista and XP.\\n\\nHowever, when looking at the changes made by the HP Macintosh hardware and software versions of space: legal space science and space In summary:\\\\n\\\\n The purpose of this paper is to investigate and test the theory of space space and other objects. This project will support NASA.s efforts to demonstrate these theories, and to propose other relevant new theories.\\\\n This paper provides the following arguments for the religion: space legal religion religion religion In summary, to the author the current discussion is the position of the Church and the community. While we acknowledge that we should not be commenting upon claims such as our recent cases or the other ones that contradict our view, we conclude it is appropriate to include these cases',\n",
       " '7c0c0f42-143c-46f3-a860-ff2cd9dae5a1': 'The specific strides and depths used in this figure are not advisable for real use; they are designed to be very shallow to fit onto the page.\\n\\nReal convolutional networks also often involve significant amounts of branching, unlike the chain structures used here for simplicity. (Left)A convolutional network that processes a fixed image size. After alternating between convolution and pooling for a few layers, the tensor for the convolutional feature map is reshaped to flatten out the spatial dimensions. The rest of the network is an ordinary feedforward network classifier, as described in chapter 6. (Center)A convolutional network that processes a variably sized image but still maintains a fully connected section. This network uses a pooling operation with variably sized pools but a fixed number of pools,  https://www.deeplearningbook.org/contents/convnets.html     im order to provide a nxed-size vector of 5/6 units to the fully connected portion of the network. are designed to use pooling on some channels but not on other channels, in order to get both highly invariant features and features that will not underfit when the translation invariance prior is incorrect',\n",
       " '1acd073b-eaaf-4f41-b28c-9da9a568992d': 'This often happens when a manifold intersects itself. For example, a figure eight is a manifold that has a single dimension in most places but two dimensions at the intersection at the center. Many machine learning problems seem hopeless if we expect the machine learning algorithm to learn functions with interesting variations across all of R”.\\n\\nManifold learning algorithms surmount this obstacle by assuming that most of R” consists of invalid inputs, and that interesting inputs occur only along a collection of manifolds containing a small subset of points, with interesting variations in the output of the learned function occurring only along directions that lie on the manifold, or with interesting variations happening only when we move from one manifold to another. Manifold learning was introduced in the case of continuous-valued data and in the unsupervised learning setting, although this probability concentration idea can be generalized to both discrete data and the supervised learning setting: the key assumption remains that probability mass is highly concentrated. The assumption that the data lies along a low-dimensional manifold may not always be correct or useful',\n",
       " 'fd8544e1-5f79-4795-b629-7a99234f9ca9': 'The Expectation Maximization Algorithm A short tutorial Comments and corrections to: em-tut@seanborman.com Added Figure (1). Corrected typo above Equation (5). Minor corrections. Added hyperlinks. This tutorial discusses the Expectation Maximization (EM) algorithm of Dempster, Laird and Rubin . The approach taken follows that of an unpublished note by Stuart Russel, but ﬂeshes out some of the gory details. In order to ensure that the presentation is reasonably self-contained, some of the results on which the derivation of the algorithm is based are presented prior to the presentation of the main results. The EM algorithm has become a popular tool in statistical estimation problems involving incomplete data, or in problems which can be posed in a similar form, such as mixture estimation . The EM algorithm has also been used in various motion estimation frameworks  and variants of it have been used in multiframe superresolution restoration methods which combine motion estimation along the lines of . Deﬁnition 1 Let f be a real valued function deﬁned on an interval I =',\n",
       " '516e4ca0-8c90-486d-b3e1-4a36fc7eeabd': 'The model transforms samples of latent variables z to samples x or to distributions over samples x using a differentiable function g(z; 99) ), which is typically represented by a neural network.\\n\\nThis model class includes variational autoencoders, which pair the generator net with an inference net; generative adversarial networks, which pair the generator network with a discriminator network; and techniques that train generator networks in isolation. Generator networks are essentially just parametrized computational procedures for generating samples, where the architecture provides the family of possible distributions to sample from and the parameters select a distribution from within that family. https://www.deeplearningbook.org/contents/generative_models.html    _ As an example, the standard procedure for drawing samples from a normal distribution with mean Ht and covariance is to feed samples 2 from a normal distribution with zero mean and identity covariance into a very simple generator  690  CHAPTER 20. DEEP GENERATIVE MODELS  network. This generator network contains just one affine layer: w= 9(z)=n4+ Lz, (20.71)  where L is given by the Cholesky decomposition of &',\n",
       " '11c65a03-46b8-40e9-a065-fe7cfc14cb86': '4.9 (⋆) www Consider a generative classiﬁcation model for K classes deﬁned by prior class probabilities p(Ck) = πk and general class-conditional densities p(φ|Ck) where φ is the input feature vector. Suppose we are given a training data set {φn, tn} where n = 1, . , N, and tn is a binary target vector of length K that uses the 1-ofK coding scheme, so that it has components tnj = Ijk if pattern n is from class Ck. Assuming that the data points are drawn independently from this model, show that the maximum-likelihood solution for the prior probabilities is given by where Nk is the number of data points assigned to class Ck.\\n\\nShow that the maximum likelihood solution for the mean of the Gaussian distribution for class Ck is given by which represents the mean of those feature vectors assigned to class Ck. Similarly, show that the maximum likelihood solution for the shared covariance matrix is given by Thus Σ is given by a weighted average of the covariances of the data associated with each class, in which the weighting coefﬁcients are given by the prior probabilities of the classes',\n",
       " '7536efe1-9599-491b-98f9-99fc46e92785': 'More concretely, we can quantify the asymptotic runtime of various operations using appropriate or inappropriate representations. For example, inserting a number into the correct position in a sorted list of numbers is an O(n) operation if the list is represented as a linked list, but only O(log n) if the list is represented as a red-black tree. In the context of machine learning, what makes one representation better than  https://www.deeplearningbook.org/contents/representation.html    CHAPTER 15. REPRESENTATION LEARNING  another? Generally speaking, a good representation is one that makes a subsequent earning task easier. The choice of representation will usually depend on the choice of the subsequent learning task. We can think of feedforward networks trained by supervised learning as per- orming a kind of representation learning. Specifically, the last layer of the network is typically a linear classifier, such as a softmax regression classifier. The rest of he network learns to provide a representation to this classifier',\n",
       " '750f7401-2109-4f1d-beb2-d8c74c10ae94': 'These issues are discussed further when we present these types of networks in part III. Pooling in convolutional Boltzmann machines is presented in section 20.6. The inverse-like operations on pooling units needed in some differentiable networks are covered in section 20.10.6. Some examples of complete convolutional network architectures for classification using convolution and pooling are shown in figure 9.11. 9.4 Convolution and Pooling as an Infinitely Strong Prior  Recall the concept of a prior probability distribution from section 5.6. This is a probability distribution over the parameters of a model that encodes our beliefs about what models are reasonable, before we have seen any data. Priors can be considered weak or strong depending on how concentrated the probability density in the prior is. A weak prior is a prior distribution with high entropy, such as a Gaussian distribution with high variance. Such a prior allows the data to move the parameters more or less freely.\\n\\nA strong prior has very low entropy, such as a Gaussian distribution with low variance. Such a prior plays a  https://www.deeplearningbook.org/contents/convnets.html   more active role in determining where the parameters end up',\n",
       " '2484f769-fc0c-4bf8-859d-9866b4ec5ab9': 'Calculus and algebra  220  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  have long been used to solve optimization problems in closed form, but gradient descent was not introduced as a technique for iteratively approximating the solution to optimization problems until the nineteenth century .\\n\\nBeginning in the 1940s, these function approximation techniques were used to motivate machine learning models such as the perceptron. However, the earliest models were based on linear models. Critics including Marvin Minsky pointed out several of the flaws of the linear model family, such as its inability to learn the XOR function, which led to a backlash against the entire neural network approach. Learning nonlinear functions required the development of a multilayer per- ceptron and a means of computing the gradient through such a model. Efficient applications of the chain rule based on dynamic programming began to appear in the 1960s and 1970s, mostly for control applications  but also for sensitivity analysis . Werbos  proposed applying these  bachetaeenn ba bententi wn 224A atn) 24-2 aden een ML tan',\n",
       " '46c32030-432e-4b1f-aa85-598fbcc688d4': 'In this case, the ﬁnal hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random. In contrast to denoising auto-encoders , we only predict the masked words rather than reconstructing the entire input.\\n\\nAlthough this allows us to obtain a bidirectional pre-trained model, a downside is that we are creating a mismatch between pre-training and ﬁne-tuning, since the  token does not appear during ﬁne-tuning. To mitigate this, we do not always replace “masked” words with the actual  token. The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the  token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time. Then, Ti will be used to predict the original token with cross entropy loss. We compare variations of this procedure in Appendix C.2',\n",
       " '02ba58cb-dbf5-4799-89bc-a8ef31c9d34a': 'Consider a Gaussian process with a two-dimensional input space x = (x1, x2), having a kernel function of the form Samples from the resulting prior over functions y(x) are shown for two different settings of the precision parameters ηi in Figure 6.9. We see that, as a particular parameter ηi becomes small, the function becomes relatively insensitive to the corresponding input variable xi. By adapting these parameters to a data set using maximum likelihood, it becomes possible to detect input variables that have little effect on the predictive distribution, because the corresponding values of ηi will be small. This can be useful in practice because it allows such inputs to be discarded. ARD is illustrated using a simple synthetic data set having three inputs x1, x2 and x3  in Figure 6.10. The target variable t, is generated by sampling 100 values of x1 from a Gaussian, evaluating the function sin(2πx1), and then adding Gaussian noise.\\n\\nValues of x2 are given by copying the corresponding values of x1 and adding noise, and values of x3 are sampled from an independent Gaussian distribution',\n",
       " 'ed99a9cc-7bc0-4d5b-aa69-9192e5eeeddd': 'This can be seen by noting that in Figure 13.19, the variables z(1) which is head-to-head at node xn and hence they are not d-separated. The exact E step for this model does not correspond to running forward and backward recursions along the M Markov chains independently. This is conﬁrmed by noting that the key conditional independence property (13.5) is not satisﬁed for the individual Markov chains in the factorial HMM model, as is shown using d-separation in Figure 13.20.\\n\\nNow suppose that there are M chains of hidden nodes and for simplicity suppose that all latent variables have the same number K of states. Then one approach would be to note that there are KM combinations of latent variables at a given time step so the conditional independence property (13.5) does not hold for the individual latent chains of the factorial HMM model. As a consequence, there is no efﬁcient exact E step for this model. and so we can transform the model into an equivalent standard HMM having a single chain of latent variables each of which has KM latent states. We can then run the standard forward-backward recursions in the E step',\n",
       " '1a4a039a-dded-4fd1-b8e5-b22d070ee3f1': 'experiment with a unique kernel filter that randomly swaps the pixel values in an 1 xx sliding window. They call this augmentation technique PatchShuffle Regularization. Experimenting across different filter sizes and probabilities of shuffling the pixels at each step, they demonstrate the effectiveness of this by achiev- ing a 5.66% error rate on CIFAR-10 compared to an error rate of 6.33% achieved with- out the use of PatchShuffle Regularization. The hyperparameter settings that achieved this consisted of 2 x 2 filters and a 0.05 probability of swapping. These experiments were done using the ResNet  CNN architecture (Figs. 5, 6). Kernel filters are a relatively unexplored area for Data Augmentation. A disadvantage of this technique is that it is very similar to the internal mechanisms of CNNs. CNNs have parametric kernels that learn the optimal way to represent images layer-by-layer',\n",
       " '680aa22b-0f1a-4323-abdd-283f23e87b18': 'Along with this, it is natural to use lower case for value functions (e.g., v⇡) and restrict capitals to their tabular estimates (e.g., Qt(s, a)). Approximate value functions are deterministic functions of random parameters and are thus also in lower case (e.g., ˆv(s,wt) ⇡ v⇡(s)). Vectors, such as the weight vector wt (formerly ✓t) and the feature vector xt (formerly φt), are bold and written in lowercase even if they are random variables. Uppercase bold is reserved for matrices. In the ﬁrst edition we used special notations, Pa probabilities and expected rewards. One weakness of that notation is that it still did not fully characterize the dynamics of the rewards, giving only their expectations, which is suﬃcient for dynamic programming but not for reinforcement learning.\\n\\nAnother weakness is the excess of subscripts and superscripts. In this edition we use the explicit notation of p(s0, r|s, a) for the joint probability for the next state and reward given the current state and action',\n",
       " 'b339ea9e-ff9b-4960-a193-1f0ed6c5321d': 'W., Atkeson, C. G. Prioritized sweeping: Reinforcement learning with less data and less real time. Machine Learning, 13(1):103–130. Moore, A. W., Schneider, J., Deng, K. Eﬃcient locally weighted polynomial regression predictions. In Proceedings of the 14th International Conference on Machine Learning . Morgan Kaufmann. cerebellar implementation of the sutton-barto-desmond model. In J. H. Byrne and W. O. Berry (Eds. ), Neural Models of Plasticity, pp. 187–207. Academic Press, San Diego, CA. Moore, J. W., Choi, J.-S., Brunzell, D. H. Predictive timing under temporal uncertainty: Collyer (Eds. ), Timing of Behavior, pp.\\n\\n3–34. MIT Press, Cambridge, MA. Moore, J. W., Desmond, J',\n",
       " '546016f6-4080-4fba-8083-fdcbca7fe519': 'Show that as a consequence of this constraint, the elements of the model prediction y(x) given by the least-squares solution (4.17) also satisfy this constraint, so that To do so, assume that one of the basis functions φ0(x) = 1 so that the corresponding parameter w0 plays the role of a bias. 4.3 (⋆ ⋆) Extend the result of Exercise 4.2 to show that if multiple linear constraints are satisﬁed simultaneously by the target vectors, then the same constraints will also be satisﬁed by the least-squares prediction of a linear model. 4.4 (⋆) www Show that maximization of the class separation criterion given by (4.23) with respect to w, using a Lagrange multiplier to enforce the constraint wTw = 1, leads to the result that w ∝ (m2 − m1)',\n",
       " '12276c32-c010-435c-9d4f-223a774fe790': 'This third dimension might be visualized as perpendicular to the plane of the page in Figure 8.11.\\n\\nIn addition to the three dimensions just discussed, we have identiﬁed a number of Deﬁnition of return Is the task episodic or continuing, discounted or undiscounted? Action values vs. state values vs. afterstate values What kind of values should be estimated? If only state values are estimated, then either a model or a separate policy (as in actor–critic methods) is required for action selection. Action selection/exploration How are actions selected to ensure a suitable trade-o↵ between exploration and exploitation? We have considered only the simplest ways to do this: \"-greedy, optimistic initialization of values, soft-max, and upper conﬁdence bound. Synchronous vs. asynchronous Are the updates for all states performed simultaneReal vs. simulated Should one update based on real experience or simulated experiLocation of updates What states or state–action pairs should be updated? Modelfree methods can choose only among the states and state–action pairs actually encountered, but model-based methods can choose arbitrarily. There are many possibilities here',\n",
       " '50a33ab6-a59e-4d2f-83c7-4671dbc11da2': 'REGULARIZATION FOR DEEP LEARNING  https://www.deeplearningbook.org/contents/regularization.html    Figure 7.7: An example of forward propagation through a feedforward network using dropout. (Top)In this example, we use a feedforward network with two input units, one hidden layer with two hidden units, and one output unit.\\n\\n(Bottom)To perform forward propagation with dropout, we randomly sample a vector 4s with one entry for each input or hidden unit in the network. The entries of ys are binary and are sampled independently from each other. The probability of each entry being 1 is a hyperparameter, usually 05 for the hidden layers and 0.8 for the input. Each unit in the network is multiplied by the corresponding mask, and then forward propagation continues through the rest of the network as usual. This is equivalent to randomly selecting one of the sub-networks from figure 7.6 and running forward propagation through it. 258  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  p® (y | x)',\n",
       " '545d9a4f-7986-4b91-a5db-5e44d8ecb7b3': 'Instead of applying an element-wise function g(z), maxout units divide z into groups of k values.\\n\\nEach maxout unit then outputs the maximum element of one of these groups:  g(Z)i = max 2;, (6.37) je) where G® is the set of indices into the inputs for group i, {G@—-1k+1,...,ik}. This provides a way of learning a piecewise linear function that responds to multiple directions in the input 2 space. A maxout unit can learn a piecewise linear, convex function with up to k pieces. Maxout units can thus be seen as learning the activation function itself rather than just the relationship between units. With large enough k, a maxout unit can learn to approximate any convex function with arbitrary fidelity. In particular, a maxout layer with two pieces can learn to implement the same function of the input x as a traditional layer using the rectified linear activation function, the absolute value rectification function, or the leaky or parametric ReLU, or it can learn to implement a totally different function altogether',\n",
       " '1edda494-af99-4501-bdc2-c3e19c47fef3': 'Fe ee ee ee ee ee SS CS SS P a A p> a es eannase | er saan ae .\\n\\nCn A i a i aie ~ Ce ee ee Se Dee ee De ys~N oF FF Se ee ee hee ev py vrnrns n »_s\\\\™  Figure 14.5: Vector field learned by a denoising autoencoder around a 1-D curved manifold near which the data concentrate in a 2-D space. Each arrow is proportional to the reconstruction minus input vector of the autoencoder and points towards higher probability according to the implicitly estimated probability distribution. The vector field has zeros at both maxima of the estimated density function (on the data manifolds) and at minima of that density function. For example, the spiral arm forms a 1-D manifold of local maxima that are connected to each other. Local minima appear near the middle of the gap between two arms',\n",
       " 'ce417a52-fe1b-48b7-8ec0-611288d0fd51': 'This applies not only to classifiers but also to models we will encounter in Part III, such as autoencoders and Boltzmann machines.\\n\\nThese models have layers whose output should resemble the input data a, and it can be very helpful to initialize the biases of such layers to match the marginal distribution over x. Sometimes we may want to choose the bias to avoid causing too much saturation at initialization. For example, we may set the bias of a ReLU hidden unit to 0.1 rather than 0 to avoid saturating the ReLU at initialization. This approach is not compatible with weight initialization schemes that do  301  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  not expect strong input from the biases though. For example, it is not recommended for use with random walk initialization . Sometimes a unit controls whether other units are able to participate in a function. In such situations, we have a unit with output u and another unit h é , and they are multiplied together to produce an output uh. We can view h as a gate that determines whether uh © u or uh = 0',\n",
       " 'a08f6bd0-8fe6-4722-9151-ed8723d69ab2': \"One way to verify that these derivatives are correct is to compare the derivatives computed by your implementation _of automatic differentiation to the derivatives computed by finite differences.\\n\\nBecause  f'(0) = tim L249 — fe) (11.5)  «0 €  we can approximate the derivative by using a small, finite e:  flv+ = fla)  f(x) = : (11.6)  We can improve the accuracy of the approximation by using the centered differ-  ence: f(at+ 36) — f(a@- 36) €  f(x) ®  The perturbation size € must be large enough to ensure that the perturbation is not rounded down too much by finite-precision numerical computations. (11.7)  433  CHAPTER 11. PRACTICAL METHODOLOGY  Usually, we will want to test the gradient or Jacobian of a vector-valued function g:R™ — R”. Unfortunately, finite differencing only allows us to take a single derivative at a time\",\n",
       " 'd8028471-6c78-49c1-b9d4-4e7d9d7a54ff': \"Within one episode, it works as follows:  https://lilianweng.github.io/posts/2018-02-19-rl-overview/  A (Long) Peek into Reinforcement Learning | Lil'Log   Initialize t = 0. Starts with So. At time step t, we pick the action according to Q values, A; = arg MaXye4 Q(S;, a) and e- greedy is commonly applied. After applying action A;, we observe reward R,,; and get into the next state S;, 1. Update the Q-value function:  Q(St, At) — Q(S:, At) + (Rey + y¥Maxacg Q(St41,4) — Q(St, Ar). t =t+ 1 and repeat from step 3.\\n\\nThe key difference from SARSA is that Q-learning does not follow the current policy to pick the second action A,;, 1. It estimates Q* out of the best Q values, but which action (denoted as a*) leads to this maximal Q does not matter and in the next step Q-learning may not follow a*\",\n",
       " 'a4603df8-dee0-4aea-ac9b-8b6edd4e4c12': 'Self-supervised learning: The dark matter of intelligence   Research Share on  Facebook  Self-supervised  learning: The 5 ee dark matter of Twitter intelligence  March 4, 2021 Our Work  OO Meta Al Research Blog Resources About Q  In recent years, the Al field has made tremendous progress in developing Al wescares systems that can learn from massive  amounts of carefully labeled data. This  ; , . Faceboo paradigm of supervised learning has a proven track record for training Al’s... specialist models that perform extremely well on the task they were The award trained to do. Unfortunately, there’s a recognizes  https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/  Self-supervised learning: The dark matter of intelligence  limit to how far the field of Al can go with supervised learning alone. Supervised learning is a bottleneck for building more intelligent generalist models that can do multiple tasks and acquire new skills without massive amounts of labeled data. Practically speaking, it’s impossible to label everything in the world. There are also some tasks for which there’s simply not enough labeled data, such as training translation systems for low-resource languages',\n",
       " 'bf55cf34-36be-436b-b906-650a14571b3a': 'Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26, pages 3111–3119.\\n\\nCurran Associates, Inc. Andriy Mnih and Geoffrey E Hinton. 2009. A scalable hierarchical distributed language model. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 1081–1088. Curran Associates, Inc. Ankur P Parikh, Oscar T¨ackstr¨om, Dipanjan Das, and Jakob Uszkoreit. 2016. A decomposable attention model for natural language inference. In EMNLP. Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP), pages 1532– 1543. Matthew Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power',\n",
       " 'f6b4a901-3588-4da9-b937-1af1ca4a18c7': 'The variance of f(x) is deﬁned by and provides a measure of how much variability there is in f(x) around its mean value E. Expanding out the square, we see that the variance can also be written in terms of the expectations of f(x) and f(x)2 Exercise 1.5 In particular, we can consider the variance of the variable x itself, which is given by For two random variables x and y, the covariance is deﬁned by which expresses the extent to which x and y vary together. If x and y are independent, then their covariance vanishes. Exercise 1.6 In the case of two vectors of random variables x and y, the covariance is a matrix If we consider the covariance of the components of a vector x with each other, then we use a slightly simpler notation cov ≡ cov. So far in this chapter, we have viewed probabilities in terms of the frequencies of random, repeatable events. We shall refer to this as the classical or frequentist interpretation of probability.\\n\\nNow we turn to the more general Bayesian view, in which probabilities provide a quantiﬁcation of uncertainty',\n",
       " 'b9b0da73-7386-4fe3-9b62-10892f0dc0df': 'Adaptation in Natural and Artiﬁcial Systems. University of Michigan Holland, J. H. Adaptation. In R. Rosen and F. M. Snell (Eds. ), Progress in Theoretical Biology, vol. 4, pp. 263–293. Academic Press, New York. Holland, J. H. Escaping brittleness: The possibility of general-purpose learning algorithms prediction of reward during learning. Nature Neuroscience, 1(4):304–309. Houk, J. C., Adams, J. L., Barto, A. G. A model of how the basal ganglia generates and uses neural signals that predict reinforcement. In J. C. Houk, J. L. Davis, and D. G. Beiser (Eds. ), Models of Information Processing in the Basal Ganglia, pp. 249–270. MIT Press, Cambridge, MA',\n",
       " '21f40f87-5e56-49a0-b156-f5cbcd0ae217': \"Mastering the game of go without human knowledge. Nature 550.7676 : 354. David Silver, et al. Mastering the game of Go with deep neural networks and tree search. Nature 529.7587 : 484-489.  https://lilianweng.github.io/posts/2018-02-19-rl-overview/  A (Long) Peek into Reinforcement Learning | Lil'Log    Volodymyr Mnih, et al. Human-level control through deep reinforcement learning. Nature 518.7540 : 529. Ziyu Wang, et al. Dueling network architectures for deep reinforcement learning. ICML. 2016. Reinforcement Learning lectures by David Silver on YouTube. OpenAl Blog: Evolution Strategies as a Scalable Alternative to Reinforcement Learning   Frank Sehnke, et al. Parameter-exploring policy gradients. Neural Networks 23.4 : 551- 559. Csaba Szepesvari. Algorithms for reinforcement learning. 1st Edition\",\n",
       " 'cf044844-3566-461f-ba96-8d633c3d9957': 'At least partly due to this advantage, OpenAI GPT  achieved previously state-of-the-art results on many sentencelevel tasks from the GLUE benchmark .\\n\\nLeft-to-right language modelUnlabeled Sentence A and B Pair  ing and auto-encoder objectives have been used for pre-training such models . There has also been work showing effective transfer from supervised tasks with large datasets, such as natural language inference  and machine translation . Computer vision research has also demonstrated the importance of transfer learning from large pre-trained models, where an effective recipe is to ﬁne-tune models pre-trained with ImageNet . We introduce BERT and its detailed implementation in this section. There are two steps in our framework: pre-training and ﬁne-tuning. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For ﬁnetuning, the BERT model is ﬁrst initialized with the pre-trained parameters, and all of the parameters are ﬁne-tuned using labeled data from the downstream tasks. Each downstream task has separate ﬁne-tuned models, even though they are initialized with the same pre-trained parameters',\n",
       " 'ff9607a1-3fbb-49eb-a40f-54e2fbadecee': 'Monte Carlo ES (Exploring Starts), for estimating ⇡ ⇡ ⇡⇤ Choose S0 2 S, A0 2 A(S0) randomly such that all pairs have probability > 0 Generate an episode from S0, A0, following ⇡: S0, A0, R1, . , ST −1, AT −1, RT G  0 Loop for each step of episode, t = T −1, T −2, . , 0: Exercise 5.4 The pseudocode for Monte Carlo ES is ineﬃcient because, for each state– action pair, it maintains a list of all returns and repeatedly calculates their mean. It would be more eﬃcient to use techniques similar to those explained in Section 2.4 to maintain just the mean and a count (for each state–action pair) and update them incrementally. Describe how the pseudocode would be altered to achieve this. ⇤ In Monte Carlo ES, all the returns for each state–action pair are accumulated and averaged, irrespective of what policy was in force when they were observed. It is easy to see that Monte Carlo ES cannot converge to any suboptimal policy',\n",
       " '36c0a156-6492-4f32-b187-cbdc538225e9': 'Two dimensions may be  plotted directly on the page for visualization, so we can gain an understanding of how the model works by training a model with a 2-D latent code, even if we believe the intrinsic dimensionality of the data manifold is much higher.\\n\\nThe images shown are not examples rom the training set but images x actually generated by the model p(z | z), simply by changing the 2-D “code” z (each image corresponds to a different choice of “code” z on a 2-D uniform grid). (Left) The 2-D map of the Frey faces manifold. One dimension that has been discovered (horizontal) mostly corresponds to a rotation of the face, while the other (vertical) corresponds to the emotional expression. (Right) The 2-D map of the MNIST manifold. 20.10.4 Generative Adversarial Networks  Generative adversarial networks, or GANs , are another generative modeling approach based on differentiable generator networks. Generative adversarial networks are based on a game theoretic scenario in which the generator network must compete against an adversary',\n",
       " '4579fe44-9eec-4083-8289-79cdddda0c4a': 'In some cases, ¢(a) can even be infinite dimensional, which would result in an infinite computational cost for the naive, explicit approach. In many cases, k(x, a’) is a nonlinear, tractable function of a even when ¢(a) is intractable. As an example of an infinite-dimensional feature space with a tractable kernel, we construct a feature mapping ¢() over the nonnegative integers x. Suppose that this mapping returns a vector containing x ones followed by infinitely many zeros. We can write a kernel function k(x, 2) = min(x,«™) that is exactly equivalent to the corresponding infinite-dimensional dot product. The most commonly used kernel is the Gaussian kernel, k(u,v) =N(u— v;0,07D), (5.84)  where NV (a; 4, =) is the standard normal density. This kernel is also known as the radial basis function (RBF) kernel, because its value decreases along lines in v space radiating outward from u',\n",
       " 'e2c9f69a-1d9f-428e-b5a5-16de021813f2': 'The novelty in this chapter is that the approximate value function is represented not as a table but as a parameterized functional form with weight vector w 2 Rd. We will write ˆv(s,w) ⇡ v⇡(s) for the approximate value of state s given weight vector w. For example, ˆv might be a linear function in features of the state, with w the vector of feature weights. More generally, ˆv might be the function computed by a multi-layer artiﬁcial neural network, with w the vector of connection weights in all the layers. By adjusting the weights, any of a wide range of di↵erent functions can be implemented by the network. Or ˆv might be the function computed by a decision tree, where w is all the numbers deﬁning the split points and leaf values of the tree. Typically, the number of weights (the dimensionality of w) is much less than the number of states (d ⌧ |S|), and changing one weight changes the estimated value of many states.\\n\\nConsequently, when a single state is updated, the change generalizes from that state to a↵ect the values of many other states',\n",
       " 'f52d83c4-d4dd-4d6d-9b8a-3d7a8fbd3cfc': 'for examples of such an  application. ‘Respectively available from these web sites: freebase.com, cyc.com/opencyc, wordnet. princeton.edu, wikiba.se >geneontology .org  479  CHAPTER 12. APPLICATIONS  Evaluating the performance of a model on a link prediction task is difficult because we have only a dataset of positive examples (facts that are known to be true). If the model proposes a fact that is not in the dataset, we are unsure whether the model has made a mistake or discovered a new, previously unknown fact.\\n\\nThe metrics are thus somewhat imprecise and are based on testing how the model ranks a held-out set of known true positive facts compared to other facts hat are less likely to be true. A common way to construct interesting examples hat are probably negative (facts that are probably false) is to begin with a true act and create corrupted versions of that fact, for example, by replacing one entity in the relation with a different entity selected at random',\n",
       " '476fdfe7-a08e-452f-8f72-c01324e880ef': 'J. The role of the basal ganglia in habit formation. Nature Young, P. Recursive Estimation and Time-Series Analysis. Springer-Verlag, Berlin. Yu, H. Convergence of least squares temporal di↵erence methods under general conditions. International Conference on Machine Learning 27, pp.\\n\\n1207–1214. Yu, H. Least squares temporal di↵erence methods: An analysis under general conditions. SIAM Journal on Control and Optimization, 50(6):3310–3343. Yu, H. On convergence of emphatic temporal-di↵erence learning. In Proceedings of the 28th Annual Conference on Learning Theory, JMLR W&CP 40. Also ArXiv:1506.02582. Yu, H. Weak convergence properties of constrained emphatic temporal-di↵erence learning Yu, H. On convergence of some gradient-based temporal-di↵erences algorithms for Yu, H., Mahmood, A. R., Sutton, R. S',\n",
       " '5722c097-1f30-4039-8478-05d213643c23': 'Let us now consider the maximum entropy conﬁguration for a continuous variable.\\n\\nIn order for this maximum to be well deﬁned, it will be necessary to constrain the ﬁrst and second moments of p(x) as well as preserving the normalization constraint. We therefore maximize the differential entropy with the dynamics, which states that the entropy of a closed system tends to increase with time. By contrast, at the microscopic level the classical Newtonian equations of physics are reversible, and so they found it difﬁcult to see how the latter could explain the former. They didn’t fully appreciate Boltzmann’s arguments, which were statistical in nature and which concluded not that entropy could never decrease over time but simply that with overwhelming probability it would generally increase. Boltzmann even had a longrunning dispute with the editor of the leading German physics journal who refused to let him refer to atoms and molecules as anything other than convenient theoretical constructs. The continued attacks on his work lead to bouts of depression, and eventually he committed suicide. Shortly after Boltzmann’s death, new experiments by Perrin on colloidal suspensions veriﬁed his theories and conﬁrmed the value of the Boltzmann constant',\n",
       " '32cda916-93fc-44fa-86f8-c0b74c453f8c': 'The counterfeiter versus robber analogy is a seamless bridge to understand GANs in the context of network intrusion detection. Lin et al. use a generator network to learn how to fool a black-box detection system. This highlights one of the most interesting characteristics of GANs. Analysis tools derived from game theory such as minimax strategy and the Nash Equilibrium  suggest that the generator will even- tually fool the discriminator. The success of the generator to overcome the discrimi- nator makes it very powerful for generative modeling. GANs are the most promising  generative modeling technique for use in Data Augmentation. Shorten and Khoshgoftaar J Big Data  6:60   The vanilla GAN architecture uses multilayer perceptron networks in the gen- erator and discriminator networks. This is able to produce acceptable images on a simple image dataset such as the MNIST handwritten digits. However, it fails to produce quality results for higher resolution, more complicated datasets. In the MNIST dataset, each image is only 28 x 28 x 1 for a total of 784 pixels',\n",
       " 'fa992b52-57c3-4db8-a0bb-a96d79857992': 'Intrinsic reward. Rewards provided by the extrinsic environment can be sparse in many real-world sequential decision problems. Learning in such problems is thus diﬃcult due to the lack of supervision signals.\\n\\nA method to alleviate the diﬃculty is to supplement the extrinsic reward It is straightforward to derive the intrinsically motivated variant of the policy gradient algorithm (and other RL algorithms discussed below), by replacing the standard extrinsic-only Qθ(x, y) in the experience function Equation 4.16 with the combined Qθ(x, y)+Qin,θ(x, y). Let f θ reward,ex+in(x, y) denote the resulting experience function that incorporates both the extrinsic and the additive intrinsic rewards. We can notice some sort of symmetry between f θ reward,ex+in(x, y) and the actively supervised data experience factive in Equation 4.10, which augments the standard supervised data experience with the additive informativeness measure u(x). The resemblance could naturally inspire mutual exchange between the research areas of intrinsic reward and active learning, for example, using the active learning informativeness measure as the intrinsic reward rin, as was studied in earlier work',\n",
       " '476fa4e5-296e-4411-906f-251e6efcddc8': 'If any of the diagonal elements are negative, then the corresponding diagonal element of I − ↵A will be greater than one, and the corresponding component of wt will be ampliﬁed, which will lead to divergence if continued. On the other hand, if the diagonal elements of A are all positive, then ↵ can be chosen smaller than one over the largest of them, such that I − ↵A is diagonal with all diagonal elements between 0 and 1. In this case the ﬁrst term of the update tends to shrink wt, and stability is assured. In general, wt will be reduced toward zero whenever A is positive deﬁnite, meaning y>Ay > 0 for any real vector y 6= 0',\n",
       " '5a4b2cbd-8f56-4859-a668-737687320070': 'The top plots compare a majority vote of all labeling functions, Snorkel’s generative model, and Snorkel’s discriminative model.\\n\\nThey show that the generative model improves over majority vote by providing more granular information about candidates, and that the discriminative model can generalize to candidates that no labeling functions label. The bottom plots compare the discriminative model trained on an unweighted combination of the labeling functions, hand supervision (when available), and Snorkel’s discriminative model. They show that the discriminative model beneﬁts from the weighted labels provided by the generative model, and that Snorkel is competitive with hand supervision, particularly in the high-precision region sets were labeled by individuals not involved with labeling function development to keep the test sets properly blinded. We ﬁrst focus on four relation extraction tasks on text data, as it is a challenging and common class of problems that are well studied and for which distant supervision is often considered. Predictive performance is summarized in Table 3, and precision–recall curves are shown in Fig. 10. We brieﬂy describe each task',\n",
       " '3801ca88-3e6b-4ffa-a75a-d4d5a3d2cc73': 'The only places where this cosine was negative was in these situations of instability. We therefore switched to RMSProp  which is known to perform well even on very nonstationary problems . One of the beneﬁts of WGAN is that it allows us to train the critic till optimality. When the critic is trained to completion, it simply provides a loss to the generator that we can train as any other neural network. This tells us that we no longer need to balance generator and discriminator’s capacity properly. The better the critic, the higher quality the gradients we use to train the generator. We observe that WGANs are much more robust than GANs when one varies the architectural choices for the generator. We illustrate this by running experiments on three generator architectures: (1) a convolutional DCGAN generator, (2) a convolutional DCGAN generator without batch normalization and with a constant number of ﬁlters, and (3) a 4-layer ReLU-MLP with 512 hidden units.\\n\\nThe last two are known to perform very poorly with GANs. We keep the convolutional DCGAN architecture for the WGAN critic or the GAN discriminator',\n",
       " 'f09ab72a-eb46-4a7b-9444-bb535e1e1946': 'Let us consider for a moment the problem of approximating a general distribution by a factorized distribution. To begin with, we discuss the problem of approximating a Gaussian distribution using a factorized Gaussian, which will provide useful insight into the types of inaccuracy introduced in using factorized approximations. Consider a Gaussian distribution p(z) = N(z|µ, Λ−1) over two correlated variables z = (z1, z2) in which the mean and precision have elements and Λ21 = Λ12 due to the symmetry of the precision matrix. Now suppose we wish to approximate this distribution using a factorized Gaussian of the form q(z) = q1(z1)q2(z2). We ﬁrst apply the general result (10.9) to ﬁnd an expression for the optimal factor q⋆ 1(z1)',\n",
       " '33dbf7ed-1bc9-4feb-ba5c-39fd7bd1993a': '8.24 (⋆ ⋆) Show that the marginal distribution for the variables xs in a factor fs(xs) in a tree-structured factor graph, after running the sum-product message passing algorithm, can be written as the product of the message arriving at the factor node along all its links, times the local factor f(xs), in the form (8.72). 8.25 (⋆ ⋆) In (8.86), we veriﬁed that the sum-product algorithm run on the graph in Figure 8.51 with node x3 designated as the root node gives the correct marginal for x2. Show that the correct marginals are obtained also for x1 and x3. Similarly, show that the use of the result (8.72) after running the sum-product algorithm on this graph gives the correct joint distribution for x1, x2.\\n\\n8.26 (⋆) Consider a tree-structured factor graph over discrete variables, and suppose we wish to evaluate the joint distribution p(xa, xb) associated with two variables xa and xb that do not belong to a common factor. Deﬁne a procedure for using the sumproduct algorithm to evaluate this joint distribution in which one of the variables is successively clamped to each of its allowed values',\n",
       " 'c7b90a43-ab22-49b1-9369-14f5e767ab60': 'If the tree is sufficiently balanced, the maximum depth (number of binary decisions) is on the order of the logarithm of the number of words |V|: the choice of one out of |V| words can be obtained by doing O(log |V|) operations (one for each of the nodes on the path from the root).\\n\\nIn this example, computing the probability of a wordy can be done by multiplying three probabilities, associated with the binary decisions to move left or right at each node on the path from the root to a nodey. Let b;(y) be the +th binary decision when traversing the tree toward the value y. The probability of sampling an output y decomposes into a product of conditional probabilities, using the chain rule for conditional probabilities, with each node indexed by the prefix of these bits',\n",
       " 'c791bfe8-f6b0-43c4-ba1f-b56febf57b76': 'Now we advance the horizon to step 3 and repeat, going all the way back to produce three new targets, redoing all updates starting from the original w0 to produce w3, and so on.\\n\\nEach time the horizon is advanced, all the updates are redone starting from w0 using the weight vector from the preceding horizon. This conceptual algorithm involves multiple passes over the episode, one at each horizon, each generating a di↵erent sequence of weight vectors. To describe it clearly we have to distinguish between the weight vectors computed at the di↵erent horizons. Let us use wh t to denote the weights used to generate the value at time t in the sequence up to 0 in each sequence is that inherited from the previous episode (so they are the same for all h), and the last weight vector wh deﬁnes the ultimate weight-vector sequence of the algorithm. At the ﬁnal horizon h = T we obtain the ﬁnal weights wT T which will be passed on to form the initial weights of the next episode',\n",
       " '73973eb0-ac99-480c-afa7-de8a23db4b10': 'Hence prove the result (2.124).\\n\\n2.36 (⋆ ⋆) www Using an analogous procedure to that used to obtain (2.126), derive an expression for the sequential estimation of the variance of a univariate Gaussian distribution, by starting with the maximum likelihood expression Verify that substituting the expression for a Gaussian distribution into the RobbinsMonro sequential estimation formula (2.135) gives a result of the same form, and hence obtain an expression for the corresponding coefﬁcients aN. 2.37 (⋆ ⋆) Using an analogous procedure to that used to obtain (2.126), derive an expression for the sequential estimation of the covariance of a multivariate Gaussian distribution, by starting with the maximum likelihood expression (2.122). Verify that substituting the expression for a Gaussian distribution into the Robbins-Monro sequential estimation formula (2.135) gives a result of the same form, and hence obtain an expression for the corresponding coefﬁcients aN. 2.38 (⋆) Use the technique of completing the square for the quadratic form in the exponent to derive the results (2.141) and (2.142)',\n",
       " '2c814a92-1240-44f0-9644-218b538454fe': 'For the quadratic model, the training error increases as the size of the training set increases. This is because larger datasets are harder to fit. Simultaneously, the test error decreases, because fewer incorrect hypotheses are consistent with the training data. The quadratic model does not have enough capacity to solve the task, so its test error asymptotes to a high value. The test error at optimal capacity asymptotes to the Bayes error.\\n\\nThe training error can fall below the Bayes error, due to the ability of the training algorithm to memorize specific instances of the training set. As the training size increases to infinity, the training error of any fixed-capacity model (here, the quadratic model) must rise to at least the Bayes error. (Bottom)As the training set size increases, the optimal capacity (shown here as the degree of the optimal polynomial regressor) increases. The optimal capacity plateaus after reaching sufficient complexity to solve the task. 115  CHAPTER 5. MACHINE LEARNING BASICS  same error rate when classifying previously unobserved points',\n",
       " 'b29eb380-acae-4cf3-91db-bf206a69cea3': 'Furthermore suppose, as is often the case, that we are easily able to evaluate p(z) for any given value of z, up to some normalizing constant Z, so that where �p(z) can readily be evaluated, but Zp is unknown. In order to apply rejection sampling, we need some simpler distribution q(z), sometimes called a proposal distribution, from which we can readily draw samples. We next introduce a constant k whose value is chosen such that kq(z) ⩾ �p(z) for all values of z. The function kq(z) is called the comparison function and is illustrated for a univariate distribution in Figure 11.4. Each step of the rejection sampler involves generating two random numbers. First, we generate a number z0 from the distribution q(z). Next, we generate a number u0 from the uniform distribution over . This pair of random numbers has uniform distribution under the curve of the function kq(z). Finally, if u0 > �p(z0) then the sample is rejected, otherwise u0 is retained',\n",
       " '951ee0e0-c668-45ad-8dee-047d5cc29040': 'This is easily done by using the initial condition (8.71) and noting that α(z1) is given by h(z1) = p(z1)p(x1|z1) which is identical to (13.37).\\n\\nBecause the initial α is the same, and because they are iteratively computed using the same equation, all subsequent α quantities must be the same. Next we consider the messages that are propagated from the root node back to where, as before, we have eliminated the messages of the type z → f since the variable nodes perform no computation. Using the deﬁnition (13.46) to substitute for fn+1(zn, zn+1), and deﬁning we obtain the beta recursion given by (13.38). Again, we can verify that the beta variables themselves are equivalent by noting that (8.70) implies that the initial message send by the root variable node is µzN→fN (zN) = 1, which is identical to the initialization of β(zN) given in Section 13.2.2. The sum-product algorithm also speciﬁes how to evaluate the marginals once all the messages have been evaluated',\n",
       " 'bc52c5fe-803a-4291-96e2-a9208f119f0f': \"The weight is generated by a kernel function kg, measuring the similarity between two data samples. https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Log   Po(y|x, S) = > ko(X, X;) Yi  (xi,yi)ES  To learn a good kernel is crucial to the success of a metric-based meta-learning model. Metric learning is well aligned with this intention, as it aims to learn a metric or distance function over objects. The notion of a good metric is problem-dependent. It should represent the relationship between inputs in the task space and facilitate problem solving. All the models introduced below learn embedding vectors of input data explicitly and use them to design proper kernel functions.\\n\\nConvolutional Siamese Neural Network  The Siamese Neural Network is composed of two twin networks and their outputs are jointly trained on top with a function to learn the relationship between pairs of input data samples. The twin networks are identical, sharing the same weights and network parameters\",\n",
       " 'c74ca66a-19f4-4919-bebf-bdcce84ff77f': 'It’s easy to see that since U was open, Uθ is as well. Furthermore, by assumption 1, we can deﬁne L(θ) = Ez and achieve |W(Pr, Pθ) − W(Pr, Pθ′)| ≤ W(Pθ, Pθ′) ≤ L(θ)∥θ − θ′∥ for all θ′ ∈ Uθ, meaning that W(Pr, Pθ) is locally Lipschitz.\\n\\nThis obviously implies that W(Pr, Pθ) is everywhere continuous, and by Radamacher’s theorem we know it has to be diﬀerentiable almost everywhere. The counterexample for item 3 of the Theorem is indeed Example 1. Proof of Corollary 1. We begin with the case of smooth nonlinearities. Since g is C1 as a function of (θ, z) then for any ﬁxed (θ, z) we have L(θ, Z) ≤ ∥∇θ,xgθ(z)∥+ϵ is an acceptable local Lipschitz constant for all ϵ > 0. Therefore, it suﬃces to prove 2P (note that Pm depends on n)',\n",
       " '2768f6c5-db24-4551-87a3-bfa2c88fe991': 'This is a valid kernel function because it can be shown to correspond to an inner product in a feature space.\\n\\nExercise 6.12 One powerful approach to the construction of kernels starts from a probabilistic generative model , which allows us to apply generative models in a discriminative setting. Generative models can deal naturally with missing data and in the case of hidden Markov models can handle sequences of varying length. By contrast, discriminative models generally give better performance on discriminative tasks than generative models. It is therefore of some interest to combine these two approaches . One way to combine them is to use a generative model to deﬁne a kernel, and then use this kernel in a discriminative approach. Given a generative model p(x) we can deﬁne a kernel by This is clearly a valid kernel function because we can interpret it as an inner product in the one-dimensional feature space deﬁned by the mapping p(x). It says that two inputs x and x′ are similar if they both have high probabilities',\n",
       " '9086dd11-0d5e-40f1-82e6-26831dda7097': 'Special issue on applications of neural networks. Bishop, C. M. Neural Networks for Pattern Recognition. Oxford University Press. Bishop, C. M. Training with noise is equivalent to Tikhonov regularization. Neural Computation 7(1), 108–116. Bishop, C. M. Bayesian PCA. In M. S. Kearns, S. A. Solla, and D. A. Cohn (Eds. ), Advances in Neural Information Processing Systems, Volume 11, pp. 382–388. MIT Press. Bishop, C. M. and G. D. James . Analysis of multiphase ﬂows using dual-energy gamma densitometry and neural networks. Nuclear Instruments and Methods in Physics Research A327, 580–593. Bishop, C. M. and I. T. Nabney',\n",
       " '5a7598c9-6c9f-4bed-9e57-9899360311e9': 'a  https://www.deeplearningbook.org/contents/representation.html    V\\\\Y | X) Wil be strongly tied, and unsupervised representation learning that tries to disentangle the underlying factors of variation is likely to be useful as a semi-supervised learning strategy. Consider the assumption that y is one of the causal factors of x, and let h represent all those factors. The true generative process can be conceived as  structured according to this directed graphical model, with h as the parent of x:  p(h, x) = p(x | h)p(hh). (15.1) As a consequence, the data has marginal probability p(x) = Enp(# | h).\\n\\n(15.2)  From this straightforward observation, we conclude that the best possible model of x (from a generalization point of view) is the one that uncovers the above “true”  540  CHAPTER 15. REPRESENTATION LEARNING  structure, with h as a latent variable that explains the observed variations in 2. The “ideal” representation learning discussed above should thus recover these latent factors',\n",
       " '6f081070-435d-47d5-864b-6d8118b7abdc': '9.6 Automatic methods for adapting the step-size parameter include RMSprop , Adam , stochastic meta-descent methods such as Delta-Bar-Delta , its incremental generalization , and nonlinear generalizations . Methods explicitly designed for reinforcement learning include AlphaBound , SID and NOSID , TIDBD (Kearney et al., in preparation) and the application of stochastic meta-descent to policy gradient learning . 9.6 The introduction of the threshold logic unit as an abstract model neuron by McCulloch and Pitts  was the beginning of ANNs. The history of ANNs as learning methods for classiﬁcation or regression has passed through several stages: roughly, the Perceptron  and ADALINE (ADAptive LINear Element)  stage of learning by single-layer ANNs, the error-backpropagation stage  of learning by multi-layer ANNs, and the current deep-learning stage with its emphasis on representation learning . Examples of the many books on ANNs are Haykin , Bishop , and Ripley .\\n\\nANNs as function approximation for reinforcement learning goes back to the early work of Farley and Clark , who used reinforcement-like learning to modify the weights of linear threshold functions representing policies',\n",
       " '994e885c-e7da-4570-b296-86a3008f5cf1': 'Hastings, W. K. .\\n\\nMonte Carlo sampling methods using Markov chains and their applications. Biometrika 57, 97–109. Hathaway, R. J. Another interpretation of the EM algorithm for mixture distributions. Statistics and Probability Letters 4, 53–56. Haussler, D. Convolution kernels on discrete structures. Technical Report UCSC-CRL-99-10, University of California, Santa Cruz, Computer Science Department. Henrion, M. Propagation of uncertainty by logic sampling in Bayes’ networks. In J. F. Lemmer and L. N. Kanal (Eds. ), Uncertainty in Artiﬁcial Intelligence, Volume 2, pp. 149–164. North Holland. Hinton, G. E., P. Dayan, and M. Revow . Modelling the manifolds of images of handwritten digits. IEEE Transactions on Neural Networks 8(1), 65–74',\n",
       " '2ffa08aa-3a80-4217-8b90-53a835fca015': 'For example, it learned to play certain opening positions di↵erently than was the convention among the best human players. Based on TD-Gammon’s success and further analysis, the best human players now play these positions as TD-Gammon does . The impact on human play was greatly accelerated when several other self-teaching ANN backgammon programs inspired by TD-Gammon, such as Jellyﬁsh, Snowie, and GNUBackgammon, became widely available. These programs enabled wide dissemination of new knowledge generated by the ANNs, resulting in great improvements in the overall caliber of human tournament play . An important precursor to Tesauro’s TD-Gammon was the seminal work of Arthur Samuel  in constructing programs for learning to play checkers. Samuel was one of the ﬁrst to make e↵ective use of heuristic search methods and of what we would now call temporal-di↵erence learning. His checkers players are instructive case studies in addition to being of historical interest.\\n\\nWe emphasize the relationship of Samuel’s methods to modern reinforcement learning methods and try to convey some of Samuel’s motivation for using them. Samuel ﬁrst wrote a checkers-playing program for the IBM 701 in 1952',\n",
       " 'dd7d3d67-74b6-4761-9250-d599896246f9': 'This can be understood as a memory containing a sequence of facts, which can be retrieved later, not necessarily in the same order, without having to visit all of them. 3. A process that exploits the content of the memory to sequentially perform a task, at each time step having the ability put attention on the content of one memory element (or a few, with a different weight). The third component generates the translated sentence. When words in a sentence written in one language are aligned with correspond- ing words in a translated sentence in another language, it becomes possible to relate the corresponding word embeddings. Earlier work showed that one could learn a kind of translation matrix relating the word embeddings in one language with the word embeddings in another , yielding lower alignment error  470  CHAPTER 12. APPLICATIONS  https://www.deeplearningbook.org/contents/applications.html    ‘( ) ‘( ) ‘C )  Figure 12.6: A modern attention mechanism, as introduced by Bahdanau et al. , is essentially a weighted average',\n",
       " '6ed1c04b-3711-46a6-b489-b9042beee853': 'The  prior distribution over the underlying factors, p(h), must be fixed ahead of time by the user. The model then deterministically generates 2 = Wh. We can perform a  \"See section 3.8 for a discussion of the difference between uncorrelated variables and indepen- dent variables. 487  CHAPTER 13. LINEAR FACTOR MODELS  nonlinear change of variables (using equation 3.47) to determine p(a).\\n\\nLearning the model then proceeds as usual, using maximum likelihood. The motivation for this approach is that by choosing p(h) to be independent, we can recover underlying factors that are as close as possible to independent. This is commonly used, not to capture high-level abstract causal factors, but to recover low-level signals that have been mixed together. In this setting, each training example is one moment in time, each 2; is one sensor’s observation of the mixed signals, and each h; is one estimate of one of the original signals. For example, we might have n people speaking simultaneously',\n",
       " 'cc0ba47d-c934-4b1c-8cf6-9b5cb2fcae3b': \"Reptile vs FOMAML  To demonstrate the deeper connection between Reptile and MAML, let's expand the update formula with an example performing two gradient steps, k=2 in SGD(. ). Same as defined above, £ and LM are losses using different mini-batches of data. For ease of reading, we adopt two simplified annotations: g\\\\” = V_£L“(0;) and HY = V2L(6,). A — Omneta 6, = 0) — aVeL (8) = Oy — ag”  9) = 0; —aVeL (0:) = % — ag — ag”  According to the early section, the gradient of FOMAML is the last inner gradient update result\",\n",
       " 'd8818837-9d94-453a-b774-b73c7ec5c7e3': 'In particular, the heuristic estimate of Q ignores interactions between hidden units within the same layer as well as the top-down feedback influence of hidden units in deeper layers on hidden units that are closer to the input. Because the heuristic MLP-based inference procedure in the DBN is not able to account for these interactions, the resulting Q is presumably  662  https://www.deeplearningbook.org/contents/generative_models.html    CHAPTER 20 DEEP GENERATIVE MODELS  far from optimal.\\n\\nIn DBMs, all the hidden units within a layer are conditionally independent given the other layers. This lack of intralayer interaction makes it possible to use fixed-point equations to optimize the variational lower bound and find the true optimal mean field expectations (to within some numerical tolerance). The use of proper mean field allows the approximate inference procedure for DBMs to capture the influence of top-down feedback interactions. This makes DBMs interesting from the point of view of neuroscience, because the human brain is known to use many top-down feedback connections. Because of this property, DBMs have been used as computational models of real neuroscientific phenomena . One unfortunate property of DBMs is that sampling from them is relatively difficult',\n",
       " '077cbbec-bb19-409e-990c-a783f6300496': 'In practice we often construct the transition probabilities from a set of ‘base’ transitions B1, . , BK. This can be achieved through a mixture distribution of the form for some set of mixing coefﬁcients α1, . , αK satisfying αk ⩾ 0 and � If a distribution is invariant with respect to each of the base transitions, then obviously it will also be invariant with respect to either of the T(z′, z) given by (11.42) or (11.43).\\n\\nFor the case of the mixture (11.42), if each of the base transitions satisﬁes detailed balance, then the mixture transition T will also satisfy detailed balance. This does not hold for the transition probability constructed using (11.43), although by symmetrizing the order of application of the base transitions, in the form B1, B2, . , BK, BK, . , B2, B1, detailed balance can be restored. A common example of the use of composite transition probabilities is where each base transition changes only a subset of the variables. Earlier we introduced the basic Metropolis algorithm, without actually demonstrating that it samples from the required distribution',\n",
       " '6d03bb5c-4772-417b-9c48-3abde46de45e': 'Exact inference in this model is intractable, but variational methods lead to an efﬁcient inference scheme involving forward-backward recursions along each of the continuous and discrete Markov chains independently.\\n\\nNote that, if we consider multiple chains of discrete latent variables, and use one as the switch to select from the remainder, we obtain an analogous model having only discrete latent variables known as the switching hidden Markov model. For dynamical systems which do not have a linear-Gaussian, for example, if they use a non-Gaussian emission density, we can turn to sampling methods in order Chapter 11 to ﬁnd a tractable inference algorithm. In particular, we can apply the samplingimportance-resampling formalism of Section 11.1.5 to obtain a sequential Monte Carlo algorithm known as the particle ﬁlter. Consider the class of distributions represented by the graphical model in Figure 13.5, and suppose we are given the observed values Xn = (x1, . , xn) and we wish to draw L samples from the posterior distribution p(zn|Xn)',\n",
       " 'e2ac7e97-1c94-4ac7-9c4d-d7e43a6b4dae': 'We can take this a stage further to provide a deeper test of the correctness of both the mathematical derivation of the update equations and of their software implementation by using ﬁnite differences to check that each update does indeed give a (constrained) maximum of the bound .\\n\\nFor the variational mixture of Gaussians, the lower bound (10.3) is given by where, to keep the notation uncluttered, we have omitted the ⋆ superscript on the q distributions, along with the subscripts on the expectation operators because each expectation is taken with respect to all of the random variables in its argument. The various terms in the bound are easily evaluated to give the following results Exercise 10.16 where D is the dimensionality of x, H is the entropy of the Wishart distribution given by (B.82), and the coefﬁcients C(α) and B(W, ν) are deﬁned by (B.23) and (B.79), respectively. Note that the terms involving expectations of the logs of the q distributions simply represent the negative entropies of those distributions. Some simpliﬁcations and combination of terms can be performed when these expressions are summed to give the lower bound. However, we have kept the expressions separate for ease of understanding',\n",
       " 'd100042f-8bad-4a73-b5c9-9ad42b65c476': 'where Zp = � �p(z) dz. The marginal distribution over z is given by and so we can sample from p(z) by sampling from �p(z, u) and then ignoring the u values. This can be achieved by alternately sampling z and u. Given the value of z we evaluate �p(z) and then sample u uniformly in the range 0 ⩽ u ⩽ �p(z), which is straightforward. Then we ﬁx u and sample z uniformly from the ‘slice’ through the distribution deﬁned by {z : �p(z) > u}. This is illustrated in Figure 11.13(a). In practice, it can be difﬁcult to sample directly from a slice through the distribution and so instead we deﬁne a sampling scheme that leaves the uniform distribution under �p(z, u) invariant, which can be achieved by ensuring that detailed balance is satisﬁed. Suppose the current value of z is denoted z(τ) and that we have obtained a corresponding sample u',\n",
       " 'f6f5bfae-0451-48ca-bdf2-902e1947992d': 'We have seen how the problem of polynomial curve ﬁtting can be expressed in terms of error minimization. Here we return to the curve ﬁtting example and view it Section 1.1 from a probabilistic perspective, thereby gaining some insights into error functions and regularization, as well as taking us towards a full Bayesian treatment. The goal in the curve ﬁtting problem is to be able to make predictions for the target variable t given some new value of the input variable x on the basis of a set of training data comprising N input values x = (x1, . , xN)T and their corresponding target values t = (t1, . , tN)T. We can express our uncertainty over the value of the target variable using a probability distribution. For this purpose, we shall assume that, given the value of x, the corresponding value of t has a Gaussian distribution with a mean equal to the value y(x, w) of the polynomial curve given by (1.1). Thus we have where, for consistency with the notation in later chapters, we have deﬁned a precision parameter β corresponding to the inverse variance of the distribution.\\n\\nThis is illustrated schematically in Figure 1.16',\n",
       " '3856218e-00dc-4df8-bb0a-abc73e9f4e1f': 'Finally, we consider a large number of labeling functions that are likely to be correlated. In our user study (described 12 Speciﬁcally, ϵ is both the coefﬁcient of the ℓ1 regularization term used to induce sparsity, and the minimum absolute weight in log scale that a dependency must have to be selected. Fig. 9 Predictive performance of the generative model and number of learned correlations versus the correlation threshold ϵ. The selected elbow point achieves a good trade-off between predictive performance and computational cost (linear in the number of correlations). Left: simulation of structure learning correcting the generative model. Middle: the CDR task.\\n\\nRight: all user study labeling functions for the Spouses task in Sect. 4.2), participants wrote labeling functions for the Spouses task. We combined all 125 of their functions and studied the effect of varying ϵ. Here, we expect there to be many correlations since it is likely that users wrote redundant functions. We see in Fig. 9, right, that structure learning surpasses the best performing individual’s generative model (50.0 F1). Computational Cost Computational cost is correlated with model complexity',\n",
       " '99e7aac6-1cdd-4b0d-8664-967af1306c2b': '.\\n\\n, tN}, the likelihood function is given by and so the resulting posterior distribution is then which, as a consequence of the nonlinear dependence of y(x, w) on w, will be nonGaussian. We can ﬁnd a Gaussian approximation to the posterior distribution by using the Laplace approximation. To do this, we must ﬁrst ﬁnd a (local) maximum of the posterior, and this must be done using iterative numerical optimization. As usual, it is convenient to maximize the logarithm of the posterior, which can be written in the which corresponds to a regularized sum-of-squares error function. Assuming for the moment that α and β are ﬁxed, we can ﬁnd a maximum of the posterior, which we denote wMAP, by standard nonlinear optimization algorithms such as conjugate gradients, using error backpropagation to evaluate the required derivatives. Having found a mode wMAP, we can then build a local Gaussian approximation by evaluating the matrix of second derivatives of the negative log posterior distribution',\n",
       " 'da805182-8bd1-421b-b85b-2734bd7d903b': 'Although not in the context of the TD model, representations like the microstimulus representation of Ludvig et al. have been proposed and studied by Grossberg and Schmajuk , Brown, Bullock, and Grossberg , Buhusi and Schmajuk , and Machado . The ﬁgures on pages 353–355 are adapted from Sutton and Barto . 14.3 Section 1.7 includes comments on the history of trial-and-error learning and the Law of E↵ect. The idea that Thorndike’s cats might have been exploring according to an instinctual context-speciﬁc ordering over actions rather than by just selecting from a set of instinctual impulses was suggested by Peter Dayan (personal communication).\\n\\nSelfridge, Sutton, and Barto  illustrated the e↵ectiveness of shaping in a pole-balancing reinforcement learning task. Other examples of shaping in reinforcement learning are Gullapalli and Barto , Mahadevan and Connell , Mataric , Dorigo and Colombette , Saksida, Raymond, and Touretzky , and Randløv and Alstrøm',\n",
       " '21cc3868-60da-41b4-8690-7cbde03a85b1': 's at the end of each episode, averaged over the ﬁrst 10 episodes, as well as 100 independent runs, for Finally, there is also a truncated version of Sarsa(λ), called forward Sarsa(λ) (van Seijen, 2016), which appears to be a particularly e↵ective model-free control method for use in conjunction with multi-layer artiﬁcial neural networks. We are starting now to reach the end of our development of fundamental TD learning algorithms. To present the ﬁnal algorithms in their most general forms, it is useful to generalize the degree of bootstrapping and discounting beyond constant parameters to functions potentially dependent on the state and action',\n",
       " '32c2a4d0-74dc-45f0-9a8e-33efbb28e8c1': 'We can also derive an on-line stochastic algorithm  by applying the Robbins-Monro procedure Section 2.3.5 to the problem of ﬁnding the roots of the regression function given by the derivatives of J in (9.1) with respect to µk. This leads to a sequential update in which, for each Exercise 9.2 data point xn in turn, we update the nearest prototype µk using where ηn is the learning rate parameter, which is typically made to decrease monotonically as more data points are considered. The K-means algorithm is based on the use of squared Euclidean distance as the measure of dissimilarity between a data point and a prototype vector. Not only does this limit the type of data variables that can be considered (it would be inappropriate for cases where some or all of the variables represent categorical labels for instance), but it can also make the determination of the cluster means nonrobust to outliers. We Section 2.3.7 can generalize the K-means algorithm by introducing a more general dissimilarity measure V(x, x′) between two vectors x and x′ and then minimizing the following distortion measure which gives the K-medoids algorithm',\n",
       " '46542774-3cc7-4c31-a5cf-11273860635b': 'This update rule (2.3) is of a form that occurs frequently throughout this book. The a step toward the “Target.” The target is presumed to indicate a desirable direction in which to move, though it may be noisy. In the case above, for example, the target is the nth reward. Note that the step-size parameter (StepSize) used in the incremental method (2.3) changes from time step to time step.\\n\\nIn processing the nth reward for action a, the Pseudocode for a complete bandit algorithm using incrementally computed sample averages and \"-greedy action selection is shown in the box below. The function bandit(a) is assumed to take an action and return a corresponding reward. The averaging methods discussed so far are appropriate for stationary bandit problems, that is, for bandit problems in which the reward probabilities do not change over time. As noted earlier, we often encounter reinforcement learning problems that are e↵ectively nonstationary. In such cases it makes sense to give more weight to recent rewards than to long-past rewards. One of the most popular ways of doing this is to use a constant step-size parameter',\n",
       " '00ac8eb3-cb36-4cce-b9c8-1d7e02c33fa5': 'Because we already know the values of the δ’s for the output units, it follows that by recursively applying (5.56) we can evaluate the δ’s for all of the hidden units in a feed-forward network, regardless of its topology. The backpropagation procedure can therefore be summarized as follows. 1. Apply an input vector xn to the network and forward propagate through the network using (5.48) and (5.49) to ﬁnd the activations of all the hidden and output units. 2. Evaluate the δk for all the output units using (5.54). 3. Backpropagate the δ’s using (5.56) to obtain δj for each hidden unit in the 4. Use (5.53) to evaluate the required derivatives. In the above derivation we have implicitly assumed that each hidden or output unit in the network has the same activation function h(·).\\n\\nThe derivation is easily generalized, however, to allow different units to have individual activation functions, simply by keeping track of which form of h(·) goes with which unit',\n",
       " '63b458d3-83c1-49e1-971e-501e78b222fd': 'Illuminating this result is Barnard’s  derivation of the TD algorithm as a combination of one step of an incremental method for learning a model of the Markov chain and one step of a method for computing predictions from the model. The term certainty equivalence is from the adaptive control literature . 6.4 The Sarsa algorithm was introduced by Rummery and Niranjan . They explored it in conjunction with artiﬁcial neural networks and called it “Modiﬁed Connectionist Q-learning”. The name “Sarsa” was introduced by Sutton .\\n\\nThe convergence of one-step tabular Sarsa (the form treated in this chapter) has been proved by Singh, Jaakkola, Littman, and Szepesv´ari . The “windy gridworld” example was suggested by Tom Kalt. Holland’s  bucket brigade idea evolved into an algorithm closely related to Sarsa. The original idea of the bucket brigade involved chains of rules triggering each other; it focused on passing credit back from the current rule to the rules that triggered it',\n",
       " '74df064f-fa3c-4054-9fbd-c4c0256ead93': 'In particular, for an on-policy method we must estimate q⇡(s, a) for the current behavior policy ⇡ and for all states s and actions a. This can be done using essentially the same TD method described above for learning v⇡. Recall that an episode consists of an alternating sequence of states and state–action pairs: In the previous section we considered transitions from state to state and learned the values of states. Now we consider transitions from state–action pair to state–action pair, and learn the values of state–action pairs. Formally these cases are identical: they are both Markov chains with a reward process. The theorems assuring the convergence of state values under TD(0) also apply to the corresponding algorithm for action values: This update is done after every transition from a nonterminal state St. If St+1 is terminal, then Q(St+1, At+1) is deﬁned as zero. This rule uses every element of the quintuple of events, (St, At, Rt+1, St+1, At+1), that make up a transition from one state–action pair to the next. This quintuple gives rise to the name Sarsa for the algorithm',\n",
       " '05620289-0a79-449d-82c4-e94754750d5d': 'Plug these samples, as well as the ﬁtted q(z), into the following estimator: The Monte Carlo EM algorithm does not employ an encoder, instead it samples from the posterior of the latent variables using gradients of the posterior computed with ∇z log pθ(z|x) = ∇z log pθ(z) + ∇z log pθ(x|z). The Monte Carlo EM procedure consists of 10 HMC leapfrog steps with an automatically tuned stepsize such that the acceptance rate was 90%, followed by 5 weight updates steps using the acquired sample. For all algorithms the parameters were updated using the Adagrad stepsizes (with accompanying annealing schedule). The marginal likelihood was estimated with the ﬁrst 1000 datapoints from the train and test sets, for each datapoint sampling 50 values from the posterior of the latent variables using Hybrid Monte Carlo with 4 leapfrog steps. As written in the paper, it is possible to perform variational inference on both the parameters θ and the latent variables z, as opposed to just the latent variables as we did in the paper.\\n\\nHere, we’ll derive our estimator for that case',\n",
       " 'dbad713d-3872-44cc-9312-caee16786a19': 'Most deep learning research on computer vision has focused not on such exotic  447  CHAPTER 12. APPLICATIONS  applications that expand the realm of what is possible with imagery but rather on a small core of AI goals aimed at replicating human abilities.\\n\\nMost deep learning for computer vision is used for object recognition or detection of some form, whether this means reporting which object is present in an image, annotating  https://www.deeplearningbook.org/contents/applications.html    an image with bounding boxes around each object, transcribing a sequence of syinbols from an image, or labeling each pixel in an image with the identity of the object it belongs to. Because generative modeling has been a guiding principle  of deep learning research, there is also a large body of work on image synthesis using deep models. While image synthesis ex nihilo is usually not considered a computer vision endeavor, models capable of image synthesis are usually useful for image restoration, a computer vision task involving repairing defects in images or removing objects from images. 12.2.1 Preprocessing  Many application areas require sophisticated preprocessing because the original input comes in a form that is difficult for many deep learning architectures to represent. Computer vision usually requires relatively little of this kind of pre- processing',\n",
       " '18fb1252-1442-4a34-ae5b-52fc00000b4d': 'DEEP FEEDFORWARD NETWORKS  https://www.deeplearningbook.org/contents/mlp.html    mean squared error holds for a linear model, but in fact, the equivalence holds regardless of the f(a;0) used to predict the mean of the Gaussian.\\n\\nAn advantage of this approach of deriving the cost function from maximum ikelihood is that it removes the burden of designing cost functions for each model. Specifying a model p(y | x) automatically determines a cost function log p(y | x). One recurring theme throughout neural network design is that the gradient of he cost function must be large and predictable enough to serve as a good guide for the learning algorithm. Functions that saturate (become very flat) undermine chis objective because they make the gradient become very small. In many cases his happens because the activation functions used to produce the output of the hidden units or the output units saturate. The negative log-likelihood helps to avoid this problem for many models. Several output units involve an exp function hat can saturate when its argument is very negative',\n",
       " '214afe83-e426-45cd-a39e-d9a18d31c5fd': 'Discrete convolution can be viewed as multiplication by a matrix, but the matrix has several entries constrained to be equal to other entries. For example, for univariate discrete convolution, each row of the matrix is constrained to be equal to the row above shifted by one element. This is known as a Toeplitz matrix. In two dimensions, a doubly block circulant matrix corresponds to convolution. In addition to these constraints that several elements be equal to each other, convolution usually corresponds to a very sparse matrix (a matrix whose entries are mostly equal to zero). This is because the kernel is usually much smaller than the input image.\\n\\nAny neural network algorithm that works with  https://www.deeplearningbook.org/contents/convnets.html    matrix multiplication and does not depend on specific properties of the matrix structure should work with convolution, without requiring any further changes to the neural network. Typical convolutional neural networks do make use of  further specializations in order to deal with large inputs efficiently, but these are not strictly necessary from a theoretical perspective',\n",
       " '779d9f8e-6902-4c58-96c5-6e37e3e3512a': 'The sequence of states, times, and predictions is thus as follows: The rewards in this example are the elapsed times on each leg of the journey.1 We are not discounting (γ = 1), and thus the return for each state is the actual time to go from that state. The value of each state is the expected time to go. The second column of numbers gives the current estimated value for each state encountered. A simple way to view the operation of Monte Carlo methods is to plot the predicted total time (the last column) over the sequence, as in Figure 6.1 (left). The red arrows show the changes in predictions recommended by the constant-↵ MC method (6.1), for ↵ = 1. These are exactly the errors between the estimated value (predicted time to go) in each state and the actual return (actual time to go). For example, when you exited the highway you thought it would take only 15 minutes more to get home, but in fact it took 23 minutes. Equation 6.1 applies at this point and determines an increment in the estimate of time to go after exiting the highway.\\n\\nThe error, Gt − V (St), at this time is eight minutes. Suppose the step-size parameter, ↵, is 1/2',\n",
       " '72e72ef1-4c05-49b7-b518-43f158ed4114': 'A straightforward technique  for doing so is to compute  https://www.deeplearningbook.org/contents/mlp.html    Hv =V2|(Ves(x)) VU.\\n\\n(6.59) Both gradient computations in this expression may be computed automatically by the appropriate software library. Note that the outer gradient expression takes the gradient of a function of the inner gradient expression. If v is itself a vector produced by a computational graph, it is important to specify that the automatic differentiation software should not differentiate through the graph that produced v.  While computing the Hessian is usually not advisable, it is possible to do with Hessian vector products. One simply computes He for all i =1,...,n, where  2)  e() is the one-hot vector with e;’ = 1 and all other entries are equal to 0. 6.6 Historical Notes  Feedforward networks can be seen as efficient nonlinear function approximators based on using gradient descent to minimize the error in a function approximation. From this point of view, the modern feedforward network is the culmination of centuries of progress on the general function approximation task. The chain rule that underlies the back-propagation algorithm was invented in the seventeenth century',\n",
       " 'f24393e5-3cd5-4747-8181-a3e730bfcd8d': 'In this example, we do not explain how the back-propagation algorithm works. The purpose is only to illustrate what the desired result is: a computational graph with a symbolic description of the derivative. to the graph that provide a symbolic description of the desired derivatives. This is the approach taken by Theano  and TensorFlow . An example of how it works is illustrated in figure 6.10. The primary advantage of this approach is that the derivatives are described in the same language as the original expression.\\n\\nBecause the derivatives are just another computational graph, it is possible to run back-propagation again, differentiating the derivatives to obtain higher derivatives. (Computation of higher-order derivatives is described in section 6.5.10.) We will use the latter approach and describe the back-propagation algorithm in terms of constructing a computational graph for the derivatives. Any subset of the graph may then be evaluated using specific numerical values at a later time. This allows us to avoid specifying exactly when each operation should be computed. Instead, a generic graph evaluation engine can evaluate every node as soon as its parents’ values are available',\n",
       " 'eefaac35-6dd8-4709-8ea2-f9bbe07fec30': 'They found that when APV-MCTS used the value function derived from the RL policy, it performed better than if it used the value function derived from the SL policy. Several methods worked together to produce AlphaGo’s impressive playing skill. The DeepMind team evaluated di↵erent versions of AlphaGo in order to assess the contributions made by these various components. The parameter ⌘ in (16.4) controlled the mixing of game state evaluations produced by the value network and by rollouts. With ⌘ = 0, AlphaGo used just the value network without rollouts, and with ⌘ = 1, evaluation relied just on rollouts. They found that AlphaGo using just the value network played better than the rollout-only AlphaGo, and in fact played better than the strongest of all other Go programs existing at the time. The best play resulted from setting ⌘ = 0.5, indicating that combining the value network with rollouts was particularly important to AlphaGo’s success',\n",
       " '03fd3305-fe6e-48af-a43e-df0ee65e5ff9': 'The optimal model parameters are:  0” = arg min Ep. »(p)  It looks very similar to a normal learning task, but one dataset is considered as one data sample. Few-shot classification is an instantiation of meta-learning in the field of supervised learning. The dataset D is often split into two parts, a support set S\\' for learning and a prediction set B for training or testing, D = (S, B). Often we consider a K-shot N-class classification task: the support set contains K labelled examples for each of N classes. Training Testing  ee | Test dataset: “dog-otter\"” iii aaa  te ail : | .\\n\\nLf\") |  https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil\\'Log   Training in the Same Way as Testing  A dataset D contains pairs of feature vectors and labels, D = {(x;, y;)} and each label belongs to a known label set £/#>*!',\n",
       " '209a727d-62c6-4572-9873-a33237ed5a60': 'We can see immediately that the variational posterior distribution over the parameters must factorize between π and the remaining parameters µ and Λ because all paths connecting π to either µ or Λ must pass through one of the nodes zn all of which are in the conditioning set for our conditional independence test and all of which are head-to-tail with respect to such paths. As a second illustration of variational inference, we return to the Bayesian linear regression model of Section 3.3. In the evidence framework, we approximated the integration over α and β by making point estimates obtained by maximizing the log marginal likelihood. A fully Bayesian approach would integrate over the hyperparameters as well as over the parameters. Although exact integration is intractable, we can use variational methods to ﬁnd a tractable approximation.\\n\\nIn order to simplify the discussion, we shall suppose that the noise precision parameter β is known, and is ﬁxed to its true value, although the framework is easily extended to include the distribution over β. For the linear regression model, the variational treatment Exercise 10.26 will turn out to be equivalent to the evidence framework',\n",
       " 'deed21ea-af26-4799-8344-887869ea0cdb': 'Fortunately, if we also know P(x), we can compute the desired quantity  using Bayes’ rule:  P(x)P(y | x) Ply)  Note that while P(y) appears in the formula, it is usually feasible to compute  P(y) =, Ply | 2) P(2), so we do not need to begin with knowledge of P(y). P(x|y)= (3.42)  68  CHAPTER 3.\\n\\nPROBABILITY AND INFORMATION THEORY  Bayes’ rule is straightforward to derive from the definition of conditional probability, but it is useful to know the name of this formula since many texts refer to it by name. It is named after the Reverend Thomas Bayes, who first discovered a special case of the formula. The general version presented here was independently discovered by Pierre-Simon Laplace. 3.12 Technical Details of Continuous Variables  A proper formal understanding of continuous random variables and probability density functions requires developing probability theory in terms of a branch of mathematics known as measure theory. Measure theory is beyond the scope of this textbook, but we can briefly sketch some of the issues that measure theory is employed to resolve',\n",
       " '13bc7744-5cd3-472e-97a3-7a6fe58385ac': 'Environment models in reinforcement learning are like cognitive maps in that they can be learned by supervised learning methods without relying on reward signals, and then they can be Reinforcement learning’s distinction between model-free and model-based algorithms corresponds to the distinction in psychology between habitual and goal-directed behavior.\\n\\nModel-free algorithms make decisions by accessing information that has been stored in a policy or an action-value function, whereas model-based methods select actions as the result of planning ahead using a model of the agent’s environment. Outcome-devaluation experiments provide information about whether an animal’s behavior is habitual or under goal-directed control. Reinforcement learning theory has helped clarify thinking about these issues. Animal learning clearly informs reinforcement learning, but as a type of machine learning, reinforcement learning is directed toward designing and understanding e↵ective learning algorithms, not toward replicating or explaining details of animal behavior. We focused on aspects of animal learning that relate in clear ways to methods for solving prediction and control problems, highlighting the fruitful two-way ﬂow of ideas between reinforcement learning and psychology without venturing deeply into many of the behavioral details and controversies that have occupied the attention of animal learning researchers',\n",
       " '9cfc418f-8c9e-4be7-af16-fcbdc3be8801': 'Given a batch of samples, {x;, y;)}2_, where y; is the class label of x; and a function f(.,.) for measuring similarity between two inputs, the soft nearest neighbor loss at temperature T is defined as:  c= Slog Vi diyimypit,..B PCF» X5)/7) ™ B igkk=1,....B exp(—f (xj, xx) /T)  i=l  The temperature 7 is used for tuning how concentrated the features are in the representation space. For example, when at low temperature, the loss is dominated by the small distances and widely separated representations cannot contribute much and become irrelevant. Common Setup  We can loosen the definition of “classes” and “labels” in soft nearest-neighbor loss to create positive and negative sample pairs out of unsupervised data by, for example, applying data augmentation to create noise versions of original samples. Most recent studies follow the following definition of contrastive learning objective to incorporate multiple positive and negative samples',\n",
       " 'fc92cc30-dc4e-458a-82d8-ae945a31725e': 'We study the modeling trade-offs in this new setting and propose an optimizer for automating trade-off decisions that gives up to 1.8× speedup per pipeline execution.\\n\\nIn two collaborations, with the US Department of Veterans Affairs and the US Food and Drug Administration, and on four open-source text and image data sets representative of other deployments, Snorkel provides 132% average improvements to predictive performance over prior heuristic approaches and comes within an average 3.60% of the predictive performance of large hand-curated training sets. Keywords Machine learning · Weak supervision · Training data In the last several years, there has been an explosion of interest in machine learning-based systems across industry, government, and academia, with an estimated spend this year of $12.5 billion . A central driver has been the advent of deep learning techniques, which can learn task-speciﬁc representations of input data, obviating what used to be the most time-consuming development task: feature engineering. These learned representations are particularly effective for tasks like natural language processing and image analysis, which have high-dimensional, high-variance input that is impossible to fully capture with simple rules or handengineered features',\n",
       " '56d613df-1809-4b2e-ad2a-05450bbf7f45': 'Various other extensions of GausExercise 6.23 sian process regression have also been considered, for purposes such as modelling the distribution over low-dimensional manifolds for unsupervised learning  and the solution of stochastic differential equations . The predictions of a Gaussian process model will depend, in part, on the choice of covariance function.\\n\\nIn practice, rather than ﬁxing the covariance function, we may prefer to use a parametric family of functions and then infer the parameter values from the data. These parameters govern such things as the length scale of the correlations and the precision of the noise and correspond to the hyperparameters in a standard parametric model. Techniques for learning the hyperparameters are based on the evaluation of the likelihood function p(t|θ) where θ denotes the hyperparameters of the Gaussian process model. The simplest approach is to make a point estimate of θ by maximizing the log likelihood function. Because θ represents a set of hyperparameters for the regression problem, this can be viewed as analogous to the type 2 maximum likelihood procedure for linear regression models. Maximization of the log likelihood Section 3.5 can be done using efﬁcient gradient-based optimization algorithms such as conjugate gradients',\n",
       " '13314ef9-e14e-4521-88b4-e8f3105c7072': 'where I0(m) is the zeroth-order Bessel function of the ﬁrst kind. The distribution has period 2π so that p(θ + 2π) = p(θ) for all θ. Care must be taken in interpreting this distribution because simple expectations will be dependent on the (arbitrary) choice of origin for the variable θ. The parameter θ0 is analogous to the mean of a univariate Gaussian, and the parameter m > 0, known as the concentration parameter, is analogous to the precision (inverse variance). For large m, the von Mises distribution is approximately a Gaussian centred on θ0.\\n\\nThe Wishart distribution is the conjugate prior for the precision matrix of a multivariate Gaussian. where W is a D × D symmetric, positive deﬁnite matrix, and ψ(·) is the digamma function deﬁned by (B.25). The parameter ν is called the number of degrees of freedom of the distribution and is restricted to ν > D − 1 to ensure that the Gamma function in the normalization factor is well-deﬁned',\n",
       " 'cf4982eb-32e5-43a9-b843-14cfcfbdc9cd': 'They are sampled such that the combined length is ≤ 512 tokens. The LM masking is applied after WordPiece tokenization with a uniform masking rate of 15%, and no special consideration given to partial word pieces. We train with batch size of 256 sequences (256 sequences * 512 tokens = 128,000 tokens/batch) for 1,000,000 steps, which is approximately 40 epochs over the 3.3 billion word corpus. We use Adam with learning rate of 1e-4, β1 = 0.9, β2 = 0.999, L2 weight decay of 0.01, learning rate warmup over the ﬁrst 10,000 steps, and linear decay of the learning rate. We use a dropout probability of 0.1 on all layers. We use a gelu activation  rather than the standard relu, following OpenAI GPT.\\n\\nThe training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood. Training of BERTBASE was performed on 4 Cloud TPUs in Pod conﬁguration (16 TPU chips total).13 Training of BERTLARGE was performed on 16 Cloud TPUs (64 TPU chips total). Each pretraining took 4 days to complete',\n",
       " '5f717ff8-9ff2-47d3-8679-757ca4afad79': 'These techniques, however, can’t be easily extended to new domains, such as CV. Despite promising early results, SSL has not yet brought about the same improvements in computer vision that we have seen in NLP (though this will change).\\n\\nThe main reason is that it is considerably more difficult to represent uncertainty in the prediction for images than it is for words. When the missing word cannot be predicted exactly (is it “lion” or “cheetah”? ), the system can associate a score or a probability to all possible words in the vocabulary: high  ”  score for “lion,” “cheetah,” and a few other predators, and low scores for all  other words in the vocabulary. Training models at this scale also required a model architecture that was efficient in terms of both runtime and  https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/  Self-supervised learning: The dark matter of intelligence   memory, without compromising on accuracy. Fortunately, a recent innovation by FAIR in the realm of architecture design led to anew model family called RegNets that perfectly fit these needs',\n",
       " '6ec8d5a2-9964-4ecb-97af-9e78ec082df5': 'Then in the back-tracking step, having found xmax, we can then use these stored values to assign consistent maximizing states xmax 1 , . , xmax M . The max-sum algorithm, with back-tracking, gives an exact maximizing conﬁguration for the variables provided the factor graph is a tree. An important application of this technique is for ﬁnding the most probable sequence of hidden states in a hidden Markov model, in which case it is known as the Viterbi algorithm. Section 13.2 As with the sum-product algorithm, the inclusion of evidence in the form of observed variables is straightforward. The observed variables are clamped to their observed values, and the maximization is performed over the remaining hidden variables. This can be shown formally by including identity functions for the observed variables into the factor functions, as we did for the sum-product algorithm.\\n\\nIt is interesting to compare max-sum with the iterated conditional modes (ICM) algorithm described on page 389. Each step in ICM is computationally simpler because the ‘messages’ that are passed from one node to the next comprise a single value consisting of the new state of the node for which the conditional distribution is maximized',\n",
       " 'd4d50c97-0cf9-4415-b5cb-5e234ddc80f1': 'These embeddings were learned using the SVD. Later, embeddings would be learned by neural networks. The history of natural language processing is marked by transitions in the popularity of different ways of representing the input to the model. Following this early work on symbols and words, some of the earliest applications of neural networks to NLP  represented the input as a sequence of characters. Bengio ef al. returned the focus to modeling words and introduced neural language models, which produce interpretable word embeddings.\\n\\nThese neural models have scaled up from defining representations of a small set of symbols in the 1980s to millions of words (including proper nouns and misspellings) in modern applications. This computational scaling effort led to the invention of the techniques described in section 12.4.3. Initially, the use of words as the fundamental units of language models yielded improved language modeling performance . To this day, new techniques continually push both character-based models  and word-based models forward, with recent work  even modeling individual bytes of Unicode characters. The ideas behind neural language models have been extended into several  472  CHAPTER 12',\n",
       " '96ab6a40-e5af-42ee-97f2-63631af65bf4': 'When applied to natural images, this topographic ICA approach learns Gabor filters, such that neighboring features have similar orientation, location or frequency.\\n\\nMany different phase offsets of similar Gabor functions occur within each region, so that pooling over small regions yields translation invariance. 13.3 Slow Feature Analysis  Slow feature analysis (SFA) is a linear factor model that uses information from time signals to learn invariant features . 489  CHAPTER 13. LINEAR FACTOR MODELS  Slow feature analysis is motivated by a general principle called the slowness principle. The idea is that the important characteristics of scenes change very slowly compared to the individual measurements that make up a description of a scene. For example, in computer vision, individual pixel values can change very rapidly. If a zebra moves from left to right across the image, an individual pixel will rapidly change from black to white and back again as the zebra’s stripes pass over the pixel. By comparison, the feature indicating whether a zebra is in the image will not change at all, and the feature describing the zebra’s position will change slowly. We therefore may wish to regularize our model to learn features that change slowly over time',\n",
       " '0bbda20a-0fec-455d-a6a5-72e563494126': '(The name “immorality” may seem strange; it was coined in the graphical  Vwi 1 od 1 4 74 + \\\\m 1 a a4 rd  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    models lverayure as a JOKE ADOUL ULMALITIed pares.) LO COouvert a airecved model with graph P into an undirected model, we need to create a new graph 4. For every pair of variables x and y, we add an undirected edge connecting x and y to U if there is a directed edge tin either direction) connecting x and y in D or if x  and y are both parents in D of a third variable z. The resulting YU is known as a moralized graph. See figure 16.11 for examples of converting directed models to undirected models via moralization',\n",
       " '6a79aa23-986f-43ee-af0a-2bcf24c5c9fe': 'Miller’s idea not only parallels Klopf’s (with the exception of its explicit invocation of a distinct “strengthening signal”), it also anticipated the general features of reward-modulated STDP. A related though di↵erent idea, which Seung  called the “hedonistic synapse,” is that synapses individually adjust the probability that they release neurotransmitter in the manner of the Law of E↵ect: if reward follows release, the release probability increases, and decreases if reward follows failure to release.\\n\\nThis is essentially the same as the learning scheme Minsky used in his 1954 Princeton Ph.D. dissertation, where he called the synapse-like learning element a SNARC (Stochastic Neural-Analog Reinforcement Calculator). Contingent eligibility is involved in these ideas too, although it is contingent on the activity of an individual synapse instead of the postsynaptic neuron. Also related is the proposal of Unnikrishnan and Venugopal  that uses the correlation-based method of Harth and Tzanakou  to adjust ANN weights',\n",
       " '46ef44b3-e04c-4c79-b65e-16d7ab2ecb13': 'It is easily seen that the quantities ω(zn) have the probabilistic Exercise 13.16 Once we have completed the ﬁnal maximization over zN, we will obtain the value of the joint distribution p(X, Z) corresponding to the most probable path. We also wish to ﬁnd the sequence of latent variable values that corresponds to this path. To do this, we simply make use of the back-tracking procedure discussed in Section 8.4.5.\\n\\nSpeciﬁcally, we note that the maximization over zn must be performed for each of the K possible values of zn+1. Suppose we keep a record of the values of zn that correspond to the maxima for each value of the K values of zn+1. Let us denote this function by ψ(kn) where k ∈ {1, . , K}. Once we have passed messages to the end of the chain and found the most probable state of zN, we can then use this function to backtrack along the chain by applying it recursively Intuitively, we can understand the Viterbi algorithm as follows',\n",
       " 'd55db94c-8beb-4870-a9c8-84e9369e60c9': 'CONVOLUTIONAL NETWORKS  Complex layer terminology Simple layer terminology  https://www.deeplearningbook.org/contents/convnets.html    Convolutional Layer  Pooling stage Pooling layer  Detector st  Nonlinearity Nonlinearity y  ified linear e.g., rectified linear  Convolution stage: Convolution layer:  Affine transform Affine transform  Input to layer Input to layers  Figure 9.7: The components of a typical convolutional neural network layer. There are two commonly used sets of terminology for describing these layers. (Left)In this terminology, the convolutional net is viewed as a small number of relatively complex layers, with each layer having many “stages.” In this terminology, there is a one-to-one mapping between kernel tensors and network layers.\\n\\nIn this book we generally use this terminology. (Right)In this terminology, the convolutional net is viewed as a larger number of simple layers; every step of processing is regarded as a layer in its own right. This means that not every “layer” has parameters. In all cases, pooling helps to make the representation approximately invariant to small translations of the input',\n",
       " 'c7d51537-bf06-4c03-b92f-b00d0b8b05c1': 'Back-translation  CERT ; code) generates augmented sentences via back-translation. Various translation models for different languages can be employed for creating different versions of augmentations. Once we have a noise version of text samples, many contrastive learning frameworks introduced above, such as MoCo, can be used to learn sentence embedding. Dropout and Cutoff  Shen et al. proposed to apply Cutoff to text augmentation, inspired by cross-view training. They proposed three cutoff augmentation strategies:  Token cutoff removes the information of a few selected tokens.\\n\\nTo make sure there is no data leakage, corresponding tokens in the input, positional and other relevant embedding matrices should all be zeroed out.,  Feature cutoff removes a few feature columns. Span cutoff removes a continuous chunk of texts',\n",
       " '94cbb518-40c6-45b0-a61b-48166503d079': 'This can be done using the following properties: Techniques for Constructing New Kernels.\\n\\nGiven valid kernels k1(x, x′) and k2(x, x′), the following new kernels will also be valid: where c > 0 is a constant, f(·) is any function, q(·) is a polynomial with nonnegative coefﬁcients, φ(x) is a function from x to RM, k3(·, ·) is a valid kernel in RM, A is a symmetric positive semideﬁnite matrix, xa and xb are variables (not necessarily disjoint) with x = (xa, xb), and ka and kb are valid kernel functions over their respective spaces. Equipped with these properties, we can now embark on the construction of more complex kernels appropriate to speciﬁc applications. We require that the kernel k(x, x′) be symmetric and positive semideﬁnite and that it expresses the appropriate form of similarity between x and x′ according to the intended application. Here we consider a few common examples of kernel functions',\n",
       " 'ce6a0002-6401-4dd5-a227-b058035679c4': 'Future development of reinforcement learning theory and algorithms will likely exploit links to many other features of animal learning as the computational utility of these features becomes better appreciated. We expect that a ﬂow of ideas between reinforcement learning and psychology will continue to bear fruit for both disciplines.\\n\\nMany connections between reinforcement learning and areas of psychology and other behavioral sciences are beyond the scope of this chapter. We largely omit discussing links to the psychology of decision making, which focuses on how actions are selected, or how decisions are made, after learning has taken place. We also do not discuss links to ecological and evolutionary aspects of behavior studied by ethologists and behavioral ecologists: how animals relate to one another and to their physical surroundings, and how their behavior contributes to evolutionary ﬁtness. Optimization, MDPs, and dynamic programming ﬁgure prominently in these ﬁelds, and our emphasis on agent interaction with dynamic environments connects to the study of agent behavior in complex “ecologies.” Multi-agent reinforcement learning, omitted in this book, has connections to social aspects of behavior. Despite the lack of treatment here, reinforcement learning should by no means be interpreted as dismissing evolutionary perspectives. Nothing about reinforcement learning implies a tabula rasa view of learning and behavior',\n",
       " 'b7daa2b2-2dd0-461b-bd60-1530bf09f8f5': 'For instance, the Gaussian version of the Markov random ﬁeld, Section 8.3 which is widely used as a probabilistic model of images, is a Gaussian distribution over the joint space of pixel intensities but rendered tractable through the imposition of considerable structure reﬂecting the spatial organization of the pixels. Similarly, the linear dynamical system, used to model time series data for applications such Section 13.3 as tracking, is also a joint Gaussian distribution over a potentially large number of observed and latent variables and again is tractable due to the structure imposed on the distribution. A powerful framework for expressing the form and properties of such complex distributions is that of probabilistic graphical models, which will form the subject of Chapter 8. An important property of the multivariate Gaussian distribution is that if two sets of variables are jointly Gaussian, then the conditional distribution of one set conditioned on the other is again Gaussian.\\n\\nSimilarly, the marginal distribution of either set is also Gaussian. Consider ﬁrst the case of conditional distributions. Suppose x is a D-dimensional vector with Gaussian distribution N(x|µ, Σ) and that we partition x into two disjoint subsets xa and xb',\n",
       " 'e5863412-b811-49cb-af85-70849323ad93': ', xN, then, because we have represented the marginal distribution in the form p(x) = � z p(x, z), it follows that for every observed data point xn there is a corresponding We have therefore found an equivalent formulation of the Gaussian mixture involving an explicit latent variable. It might seem that we have not gained much by doing so. However, we are now able to work with the joint distribution p(x, z) instead of the marginal distribution p(x), and this will lead to signiﬁcant simpliﬁcations, most notably through the introduction of the expectation-maximization (EM) algorithm. Another quantity that will play an important role is the conditional probability of z given x. We shall use γ(zk) to denote p(zk = 1|x), whose value can be found using Bayes’ theorem We shall view πk as the prior probability of zk = 1, and the quantity γ(zk) as the corresponding posterior probability once we have observed x.\\n\\nAs we shall see later, γ(zk) can also be viewed as the responsibility that component k takes for ‘explaining’ the observation x',\n",
       " 'd9fd8518-3906-4ef0-80bd-20ee8c5ff83a': 'Committee-based sampling for training probabilistic classiﬁers. Proceedings of the Twelfth International Conference on International Conference on Machine Learning, 150–157. Dayan, P., & Hinton, G. E. Using expectation-maximization for reinforcement learning. Neural Computation, 9(2), 271–278. Deisenroth, M. P., Neumann, G., Peters, J., et al. A survey on policy search for robotics. Foundations and Trends® in Robotics, 2(1–2), 1–142. Dempster, A. P., Laird, N. M., & Rubin, D. B. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society: Series B (Methodological), 39(1), 1–22. Deng, M., Wang, J., Hsieh, C.-P., Wang, Y., Guo, H., Shu, T., Song, M., Xing, E',\n",
       " 'ee3d7d1c-4afa-43f9-87f8-c2805df1c7e4': 'In fact, even simply taking a random step when the gradient magnitude is above a threshold tends to work almost as well. If the explosion is so severe that the gradient is numerically Inf or Nan (considered infinite or not-a-number), then a random step of size v can be taken and will typically move away from the numerically unstable configuration. Clipping the gradient norm per minibatch will not change the direction of the gradient for an individual minibatch. Taking the average of the norm-clipped gradient from many minibatches, however, is not equivalent to clipping the norm of the true gradient (the gradient formed from using all examples).\\n\\nExamples that have large gradient norm, as well as examples that appear in the same minibatch as such examples, will have their contribution to the final direction diminished. This stands in contrast to traditional minibatch gradient descent, where the true gradient direction is equal to the average over all minibatch gradients. Put another way, traditional stochastic gradient descent uses an unbiased estimate of the gradient, while gradient descent with norm clipping introduces a heuristic bias that we know empirically to be useful. With element-  410  CHAPTER 10',\n",
       " '44e5047e-b0db-4fa3-b968-518eb648b552': 'Efﬁcient approximate ML or MAP estimation for the parameters θ. The parameters can be of interest themselves, e.g. if we are analyzing some natural process. They also allow us to mimic the hidden random process and generate artiﬁcial data that resembles the real data. 2.\\n\\nEfﬁcient approximate posterior inference of the latent variable z given an observed value x for a choice of parameters θ. This is useful for coding or data representation tasks. 3. Efﬁcient approximate marginal inference of the variable x. This allows us to perform all kinds of inference tasks where a prior over x is required. Common applications in computer vision include image denoising, inpainting and super-resolution. For the purpose of solving the above problems, let us introduce a recognition model qφ(z|x): an approximation to the intractable true posterior pθ(z|x). Note that in contrast with the approximate posterior in mean-ﬁeld variational inference, it is not necessarily factorial and its parameters φ are not computed from some closed-form expectation',\n",
       " 'cf6ad79a-91d7-48cd-862f-b7190b3859d0': 'Tan, B., Hu, Z., ZichaoYang, R., & Xing, E. Connecting the dots between mle and rl for sequence generation. arXiv. https://arxiv.org/abs/1811.09740 Tan, B., Qin, L., Xing, E., & Hu, Z. Summarizing text on any aspects: A knowledge-informed weakly-supervised approach. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 6301–6309.\\n\\nTaskar, B., Guestrin, C., & Koller, D. Max-margin markov networks. Advances in Neural Information Processing systems, 25–32. Thrun, S. Lifelong learning algorithms. Learning to learn (pp. 181–209). Springer. Titsias, M. Variational learning of inducing variables in sparse Gaussian processes. Artiﬁcial Intelligence and Statistics, 567–574. Vapnik, V. N',\n",
       " '53b986d5-6a62-4f45-a857-8d0459cb7df4': 'But the phrase is lately gaining currency in psychology and neuroscience, likely because strong parallels have surfaced between reinforcement learning algorithms and animal learning—parallels described in this chapter and the next.\\n\\nAccording to common usage, a reward is an object or event that an animal will approach and work for. A reward may be given to an animal in recognition of its ‘good’ behavior, or given in order to make the animal’s behavior ‘better.’ Similarly, a penalty is an object or event that the animal usually avoids and that is given as a consequence of ‘bad’ behavior, usually in order to change that behavior. Primary reward is reward due to machinery built into an animal’s nervous system by evolution to improve its chances of survival and reproduction, for example, reward produced by the taste of nourishing food, sexual contact, successful escape, and many other stimuli and events that predicted reproductive success over the animal’s ancestral history. As explained in Section 14.2.1, higher-order reward is reward delivered by stimuli that predict primary reward, either directly or indirectly by predicting other stimuli that predict primary reward. Reward is secondary if its rewarding quality is the result of directly predicting primary reward',\n",
       " '8891b891-cf86-43d5-9c13-bb8ab3a34328': 'Intuitively, from Figure 3.2, we anticipate that this solution corresponds to the orthogonal projection of t onto the subspace S. This is indeed the case, as can easily be veriﬁed by noting that the solution for y is given by ΦwML, and then conﬁrming that this takes the form of an orthogonal projection. Exercise 3.2 In practice, a direct solution of the normal equations can lead to numerical difﬁculties when ΦTΦ is close to singular. In particular, when two or more of the basis vectors ϕj are co-linear, or nearly so, the resulting parameter values can have large magnitudes. Such near degeneracies will not be uncommon when dealing with real data sets. The resulting numerical difﬁculties can be addressed using the technique of singular value decomposition, or SVD . Note that the addition of a regularization term ensures that the matrix is nonsingular, even in the presence of degeneracies',\n",
       " 'c4b65725-154d-4208-a903-01fb2fa86826': 'Also, we note that scaling the log likelihood by a positive constant coefﬁcient does not alter the location of the maximum with respect to w, and so we can replace the coefﬁcient β/2 with 1/2. Finally, instead of maximizing the log likelihood, we can equivalently minimize the negative log likelihood.\\n\\nWe therefore see that maximizing likelihood is equivalent, so far as determining w is concerned, to minimizing the sum-of-squares error function deﬁned by (1.2). Thus the sum-of-squares error function has arisen as a consequence of maximizing likelihood under the assumption of a Gaussian noise distribution. We can also use maximum likelihood to determine the precision parameter β of the Gaussian conditional distribution. Maximizing (1.62) with respect to β gives Again we can ﬁrst determine the parameter vector wML governing the mean and subsequently use this to ﬁnd the precision βML as was the case for the simple Gaussian distribution. Section 1.2.4 Having determined the parameters w and β, we can now make predictions for new values of x',\n",
       " 'a2e457bd-5286-43e5-a3df-28ed6ba34da2': 'This  https://www.deeplearningbook.org/contents/generative_models.html    _ This specific choice of MLP is somewhat arbitrary, compared to many of the inference equations in chapter 19 that are derived from first punciples. This MLP is a heuristic choice that seems to work well in practice and is used consistently  in the literature. Many approximate inference techniques are motivated by their ability to find a maximally tight variational lower bound on the log-likelihood under some set of constraints. One can construct a variational lower bound on the log-likelihood using the hidden unit expectations defined by the DBN’s MLP, but this is true of any probability distribution over the hidden units, and there is no reason to believe that this MLP provides a particularly tight bound. In particular, the MLP ignores many important interactions in the DBN graphical model. The MLP propagates information upward from the visible units to the deepest hidden units, but it does not propagate any information downward or sideways.\\n\\nThe DBN graphical model has explaining away interactions between all the hidden units within the same layer as well as in top-down interactions between layers',\n",
       " 'c21e4a2c-8adb-4b8b-9a57-b6bfa59580f5': 'Proceedings of the IEEE, 86(11), 2278–2324. Mnih, A. and Gregor, K. Neural variational inference and learning in belief networks. Technical report, arXiv preprint arXiv:1402.0030. Rezende, D. J., Mohamed, S., and Wierstra, D. Stochastic backpropagation and approximate inference in deep generative models.\\n\\nTechnical report, arXiv:1401.4082. Rifai, S., Bengio, Y., Dauphin, Y., and Vincent, P. A generative process for sampling contractive auto-encoders. In ICML’12. Salakhutdinov, R. and Hinton, G. E. Deep Boltzmann machines. In AISTATS’2009, pages 448– 455. Schmidhuber, J. Learning factorial codes by predictability minimization. Neural Computation, 4(6), 863–879',\n",
       " 'bc5cd9e7-e3e7-4c2f-92d1-88e9d43e200e': 'It has the following properties:  P(ix=1)=6¢  P(x=2)=¢\"(1-¢)**  Vary (x) = e(1 — 9)  3.9.2 Multinoulli Distribution  The multinoulli, or categorical, distribution is a distribution over a single dis- crete variable with k different states, where k is finite.! The multinoulli distribution  } “Multinoulli” is a term that was recently coined by Gustavo Lacerda and popularized by Murphy .\\n\\nThe multinoulli distribution is a special case of the multinomial distribution A multinomial distribution is the distribution over vectors in {0,...,n}* representing how many times each of the k categories is visited when n samples are drawn from a multinoulli distribution. Many texts use the term “multinomial” to refer to multinoulli distributions without clarifying that they are referring only to the n = 1 case. 60  CHAPTER 3. PROBABILITY AND INFORMATION THEORY  is parametrized by a vector p € (0, yr, where p; gives the probability of the  i-th state',\n",
       " 'a5ada511-b630-4214-b199-bfe8952e44ea': 'Common sense helps people learn new skills without requiring massive amounts of teaching for every single task. For example, if we show just a few drawings of cows to small children, they’ll eventually be able to recognize any cow they see. By contrast, Al systems trained with supervised learning require many examples of cow images and might still fail to classify cows in unusual situations, such as lying on a beach.\\n\\nHow is it that humans can learn to drive a car in about 20 hours of practice with very little supervision, while fully autonomous driving still eludes our best Al systems trained with thousands of hours of data from human drivers? The short answer is that humans rely on their previously acquired background knowledge of how the world works. How do we get machines to do the same? We believe that self-supervised learning is one of the most promising ways to build such background  https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/  Self-supervised learning: The dark matter of intelligence   knowledge and approximate a torm ot common sense in Al systems. We believe that self-supervised learning (SSL) is one of the most promising ways to build such background knowledge and approximate a form of common sense in Al systems',\n",
       " '0b23a3ae-cad4-4b96-b323-c149650a8aa7': 'Exercise 5.17 By neglecting the second term in (5.83), we arrive at the Levenberg–Marquardt approximation or outer product approximation (because the Hessian matrix is built up from a sum of outer products of vectors), given by where bn = ∇yn = ∇an because the activation function for the output units is simply the identity. Evaluation of the outer product approximation for the Hessian is straightforward as it only involves ﬁrst derivatives of the error function, which can be evaluated efﬁciently in O(W) steps using standard backpropagation. The elements of the matrix can then be found in O(W 2) steps by simple multiplication. It is important to emphasize that this approximation is only likely to be valid for a network that has been trained appropriately, and that for a general network mapping the second derivative terms on the right-hand side of (5.83) will typically not be negligible',\n",
       " '3954234f-3441-4336-9c3c-646d93bad63f': 'In the subsequent maximization (M) step, L(q(n+1), θ) is minimized w.r.t. θ: which is to maximize the expected complete data log-likelihood. The EM algorithm has an appealing property that it monotonically decreases the negative marginal log-likelihood over iterations. To see this, notice that after the E-step the upper bound L(q(n+1), θ(n)) is equal to the negative marginal Variational EM.\\n\\nWhen the model pθ(x, y) is complex (e.g., a neural network or a multilayer graphical model), directly working with the true posterior in the E-step becomes intractable. Variational EM overcomes the diﬃculty with approximations. It considers a restricted family Q′ of the variational distribution q(y) such that optimization w.r.t. q within the family is tractable: A common way to restrict the q family is the mean-ﬁeld methods, which partition the components of y into sub-groups y = (y1, . , yM) and assume that q factorizes w.r.t',\n",
       " '836a17a1-96e1-4a5d-8859-fd18850e427e': 'We call this way of generating experience and updates trajectory sampling. It is hard to imagine any eﬃcient way of distributing updates according to the on-policy distribution other than by trajectory sampling. If one had an explicit representation of the on-policy distribution, then one could sweep through all states, weighting the update of each according to the on-policy distribution, but this leaves us again with all the computational costs of exhaustive sweeps. Possibly one could sample and update individual state–action pairs from the distribution, but even if this could be done eﬃciently, what beneﬁt would this provide over simulating trajectories? Even knowing the on-policy distribution in an explicit form is unlikely.\\n\\nThe distribution changes whenever the policy changes, and computing the distribution requires computation comparable to a complete policy evaluation. Consideration of such other possibilities makes trajectory sampling seem both eﬃcient and elegant. Is the on-policy distribution of updates a good one? Intuitively it seems like a good choice, at least better than the uniform distribution. For example, if you are learning to play chess, you study positions that might arise in real games, not random positions of chess pieces',\n",
       " 'a7c99688-d9f3-449b-ac19-afc59b84c795': 'First, actions that had never been tried before from a state were allowed to be considered in the planning step (f) of the Tabular Dyna-Q algorithm in the box above. Second, the initial model for such actions was that they would lead back to the same state with a reward of zero. what happens during the second episode of the ﬁrst maze task (Figure 8.3). At the beginning of the second episode, only the state–action pair leading directly into the goal has a positive value; the values of all other pairs are still zero.\\n\\nThis means that it is pointless to perform updates along almost all transitions, because they take the agent from one zero-valued state to another, and thus the updates would have no e↵ect. Only an update along a transition into the state just prior to the goal, or from it, will change any values. If simulated transitions are generated uniformly, then many wasteful updates will be made before stumbling onto one of these useful ones. As planning progresses, the region of useful updates grows, but planning is still far less eﬃcient than it would be if focused where it would do the most good. In the much larger problems that are our real objective, the number of states is so large that an unfocused search would be extremely ineﬃcient',\n",
       " '31029187-388f-4f0f-beae-d213cf74b6cd': 'Many aspects of Deep Learning and neural network models draw comparisons with human intelligence. For example, a human intelligence anecdote of transfer learning is illustrated in learning music.\\n\\nIf two people are trying to learn how to play the guitar, and one already knows how to play the piano, it seems likely that the piano-player will learn to play the guitar faster. Analogous to learning music, a model that can classify Ima- geNet images will likely perform better on CIFAR-10 images than a model with random weights. Data Augmentation is similar to imagination or dreaming. Humans imagine differ- ent scenarios based on experience. Imagination helps us gain a better understanding of our world. Data Augmentation methods such as GANs and Neural Style Transfer can ‘imagine’ alterations to images such that they have a better understanding of them. The remainder of the paper is organized as follows: A brief “Background” is provided to give readers a historical context of Data Augmentation and Deep Learning. “Image Data Augmentation techniques” discusses each image augmentation technique in detail along with experimental results',\n",
       " 'f2e6ac80-7257-4179-8d82-e14ec287d8f7': 'An example of greedy supervised pretraining is illustrated in figure 8.7, in which each added hidden layer is pretrained as part of a shallow supervised MLP, taking as input the output of the previously trained hidden layer. Instead of pretraining one layer at a time, Simonyan and Zisserman  pretrain a deep convolutional network (eleven weight layers) and then use the first four and last three layers from this network to initialize even deeper networks (with up to nineteen layers of weights). The middle layers of the new, very deep network are initialized randomly. The new network is then jointly trained. Another option, explored by Yu et al.\\n\\n, is to use the outputs of the previously trained MLPs, as well as the raw input, as inputs for each added stage. Why would greedy supervised pretraining help? The hypothesis initially discussed by Bengio ef al. is that it helps to provide better guidance to the  319  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  https://www.deeplearningbook.org/contents/optimization.html  (c) (a)  Figure 8.7: Illustration of one form of greedy supervised pretraining',\n",
       " 'e9cdaaeb-0c6a-414b-b02e-a8bfbfa3c67c': 'We could explicitly search for a large set of basis functions that are all mutually different from each other, but this often incurs a noticeable computational cost.\\n\\nFor example, if we have at most aS many outputs as inputs, we could use Gram-Schmidt orthogonalization on an initial weight matrix and be guaranteed that each unit would compute a very different function from each other unit. Random initialization from a high-entropy distribution over a high-dimensional space is computationally cheaper and unlikely to assign any units to compute the same function as each other. Typically, we set the biases for each unit to heuristically chosen constants, and initialize only the weights randomly. Extra parameters—for example, parameters encoding the conditional variance of a prediction—are usually set to heuristically chosen constants much like the biases are. We almost always initialize all the weights in the model to values drawn randomly from a Gaussian or uniform distribution. The choice of Gaussian or uniform distribution does not seem to matter much but has not been exhaustively studied. The scale of the initial distribution, however, does have a large effect on both the outcome of the optimization procedure and the ability of the network to generalize',\n",
       " '31603194-6345-4ae6-a101-0e76be0ef306': '# £: encoder network  # lambda: weight on the off-diagonal terms  #N: batch size  # D: dimensionality of the representation  #  # mm: matrix-matrix multiplication  # off_diagonal: off-diagonal elements of a matrix # eye: identity matrix  for x in loader: # load a batch with N samples ## two randomly augmented versions of x y_a, y_b = augment (x) # compute representations za= f(y a) # NxD zb = f(y_b) # NxD # normalize repr.\\n\\nalong the batch dimension z_a_norm = (za - z_a.mean(0)) / z_a.std(0) # NxD z_b_norm = (z_b - z_b.mean(0)) / z_b.std(0) # NxD  # cross-correlation matrix  c = mm(z_a_norm.T, z_b_norm) / N # DxD  # loss  c_diff = (c - eye(D)).pow(2) # DxD  # multiply off-diagonal elems of c_diff by lambda off_diagonal (c_diff) .mul_ (lambda)  loss = c_diff.sum()  # optimization step  loss',\n",
       " 'c6973482-2237-424d-87ac-b8a24f9a66b2': 'This differs from Transfer Learning because in Transfer Learning, the network architecture such as VGG-16  or ResNet  must be transferred as well as the  weights. Pretraining enables the initialization of weights using big datasets, while still  enabling flexibility in network architecture design. + One-shot and Zero-shot learning  algorithms represent another paradigm for building models with extremely limited data. One-shot learning is commonly used in facial recognition applications .\\n\\nAn approach to one-shot learning is the use of siamese networks  that learn a distance function such that image classification is possible even if the network has only been trained on one or a few instances. Another very popular approach to one-shot learning is the use of memory-augmented net- works . Zero-shot learning is a more extreme paradigm in which a network uses input and output vector embeddings such as Word2Vec  or GloVe  to classify  images based on descriptive attributes. In contrast to the techniques mentioned above, Data Augmentation approaches overfitting from the root of the problem, the training dataset. This is done under the assumption that more information can be extracted from the original dataset through augmentations. These augmentations artificially inflate the training dataset size by either data warping or oversampling',\n",
       " '0d16fd10-de99-48bf-bc63-b3c32f374f55': 'This means that any individual agent has only limited ability to a↵ect the reward signal because any single agent contributes just one component of the collective action evaluated by the common reward signal. E↵ective learning in this scenario requires addressing a structural credit assignment problem: which team members, or groups of team members, deserve credit for a favorable reward signal, or blame for an unfavorable reward signal? It is a cooperative game, or a team problem, because the agents are united in seeking to increase the same reward signal: there are no conﬂicts of interest among the agents.\\n\\nThe scenario would be a competitive game if di↵erent agents receive di↵erent reward signals, where each reward signal again evaluates the collective action of the population, and the objective of each agent is to increase its own reward signal. In this case there might be conﬂicts of interest among the agents, meaning that actions that are good for some agents are bad for others. Even deciding what the best collective action should be is a non-trivial aspect of game theory. This competitive setting might be relevant to neuroscience too (for example, to account for heterogeneity of dopamine neuron activity), but here we focus only on the cooperative, or team, case',\n",
       " 'dcbaed06-5257-47b6-9215-020204b89020': 'end while  training procedure presented in algorithm 18.1.\\n\\nThe high cost of burning in the Markov chains in the inner loop makes this procedure computationally infeasible, but this procedure is the starting point that other more practical algorithms aim to approximate. We can view the MCMC approach to maximum likelihood as trying to achieve balance between two forces, one pushing up on the model distribution where the data occurs, and another pushing down on the model distribution where the model  https://www.deeplearningbook.org/contents/partition.html    samples occur. Figure 18.1 lustrates this process. ‘Lhe two torces correspond to maximizing log p and minimizing log Z. Several approximations to the negative phase are possible. Each of these approximations can be understood as making the negative phase computationally cheaper but also making it push down in the wrong locations. Because the negative phase involves drawing samples from the model’s distri- bution, we can think of it as finding points that the model believes in strongly. Because the negative phase acts to reduce the probability of those points, they are generally considered to represent the model’s incorrect beliefs about the world',\n",
       " 'db1097d0-2212-4a1f-9a2a-2cb74c4b1afd': 'We can similarly introduce priors into the Section 2.1.1 It is straightforward to extend the analysis of Bernoulli mixtures to the case of multinomial binary variables having M > 2 states by making use of the discrete disExercise 9.19 tribution (2.26). Again, we can introduce Dirichlet priors over the model parameters if desired. As a third example of the application of EM, we return to the evidence approximation for Bayesian linear regression. In Section 3.5.2, we obtained the reestimation equations for the hyperparameters α and β by evaluation of the evidence and then setting the derivatives of the resulting expression to zero. We now turn to an alternative approach for ﬁnding α and β based on the EM algorithm. Recall that our goal is to maximize the evidence function p(t|α, β) given by (3.77) with respect to α and β',\n",
       " '2540df49-6f03-47be-aa16-0cef07d487c1': \"\\\\ 12.26 (**) Show that any vector ai that satisfies (12.80) will also satisfy (12.79). Also, show that for any solution of (12.80) having eigenvalue A, we can add any multiple of an eigenvector of K having zero eigenvalue, and obtain a solution to (12.79) that also has eigenvalue A. Finally, show that such modifications do not affect the principal-component projection given by (12.82). 12.27 (* *) Show that the conventional linear PCA algorithm is recovered as a special case of kernel PCA if we choose the linear kernel function given by k(x, x') = x T x'.\\n\\n12.28 (* *) III!I Use the transformation property (1.27) of a probability density under a change of variable to show that any density p(y) can be obtained from a fixed density q(x) that is everywhere nonzero by making a nonlinear change of variable y = f(x) in which f(x) is a monotonic function so that 0 :::; j'(x) < 00\",\n",
       " '32d579eb-40b9-4c91-b36b-9577837b2502': 'We start by writing down the M th order term for a polynomial in D dimensions in the form The coefﬁcients wi1i2···iM comprise DM elements, but the number of independent parameters is signiﬁcantly fewer due to the many interchange symmetries of the factor xi1xi2 · · · xiM . Begin by showing that the redundancy in the coefﬁcients can be removed by rewriting this M th order term in the form Note that the precise relationship between the �w coefﬁcients and w coefﬁcients need not be made explicit.\\n\\nUse this result to show that the number of independent parameters n(D, M), which appear at order M, satisﬁes the following recursion relation Next use proof by induction to show that the following result holds which can be done by ﬁrst proving the result for D = 1 and arbitrary M by making use of the result 0! = 1, then assuming it is correct for dimension D and verifying that it is correct for dimension D + 1. Finally, use the two previous results, together with proof by induction, to show To do this, ﬁrst show that the result is true for M = 2, and any value of D ⩾ 1, by comparison with the result of Exercise 1.14',\n",
       " '41712a64-1828-40ec-86df-51578761d0dc': 'This makes sense since for paraphrasing tasks, augmenting the text usually consists of paraphrases, and so can easily change whether two texts are paraphrases of each other. Single Sentence Tasks. Based on the singlesentence tasks results in Table 3, hidden space augmentations (cutoff) provides the biggest boost in performance in supervised settings, while in semi-supervised settings, sentence level augmentations (roundtrip translation) works best. We note most augmentation methods hurt performance on CoLA, a task for judging grammatical acceptability. This could be caused by the fact that most of augmentation methods try to preserve meaning and not grammatical correctness. Overall, no single augmentation works the best for every task in the supervised or semisupervised setting. However, several overall conclusions can be made: ﬁrst, augmentation does not always improve performance, and can sometimes hurt performances, even in the semi-supervised setting. This suggests that we may need to design different augmentations for different tasks',\n",
       " 'e51f1f5f-e7b7-4f65-9890-199a833cd13f': 'Because it is independent of y(x), it represents the irreducible minimum value of the loss function. As with the classiﬁcation problem, we can either determine the appropriate probabilities and then use these to make optimal decisions, or we can build models that make decisions directly. Indeed, we can identify three distinct approaches to solving regression problems given, in order of decreasing complexity, by: (a) First solve the inference problem of determining the joint density p(x, t). Then normalize to ﬁnd the conditional density p(t|x), and ﬁnally marginalize to ﬁnd the conditional mean given by (1.89). (b) First solve the inference problem of determining the conditional density p(t|x), and then subsequently marginalize to ﬁnd the conditional mean given by (1.89). (c) Find a regression function y(x) directly from the training data. The relative merits of these three approaches follow the same lines as for classiﬁcation problems above. The squared loss is not the only possible choice of loss function for regression. Indeed, there are situations in which squared loss can lead to very poor results and where we need to develop more sophisticated approaches',\n",
       " '56d0e286-9a0c-4e26-ad54-5a6d7c69a658': 'Inevitably, however, the joint process is brought closer to the overall goal of optimality.\\n\\nThe arrows in this diagram correspond to the behavior of policy iteration in that each takes the system all the way to achieving one of the two goals completely. In GPI one could also take smaller, incomplete steps toward each goal. In either case, the two processes together achieve the overall goal of optimality even though neither is attempting to achieve it directly. DP may not be practical for very large problems, but compared with other methods for solving MDPs, DP methods are actually quite eﬃcient. If we ignore a few technical details, then the (worst case) time DP methods take to ﬁnd an optimal policy is polynomial in the number of states and actions. If n and k denote the number of states and actions, this means that a DP method takes a number of computational operations that is less than some polynomial function of n and k. A DP method is guaranteed to ﬁnd an optimal policy in polynomial time even though the total number of (deterministic) policies is kn',\n",
       " '05af5a1e-b3ec-4a2d-af05-550db75becfb': 'A. Cohn (Eds. ), Advances in Neural Information Processing Systems, Volume 11. MIT Press.\\n\\nJacobs, R. A., M. I. Jordan, S. J. Nowlan, and G. E. Hinton . Adaptive mixtures of local experts. Neural Computation 3(1), 79–87. Jaynes, E. T. Probability Theory: The Logic of Science. Cambridge University Press. Jebara, T. Machine Learning: Discriminative and Generative. Kluwer. Jeffries, H. An invariant form for the prior probability in estimation problems. Pro. Roy. Soc. AA 186, 453–461. Jensen, C., A. Kong, and U. Kjaerulff . Blocking gibbs sampling in very large probabilistic expert systems. International Journal of Human Computer Studies',\n",
       " '743b3837-2427-4f57-9773-cd78d9f18bb5': 'Hint: If c = 1, then the spike is less prominent. ⇤ So far in this chapter we have considered methods that estimate action values and use those estimates to select actions. This is often a good approach, but it is not the only one possible. In this section we consider learning a numerical preference for each action a, which we denote Ht(a). The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward.\\n\\nOnly the relative preference of one action over another is important; if we add 1000 to all the action preferences there is no e↵ect on the action probabilities, which are determined according to a soft-max distribution (i.e., Gibbs or Boltzmann distribution) as follows: where here we have also introduced a useful new notation, ⇡t(a), for the probability of taking action a at time t. Initially all action preferences are the same (e.g., H1(a) = 0, for all a) so that all actions have an equal probability of being selected. There is a natural learning algorithm for soft-max action preferences based on the idea of stochastic gradient ascent',\n",
       " '83e28796-cbd3-4a58-afd7-754a289e8b85': 'That is, at each time, the weights are updated by multiplying them with a factor that depends on the reward. 6.2. Dynamic SE as Interpolation of Algorithms. Besides the dynamic experience, other components in the SE, such as the divergence function D and the balancing weights (α, β), can also be indexed by the time τ with desired evolution. In particular, the previous sections have shown that the diﬀerent speciﬁcations of the SE components correspond to diﬀerent speciﬁc algorithms, many of which are well-known and have diﬀerent properties. The dynamic SE with the evolving speciﬁcations, therefore, can be seen as interpolating between the algorithms during the course of training.\\n\\nThe interpolation allows the learning to enjoy diﬀerent desired properties in diﬀerent training stages, resulting in improved eﬃcacy. As a concrete example, Tan et al. learn a text generation model by starting with the simple supervised MLE algorithm, with the experience function fτ=0 = fdata (Equation 4.2) and balancing weights (ατ=0 = 1, βτ=0 = ϵ) as described in Section 4.1.1',\n",
       " '880fae86-6a61-45b3-8415-169f895dff54': 'Of course, in the case of the RVM we always have the option of starting with a smaller number of basis functions than N + 1.\\n\\nMore signiﬁcantly, in the relevance vector machine the parameters governing complexity and noise variance are determined automatically from a single training run, whereas in the support vector machine the parameters C and ϵ (or ν) are generally found using cross-validation, which involves multiple training runs. Furthermore, in the next section we shall derive an alternative procedure for training the relevance vector machine that improves training speed signiﬁcantly. We have noted earlier that the mechanism of automatic relevance determination causes a subset of parameters to be driven to zero. We now examine in more detail set vector of target values given by t = (t1, t2)T, indicated by the cross, for a model with one basis vector ϕ = (φ(x1), φ(x2))T, which is poorly aligned with the target data vector t. On the left we see a model having only isotropic noise, so that C = β−1I, corresponding to α = ∞, with β set to its most probable value. On the right we see the same model but with a ﬁnite value of α',\n",
       " 'dcada4a5-5fa1-4250-b3c7-884398529feb': 'In addition to RGB versus grayscale images, there are many other ways of representing digital color such as HSV (Hue, Saturation, and Value). Jurio et al.\\n\\nexplore the performance of Image Segmentation on many dif- ferent color space representations from RGB to YUV, CMY, and HSV. Similar to geometric transformations, a disadvantage of color space transforma- tions is increased memory, transformation costs, and training time. Additionally, color transformations may discard important color information and thus are not always a label-preserving transformation. For example, when decreasing the pixel values of an image to simulate a darker environment, it may become impossible to see the objects in the image. Another indirect example of non-label preserving color transformations is in Image Sentiment Analysis . In this application, CNNs try to visually predict the sentiment score of an image such as: highly negative, nega- tive, neutral, positive, or highly positive. One indicator of a negative/highly negative image is the presence of blood. The dark red color of blood is a key component to distinguish blood from water or paint',\n",
       " '6843fa03-d4b3-4b58-9ac5-425fc75916b7': 'How can they learn about the optimal policy while behaving according to an exploratory policy? The on-policy approach in the preceding section is actually a compromise—it learns action values not for the optimal policy, but for a near-optimal policy that still explores. A more straightforward approach is to use two policies, one that is learned about and that becomes the optimal policy, and one that is more exploratory and is used to generate behavior. The policy being learned about is called the target policy, and the policy used to generate behavior is called the behavior policy. In this case we say that learning is from data “o↵” the target policy, and the overall process is termed o↵-policy learning. Throughout the rest of this book we consider both on-policy and o↵-policy methods. On-policy methods are generally simpler and are considered ﬁrst. O↵-policy methods require additional concepts and notation, and because the data is due to a di↵erent policy, o↵-policy methods are often of greater variance and are slower to converge. On the other hand, o↵-policy methods are more powerful and general.\\n\\nThey include on-policy methods as the special case in which the target and behavior policies are the same',\n",
       " '0d90fc99-99e3-4f74-841c-40036303f835': 'Today, unsupervised pretraining has been largely abandoned, except in the field of natural language processing, where the natural representation of words as one-hot vectors conveys no similarity information and where very large unlabeled sets are available.\\n\\nIn that case, the advantage of pretraining is that one can pretrain once on a huge unlabeled set (for example with a corpus containing billions of words), learn a good representation (typically of words, but also of sentences), and then use this representation or fine-tune it for a supervised task for which the training set contains substantially fewer examples. This approach was pioneered by Collobert and Weston , Turian et al. , and Collobert et al. and remains in common use today. Deep learning techniques based on supervised learning, regularized with dropout or batch normalization, are able to achieve human-level performance on many tasks, but only with extremely large labeled datasets. These same techniques outperform unsupervised pretraining on medium-sized datasets such as CIFAR-10 and MNIST, which have roughly 5,000 labeled examples per class',\n",
       " 'ee0bab45-aef9-42c4-9877-929be081f415': 'Specifically, on each step  con  https://www.deeplearningbook.org/contents/ml.html    of the algorithm, we can sample a minibatch of examples B= ={aY,..., 0\") drawn uniformly from the training set. The minibatch size m’ is typically \"chosen to be a relatively small number of examples, ranging from one to a few hundred. Crucially, m‘ is usually held fixed as the training set size m grows. We may fit a training set with billions of examples using updates computed on only a hundred examples. The estimate of the gradient is formed as  1 = 4 4 g = mV OD L(2,y 6) (5.98)  149  CHAPTER 5. MACHINE LEARNING BASICS  using examples from the minibatch B.\\n\\nThe stochastic gradient descent algorithm then follows the estimated gradient downhill:  6+ 6-cg, (5.99)  where ¢ is the learning rate. Gradient descent in general has often been regarded as slow or unreliable. In the past, the application of gradient descent to nonconvex optimization problems was regarded as foolhardy or unprincipled',\n",
       " 'd5d468d3-01cf-48f3-9453-d2da8b094c6f': 'Understanding data augmentation for classification: when to warp? CORR, abs/1609.08764, 2016. Seyed-Mohsen MD, Alhussein F, Pascal F. DeepFool: a simple and accurate method to fool deep neural networks.\\n\\narXiv preprint. 2016. Jiawei S, Danilo VV, Sakurai K. One pixel attack for fooling deep neural networoks. arXiv preprints. 2018. Michal Z, Konrad Z, Negar R, Pedro OP. Adversarial framing for image and video classification. arXiv preprints. 2018. Logan E, Brandon T, Dimitris T, Ludwig S, Aleksander M. A rotation and a translation suffice: fooling CNNs with simple transformations. ArXiv preprint. 2018. Goodfellow |, Shlens J, Szegedy C. Explaining and Harnessing Adversarial Examples. International Conference on Learning Representations, 2015',\n",
       " '026b1f1d-c0dd-4d22-8859-2b3fa323dddf': 'For x < 0, we have f’(x) For « > 0, we have f’(x) > so we can decrease f by so we can decrease f by i i moving leftward. Figure 4.1: Gradient descent. An illustration of how the gradient descent algorithm uses the derivatives of a function to follow the function downhill to a minimum. We assume the reader is already familiar with calculus but provide a brief review of how calculus concepts relate to optimization here. Suppose we have a function y = f(x), where both x and y are real numbers. The derivative of this function is denoted as f’(a) or as S¥.\\n\\nThe derivative f’ (x) gives the slope of f(x) at the point x. In other words, it if how to scale a small change in the input to obtain the corresponding change in the output:  \"(a). f (a Fhe) derifatjve és therefore useful for minimizing a function because it tells us  https://www.deeplearningbook.org/contents/numerical.html    how to change 7 in order to make a small improvement in ¥',\n",
       " '469cbe57-3b57-4424-b999-3ed9334d485e': 'In the unsupervised learning context, it makes sense for some of the top-level factors to be associated with none of the output tasks (A): these are the factors that explain some of the input variations but are not relevant for predicting y\") or y®). 242  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  0.20 e— Training set lo — Validation set loss  £ S a  Loss (negative log-likelihood) oO S  £ f=) 6  0 50 100 150 200 250 Time (epochs)  Figure 7.3: Learning curves showing how the negative log-likelihood loss changes over time (indicated as number of training iterations over the dataset, or epochs). In this example, we train a maxout network on MNIST. Observe that the training objective decreases consistently over time, but the validation set average loss eventually begins to increase again, forming an asymmetric U-shaped curve. https://www.deeplearningbook.org/contents/regularization.html    validation set error begins to rise again. See figure 7.3 for an example of this behavior, which occurs reliably',\n",
       " 'f8c23f66-ab0e-43cc-a778-fdcf0818f12e': 'Our quadratic approximation of the L! regularized objective function decom- poses into a sum over the parameters:  , 1 J(w:X,y)=J(wsX,y)+ — GHia(wi-w7)’ + alwil (7.22)  a  The problem of minimizing this spprodte cost function has an anata solution (for each dimension 7), with the following form:  https://www.deeplearningbook.org/contents/regularization.html    wi = sign(ui ) max ie - Fh “Ol. i,t (e)  Consider the situation where wy > 0 f  (7.23)  all i. There are two possible outcomes: ow Hii objective is simply w; = 0. This occurs because the contribution of J(w; X, y) to the regularized objective J(w; X,y) is overwhelmed—in direction i—by the L! regularization, which pushes the value of w; to zero. 1. The case where w; < Here the optimal value of w; under the regularized  2. The case where w; > +~',\n",
       " '11d215b2-c8eb-40c4-b9aa-58cd10908389': \"The model has two major parts, transition probability function P and reward function R.  Let's say when we are in state s, we decide to take action a to arrive in the next state s' and obtain reward r. This is known as one transition step, represented by a tuple (s, a, $', r). The transition function P records the probability of transitioning from state s to s' after taking action a while obtaining reward r. We use P as a symbol of “probability”.\\n\\nP(s',r|s,a) = P  Thus the state-transition function can be defined as a function of P(s’, r|s, a):  P2, = P(s'|s,a) = P = )- P(s',r|s, a) rER  The reward function R predicts the next reward triggered by one action:  R(s,a) = E = Sor > P(s',r|s, a)  reR s'cS  Policy Policy, as the agent's behavior function 77, tells us which action to take in state s\",\n",
       " 'bc872bdb-af2a-4311-8ec2-470961548da2': 'Consider a Gaussian mixture model in which the covariance matrices of the mixture components are given by ϵI, where ϵ is a variance parameter that is shared by all of the components, and I is the identity matrix, so that We now consider the EM algorithm for a mixture of K Gaussians of this form in which we treat ϵ as a ﬁxed constant, instead of a parameter to be re-estimated. From (9.13) the posterior probabilities, or responsibilities, for a particular data point xn, are given by If we consider the limit ϵ → 0, we see that in the denominator the term for which ∥xn − µj∥2 is smallest will go to zero most slowly, and hence the responsibilities γ(znk) for the data point xn all go to zero except for term j, for which the responsibility γ(znj) will go to unity',\n",
       " 'ac63a52d-738b-43e9-8e7c-11fafd8ace5e': 'P., & Hu, Z. RLPrompt: Optimizing discrete text prompts with reinforcement learning. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding.\\n\\nProceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4171–4186. Domingos, P. The master algorithm: How the quest for the ultimate learning machine will remake our world. Basic Books. Ertekin, S., Huang, J., Bottou, L., & Giles, L. Learning on the border: Active learning in imbalanced data classiﬁcation. Proceedings of the sixteenth ACM conference on Conference on Information and Knowledge Management, 127–136. Farnia, F., & Tse, D. A convex duality framework for GANs. Advances in Neural Information Processing Systems, 31, 5248–5258',\n",
       " 'da6555b9-1e54-4c83-ba70-199d6014ed24': 'Noise-contrastive estimation (NCE)  involves training a generative model by learning the weights that make the model useful for discriminating data from a ﬁxed noise distribution. Using a previously trained model as the noise distribution allows training a sequence of models of increasing quality. This can be seen as an informal competition mechanism similar in spirit to the formal competition used in the adversarial networks game. The key limitation of NCE is that its “discriminator” is deﬁned by the ratio of the probability densities of the noise distribution and the model distribution, and thus requires the ability to evaluate and backpropagate through both densities. Some previous work has used the general concept of having two neural networks compete. The most relevant work is predictability minimization .\\n\\nIn predictability minimization, each hidden unit in a neural network is trained to be different from the output of a second network, which predicts the value of that hidden unit given the value of all of the other hidden units. This work differs from predictability minimization in three important ways: 1) in this work, the competition between the networks is the sole training criterion, and is sufﬁcient on its own to train the network',\n",
       " '1f61f6bb-1bda-4ea4-88e2-77ea34fdb498': 'Adversarial attacks is an increasingly important research topic as they reveal models’ vulnerabilities and ﬂaws. This is especially true for universal attacks , where we want to generate universal examples that trick the model on all possible inputs. For instance, consider the context of entailment classiﬁcation. Our goal is to ﬁnd universal humanreadable hypotheses that are going to be classiﬁed as “entailment” with as high probability as possible, regardless of the input premises. This is a more challenging setting compared to previous instancespeciﬁc attack  where the attack model conditions on a premise and generates an adversarial hypothesis speciﬁc to the premise. Setup (more in §A.2.2). We aim to attack one of the most popular MultiNLI  entailment classiﬁers on HuggingFaceHub.4 The attack generation model generates adversarial text without conditioning on any inputs so that the generated attacks are universal to all premises. We compare our SQL with MLE+PG. We use all hypotheses in the MultiNLI dataset as the training data for the MLE training in MLE+PG and the offpolicy updates for our SQL',\n",
       " '452a7f63-fe9b-4916-a1b4-47bafed3c04d': 'is maximized by MAP Bayesian inference when the prior is an isotropic Laplace distribution (equation 3.26) over w € R™  log p(w = dilog Laplace(w 4; 0, *) = —a||w]|1 + nlog a — nlog 2. (7.24)  From the point of view of learning via maximization with respect to w, we can ignore the log a — log 2 terms because they do not depend on w.  7.2 Norm Penalties as Constrained Optimization  Consider the cost function regularized by a parameter norm penalty: J(O;X,y) = JO; X,y) + aX(8). (7.25)  Recall from section 4.4 that we can minimize a function subject to constraints by constructing a generalized Lagrange function, consisting of the original objective function plus a set of penalties. Each penalty is a product between a coefficient, called a Karush-Kuhn—Tucker (KKT) multiplier, and a function representing whether the constraint is satisfied',\n",
       " 'c1039624-dc1d-4415-8507-9fe288c0f942': 'The discount rate γ thus has no e↵ect on the problem formulation. It could in fact be zero and the ranking would be unchanged. This surprising fact is proven in the box on the next page, but the basic idea can be seen via a symmetry argument. Each time step is exactly the same as every other. With discounting, every reward will appear exactly once in each position in some return. The tth reward will appear undiscounted in the t − 1st return, discounted once in the t − 2nd return, and discounted 999 times in the t − 1000th return. The weight on the tth reward is thus 1 + γ + γ2 + γ3 + · · · = 1/(1 − γ). Because all states are the same, they are all weighted by this, and thus the average of the returns will be this times the average reward, or r(⇡)/(1 − γ)',\n",
       " 'f9e9b85e-cc2d-4e58-9f0a-41e174799fe7': 'Sutton and Barto  contains the earliest recognition of the near identity between the Rescorla– Wagner model and the Least-Mean-Square (LMS), or Widrow-Ho↵, learning rule . This early model was revised following Sutton’s development of the TD algorithm  and was ﬁrst presented as the TD model in Sutton and Barto  and more completely in Sutton and Barto , upon which this section is largely based. Additional exploration of the TD model and its possible neural implementation was conducted by Moore and colleagues .\\n\\nKlopf’s  drive-reinforcement theory of classical conditioning extends the TD model to address additional experimental details, such as the S-shape of acquisition curves. In some of these publications TD is taken to mean Time Derivative instead of Temporal Di↵erence. 14.2.4 Ludvig, Sutton, and Kehoe  evaluated the performance of the TD model in previously unexplored tasks involving classical conditioning and examined the inﬂuence of various stimulus representations, including the microstimulus representation that they introduced earlier . Earlier investigations of the inﬂuence of various stimulus representations and their possible neural implementations on response timing and topography in the context of the TD model are those of Moore and colleagues cited above',\n",
       " 'd391a368-efd7-4037-9888-f91967703b35': 'grading, on the layer’s output into a gradient on the pre-  https://www.deeplearningbook.org/contents/mlp.html    nonlinearity activation (element-wise multiplication if f is element-wise): 9 Van J =90 f(a) Compute gradients on weights and biases (including the regularization term, where needed): Vows = 9 + dV yon Q(9) Vwwd =g REY + Vy (8) Propagate the gradients w.r.t. the next lower-level hidden layer’s activations: g9— Viye-y J = WT g  end for  6.5.5 Symbol-to-Symbol Derivatives  Algebraic expressions and computational graphs both operate on symbols, or variables that do not have specific values. These algebraic and graph-based representations are called symbolic representations. When we actually use or train a neural network, we must assign specific values to these symbols. We replace a symbolic input to the network ax with a specific numeric value, such as \"',\n",
       " '010c9e87-fb55-43a3-9e4d-565947cc402f': '(20.15)  A similar derivation will show that the other condition of interest to us, P(v | h), is also a factorial distribution:  P(v|h) = Ile ((2v— 1) @ (b+ Wh)),.\\n\\n(20.16) i=1  20.2.2 Training Restricted Boltzmann Machines  Because the RBM admits efficient evaluation and differentiation of P(v) and efficient MCMC sampling in the form of block Gibbs sampling, it can readily be trained with any of the techniques described in chapter 18 for training models that have intractable partition functions. This includes CD, SML (PCD), ratio matching, and so on. Compared to other undirected models used in deep learning, the RBM is relatively straightforward to train because we can compute P(h | v)  656  CHAPTER 20. DEEP GENERATIVE MODELS  exactly in closed form. Some other deep models, such as the deep Boltzmann machine, combine both the difficulty of an intractable partition function and the difficulty of intractable inference. 20.3 Deep Belief Networks  Deep belief networks (DBNs) were one of the first nonconvolutional models to successfully admit training of deep architectures',\n",
       " '2f9fab5e-e7b5-403e-b406-4adb954d07cb': 'Thus these fixed-point update equations define an iterative algorithm where we alternate updates of A (using equation 20.33) and updates of A?) (using equation 20.34).\\n\\nOn small problems such as MNIST, as few as ten iterations can be sufficient to find an approximate positive phase gradient for learning, and fifty usually suffice to obtain a high-quality representation of a single specific example to be used for high-accuracy classification. Extending approximate variational inference to deeper DBMs is straightforward. 20.4.3 DBM Parameter Learning  Learning in the DBM must confront both the challenge of an intractable partition function, using the techniques from chapter 18, and the challenge of an intractable posterior distribution, using the techniques from chapter 19.  https://www.deeplearningbook.org/contents/generative_models.html    As described In section ZU.4.Z, varlational interence allows the construction ot a distribution Qh | v) v) that a apmpates the intractable Mh | v). Learning then proceeds by a atliene Liv, , the variational lower bound on the intractable log-likelihood, log P(v; @). 665  CHAPTER 20',\n",
       " 'cbaf5194-c69e-4f13-be84-25875e1685a1': 'There are thus a factorial number of complete graphs for every set of random variables. In this example, we order the variables from left to right, top to bottom. Directed models and undirected models both have their advantages and disad- vantages. Neither approach is clearly superior and universally preferred. Instead, we should choose which language to use for each task. This choice will partially depend on which probability distribution we wish to describe. We may choose to use either directed modeling or undirected modeling based on which approach can  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    capture the most dependences Im the probability distribution or which approach uses the fewest edges to describe the distribution. Other factors can affect the decision of which language to use. Even while working with a single probabil- ity distribution, we may sometimes switch between different modeling languages',\n",
       " '16293ffa-27df-417d-b3bf-5a544aa66c28': 'Because we now have a probabilistic model, these are expressed in terms of the predictive distribution that gives the probability distribution over t, rather than simply a point estimate, and is obtained by substituting the maximum likelihood parameters into (1.60) to give Now let us take a step towards a more Bayesian approach and introduce a prior distribution over the polynomial coefﬁcients w. For simplicity, let us consider a Gaussian distribution of the form where α is the precision of the distribution, and M +1 is the total number of elements in the vector w for an M th order polynomial. Variables such as α, which control the distribution of model parameters, are called hyperparameters.\\n\\nUsing Bayes’ theorem, the posterior distribution for w is proportional to the product of the prior distribution and the likelihood function We can now determine w by ﬁnding the most probable value of w given the data, in other words by maximizing the posterior distribution. This technique is called maximum posterior, or simply MAP',\n",
       " '265b5b55-d182-475b-ac02-87680c771137': 'This may mean that we maximize che log-likelihood  log p(y? | #,...,@), (10.29) or, if the model includes connections from the output at one time step to the next ‘ime step,  logp(y |w,...,a yO)... yf). (10.30)  Decomposing the joint probability over the sequence of y values as a series of one-step probabilistic predictions is one way to capture the full joint distribution across the whole sequence. When we do not feed past y values as inputs that condition the next step prediction, the directed graphical model contains no edges from any y® in the past to the current y, In this case, the outputs y are conditionally independent given the sequence of a values. When we do feed the actual y values (not their prediction, but the actual observed or generated values) back into the network, the directed graphical model contains edges from all y values in the past to the current y® value.\\n\\nAs a simple example, let us consider the case where the RNN models only a sequence of scalar random variables Y = fy, yO }, with no additional inputs x',\n",
       " 'c76cbdab-9ddc-4062-bdae-a5b096c51c59': 'Because the manifold in GTM is defined as a continuous surface, not just at the prototype vectors as in the SOM, it is possible to compute the magnification factors corresponding to the local expansions and compressions of the manifold needed to fit the data set  as well as the directional curvatures of the manifold .\\n\\nThese can be visualized along with the projected data and provide additional insight into the model. Exercises 12.1 (* *) lIB In this exercise, we use proof by induction to show that the linear projection onto an M -dimensional subspace that maximizes the variance of the projected data is defined by the M eigenvectors of the data covariance matrix S, given by (12.3), corresponding to the M largest eigenvalues. In Section 12.1, this result was proven for the case of M = 1. Now suppose the result holds for some general value of M and show that it consequently holds for dimensionality M + 1. To do this, first set the derivative of the variance of the projected data with respect to a vector UM+1 defining the new direction in data space equal to zero',\n",
       " '6b9de3b6-a309-4526-880b-9eb4d2ead173': 'With continuous policy parameterization the action probabilities change smoothly as a function of the learned parameter, whereas in \"-greedy selection the action probabilities may change dramatically for an arbitrarily small change in the estimated action values, if that change results in a di↵erent action having the maximal value. Largely because of this, stronger convergence guarantees are available for policy-gradient methods than for action-value methods. In particular, it is the continuity of the policy dependence on the parameters that enables policy-gradient methods to approximate gradient ascent (13.1). The episodic and continuing cases deﬁne the performance measure, J(✓), di↵erently and thus have to be treated separately to some extent. Nevertheless, we will try to present both cases uniformly, and we develop a notation so that the major theoretical results can be described with a single set of equations. In this section we treat the episodic case, for which we deﬁne the performance measure as the value of the start state of the episode.\\n\\nWe can simplify the notation without losing any meaningful generality by assuming that every episode starts in some particular (non-random) state s0',\n",
       " 'c19adbe2-3704-4edf-973b-ce7c00d710bc': 'Note that Mt, like δt, is not really an additional memory variable. It can be removed from the algorithm by substituting its deﬁnition into the eligibility-trace equation. Pseudocode and software for the true online version of Emphatic-TD(λ) are available on the web . In the on-policy case (⇢t = 1, for all t), Emphatic-TD(λ) is similar to conventional TD(λ), but still signiﬁcantly di↵erent. In fact, whereas Emphatic-TD(λ) is guaranteed to converge for all state-dependent λ functions, TD(λ) is not. TD(λ) is guaranteed convergent only for all constant λ. See Yu’s counterexample . It might at ﬁrst appear that tabular methods using eligibility traces are much more complex than one-step methods.\\n\\nA naive implementation would require every state (or state–action pair) to update both its value estimate and its eligibility trace on every time step. This would not be a problem for implementations on single-instruction, multipledata, parallel computers or in plausible artiﬁcial neural network (ANN) implementations, but it is a problem for implementations on conventional serial computers',\n",
       " 'aa9f7e23-e776-4bbf-b619-6d6c25b24e41': 'A ﬁnal issue that demands attention in future research is that of developing methods to make it acceptably safe to embed reinforcement learning agents into physical environments. This is one of the most pressing areas for future research, and we discuss it further in the When we were writing the ﬁrst edition of this book in the mid-1990s, artiﬁcial intelligence was making signiﬁcant progress and was having an impact on society, though it was mostly still the promise of artiﬁcial intelligence that was inspiring developments. Machine learning was part of that outlook, but it had not yet become indispensable to artiﬁcial intelligence.\\n\\nBy today that promise has transitioned to applications that are changing the lives of millions of people, and machine learning has come into its own as a key technology. As we write this second edition, some of the most remarkable developments in artiﬁcial intelligence have involved reinforcement learning, most notably “deep reinforcement learning”—reinforcement learning with function approximation by deep artiﬁcial neural networks. We are at the beginning of a wave of real-world applications of artiﬁcial intelligence, many of which will include reinforcement learning, deep and otherwise, that will impact our lives in ways that are hard to predict. But an abundance of successful real-world applications does not mean that true artiﬁcial intelligence has arrived',\n",
       " '623477e5-00ac-4964-be48-b2b660ecacec': 'For instance, we might seek a bound on a conditional distribution p(y|x), which is itself just one factor in a much larger probabilistic model speciﬁed by a directed graph. The purpose of introducing the bound of course is to simplify the resulting distribution. This local approximation can be applied to multiple variables in turn until a tractable approximation is obtained, and in Section 10.6.1 we shall give a practical example of this approach in the context of logistic regression. Here we focus on developing the bounds themselves. We have already seen in our discussion of the Kullback-Leibler divergence that the convexity of the logarithm function played a key role in developing the lower bound in the global variational approach. We have deﬁned a (strictly) convex function as one for which every chord lies above the function. Convexity also plays a Section 1.6.1 central role in the local variational framework',\n",
       " '86504662-f570-4288-aa50-e32dc8ec33c4': 'Sometimes a different language becomes more appropriate if we observe a certain subset of variables, or if we wish to perform a different computational task.\\n\\nFor example, the directed model description often provides a straightforward approach to efficiently draw samples from the model (described in section 16.3), while the undirected model formulation is often useful for deriving approximate inference procedures (as we will see in chapter 19, where the role of undirected models is highlighted in equation 19.56). Every probability distribution can be represented by either a directed model or an undirected model. In the worst case, one can always represent any distribution by using a “complete graph.” For a directed model, the complete graph is any directed acyclic graph in which we impose some ordering on the random variables, and each variable has all other variables that precede it in the ordering as its ancestors in the graph. For an undirected model, the complete graph is simply a graph containing a single clique encompassing all the variables. See figure 16.10 for an example. 573  CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  Of course, the utility of a graphical model is that the graph implies that some variables do not interact directly',\n",
       " '69966ac1-ff29-4cea-9288-2960be99751c': 'From (C.38), it follows that and because Λ is a diagonal matrix, we say that the matrix A is diagonalized by the matrix U. If we left multiply by U and right multiply by UT, we obtain These last two equations can also be written in the form If we take the determinant of (C.43), and use (C.12), we obtain Similarly, taking the trace of (C.43), and using the cyclic property (C.8) of the trace operator together with UTU = I, we have We leave it as an exercise for the reader to verify (C.22) by making use of the results (C.33), (C.45), (C.46), and (C.47). A matrix A is said to be positive deﬁnite, denoted by A ≻ 0, if wTAw > 0 for all values of the vector w. Equivalently, a positive deﬁnite matrix has λi > 0 for all of its eigenvalues (as can be seen by setting w to each of the eigenvectors in turn, and by noting that an arbitrary vector can be expanded as a linear combination of the eigenvectors)',\n",
       " '55ae77b9-4c86-4c1e-904e-ea085e417f0b': 'The most important form of this is when planning is done at decision time, that is, as part of the action-selection process. Classical heuristic search as studied in artiﬁcial intelligence is an example of this. Other examples are rollout algorithms and Monte Carlo Tree Search that beneﬁt from online, incremental, sample-based value estimation and policy improvement. This chapter concludes Part I of this book. In it we have tried to present reinforcement learning not as a collection of individual methods, but as a coherent set of ideas cutting across methods.\\n\\nEach idea can be viewed as a dimension along which methods vary. The set of such dimensions spans a large space of possible methods. By exploring this space at the level of dimensions we hope to obtain the broadest and most lasting understanding. In this section we use the concept of dimensions in method space to recapitulate the view of reinforcement learning developed so far in this book',\n",
       " '6a642fed-b5f6-4ceb-a1ca-9a6ca2fee51b': 'Note that any elements of π or A that are set to zero initially will remain zero in subsequent EM updates. A typical initialization procedure would Exercise 13.6 involve selecting random starting values for these parameters subject to the summation and non-negativity constraints. Note that no particular modiﬁcation to the EM results are required for the case of left-to-right models beyond choosing initial values for the elements Ajk in which the appropriate elements are set to zero, because these will remain zero throughout. To maximize Q(θ, θold) with respect to φk, we notice that only the ﬁnal term in (13.17) depends on φk, and furthermore this term has exactly the same form as the data-dependent term in the corresponding function for a standard mixture distribution for i.i.d. data, as can be seen by comparison with (9.40) for the case of a Gaussian mixture. Here the quantities γ(znk) are playing the role of the responsibilities',\n",
       " '2901b719-ee5f-4d4b-9d41-9f4725194911': 'For example, the Laplace prior parametrized in terms of the sparsity penalty coefficient » is given by  2 Xr p(hi) = Laplace(hi; 0, ~) = cen , (13.13) and the Student t prior by  1 _ (13.14)  Phi) & —aaae (1+ +) 2  Training sparse coding with maximum likelihood is intractable. Instead, the training alternates between encoding the data and training the decoder to better  492  CHAPTER 13. LINEAR FACTOR MODELS  reconstruct the data given the encoding. This approach will be justified further as a principled approximation to maximum likelihood later, in section 19.3. For models such as PCA, we have seen the use of a parametric encoder function that predicts h and consists only of multiplication by a weight matrix. The encoder that we use with sparse coding is not a parametric encoder',\n",
       " '159a8471-e037-4fdb-a556-385d51c40bd0': 'Gibbs  proposes a method for constructing a Gaussian distribution that is conjectured to be a bound (although no rigorous proof is given), which may be used to apply local variational methods to multiclass problems. We shall see an example of the use of local variational bounds in Sections 10.6.1. For the moment, however, it is instructive to consider in general terms how these bounds can be used. Suppose we wish to evaluate an integral of the form where σ(a) is the logistic sigmoid, and p(a) is a Gaussian probability density. Such integrals arise in Bayesian models when, for instance, we wish to evaluate the predictive distribution, in which case p(a) represents a posterior parameter distribution.\\n\\nBecause the integral is intractable, we employ the variational bound (10.144), which we write in the form σ(a) ⩾ f(a, ξ) where ξ is a variational parameter. The integral now becomes the product of two exponential-quadratic functions and so can be integrated analytically to give a bound on I We now have the freedom to choose the variational parameter ξ, which we do by ﬁnding the value ξ⋆ that maximizes the function F(ξ)',\n",
       " 'c4937b90-9e93-4702-8170-b0b5b84fe5ba': 'The policy gradient theorem for the episodic case establishes that where the gradients are column vectors of partial derivatives with respect to the components of ✓, and ⇡ denotes the policy corresponding to parameter vector ✓. The symbol / here means “proportional to”. In the episodic case, the constant of proportionality is the average length of an episode, and in the continuing case it is 1, so that the relationship is actually an equality. The distribution µ here (as in Chapters 9 and 10) is the on-policy distribution under ⇡ (see page 199).\\n\\nThe policy gradient theorem is proved for the episodic case in the box on the previous page. We are now ready to derive our ﬁrst policy-gradient learning algorithm. Recall our overall strategy of stochastic gradient ascent (13.1), which requires a way to obtain samples such that the expectation of the sample gradient is proportional to the actual gradient of the performance measure as a function of the parameter. The sample gradients need only be proportional to the gradient because any constant of proportionality can be absorbed into the step size ↵, which is otherwise arbitrary',\n",
       " 'baa3acca-9f58-4913-b88f-74ceefca17eb': '(20.75) 691  CHAPTER 20.\\n\\nDEEP GENERATIVE MODELS  Both approaches define a distribution p,(a) and allow us to train various criteria of pg using the reparametrization trick of section 20.9. The two different approaches to formulating generator nets—emitting the parameters of a conditional distribution versus directly emitting samples—have complementary strengths and weaknesses. When the generator net defines a conditional distribution over a, it is capable of generating discrete data as well as continuous data. When the generator net provides samples directly, it is capable of generating only continuous data (we could introduce discretization in the forward propagation, but doing so would mean the model could no longer be trained using back-propagation). The advantage to direct sampling is that we are no longer forced to use conditional distributions whose form can be easily written down and algebraically manipulated by a human designer. Approaches based on differentiable generator networks are motivated by the success of gradient descent applied to differentiable feedforward networks for classification. In the context of supervised learning, deep feedforward networks trained with gradient-based learning seem practically guaranteed to succeed given enough hidden units and enough training data',\n",
       " '29e544ed-55b1-4aa3-b035-f8b5740fdb75': 'However, a deep neural network trained on probabilistic training labels from Snorkel correctly identiﬁes it as a true mention. Snorkel provides connectors for popular machine learning libraries such as TensorFlow , allowing users to exploit commodity models like deep neural networks that do not require hand-engineering of features and have robust predictive performance across a wide range of tasks.\\n\\nWe study the fundamental question of when—and at what level of complexity—we should expect Snorkel’s generative model to yield the greatest predictive performance gains. Understanding these performance regimes can help guide users and introduces a trade-off space between predictive performance and speed. We characterize this space in two parts: ﬁrst, by analyzing when the generative model can be approximated by an unweighted majority vote, and second, by automatically selecting the complexity of the correlation structure to model. We then introduce a two-stage, rule-based optimizer to support fast development cycles. The natural ﬁrst question when studying systems for weak supervision is, “When does modeling the accuracies of sources improve end-to-end predictive performance?” We study that question in this subsection and propose a heuristic to identify settings in which this modeling step is most beneﬁcial',\n",
       " '3dce083d-613f-4bc5-816c-0e63128275ce': 'The idea that stimuli produce after e↵ects in the nervous system that are important for learning is very old (see Chapter 14). Some of the earliest uses of eligibility traces were in the actor–critic methods discussed in Chapter 13 . The λ-return and its error-reduction properties were introduced by Watkins  and further developed by Jaakkola, Jordan, and Singh .\\n\\nThe random walk results in this and subsequent sections are new to this text, as are the terms “forward view” and “backward view.” The notion of a λ-return algorithm was introduced in the ﬁrst edition of this text. The more reﬁned treatment presented here was developed in conjunction with Harm van Seijen . 12.2 TD(λ) with accumulating traces was introduced by Sutton . Convergence in the mean was proved by Dayan , and with probability 1 by many researchers, including Peng , Dayan and Sejnowski , Tsitsiklis , and Gurvits, Lin, and Hanson . The bound on the error of the asymptotic λ-dependent solution of linear TD(λ) is due to Tsitsiklis and Van Roy',\n",
       " '124193b9-1fb6-4caf-919c-146f53a519ae': 'Instead, the MP-DBM treats it as a latent variable to be inferred. One could imagine applying dropout to the MP-DBM by additionally removing some units  https://www.deeplearningbook.org/contents/generative_models.html   rather than making them latent. 20.5 Boltzmann Machines for Real-Valued Data  While Boltzmann machines were originally developed for use with binary data, many applications such as image and audio modeling seem to require the ability to represent probability distributions over real values. In some cases, it is possible to treat real-valued data in the interval  as representing the expectation of a binary variable. For example, Hinton  treats grayscale images in the training set as defining  probability values. Each pixel defines the probability of a binary value being 1, and the binary pixels are all sampled independently from each other. This is a common procedure for evaluating binary models on grayscale image datasets. Nonetheless, it is not a particularly theoretically satisfying approach, and binary images sampled independently in this way have a noisy appearance',\n",
       " '7ddd2e3f-70f1-4f2a-835d-19658eebf78e': 'Each component i of the eligibility-trace vector zt increments or decrements according Here λ is the usual eligibility trace decay parameter.\\n\\nNote that if γ = 0, the TD model reduces to the Rescorla–Wagner model with the exceptions that: the meaning of t is di↵erent in each case (a trial number for the Rescorla–Wagner model and a time step for the TD model), and in the TD model there is a one-time-step lead in the prediction target R. The TD model is equivalent to the backward view of the semi-gradient TD(λ) algorithm with linear function approximation (Chapter 12), except that Rt in the model does not have to be a reward signal as it does when the TD algorithm is used to learn a value function for policy-improvement. Real-time conditioning models like the TD model are interesting primarily because they make predictions for a wide range of situations that cannot be represented by trial-level models. These situations involve the timing and durations of conditionable stimuli, the timing of these stimuli in relation to the timing of the US, and the timing and shapes of CRs',\n",
       " '4999c522-39ca-4d8d-86c6-5e41ec5823b4': 'Association for Computational Linguistics. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586. Oﬁr Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. 2018.\\n\\nTrust-PCL: An off-policy trust region method for continuous control. In International Conference on Learning Representations. Karthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. 2015. Language understanding for text-based games using deep reinforcement learning. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1–11. Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2020. Adversarial nli: A new benchmark for natural language understanding',\n",
       " 'ed032e76-97fd-45d1-8c84-3e6479d23eac': 'Barto and Anandan  introduced a stochastic version of Widrow et al.’s  selective bootstrap algorithm called the associative reward-penalty (AR−P ) algorithm.\\n\\nBarto  and Barto and Jordan  described multi-layer ANNs consisting of AR−P units trained with a globally-broadcast reinforcement signal to learn classiﬁcation rules that are not linearly separable. Barto  discussed this approach to ANNs and how this type of learning rule is related to others in the literature at that time. (See Section 15.10 for additional discussion of this approach to training multi-layer ANNs.) Anderson  evaluated numerous methods for training multilayer ANNs and showed that an actor–critic algorithm in which both the actor and critic were implemented by two-layer ANNs trained by error backpropagation outperformed single-layer ANNs in the pole-balancing and tower of Hanoi tasks. Williams  described several ways that backpropagation and reinforcement learning can be combined for training ANNs. Gullapalli  and Williams  devised reinforcement learning algorithms for neuron-like units having continuous, rather than binary, outputs',\n",
       " 'b13dc287-ae95-4456-8999-1def11443995': 'gave a different and very simple interpretation of boosting in terms of the sequential minimization of an exponential error function. Consider the exponential error function deﬁned by where fm(x) is a classiﬁer deﬁned in terms of a linear combination of base classiﬁers yl(x) of the form and tn ∈ {−1, 1} are the training set target values. Our goal is to minimize E with respect to both the weighting coefﬁcients αl and the parameters of the base classiﬁers yl(x). other of the axes. Each ﬁgure shows the number m of base learners trained so far, along with the decision boundary of the most recent base learner (dashed black line) and the combined decision boundary of the ensemble (solid green line). Each data point is depicted by a circle whose radius indicates the weight assigned to that data point when training the most recently added base learner. Thus, for instance, we see that points that are misclassiﬁed by the m = 1 base learner are given greater weight when training the m = 2 base learner. Instead of doing a global error function minimization, however, we shall suppose that the base classiﬁers y1(x),',\n",
       " '9255ff15-0cd3-49d7-89a2-b392c41c1742': 'An extension of least-squares methods to control was introduced by Lagoudakis and Parr . 9.9 Our discussion of memory-based function approximation is largely based on the review of locally weighted learning by Atkeson, Moore, and Schaal . Atkeson  discussed the use of locally weighted regression in memory-based robot learning and supplied an extensive bibliography covering the history of the idea. Stanﬁll and Waltz  inﬂuentially argued for the importance of memory based methods in artiﬁcial intelligence, especially in light of parallel architectures then becoming available, such as the Connection Machine. Baird and Klopf  introduced a novel memory-based approach and used it as the function approximation method for Q-learning applied to the pole-balancing task. Schaal and Atkeson  applied locally weighted regression to a robot juggling control problem, where it was used to learn a system model.\\n\\nPeng  used the pole-balancing task to experiment with several nearest-neighbor methods for approximating value functions, policies, and environment models. Tadepalli and Ok  obtained promising results with locally-weighted linear regression to learn a value function for a simulated automatic guided vehicle task',\n",
       " 'eb79942e-12f1-41a7-ac5a-92fa33b40b55': '571-82  Buda Mateusz, Maki Atsuto, Mazurowski Maciej A. A systematic study of the class imbalance problem in convolu- tional neural networks. Neural Networks. 2018;106:249-59. Drown DJ, Khoshgoftaar TM, Seliya N. Evolutionary sampling and software quality modeling of high-assurance systems. IEEE Trans Syst. 2009;39(5):1097-107,  Jason Y, Jeff C, Anh N, Thomas F, Hod L. Understanding neural networks through deep visualization. In: European conference on computer vision (ECCV). Berlin: Springer; 2015. p. 818-33. Xiaofeng Z, Zhangyang W, Dong L, Qing L. DADA: deep adversarial data augmentation for extremely low data regime classification. arXiv preprint',\n",
       " '60e530c0-9614-49eb-b243-a53f97cc82f5': 'In the experiment’s third phase, the second stimulus alone—the light—is presented to the rabbit to see if the rabbit has learned to respond to it with a CR.\\n\\nIt turns out that the rabbit produces very few, or no, CRs in response to the light: learning to the light had been blocked by the previous learning to the tone.2 Blocking results like this challenged the idea that conditioning depends only on simple temporal contiguity, that is, that a necessary and suﬃcient condition for conditioning is that a US frequently follows a CS closely in time. In the next section we describe the Rescorla–Wagner model  that o↵ered an inﬂuential explanation for blocking. Higher-order conditioning occurs when a previously-conditioned CS acts as a US in conditioning another initially neutral stimulus. Pavlov described an experiment in which his assistant ﬁrst conditioned a dog to salivate to the sound of a metronome that predicted a food US, as described above. After this stage of conditioning, a number of trials were conducted in which a black square, to which the dog was initially indi↵erent, was placed in the dog’s line of vision followed by the sound of the metronome—and this was not followed by food',\n",
       " '84e85274-4a6d-43aa-a408-7d40cdfcf8f6': 'This is called the problem of cold- start recommendations. A general way of solving the cold-start recommendation problem is to introduce extra information about the individual users and items. For example, this extra information could be user profile information or features of each item. Systems that use such information are called content-based recommender systems. The mapping from a rich set of user features or item features to an embedding can be learned through a deep learning architecture . Specialized deep learning architectures, such as convolutional networks, have also been applied to learn to extract features from rich content, such as from musical audio tracks for music recommendation . In that work, the convolutional net takes acoustic features as input and computes an embedding for the associated song. The dot product between this song embedding and the embedding for a user is then used to predict whether a user will listen to the song.\\n\\n475  https://www.deeplearningbook.org/contents/applications.html    CHAPTER 12. APPLICATIONS  12.5.1.1 Exploration versus Exploitation  When making recommendations to users, an issue arises that goes beyond ordinary supervised learning and into the realm of reinforcement learning',\n",
       " 'd549c57c-a684-4e64-9300-4cf76ad50ef8': 'Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of The Thirteenth International Conference on Artiﬁcial Intelligence and Statistics (AISTATS’10). Hinton, G., Deng, L., Dahl, G. E., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T., and Kingsbury, B. .\\n\\nDeep neural networks for acoustic modeling in speech recognition. IEEE Signal Processing Magazine, 29(6), 82–97. Hinton, G. E., Dayan, P., Frey, B. J., and Neal, R. M. The wake-sleep algorithm for unsupervised neural networks. Science, 268, 1558–1161. Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. Improving neural networks by preventing co-adaptation of feature detectors',\n",
       " 'aa6640d3-43e4-4601-936d-fbaf7b410ee8': 'Although not every dopamine neuron monitored in the experiments of Schultz and 2In the literature relating TD errors to the activity of dopamine neurons, their δt is the same as our colleagues behaved in all of these ways, the striking correspondence between the activities of most of the monitored neurons and TD errors lends strong support to the reward prediction error hypothesis. There are situations, however, in which predictions based on the hypothesis do not match what is observed in experiments.\\n\\nThe choice of input representation is critical to how closely TD errors match some of the details of dopamine neuron activity, particularly details about the timing of dopamine neuron responses. Di↵erent ideas, some of which we discuss below, have been proposed about input representations and other features of TD learning to make the TD errors ﬁt the data better, though the main parallels appear with the CSC representation that Montague et al. used. Overall, the reward prediction error hypothesis has received wide acceptance among neuroscientists studying reward-based learning, and it has proven to be remarkably resilient in the face of accumulating results from neuroscience experiments',\n",
       " '5909bcec-0c2b-4e80-8eec-efb46f5353e9': '634  CHAPTER 19.\\n\\nAPPROXIMATE INFERENCE  MAP inference is commonly used in deep learning as both a feature extractor and a learning mechanism. It is primarily used for sparse coding models. Recall from section 13.4 that sparse coding is a linear factor model that imposes a sparsity-inducing prior on its hidden units. A common choice is a factorial Laplace prior, with  p(hi) = AeA (19.14)  The visible units are then generated by performing a linear transformation and adding noise:  p(a | h) =N(v;Wh+ b, 61D). (19.15)  Computing or even representing p(h | v) is difficult. Every pair of variables h; and h; are both parents of v. This means that when v is observed, the graphical model contains an active path connecting h; and hj. All the hidden units thus participate in one massive clique in p(h | v)',\n",
       " 'cf5c0669-4972-438e-8e58-7a8f2798adc5': \"In reality, the scenario could be a bot playing a game to achieve high scores, or a robot trying to complete physical tasks with physical items; and not just limited to these.\\n\\nhttps://lilianweng.github.io/posts/2018-02-19-rl-overview/  A (Long) Peek into Reinforcement Learning | Lil'Log   AGENT ENVIRONMENT -State s ES  - Take action a € A  & ©  - Get reward 7 -Newstate s’ € S  The goal of Reinforcement Learning (RL) is to learn a good strategy for the agent from experimental trials and relative simple feedback received. With the optimal strategy, the agent is capable to actively adapt to the environment to maximize future rewards. Key Concepts  Now Let's formally define a set of key concepts in RL. The agent is acting in an environment. How the environment reacts to certain actions is defined by a model which we may or may not know. The agent can stay in one of many states (s € S) of the environment, and choose to take one of many actions (a € A) to switch from one state to another. Which state the agent will arrive in is decided by transition probabilities between states (P)\",\n",
       " '5a493a35-071f-4468-85ee-58dcb6604e2d': 'As an example, suppose that the probability density within each class is chosen to be Gaussian.\\n\\nIn this case, the naive Bayes assumption then implies that the covariance matrix for each Gaussian is diagonal, and the contours of constant density within each class will be axis-aligned ellipsoids. The marginal density, however, is given by a superposition of diagonal Gaussians (with weighting coefﬁcients given by the class priors) and so will no longer factorize with respect to its components. The naive Bayes assumption is helpful when the dimensionality D of the input space is high, making density estimation in the full D-dimensional space more challenging. It is also useful if the input vector contains both discrete and continuous variables, since each can be represented separately using appropriate models (e.g., Bernoulli distributions for binary observations or Gaussians for real-valued variables). The conditional independence assumption of this model is clearly a strong one that may lead to rather poor representations of the class-conditional densities',\n",
       " '394bd4dd-e87f-4537-a2a3-e8ebd59fef9b': 'Greedy function approximation: a gradient boosting machine. Annals of Statistics 29(5), 1189–1232. Friedman, J. H., T. Hastie, and R. Tibshirani . Additive logistic regression: a statistical view of boosting. Annals of Statistics 28, 337–407. Friedman, N. and D. Koller . Being Bayesian about network structure: A Bayesian approach to structure discovery in Bayesian networks. Machine Learning 50, 95–126. Fukunaga, K. Introduction to Statistical Pattern Recognition (Second ed.). Academic Press. Funahashi, K. On the approximate realization of continuous mappings by neural networks. Neural Networks 2(3), 183–192. Fung, R. and K. C. Chang . Weighting and integrating evidence for stochastic simulation in Bayesian networks. In P. P. Bonissone, M',\n",
       " 'c3eb9990-231b-4243-8c3b-5677d8bb4ac2': '9.4 (⋆) Suppose we wish to use the EM algorithm to maximize the posterior distribution over parameters p(θ|X) for a model containing latent variables, where X is the observed data set. Show that the E step remains the same as in the maximum likelihood case, whereas in the M step the quantity to be maximized is given by Q(θ, θold) + ln p(θ) where Q(θ, θold) is deﬁned by (9.30). 9.5 (⋆) Consider the directed graph for a Gaussian mixture model shown in Figure 9.6.\\n\\nBy making use of the d-separation criterion discussed in Section 8.2, show that the posterior distribution of the latent variables factorizes with respect to the different data points so that 9.6 (⋆ ⋆) Consider a special case of a Gaussian mixture model in which the covariance matrices Σk of the components are all constrained to have a common value Σ. Derive the EM equations for maximizing the likelihood function under such a model',\n",
       " 'ebb995b9-fd79-495f-84e4-712e937fef87': '˙Ipek et al. found that the number of legal commands for any state was rarely greater than this, and that performance loss was negligible if enough time was not always available to consider all legal commands. These and other clever design details made it feasible to implement the complete controller and learning algorithm on a multi-processor chip. ˙Ipek et al. evaluated their learning controller in simulation by comparing it with three other controllers: 1) the FR-FCFS controller mentioned above that produces the best on-average performance, 2) a conventional controller that processes each request in order, and 3) an unrealizable ideal controller, called the Optimistic controller, able to sustain 100% DRAM throughput if given enough demand by ignoring all timing and resource constraints, but otherwise modeling DRAM latency (as row bu↵er hits) and bandwidth.\\n\\nThey simulated nine memory-intensive parallel workloads consisting of scientiﬁc and data-mining applications. Figure 16.4 shows the performance (the inverse of execution time normalized to the performance of FR-FCFS) of each controller for the nine applications, together with the geometric mean of their performances over the applications',\n",
       " '07745eb2-84b7-4a3c-a6d6-3bcb8e7094df': 'CONVOLUTIONAL NETWORKS  QO @QQ © oO) OOOH  https://www.deeplearningbook.org/contents/convnets.html    figure 9.3: Sparse connectivity, viewed from above. We highlight one output unit,s?, and highlight the input units in & that affect this unit. These units are known as thereceptive field of s3. (Top)When s is formed by convolution with a kernel of width 3, only three  inputs affect s3. (Bottom)When s is formed by matrix multiplication, connectivity is no longer sparse, so all the inputs affect s3.\\n\\nQQ @ GQ ©)  SeoC%  Figure 9.4: The receptive field of the units in the deeper layers of a convolutional network is larger than the receptive field of the units in the shallow layers. This effect increases if the network includes architectural features like strided convolution (figure 9.12) or pooling (section 9.3). This means that even though direct connections in a convolutional net are very sparse, units in the deeper layers can be indirectly connected to all or most of the input image. CHAPTER 9',\n",
       " 'a7abed37-65b2-480b-a074-ecaef43f965c': 'This has been implemented in Tensorﬂow as “slim.preprocessing.inception_preprocessing.distorted_bounding_box_crop”, or in Pytorch as “torchvision.transforms.RandomResizedCrop”.\\n\\nAdditionally, the random crop (with resize) is always followed by a random horizontal/left-to-right ﬂip with 50% probability. This is helpful but not essential. By removing this from our default augmentation policy, the top-1 linear evaluation drops from 64.5% to 63.4% for our ResNet-50 model trained in 100 epochs. Color distortion Color distortion is composed by color jittering and color dropping. We ﬁnd stronger color jittering usually helps, so we set a strength parameter. A pseudo-code for color distortion using TensorFlow is as follows. import tensorflow as tf def color_distortion(image, s=1.0): # image is a tensor with value range in . # s is the strength of color distortion. def color_jitter(x): # one can also shuffle the order of following augmentations # each time they are applied',\n",
       " 'b368b307-3930-421d-b31d-ee3b0aec0f4d': 'For example, if we train a generative model of images of cars and motorcycles, it will need to know about wheels, and about how many wheels should be in an image. If we are fortunate, he representation of the wheels will take on a form that is easy for the supervised earner to access. This is not yet understood at a mathematical, theoretical level, so it is not always possible to predict which tasks will benefit from unsupervised learning in this way.\\n\\nMany aspects of this approach are highly dependent on he specific models used. For example, if we wish to add a linear classifier on  529  CHAPTER 15. REPRESENTATION LEARNING  top of pretrained features, the features must make the underlying classes linearly separable. These properties often occur naturally but do not always do so. This is another reason that simultaneous supervised and unsupervised learning can be preferable—the constraints imposed by the output layer are naturally included from the start. From the point of view of unsupervised pretraining as learning a representation, we can expect unsupervised pretraining to be more effective when the initial representation is poor. One key example of this is the use of word embeddings',\n",
       " 'c43ef083-5c23-49ec-a1fa-7581ddeb3698': 'The empirical mean return for state s is:  Dir 1S: = s]Ge  V(s) = ‘) Ver US = 5]  https://lilianweng.github.io/posts/2018-02-19-rl-overview/  A (Long) Peek into Reinforcement Learning | Lil\\'Log   where 1 is a binary indicator function. We may count the visit of state s every time so that there could exist multiple visits of one state in one episode (“every-visit\"), or only count it the first time we encounter a state in one episode (“first-visit\"). This way of approximation can be easily extended to action-value functions by counting (s, a) pair. ran 1  To learn the optimal policy by MC, we iterate it by following a similar idea to GPI. evaluation Q~ ad. 7 Q) z ~» greedy(Q improvement  Improve the policy greedily with respect to the current value function:  m(s) = arg max,cy4 Q(s, a)',\n",
       " 'a07d0c38-36cd-4bdc-af50-ef6657d96d8f': 'This problem could have been resolved by spending weeks improving the accuracy of the address number detection system responsible for determining the cropping regions. Instead, the team made a much more practical decision, to simply expand the width of the crop region to be systematically wider than the address number detection system predicted. This single change added ten percentage points to the transcription system’s coverage. 436  CHAPTER 11. PRACTICAL METHODOLOGY  Finally, the last few percentage points of performance came from adjusting hyperparameters. This mostly consisted of making the model larger while main- taining some restrictions on its computational cost. Because train and test error remained roughly equal, it was always clear that any performance deficits were due to underfitting, as well as to a few remaining problems with the dataset itself. Overall, the transcription project was a great success and allowed hundreds of millions of addresses to be transcribed both faster and at lower cost than would have been possible via human effort.\\n\\nWe hope that the design principles described in this chapter will lead to many  https://www.deeplearningbook.org/contents/guidelines.html    other similar successes. 437  https://www.deeplearningbook.org/contents/guidelines.html',\n",
       " '09e613a0-8e52-4fbe-9b15-a582292074d3': 'The result of the ﬁrst summation over zN can be stored and used once the value of xN+1 is observed in order to run the α recursion forward to the next step in order to predict the subsequent value xN+2. Note that in (13.44), the inﬂuence of all data from x1 to xN is summarized in the K values of α(zN). Thus the predictive distribution can be carried forward indeﬁnitely using a ﬁxed amount of storage, as may be required for real-time applications.\\n\\nHere we have discussed the estimation of the parameters of an HMM using maximum likelihood. This framework is easily extended to regularized maximum likelihood by introducing priors over the model parameters π, A and φ whose values are then estimated by maximizing their posterior probability. This can again be done using the EM algorithm in which the E step is the same as discussed above, and the M step involves adding the log of the prior distribution p(θ) to the function Q(θ, θold) before maximization and represents a straightforward application of the techniques developed at various points in this book. Furthermore, we can use variational methods to give a fully Bayesian treatment of the HMM in which we marginalize over the Section 10.1 parameter distributions',\n",
       " '87bd5600-4507-4e53-9127-503d8f083362': 'sone<! into decreasing order. is shown in Figure 12.4{ai. The di\\'tortion measure J aSSQCiated wilh choo<ing a particular value of M is gi. \\'en by tho sum of the eig.n\",lues from M + I up to 0 and is ptO!ted for different ,\\'aluo< of .\\\\1 in Figure 12,4(b). If \"\\'e <utlslitut. (12, 12) and (12.13) into (12.10). we can write the I\\'CA appro~\\xad imation to a data \"eel\\'\" x~ i\" the fonn FIIIUre 12,4 (a) PIol at !he eJoI;nv.loo . \".,etrum lor the off·1ine digits data set (b) P10t 01 !he sum at the <:liscarded',\n",
       " '2ed8ff2f-aa36-47ab-b8a3-030683c31648': 'Exercise 8.18 If there are nodes in a directed graph that have more than one parent, but there is still only one path (ignoring the direction of the arrows) between any two nodes, then the graph is a called a polytree, as illustrated in Figure 8.39(c). Such a graph will have more than one node with the property of having no parents, and furthermore, the corresponding moralized undirected graph will have loops.\\n\\nThe sum-product algorithm that we derive in the next section is applicable to undirected and directed trees and to polytrees. It can be cast in a particularly simple and general form if we ﬁrst introduce a new graphical construction called a factor graph . Both directed and undirected graphs allow a global function of several variables to be expressed as a product of factors over subsets of those variables. Factor graphs make this decomposition explicit by introducing additional nodes for the factors themselves in addition to the nodes representing the variables. They also allow us to be more explicit about the details of the factorization, as we shall see',\n",
       " '4342978c-15ff-4625-91d2-e9b8406903b4': 'While score matching can be used o pretrain the first hidden layer of a larger model, it has not been applied as a pretraining strategy for the deeper layers of a larger model. This is probably because the hidden layers of such models usually contain some discrete variables. While score matching does not explicitly have a negative phase, it can be viewed as a version of contrastive divergence using a specific kind of Markov chain Hyvarinen, 2007a). The Markov chain in this case is not Gibbs sampling, but rather a different approach that makes local moves guided by the gradient. Score matching is equivalent to CD with this type of Markov chain when the size of the ocal moves approaches zero. Lyu  generalized score matching to the discrete case (but made an error in he derivation that was corrected by Marlin ef al. ). Marlin et al. found hat generalized score matching (GSM) does not work in high-dimensional discrete spaces where the observed probability of many events is 0.\\n\\nA more successful approach to extending the basic ideas of score matching o discrete data is ratio matching . Ratio matching applies specifically to binary data',\n",
       " 'bfd2c7a5-19d6-4b2b-b38b-a9fa2f565c03': 'Moreover, the time step index need not literally refer to the passage of time in the real world. Sometimes it refers only to the position in the sequence. RNNs may also be applied in two dimensions across spatial data such as images, and even when applied to data involving time, the network may have connections that go backward in time, provided that the entire sequence is observed before it is provided to the network. This chapter extends the idea of a computational graph to include cycles. These cycles represent the influence of the present value of a variable on its own value at a future time step. Such computational graphs allow us to define recurrent neural networks. We then describe many different ways to construct, train, and use recurrent neural networks.\\n\\nFor more information on recurrent neural networks than is available in this  1 1 c wd 1 sad 1 ad 1 on faAn4 AN  https://www.deeplearningbook.org/contents/rnn.html    Chlapter, we reler tle reader LO LIE LEXLDOOK OL Graves (4U14). 368  CHAPTER 10',\n",
       " 'af4aa45b-809d-49b8-b3d9-880320126aee': 'However, the dependence becomes weaker for longer-delayed rewards, falling by γλ for each step of delay. A natural approximation, then, would be to truncate the sequence after some number of steps. Our existing notion of n-step returns provides a natural way to do this in which the missing rewards are replaced with estimated values. In general, we deﬁne the truncated λ-return for time t, given data only up to some If you compare this equation with the λ-return (12.3), it is clear that the horizon h is playing the same role as was previously played by T, the time of termination. Whereas in the λ-return there is a residual weight given to the conventional return Gt, here it is given to the longest available n-step return, Gt:h (Figure 12.2). The truncated λ-return immediately gives rise to a family of n-step λ-return algorithms similar to the n-step methods of Chapter 7',\n",
       " 'fc79a883-1784-401d-9667-091827c9380a': 'Another is to clip the norm |\\\\g|| of the gradient g  just before the parameter update:  if ||g|| >v (10.48) (10.49)  where v is the norm threshold and g is used to update parameters.\\n\\nBecause the gradient of all the parameters (including different groups of parameters, such as weights and biases) is renormalized jointly with a single scaling factor, the latter wn nth nw dD Lan tlw 6 Aen nn AL ee nn bn tw LL AA AL nde 8 nt Sew dL we AS ne  https://www.deeplearningbook.org/contents/rnn.html    IUCLUUU Lad LUC AUVaALLLAXS UL BUALALILCCLIL LIAL CACLL SLEP 1S SUL Ll LLC RIAUIELIL direction, but experiments suggest that both forms work similarly. ‘Although the parameter update has the same direction as the true gradient, with gradient norm clipping, the parameter update vector norm is now bounded. This bounded  gradient avoids performing a detrimental step when the gradient explodes',\n",
       " '0cf47c25-b27a-4edb-b338-c08cb3148213': 'The problem with the table-based approach is that we are explicitly modeling every possible kind of interaction between every possible subset of variables. The probability distributions we encounter in real tasks are much simpler than this. TT u 4 s 4d ral 1 a Porat a1  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html       UsuUaLLy, MOSL VarlaDies WiUeLCce each OLUeL OLY MaIreculy.\\n\\nFor example, consider modeling the finishing times of a team in a relay race. Suppose the team consists of three runners: Alice, Bob and Carol. At the start of the race, Alice carries a baton and begins running around a track. After completing  her lap around the track, she hands the baton to Bob. Bob then runs his own lap and hands the baton to Carol, who runs the final lap. We can model each of their finishing times as a continuous random variable. Alice’s finishing time does not depend on anyone else’s, since she goes first',\n",
       " 'ddfb8149-c4b2-41fd-9c6d-73601b31f247': 'Typ) 2 — = 6~N(u,02) F(9)  L o2  We can rewrite this formula in terms of a “mean\" parameter 6 (different from the @ above; this 0 is the base gene for further mutation), € ~ N(0, I) and therefore 6 + ea ~ N(0, 07).\\n\\n€ controls how  https://lilianweng.github.io/posts/2018-02-19-rl-overview/  A (Long) Peek into Reinforcement Learning | Lil\\'Log   much Gaussian noises should be added to create mutation:  x 1_ VoE.w(o,) F(9 + o€) = oe N (01)   Algorithm 2 Parallelized Evolution Strategies 1: Input: Learning rate a, noise standard deviation o, initial policy parameters 69 2: Initialize: n workers with known random seeds, and initial parameters 69 3: fort = 0,1,2,..',\n",
       " '44bfe43b-f9ec-4053-b008-33421a3e3119': 'This leaves the following contribution to the derivative with respect to a component θj of θ Combining (6.91), (6.92), and (6.94), we can evaluate the gradient of the log likelihood function, which can be used with standard nonlinear optimization algorithms in order to determine a value for θ. We can illustrate the application of the Laplace approximation for Gaussian processes using the synthetic two-class data set shown in Figure 6.12. Extension of the Appendix A Laplace approximation to Gaussian processes involving K > 2 classes, using the softmax activation function, is straightforward . with the optimal decision boundary from the true distribution in green, and the decision boundary from the Gaussian process classiﬁer in black. On the right is the predicted posterior probability for the blue and red classes together with the Gaussian process decision boundary.\\n\\nWe have seen that the range of functions which can be represented by a neural network is governed by the number M of hidden units, and that, for sufﬁciently large M, a two-layer network can approximate any given function with arbitrary accuracy. In the framework of maximum likelihood, the number of hidden units needs to be limited (to a level dependent on the size of the training set) in order to avoid over-ﬁtting',\n",
       " 'ce3330df-4092-4e9f-b3e6-a83d793cd532': 'whose mean and covariance are given by (2.108) and (2.105) respectively.\\n\\nBy making use of the results (2.92) and (2.93) show that the marginal distribution p(x) is given (2.99). Similarly, by making use of the results (2.81) and (2.82) show that the conditional distribution p(y|x) is given by (2.100). 2.29 (⋆ ⋆) Using the partitioned matrix inversion formula (2.76), show that the inverse of the precision matrix (2.104) is given by the covariance matrix (2.105). 2.31 (⋆ ⋆) Consider two multidimensional random vectors x and z having Gaussian distributions p(x) = N(x|µx, Σx) and p(z) = N(z|µz, Σz) respectively, together with their sum y = x+z. Use the results (2.109) and (2.110) to ﬁnd an expression for the marginal distribution p(y) by considering the linear-Gaussian model comprising the product of the marginal distribution p(x) and the conditional distribution p(y|x)',\n",
       " '92dcddee-1a0b-44c5-8f12-c300e739006a': 'In this case, a stimulus that 2Comparison with a control group is necessary to show that the previous conditioning to the tone is responsible for blocking learning to the light. This is done by trials with the tone/light CS but with no prior conditioning to the tone. Learning to the light in this case is unimpaired. Moore and Schmajuk  give a full account of this procedure. consistently predicts primary reinforcement becomes a reinforcer itself, where reinforcement is primary if its rewarding or penalizing quality has been built into the animal by evolution.\\n\\nThe predicting stimulus becomes a secondary reinforcer, or more generally, a higher-order or conditioned reinforcer—the latter being a better term when the predicted reinforcing stimulus is itself a secondary, or an even higher-order, reinforcer. A conditioned reinforcer delivers conditioned reinforcement: conditioned reward or conditioned penalty. Conditioned reinforcement acts like primary reinforcement in increasing an animal’s tendency to produce behavior that leads to conditioned reward, and to decrease an animal’s tendency to produce behavior that leads to conditioned penalty',\n",
       " '65023d2e-64b1-4a4b-bc43-edf50a83b1f7': 'Planning can determine the consequences of changes in the environment that have never been linked together in the agent’s own experience. For example, again referring to the maze task of Figure 14.5, imagine that a rat with a previously learned transition and reward model is placed directly in the goal box to the right of S2 to ﬁnd that the reward available there now has value 1 instead of 4. The rat’s reward model will change even though the action choices required to ﬁnd that goal box in the maze were not involved. The planning process will bring knowledge of the new reward to bear on maze running without the need for additional experience in the maze; in this case changing the policy to right turns at both S1 and S3 to obtain a return of 3. Exactly this logic is the basis of outcome-devaluation experiments with animals. Results from these experiments provide insight into whether an animal has learned a habit or if its behavior is under goal-directed control.\\n\\nOutcome-devaluation experiments are like latent-learning experiments in that the reward changes from one stage to the next. After an initial rewarded stage of learning, the reward value of an outcome is changed, including being shifted to zero or even to a negative value',\n",
       " '427f4696-bf7f-43aa-acb0-2b9d72b37a20': 'Furthermore, Tipping and Bishop  showed that the maximum of the likelihood function is obtained when the M eigenvectors are chosen to be those whose eigenvalues are the M largest (all other solutions being saddle points). A similar result was conjectured independently by Roweis , although no proof was given.\\n\\nAgain, we shall assume that the eigenvectors have been arranged in order of decreasing values of the corresponding eigenvalues, so that the M principal eigenvectors are Ul,\"\" UM. In this case, the columns of W define the principal subspace of standard PCA. The corresponding maximum likelihood solution for (J\\'2 is then given by so that (J\\'~L is the average variance associated with the discarded dimensions. Because R is orthogonal, it can be interpreted as a rotation matrix in the M x M latent space. If we substitute the solution for W into the expression for C, and make use of the orthogonality property RRT = I, we see that C is independent of R. This simply says that the predictive density is unchanged by rotations in the latent space as discussed earlier',\n",
       " 'dde41220-c39d-4018-83fb-478651914b46': 'Both of these prior works used a featurebased approach — we hypothesize that when the model is ﬁne-tuned directly on the downstream tasks and uses only a very small number of randomly initialized additional parameters, the taskspeciﬁc models can beneﬁt from the larger, more expressive pre-trained representations even when downstream task data is very small. All of the BERT results presented so far have used the ﬁne-tuning approach, where a simple classiﬁcation layer is added to the pre-trained model, and all parameters are jointly ﬁne-tuned on a downstream task. However, the feature-based approach, where ﬁxed features are extracted from the pretrained model, has certain advantages.\\n\\nFirst, not all tasks can be easily represented by a Transformer encoder architecture, and therefore require a task-speciﬁc model architecture to be added. Second, there are major computational beneﬁts to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation. In this section, we compare the two approaches by applying BERT to the CoNLL-2003 Named Entity Recognition (NER) task . In the input to BERT, we use a case-preserving WordPiece model, and we include the maximal document context provided by the data',\n",
       " '7587729f-2a47-499b-a762-254da560b35d': 'This is illustrated in figure 17.1. The problem arises when there are multiple modes with high probability that are separated by regions of low probability, especially when each Gibbs sampling step must update only a small subset of variables whose values are largely determined by the other variables.\\n\\nAs a simple example, consider an energy-based model over two variables a and b, which are both binary with a sign, taking on values —1 and 1. If E(a,b) = —wab  597  CHAPTER 17. MONTE CARLO METHODS  https://www.deeplearningbook.org/contents/monte_carlo.html    for some large positive number W, then the model expresses a strong belief that a and b have the same sign. Consider updating b using a Gibbs samp ing, step with a= 1. The conditional distribution over b is given by P(b = 1 ja= ) = o(w) If w is large, the sigmoid saturates, and the probability of also assigning b to be 1 is close to 1',\n",
       " 'd8303e09-09f4-4ed4-aa6a-407c160ef1ff': 'Michael Bachaes 1 Dickie Powell 2 Lt. Zachary Bram 9 *Gen. Robert Eisen: Warﬁghter – Soldier + Genoured\\\\n\\\\n\\\\n – Senior Bush Doctrine legal: This essay discusses Illinois cases on issues such as drug trafﬁcking and drug Social Security. politics: This essay discusses federal ethics as the key area on which current and past state and local governments have been operating. computers: This essay discusses the very development of alternative technology for young people. space: This essay discusses NASA’s StarHubble satellite mission development. Transcript here. religion: This essay discusses various aspects of the relays of mediocality and Hammazanna. science: This essay discusses Linux desktop computing, and IRI video-game applications.\\\\n\\\\n The zooming in — even after the GNOME 3 transition came to an end, is all about ﬁguring out how you have run a software operating system so vital that any hacker can mine it military: This essay discusses military courage that included in the combat operations in Iraq and Afghanistan',\n",
       " 'eb13fb75-3236-49b6-8d1c-15ac9cce634a': 'Manual hyperparameter tuning can work very well when the user has a good starting point, such as one determined by others having worked on the same type of application and architecture, or when the user has months or years of experience in exploring hyperparameter values for neural networks applied to similar tasks. For many applications, however, these starting points are not available. In these cases, automated algorithms can find useful values of the hyperparameters.\\n\\nIf we think about the way in which the user of a learning algorithm searches for good values of the hyperparameters, we realize that an optimization is taking place: we are trying to find a value of the hyperparameters that optimizes an objective function, such as validation error, sometimes under constraints (such as a budget for training time, memory or recognition time). It is therefore possible, in principle, to develop hyperparameter optimization algorithms that wrap a learning algorithm and choose its hyperparameters, thus hiding the hyperparameters of the learning algorithm from the user. Unfortunately, hyperparameter optimization algorithms often have their own hyperparameters, such as the range of values that should be explored for each of the learning algorithm’s hyperparameters',\n",
       " '2c7d6a3a-d9bf-45fd-8bdb-07b4e97e1ee1': 'Most optimization algorithms converge much faster (in terms of total computation, not in terms of number of updates) if they are allowed to rapidly compute approximate estimates of the gradient rather than slowly computing the exact gradient. Another consideration motivating statistical estimation of the gradient from a small number of samples is redundancy in the training set. In the worst case, all m samples in the training set could be identical copies of each other. A sampling- based estimate of the gradient could compute the correct gradient with a single sample, using m times less computation than the naive approach. In practice, we are unlikely to encounter this worst-case situation, but we may find large numbers of examples that all make very similar contributions to the gradient. https://www.deeplearningbook.org/contents/optimization.html    Optimization algorithms that use the entire training set are called batch or deterministic gradient methods, because they process all the training examples simultaneously in a large batch.\\n\\nThis terminology can be somewhat confusing  because the word “batch” is also often used to describe the minibatch used by minibatch stochastic gradient descent. Typically the term “batch gradient descent” implies the use of the full training set, while the use of the term “batch” to describe a group of examples does not',\n",
       " '9cf2fd9a-6b35-447d-bbdf-7b43b38b520c': 'The target policy ⇡, on the other hand, may be deterministic, and, in fact, this is a case of particular interest in control applications. In control, the target policy is typically the deterministic greedy policy with respect to the current estimate of the action-value function.\\n\\nThis policy becomes a deterministic optimal policy while the behavior policy remains stochastic and more exploratory, for example, an \"-greedy policy. In this section, however, we consider the prediction problem, in which ⇡ is unchanging and given. Almost all o↵-policy methods utilize importance sampling, a general technique for estimating expected values under one distribution given samples from another. We apply importance sampling to o↵-policy learning by weighting returns according to the relative probability of their trajectories occurring under the target and behavior policies, called the importance-sampling ratio. Given a starting state St, the probability of the subsequent state–action trajectory, At, St+1, At+1,',\n",
       " 'd0acdbc1-2c06-4dbd-ab7a-2d0ac6e6995e': 'This would work, but would require a lot of computational resources, particularly to store the ﬁrst two expectations, which are d ⇥ d matrices, and to compute the inverse of the second. This idea can be improved. If two of the three expectations are estimated and stored, then the third could be sampled and used in conjunction with the two stored quantities. For example, you could store estimates of the second two quantities (using the increment inverse-updating techniques in Section 9.8) and then sample the ﬁrst expression. Unfortunately, the overall algorithm would still be of quadratic complexity (of order O(d2)). The idea of storing some estimates separately and then combining them with samples is a good one and is also used in Gradient-TD methods.\\n\\nGradient-TD methods estimate and store the product of the second two factors in (11.27). These factors are a d ⇥ d matrix and a d-vector, so their product is just a d-vector, like w itself. We denote this second learned vector as v: This form is familiar to students of linear supervised learning. It is the solution to a linear least-squares problem that tries to approximate ⇢tδt from the features',\n",
       " 'df7398dc-c4c1-415f-8cbd-6062c870ea76': 'Thus to evaluate the message sent by a variable node to an adjacent factor node along the connecting link, we simply take the product of the incoming messages along all of the other links. Note that any variable node that has only two neighbours performs no computation but simply passes messages through unchanged. Also, we note that a variable node can send a message to a factor node once it has received incoming messages from all other neighbouring factor nodes. Recall that our goal is to calculate the marginal for variable node x, and that this marginal is given by the product of incoming messages along all of the links arriving at that node. Each of these messages can be computed recursively in terms of other messages. In order to start this recursion, we can view the node x as the root of the tree and begin at the leaf nodes',\n",
       " 'e3639e07-9832-412c-9fa6-3930bd3fdd71': 'The parametrization of each P(a; | vj-1,-..,21) by a neural network with (i — 1) x k inputs and k& outputs (if the variables are discrete and take k values, encoded one-hot) enables one to estimate the conditional probability without requiring an exponential number of parameters (and examples), yet still is able to capture high-order dependencies between the random variables. 2. Instead of having a different neural network for the prediction of each z,;, a left-to-right connectivity, illustrated in figure 20.9, allows one to merge all the neural networks into one. Equivalently, it means that the hidden layer features computed for predicting x; can be reused for predicting x;+, (k > 0). The hidden units are thus organized in groups that have the particularity that all the units in the +th group only depend on the input values 41,...,2;. The parameters used to compute these hidden units are jointly optimized to improve the prediction of all the variables in the sequence.\\n\\nThis is an instance of the reuse principle that recurs throughout deep learning in scenarios ranging from recurrent and convolutional network architectures to multitask and transfer learning',\n",
       " 'df027218-fd10-4f69-8e8d-24f7ff5a0ef8': 'Widrow, Gupta, and Maitra  modiﬁed the Least-Mean-Square (LMS) algorithm of Widrow and Ho↵  to produce a reinforcement learning rule that could learn from success and failure signals instead of from training examples. They called this form of learning “selective bootstrap adaptation” and described it as “learning with a critic” instead of “learning with a teacher.” They analyzed this rule and showed how it could learn to play blackjack. This was an isolated foray into reinforcement learning by Widrow, whose contributions to supervised learning were much more inﬂuential. Our use of the term “critic” is derived from Widrow, Gupta, and Maitra’s paper. Buchanan, Mitchell, Smith, and Johnson  independently used the term critic in the context of machine learning , but for them a critic is an expert system able to do more than evaluate performance',\n",
       " 'd7b632bd-3623-43f3-b208-1d28fdf0c8d7': 'If we consider two nodes xi and xj that are not connected by a link, then these variables must be conditionally independent given all other nodes in the graph. This follows from the fact that there is no direct path between the two nodes, and all other paths pass through nodes that are observed, and hence those paths are blocked. This conditional independence property can be expressed as where x\\\\{i,j} denotes the set x of all variables with xi and xj removed. The factorization of the joint distribution must therefore be such that xi and xj do not appear in the same factor in order for the conditional independence property to hold for all possible distributions belonging to the graph.\\n\\nThis leads us to consider a graphical concept called a clique, which is deﬁned as a subset of the nodes in a graph such that there exists a link between all pairs of nodes in the subset. In other words, the set of nodes in a clique is fully connected. Furthermore, a maximal clique is a clique such that it is not possible to include any other nodes from the graph in the set without it ceasing to be a clique',\n",
       " '498244d9-3792-4627-8fe6-a569f8388c3d': 'Keep in mind that not allh values are feasible (there is no h = 0 in this example), and that a linear classifier on top of the distributed representation is not able to assign different class identities to every neighboring region; even a deep linear-threshold network has a VC dimension of only O(w log w), where w is the number of weights .\\n\\nThe combination of a powerful representation layer and a weak classifier layer can be a strong regularizer; a classifier trying to learn the concept of “person” versus “not a person” does not need to assign a different class to an input represented as “woman with glasses” than it assigns to an input represented as “man without glasses.” This capacity constraint encourages each classifier to focus on few  h, and encourages h to learn to represent the classes in a linearly separable way. 545  CHAPTER 15. REPRESENTATION LEARNING  e Decision trees: only one leaf (and the nodes on the path from root to leaf) is activated when an input is given. e Gaussian mixtures and mixtures of experts: the templates (cluster centers) or experts are now associated with a degree of activation',\n",
       " 'cd376628-2d6b-4825-b42d-ec615f0669e1': '(a)We start by training a sufficiently shallow architecture. (b)Another drawing of the same architecture. (c)We keep only the input-to-hidden layer of the original network and discard the hidden-to-output layer. We send the output of the first hidden layer as input to another supervised single hidden layer MLP that is trained with the same objective as the first network was, thus adding a second hidden layer. This can be repeated for as many layers as desired. (d)Another drawing of the result, viewed as a feedforward network. To further improve the optimization, we can jointly fine-tune all the layers, either only at  the end or at each stage of this process. 320  https://www.deeplearningbook.org/contents/optimization.html       CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  intermediate levels of a deep hierarchy. In general, pretraining may help both in terms of optimization and generalization',\n",
       " 'e29b042c-7373-4083-899b-9abb15ecc69c': 'This can be seen by observing that the directional second derivative in any direction must be positive, and making reference to the univariate second derivative test. Likewise, when the Hessian is negative definite (all its eigenvalues are negative), the point is a local maximum. In multiple dimensions, it is actually possible to find positive evidence of saddle points in some cases. When at least one eigenvalue is positive and at least one eigenvalue is negative, we know that x is a local maximum on one cross section of f but a local minimum on another cross section. See figure 4.5 for an example. Finally, the multidimensional second derivative test can be inconclusive, just as the univariate version can. The test is inconclusive whenever all the nonzero eigenvalues have the same sign but at least one eigenvalue is zero. This is because the univariate second derivative test is inconclusive in the cross section corresponding to the zero eigenvalue.\\n\\nIn multiple dimensions, there is a different second derivative for each direction at a single point. The condition number of the Hessian at this point measures how much the second derivatives differ from each other',\n",
       " '2ecf514b-1eee-4a01-8565-543fb1726c9f': 'One way to assign directions to the edges in D is to impose an ordering on the random variables, then  574  CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  P B86 5 6 SSB  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html     Figure 16.11: Examples of converting directed models (top row) to undirected models (bottom row) by constructing moralized graphs.\\n\\n(Left) This simple chain can be converted to a moralized graph merely by replacing its directed edges with undirected edges. The resulting undirected model implies exactly the same set of independences and conditional independences. (Center) This graph is the simplest directed model that cannot be converted to an undirected model without losing some independences. This graph consists entirely of a single immorality. Becausea and b are parents ofc, they are connected by an active path when c is observed. To capture this dependence, the undirected model must include a clique encompassing all three variables. This clique fails to encode the fact that aLb',\n",
       " 'e1b47aa3-3156-49b3-b40b-0f75bca865ed': 'This is beyond what we treat in any detail in this book, but vector-valued RPE signals make sense from the perspective of reinforcement learning when decisions can be decomposed into separate sub-decisions, or more generally, as a way to address the structural version of the credit assignment problem: How do you distribute credit for success (or blame for failure) of a decision among the many component structures that could have been involved in producing it? We say a bit more about this in Section 15.10 below.\\n\\nThe axons of most dopamine neurons make synaptic contact with neurons in the frontal cortex and the basal ganglia, areas of the brain involved in voluntary movement, decision making, learning, and cognitive functions such as planning. Because most ideas relating dopamine to reinforcement learning focus on the basal ganglia, and the connections from dopamine neurons are particularly dense there, we focus on the basal ganglia here. The basal ganglia are a collection of neuron groups, or nuclei, lying at the base of the forebrain. The main input structure of the basal ganglia is called the striatum. Essentially all of the cerebral cortex, among other structures, provides input to the striatum',\n",
       " '7ce77aaf-bbd6-40f3-a5e4-536067cdb6dc': 'For speciﬁc choices of the class-conditional densities p(x|Ck), we have used maximum likelihood to determine the parameters of the densities as well as the class priors p(Ck) and then used Bayes’ theorem to ﬁnd the posterior class probabilities.\\n\\nHowever, an alternative approach is to use the functional form of the generalized linear model explicitly and to determine its parameters directly by using maximum likelihood. We shall see that there is an efﬁcient algorithm ﬁnding such solutions known as iterative reweighted least squares, or IRLS. The indirect approach to ﬁnding the parameters of a generalized linear model, by ﬁtting class-conditional densities and class priors separately and then applying shows the original input space (x1, x2) together with data points from two classes labelled red and blue. Two ‘Gaussian’ basis functions φ1(x) and φ2(x) are deﬁned in this space with centres shown by the green crosses and with contours shown by the green circles. The right-hand plot shows the corresponding feature space (φ1, φ2) together with the linear decision boundary obtained given by a logistic regression model of the form discussed in Section 4.3.2',\n",
       " '664188f4-8205-44d1-8ee1-36bfc211fb19': \"-| Enc } ~(eece) -| Dec } > And yet his crops didn’t grow. Spring had come. - Ene (f)| ~(eece)  They were so black. ~- (Eee) - oes And yet his crops didn’t grow. Be) ee @ooe—|  He had blue eyes. [BER —-@2e 9)  (a) Conventional approach  Classifier | nN  (b) Proposed approach  Let f(.) and g(.) be two functions that encode a sentence s into a fixed-length vector. Let C(s)  https://lilianweng.github.io/posts/2021-05-31-contrastive/     Contrastive Representation Learning | Lil'Log   be the set of sentences in the context of s and S(s) be the set of candidate sentences including only one sentence s, € C(s) and many other non-context negative sentences. Quick Thoughts model learns to optimize the probability of predicting the only true context sentence s, € S(s)\",\n",
       " '19020208-0591-4285-b2c1-9bdb49a4874a': 'By redefining the parameters of the model, show that this leads to an identical model for the marginal distribution p(x) over the observed variables for any valid choice of m and ~.\\n\\n12.5 (* *) Let x be a D-dimensional random variable having a Gaussian distribution given by N(xIJL, ~), and consider the M-dimensional random variable given by y = Ax + b where A is an M x D matrix. Show that y also has a Gaussian distribution, and find expressions for its mean and covariance. Discuss the form of this Gaussian distribution for M < D, for M = D, and for M > D. 12.6 (*) Imm Draw a directed probabilistic graph for the probabilistic PCA model described in Section 12.2 in which the components of the observed variable x are shown explicitly as separate nodes. Hence verify that the probabilistic PCA model has the same independence structure as the naive Bayes model discussed in Section 8.2.2',\n",
       " 'd55a7195-83b4-4cd1-8060-56f4b79b5fe4': 'Most SGD methods require the designer to select an appropriate step-size parameter ↵. Ideally this selection would be automated, and in some cases it has been, but for most cases it is still common practice to set it manually. To do this, and to better understand the algorithms, it is useful to develop some intuitive sense of the role of the step-size parameter. Can we say in general how it should be set? Theoretical considerations are unfortunately of little help. The theory of stochastic approximation gives us conditions (2.7) on a slowly decreasing step-size sequence that are suﬃcient to guarantee convergence, but these tend to result in learning that is too slow. The classical choice ↵t = 1/t, which produces sample averages in tabular MC methods, is not appropriate for TD methods, for nonstationary problems, or for any method using function approximation.\\n\\nFor linear methods, there are recursive least-squares methods that set an optimal matrix step size, and these methods can be extended to temporaldi↵erence learning as in the LSTD method described in Section 9.8, but these require O(d2) step-size parameters, or d times more parameters than we are learning',\n",
       " '293ee378-5d15-4f64-aa8e-822a0fa4e3bf': 'Given a joint Gaussian distribution N(x|µ, Σ) with Λ ≡ Σ−1 and the plot on the right shows the marginal distribution p(xa) (blue curve) and the conditional distribution p(xa|xb) for xb = 0.7 (red curve). We illustrate the idea of conditional and marginal distributions associated with a multivariate Gaussian using an example involving two variables in Figure 2.9. In Sections 2.3.1 and 2.3.2, we considered a Gaussian p(x) in which we partitioned the vector x into two subvectors x = (xa, xb) and then found expressions for the conditional distribution p(xa|xb) and the marginal distribution p(xa). We noted that the mean of the conditional distribution p(xa|xb) was a linear function of xb.\\n\\nHere we shall suppose that we are given a Gaussian marginal distribution p(x) and a Gaussian conditional distribution p(y|x) in which p(y|x) has a mean that is a linear function of x, and a covariance which is independent of x',\n",
       " '3c6e1b29-aa79-4aed-ac68-5a9032883d8b': 'Assuming independent and identically distributed data, write down the error function corresponding to the negative log likelihood. Verify that the error function (5.21) is obtained when ϵ = 0. Note that this error function makes the model robust to incorrectly labelled data, in contrast to the usual error function. 5.5 (⋆) www Show that maximizing likelihood for a multiclass neural network model in which the network outputs have the interpretation yk(x, w) = p(tk = 1|x) is equivalent to the minimization of the cross-entropy error function (5.24). 5.7 (⋆) Show the derivative of the error function (5.24) with respect to the activation ak for output units having a softmax activation function satisﬁes (5.18). 5.8 (⋆) We saw in (4.88) that the derivative of the logistic sigmoid activation function can be expressed in terms of the function value itself. Derive the corresponding result for the ‘tanh’ activation function deﬁned by (5.59)',\n",
       " 'babe7dab-25cb-4ff2-8408-cee830eadcd7': 'First note that if we make a small step in weight space from w to w+δw then the change in the error function is δE ≃ δwT∇E(w), where the vector ∇E(w) points in the direction of greatest rate of increase of the error function. Because the error E(w) is a smooth continuous function of w, its smallest value will occur at a point in weight space such that the gradient of the error function vanishes, so that as otherwise we could make a small step in the direction of −∇E(w) and thereby further reduce the error. Points at which the gradient vanishes are called stationary points, and may be further classiﬁed into minima, maxima, and saddle points. Our goal is to ﬁnd a vector w such that E(w) takes its smallest value. However, the error function typically has a highly nonlinear dependence on the weights and bias parameters, and so there will be many points in weight space at which the gradient vanishes (or is numerically very small). Indeed, from the discussion in Section 5.1.1 we see that for any point w that is a local minimum, there will be other points in weight space that are equivalent minima',\n",
       " 'ecc96fcc-4778-4e6f-aea0-8438c23e6d79': '(2.36)  A vector & and a vector y are orthogonal to each other if «y= 0. If both vectors have nonzero norm, this means that they are at a 90 degree angle to each other. In R\", at most n vectors may be mutually orthogonal with nonzero norm. If the vectors not only are orthogonal but also have unit norm, we call them orthonormal. An orthogonal matrix is a square matrix whose rows are mutually orthonor- mal and whose columns are mutually orthonormal:  A\\'A=AA\\' =I. (2.37)  This implies that AT=A\", (2.38) 39  CHAPTER 2. LINEAR ALGEBRA  https://www.deeplearningbook.org/contents/linear_algebra.html    so orthogonal matrices are of interest because their inverse is very cheap to compute. Pay careful attention to the definition of orthogonal matrices. Counterintuitively, their rows are not merely orthogonal but fully orthonormal',\n",
       " '644048d2-79fb-46fd-b0b1-a9db077f7dc3': 'One attempt to improve the tightness of the PAC bounds is the PAC-Bayesian framework , which considers a distribution over the space F of functions, somewhat analogous to the prior in a Bayesian treatment.\\n\\nThis still considers any possible choice for p(x, t), and so although the bounds are tighter, they are still very conservative. Support vector machines have been used in a variety of classiﬁcation and regression applications. Nevertheless, they suffer from a number of limitations, several of which have been highlighted already in this chapter. In particular, the outputs of an SVM represent decisions rather than posterior probabilities. Also, the SVM was originally formulated for two classes, and the extension to K > 2 classes is problematic. There is a complexity parameter C, or ν (as well as a parameter ϵ in the case of regression), that must be found using a hold-out method such as cross-validation. Finally, predictions are expressed as linear combinations of kernel functions that are centred on training data points and that are required to be positive deﬁnite',\n",
       " 'c15fcaf7-66b6-4102-8c74-3c799595fb34': 'Essentially, one can use adversarial nets to implement a stochastic extension of the deterministic MP-DBM . 4. Semi-supervised learning: features from the discriminator or inference net could improve performance of classiﬁers when limited labeled data is available. 5. Efﬁciency improvements: training could be accelerated greatly by devising better methods for coordinating G and D or determining better distributions to sample z from during training. This paper has demonstrated the viability of the adversarial modeling framework, suggesting that these research directions could prove useful.\\n\\nWe would like to acknowledge Patrice Marcotte, Olivier Delalleau, Kyunghyun Cho, Guillaume Alain and Jason Yosinski for helpful discussions. Yann Dauphin shared his Parzen window evaluation code with us. We would like to thank the developers of Pylearn2  and Theano , particularly Fr´ed´eric Bastien who rushed a Theano feature speciﬁcally to beneﬁt this project. Arnaud Bergeron provided much-needed support with LATEX typesetting',\n",
       " 'cfc03909-fe94-4553-9c26-2921ff769aa0': 'We therefore have ∇f(x) = −λ∇g(x) for some value of λ > 0. For either of these two cases, the product λg(x) = 0. Thus the solution to the problem of maximizing f(x) subject to g(x) ⩾ 0 is obtained by optimizing the Lagrange function (E.4) with respect to x and λ subject to the conditions These are known as the Karush-Kuhn-Tucker (KKT) conditions . Note that if we wish to minimize (rather than maximize) the function f(x) subject to an inequality constraint g(x) ⩾ 0, then we minimize the Lagrangian function L(x, λ) = f(x) − λg(x) with respect to x, again subject to λ ⩾ 0.\\n\\nFinally, it is straightforward to extend the technique of Lagrange multipliers to the case of multiple equality and inequality constraints. Suppose we wish to maximize f(x) subject to gj(x) = 0 for j = 1, . , J, and hk(x) ⩾ 0 for k = 1, . , K',\n",
       " 'f285b80c-e14c-471c-9297-594fa4f78d89': \"Feature selection simplifies a machine learning problem by choosing which subset of the available features should be used. In  232  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  particular, the well known LASSO  (least absolute shrinkage and selection operator) model integrates an L' penalty with a linear model and a least-squares cost function. The L! penalty causes a subset of the weights to become zero, suggesting that the corresponding features may safely be discarded. In section 5.6.1, we saw that many regularization strategies can be interpreted as MAP Bayesian inference, and that in particular, L? regularization is equivalent to MAP Bayesian inference with a Gaussian prior on the weights. For L!\\n\\nregu-  https://www.deeplearningbook.org/contents/regularization.html    larization, the pete aQ(w oo i|wi| used to regularize a cost function is equivalent to the log-prior Sas 1 th\",\n",
       " '6540cbec-1a8b-484e-9d26-880c4325d869': 'Convolution is equivalent to converting both the input and the kernel to the frequency domain using a Fourier transform, performing point-wise multiplication of the two signals, and converting back to the time domain using an inverse Fourier transform. For some problem sizes, this can be faster than the naive implementation of discrete convolution. When a ddimensional kernel can be expressed as the outer product of d vectors, one vector per dimension, the kernel is called separable. When the kernel is separable, naive convolution is inefficient. It is equivalent to compose d one-dimensional convolutions with each of these vectors. The composed approach is significantly faster than performing one d-dimensional convolution with their outer product. The kernel also takes fewer parameters to represent as vectors. re 1 wel a 1 sae yo. a . vende',\n",
       " 'cd9f691f-7a8c-4374-9960-97a898ee44ee': 'The resulting estimator for this model and datapoint x(i) is: As explained above and in appendix C, the decoding term log pθ(x(i)|z(i,l)) is a Bernoulli or Gaussian MLP, depending on the type of data we are modelling. 2Note that this is just a (simplifying) choice, and not a limitation of our method. The wake-sleep algorithm  is, to the best of our knowledge, the only other on-line learning method in the literature that is applicable to the same general class of continuous latent variable models.\\n\\nLike our method, the wake-sleep algorithm employs a recognition model that approximates the true posterior. A drawback of the wake-sleep algorithm is that it requires a concurrent optimization of two objective functions, which together do not correspond to optimization of (a bound of) the marginal likelihood. An advantage of wake-sleep is that it also applies to models with discrete latent variables. Wake-Sleep has the same computational complexity as AEVB per datapoint. Stochastic variational inference  has recently received increasing interest',\n",
       " '7a17313f-33a4-4ab3-bb03-3f8ec2603782': 'It therefore beneﬁts from online, incremental, sample-based value estimation and policy improvement. Beyond this, it saves action-value estimates attached to the tree edges and updates them using reinforcement learning’s sample updates.\\n\\nThis has the e↵ect of focusing the Monte Carlo trials on trajectories whose initial segments are common to high-return trajectories previously simulated. Further, by incrementally expanding the tree, MCTS e↵ectively grows a lookup table to store a partial action-value function, with memory allocated to the estimated values of state–action pairs visited in the initial segments of high-yielding sample trajectories. MCTS thus avoids the problem of globally approximating an action-value function while it retains the beneﬁt of using past experience to guide exploration. The striking success of decision-time planning by MCTS has deeply inﬂuenced artiﬁcial intelligence, and many researchers are studying modiﬁcations and extensions of the basic procedure for use in both games and single-agent applications. Planning requires a model of the environment. A distribution model consists of the probabilities of next states and rewards for possible actions; a sample model produces single transitions and rewards generated according to these probabilities',\n",
       " 'af8437e8-02c7-4b29-ba43-661afbb31f60': 'The target model can even be a symbolic system such as a knowledge graph (KG) or a rule set. Here θ denotes the KG structure to be learned, and pθ(t) can be seen as a distribution assigning a (non-)uniform nonzero probability to any knowledge tuple t in the KG and zero to all other tuples. Hao et al. presents a concrete instance of the learning system that incrementally learns (extracts) a commonsense relational KG (Figure 6, right), using the pretrained language models such as BERT  as the experience. Probabilistic graphical models and composite models. The target model pθ(t) can also be probabilistic graphical models , a rich family of models characterizing the conditional dependence structure between random variables with a directed/undirected graph (Figure 5, right). Graphical models may also be composed with the neural modules to form more complex composite models, typically with the neural modules extracting features from the raw inputs and the graphical modules capturing the high-level structures',\n",
       " 'd4ad6579-38a7-41e2-9956-8d75c2958402': 'For example, consider estimating the mean parameter py of a normal distribution M(2; 1,07), with a dataset consisting of m samples: {2, eey xm}, We could use the first sample 2) of the dataset as an unbiased estimator: 6 = 2). In that case, E( Om) = 6, so the estimator is unbiased no matter how many data points are seen. This, of course, implies that the estimate is asymptotically unbiased. However, this is not a consistent estimator as it is not the case that 6,,,  > fAasm— oo. https://www.deeplearningbook.org/contents/ml.html    5.5 Maximum Likelihood Estimation  We have seen some definitions of common estimators and analyzed their properties. But where did these estimators come from? Rather than guessing that some ‘unction might make a good estimator and then analyzing its bias and variance, we would like to have some principle from which we can derive specific functions chat are good estimators for different models. The most common such principle is the maximum likelihood principle',\n",
       " '5994bac3-b6a2-4ede-8c93-a1ddfc8d7a92': 'It concerned Fermat’s last theorem, which claims that there are no positive integer solutions to xn + yn = zn for n > 2. Dirichlet gave a partial proof for the case n = 5, which was sent to Legendre for review and who in turn completed the proof. Later, Dirichlet gave a complete proof for n = 14, although a full proof of Fermat’s last theorem for arbitrary n had to wait until the work of Andrew Wiles in the closing years of the 20th century. in the plane of the simplex and the vertical axis corresponds to the value of the density.\\n\\nHere {αk} = 0.1 on the left plot, {αk} = 1 in the centre plot, and {αk} = 10 in the right plot. modelled using the binomial distribution (2.9) or as 1-of-2 variables and modelled using the multinomial distribution (2.34) with K = 2. The Gaussian, also known as the normal distribution, is a widely used model for the distribution of continuous variables. In the case of a single variable x, the Gaussian distribution can be written in the form where µ is the mean and σ2 is the variance',\n",
       " 'f57efa6f-42aa-41ad-95fb-5a89a71ffc11': 'The natural ﬁrst question is: when does modeling the accuracies of sources improve predictive performance? Further, how many dependencies, such as correlations, are worth modeling? We study the trade-offs between predictive performance and training time in generative models for weak supervision. While modeling source accuracies and correlations will not hurt predictive performance, we present a theoretical analysis of when a simple majority vote will work just as well. Based on our conclusions, we introduce an optimizer for deciding when to model accuracies of labeling functions, and when learning can be skipped in favor of a simple majority vote. Further, our optimizer automatically decides which correlations to model among labeling functions. This optimizer correctly predicts the advantage of generative modeling over majority vote to within 2.16 accuracy points on average on our evaluation tasks, and accelerates pipeline executions by up to 1.8×. It also enables us to gain 60–70% of the beneﬁt of correlation learning while saving up to 61% of training time (34 minutes per execution)',\n",
       " '93162d8a-f194-40a8-8199-18d498e31bf3': 'Data augmentation for low-resource neural machine translation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 567– 573, Vancouver, Canada. Association for Computational Linguistics. Lisa Fan, Dong Yu, and L. Wang. 2018. Robust neural abstractive summarization systems and evaluation against adversarial information. Interpretability and Robustness for Audio, Speech and Language Workshop at Neurips 2018. Steven Y Feng, Varun Gangal, Dongyeop Kang, Teruko Mitamura, and Eduard Hovy. 2020. Genaug: Data augmentation for ﬁnetuning text generators.\\n\\nIn Proceedings of Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 29–42. Steven Y. Feng, Varun Gangal, Jason Wei, Chandar Sarath, Soroush Vosoughi, Teruko Mitamura, and Eduard Hovy. 2021. A survey of data augmentation approaches for nlp',\n",
       " 'eb5b060f-31ab-4269-bfa1-ed10763b3a8e': 'Figure 3.2 shows samples from a Gaussian mixture model. 3.10 Useful Properties of Common Functions  https://www.deeplearningbook.org/contents/prob.html    Certain functions arise often while working with probability distributions, especially the probability distributions used in deep learning models. One of these functions is the logistic sigmoid:  1 o(x) = 1+ exp(—2))  65  (3.30)  CHAPTER 3. PROBABILITY AND INFORMATION THEORY  Figure 3.2: Samples from a Gaussian mixture model. In this example, there are three components. From left to right, the first component has an isotropic covariance matrix, meaning it has the same amount of variance in each direction. The second has a diagonal covariance matrix, meaning it can control the variance separately along each axis-aligned direction. This example has more variance along thex2 axis than along the xz; axis.\\n\\nThe third component has a full-rank covariance matrix, enabling it to control the variance separately along an arbitrary basis of directions',\n",
       " 'ec96ea92-e791-42d0-a835-01bd9f5e1336': 'We have seen that kernel functions correspond to inner products in feature spaces that can have high, or even inﬁnite, dimensionality. By working directly in terms of the kernel function, without introducing the feature space explicitly, it might therefore seem that support vector machines somehow manage to avoid the curse of dimensionality. This is not the case, however, because there are constraints amongst Section 1.4 the feature values that restrict the effective dimensionality of feature space. To see this consider a simple second-order polynomial kernel that we can expand in terms of its components This kernel function therefore represents an inner product in a feature space having six dimensions, in which the mapping from input space to feature space is described by the vector function φ(x). However, the coefﬁcients weighting these different features are constrained to have speciﬁc forms.\\n\\nThus any set of points in the original two-dimensional space x would be constrained to lie exactly on a two-dimensional nonlinear manifold embedded in the six-dimensional feature space. We have already highlighted the fact that the support vector machine does not provide probabilistic outputs but instead makes classiﬁcation decisions for new input vectors. Veropoulos et al. discuss modiﬁcations to the SVM to allow the trade-off between false positive and false negative errors to be controlled',\n",
       " '4b6c2e3d-b0ce-4a09-9c18-a7306ce3fe72': 'Techniques such as “replay bu↵ers” are often used to retain and replay old data so that its beneﬁts are not permanently lost. An honest assessment has to be that current deep learning methods are not well suited to online learning. We see no reason that this limitation is insurmountable, but algorithms that address it, while at the same time retaining the advantages of deep learning, have not yet been devised. Most current deep learning research is directed toward working around this limitation rather than removing it. Second (and perhaps closely related), we still need methods for learning features such that subsequent learning generalizes well. This issue is an instance of a general problem variously called “representation learning,” “constructive induction,” and “meta-learning”— how can we use experience not just to learn a given desired function, but to learn inductive biases such that future learning generalizes better and is thus faster?\\n\\nThis is an old problem, dating back to the origins of artiﬁcial intelligence and pattern recognition in the 1950s and 1960s.1 Such age should give one pause. Perhaps there is no solution. But it is equally likely that the time for ﬁnding a solution and demonstrating its e↵ectiveness has not yet arrived',\n",
       " '4bf03eea-ba63-4c1e-94f3-af5fc33d28ad': 'The weights of the critic are incremented according to the rule above by ↵wδtzw reinforcement signal, δt, corresponds to a dopamine signal being broadcast to all of the critic unit’s synapses. The eligibility trace vector, zw of recent values) of rˆv(St,w). Because ˆv(s,w) is linear in the weights, rˆv(St,w) = x(St).\\n\\nIn neural terms, this means that each synapse has its own eligibility trace, which is t . A synapse’s eligibility trace accumulates according to the level of activity arriving at that synapse, that is, the level of presynaptic activity, represented here by the component of the feature vector x(St) arriving at that synapse. The trace otherwise decays toward zero at a rate governed by the fraction λw. A synapse is eligible for modiﬁcation as long as its eligibility trace is non-zero. How the synapse’s eﬃcacy is actually modiﬁed depends on the reinforcement signals that arrive while the synapse is eligible',\n",
       " 'ed9392a0-ada3-4610-a67f-ba79bbab6700': 'Sterling and Laughlin  examined the neural basis of learning in terms of general design principles that enable eﬃcient adaptive behavior. 15.2 Berridge and Kringelbach  reviewed the neural basis of reward and pleasure, pointing out that reward processing has many dimensions and involves many neural systems.\\n\\nSpace prevents discussion of the inﬂuential research of Berridge and Robinson , who distinguish between the hedonic impact of a stimulus, which they call “liking,” and the motivational e↵ect, which they call “wanting.” Hare, O’Doherty, Camerer, Schultz, and Rangel  examined the neural basis of value-related signals from an economic perspective, distinguishing between goal values, decision values, and prediction errors. Decision value is goal value minus action cost. See also Rangel, Camerer, and Montague , Rangel and Hare , and Peters and B¨uchel . 15.3 The reward prediction error hypothesis of dopamine neuron activity is most prominently discussed by Schultz, Dayan, and Montague . The hypothesis was ﬁrst explicitly put forward by Montague, Dayan, and Sejnowski',\n",
       " '6951bab6-918f-49bd-8703-f7e85bff649f': 'Because the value of the map degrades considerably if the map is inaccurate, it is important to add an address only if the transcription is correct.\\n\\nIf the machine learning system thinks hat it is less likely than a human being to obtain the correct transcription, then the best course of action is to allow a human to transcribe the photo instead. Of course, che machine learning system is only useful if it is able to dramatically reduce the amount of photos that the human operators must process. A natural performance metric to use in this situation is coverage. Coverage is the fraction of examples or which the machine learning system is able to produce a response. It is possible o trade coverage for accuracy. One can always obtain 100 percent accuracy by refusing to process any example, but this reduces the coverage to 0 percent. For the  Street View task, the goal for the project was to reach human-level transcription  https://www.deeplearningbook.org/contents/guidelines.html    accuracy while maintaining 95 percent coverage. Human-level performance on this task is 98 percent accuracy. Many other metrics are possible. We can, for example, measure click-through rates, collect user satisfaction surveys, and so on',\n",
       " '9e6dbac5-a81f-4259-b704-4f046239b498': 'The corresponding decision boundary is therefore deﬁned by the relation y(x) = 0, which corresponds to a (D − 1)-dimensional hyperplane within the D-dimensional input space. Consider two points xA and xB both of which lie on the decision surface. Because y(xA) = y(xB) = 0, we have wT(xA −xB) = 0 and hence the vector w is orthogonal to every vector lying within the decision surface, and so w determines the orientation of the decision surface. Similarly, if x is a point on the decision surface, then y(x) = 0, and so the normal distance from the origin to the decision surface is given by wTx ∥w∥ = − w0 We therefore see that the bias parameter w0 determines the location of the decision surface.\\n\\nThese properties are illustrated for the case of D = 2 in Figure 4.1. Furthermore, we note that the value of y(x) gives a signed measure of the perpendicular distance r of the point x from the decision surface. To see this, consider linear discriminant function in two dimensions',\n",
       " 'eebd1969-c5c2-47e7-8a8f-8849cd2afba5': 'This was repeated to increase the dataset size from N to 2 N. The GAN style transfer baseline uses 6 different styles to transform images (Cezanne, Enhance, Monet, Uki- yoe, Van Gogh and Winter). The Neural Augmentation techniques tested consist of three levels based on the design of the loss function for the augmentation net (Con- tent loss, Style loss via gram matrix, and no loss computer at this layer).\\n\\nAll experi- ments are tested with a convolutional network consisting of 3 convolutional layers each followed by max pooling and batch normalization, followed by 2 fully-connected layers. Each experiment runs for 40 epochs at a learning rate of 0.0001 with the Adam optimization technique (Table 7). The results of the experiment are very promising. The Neural Augmentation tech- nique performs significantly better on the Dogs versus Goldfish study and only slightly worse on Dogs versus Cats. The technique does not have any impact on the MNIST problem. The paper suggests that the likely best strategy would be to combine  the traditional augmentations and the Neural Augmentations',\n",
       " 'b2b2d492-00c3-4180-8826-12edf4307ee6': 'Consider the learning algorithm that is just like Q-learning except that instead of the maximum over next state–action pairs it uses the expected value, taking into account how likely each action is under the current policy. That is, consider the algorithm with the update rule but that otherwise follows the schema of Q-learning. Given the next state, St+1, this algorithm moves deterministically in the same direction as Sarsa moves in expectation, and accordingly it is called Expected Sarsa. Its backup diagram is shown on the right in Figure 6.4. Expected Sarsa is more complex computationally than Sarsa but, in return, it eliminates the variance due to the random selection of At+1. Given the same amount of experience we might expect it to perform slightly better than Sarsa, and indeed it generally does. Figure 6.3 shows summary results on the cli↵-walking task with Expected Sarsa compared to Sarsa and Q-learning. Expected Sarsa retains the signiﬁcant advantage of Sarsa over Q-learning on this problem.\\n\\nIn addition, Expected Sarsa shows a signiﬁcant improvement For n = 100, 000, the average return is equal for all ↵ values in case of Expected Sarsa and Q-learning',\n",
       " 'ec731ddd-7c1d-4b79-ab13-5e1930adb1f5': 'The loss ty. y) depends on the output g and on the target y (see section 6.2.1.1 for examples of loss functions). To  obtain the total cost J, the loss may be added to a regularizer 2(@), where 0 contains all the parameters (weights and biases). Algorithm 6.4 shows how to compute gradients of J with respect to parameters W and b. For simplicity, this demonstration uses only a single input example x. Practical applications should use a minibatch. See section 6.5.7 for a more realistic demonstration. Require: Network depth, | Require: W“),i € {1,...,1}, the weight matrices of the model Require: b®,i € {1,...,1}, the bias parameters of the model Require: x, the input to process Require: y, the target output  hO =a  h®) = f(a) end for g=hO J = LY,y) + AQ(8)  208  CHAPTER 6',\n",
       " 'e35cd4dc-01d8-4637-b887-0bfa845f559f': 'Data warping augmentations transform exist- ing images such that their label is preserved. This encompasses augmentations such as geometric and color transformations, random erasing, adversarial training, and neural style transfer. Oversampling augmentations create synthetic instances and add them to the training set. This includes mixing images, feature space augmentations, and genera- tive adversarial networks (GANs). Oversampling and Data Warping augmentations do not form a mutually exclusive dichotomy. For example, GAN samples can be stacked  with random cropping to further inflate the dataset.\\n\\nDecisions around final dataset size,   Shorten and Khoshgoftaar J Big Data  6:60   Image Data ‘Augmentation  Basic Image Deep Learning Manipulations Approaches  Color Space Transformations  | AutoAugment  Kemel Filters Random Erasing Geometric Transformations  ‘Adversarial Training |__| Neural Style Transfer  ‘Augmentation  | GAN Data |  Neural Augmentation ‘Smart Augmentation  Fig',\n",
       " '3f53e1b0-7ce3-4e01-bba1-1ef7f2247bdf': 'The L1-distance between two embeddings is | fg(x;) — fo(x;)|. The distance is converted to a probability p by a linear feedforward layer and sigmoid.\\n\\nIt is the probability of whether two images are drawn from the same class. Intuitively the loss is cross entropy because the label is binary. P(X, Xj) = o(W| fo(x;) — fo(x;))) L£(B) = S Ly,=y; log p(xi, xj) + (1 — 1y,=y;) log(1 — p(xi, x;))  (xixj,yiys)EB  Images in the training batch B can be augmented with distortion. Of course, you can replace the L1 distance with other distance metric, L2, cosine, etc. Just make sure they are differential and then  everything else works the same',\n",
       " 'cf0f895a-fd32-4713-80b3-a1d454bdbebd': ', αK)T, we have is known as the digamma function . The parameters αk are subject to the constraint αk > 0 in order to ensure that the distribution can be normalized. The Dirichlet forms the conjugate prior for the multinomial distribution and represents a generalization of the beta distribution. In this case, the parameters αk can be interpreted as effective numbers of observations of the corresponding values of the K-dimensional binary observation vector x. As with the beta distribution, the Dirichlet has ﬁnite density everywhere provided αk ⩾ 1 for all k. The Gamma is a probability distribution over a positive random variable τ > 0 governed by parameters a and b that are subject to the constraints a > 0 and b > 0 to ensure that the distribution can be normalized. where ψ(·) is the digamma function deﬁned by (B.25). The gamma distribution is the conjugate prior for the precision (inverse variance) of a univariate Gaussian',\n",
       " 'cb9a31cb-44c7-45bf-ba7a-7e625ce61eb4': 'Non-contrastive methods for joint- embedding include DeeperCluster, ClusterFit, MoCo-v2, SwAV, SimSiam, Barlow Twins, BYOL from DeepMind, and a few others. They use various  tricks, such as computing virtual target embeddings for groups of similar images (DeeperCluster, SwAV, SimSiam) or making the two joint embedding architectures slightly different through the architecture or the parameter vector (BYOL, MoCo). Barlow Twins tries to minimize the redundancy between the individual components of the embedding vectors. Perhaps a better alternative in the long run will be to devise non-contrastive methods with latent-variable predictive models. The main obstacle is that they require a way to minimize the capacity of the latent variable.\\n\\nThe volume of the set over which the latent variable can vary limits the volume of outputs that take low energy. By minimizing this volume, one automatically shapes the energy in the right way',\n",
       " '73a03ff3-b17e-4281-9884-4b67644717dc': 'Here η are called the natural parameters of the distribution, and u(x) is some function of x.\\n\\nThe function g(η) can be interpreted as the coefﬁcient that ensures that the distribution is normalized and therefore satisﬁes where the integration is replaced by summation if x is a discrete variable. We begin by taking some examples of the distributions introduced earlier in the chapter and showing that they are indeed members of the exponential family. Consider ﬁrst the Bernoulli distribution Expressing the right-hand side as the exponential of the logarithm, we have Comparison with (2.194) allows us to identify which we can solve for µ to give µ = σ(η), where is called the logistic sigmoid function. Thus we can write the Bernoulli distribution using the standard representation (2.194) in the form where we have used 1 − σ(η) = σ(−η), which is easily proved from (2.199). Comparison with (2.194) shows that Next consider the multinomial distribution that, for a single observation x, takes the form where ηk = ln µk, and we have deﬁned η = (η1, . , ηM)T',\n",
       " '92b0a4ee-0838-43ab-950a-a022334c8921': 'Thus, each actor unit is itself a reinforcement learning agent—a hedonistic neuron if you will. Now, to make the situation as simple as possible, assume that each of these units receives the same reward signal at the same time (although, as indicated above, the assumption that dopamine is released at all the corticostriatal synapses under the same conditions and at the same times is likely an oversimpliﬁcation). What can reinforcement learning theory tell us about what happens when all members of a population of reinforcement learning agents learn according to a common reward signal?\\n\\nThe ﬁeld of multi-agent reinforcement learning considers many aspects of learning by populations of reinforcement learning agents. Although this ﬁeld is beyond the scope of this book, we believe that some of its basic concepts and results are relevant to thinking about the brain’s di↵use neuromodulatory systems. In multi-agent reinforcement learning (and in game theory), the scenario in which all the agents try to maximize a common reward signal that they simultaneously receive is known as a cooperative game or a team problem. What makes a team problem interesting and challenging is that the common reward signal sent to each agent evaluates the pattern of activity produced by the entire population, that is, it evaluates the collective action of the team members',\n",
       " '9b65f6f6-2688-4f7d-9e8f-8682ca75f4b9': \"ie tee eet  MN MN WAN ”™ MN ™ aA AA ABA AA AAA ABA AA AAA APA AA A Lull. i. alias a, Hs ts z  https://lilianweng.github.io/posts/2018-02-19-rl-overview/  A (Long) Peek into Reinforcement Learning | Lil'Log  The network is trained with the samples in the replay memory to minimize the loss: L=(z-v)?—-7' logp+eljd||? where c is a hyperparameter controlling the intensity of L2 penalty to avoid overfitting. AlphaGo Zero simplified AlphaGo by removing supervised learning and merging separated policy and value networks into one. It turns out that AlphaGo Zero achieved largely improved performance with a much shorter training time! | strongly recommend reading these two papers side by side and compare the difference, super fun. | know this is a long read, but hopefully worth it. /f you notice mistakes and errors in this post, don’t hesitate to contact me at\",\n",
       " '460023b1-0a35-49a0-b79c-ac5ca7c7de61': 'A candidate is then a tuple of two Spans.\\n\\nSnorkel uses the core abstraction of a labeling function to allow users to specify a wide range of weak supervision sources such as patterns, heuristics, external knowledge bases, crowdsourced labels, and more. This higher-level, less precise input is more efﬁcient to provide (see Sect. 4.2) and can be automatically denoised and synthesized, as described in subsequent sections. In this section, we describe our design choices in building an interface for writing labeling functions, which we envision as a unifying programming language for weak supervision. These choices were informed to a large degree by our interactions—primarily through weekly ofﬁce hours—with Snorkel users in bioinformatics, defense, industry, and other areas over the past year.3 For example, while we initially intended to have a more complex structure for labeling functions,withmanuallyspeciﬁedtypesandcorrelationstructure, we quickly found that simplicity in this respect was critical to usability (and not empirically detrimental to our ability to model their outputs). We also quickly discovered that users wanted either far more expressivity or far less of it, compared to our ﬁrst library of function templates',\n",
       " '92cebc60-7a64-4c7d-8006-57ba3c8ad1f9': 'Training generative neural networks via maximum mean discrepancy optimization. CoRR, abs/1505.03906, 2015.\\n\\nAude Genevay, Marco Cuturi, Gabriel Peyr´e, and Francis Bach. Stochastic optimization for large-scale optimal transport. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 3440–3448. Curran Associates, Inc., 2016. Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David WardeFarley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, pages 2672–2680. Curran Associates, Inc., 2014. Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Sch¨olkopf, and Alexander Smola',\n",
       " 'f097da1c-f2f6-4c45-9604-6c3a7dc1dfe0': 'The system then explores the distribution along the more extended direction by means of a random walk, and so the number of steps to arrive at a state that is more or less independent of the original state is of order (σmax/σmin)2.\\n\\nIn fact in two dimensions, the increase in rejection rate as ρ increases is offset by the larger steps sizes of those transitions that are accepted, and more generally for a multivariate Gaussian the number of steps required to obtain independent samples scales like (σmax/σ2)2 where σ2 is the second-smallest standard deviation . These details aside, it remains the case that if the length scales over which the distributions vary are very different in different directions, then the Metropolis Hastings algorithm can have very slow convergence. Gibbs sampling  is a simple and widely applicable Markov chain Monte Carlo algorithm and can be seen as a special case of the MetropolisHastings algorithm. Consider the distribution p(z) = p(z1, . , zM) from which we wish to sample, and suppose that we have chosen some initial state for the Markov chain',\n",
       " '01b89801-fb59-462e-bc29-742bd9c4a4f8': \"i j == do fa)  i=1,xOnp  can be transformed into an importance sampling estimator  42 SS pa) fa) (17.10)  n ining q(x)  We see readily that the expected value of the estimator does not depend on q: Eq = Eq = s. (17.11)  The variance of an importance sampling estimator, however, can be greatly sensitive to the choice of g. The variance is given by  Var = vane. (17.12) The minimum variance occurs when q is q(x) = Pee) (17.13)  https://www.deeplearningbook.org/contents/monte_carlo.html    where Z is the normalization constant, chosen so that q7 (2) sums or integrates to 1 as appropriate. Better importance sainplin distributions put more weight where the integrand is larger. In fact, when /' a) does not change sign, Var  = 0,  meaning that a single sample is yen when the optimal distribution is used. Of course, this is only because the computation of g* has essentially solved the original problem, so it is usually not practical to use this approach of drawing a single sample from the optimal distribution\",\n",
       " 'cbd10023-7bbd-4a87-92f4-62d44a76ffef': 'One consequence of the generality of the potential functions ψC(xC) is that their product will in general not be correctly normalized. We therefore have to introduce an explicit normalization factor given by (8.40). Recall that for directed graphs, the joint distribution was automatically normalized as a consequence of the normalization of each of the conditional distributions in the factorization.\\n\\nThe presence of this normalization constant is one of the major limitations of undirected graphs. If we have a model with M discrete nodes each having K states, then the evaluation of the normalization term involves summing over KM states and so (in the worst case) is exponential in the size of the model. The partition function is needed for parameter learning because it will be a function of any parameters that govern the potential functions ψC(xC). However, for evaluation of local conditional distributions, the partition function is not needed because a conditional is the ratio of two marginals, and the partition function cancels between numerator and denominator when evaluating this ratio. Similarly, for evaluating local marginal probabilities we can work with the unnormalized joint distribution and then normalize the marginals explicitly at the end',\n",
       " 'e37eef21-2403-497d-9098-d29a1a4632a4': 'This simple design choice conveniently decouples the predictive task from other components such as the neural network architecture. Broader contrastive prediction tasks can be deﬁned by extending the family of augmentations and composing them stochastically. 3.1. Composition of data augmentation operations is crucial for learning good representations To systematically study the impact of data augmentation, we consider several common augmentations here. One type of augmentation involves spatial/geometric transformation of data, such as cropping and resizing (with horizontal ﬂipping), rotation  and cutout .\\n\\nThe other type of augmentation involves appearance transformation, such as color distortion (including color dropping, brightness, contrast, saturation, hue) , Gaussian blur, and Sobel ﬁltering. Figure 4 visualizes the augmentations that we study in this work. To understand the effects of individual data augmentations and the importance of augmentation composition, we investigate the performance of our framework when applying augmentations individually or in pairs. Since ImageNet images are of different sizes, we always apply crop and resize images , which makes it difﬁcult to study other augmentations in the absence of cropping. To eliminate this confound, we consider an asymmetric data transformation setting for this ablation',\n",
       " '163cc60e-d136-45ed-bed2-f09de0130fb3': 'The output unit formed the weighted sum and then passed it through the same sigmoid nonlinearity. TD-Gammon used the semi-gradient form of the TD(λ) algorithm described in Section 12.2, with the gradients computed by the error backpropagation algorithm . Recall that the general update rule for this case is where wt is the vector of all modiﬁable parameters (in this case, the weights of the network) and zt is a vector of eligibility traces, one for each component of wt, updated by with z0 .= 0. The gradient in this equation can be computed eﬃciently by the backpropagation procedure. For the backgammon application, in which γ = 1 and the reward is always zero except upon winning, the TD error portion of the learning rule is usually just ˆv(St+1,w) − ˆv(St,w), as suggested in Figure 16.1. To apply the learning rule we need a source of backgammon games.\\n\\nTesauro obtained an unending sequence of games by playing his learning backgammon player against itself',\n",
       " '820cda60-7856-458e-929e-573641baea7d': 'For example, it is not well suited to nonstationary problems because its drive for exploration is inherently temporary. If the task changes, creating a renewed need for exploration, this method cannot help. Indeed, any method that focuses on the initial conditions in any special way is unlikely to help with the general nonstationary case. The beginning of time occurs only once, and thus we should not focus on it too much. This criticism applies as well to the sample-average methods, which also treat the beginning of time as a special event, averaging all subsequent rewards with equal weights. Nevertheless, all of these methods are very simple, and one of them—or some simple combination of them—is often adequate in practice. In the rest of this book we make frequent use of several of these simple exploration techniques. because they are averages over 2000 individual, randomly chosen 10-armed bandit tasks. Why, then, are there oscillations and spikes in the early part of the curve for the optimistic method?\\n\\nIn other words, what might make this method perform particularly better or worse, on average, on particular early steps? ⇤ sample averages to estimate action values because sample averages do not produce the initial bias that constant step sizes do (see the analysis leading to (2.6))',\n",
       " '493d50a5-9344-4f0a-8522-0ba38a4e95df': 'From each of the |S| states, two actions were possible, each of which resulted in one of b next states, all equally likely, with a di↵erent random selection of b states for each state–action pair. The branching factor, b, was the same for all state–action pairs. In addition, on all transitions there was a 0.1 probability of transition to the terminal state, ending the episode. The expected reward on each transition was selected from a Gaussian distribution with mean 0 and variance 1. the start state. If there are many states and a small branching factor, this e↵ect will be large and long-lasting. In the long run, focusing on the on-policy distribution may hurt because the commonly occurring states all already have their correct values. Sampling them is useless, whereas sampling other states may actually perform some useful work',\n",
       " '6883101d-5e83-410d-8a37-3f614d874e1b': 'Sometimes, however, what is shared among the different tasks is not the semantics of the input but the semantics of the output. For example, a speech recognition system needs to produce valid sentences at the output layer, but the earlier layers near the input may need to recognize very different versions of the same phonemes or subphonemic vocalizations depending on which person is speaking. In cases like these, it makes more sense to share the upper layers  534  CHAPTER 15.\\n\\nREPRESENTATION LEARNING  7 ™  https://www.deeplearningbook.org/contents/representation.html    Figure 15.2: Example architecture for multitask or transfer learning when the output variable y has the same semantics for all tasks while the input variable x has a different meaning (and possibly even a different dimension) for each task (or, for example, each user), called x, x) and x) for three tasks. The lower levels (up to the selection switch) are task-specific, while the upper levels are shared. The lower levels learn to translate their task-specific input into a generic set of features. (near the output) of the neural network and have a task-specific preprocessing, as illustrated in figure 15.2',\n",
       " '29cf5952-4803-432f-8359-9f8f3b8233a4': 'Recognizing that most machine learning algorithms can be described using this recipe helps to see the different algorithms as part of a axonomy of methods for doing related tasks that work for similar reasons, rather chan as a long list of algorithms that each have separate justifications.\\n\\n5.11 Challenges Motivating Deep Learning  The simple machine learning algorithms described in this chapter work well on a  sa to ee 4 4 11 mi 1 4 road : 1:  https://www.deeplearningbook.org/contents/ml.html    WIUE VaLIELy OL LILpPOrlallt P such as illey lave WO SUCCECUCU, LLOWEVEL, Ll SOLVILLY the central problems in Al, such as recognizing speech or recognizing objects. The development of deep learning was motivated in part by the failure of traditional algorithms to generalize well on such AI tasks. This section is about how the challenge of generalizing to new examples becomes exponentially more difficult when working with high-dimensional data, and how the mechanisms used to achieve generalization in traditional machine learning are insufficient to learn complicated functions in high-dimensional spaces. Such spaces also often impose high computational costs. Deep learning was designed to overcome these and other obstacles',\n",
       " '639fae02-c847-44f5-a035-7b7122aa538d': 'In particular, by considering the speciﬁc instantiation in Equation 5.1 of the SE and setting D to the JS divergence, we can derive the algorithm for learning the generative adversarial networks  as shown below. From this perspective, the key concept in generative adversarial learning, namely the discriminator, arises as an approximation to the optimization procedure. We discuss in Section 6 an alternative view of the learning paradigm where the discriminator plays the role of ‘dynamic’ experience in SE. Generative adversarial learning: The functional descent view.\\n\\nTo optimize the objective in Equation 5.1 with the JS divergence, probability functional descent  oﬀers an elegant way that recovers the optimization procedure of GANs originally developed in Goodfellow et al. Here we give the PFD result directly, and provide a more detailed review of the PFD optimization in Section 7. Speciﬁcally, let J(p) := D(pd, p) in Equation 5.1, which is a functional on the distribution p ∈ P(T )',\n",
       " 'd569dcd1-e339-45cd-bcfa-89825557875e': ', N, and we set �f0(θ) equal to the prior p(θ).\\n\\nNote that the use of N(θ|·, ·) does not imply that the right-hand side is a well-deﬁned Gaussian density (in fact, as we shall see, the variance parameter vn can be negative) but is simply a convenient shorthand notation. The approximations �fn(θ), for n = 1, . , N, can be initialized to unity, corresponding to sn = (2πvn)D/2, vn → ∞ and mn = 0, where D is the dimensionality of x and hence of θ. The initial q(θ), deﬁned by (10.191), is therefore equal to the prior. We then iteratively reﬁne the factors by taking one factor fn(θ) at a time and applying (10.205), (10.206), and (10.207). Note that we do not need to revise the term f0(θ) because an EP update will leave this term unchanged. Here we state the Exercise 10.37 results and leave the reader to ﬁll in the details',\n",
       " 'f037a68c-bdc4-443d-8658-c7a03bfb6b0e': 'Jason Wei and Kai Zou. 2019. EDA: Easy data augmentation techniques for boosting performance on text classiﬁcation tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 6382–6388, Hong Kong, China. Association for Computational Linguistics. Xing Wu, Shangwen Lv, Liangjun Zang, Jizhong Han, and Songlin Hu. 2019a. Conditional bert contextual augmentation. In International Conference on Computational Science, pages 84–95. Springer. Zhanghao Wu, Shuai Wang, Yanmin Qian, and Kai Yu. 2019b.\\n\\nData Augmentation Using Variational Autoencoder for Embedding Based Speaker Veriﬁcation. In Proc. Interspeech 2019, pages 1163–1167. Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. 2020. Unsupervised data augmentation for consistency training',\n",
       " '74e6a0a3-e275-437b-bdd2-791cd5b1d5e2': 'That transition would reduce w, unless it were to a state whose value was higher (because γ < 1) than 2w, and then that state would have to be followed by a state of even higher value, or else again w would be reduced. Each state can support the one before only by creating a higher expectation. Eventually the piper must be paid. In the on-policy case the promise of future reward must be kept and the system is kept in check. But in the o↵-policy case, a promise can be made and then, after taking an action that the target policy never would, forgotten and forgiven. This simple example communicates much of the reason why o↵-policy training can lead to divergence, but it is not completely convincing because it is not complete—it is just a fragment of a complete MDP. Can there really be a complete system with instability? A simple complete example of divergence is Baird’s counterexample. Consider the episodic seven-state, two-action MDP shown in Figure 11.1',\n",
       " 'faa23c68-88c2-4d2a-9ba5-cfea8ccb8aac': 'This has led to a sequence of progressively more complex architec- tures from AlexNet  to VGG-16 , ResNet , Inception-V3 , and DenseNet .\\n\\nFunctional solutions such as dropout regularization, batch normalization, transfer learn-  ing, and pretraining have been developed to try to extend Deep Learning for application on smaller datasets. A brief description of these overfitting solutions is provided below. A complete survey of regularization methods in Deep Learning has been compiled by Kukacka et al. Knowledge of these overfitting solutions will inform readers about other existing tools, thus framing the high-level context of Data Augmentation and Deep  Learning. + Dropout  is a regularization technique that zeros out the activation values of ran- domly chosen neurons during training. This constraint forces the network to learn more robust features rather than relying on the predictive capability of a small subset  of neurons in the network. Tompson et al. extended this idea to convolutional Shorten and Khoshgoftaar J Big Data  6:60  networks with Spatial Dropout, which drops out entire feature maps rather than individual neurons',\n",
       " '7ef6e7c5-cd0b-461f-b8d3-0bea61303851': 'Consider a set of m examples X = {a, weey x(™)} drawn independently from she true but unknown data-generating distribution p gata(x).\\n\\nLet Pmodel(x;@) be a parametric family of probability distributions over the same space indexed by @. In other words, pmodel(@;@) maps any configuration x (0 a real number estimating the true probability paata(a). The maximum likelihood estimator for 8 is then defined as  Oui = arg max Dmodel(X; 0), (5.56) 0 = arg max | | pmoae(«; 0). (5.57) o j=l  This product over many probabilities can be inconvenient for various reasons. For example, it is prone to numerical underflow. To obtain a more convenient but equivalent optimization problem, we observe that taking the logarithm of the likelihood does not change its arg max but does conveniently transform a product  129  CHAPTER 5. MACHINE LEARNING BASICS  into a sum: m  Ov, =argmax — log pmodei(a\"” ; 8). (5.58) 0',\n",
       " '14cc5937-8840-4584-b7dd-e0079b0340d9': 'Intractable inference problems in deep learning usually arise from interactions  between latent variables in a structured graphical model. See figure 19.1 for some examples.\\n\\nThese interactions may be due to direct interactions in undirected models or “explaining away” interactions between mutual ancestors of the same visible unit in directed models. https://www.deeplearningbook.org/contents/inference.html    629  CHAPTER 19. APPROXIMATE INFERENCE  OVO  Figure 19.1: Intractable inference problems in deep learning are usually the result of interactions between latent variables in a structured graphical model. These interactions can be due to edges directly connecting one latent variable to another or longer paths that are activated when the child of a V-structure is observed. (Left)A semi-restricted Boltzmann machine  with connections between hidden units. These direct connections between latent variables make the posterior distribution intractable because of large cliques of latent variables. (Center)A deep Boltzmann machine, organized into layers of variables without intralayer connections, still has an intractable posterior distribution because of the connections between layers',\n",
       " '50ce2292-055a-460f-af4b-ecdf1f84ddc4': 'Batch techniques, such as the maximum likelihood solution (3.15), which involve processing the entire training set in one go, can be computationally costly for large data sets.\\n\\nAs we have discussed in Chapter 1, if the data set is sufﬁciently large, it may be worthwhile to use sequential algorithms, also known as on-line algorithms, in which the data points are considered one at a time, and the model parameters updated after each such presentation. Sequential learning is also appropriate for realtime applications in which the data observations are arriving in a continuous stream, and predictions must be made before all of the data points are seen. We can obtain a sequential learning algorithm by applying the technique of stochastic gradient descent, also known as sequential gradient descent, as follows. If the error function comprises a sum over data points E = � where τ denotes the iteration number, and η is a learning rate parameter. We shall discuss the choice of value for η shortly. The value of w is initialized to some starting vector w(0). For the case of the sum-of-squares error function (3.12), this gives where φn = φ(xn). This is known as least-mean-squares or the LMS algorithm',\n",
       " 'f6849d87-9d7d-49f9-acb5-cf0dac7765ca': 'Multiple manifolds are likely involved in most applications. For example, the manifold of human face images may not be connected to the manifold of cat face images. These thought experiments convey some intuitive reasons supporting the mani- fold hypothesis. More rigorous experiments  clearly support the hypothesis for a large class of datasets of interest in AI. When the data lies on a low-dimensional manifold, it can be most natural for machine learning algorithms to represent the data in terms of coordinates on the manifold, rather than in terms of coordinates in IR”. In everyday life, we can think of roads as 1-D manifolds embedded in 3-D space. We give directions to specific addresses in terms of address numbers along these 1-D roads, not in terms of coordinates in 3-D space.\\n\\nExtracting these manifold coordinates is challenging but holds the promise of improving many machine learning algorithms. This general  https://www.deeplearningbook.org/contents/ml.html    principle is applied in many contexts. Figure 5.13 shows the manifold structure of a dataset consisting of faces. By the end of this book, we will have developed the methods necessary to learn such a manifold structure',\n",
       " '74169b67-e016-4ac5-9389-59045e2c4c43': 'A weight representing the eﬃcacy of a synapse is associated with each connection from each feature xi to the critic unit, V , and to each of the action units, Ai. The weights in the critic network parameterize the value function, and the weights in the actor network parameterize the policy.\\n\\nThe networks learn as these weights change according to the critic and actor learning rules that we describe in the following section. The TD error produced by circuitry in the critic is the reinforcement signal for changing the weights in both the critic and the actor networks. This is shown in Figure 15.5a by the line labeled ‘TD error δ’ extending across all of the connections in the critic and actor networks. This aspect of the network implementation, together with the reward prediction error hypothesis and the fact that the activity of dopamine neurons is so widely distributed by the extensive axonal arbors of these neurons, suggests that an actor–critic network something like this may not be too farfetched as a hypothesis about how reward-related learning might happen in the brain. map onto structures in the brain according to the hypothesis of Takahashi et al',\n",
       " 'd82061df-70a3-4a6a-96a3-ef7c286ad3be': 'Alternatively, Rφ can return a valid reward even when x matches a data example only in part, or (x, y) is an entire new sample not in D, which in effect makes data augmentation and data synthesis, respectively, in which cases φ is either a data transformer or a generator. In the next section, we demonstrate two particular parameterizations for data augmentation and weighting, respectively. We thus have shown that the diverse types of manipulation all boil down to a parameterized data reward Rφ. Such an concise, uniform formulation of data manipulation has the advantage that, once we devise a method of learning the manipulation parameters φ, the resulting algorithm can directly be applied to automate any manipulation type. We present a learning algorithm next.\\n\\nLearning Manipulation Parameters To learn the parameters φ in the manipulation reward Rφ(x, y|D), we could in principle adopt any off-the-shelf reward learning algorithm in the literature. In this work, we draw inspiration from the above gradient-based reward learning (section 3) due to its simplicity and efﬁciency',\n",
       " '33e829bd-8595-4227-adaa-80c87facad42': 'A more powerful approach, however, models the conditional probability distribution p(Ck|x) in an inference stage, and then subsequently uses this distribution to make optimal decisions. By separating inference and decision, we gain numerous beneﬁts, as discussed in Section 1.5.4. There are two different approaches to determining the conditional probabilities p(Ck|x). One technique is to model them directly, for example by representing them as parametric models and then optimizing the parameters using a training set. Alternatively, we can adopt a generative approach in which we model the class-conditional densities given by p(x|Ck), together with the prior probabilities p(Ck) for the classes, and then we compute the required posterior probabilities using Bayes’ theorem We shall discuss examples of all three approaches in this chapter. In the linear regression models considered in Chapter 3, the model prediction y(x, w) was given by a linear function of the parameters w. In the simplest case, the model is also linear in the input variables and therefore takes the form y(x) = wTx+w0, so that y is a real number',\n",
       " '7aa46f62-a10d-4dd1-a0af-742efacbc44b': 'The centres Section 2.3.9 and variances of the Gaussian components, as well as the mixing coefﬁcients, will be considered as adjustable parameters to be determined as part of the learning process. Thus, we have a probability density of the form and πj are the mixing coefﬁcients. Taking the negative logarithm then leads to a regularization function of the form The total error function is then given by where λ is the regularization coefﬁcient. This error is minimized both with respect to the weights wi and with respect to the parameters {πj, µj, σj} of the mixture model. If the weights were constant, then the parameters of the mixture model could be determined by using the EM algorithm discussed in Chapter 9. However, the distribution of weights is itself evolving during the learning process, and so to avoid numerical instability, a joint optimization is performed simultaneously over the weights and the mixture-model parameters. This can be done using a standard optimization algorithm such as conjugate gradients or quasi-Newton methods',\n",
       " '837a9414-c572-48cd-8490-5080792f8a95': 'If all of the eigenvalues λi are positive, then these surfaces represent ellipsoids, with their centres at µ and their axes oriented along ui, and with scaling factors in the directions of the axes given by λ1/2 For the Gaussian distribution to be well deﬁned, it is necessary for all of the eigenvalues λi of the covariance matrix to be strictly positive, otherwise the distribution cannot be properly normalized.\\n\\nA matrix whose eigenvalues are strictly positive is said to be positive deﬁnite. In Chapter 12, we will encounter Gaussian distributions for which one or more of the eigenvalues are zero, in which case the distribution is singular and is conﬁned to a subspace of lower dimensionality. If all of the eigenvalues are nonnegative, then the covariance matrix is said to be positive semideﬁnite. Now consider the form of the Gaussian distribution in the new coordinate system deﬁned by the yi. In going from the x to the y coordinate system, we have a Jacobian matrix J with elements given by where Uji are the elements of the matrix UT',\n",
       " 'e229acc4-7aeb-41bc-a6f9-69991368a0e3': 'The indices into W are respectively: 7; the  345  CHAPTER 9. CONVOLUTIONAL NETWORKS  output channel; 7, the output row; k, the output column; J, the input channel; m, the row offset within the input; and n, the column offset within the input. The linear part of a locally connected layer is then given by  Zijk = >  « (9.9)  lym,n  This is sometimes also called unshared convolution, because it is a similar oper- ation to discrete convolution with a small kernel, but without sharing parameters across locations. Figure 9.14 compares local connections, convolution, and full connections. Locally connected layers are useful when we know that each feature should be a function of a small part of space, but there is no reason to think that the same  https://www.deeplearningbook.org/contents/convnets.html    feature should occur across all of space.\\n\\nFor example, if we want to tell if an image is a picture of a face, we only need to look for the mouth in the bottom half of the image',\n",
       " 'd172bfcf-1721-45ea-a158-61db215f8ed8': 'All methods that follow this general schema we call policy gradient methods, whether or not they also learn an approximate value function. Methods that learn approximations to both policy and value functions are often called actor–critic methods, where ‘actor’ is a reference to the learned policy, and ‘critic’ refers to the learned value function, usually a state-value function. First we treat the episodic case, in which performance is deﬁned as the value of the start state under the parameterized policy, before going on to consider the continuing case, in which performance is deﬁned as the average reward rate, as in Section 10.3.\\n\\nIn the end, we are able to express the algorithms for both cases in very similar terms. 1The lone exception is the gradient bandit algorithms of Section 2.8. In fact, that section goes through many of the same steps, in the single-state bandit case, as we go through here for full MDPs. Reviewing that section would be good preparation for fully understanding this chapter',\n",
       " 'd4b75302-4adb-49c2-9a1a-7e0e3b214329': 'In this case there is no linear term, because ∇E = 0 at w⋆, and (5.28) becomes where the Hessian H is evaluated at w⋆.\\n\\nIn order to interpret this geometrically, consider the eigenvalue equation for the Hessian matrix where the eigenvectors ui form a complete orthonormal set (Appendix C) so that We now expand (w − w⋆) as a linear combination of the eigenvectors in the form This can be regarded as a transformation of the coordinate system in which the origin is translated to the point w⋆, and the axes are rotated to align with the eigenvectors (through the orthogonal matrix whose columns are the ui), and is discussed in more detail in Appendix C. Substituting (5.35) into (5.32), and using (5.33) and (5.34), allows the error function to be written in the form A matrix H is said to be positive deﬁnite if, and only if, Because the eigenvectors {ui} form a complete set, an arbitrary vector v can be written in the form and so H will be positive deﬁnite if, and only if, all of its eigenvalues are positive',\n",
       " '90b10eed-c578-42b9-9404-d1ecb936d3de': 'Likewise, Carol only gets to start running after Bob finishes, so Bob’s finishing time t; directly influences Carol’s finishing time tz. bar. In other words, the distribution over b depends on the value of a. Continuing with the relay race example from section 16.1, suppose we name Alice’s finishing time to, Bob’s finishing time ti, and Carol’s finishing time to. As we saw earlier, our estimate of t; depends on to.\\n\\nOur estimate of tz depends directly on ty but only indirectly on to. We can draw this relationship in a directed graphical model, illustrated in figure 16.2. Formally, a directed graphical model defined on variables x is defined by a directed acyclic graph whose vertices are the random variables in the model, and  eV i-d 22 ata Lee bettal',\n",
       " 'acc60d64-01b6-4248-a275-789dae638046': 'DEEP FEEDFORWARD NETWORKS  same cost, then we may analyze the computational cost in terms of the number of operations executed. Keep in mind here that we refer to an operation as the fundamental unit of our computational graph, which might actually consist of several arithmetic operations (for example, we might have a graph that treats matrix multiplication as a single operation). Computing a gradient in a graph with n nodes will never execute more than O(n?) operations or store the output of more than O(n?) operations. Here we are counting operations in the computational graph, not individual operations executed by the underlying hardware, so it is important to remember that the runtime of each operation may be highly variable. For example, multiplying two matrices that each contain millions of entries might correspond to a single operation in the graph.\\n\\nWe can see that computing the gradient requires at most O(n”) operations because the forward propagation stage will at worst execute all n nodes in the original graph (depending on which values we want to compute, we may not need to execute the entire graph). The back-propagation algorithm adds one Jacobian-vector product, which should be expressed with O(1) nodes, per edge in the original graph',\n",
       " 'b89e6041-d586-47ef-aeea-fb60d22d0645': 'Using a Lagrange multiplier A2 to enforce the constraint, we consider the minimization of Setting the derivative with respect to U2 to zero, we obtain SU2 = A2U2 so that U2 is an eigenvector of S with eigenvalue A2. Thus any eigenvector will define a stationary point of the distortion measure. To find the value of J at the minimum, we back-substitute the solution for U2 into the distortion measure to give J = A2. We therefore obtain the minimum value of J by choosing U2 to be the eigenvector corresponding to the smaller of the two eigenvalues. Thus we should choose the principal subspace to be aligned with the eigenvector having the larger eigenvalue. This result accords with our intuition that, in order to minimize the average squared projection distance, we should choose the principal component subspace to pass through the mean of the data points and to be aligned with the directions of maximum variance.\\n\\nFor the case when the eigenvalues are equal, any choice of principal direction will give rise to the same value of J',\n",
       " '5b33600f-4e73-449d-8d03-5e2ebbeeec5b': 'Since the visual quality of samples is not a reliable guide, we often also evaluate the log-likelihood that the model assigns to the test data, when this is computationally feasible. Unfortunately, in some cases the likelihood seems not to measure any attribute of the model that we really care about. For example, real-valued models of MNIST can obtain arbitrarily high likelihood by assigning arbitrarily low variance to background pixels that never change. Models and algorithms that detect these constant features can reap unlimited rewards, even though this is not a very useful thing to do. The potential to achieve a cost approaching negative infinity is present for any kind of maximum likelihood problem with real values, but it is especially problematic for generative models of MNIST because so many of the output values are trivial to predict. This strongly suggests a need for developing other ways of evaluating generative models. Theis et al. review many of the issues involved in evaluating generative models, including many of the ideas described above.\\n\\nThey highlight the fact that there are many different uses of generative models and that the choice of metric must match the intended use of the model',\n",
       " 'af0f661b-46a0-492c-a83e-339c3eb3469b': 'The cognitive map explanation of latent learning experiments is analogous to the claim that animals use model-based algorithms, and that environment models can be learned even without explicit rewards or penalties. Models are then used for planning when the animal is motivated by the appearance of rewards or penalties. Tolman’s account of how animals learn cognitive maps was that they learn stimulusstimulus, or S–S, associations by experiencing successions of stimuli as they explore an environment. In psychology this is called expectancy theory: given S–S associations, the occurrence of a stimulus generates an expectation about the stimulus to come next. This is much like what control engineers call system identiﬁcation, in which a model of a system with unknown dynamics is learned from labeled training examples. In the simplest discrete-time versions, training examples are S–S0 pairs, where S is a state and S0, the subsequent state, is the label. When S is observed, the model creates the “expectation” that S0 will be observed next. Models more useful for planning involve actions as well, so that examples look like SA–S0, where S0 is expected when action A is executed in state S',\n",
       " '0f0123bc-98ef-49cf-a055-680a49a10042': 'For a more extensive discussion of ‘kernel engineering’, see Shawe-Taylor and Cristianini . We saw that the simple polynomial kernel k(x, x′) = � xTx′�2 contains only terms of degree two. If we consider the slightly generalized kernel k(x, x′) = � xTx′ + c�2 with c > 0, then the corresponding feature mapping φ(x) contains concontains all monomials of order M. For instance, if x and x′ are two images, then the kernel represents a particular weighted sum of all possible products of M pixels in the ﬁrst image with M pixels in the second image. This can similarly be generalized to include all terms up to degree M by considering k(x, x′) = � xTx′ + c �M with c > 0.\\n\\nUsing the results (6.17) and (6.18) for combining kernels we see that these will all be valid kernel functions. Another commonly used kernel takes the form and is often called a ‘Gaussian’ kernel. Note, however, that in this context it is not interpreted as a probability density, and hence the normalization coefﬁcient is omitted',\n",
       " '4dd4aab9-2acb-4b3a-b558-0deb27d32cea': '5050–5060, 2019. Bossard, L., Guillaumin, M., and Van Gool, L. Food-101–mining discriminative components with random forests. In European conference on computer vision, pp. 446–461. Springer, 2014. Chen, T., Sun, Y., Shi, Y., and Hong, L. On sampling strategies for neural network-based collaborative ﬁltering.\\n\\nIn Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 767–776, 2017. Chen, T., Zhai, X., Ritter, M., Lucic, M., and Houlsby, N. Selfsupervised gans via auxiliary rotation loss. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 12154–12163, 2019. Cimpoi, M., Maji, S., Kokkinos, I., Mohamed, S., and Vedaldi, A. Describing textures in the wild',\n",
       " 'd0086cdf-b078-4f74-8baf-bd050694b2b1': 'Our example of the cold spreading between you, your roommate, and your  colleague contains two cliques. One clique contains h, and h,. The factor for this clique can be defined by a table and might have values resembling these:  0 hy=1  he =0 2 1 he=1 1  A state of 1 indicates good health, while a state of 0 indicates poor health (having been infected with a cold). Both of you are usually healthy, so the corresponding state has the highest affinity. The state where only one of you is sick has the lowest affinity, because this is a rare state. The state where both of you are sick (because one of you has infected the other) is a higher affinity state, though still not as common as the state where both are healthy. 3A clique of the graph is a subset of nodes that are all connected to each other by an edge of the graph. CHAPTER 16',\n",
       " 'fc33a333-b774-4f57-9bf8-f79adba3eeec': 'There is one more way in which the structure of the return as a sum of rewards can be taken into account in o↵-policy importance sampling, a way that may be able to reduce variance even in the absence of discounting (that is, even if γ = 1). In the o↵-policy estimators (5.5) and (5.6), each term of the sum in the numerator is itself a sum: The o↵-policy estimators rely on the expected values of these terms, which can be written in a simpler way. Note that each sub-term of (5.11) is a product of a random reward and a random importance-sampling ratio. For example, the ﬁrst sub-term can be written, using (5.3), as Of all these factors, one might suspect that only the ﬁrst and the last (the reward) are related; all the others are for events that occurred after the reward',\n",
       " '73402c87-8963-42e0-abfa-5204c5950a72': \"Contrastive Representation Learning | Lil'Log   Posts Archive Search Tags FAQ emojisearch.app  Contrastive Representation Learning  Table of Contents  The goal of contrastive representation learning is to learn such an embedding space in which similar sample pairs stay close to each other while dissimilar ones are far apart. Contrastive learning can be applied to both supervised and unsupervised settings. When working with unsupervised data, contrastive learning is one of the most powerful approaches in self-supervised learning. Contrastive Training Objectives  In early versions of loss functions for contrastive learning, only one positive and one negative sample are involved. The trend in recent training objectives is to include multiple positive and negative pairs in one batch. Contrastive Loss  Contrastive loss  is one of the earliest training objectives used for deep metric learning in a contrastive fashion. Given a list of input samples {x;}, each has a corresponding label y; € {1, see , L} among L classes. We would like to learn a function fg(.) : ¥ > R¢ that encodes 2; into an embedding vector such that examples from the same class have similar embeddings and samples from different classes have very different ones\",\n",
       " '14068a5e-e21b-4c7b-99bd-d8cee9fb2e37': 'Next a sample value is drawn from the envelope distribution. This is straightforward because the log of the envelope distribution is a succession Exercise 11.9 of linear functions, and hence the envelope distribution itself comprises a piecewise exponential distribution of the form Once a sample has been drawn, the usual rejection criterion can be applied. If the sample is accepted, then it will be a draw from the desired distribution. If, however, the sample is rejected, then it is incorporated into the set of grid points, a new tangent line is computed, and the envelope function is thereby reﬁned.\\n\\nAs the number of grid points increases, so the envelope function becomes a better approximation of the desired distribution p(z) and the probability of rejection decreases. A variant of the algorithm exists that avoids the evaluation of derivatives (Gilks, 1992). The adaptive rejection sampling framework can also be extended to distributions that are not log concave, simply by following each rejection sampling step with a Metropolis-Hastings step (to be discussed in Section 11.2.2), giving rise to adaptive rejection Metropolis sampling . Clearly for rejection sampling to be of practical value, we require that the comparison function be close to the required distribution so that the rate of rejection is kept to a minimum',\n",
       " '6f2d7b1f-e8f5-44b8-a25a-7eb898a76539': 'Wei, C.-H., Peng, Y., Leaman, R., P, D.A., Mattingly, C.J., Li, J., Wiegers, T., Lu, Z.: Overview of the BioCreative V chemical disease relation (CDR) task. In: BioCreative Challenge Evaluation Workshop  61. Worldwide semiannual cognitive/artiﬁcial intelligence systems spending guide. Technical report, International Data Corporation  62. Wu, S., Hsiao, L., Cheng, X., Hancock, B., Rekatsinas, T., Levis, P., Ré, C.: Fonduer: Knowledge base construction from richly formatted data. In: Proceedings of the 2018 International Conference on Management of Data, pp. 1301–1316. ACM  63. Yuen, M.-C., King, I., Leung, K.-S.: A survey of crowdsourcing systems. In: Privacy, Security, Risk and Trust (PASSAT) and International Conference on Social Computing (SocialCom)  64',\n",
       " '7d42680c-93b1-4758-8528-8a1f3c1a8771': 'In a practical application, however, we must often make a speciﬁc prediction for the value of t, or more generally take a speciﬁc action based on our understanding of the values t is likely to take, and this aspect is the subject of decision theory.\\n\\nConsider, for example, a medical diagnosis problem in which we have taken an X-ray image of a patient, and we wish to determine whether the patient has cancer or not. In this case, the input vector x is the set of pixel intensities in the image, and output variable t will represent the presence of cancer, which we denote by the class C1, or the absence of cancer, which we denote by the class C2. We might, for instance, choose t to be a binary variable such that t = 0 corresponds to class C1 and t = 1 corresponds to class C2. We shall see later that this choice of label values is particularly convenient for probabilistic models. The general inference problem then involves determining the joint distribution p(x, Ck), or equivalently p(x, t), which gives us the most complete probabilistic description of the situation',\n",
       " 'f61a1b3c-e4a8-4e9e-83da-5b75a8337aa5': 'STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    ID  Figure 16.4: This graph implies that p(a,b,c,d,e,f) can be written as Z bab (a, b) b,c (b, c)Ga,a(a, d)bb,c(b, €) de,e(e, f) for an appropriate choice of the @ func- tions.\\n\\nTo complete the model, we would need to also define a similar factor for the clique containing h, and h,. 16.2.3. The Partition Function  While the unnormalized probability distribution is guaranteed to be nonnegative everywhere, it is not guaranteed to sum or integrate to 1. To obtain a valid probability distribution, we must use the corresponding normalized probability distribution* A  p(x) =F D(x), (16.4) where Z is the value that results in the probability distribution summing or integrating to 1:  Z= [peer (16.5)  You can think of Z as a constant when the ¢ functions are held constant',\n",
       " 'c613e72a-d490-49d4-88da-8e505cc10eb4': 'A natural form of experience that has encoded the concept of ‘sentiment’ is a pretrained sentiment classiﬁer, which can be used to measure the likelihood (plausibility) that the given sentence y has the given sentiment x (Hu which trains the target model pθ by encouraging it to mimic the source model outputs (and thus the source model is also called ‘teacher’ model—in a similar sense to but not to be confused with the teacher-student mechanism for optimization described in Sections 3 and 7). We now turn to the divergence term D(q, pθ) that measures the distance between the auxiliary distribution q and the model distribution pθ in the SE. The discussion in the prior section has assumed speciﬁc case of D being the cross entropy.\\n\\nYet there is a rather rich set of choices for the divergence function, such as f-divergence (e.g., KL divergence, Jensen-Shannon divergence), optimal transport distance (e.g., Wasserstein distance), and so on',\n",
       " 'e12f2fae-4035-4b73-a8da-7633e91b0788': 'However, we do not claim that this is a new method to quantitatively evaluate generative models yet. The constant scaling factor that depends on the critic’s architecture means it’s hard to compare models with diﬀerent critics.\\n\\nEven more, in practice the fact that the critic doesn’t have inﬁnite capacity makes it hard to know just how close to the EM distance our estimate really is. This being said, we have succesfully used the loss metric to validate our experiments repeatedly and without failure, and we see this as a huge improvement in training GANs which previously had no such facility. In contrast, Figure 4 plots the evolution of the GAN estimate of the JS distance during GAN training. More precisely, during GAN training, the discriminator is trained to maximize L(D, gθ) = Ex∼Pr + Ex∼Pθ which is is a lower bound of 2JS(Pr, Pθ)−2 log 2. In the ﬁgure, we plot the quantity 1 2L(D, gθ) + log 2, which is a lower bound of the JS distance. This quantity clearly correlates poorly the sample quality. Note also that the JS estimate usually stays constant or goes up instead of going down',\n",
       " 'cc2dc851-6c35-4c07-8ac3-e1aee6aea308': ', xD) represented by a directed graph having D nodes, and consider the conditional distribution of a particular node with variables xi conditioned on all of the remaining variables xj̸=i.\\n\\nUsing the factorization property (8.5), we can express this conditional distribution in the form in which the integral is replaced by a summation in the case of discrete variables. We now observe that any factor p(xk|pak) that does not have any functional dependence on xi can be taken outside the integral over xi, and will therefore cancel between numerator and denominator. The only factors that remain will be the conditional distribution p(xi|pai) for node xi itself, together with the conditional distributions for any nodes xk such that node xi is in the conditioning set of p(xk|pak), in other words for which xi is a parent of xk',\n",
       " '96839791-3d43-4747-84bf-747076be5165': 'The state unit has a linear self-loop whose weight is controlled by the forget gate. The output of the cell can be shut off by the output gate. All the gating units have a sigmoid nonlinearity, while the input unit can have any squashing nonlinearity. The state unit can also be used as an extra input to the gating units. The black square indicates a delay of a single time step.\\n\\nforget gate output gate  https://www.deeplearningbook.org/contents/rnn.html    405  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  architecture. Deeper architectures have also been successfully used . Instead of a unit that simply applies an element-wise nonlinearity to the affine transformation of inputs and recurrent units, LSTM recurrent networks have “LSTM cells” that have an internal recurrence (a self-loop), in addition to the outer recurrence of the RNN. Each cell has the same inputs and outputs as an ordinary recurrent network, but also has more parameters and a system of gating units that controls the flow of information',\n",
       " 'c679efeb-bbad-4f8d-bfea-9e26a17dc0cc': 'All of the ideas go through in the discounted case with appropriate adjustments (including to the box on page 199) but involve additional complexity that distracts from the main ideas. ⇤Exercise 13.2 Generalize the box on page 199, the policy gradient theorem (13.5), the proof of the policy gradient theorem (page 325), and the steps leading to the REINFORCE update equation (13.8), so that (13.8) ends up with a factor of γt and thus aligns with the general algorithm given in the pseudocode. ⇤ As a stochastic gradient method, REINFORCE has good theoretical convergence properties. By construction, the expected update over an episode is in the same direction as the performance gradient. This assures an improvement in expected performance for suﬃciently small ↵, and convergence to a local optimum under standard stochastic approximation conditions for decreasing ↵.\\n\\nHowever, as a Monte Carlo method REINFORCE may be of high variance and thus produce slow learning. Exercise 13.3 In Section 13.1 we considered policy parameterizations using the soft-max in action preferences (13.2) with linear action preferences (13.3)',\n",
       " 'e78eac71-934b-43b5-bed2-d9b8e20d955b': 'For every setup, we ﬁne-tune the model with the same seed as the dataset seed (in contrast to many works which report the max across different seeds). The detailed experimental setup is described in the Appendix. 2The results for 100 labeled data points per class are shown in the Appendix. every time; in the semi-supervised settings, sentence level augmentations (round-trip translation) works the best, getting the highest or second highest score every time. This makes sense since for many classiﬁcation tasks, multiple words indicate the label, and so dropping several words will not affect the label. Inference Tasks. As shown in Table 3, we observe that token-level augmentations work the best overall (e.g., random insertion, random deletion, and word replacement) for both supervised and semi-supervised settings. This is a bit surprising since the inference tasks usually heavily depend on several words, and changing these words can easily change the label for inferene tasks. Similarity and Paraphrase Tasks.\\n\\nFrom Table 3, in the supervised settings, we observe that token-level augmentations (random swapping) achieve the best performances, while hidden space augmentations work well in semisupervised settings, with cutoff performing the best on average',\n",
       " '6b20f862-5a74-42f7-9d9a-c44213aae382': 'We sometimes annotate the output node with the name of the operation applied, and other times omit this label when the operation is clear from context. Examples of computational graphs are shown in figure 6.8. 6.5.2 Chain Rule of Calculus  The chain rule of calculus (not to be confused with the chain rule of probability) is used to compute the derivatives of functions formed by composing other functions whose derivatives are known. Back-propagation is an algorithm that computes the chain rule, with a specific order of operations that is highly efficient. Let x be a real number, and let f and g both be functions mapping from a real number to a real number. Suppose that y = g(r) and z= f(g(x)) = f(y). Then  201  https://www.deeplearningbook.org/contents/mlp.html    CHAPTER 6. DEEP FEEDFORWARD NETWORKS  (.) + (.°) dot x  Figure 6.8: Examples of computational graphs. (a) The graph using the x operation to compute z = ry',\n",
       " 'feaea9d0-23e8-4a89-a544-82859e921eed': 'In Section 3.3.3, we saw that the prediction of a linear regression model for a new input x takes the form of a linear combination of the training set target values with coefﬁcients given by the ‘equivalent kernel’ (3.62) where the equivalent kernel satisﬁes the summation constraint (3.64). We can motivate the kernel regression model (3.61) from a different perspective, starting with kernel density estimation. Suppose we have a training set {xn, tn} and we use a Parzen density estimator to model the joint distribution p(x, t), so that Section 2.5.1 where f(x, t) is the component density function, and there is one such component centred on each data point. We now ﬁnd an expression for the regression function y(x), corresponding to the conditional average of the target variable conditioned on for all values of x. Using a simple change of variable, we then obtain where n, m = 1,',\n",
       " '287bb1fb-73b1-4417-b27f-926ad2fb596b': ', xN}.\\n\\nWe can create a new data set XB by drawing N points at random from X, with replacement, so that some points in X may be replicated in XB, whereas other points in X may be absent from XB. This process can be repeated L times to generate L data sets each of size N and each obtained by sampling from the original data set X. The statistical accuracy of parameter estimates can then be evaluated by looking at the variability of predictions between the different bootstrap data sets. One advantage of the Bayesian viewpoint is that the inclusion of prior knowledge arises naturally. Suppose, for instance, that a fair-looking coin is tossed three times and lands heads each time. A classical maximum likelihood estimate of the probability of landing heads would give 1, implying that all future tosses will land Section 2.1 heads! By contrast, a Bayesian approach with any reasonable prior will lead to a much less extreme conclusion. There has been much controversy and debate associated with the relative merits of the frequentist and Bayesian paradigms, which have not been helped by the fact that there is no unique frequentist, or even Bayesian, viewpoint',\n",
       " '0af9b331-b697-4ae2-802a-eb0c11d43186': '7.13  Adversarial Training  In many cases, neural networks have begun to reach human performance when evaluated on an i.i.d. test set. It is natural therefore to wonder whether these models have obtained a true human-level understanding of these tasks. To probe the level of understanding a network has of the underlying task, we can search for examples that the model misclassifies. Szegedy ef al. found that even neural networks that perform at human level accuracy have a nearly 100 percent error rate on examples that are intentionally constructed by using an optimization procedure to search for an input 2’ near a data point a such that the model output is very different at a’',\n",
       " '75e3f5b9-38f1-4138-a432-5f183a4d760a': '(19.45)  JAG In this reformulation, ( see the input on step as consisting gf W. j hy _ re . U-Sri4i  https://www.deeplearningbook.org/contents/inference.html    rather than UV. We can thus think of unit ? as attempting to encode The residual error in U given the code of the other units. We can thus think of sparse coding as an  642  CHAPTER 19. APPROXIMATE INFERENCE  iterative autoencoder, which repeatedly encodes and decodes its input, attempting to fix mistakes in the reconstruction after each iteration. In this example, we have derived an update rule that updates a single unit at a time. It would be advantageous to be able to update more units simultaneously. Some graphical models, such as deep Boltzmann machines, are structured in such a way that we can solve for many entries of h simultaneously. Unfortunately, binary sparse coding does not admit such block updates. Instead, we can use a heuristic technique called damping to perform block updates',\n",
       " '7c037cf2-bd78-4518-8b37-3639bd0ee0ed': 'In this book we call Rt the ‘reward signal at time t,’ or sometimes just the ‘reward at time t,’ but we do not think of it as an object or event in the agent’s environment. Because Rt is a number—not an object or an event—it is more like a reward signal in neuroscience, which is a signal internal to the brain, like the activity of neurons, that inﬂuences decision making and learning.\\n\\nThis signal might be triggered when the animal perceives an attractive (or an aversive) object, but it can also be triggered by things that do not physically exist in the animal’s external environment, such as memories, ideas, or hallucinations. Because our Rt can be positive, negative, or zero, it might be better to call a negative Rt a penalty, and an Rt equal to zero a neutral signal, but for simplicity we generally avoid these terms. In reinforcement learning, the process that generates all the Rts deﬁnes the problem the agent is trying to solve. The agent’s objective is to keep the magnitude of Rt as large as possible over time',\n",
       " '8c743f6d-4253-4a78-9db5-0d9ceaf011dc': 'This is more general than the corresponding least-squares result because the variance is a function of x. We have seen that for multimodal distributions, the conditional mean can give a poor representation of the data. For instance, in controlling the simple robot arm shown in Figure 5.18, we need to pick one of the two possible joint angle settings in order to achieve the desired end-effector location, whereas the average of the two solutions is not itself a solution. In such cases, the conditional mode may be of more value. Because the conditional mode for the mixture density network does not have a simple analytical solution, this would require numerical iteration.\\n\\nA simple alternative is to take the mean of the most probable component (i.e., the one with the largest mixing coefﬁcient) at each value of x. This is shown for the toy data set in Figure 5.21(d). So far, our discussion of neural networks has focussed on the use of maximum likelihood to determine the network parameters (weights and biases). Regularized maximum likelihood can be interpreted as a MAP (maximum posterior) approach in which the regularizer can be viewed as the logarithm of a prior parameter distribution',\n",
       " '48c21f85-a954-411c-aa5e-9b4eed77c313': 'We further consider the speciﬁc setting where we submit a ‘soft’ prediction pτ(t), that is, the distribution (a vector of normalized weights) over the K experts, and receive the rewards rτ(t) ∈ R for each expert t. The goal of the learning problem is then to update the distribution pτ(t) at every step τ for minimal regret �T τ=1 rτ(t∗) − �T τ=1 Epτ (t), where t∗ is the best single expert. The rewards from the environment naturally serves as the dynamic experience: In the following, we show that the SE rediscovers the classical multiplicative weights (MW), or Hedge algorithm  to the above online problem. Similar ideas of multiplicative weights have been widely used in diverse ﬁelds such as optimization, game theory, and economics . Multiplicative weights',\n",
       " '8fb44230-c7c9-4ac6-ab4a-ccfbaa802870': 'REPRESENTATION LEARNING  Algorithm 15.1 Greedy layer-wise unsupervised pretraining protocol  Given the following: Unsupervised feature learning algorithm £, which takes a training set of examples and returns an encoder or feature function f. The raw input data is X, with one row per example, and fH) (X) is the output of the first stage encoder on X. In the case where fine-tuning is performed, we use a learner T, which takes an initial function f, input examples X (and in the supervised fine-tuning case, associated targets Y), and returns a tuned function. The number of stages is m.  f < Identity function X=xX  end for  if fine-tuning then feT X.Y)  end if  Return /  2006; Bengio et al., 2007; Ranzato et al., 2007a).\\n\\nOn many other tasks, however, unsupervised pretraining either does not confer a benefit or even causes noticeable harm. Ma et al',\n",
       " '952d0978-a591-431c-82be-f087822c2b10': 'Essentially, modes in the model  608  CHAPTER 18.\\n\\nCONFRONTING THE PARTITION FUNCTION  —  Pmodel (x) © © Pdata(x)  Figure 18.2: A spurious mode. An illustration of how the negative phase of contrastive divergence (algorithm 18.2) can fail to suppress spurious modes. A spurious mode is a mode that is present in the model distribution but absent in the data distribution. Because contrastive divergence initializes its Markov chains from data points and runs the Markov chain for only a few steps, it is unlikely to visit modes in the model that are far from the data points. This means that when sampling from the model, we will sometimes  1 ayou 1 rroad ta TO ayooa od 1 at c  https://www.deeplearningbook.org/contents/partition.html    BEL Sal ples | tLudat GO LOL resellivie Lue Gala. 1b alsV ilealis tllatb GUE LO Waslllg SOME OL its probability mass on these modes, the model will struggle to place high probability mass on the correct modes',\n",
       " '5d371925-8226-438d-b0bc-037a5efb3213': 'for a comparison of the performance of centered  DBMs with and without the use of partial mean field in the negative phase. 20.4.5 Jointly Training Deep Boltzmann Machines  Classic DBMs require greedy unsupervised pretraining and, to perform classification well, require a separate MLP-based classifier on top of the hidden features they extract. This has some undesirable properties. It is hard to track performance during training because we cannot evaluate properties of the full DBM while training the first RBM. Thus, it is hard to tell how well our hyperparameters are working until quite late in the training process. Software implementations of DBMs need to have many different components for CD training of individual RBMs, PCD training of the full DBM, and training based on back-propagation  668  CHAPTER 20. DEEP GENERATIVE MODELS  through the MLP.\\n\\nFinally, the MLP on top of the Boltzmann machine loses many of the advantages of the Boltzmann machine probabilistic model, such as being able to perform inference when some input values are missing. There are two main ways to resolve the joint training problem of the deep  Bl aBe 2',\n",
       " '86f61f04-1228-4335-b864-ff5cd0b81bd9': 'for examples of modern uses of the REINFORCE algorithm with reduced variance in the context of deep learning.\\n\\nIn addition to the use of an input-dependent baseline bW), Mnih and Gregor  found that the scale of (J(y) — b(w)) could be adjusted during training by dividing it by its standard deviation estimated by a moving average during training, as a kind of adaptive learning rate, to counter the effect of important variations that occur during the course of training in the magnitude of this quantity. Mnih and Gregor  called this heuristic variance normalization. REINFORCE-based estimators can be understood as estimating the gradient by correlating choices of y with corresponding values of J(y). If a good value of y is unlikely under the current parametrization, it might take a long time to obtain it by chance and get the required signal that this configuration should be reinforced. 20.10 Directed Generative Nets  As discussed in chapter 16, directed graphical models make up a prominent class of graphical models. While directed graphical models have been very popular within the greater machine learning community, within the smaller deep learning community they have until roughly 2013 been overshadowed by undirected models such as the RBM',\n",
       " 'fe1cd843-006b-423e-865e-be924898e1b7': 'Based on the goal of the task, we use the following intuitive rewards to ensure entailment accuracy and language quality: (1) a robust entailment classiﬁer  that measures the entailment score of a generation in terms of the input premise, (2) a GPT-2 language model  that measures the log-likelihood of the generation as an indicator of language quality, and (3) BLEU score w.r.t the input premises as another language quality reward that avoids trivial outputs. We sum together all rewards with weights 1.0. We study the task of attacking an entailment classiﬁer. In particular, we aim to attack one of the most popular entailment classiﬁers on HuggingFaceHub.13 The attack generation model generates adversarial text without conditioning on any inputs so that the generated attacks are universal to all premises. The generation model is trained with mostly the same setting as in §4.1, where the entailment classiﬁer to be attacked is used as entailment score reward functions. Besides, we additionally include a token-level repetition penalty reward, which empirically beneﬁts readability. Finally, we use the MultiNLI dataset  which includes more diverse examples than the SNLI used above',\n",
       " 'eb2206b8-26af-4b45-81df-6bec893b0e04': 'Preliminaries: The Maximum Entropy View of Learning and Inference Depending on the nature of the task (e.g., classiﬁcation or regression), data (e.g., labeled or unlabeled), information scope (e.g., with or without latent variables), and form of domain knowledge (e.g., prior distributions or parameter constraints), and so on, diﬀerent learning paradigms with often complementary (but not necessarily easy to combine) advantages have been developed for diﬀerent needs.\\n\\nFor example, the paradigms built on the maximum likelihood principles, Bayesian theories, variational calculus, and Monte Carlo simulation have led to much of the foundation underlying a wide spectrum of probabilistic graphical models, exact/approximate inference algorithms, and even probabilistic logic programs suitable for probabilistic inference and parameter estimations in multivariate, structured, and fully or partially observed domains, while the paradigms built on convex optimization, duality theory, regularization, and risk minimization have led to much of the foundation underlying algorithms such as support vector machine (SVM), boosting, sparse learning, structure learning, and so on',\n",
       " '7f9cc45d-6413-49e0-9b07-060aa95f85a9': 'As shown in section 4, we ﬁnd it beneﬁcial to deﬁne the contrastive loss on zi’s rather than hi’s. • A contrastive loss function deﬁned for a contrastive prediction task. Given a set {˜xk} including a positive pair of examples ˜xi and ˜xj, the contrastive prediction task aims to identify ˜xj in {˜xk}k̸=i for a given ˜xi. We randomly sample a minibatch of N examples and deﬁne the contrastive prediction task on pairs of augmented examples derived from the minibatch, resulting in 2N data points. We do not sample negative examples explicitly. Instead, given a positive pair, similar to , we treat the other 2(N − 1) augmented examples within a minibatch as negative examples. Let sim(u, v) = u⊤v/∥u∥∥v∥ denote the dot product between ℓ2 normalized u and v (i.e. cosine similarity)',\n",
       " '72d8e68e-8dcf-488c-b2a3-6076073ab610': 'Larger initial weights will yield a stronger symmetry-breaking effect, helping to avoid redundant units. They also help to avoid losing signal during forward or back-propagation through the linear component of each layer—larger values in the matrix result in larger outputs of matrix multiplication.\\n\\nInitial weights that are too large may, however, result in exploding values during forward propagation or back-propagation. In recurrent networks, large weights can also result in chaos (such extreme sensitivity to small perturbations of the input that the behavior  Af 4A Antnnninintin fanned nnn nn wn tinn nnn nn denn nnn nner wen dn) TRH 24  https://www.deeplearningbook.org/contents/optimization.html    VL LUO UCLELILIIIIIDSLIC LULWaLU plupa} AULLULL PpLlLUCccuuLle appeals Lauuviit). LU DSULLIC  extent, the exploding gradient problem can be mitigated by gradient clippin  (thresholding the values of the gradients before performing a gradient descent step)',\n",
       " '24ea5b72-6d3d-46ec-854e-fe6281ecc468': 'This makes possible the construction of general purpose software for variational inference in which the form of the model does not need to be speciﬁed in advance . If we now specialize to the case of a model in which all of the conditional distributions have a conjugate-exponential structure, then the variational update procedure can be cast in terms of a local message passing algorithm . In particular, the distribution associated with a particular node can be updated once that node has received messages from all of its parents and all of its children. This in turn requires that the children have already received messages from their coparents. The evaluation of the lower bound can also be simpliﬁed because many of the required quantities are already evaluated as part of the message passing scheme. This distributed message passing formulation has good scaling properties and is well suited to large networks.\\n\\nThe variational framework discussed in Sections 10.1 and 10.2 can be considered a ‘global’ method in the sense that it directly seeks an approximation to the full posterior distribution over all random variables. An alternative ‘local’ approach involves ﬁnding bounds on functions over individual variables or groups of variables within a model',\n",
       " 'eb0e8517-0097-424b-8463-5fbf21fe741c': 'In the M step, the bound is maximized giving the value θ(new), which gives a larger value of log likelihood than θ(old). The subsequent E step then constructs a bound that is tangential at θ(new) as shown by the green curve. For the particular case of an independent, identically distributed data set, X will comprise N data points {xn} while Z will comprise N corresponding latent variables {zn}, where n = 1, . , N. From the independence assumption, we have p(X, Z) = � n p(xn).\\n\\nUsing the sum and product rules, we see that the posterior probability that is evaluated in the E step takes the form and so the posterior distribution also factorizes with respect to n. In the case of the Gaussian mixture model this simply says that the responsibility that each of the mixture components takes for a particular data point xn depends only on the value of xn and on the parameters θ of the mixture components, not on the values of the other data points',\n",
       " '9988f7f3-a1c6-41c3-8955-5716ffd10bea': 'Although there is no longer any evidence of the carving, there is now a stone plaque on the bridge commemorating the discovery and displaying the quaternion equations. During the evolution of this dynamical system, the value of the Hamiltonian H is constant, as is easily seen by differentiation A second important property of Hamiltonian dynamical systems, known as Liouville’s Theorem, is that they preserve volume in phase space.\\n\\nIn other words, if we consider a region within the space of variables (z, r), then as this region evolves under the equations of Hamiltonian dynamics, its shape may change but its volume will not. This can be seen by noting that the ﬂow ﬁeld (rate of change of location in phase space) is given by and that the divergence of this ﬁeld vanishes Now consider the joint distribution over phase space whose total energy is the Hamiltonian, i.e., the distribution given by Using the two results of conservation of volume and conservation of H, it follows that the Hamiltonian dynamics will leave p(z, r) invariant. This can be seen by considering a small region of phase space over which H is approximately constant',\n",
       " '11fc9121-c3fc-4785-aea2-bdfb31ee95ef': 'Note that neither ¥V nor ¥ need to be symmetric (al- though they are square and real), so they can have complex-valued eigenvalues and eigenvectors, with imaginary components corresponding to potentially oscillatory behavior (if the same Jacobian was applied iteratively). Even though h™ or a  small variation of h of interest in back-propagation are real valued, they can be expressed in such a complex-valued basis. What matters is what happens to the magnitude (complex absolute value) of these possibly complex-valued ba- sis coefficients when we multiply the matrix by the vector.\\n\\nAn eigenvalue with magnitude greater than one corresponds to magnification (exponential growth, if applied iteratively) while a magnitude smaller than one corresponds to shrinking (exponential decay, if applied iteratively). With a nonlinear map, the Jacobian is free to change at each step. The dynamics therefore become more complicated. It remains true, however, that a small initial variation can turn into a large variation after several steps',\n",
       " '9e47c16e-c036-4ab9-85b0-1e7f3372cd55': 'Although the main technical achievement demonstrated by Watson was its ability to quickly and accurately answer natural language questions over broad areas of general knowledge, its winning Jeopardy! performance also relied on sophisticated decision-making strategies for critical parts of the game.\\n\\nTesauro, Gondek, Lechner, Fan, and Prager  adapted Tesauro’s TD-Gammon system described above to create the strategy used by Watson in “Daily-Double” (DD) wagering in its celebrated winning performance against human champions. These authors report that the e↵ectiveness of this wagering strategy went well beyond what human players are able to do in live game play, and that it, along with other advanced strategies, was an important contributor to Watson’s impressive winning performance. Here we focus only on DD wagering because it is the component of Watson that owes the most to reinforcement learning. Jeopardy! is played by three contestants who face a board showing 30 squares, each of which hides a clue and has a dollar value. The squares are arranged in six columns, each corresponding to a di↵erent category',\n",
       " '140ea5cb-29ab-4db8-b4f2-e1a8fd64d850': 'Unfortunately, this alternative formulation does not seem to improve convergence in practice, possibly because of suboptimality of the discriminator or high variance around the expected gradient.\\n\\nIn realistic experiments, the best-performing formulation of the GAN game is a different formulation that is neither zero-sum nor equivalent to maximum  https://www.deeplearningbook.org/contents/generative_models.html    likelihood, introduced by Goodfellow et al. with a heuristic motivation. In this best-performing formulation, the generator aims to increase the log-probability that the discriminator makes a mistake, rather than aiming to decrease the log- probability that the discriminator makes the correct prediction. This reformulation is motivated solely by the observation that it causes the derivative of the generator’s cost function with respect to the discriminator’s logits to remain large even in the situation when the discriminator confidently rejects all generator samples. Stabilization of GAN learning remains an open problem. Fortunately, GAN learning performs well when the model architecture and hyperparameters are care- fully selected. Radford et al',\n",
       " '3a37bd63-da6e-47ca-924e-d499cbee254f': 'No matter what kind of unsupervised learning algorithm or what model type is employed, in most cases, the overall training scheme is nearly the same. While the choice of unsupervised learning algorithm will obviously affect the details, most applications of unsupervised pretraining follow this basic protocol. Greedy layer-wise unsupervised pretraining can also be used as initialization for other unsupervised learning algorithms, such as deep autoencoders  and probabilistic models with many layers of latent variables.\\n\\nSuch models include deep belief networks  and deep Boltzmann machines . These deep generative models are described in chapter 20.  https://www.deeplearningbook.org/contents/representation.html  As discussed in section 8.7.4, it is also possible to have greedy layer-wise supervised pretraining. This builds on the premise that training a shallow network is easier than training a deep one, which seems to have been validated in several contexts . 15.1.1 When and Why Does Unsupervised Pretraining Work? On many tasks, greedy layer-wise unsupervised pretraining can yield substantial improvements in test error for classification tasks. This observation was responsible for the renewed interested in deep neural networks starting in 2006 (Hinton ef al.,  527  CHAPTER 15',\n",
       " '0998d43e-37e6-4c02-988f-2f15dc559b47': 'The survey by Kumar  provides a good discussion of Bayesian and non-Bayesian approaches to these problems. The term information state comes from the literature on partially observable MDPs; see, e.g., Lovejoy . Other theoretical research focuses on the eﬃciency of exploration, usually expressed as how quickly an algorithm can approach an optimal decision-making policy. One way to formalize exploration eﬃciency is by adapting to reinforcement learning the notion of sample complexity for a supervised learning algorithm, which is the number of training examples the algorithm needs to attain a desired degree of accuracy in learning the target function. A deﬁnition of the sample complexity of exploration for a reinforcement learning algorithm is the number of time steps in which the algorithm does not select near-optimal actions . Li  discusses this and several other approaches in a survey of theoretical approaches to exploration eﬃciency in reinforcement learning. A thorough modern treatment of Thompson sampling is provided by Russo, Van Roy, Kazerouni, Osband, and Wen .\\n\\nIn this chapter we introduce the formal problem of ﬁnite Markov decision processes, or ﬁnite MDPs, which we try to solve in the rest of the book',\n",
       " 'f36bfd7f-44e0-4ef5-b906-5ab8a0a1e8b4': 'Alternately, we could imagine that this is a truncated Taylor series approximating the cost function of a more sophisticated model. The gradient in this setting is given by  VwJ(w) = H(w —w%), (7.21) where, again, H is the Hessian matrix of J with respect to w evaluated at w*. Because the L! penalty does not admit clean algebraic expressions in the case of a fully general Hessian, we will also make the further simplifying assumption that the Hessian is diagonal, H = diag(), where each Hi; > 0.  ? As with L? regularization, we could regularize the parameters toward a value that is not zero, but instead toward some parameter value w). In that case the Lt regularization would introduce the term (0) = ||w — w ||. = DY |wi w\\\\? |. 231  CHAPTER 7.\\n\\nREGULARIZATION FOR DEEP LEARNING  This assumption holds if the data for the linear regression problem has been preprocessed to remove all correlation between the input features, which may be accomplished using PCA',\n",
       " '71b20c6e-2b51-4730-8f2d-0d6364ab93c2': 'deﬁned “Expected Sarsa” to be an on-policy method exclusively (as we did in the ﬁrst edition), whereas now we use this name for the general algorithm in which the target and behavior policies may di↵er.\\n\\nThe general o↵-policy view of Expected Sarsa was noted by van Hasselt , who called it “General Q-learning.” In this chapter we unify the Monte Carlo (MC) methods and the one-step temporaldi↵erence (TD) methods presented in the previous two chapters. Neither MC methods nor one-step TD methods are always the best. In this chapter we present n-step TD methods that generalize both methods so that one can shift from one to the other smoothly as needed to meet the demands of a particular task. n-step methods span a spectrum with MC methods at one end and one-step TD methods at the other. The best methods are often intermediate between the two extremes. Another way of looking at the beneﬁts of n-step methods is that they free you from the tyranny of the time step. With one-step TD methods the same time step determines how often the action can be changed and the time interval over which bootstrapping is done',\n",
       " 'fa4ae9d4-0114-4829-afd2-4cadd0734888': 'Sch¨olkopf, B., J. Platt, J. Shawe-Taylor, A. Smola, and R. C. Williamson . Estimating the support of a high-dimensional distribution. Neural Computation 13(7), 1433–1471. Sch¨olkopf, B., A. Smola, R. C. Williamson, and P. L. Bartlett . New support vector algorithms. Neural Computation 12(5), 1207–1245. Schwarz, G. Estimating the dimension of a model. Annals of Statistics 6, 461–464. Seeger, M. Bayesian Gaussian Process Models: PAC-Bayesian Generalization Error Bounds and Sparse Approximations. Ph. D. thesis, University of Edinburg. Seeger, M., C. K. I. Williams, and N. Lawrence',\n",
       " 'e965a20d-9504-4cc7-93f1-3d0fb1aad3eb': '(d) After several steps of training, if G and D have enough capacity, they will reach a point at which both cannot improve because pg = pdata.\\n\\nThe discriminator is unable to differentiate between the two distributions, i.e. D(x) = 1 Algorithm 1 Minibatch stochastic gradient descent training of generative adversarial nets. The number of steps to apply to the discriminator, k, is a hyperparameter. We used k = 1, the least expensive option, in our experiments. • Sample minibatch of m noise samples {z(1), . , z(m)} from noise prior pg(z). • Sample minibatch of m examples {x(1), . , x(m)} from data generating distribution pdata(x). • Update the discriminator by ascending its stochastic gradient: We ﬁrst consider the optimal discriminator D for any given generator G. Proposition 1. For G ﬁxed, the optimal discriminator D is Proof',\n",
       " 'c265821d-1f45-4cc9-952c-84e44280cf33': 'Instead, the most computationally demanding steps are those involving sums over the data set that are 0 (NDM). For large D, and M « D, this can be a significant saving compared to 0 (ND 2 ) and can offset the iterative nature of the EM algorithm. Note that this EM algorithm can be implemented in an on-line form in which each D-dimensional data point is read in and processed and then discarded before the next data point is considered. To see this, note that the quantities evaluated in the E step (an M-dimensional vector and an M x M matrix) can be computed for each data point separately, and in the M step we need to accumulate sums over data points, which we can do incrementally. This approach can be advantageous if both Nand D are large.\\n\\nBecause we now have a fully probabilistic model for PCA, we can deal with missing data, provided that it is missing at random, by marginalizing over the distribution of the unobserved variables. Again these missing values can be treated using the EM algorithm. We give an example of the use of this approach for data visualization in Figure 12.11',\n",
       " 'bee066a4-9153-4d79-877e-237404e074ba': '‘Token embeddings (A) Tenis Onn arco  Positive scores  fe]  Local representation (A)  Sentence representation (A)   uoo ue Buyjno si uewom y  :4 coucueS  seun6 e Burke si wb y  *\\\\v eouelueS  iS]  a  o g  i Token embeddings (8) —— 3  3 Pooling a Negative scores Local representation (B) I ——— q Concat  IS-BERT works as follows:  Use BERT to encode an input sentence s to a token embedding of length 1, h,.;. Then apply 1-D conv net with different kernel sizes (e.g. 1, 3, 5) to process the token embedding sequence to capture the n-gram local contextual dependencies: ¢; = ReLU(w - h;. ;.4_1; +b) . The output sequences are padded to stay the same sizes of the inputs. The final local representation of the i-th token F(x) is the concatenation of representations of different kernel sizes',\n",
       " 'efb74310-9c75-438d-80d4-fd95719668e8': 'Every past observation y may influence the conditional distribution of some y (for t >“), given the previous values. Parametrizing the graphical model directly according to this graph (as in equation 10.6) might be very inefficient, with an ever growing number of inputs and parameters for each element of the sequence. RNNs obtain the same full connectivity but efficient parametrization, as illustrated in figure 10.8. The edges in a graphical model indicate which variables depend directly on other variables.\\n\\nMany graphical models aim to achieve statistical and computational efficiency by omitting edges that do not correspond to strong interactions. For example, it is common to make the Markov assumption that the graphical model should contain only edges from {y-*),...,y¢-)} to y, rather than containing edges from the entire history. In some cases, however, we believe that all past inputs should have an influence on the next element of the sequence',\n",
       " 'f2f3d166-c2a8-436e-83f4-f8ee2d5b64e4': 'On each ﬂip, the gambler must decide what portion of his capital to stake, in integer numbers of dollars.\\n\\nThis problem can be formulated as an undiscounted, episodic, ﬁnite Exercise 4.8 Why does the optimal policy for the gambler’s problem have such a curious form? In particular, for capital of 50 it bets it all on one ﬂip, but for capital of 51 it does not. Why is this a good policy? ⇤ Exercise 4.9 (programming) Implement value iteration for the gambler’s problem and solve it for ph = 0.25 and ph = 0.55. In programming, you may ﬁnd it convenient to introduce two dummy states corresponding to termination with capital of 0 and 100, giving them values of 0 and 1 respectively. Show your results graphically, as in Figure 4.3. A major drawback to the DP methods that we have discussed so far is that they involve operations over the entire state set of the MDP, that is, they require sweeps of the state set. If the state set is very large, then even a single sweep can be prohibitively expensive. For example, the game of backgammon has over 1020 states',\n",
       " 'd508b224-10b8-4340-8daa-6cd6639cf43d': 'https://www.deeplearningbook.org/contents/optimization.html    CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  8.7.5 Designing Models to Aid Optimization  To improve optimization, the best strategy is not always to improve the optimization algorithm. Instead, many improvements in the optimization of deep models have come from designing the models to be easier to optimize. In principle, we could use activation functions that increase and decrease in jagged nonmonotonic patterns, but this would make optimization extremely difficult.\\n\\nIn practice, it is more important to choose a model family that is easy to optimize than to use a powerful optimization algorithm. Most of the advances in neural network learning over the past thirty years have been obtained by changing the model family rather than changing the optimization procedure. Stochastic gradient descent with momentum, which was used to train neural networks in the 1980s, remains in use in modern state-of-the-art neural network applications. Specifically, modern neural networks reflect a design choice to use linear trans- formations between layers and activation functions that are differentiable almost everywhere, with significant slope in large portions of their domain',\n",
       " '4a50b107-b575-45b9-9124-7f53f4feaae1': 'We know that by a simple envelope theorem (, Theorem 1) that ∇θW(Pr, Pθ) = ∇θV (f, θ) under the condition that the ﬁrst and last terms are well-deﬁned. The rest of the proof will be dedicated to show that when the right hand side is deﬁned. For the reader who is not interested in such technicalities, he or she can skip the rest of the proof. Since f ∈ F, we know that it is 1-Lipschitz. Furthermore, gθ(z) is locally Lipschitz as a function of (θ, z). Therefore, f(gθ(z)) is locally Lipschitz on (θ, z) with constants L(θ, z) (the same ones as g). By Radamacher’s Theorem, f(gθ(z)) has to be diﬀerentiable almost everywhere for (θ, z) jointly. Rewriting this, the set A = {(θ, z) : f ◦ g is not diﬀerentiable} has measure 0',\n",
       " '38113380-e5d4-4fa7-82c6-85b80e180e21': 'Finally, suppose that no cans can be collected during a run home for recharging, and that no cans can be collected on a step in which the battery is depleted.\\n\\nThis system is then a ﬁnite MDP, and we can write down the transition probabilities and the expected rewards, with dynamics as indicated in the table on the left: Note that there is a row in the table for each possible combination of current state, s, action, a 2 A(s), and next state, s0. Some transitions have zero probability of occurring, so no expected reward is speciﬁed for them. Shown on the right is another useful way of summarizing the dynamics of a ﬁnite MDP, as a transition graph. There are two kinds of nodes: state nodes and action nodes. There is a state node for each possible state (a large open circle labeled by the name of the state), and an action node for each state–action pair (a small solid circle labeled by the name of the action and connected by a line to the state node). Starting in state s and taking action a moves you along the line from state node s to action node (s, a)',\n",
       " '2865e907-823d-4a22-a604-48fdd9af41c3': 'sian conditional distribution for t given x given by (1.60), in which the mean is given by the polynomial function y(x, w), and the precision is given by the parameter β, which is related to the variance by β−1 = σ2. We now use the training data {x, t} to determine the values of the unknown parameters w and β by maximum likelihood. If the data are assumed to be drawn independently from the distribution (1.60), then the likelihood function is given by As we did in the case of the simple Gaussian distribution earlier, it is convenient to maximize the logarithm of the likelihood function. Substituting for the form of the Gaussian distribution, given by (1.46), we obtain the log likelihood function in the form Consider ﬁrst the determination of the maximum likelihood solution for the polynomial coefﬁcients, which will be denoted by wML. These are determined by maximizing (1.62) with respect to w. For this purpose, we can omit the last two terms on the right-hand side of (1.62) because they do not depend on w',\n",
       " 'e77673b5-94b8-4551-b0a2-d435cec78a06': 'In this section we present the semi-gradient version of TD(λ) with function approximation. With function approximation, the eligibility trace is a vector zt 2 Rd with the same number of components as the weight vector wt. Whereas the weight vector is a long-term memory, accumulating over the lifetime of the system, the eligibility trace is a short-term memory, typically lasting less time than the length of an episode. Eligibility traces assist in the learning process; their only consequence is that they a↵ect the weight vector, and then the weight vector determines the estimated value. In TD(λ), the eligibility trace vector is initialized to zero at the beginning of the where γ is the discount rate and λ is the parameter introduced in the previous section, which we henceforth call the trace-decay parameter.\\n\\nThe eligibility trace keeps track of which components of the weight vector have contributed, positively or negatively, to recent state valuations, where “recent” is deﬁned in terms of γλ',\n",
       " '77754ece-4808-40d5-b167-4ba9c817754f': 'Following standard practice, we formulate this as a tagging task but do not use a CRF layer in the output. We use the representation of the ﬁrst sub-token as the input to the token-level classiﬁer over the NER label set. To ablate the ﬁne-tuning approach, we apply the feature-based approach by extracting the activations from one or more layers without ﬁne-tuning any parameters of BERT. These contextual embeddings are used as input to a randomly initialized two-layer 768-dimensional BiLSTM before the classiﬁcation layer. Results are presented in Table 7. BERTLARGE performs competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind ﬁne-tuning the entire model.\\n\\nThis demonstrates that BERT is effective for both ﬁnetuning and feature-based approaches. Recent empirical improvements due to transfer learning with language models have demonstrated that rich, unsupervised pre-training is an integral part of many language understanding systems. In particular, these results enable even low-resource tasks to beneﬁt from deep unidirectional architectures',\n",
       " '0cd5016d-6fe7-4c72-8db3-7d65ca618fd6': 'Hence show by direct evaluation that p(a, b, c) = p(a)p(c|a)p(b|c). Draw the corresponding directed graph. 8.6 (⋆) For the model shown in Figure 8.13, we have seen that the number of parameters required to specify the conditional distribution p(y|x1, . , xM), where xi ∈ {0, 1}, could be reduced from 2M to M +1 by making use of the logistic sigmoid representation (8.10). An alternative representation  is given by p(y = 1|x1, . , xM) = 1 − (1 − µ0) where the parameters µi represent the probabilities p(xi = 1), and µ0 is an additional parameters satisfying 0 ⩽ µ0 ⩽ 1. The conditional distribution (8.104) is known as the noisy-OR.\\n\\nShow that this can be interpreted as a ‘soft’ (probabilistic) form of the logical OR function (i.e., the function that gives y = 1 whenever at least one of the xi = 1). Discuss the interpretation of µ0',\n",
       " '5c8ae5e1-2e8f-4c0a-b2ba-cc1a2e3f3ffd': 'Despite this, the Law of E↵ect—in one form or another—is widely regarded as a basic principle underlying much behavior . It is the basis of the inﬂuential learning theories of Clark Hull  and the inﬂuential experimental methods of B. F. Skinner . The term “reinforcement” in the context of animal learning came into use well after Thorndike’s expression of the Law of E↵ect, ﬁrst appearing in this context (to the best of our knowledge) in the 1927 English translation of Pavlov’s monograph on conditioned reﬂexes. Pavlov described reinforcement as the strengthening of a pattern of behavior due to an animal receiving a stimulus—a reinforcer—in an appropriate temporal relationship with another stimulus or with a response. Some psychologists extended the idea of reinforcement to include weakening as well as strengthening of behavior, and extended the idea of a reinforcer to include possibly the omission or termination of stimulus.\\n\\nTo be considered reinforcer, the strengthening or weakening must persist after the reinforcer is withdrawn; a stimulus that merely attracts an animal’s attention or that energizes its behavior without producing lasting changes would not be considered a reinforcer',\n",
       " 'a3848164-2ac5-4a7b-b8f3-9ba7e792d725': 'The TD model is a real-time model, as opposed to a trial-level model like the Rescorla– Wagner model. A single step t in the Rescorla–Wagner model represents an entire conditioning trial. The model does not apply to details about what happens during the time a trial is taking place, or what might happen between trials. Within each trial an animal might experience various stimuli whose onsets occur at particular times and that have particular durations. These timing relationships strongly inﬂuence learning. The Rescorla–Wagner model also does not include a mechanism for higher-order conditioning, whereas for the TD model, higher-order conditioning is a natural consequence of the bootstrapping idea that is at the base of TD algorithms',\n",
       " '96fb541e-4d70-4382-8774-b51f5acb4c43': 'Neural Style Transfer  Neural Style Transfer  is one of the flashiest demonstrations of Deep Learning capabilities. The general idea is to manipulate the representations of images created in CNNs.\\n\\nNeural Style Transfer is probably best known for its artistic applications, but it also serves as a great tool for Data Augmentation. The algorithm works by manipulat- ing the sequential representations across a CNN such that the style of one image can be transferred to another while preserving its original content. A more detailed explanation of the gram matrix operation powering Neural Style Transfer can be found by Li et al. (Fig. 25). It is important to also recognize an advancement of the original algorithm from Gatys et al. known as Fast Style Transfer . This algorithm extends the loss function from a per-pixel loss to a perceptual loss and uses a feed-forward network to stylize images. This perceptual loss is reasoned about through the use of another pre-trained net. The use of perceptual loss over per-pixel loss has also shown great promise in the applica- tion of super-resolution  as well as style transfer',\n",
       " '709ea3f2-fffe-4604-80cc-8e42da4f9855': 'Let us start with a discussion of histogram methods for density estimation, which we have already encountered in the context of marginal and conditional distributions in Figure 1.11 and in the context of the central limit theorem in Figure 2.6. Here we explore the properties of histogram density models in more detail, focussing on the case of a single continuous variable x. Standard histograms simply partition x into distinct bins of width ∆i and then count the number ni of observations of x falling in bin i.\\n\\nIn order to turn this count into a normalized probability density, we simply divide by the total number N of observations and by the width ∆i of the bins to obtain probability values for each bin given by for which it is easily seen that � p(x) dx = 1. This gives a model for the density p(x) that is constant over the width of each bin, and often the bins are chosen to have the same width ∆i = ∆. In Figure 2.24, we show an example of histogram density estimation. Here the data is drawn from the distribution, corresponding to the green curve, which is formed from a mixture of two Gaussians',\n",
       " 'e34bd17a-1bae-4851-ac95-06dca01a0bc3': 'Chapter 20  Deep Generative Models  In this chapter, we present several of the specific kinds of generative models that can be built and trained using the techniques presented in chapters16—19. All these models represent probability distributions over multiple variables in some way. Some allow the probability distribution function to be evaluated explicitly. Others do not allow the evaluation of the probability distribution function but support operations that implicitly require knowledge of it, such as drawing samples from the distribution. Some of these models are structured probabilistic models described in terms of graphs and factors, using the language of graphical models presented in chapter 16. Others cannot be easily described in terms of factors but represent probability distributions nonetheless. 20.1 Boltzmann Machines  Boltzmann machines were originally introduced as a general “connectionist” ap- proach to learning arbitrary probability distributions over binary vectors . Variants of the Boltzmann machine that include other kinds of variables have long ago surpassed the popularity of the original. In this section we briefly introduce the binary Boltzmann machine and discuss the issues that come up when trying to train and perform inference in the model',\n",
       " 'c8f0dda8-6f4f-41fb-b169-3467760ab042': 'For example, if we want a robot to be able to walk, then walking is the task. We could program the robot to learn to walk, or we could attempt to directly write a program that specifies how to walk manually.\\n\\nMachine learning tasks are usually described in terms of how the machine learning system should process an example. An example is a collection of features  https://www.deeplearningbook.org/contents/ml.html    that have been quantitatively measured from some object or event that we want the machine learning system to process. We typically represent an example as a vector & € IR” where each entry “i of the vector is another feature. For example, the features of an image are usually the values of the pixels in the image. 97  CHAPTER 5. MACHINE LEARNING BASICS  Many kinds of tasks can be solved with machine learning. Some of the most common machine learning tasks include the following:  e Classification: In this type of task, the computer program is asked to specify which of k categories some input belongs to. To solve this task, the learning algorithm is usually asked to produce a function f : R” — {1,...,4}',\n",
       " '5a233481-2a72-44f9-9968-bf11281ffe84': 'In gradient-descent methods, the weight vector is a column vector with a ﬁxed number of real valued components, w .= (w1, w2, . , wd)>,1 and the approximate value function ˆv(s,w) is a di↵erentiable function of w for all s 2 S. We will be updating w at each of a series of discrete time steps, t = 0, 1, 2, 3, . ., so we will need a notation wt for the 1The > denotes transpose, needed here to turn the horizontal row vector in the text into a vertical column vector; in this book vectors are generally taken to be column vectors unless explicitly written out horizontally or transposed. weight vector at each step. For now, let us assume that, on each step, we observe a new example St 7! v⇡(St) consisting of a (possibly randomly selected) state St and its true value under the policy.\\n\\nThese states might be successive states from an interaction with the environment, but for now we do not assume so',\n",
       " '63c8828a-038c-4562-857b-96d71df7c86a': 'Our major contribution is further generalizing these ﬁndings to deep bidirectional architectures, allowing the same pre-trained model to successfully tackle a broad set of NLP tasks. Alan Akbik, Duncan Blythe, and Roland Vollgraf. 2018. Contextual string embeddings for sequence labeling. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1638–1649. Rie Kubota Ando and Tong Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6(Nov):1817–1853. John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proceedings of the 2006 conference on empirical methods in natural language processing, pages 120–128. Association for Computational Linguistics. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In EMNLP',\n",
       " 'b5edfd8d-ce49-4870-8859-a8227ba0a8b0': ', yN)T is given by an isotropic Gaussian of the form where IN denotes the N ×N unit matrix.\\n\\nFrom the deﬁnition of a Gaussian process, the marginal distribution p(y) is given by a Gaussian whose mean is zero and whose covariance is deﬁned by a Gram matrix K so that The kernel function that determines K is typically chosen to express the property that, for points xn and xm that are similar, the corresponding values y(xn) and y(xm) will be more strongly correlated than for dissimilar points. Here the notion of similarity will depend on the application. In order to ﬁnd the marginal distribution p(t), conditioned on the input values x1, . , xN, we need to integrate over y. This can be done by making use of the results from Section 2.3.3 for the linear-Gaussian model. Using (2.115), we see that the marginal distribution of t is given by where the covariance matrix C has elements This result reﬂects the fact that the two Gaussian sources of randomness, namely that associated with y(x) and that associated with ϵ, are independent and so their covariances simply add',\n",
       " '02e4e28c-986a-4e77-867e-d7481846cd40': 'Variational auto-encoder outputs can be further improved by inputting them into GANs . Additionally, a similar vector manipulation process can be done on the noise vector inputs to GANs through the use of Bidirectional GANs . The impressive performance of GANs has resulted in increased attention on how they can be applied to the task of Data Augmentation. These networks have the ability to gen- erate new training data that results in better performing classification models. The GAN architecture first proposed by Ian Goodfellow  is a framework for generative mode- ling through adversarial training. The best anecdote for understanding GANs is the anal- ogy of a cop and a counterfeiter. The counterfeiter (generator network) takes in some form of input. This could be a random vector, another image, text, and many more.\\n\\nThe counterfeiter learns to produce money such that the cop (discriminator network) cannot tell if the money is real or fake. The real or fake dichotomy is analogous to whether or not the generated instance is from the training set or if it was created by the generator network (Fig. 16)',\n",
       " '0465c4cf-68aa-4730-8045-eb8a0dab4470': 'Consider a model with hidden variables Z, visible (observed) variables X, and parameters θ.\\n\\nThe function that is optimized with respect to θ in the M step is the expected complete-data log likelihood, given by We can use sampling methods to approximate this integral by a ﬁnite sum over samples {Z(l)}, which are drawn from the current estimate for the posterior distribution p(Z|X, θold), so that The Q function is then optimized in the usual way in the M step. This procedure is called the Monte Carlo EM algorithm. It is straightforward to extend this to the problem of ﬁnding the mode of the posterior distribution over θ (the MAP estimate) when a prior distribution p(θ) has been deﬁned, simply by adding ln p(θ) to the function Q(θ, θold) before performing the M step. A particular instance of the Monte Carlo EM algorithm, called stochastic EM, arises if we consider a ﬁnite mixture model, and draw just one sample at each E step. Here the latent variable Z characterizes which of the K components of the mixture is responsible for generating each data point',\n",
       " 'c52f3fba-94e0-4e4d-b9e0-72dc67eb95b4': 'Chapter 9  Convolutional Networks  Convolutional networks , also known as convolutional neural networks, or CNNs, are a specialized kind of neural network for processing data that has a known grid-like topology. Examples include time-series data, which can be thought of as a 1-D grid taking samples at regular time intervals, and image data, which can be thought of as a 2-D grid of pixels. Convolutional networks have been tremendously successful in practical applications. The name “convolutional neural network” indicates that the network employs a mathematical operation called convolution. Convolution is a specialized kind of linear operation. Convolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers. In this chapter, we first describe what convolution is. Next, we explain the motivation behind using convolution in a neural network. We then describe an operation called pooling, which almost all convolutional networks employ. Usually, he operation used in a convolutional neural network does not correspond precisely o the definition of convolution as used in other fields, such as engineering or pure mathematics',\n",
       " 'c8616a63-0e5e-4208-b81d-c59e6e779cb7': 'Define KFoldXV(D, A, L, k): Require: D, the given dataset, with elements 2 Require: A, the learning algorithm, seen as a function that takes a dataset as input and outputs a learned function Require: L, the loss function, seen as a function from a learned function f and an example z € D to ascalar € R Require: k, the number of folds Split D into k mutually exclusive subsets D;, whose union is D for i from 1 to k do fi = AD\\\\D;) for 2) in D; do ej = L(fi,2) end for end for Return e  (i.id.) data points. A point estimator or statistic is any function of the data:  On = g(a ,..., 0). (5.19)  The definition does not require that g return a value that is close to the true 0 or even that the range of g be the same as the set of allowable values of @. This definition of a point estimator is very general and would enable the designer of an estimator great flexibility',\n",
       " '308a346d-58e5-465f-a245-ed7035a10686': 'Finally, for the requirement of generating ﬂuent text, we can again naturally use an auxiliary model as the experience, namely, a pretrained language model (LM) fLM(x, a, y) = LM(y) that estimates the log likelihood of a sentence y under the natural language distribution. After identifying the experience (fsc, fdata, fLM), we then combine them together with Equation 9.1 and plug into the SE to train the target model pθ(y|x, a). More experimental details can be found in Hu et al. and Yang et al. Table 2 shows the empirical results on the common Yelp corpus of customer reviews. We can see that by plugging in the relevant experience, the resulting model successfully learns the respective aspects of the task. For example, with only fsc, the model is able to transfer the sentiment attribute but fails on content preservation and ﬂuency. Adding the second experience fdata encourages preservation. Further with fLM, the model substantially improves the ﬂuency, achieving the best overall performance.\\n\\nThe case study demonstrates the necessity of integrating the diverse experience for solving the problem. 9.2',\n",
       " '43a0428e-7387-46c1-9bf1-856d82edca97': 'One was that the episodes have exploring starts, and the other was that policy evaluation could be done with an inﬁnite number of episodes. To obtain a practical algorithm we will have to remove both assumptions. We postpone consideration of the ﬁrst assumption until later in this chapter. For now we focus on the assumption that policy evaluation operates on an inﬁnite number of episodes. This assumption is relatively easy to remove. In fact, the same issue arises even in classical DP methods such as iterative policy evaluation, which also converge only asymptotically to the true value function. In both DP and Monte Carlo cases there are two ways to solve the problem. One is to hold ﬁrm to the idea of approximating q⇡k in each policy evaluation.\\n\\nMeasurements and assumptions are made to obtain bounds on the magnitude and probability of error in the estimates, and then suﬃcient steps are taken during each policy evaluation to assure that these bounds are suﬃciently small. This approach can probably be made completely satisfactory in the sense of guaranteeing correct convergence up to some level of approximation. However, it is also likely to require far too many episodes to be useful in practice on any but the smallest problems',\n",
       " '42baf561-0fba-42ca-9af1-bc026abd8cdb': 'The adversarial Shorten and Khoshgoftaar J Big Data  6:60   + .007 x ° sten(VoN(%.2)) — esign(VoJ(6,2,9)) “panda” “nematode” “gibbon” 57.7% confidence 8.2% confidence 99.3 % confidence Fig. 15 Adversarial misclassification example   attacks demonstrate that representations of images are much less robust than what might have been expected. This is well demonstrated by Moosavi-Dezfooli et al. using DeepFool, a network that finds the minimum possible noise injection needed to cause a misclassification with high confidence. Su et al. show that 70.97% of images can be misclassified by changing just one pixel. Zajac et al. cause mis- classifications with adversarial attacks limited to the border of images. The success of adversarial attacks is especially exaggerated as the resolution of images increases.\\n\\nAdversarial attacking can be targeted or untargeted, referring to the deliberation in which the adversarial network is trying to cause misclassifications',\n",
       " '7a25549c-a8d8-4b0d-8d73-5f9cbfe56369': 'There is no unique decomposition because p(a) f(a) can always be rewritten as  oN ef N  https://www.deeplearningbook.org/contents/monte_carlo.html    p(a) f(a) = q(x)? Vila) , (17.8) where we now sample from q and average Pf Tn many cases, we wish to compute an expectation for a given pand an f, and the fact that the problem is specified from the start as an expectation suggests that this p and f would be a natural  1The unbiased estimator of the variance is often preferred, in which the sum of squared differences is divided by n — 1 instead of n.  589  CHAPTER 17. MONTE CARLO METHODS  choice of decomposition. However, the original specification of the problem may not be the the optimal choice in terms of the number of samples required to obtain a given level of accuracy. Fortunately, the form of the optimal choice qg* can be derived easily. The optimal qg* corresponds to what is called optimal importance sampling.\\n\\nBecause of the identity shown in equation 17.8, any Monte Carlo estimator',\n",
       " '2613ff2c-cc37-44be-bcb3-5bad3064c25b': 'In order to motivate the concept of linear dynamical systems, let us consider the following simple problem, which often arises in practical settings. Suppose we wish to measure the value of an unknown quantity z using a noisy sensor that returns a observation x representing the value of z plus zero-mean Gaussian noise. Given a single measurement, our best guess for z is to assume that z = x. However, we can improve our estimate for z by taking lots of measurements and averaging them, because the random noise terms will tend to cancel each other. Now let’s make the situation more complicated by assuming that we wish to measure a quantity z that is changing over time. We can take regular measurements of x so that at some point in time we have obtained x1, . , xN and we wish to ﬁnd the corresponding values z1, . , xN.\\n\\nIf we simply average the measurements, the error due to random noise will be reduced, but unfortunately we will just obtain a single averaged estimate, in which we have averaged over the changing value of z, thereby introducing a new source of error. Intuitively, we could imagine doing a bit better as follows',\n",
       " '3b5d86ec-1203-4504-abcf-0ac806280ace': 'A is followed by a reward of 0 and transition to a state with a value of nearly 0, which suggests vw(A) should be 0; why is its optimal value substantially negative rather than 0? The answer is that making vw(A) negative reduces the error upon arriving in A from B. The reward on this deterministic transition is 1, which implies that B should have a value 1 more than A. Because B’s value is approximately zero, A’s value is driven toward −1. The BE-minimizing value of ⇡ − 1 for A is a compromise between reducing the errors on leaving and on entering A. policy together completely determine the probability distribution over data t Assume for the moment that the state, action, and reward sets are all ﬁni for any ﬁnite sequence ξ = φ0, a0, r1, . , rk, φk, there is a well deﬁned probab sibly zero) of it occuring as the initial portion of a trajectory, which we ma P(ξ) = Pr{φ(S0) = φ0, A0 = a0, R1 = r1,',\n",
       " '770a9c6f-66c3-4369-a502-ca26fea8c545': 'There is no need to have labels for the hidden unit classifiers: gradient descent on an objective function of interest naturally learns semantically interesting features, as long as the task requires such features. We can learn about the distinction between male and female, or about the presence or absence of glasses, without having to characterize all the configurations of the n — 1 other features by examples covering all these combinations of values. This form of statistical separability is what allows one to generalize to new configurations of a person’s features that have never been seen during training. 15.5 Exponential Gains from Depth  We have seen in section 6.4.1 that multilayer perceptrons are universal approxima- tors, and that some functions can be represented by exponentially smaller deep  1 ta tou 1 1 mis 4 . rie ,oOo4  https://www.deeplearningbook.org/contents/representation.html    HELWOLKS COMpared LO SHALLOW LeELWOrKS. LUIS Gecrease 1 WOdel size Leads LO improved statistical efficiency.\\n\\nIn this section, we describe how similar results apply more generally to other kinds of models with distributed hidden representations',\n",
       " 'c4f0a2cb-5895-40d6-a4d0-0b6df27d06c9': 'Then, an optimal discriminator D∗ exists for Pr and Pθ, and Proof. First, we prove that there exists an optimal discriminator. Let D : X → . We are then interested to see if there’s an optimal discriminator for the problem min0≤D(x)≤m LD(D, gθ). Note now that if 0 ≤ D(x) ≤ m we have LD(D, gθ) = Ex∼Pr + Ez∼p(z)+] and there is an f ∗ : X →  such that Ex∼Pr−Ex∼Pθ = −δ(Pr, Pθ). This is a long known fact, found for example in , but we prove it later for completeness. In that case, we deﬁne D∗(x) = m This shows that D∗ is optimal and LD(D∗, gθ) = m − m concluding the proof. For completeness, we now show a proof for equation (7) and the existence of said f ∗ that attains the value of the inﬁmum',\n",
       " 'b5953349-1e6c-4601-a69b-69f58b9686f4': 'From a representation learning point  469  CHAPTER 12.\\n\\nAPPLICATIONS  of view, it can be useful to learn a representation in which sentences that have the same meaning have similar representations regardless of whether they were written in the source language or in the target language. This strategy was explored first using a combination of convolutions and RNNs . Later work introduced the use of an RNN for scoring proposed translations  and for generating translated sentences . Jean et al. scaled these models to larger vocabularies. 12.4.5.1 Using an Attention Mechanism and Aligning Pieces of Data  Using a fixed-size representation to capture all the semantic details of a very long sentence of, say, 60 words is very difficult. It can be achieved by training a sufficiently large RNN well enough and for long enough, as demonstrated by Cho et al. and Sutskever et al',\n",
       " '788e6100-788a-4165-a7b6-c81c2303c302': 'But as we have seen in this chapter, the extension of these ideas to signiﬁcant function approximation, even linear function approximation, involves new challenges and forces us to deepen our understanding of reinforcement learning algorithms. Why go to such lengths?\\n\\nOne reason to seek o↵-policy algorithms is to give ﬂexibility in dealing with the tradeo↵ between exploration and exploitation. Another is to free behavior from learning, and avoid the tyranny of the target policy. TD learning appears to hold out the possibility of learning about multiple things in parallel, of using one stream of experience to solve many tasks simultaneously. We can certainly do this in special cases, just not in every case that we would like to or as eﬃciently as we would like to. In this chapter we divided the challenge of o↵-policy learning into two parts. The ﬁrst part, correcting the targets of learning for the behavior policy, is straightforwardly dealt with using the techniques devised earlier for the tabular case, albeit at the cost of increasing the variance of the updates and thereby slowing learning. High variance will probably always remains a challenge for o↵-policy learning',\n",
       " '0178a32e-5d36-4961-bc90-24f87a39965f': 'We have discussed the SE for learning with all diverse forms of experience, in both static environments (e.g., ﬁxed data or reward distributions) and dynamic environments (e.g., optimized or online experience).\\n\\nAn exciting next step is to deploy the SE framework to build an AI agent that continually learns in the real-world complex and fast-evolving context, in which the AI agent must learn to identify the relevant experience out of massive external information, to acquire increasingly complex new concepts or skills. Establishing and applying the standardized formalism to the broader learning settings is expected to unleash even more power by enabling principled design of learning systems that continuously improve by interacting with and collecting diverse signals from the outer world. Theoretical analysis of panoramic learning. The paradigm of panoramic learning poses new questions about theoretical understanding. A question of particular importance in practice is about how we can guarantee better performance after integrating more experience. The analysis is challenging because the diﬀerent types of experience can each encode diﬀerent information, sometimes noisy and even conﬂicting with each other (e.g., not all data instances would comply with a logic rule), and thus plugging in an additional source of experience does not necessarily lead to positive eﬀects',\n",
       " '576fdd94-5fef-46fe-a4f6-553b8722d49d': 'This approach allowed the example rejection mechanism to function much more effectively. At this point, coverage was still below 90 percent, yet there were no obvious heoretical problems with the approach. Our methodology therefore suggested instrumenting the training and test set performance to determine whether the problem was underfitting or overfitting. In this case, training and test set error were nearly identical. Indeed, the main reason this project proceeded so smoothly was the availability of a dataset with tens of millions of labeled examples. Because raining and test set error were so similar, this suggested that the problem was due o either underfitting or a problem with the training data. One of the debugging strategies we recommend is to visualize the model’s worst errors. In this case, that  meant visualizing the incorrect training set transcriptions that the model gave the highest confidence.\\n\\nThese proved to mostly consist of examples where the input image had been cropped too tightly, with some of the digits of the address being removed by the cropping operation. For example, a photo of an address “1849” might be cropped too tightly, with only the “849” remaining visible',\n",
       " '940ecc9b-bce5-4a58-90cc-610b61e8b919': 'We shall see later how to give a probabilistic interpretation to a neural network.\\n\\nAs discussed in Section 3.1, the bias parameters in (5.2) can be absorbed into the set of weight parameters by deﬁning an additional input variable x0 whose value is clamped at x0 = 1, so that (5.2) takes the form We can similarly absorb the second-layer biases into the second-layer weights, so that the overall network function becomes As can be seen from Figure 5.1, the neural network model comprises two stages of processing, each of which resembles the perceptron model of Section 4.1.7, and for this reason the neural network is also known as the multilayer perceptron, or MLP. A key difference compared to the perceptron, however, is that the neural network uses continuous sigmoidal nonlinearities in the hidden units, whereas the perceptron uses step-function nonlinearities. This means that the neural network function is differentiable with respect to the network parameters, and this property will play a central role in network training. If the activation functions of all the hidden units in a network are taken to be linear, then for any such network we can always ﬁnd an equivalent network without hidden units',\n",
       " 'd1ea75e8-29a4-4866-a701-2df14d417e66': 'This leads to the simpliﬁed factor graph representation in Figure 13.15, in which the factors are given by To derive the alpha-beta algorithm, we denote the ﬁnal hidden variable zN as the root node, and ﬁrst pass messages from the leaf node h to the root. From the general results (8.66) and (8.69) for message propagation, we see that the messages which are propagated in the hidden Markov model take the form These equations represent the propagation of messages forward along the chain and are equivalent to the alpha recursions derived in the previous section, as we shall now show. Note that because the variable nodes zn have only two neighbours, they perform no computation. We can eliminate µzn−1→fn(zn−1) from (13.48) using (13.47) to give a recursion for the f → z messages of the form If we now recall the deﬁnition (13.46), and if we deﬁne then we obtain the alpha recursion given by (13.36). We also need to verify that the quantities α(zn) are themselves equivalent to those deﬁned previously',\n",
       " '56810b6a-f54e-408b-baa2-50641eb8a394': ', K. The value of µk gives the probability of the random variable taking state k, and so these parameters are subject to the constraints 0 ⩽ µk ⩽ 1 and � k µk = 1. The conjugate prior distribution for the parameters {µk} is the Dirichlet. The normal distribution is simply another name for the Gaussian. In this book, we use the term Gaussian throughout, although we retain the conventional use of the symbol N to denote this distribution. For consistency, we shall refer to the normalgamma distribution as the Gaussian-gamma distribution, and similarly the normalWishart is called the Gaussian-Wishart. This distribution was published by William Gosset in 1908, but his employer, Guiness Breweries, required him to publish under a pseudonym, so he chose ‘Student’. In the univariate form, Student’s t-distribution is obtained by placing a conjugate gamma prior over the precision of a univariate Gaussian distribution and then integrating out the precision variable. It can therefore be viewed as an inﬁnite mixture of Gaussians having the same mean but different variances',\n",
       " 'd39eb1d7-3bf7-4b0f-8c18-c666d40fb1e3': 'As we shall see later, it is closely related to factor analysis . Probabilistic PCA is a simple example of the linear-Gaussian framework, in which all of the marginal and conditional distributions are Gaussian. We can formulate probabilistic PCA by first introducing an explicit latent variable z corresponding to the principal-component subspace. Next we define a Gaussian prior distribution p(z) over the latent variable, together with a Gaussian conditional distribution p(xlz) for the observed variable x conditioned on the value of the latent variable. Specifically, the prior distribution over z is given by a zero-mean unit-covariance Gaussian Similarly, the conditional distribution of the observed variable x, conditioned on the value of the latent variable z, is again Gaussian, of the form in which the mean of x is a general linear function of z governed by the D x M matrix Wand the D-dimensional vector J-L. Note that this factorizes with respect to the elements of x, in other words this is an example of the naive Bayes model',\n",
       " 'b0d91be7-2107-4283-9a8c-5c4150bd5875': 'The classical state-space planning methods in artiﬁcial intelligence are decision-time planning methods collectively known as heuristic search. In heuristic search, for each state encountered, a large tree of possible continuations is considered. The approximate value function is applied to the leaf nodes and then backed up toward the current state at the root. The backing up within the search tree is just the same as in the expected updates with maxes (those for v⇤ and q⇤) discussed throughout this book. The backing up stops at the state–action nodes for the current state. Once the backed-up values of these nodes are computed, the best of them is chosen as the current action, and then all In conventional heuristic search no e↵ort is made to save the backed-up values by changing the approximate value function. In fact, the value function is generally designed by people and never changed as a result of search.\\n\\nHowever, it is natural to consider allowing the value function to be improved over time, using either the backed-up values computed during heuristic search or any of the other methods presented throughout this book. In a sense we have taken this approach all along',\n",
       " '81af250a-ad48-4ec9-9c31-648543968ac8': 'As with rejection sampling, the approximation improves as the sampling distribution q(z) gets closer to the desired distribution p(z). When q(z) = p(z), the initial samples (z(1), . , z(L)) have the desired distribution, and the weights wn = 1/L so that the resampled values also have the desired distribution. If moments with respect to the distribution p(z) are required, then they can be evaluated directly using the original samples together with the weights, because In addition to providing a mechanism for direct implementation of the Bayesian framework, Monte Carlo methods can also play a role in the frequentist paradigm, for example to ﬁnd maximum likelihood solutions. In particular, sampling methods can be used to approximate the E step of the EM algorithm for models in which the E step cannot be performed analytically',\n",
       " '1a051acb-0c4a-4656-9e84-e1ba0509969c': '(5.62) 0 If the examples are assumed to be i.i.d., then this can be decomposed into  Our = arg max y log P(y® | x: 6). (5.63) 0 ; i=1  Example: Linear Regression as Maximum Likelihood Linear regression, introduced in section 5.1.4, may be justified as a maximum likelihood procedure. Previously, we motivated linear regression as an algorithm that learns to take an input x and produce an output value y. The mapping from 2 to y is chosen to minimize mean squared error, a criterion that we introduced more or less arbitrarily. We now revisit linear regression from the point of view of maximum likelihood estimation.\\n\\nInstead of producing a single prediction y, we now think of the model as producing a conditional distribution p(y | 7). We can imagine that with an infinitely large training set, we might see several training examples with the same input value a but different values of y. The goal of the learning algorithm is now o fit the distribution p(y | a) to all those different y values that are all compatible with x',\n",
       " '923bca14-986c-41ae-b6ca-8ae54936348f': 'AdaGrad shrinks the learning rate according to the entire history of the squared gradient and may have made the learning rate too small before arriving at such a convex structure. RMSProp uses an exponentially decaying average to discard history from the extreme past so that it can converge rapidly after finding a convex bowl, as if it were an instance of the AdaGrad algorithm initialized within that bowl. Algorithm 8.4 The AdaGrad algorithm Require: Global learning rate ¢€  D AAwwtnn. Tewttinl nannecntn. A  https://www.deeplearningbook.org/contents/optimization.html  AveoyulL we. Allitial PaLalluevel U —7 . Require: Small constant 6, perhaps 10 —, for numerical stability Initialize gradient accumulation variable r = 0 while stopping criterion not met do  Sample a minibatch of m examples from the training set fa, nr 7 (my with corresponding targets y @, Compute gradient: g — +Vo >>, L( f(a; 6), y)',\n",
       " '9d2289b4-4a5a-46aa-acc0-ffc043f3cc35': 'We begin by looking in some detail at the support vector machine (SVM), which became popular in some years ago for solving problems in classiﬁcation, regression, and novelty detection. An important property of support vector machines is that the determination of the model parameters corresponds to a convex optimization problem, and so any local solution is also a global optimum. Because the discussion of support vector machines makes extensive use of Lagrange multipliers, the reader is encouraged to review the key concepts covered in Appendix E. Additional information on support vector machines can be found in Vapnik , Burges , Cristianini and Shawe-Taylor , M¨uller et al. , Sch¨olkopf and Smola , and Herbrich . The SVM is a decision machine and so does not provide posterior probabilities. We have already discussed some of the beneﬁts of determining probabilities in Section 1.5.4.\\n\\nAn alternative sparse kernel technique, known as the relevance vector machine (RVM), is based on a Bayesian formulation and provides posterior probaSection 7.2 bilistic outputs, as well as having typically much sparser solutions than the SVM',\n",
       " '74c31586-5dda-41c1-8e0b-d09ac03451c7': 'John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121– 2159, 2010. Simon Duane, Anthony D Kennedy, Brian J Pendleton, and Duncan Roweth. Hybrid monte carlo. Physics letters B, 195(2):216–222, 1987. Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference. The Journal of Machine Learning Research, 14(1):1303–1347, 2013. Geoffrey E Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The” wakesleep” algorithm for unsupervised neural networks. SCIENCE, pages 1158–1158, 1995. Koray Kavukcuoglu, Marc’Aurelio Ranzato, and Yann LeCun. Fast inference in sparse coding algorithms with applications to object recognition. Technical Report CBLLTR-2008-12-01, Computational and Biological Learning Lab, Courant Institute, NYU, 2008',\n",
       " '216ed7e3-7a15-4225-9e1a-2a90fe4a25ab': '14.16 (⋆ ⋆ ⋆) Extend the logistic regression mixture model of Section 14.5.2 to a mixture of softmax classiﬁers representing C ⩾ 2 classes. Write down the EM algorithm for determining the parameters of this model through maximum likelihood. in which each mixture component ψk(t|x) is itself a mixture model.\\n\\nShow that this two-level hierarchical mixture is equivalent to a conventional single-level mixture model. Now suppose that the mixing coefﬁcients in both levels of such a hierarchical model are arbitrary functions of x. Again, show that this hierarchical model is again equivalent to a single-level model with x-dependent mixing coefﬁcients. Finally, consider the case in which the mixing coefﬁcients at both levels of the hierarchical mixture are constrained to be linear classiﬁcation (logistic or softmax) models. Show that the hierarchical mixture cannot in general be represented by a single-level mixture having linear classiﬁcation models for the mixing coefﬁcients. Hint: to do this it is sufﬁcient to construct a single counter-example, so consider a mixture of two components in which one of those components is itself a mixture of two components, with mixing coefﬁcients given by linear-logistic models',\n",
       " '87f07ab9-774f-4197-a9ca-1494649c2d49': 'This has been an enduring challenge for artiﬁcial intelligence and explains why learning algorithms for ANNs with hidden layers have received so much attention over the years. ANNs typically learn by a stochastic gradient method (Section 9.3). Each weight is adjusted in a direction aimed at improving the network’s overall performance as measured by an objective function to be either minimized or maximized. In the most common supervised learning case, the objective function is the expected error, or loss, over a set of labeled training examples.\\n\\nIn reinforcement learning, ANNs can use TD errors to learn value functions, or they can aim to maximize expected reward as in a gradient bandit (Section 2.8) or a policy-gradient algorithm (Chapter 13). In all of these cases it is necessary to estimate how a change in each connection weight would inﬂuence the network’s overall performance, in other words, to estimate the partial derivative of an objective function with respect to each weight, given the current values of all the network’s weights. The gradient is the vector of these partial derivatives',\n",
       " 'a1df8ad2-8936-4ac3-93a5-708180fda728': 'Transfer Learning via Fine-Tuning We ﬁne-tuned the entire network using the weights of the pretrained network as initialization. We trained for 20,000 steps at a batch size of 256 using SGD with Nesterov momentum with a momentum parameter of 0.9. We set the momentum parameter for the batch normalization statistics to max(1 − 10/s, 0.9) where s is the number of steps per epoch. As data augmentation during ﬁne-tuning, we performed only random crops with resize and ﬂips; in contrast to pretraining, we did not perform color augmentation or blurring. At test time, we resized images to 256 pixels along the shorter side and took a 224 × 224 center crop. (Additional accuracy improvements may be possible with further optimization of data augmentation, particularly on the CIFAR-10 and CIFAR-100 datasets.) We selected the learning rate and weight decay, with a grid of 7 logarithmically spaced learning rates between 0.0001 and 0.1 and 7 logarithmically spaced values of weight decay between 10−6 and 10−3, as well as no weight decay',\n",
       " '08b8f2fb-35f1-4486-8ffe-8052987e79b8': 'In deﬁning the neuron-like actor and critic units, we ignored the small amount of time it takes synaptic input to e↵ect the ﬁring of a real neuron.\\n\\nWhen an action potential from the presynaptic neuron arrives at a synapse, neurotransmitter molecules are released that di↵use across the synaptic cleft to the postsynaptic neuron, where they bind to receptors on the postsynaptic neuron’s surface; this activates molecular machinery that causes the postsynaptic neuron to ﬁre (or to inhibit its ﬁring in the case of inhibitory synaptic input). This process can take several tens of milliseconds. According to (15.1) and (15.2), though, the input to a critic and actor unit instantaneously produces the unit’s output. Ignoring activation time like this is common in abstract models of Hebbian-style plasticity in which synaptic eﬃcacies change according to a simple product of simultaneous pre- and postsynaptic activity. More realistic models must take activation time into account',\n",
       " '4ff59e6f-ec0b-4c7b-9031-30d7ce5cf1a6': 'Algorithm parameters: step size ↵ 2 (0, 1], small \" > 0 Initialize Q(s, a), for all s 2 S+, a 2 A(s), arbitrarily except that Q(terminal, ·) = 0 Example 6.5: Windy Gridworld Shown inset below is a standard gridworld, with start and goal states, but with one di↵erence: there is a crosswind running upward through the middle of the grid. The actions are the standard four—up, down, right, and left—but in the middle region the resultant next states are shifted upward by a “wind,” the strength of which varies from column to column. The strength of the wind results of applying \"-greedy Sarsa to this task, with \" = 0.1, ↵ = 0.5, and the initial values Q(s, a) = 0 for all s, a.\\n\\nThe increasing slope of the graph shows that the goal was reached more quickly over time. By 8000 time steps, the greedy policy was long since optimal (a trajectory from it is shown inset); continued \"-greedy exploration kept the average episode length at about 17 steps, two more than the minimum of 15',\n",
       " '077942b3-126f-4628-b05d-f8455764126b': 'Although sometimes dismissed as irrelevant to wider issues in psychology, these experiments probe subtle properties of animal learning, often motivated by precise theoretical questions. As psychology shifted its focus to more cognitive aspects of behavior, that is, to mental processes such as thought and reasoning, animal learning experiments came to play less of a role in psychology than they once did. But this experimentation led to the discovery of learning principles that are elemental and widespread throughout the animal kingdom, principles that should not be neglected in designing artiﬁcial learning systems. In addition, as we shall see, some aspects of cognitive processing connect naturally to the computational perspective provided by reinforcement learning.\\n\\nThis chapter’s ﬁnal section includes references relevant to the connections we discuss as well as to connections we neglect. We hope this chapter encourages readers to probe all of these connections more deeply. Also included in this ﬁnal section is a discussion of how the terminology used in reinforcement learning relates to that of psychology. Many of the terms and phrases used in reinforcement learning are borrowed from animal learning theories, but the computational/engineering meanings of these terms and phrases do not always coincide with their meanings in psychology. The algorithms we describe in this book fall into two broad categories: algorithms for prediction and algorithms for control.1 These categories arise naturally in solution methods for the reinforcement learning problem presented in Chapter 3',\n",
       " '0a34ceb1-6b81-4905-a0c4-5b3f6632eb17': 'The mean and variance of the gamma distribution are given by Exercise 2.42 Consider a prior distribution Gam(λ|a0, b0).\\n\\nIf we multiply by the likelihood function (2.145), then we obtain a posterior distribution which we recognize as a gamma distribution of the form Gam(λ|aN, bN) where ML is the maximum likelihood estimator of the variance. Note that in (2.149) there is no need to keep track of the normalization constants in the prior and the likelihood function because, if required, the correct coefﬁcient can be found at the end using the normalized form (2.146) for the gamma distribution. From (2.150), we see that the effect of observing N data points is to increase the value of the coefﬁcient a by N/2. Thus we can interpret the parameter a0 in the prior in terms of 2a0 ‘effective’ prior observations. Similarly, from (2.151) we see that the N data points contribute Nσ2 ML/2 to the parameter b, where σ2 ML is the variance, and so we can interpret the parameter b0 in the prior as arising from the 2a0 ‘effective’ prior observations having variance 2b0/(2a0) = b0/a0',\n",
       " '93055adf-e780-42b5-8ea7-2cf19d6a12dc': 'It is possible to recover the new vectors into images using an auto-encoder network; however, this requires copying the entire encoding part of the CNN being trained. For deep CNNs, this results in massive auto-encoders which are very difficult and time-consuming to train. Finally, Wong et al. find that when it is possible to transform images in the data-space, data-space augmentation will outperform feature  space augmentation. Adversarial training  One of the solutions to search the space of possible augmentations is adversarial training.\\n\\nAdversarial training is a framework for using two or more networks with contrasting objectives encoded in their loss functions. This section will discuss using adversarial training as a search algorithm as well as the phenomenon of adversarial attacking. Adversarial attacking consists of a rival network that learns augmentations to images that result in misclassifications in its rival classification network. These adversarial attacks, constrained to noise injections, have been surprisingly successful from the perspective of the adversarial network. This is surprising because it com-  pletely defies intuition about how these models represent images',\n",
       " 'b7e88579-1d9d-44fc-814a-fd788f4bc9fa': 'It turned out that the injected rats had signiﬁcantly lower response rates than the non-injected rats right from the start of the extinction trials. Adams and Dickinson concluded that the injected rats associated lever pressing with consequent nausea by means of a cognitive map linking lever pressing to pellets, and pellets to nausea. Hence, in the extinction trials, the rats “knew” that the consequences of pressing the lever would be something they did not want, and so they reduced their lever-pressing right from the start. The important point is that they reduced lever-pressing without ever having experienced lever-pressing directly followed by being sick: no lever was present when they were made sick. They seemed able to combine knowledge of the outcome of a behavioral choice (pressing the lever will be followed by getting a pellet) with the reward value of the outcome (pellets are to be avoided) and hence could alter their behavior accordingly. Not every psychologist agrees with this “cognitive” account of this kind of experiment, and it is not the only possible way to explain these results, but the model-based planning explanation is widely accepted.\\n\\nNothing prevents an agent from using both model-free and model-based algorithms, and there are good reasons for using both',\n",
       " 'a29db167-cdff-4d24-b24d-df024b0b9f77': 'Again, we shall consider an error function that consists of a sum of terms, one for each pattern in the data set, so that E = � Using (5.48) and (5.49), the second derivatives on the right-hand side of (5.79) can be found recursively using the chain rule of differential calculus to give a backpropagation equation of the form If we now neglect off-diagonal elements in the second-derivative terms, we obtain  Note that the number of computational steps required to evaluate this approximation is O(W), where W is the total number of weight and bias parameters in the network, compared with O(W 2) for the full Hessian. Ricotti et al. also used the diagonal approximation to the Hessian, but they retained all terms in the evaluation of ∂2En/∂a2 j and so obtained exact expressions for the diagonal terms. Note that this no longer has O(W) scaling.\\n\\nThe major problem with diagonal approximations, however, is that in practice the Hessian is typically found to be strongly nondiagonal, and so these approximations, which are driven mainly be computational convenience, must be treated with care',\n",
       " 'c7493c85-0a13-468e-846a-f68097868dc2': 'In particular, we usually wish that, as the number of data points m in our dataset increases, our point estimates converge to the true value of the corresponding parameters. More formally, we would like that  plim,, 04m = 4. (5.55)  The symbol plim indicates convergence in probability, meaning that for any € > 0, P(\\\\Om — 6| > €) 3 0 as m - oo. The condition described by equation 5.55 is known as consistency. It is sometimes referred to as weak consistency, with strong consistency referring to the almost sure convergence of @ to @. Almost  128  CHAPTER 5.\\n\\nMACHINE LEARNING BASICS  sure convergence of a sequence of random variables x“) ,x(),... to a value  occurs when p(limm-—oo x(m) = v)=1. Consistency ensures that the bias induced by the estimator diminishes as the number of data examples grows. However, the reverse is not true—asymptotic unbiasedness does not imply consistency',\n",
       " '12a4a3b0-6d86-48a7-a7ae-8b1d050a8ce9': 'We often call the input to the RNN the “context.” We want to produce a representation of this context, C. The context C might be a vector or sequence of vectors that summarize the input sequence X = (2), ... a (=),  The simplest RNN architecture for mapping a variable-length sequence to another variable-length sequence was first proposed by Cho e¢ al. and shortly after by Sutskever et al. , who independently developed that archi-  390  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  https://www.deeplearningbook.org/contents/rnn.html    Encoder  re - ~~ re N / ~-7  Figure 10.12: Example of an encoder-decoder or sequence-to-sequence RNN architecture, for learning to generate an output sequence fy“),...,y(»)) given an input sequence (x, x@,...,x(™=))',\n",
       " 'dd82900a-11ca-42e5-94d7-d3f7f02e7ea9': 'We start by considering the label density d� of the label matrix �, deﬁned as the mean number of non-abstention labels per data point. In the low-density setting, sparsity of labels will mean that there is limited room for even an optimal weighting of the labeling functions to diverge much from the majority vote. Conversely, as the label density grows, known theory conﬁrms that the majority vote will eventually be optimal . It is the middle-density regime where we expect to most beneﬁt from applying the generative model.\\n\\nWe start by deﬁning a measure of the beneﬁt of weighting the labeling functions by their true accuracies—in other words, the predictions of a perfectly estimated generative model— versus an unweighted majority vote: Fig. 7 A plot of the modeling advantage, i.e., the improvement in label accuracyfromthegenerativemodel,asafunctionofthenumberoflabeling functions (equivalently, the label density) on a synthetic dataset',\n",
       " '25f1e943-fc91-4cd1-8b13-2e154ee51aeb': 'From (1.58) it follows that the following estimate for the variance parameter is unbiased the true Gaussian distribution from which data is generated, and the three red curves show the Gaussian distributions obtained by ﬁtting to three data sets, each consisting of two data points shown in blue, using the maximum likelihood results (1.55) and (1.56). Averaged across the three data sets, the mean is correct, but the variance is systematically under-estimated because it is measured relative to the sample mean and not relative to the true mean. In Section 10.1.3, we shall see how this result arises automatically when we adopt a Bayesian approach. Note that the bias of the maximum likelihood solution becomes less signiﬁcant as the number N of data points increases, and in the limit N → ∞ the maximum likelihood solution for the variance equals the true variance of the distribution that generated the data. In practice, for anything other than small N, this bias will not prove to be a serious problem. However, throughout this book we shall be interested in more complex models with many parameters, for which the bias problems associated with maximum likelihood will be much more severe.\\n\\nIn fact, as we shall see, the issue of bias in maximum likelihood lies at the root of the over-ﬁtting problem that we encountered earlier in the context of polynomial curve ﬁtting',\n",
       " '5299aeb2-ef5a-43d3-a4d2-8430ca3dc355': 'In other words, the key issue is that of generalization. How can experience with a limited subset of the state space be usefully generalized to produce a good approximation over a much larger subset? Fortunately, generalization from examples has already been extensively studied, and we do not need to invent totally new methods for use in reinforcement learning. To some extent we need only combine reinforcement learning methods with existing generalization methods. The kind of generalization we require is often called function approximation because it takes examples from a desired function (e.g., a value function) and attempts to generalize from them to construct an approximation of the entire function.\\n\\nFunction approximation is an instance of supervised learning, the primary topic studied in machine learning, artiﬁcial neural networks, pattern recognition, and statistical curve ﬁtting. In theory, any of the methods studied in these ﬁelds can be used in the role of function approximator within reinforcement learning algorithms, although in practice some ﬁt more easily into this role than others. Reinforcement learning with function approximation involves a number of new issues that do not normally arise in conventional supervised learning, such as nonstationarity, bootstrapping, and delayed targets. We introduce these and other issues successively over the ﬁve chapters of this part',\n",
       " '81649592-965f-4da4-9ba4-5cfdb5d95191': 'A natural way to represent the means of the Bernoulli distributions is with a vector h of probabilities, with q(hj = 1 | v) = hy. We impose a restriction that hy is never equal to 0 or to 1, in order to avoid errors when computing, for example, log hy. We will see that the variational inference equations never assign 0 or 1 to hi analytically.\\n\\nIn a software implementation, however, machine rounding error could result in 0 or 1 values. In software, we may wish to implement binary sparse  639  CHAPTER 19. APPROXIMATE INFERENCE  https://www.deeplearningbook.org/contents/inference.html    coding using.an unrestricted vector of variational parameters 2 and obtain / via the relation h = o(Z). We_can thus safely compute log ht on a computer by using the identity log (zi) = = Cla 2), relating the sigmoid and the softplus. To begin our derivation of variational learning in the binary sparse coding model, we show that the use of this mean field approximation makes learning tractable',\n",
       " 'e248d286-20a6-4854-b6e4-5e0a85372046': 'In this work, we empirically surveyed data augmentation methods for limited-data learning in NLP and compared them on 11 different NLP tasks. Despite the success, there are still certain challenges that need to be tackled for improve their performance. This section highlights some of these challenges and discusses future research directions. Theoretical Guarantees and Data Distribution Shift. Current data augmentation methods for text typically assume that they are label-preserving and will not change the data distribution.\\n\\nHowever, these assumptions are often not true in practice, which can result in noisy labels or a shift in the data distribution and consequently a decrease in performance or generalization (e.g., QQP in Table 3). Thus, providing theoretical guarantees that augmentations are label- and distributionpreserving under certain conditions would ensure the quality of augmented data and further accelerate the progress of this ﬁeld. Automatic Data Augmentation. Despite being effective, current data augmentation methods are generally manually-designed. Methods for automatically selecting the appropriate types of data augmentation still remain under-investigated. Although certain augmentation techniques have been shown effective for a particular task or dataset, they often do not transfer well to other datasets or tasks , as shown in Table 3',\n",
       " 'ee7f4e52-9228-4c15-bb64-6cb3ecdd5185': 'For an example of a sampling task using small natural images, see figure 16.1. Modeling a rich distribution over thousands or millions of random variables is a challenging task, both computationally and statistically. Suppose we wanted to model only binary variables. This is the simplest possible case, and yet already it seems overwhelming. For a small 32 x 32 pixel color (RGB) image, there are 23072 possible binary images of this form. This number is over 108°? times larger than the estimated number of atoms in the universe. In general, if we wish to model a distribution over a random vector x containing  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    nm discrete varlables capable ot taking on * values each, then the naive approach of representing /(X) by storing a lookup table with one probability value per possible outcome requires k parameters!\\n\\nThis is not feasible for several reasons:  e Memory—the cost of storing the representation: For all but very small values of n and k, representing the distribution as a table will require too many values to store',\n",
       " '924a0e30-91ec-463c-ad70-3d95b1e6cde3': 'I have tried to keep the mathematical content of the book to the minimum necessary to achieve a proper understanding of the ﬁeld. However, this minimum level is nonzero, and it should be emphasized that a good grasp of calculus, linear algebra, and probability theory is essential for a clear understanding of modern pattern recognition and machine learning techniques. Nevertheless, the emphasis in this book is on conveying the underlying concepts rather than on mathematical rigour. I have tried to use a consistent notation throughout the book, although at times this means departing from some of the conventions used in the corresponding research literature.\\n\\nVectors are denoted by lower case bold Roman letters such as x, and all vectors are assumed to be column vectors. A superscript T denotes the transpose of a matrix or vector, so that xT will be a row vector. Uppercase bold roman letters, such as M, denote matrices. The notation (w1, . , wM) denotes a row vector with M elements, while the corresponding column vector is written as w = (w1, . , wM)T',\n",
       " '5f060b3e-2de0-462e-9de9-e8fda8268d04': 'Each message sent from a node replaces any previous message sent in the same direction across the same link and will itself be a function only of the most recent messages received by that node at previous steps of the algorithm. We have seen that a message can only be sent across a link from a node when all other messages have been received by that node across its other links. Because there are loops in the graph, this raises the problem of how to initiate the message passing algorithm. To resolve this, we suppose that an initial message given by the unit function has been passed across every link in each direction. Every node is then in a position to send a message. There are now many possible ways to organize the message passing schedule. For example, the ﬂooding schedule simultaneously passes a message across every link in both directions at each time step, whereas schedules that pass one message at a time are called serial schedules. Following Kschischnang et al.\\n\\n, we will say that a (variable or factor) node a has a message pending on its link to a node b if node a has received any message on any of its other links since the last time it send a message to b',\n",
       " '63f17125-0a37-4604-b56f-281c9a37b0ab': 'In addition to the uniﬁed view of planning and learning methods, a second theme in this chapter is the beneﬁts of planning in small, incremental steps. This enables planning to be interrupted or redirected at any time with little wasted computation, which appears to be a key requirement for eﬃciently intermixing planning with acting and with learning of the model. Planning in very small steps may be the most eﬃcient approach even on pure planning problems if the problem is too large to be solved exactly. When planning is done online, while interacting with the environment, a number of interesting issues arise. New information gained from the interaction may change the model and thereby interact with planning.\\n\\nIt may be desirable to customize the planning process in some way to the states or decisions currently under consideration, or expected in the near future. If decision making and model learning are both computation-intensive processes, then the available computational resources may need to be divided between them. To begin exploring these issues, in this section we present Dyna-Q, a simple architecture integrating the major functions needed in an online planning agent. Each function appears in Dyna-Q in a simple, almost trivial, form. In subsequent sections we elaborate some of the alternate ways of achieving each function and the trade-o↵s between them',\n",
       " '25fc5553-f4f5-42c3-815f-347ffc6bdf9e': 'continues until the external stimulus ends.5 This is like assuming the animal’s nervous system has a clock that keeps precise track of time during stimulus presentations; it is what engineers call a “tapped delay line.” Like the presence representation, the CSC representation is unrealistic as a hypothesis about how the brain internally represents stimuli, but Ludvig et al. call it a “useful ﬁction” because it can reveal details of how the TD model works when relatively unconstrained by the stimulus representation. The CSC representation is also used in most TD models of dopamine-producing neurons in the brain, a topic we take up in Chapter 15. The CSC representation is often viewed as an essential part of the TD model, although this view is mistaken.\\n\\nThe MS representation (center column of Figure 14.1) is like the CSC representation in that each external stimulus initiates a cascade of internal stimuli, but in this case the internal stimuli—the microstimuli—are not of such limited and non-overlapping form; they are extended over time and overlap. As time elapses from stimulus onset, di↵erent sets of microstimuli become more or less active, and each subsequent microstimulus becomes progressively wider in time and reaches a lower maximal level',\n",
       " 'bb9792a4-a262-4f18-b0d5-f7711320a9a9': 'For this, we will need more than what can be done with a simple linear transformation. 5.8.2 k-means Clustering  Another example of a simple representation learning algorithm is k-means clustering. The k-means clustering algorithm divides the training set into k different clusters of examples that are near each other. We can thus think of the algorithm as providing a k-dimensional one-hot code vector h representing an input x.\\n\\nIf x belongs to cluster i, then h; = 1, and all other entries of the representation h are zero. The one-hot code provided by /means clustering is an example of a sparse representation, because the majority of its entries are zero for every input. Later, we develop other algorithms that learn more flexible sparse representations, where more than one entry can be nonzero for each input x. One-hot codes are an extreme example of sparse representations that lose many of the benefits of a distributed  https://www.deeplearningbook.org/contents/ml.html    representation',\n",
       " 'b78d9127-5cf6-4ca2-8843-8889ff83b376': ', xn, and multiply by the transition probability p(zn|zn−1) and the emission probability p(xn|zn) and then marginalize over zn−1, we obtain a distribution over zn that is of the same functional form as that over �α(zn−1). That is to say, the distribution must not become more complex at each stage, but must only change in its parameter values.\\n\\nNot surprisingly, the only distributions that have this property of being closed under multiplication are those belonging to the exponential family. Here we consider the most important example from a practical perspective, which is the Gaussian. In particular, we consider a linear-Gaussian state space model so that the latent variables {zn}, as well as the observed variables {xn}, are multivariate Gaussian distributions whose means are linear functions of the states of their parents in the graph. We have seen that a directed graph of linear-Gaussian units is equivalent to a joint Gaussian distribution over all of the variables. Furthermore, marginals such as �α(zn) are also Gaussian, so that the functional form of the messages is preserved and we will obtain an efﬁcient inference algorithm',\n",
       " '7043ff3d-08e9-4e63-9ddf-d188c0d304d0': 'Note that each policy evaluation, itself an iterative computation, is started with the value function for the previous policy. This typically results in a great increase in the speed of convergence of policy evaluation (presumably because the value function changes little from one policy to the next). Policy Iteration (using iterative policy evaluation) for estimating ⇡ ⇡ ⇡⇤ until ∆ < ✓ (a small positive number determining the accuracy of estimation) If policy-stable, then stop and return V ⇡ v⇤ and ⇡ ⇡ ⇡⇤; else go to 2 rental company. Each day, some number of customers arrive at each location to rent cars. If Jack has a car available, he rents it out and is credited $10 by the national company.\\n\\nIf he is out of cars at that location, then the business is lost. Cars become available for renting the day after they are returned. To help ensure that cars are available where they are needed, Jack can move them between the two locations overnight, at a cost of $2 per car moved. We assume that the number of cars requested and returned at each location are Poisson random variables, meaning that the probability that the number is n is λn n! e−λ, where λ is the expected number',\n",
       " 'e01805ab-0502-4bb1-8d1d-72020226d9a3': 'The position of the particle at any point in time is given by @(t). The particle experiences net force f(t). This force causes the particle to accelerate:  o2 t) = =59(t). 8.18 F(t) = att) (8.18) 294  CHAPTER 8.\\n\\nOPTIMIZATION FOR TRAINING DEEP MODELS  https://www.deeplearningbook.org/contents/optimization.html    Rather than viewing this as a second-order differential equation of the position, we can introduce the variable v(t) representing the velocity of the particle at time  t and rewrite the Newtonian dynamics as a first-order differential equation:  (a)  v(t) = RO)» (8.19) (a)  f= ae): (8.20)  The momentum algorithm then consists of solving the differential equations via numerical simulation. A simple numerical method for solving differential equations is Euler’s method, which simply consists of simulating the dynamics defined by the equation by taking small, finite steps in the direction of each gradient. This explains the basic form of the momentum update, but what specifically are the forces? One force is proportional to the negative gradient of the cost function: —VoJ(8)',\n",
       " 'eb4370b8-f465-45ae-8088-37438b2168cf': 'An early version of this paper was originally published in 1982.\\n\\nThe MacSX was not designed and managed by Kia.\\\\n\\\\n Macintosh\\\\n The mac has been a family invention space: comet asteroid spaceship This essay discusses a topic: the impact of two of the Earth’s two-thirds comet-sized moon Charon on Earth, and why asteroids are so close to the sun; why people are looking for ways to ﬁnd a way to keep Earth-shaped asteroids out of orbit. religion: faith faith faith salvation This essay discusses the impact religion has on the American experience and in American culture. Since the beginning of my career I have found that faith and belief have often been linked to economic growth, social development and education. I believe that all people need to know that there is no reason for science: climate research chemistry This essay discusses the role of molecular information and its interaction with the general organism and human health.\\\\n\\\\n \"The idea of biological information is not really a new concept. We used genetic information as a medium to deﬁne, identify, and store information about biology and biology,\" explains Dr',\n",
       " '0a002323-6fc2-4993-ac9c-92a758ac08b4': '16.2.5 Separation and D-Separation  The edges in a graphical model tell us which variables directly interact. We often need to know which variables indirectly interact. Some of these indirect interactions can be enabled or disabled by observing other variables. More formally, we would like to know which subsets of variables are conditionally independent from each other, given the values of other subsets of variables. Identifying the conditional independences in a graph is simple for undirected models. In this case, conditional independence implied by the graph is called separation.\\n\\nWe say that a set of variables A is separated from another set of variables B given a third set of variables S if the graph structure implies that A is independent from B given S. If two variables a and b are connected by a path involving only unobserved variables, then those variables are not separated. If no path exists between them, or all paths contain an observed variable, then they are separated. We refer to paths involving only unobserved variables as “active” and paths including an observed variable as “inactive.”  When we draw a graph, we can indicate observed variables by shading them in',\n",
       " '5e603639-c47a-4858-afa4-be00bcc21f5d': 'Regarding the update of augmentation parameters φ (Eq.8), since text samples are discrete, to enable efﬁcient gradient propagation through θ′ to φ, we use a gumbel-softmax approximation  to x when sampling substitution words from the LM. Learning Data Weights We now demonstrate the instantiation of data weighting. We aim to assign an importance weight to each training example to adapt its effect on model training. We automate the process by learning the data weights. This is achieved by parameterizing Rφ as: In practice, when minibatch stochastic optimization is used, we approximate the weighted sampling by taking the softmax over the weights of only the minibatch examples. The data weights φ are updated with Eq.(8). It is worth noting that the previous work  similarly derives data weights based on their gradient directions on a validation set.\\n\\nOur algorithm differs in that the data weights are parameters maintained and updated throughout the training, instead of re-estimated from scratch in each iteration. Experiments show the parametric treatment achieves superior performance in various settings. There are alternative parameterizations of Rφ other than Eq.(11)',\n",
       " '25eeab9d-0359-470d-af16-e15874bf6285': 'Speciﬁcally, suppose that, for any time step, the true values of actions 1 and 2 are respectively 0.1 and 0.2 with probability 0.5 (case A), and 0.9 and 0.8 with probability 0.5 (case B). If you are not able to tell which case you face at any step, what is the best expectation of success you can achieve and how should you behave to achieve it? Now suppose that on each step you are told whether you are facing case A or case B (although you still don’t know the true action values). This is an associative search task. What is the best expectation of success you can achieve in this task, and how should you behave to achieve it? ⇤ We have presented in this chapter several simple ways of balancing exploration and exploitation. The \"-greedy methods choose randomly a small fraction of the time, whereas UCB methods choose deterministically but achieve exploration by subtly favoring at each step the actions that have so far received fewer samples. Gradient bandit algorithms estimate not action values, but action preferences, and favor the more preferred actions in a graded, probabilistic manner using a soft-max distribution.\\n\\nThe simple expedient of initializing estimates optimistically causes even greedy methods to explore signiﬁcantly',\n",
       " '8f43f9fb-f362-47d4-a9d6-79350747e906': 'We can think of a location-based read instruction as saying “Retrieve the lyrics of the song in slot 347.” Location-based addressing can often be a perfectly sensible mechanism even when the memory cells are small. If the content of a memory cell is copied (not forgotten) at most time steps, then the information it contains can be propagated forward in time and the gradients propagated backward in time without either vanishing or exploding. The explicit memory approach is illustrated in figure 10.18, where we see that a “task neural network” is coupled with a memory.\\n\\nAlthough that task neural network could be feedforward or recurrent, the overall system is a recurrent network. The task network can choose to read from or write to specific memory addresses. 413  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  https://www.deeplearningbook.org/contents/rnn.html    Memory cells  Reading Writing  mechanism  mechanism  Task network, controlling the memory  Figure 10.18: A schematic of a network with an explicit memory, capturing some of the key design elements of the neural Turing machine',\n",
       " 'd715012b-14e2-40a3-bdb6-6f7d36d89d51': 'Some variants of ICA avoid this problematic operation by constraining W to be orthogonal. All variants of ICA require that p(h) be non-Gaussian. This is because if p(h)  https://www.deeplearningbook.org/contents/linear_factors.html    is an independent prior with Gaussian components, then W is not identifiable. We can obtain the same distribution over p(#) for many values of W. This is very different from other linear factor models like probabilistic PCA and factor analysis,  which often require p(h) to be Gaussian in order to make many operations on the model have closed form solutions.\\n\\nIn the maximum likelihood approach, where the user explicitly specifies the distribution, a typical choice is to use p(h;) = toh): Typical choices of these non-Gaussian distributions have larger peaks near 0 than does the Gaussian distribution, so we can also see most implementations of ICA as learning sparse features. Many variants of ICA are not generative models in the sense that we use the  488  CHAPTER 13. LINEAR FACTOR MODELS  phrase',\n",
       " '5320df97-5853-4e00-aa88-fe420d1fc4ba': 'In Proceedings of the 2nd Berkeley Symposium on Mathematical Statistics and Probabilities, pp. 481–492. University of California Press. Kullback, S. and R. A. Leibler . On information and sufﬁciency. Annals of Mathematical Statistics 22(1), 79–86. K˙urkov´a, V. and P. C. Kainen . Functionally equivalent feed-forward neural networks. Neural Computation 6(3), 543–558. Kuss, M. and C. Rasmussen . Assessing approximations for Gaussian process classiﬁcation. LeCun, Y., L. Bottou, Y. Bengio, and P. Haffner . Gradient-based learning applied to document recognition. Proceedings of the IEEE 86, 2278–2324. Lee, Y., Y. Lin, and G. Wahba',\n",
       " '4c12ce5c-aff8-4acf-a21f-2dbdbc4dccc2': \"Thus the model predicts a noise variance orthogonal to the principal subspace, which, from (12.46), is just the average of the discarded eigenvalues. Now suppose that v = Ui where Ui is one of the retained eigenvectors defining the principal subspace. Then vTCv = (Ai (J'2) +  or through the EM algorithm, then the resulting value of R is essentially arbitrary. This implies that the columns of W need not be orthogonal. If an orthogonal basis is required, the matrix W can be post-processed appropriately . Alternatively, the EM algorithm can be modified in such a way as to yield orthonormal principal directions, sorted in descending order of the corresponding eigenvalues, directly . The rotational invariance in latent space represents a form of statistical nonidentifiability, analogous to that encountered for mixture models in the case of discrete latent variables\",\n",
       " '4f6b5c73-1cc9-4162-91f1-779cced896c0': \"Known Problems  Exploration-Exploitation Dilemma  The problem of exploration vs exploitation dilemma has been discussed in my previous post.\\n\\nWhen the RL problem faces an unknown environment, this issue is especially a key to finding a good solution: without enough exploration, we cannot learn the environment well enough; without  https://lilianweng.github.io/posts/2018-02-19-rl-overview/  A (Long) Peek into Reinforcement Learning | Lil'Log  enough exploitation, we cannot complete our reward optimization task. Different RL algorithms balance between exploration and exploitation in different ways. In MC methods, Q-learning or many on-policy algorithms, the exploration is commonly implemented by ¢- greedy; In ES, the exploration is captured by the policy parameter perturbation. Please keep this into consideration when develop a new RL algorithm. Deadly Triad Issue  We do seek the efficiency and flexibility of TD methods that involve bootstrapping. However, when off-policy, nonlinear function approximation, and bootstrapping are combined in one RL algorithm, the training could be unstable and hard to converge. This issue is known as the deadly triad\",\n",
       " 'd135f37d-dd02-422d-90c7-2ff1d47408e4': 'The frame was preprocessed and added to the four-frame stack that became the next input to the network. Skipping for the moment the changes to the basic Q-learning procedure made by Mnih et al., DQN used the following semi-gradient form of Q-learning to update the network’s weights: where wt is the vector of the network’s weights, At is the action selected at time step t, and St and St+1 are respectively the preprocessed image stacks input to the network at time steps t and t + 1. The gradient in (16.3) was computed by backpropagation. Imagining again that there was a separate network for each action, for the update at time step t, backpropagation was applied only to the network corresponding to At. Mnih et al.\\n\\ntook advantage of techniques shown to improve the basic backpropagation algorithm when applied to large networks. They used a mini-batch method that updated weights only after accumulating gradient information over a small batch of images (here after 32 images). This yielded smoother sample gradients compared to the usual procedure that updates weights after each action',\n",
       " 'e2972032-300d-4d58-9de2-bc7d3bf7a78d': 'An extension to mixtures of conditional Gaussian distributions, which permit multimodal conditional distributions, will be discussed in Section 14.5.1. Now consider a data set of inputs X = {x1, . , xN} with corresponding target values t1, . , tN. We group the target variables {tn} into a column vector that we denote by t where the typeface is chosen to distinguish it from a single observation of a multivariate target, which would be denoted t. Making the assumption that these data points are drawn independently from the distribution (3.8), we obtain the following expression for the likelihood function, which is a function of the adjustable parameters w and β, in the form where we have used (3.3). Note that in supervised learning problems such as regression (and classiﬁcation), we are not seeking to model the distribution of the input variables',\n",
       " 'f1c3621d-8d8d-4a70-940e-3c5d2e536047': 'Memory-based methods such as the weighted average and locally weighted regression methods described above depend on assigning weights to examples s0 7! g in the database depending on the distance between s0 and a query states s. The function that assigns these weights is called a kernel function, or simply a kernel. In the weighted average and locally weighted regressions methods, for example, a kernel function k : R ! R assigns weights to distances between states. More generally, weights do not have to depend on distances; they can depend on some other measure of similarity between states. In this case, k : S ⇥ S ! R, so that k(s, s0) is the weight given to data about s0 in its inﬂuence on answering queries about s. Viewed slightly di↵erently, k(s, s0) is a measure of the strength of generalization from s0 to s',\n",
       " '1f6d4d47-72e8-495d-88ea-bf9e197f36f6': 'Let θ and θ′ be two parameter vectors in Rd. Then, we will ﬁrst attempt to bound W(Pθ, Pθ′), from where the theorem will come easily. The main element of the proof is the use of the coupling γ, the distribution of the joint (gθ(Z), gθ′(Z)), which clearly has γ ∈ Π(Pθ, Pθ′). By the deﬁnition of the Wasserstein distance, we have If g is continuous in θ, then gθ(z) →θ→θ′ gθ′(z), so ∥gθ − gθ′∥ → 0 pointwise as functions of z. Since X is compact, the distance of any two elements in it has to be uniformly bounded by some constant M, and therefore ∥gθ(z) − gθ′(z)∥ ≤ M for all θ and z uniformly. By the bounded convergence theorem, we therefore have whenever (θ′, z) ∈ U. Therefore, we can deﬁne Uθ = {θ′|(θ′, z) ∈ U}',\n",
       " '36026407-02c9-4ec9-b4f9-74a6de65a105': 'On the Feret dataset, accuracy improved from 83.52 to 88.46%. The audience dataset responded with an improvement of 70.02% to 76.06%.\\n\\nMost interestingly, results from another face dataset increased from 88.15 to 95.66%. This was compared with traditional augmentation techniques which increased the accuracy from 88.15 to 89.08%. Addition- ally, this experiment derived the same accuracy when using two Network-As in the aug-  mentation framework as was found with one Network-A. This experiment demonstrates    6:60  Shorten and Khoshgoftaar J Big Data  s0U9 uoljezjeW0U qno-doig 1nez|| auenbs ueaw yozeg (vq) vonsuny sso7  =|  Adosyua -$s019 jesu08aye> (87) vonzuny sso7  yo yndyri',\n",
       " '43132575-a7ce-4c4d-97ce-1043ef297cd7': 'Once again, the derivative of the error function with respect to the activation for In summary, there is a natural choice of both output unit activation function and matching error function, according to the type of problem being solved. For regression we use linear outputs and a sum-of-squares error, for (multiple independent) binary classiﬁcations we use logistic sigmoid outputs and a cross-entropy error function, and for multiclass classiﬁcation we use softmax outputs with the corresponding multiclass cross-entropy error function. For classiﬁcation problems involving two classes, we can use a single logistic sigmoid output, or alternatively we can use a network with two outputs having a softmax output activation function. We turn next to the task of ﬁnding a weight vector w which minimizes the chosen function E(w).\\n\\nAt this point, it is useful to have a geometrical picture of the error function, which we can view as a surface sitting over weight space as shown in Figure 5.5',\n",
       " '3ff62cd0-32fe-4952-bd97-c51fa132c3df': 'Dro opping terms of £ that do not vary with ys, we are left with the optimization prob  p* = arg max log p(h = p, v), (19.12) wb  which is equivalent to the MAP inference problem  h* = argmax p(h | v). (19.13) h  We can thus justify a learning procedure similar to EM, in which we alternate between performing MAP inference to infer h* and then update @ to increase log p(h*,v). As with EM, this is a form of coordinate ascent on £, where we alternate between using inference to optimize £ with respect to q and using parameter updates to optimize £ with respect to 6. The procedure as a whole can be justified by the fact that Lis a lower bound on log p(v). In the case of MAP inference, this justification is rather vacuous, because the bound is infinitely loose, due to the Dirac distribution’s differential entropy of negative infinity. Adding noise to would make the bound meaningful again',\n",
       " '2bea502b-4cf2-4454-a564-f05bf55293f9': 'Let us write the joint distribution over a set of variables in the form of a product of factors p(x) = � where xs denotes a subset of the variables. For convenience, we shall denote the individual variables by xi, however, as in earlier discussions, these can comprise groups of variables (such as vectors or matrices). Each factor fs is a function of a corresponding set of variables xs. Directed graphs, whose factorization is deﬁned by (8.5), represent special cases of (8.59) in which the factors fs(xs) are local conditional distributions. Similarly, undirected graphs, given by (8.39), are a special case in which the factors are potential functions over the maximal cliques (the normalizing coefﬁcient 1/Z can be viewed as a factor deﬁned over the empty set of variables).\\n\\nIn a factor graph, there is a node (depicted as usual by a circle) for every variable in the distribution, as was the case for directed and undirected graphs. There are also additional nodes (depicted by small squares) for each factor fs(xs) in the joint distribution',\n",
       " 'b87bf92a-1618-460f-a359-d6330c88bd22': 'When dis an eigenvector of H,, the second derivative in that direction is given by the corresponding eigenvalue.\\n\\nFor other directions of d, the directional second derivative is a weighted average of all the eigenvalues, with weights between 0 and 1, and eigenvectors that have a smaller angle with d receiving more weight. The maximum eigenvalue determines the maximum second derivative, and the minimum eigenvalue determines the minimum second derivative. The (directional) second derivative tells us how well we can expect a gradient descent step to perform. We can make a second-order Taylor series approximation  (0)  4a 40 Latta LN 3-4-2 A LL Arent Att me  https://www.deeplearningbook.org/contents/numerical.html    LO LUG LULCLIOL J (we) ALOULLU LUG CULLELL PULL w&  fe) ~ fa) +(e-2)\"g+3(e-2)\\' Hwa), (48) where g is the gradient and H is the Hessian at 2)',\n",
       " 'df3b514e-f7f1-487b-9afa-3662d95e632f': '8.21 (⋆ ⋆) www Show that the marginal distributions p(xs) over the sets of variables xs associated with each of the factors fx(xs) in a factor graph can be found by ﬁrst running the sum-product message passing algorithm and then evaluating the required marginals using (8.72).\\n\\n8.22 (⋆) Consider a tree-structured factor graph, in which a given subset of the variable nodes form a connected subgraph (i.e., any variable node of the subset is connected to at least one of the other variable nodes via a single factor node). Show how the sum-product algorithm can be used to compute the marginal distribution over that subset. 8.23 (⋆ ⋆) www In Section 8.4.4, we showed that the marginal distribution p(xi) for a variable node xi in a factor graph is given by the product of the messages arriving at this node from neighbouring factor nodes in the form (8.63). Show that the marginal p(xi) can also be written as the product of the incoming message along any one of the links with the outgoing message along the same link',\n",
       " '7cade3e0-5748-4370-8adf-91a817646936': 'Further insight into the role of the equivalent kernel can be obtained by considering the covariance between y(x) and y(x′), which is given by where we have made use of (3.49) and (3.62). From the form of the equivalent kernel, we see that the predictive mean at nearby points will be highly correlated, whereas for more distant pairs of points the correlation will be smaller. The predictive distribution shown in Figure 3.8 allows us to visualize the pointwise uncertainty in the predictions, governed by (3.59).\\n\\nHowever, by drawing samples from the posterior distribution over w, and plotting the corresponding model functions y(x, w) as in Figure 3.9, we are visualizing the joint uncertainty in the posterior distribution between the y values at two (or more) x values, as governed by the equivalent kernel. The formulation of linear regression in terms of a kernel function suggests an alternative approach to regression as follows. Instead of introducing a set of basis functions, which implicitly determines an equivalent kernel, we can instead deﬁne a localized kernel directly and use this to make predictions for new input vectors x, given the observed training set. This leads to a practical framework for regression (and classiﬁcation) called Gaussian processes, which will be discussed in detail in Section 6.4',\n",
       " '03c5e6d7-2f19-4e69-a2cc-5992f46558c2': 'Perhaps the ﬁrst to succinctly express the essence of trial-and-error learning as a principle of learning was Edward Thorndike: Of several responses made to the same situation, those which are accompanied or closely followed by satisfaction to the animal will, other things being equal, be more ﬁrmly connected with the situation, so that, when it recurs, they will be more likely to recur; those which are accompanied or closely followed by discomfort to the animal will, other things being equal, have their connections with that situation weakened, so that, when it recurs, they will be less likely to occur.\\n\\nThe greater the satisfaction or discomfort, the greater the strengthening or weakening of the bond. (Thorndike, 1911, p. 244) Thorndike called this the “Law of E↵ect” because it describes the e↵ect of reinforcing events on the tendency to select actions. Thorndike later modiﬁed the law to better account for subsequent data on animal learning (such as di↵erences between the e↵ects of reward and punishment), and the law in its various forms has generated considerable controversy among learning theorists',\n",
       " '7d9fe85b-865d-468d-901d-b2f32563531e': 'In this setting, each synapse has its own eligibility trace that records past activity involving that synapse. The only di↵erence between the actor and critic learning rules is that they use di↵erent kinds of eligibility traces: the critic unit’s traces are non-contingent because they do not involve the critic unit’s output, whereas the actor unit’s traces are contingent because in addition to the actor unit’s input, they depend on the actor unit’s output.\\n\\nIn the hypothetical implementation of an actor–critic system in the brain, these learning rules respectively correspond to rules governing plasticity of corticostriatal synapses that convey signals from the cortex to the principal neurons in the dorsal and ventral striatal subdivisions, synapses that also receive inputs from dopamine neurons. The learning rule of an actor unit in the actor–critic network closely corresponds to reward-modulated spike-timing-dependent plasticity. In spike-timing-dependent plasticity (STDP), the relative timing of pre- and postsynaptic activity determines the direction of synaptic change',\n",
       " '4d80d86f-4755-4b92-8bfe-07f12f0f740b': 'Figure 10.4 shows the results of a more detailed study of the e↵ect of the parameters ↵ and n on the rate of learning on this task. Exercise 10.1 We have not explicitly considered or given pseudocode for any Monte Carlo methods in this chapter. What would they be like? Why is it reasonable not to give pseudocode for them?\\n\\nHow would they perform on the Mountain Car task? ⇤ Exercise 10.2 Give pseudocode for semi-gradient one-step Expected Sarsa for control. ⇤ We now introduce a third classical setting—alongside the episodic and discounted settings— for formulating the goal in Markov decision problems (MDPs). Like the discounted setting, the average reward setting applies to continuing problems, problems for which the interaction between agent and environment goes on and on forever without termination or start states. Unlike that setting, however, there is no discounting—the agent cares just as much about delayed rewards as it does about immediate reward. The average-reward setting is one of the major settings commonly considered in the classical theory of dynamic programming and less-commonly in reinforcement learning. As we discuss in the next section, the discounted setting is problematic with function approximation, and thus the average-reward setting is needed to replace it',\n",
       " '77c0f777-ea97-49dd-916d-71a832188a87': 'Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19–27.\\n\\nAppendix for “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding” We organize the appendix into three sections: • Additional implementation details for BERT are presented in Appendix A; • Additional details for our experiments are presented in Appendix B; and We provide examples of the pre-training tasks in the following. hairy, and during the random masking procedure we chose the 4-th token (which corresponding to hairy), our masking procedure can be further illustrated by • 80% of the time: Replace the word with the  token, e.g., my dog is hairy → • 10% of the time: Replace the word with a random word, e.g., my dog is hairy → my is hairy. The purpose of this is to bias the representation towards the actual observed word. The advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predict or which have been replaced by random words, so it is forced to keep a distributional contextual representation of every input token',\n",
       " 'f96b09c6-e227-4211-b35e-3752f2b02aff': 'Instead of using the raw embeddings directly, we need to refine the embedding with further fine-tuning. Natural Language Inference (NLI) tasks are the main data sources to provide supervised signals for learning sentence embedding; such as SNLI, MNLI, and QQP.\\n\\nSentence-BERT  SBERT (Sentence-BERT)  relies on siamese and triplet network architectures to learn sentence embeddings such that the sentence similarity can be estimated by cosine similarity between pairs of embeddings. Note that learning SBERT depends on supervised data, as it is fine-tuned on several NLI datasets. They experimented with a few different prediction heads on top of BERT model:  Softmax classification objective: The classification head of the siamese network is built on the concatenation of two embeddings f(x), f(x’) and | f(x) — f(x’)|. The predicted output is 9 = softmax(W,|f(x); f(x’); | f(x) — f(«’)|])',\n",
       " 'c38531b0-c07b-491d-bba2-f61098294e3e': 'Thus, by (9.8), the approximate value function will be a↵ected at all states within the union of the circles, with a greater e↵ect the more circles a point has “in common” with the state, as shown in Figure 9.6. If the circles are small, then the generalization will be over a short distance, as in Figure 9.7 (left), whereas if they are large, it will be over a large distance, as in Figure 9.7 (middle). Moreover, the shape of the features will determine the nature of the generalization. For example, if they are not strictly circular, but are elongated in one direction, then generalization will be similarly a↵ected, as in Figure 9.7 (right).\\n\\nFeatures with large receptive ﬁelds give broad generalization, but might also seem to limit the learned function to a coarse approximation, unable to make discriminations much ﬁner than the width of the receptive ﬁelds. Happily, this is not the case. Initial generalization from one point to another is indeed controlled by the size and shape of the receptive ﬁelds, but acuity, the ﬁnest discrimination ultimately possible, is controlled more by the total number of features',\n",
       " 'df1ac8eb-34b8-473c-b32b-5fd879b6025a': 'An analysis of temporal-di↵erence learning with function approximation. IEEE Transactions on Automatic Control, 42(5):674–690. Tsitsiklis, J. N., Van Roy, B. Average cost temporal-di↵erence learning. Automatica, Turing, A. M. Intelligent machinery. In B. Jack Copeland (Ed.) , The Essential Turing, pp. 410–432. Oxford University Press, Oxford. Ungar, L. H. A bioreactor benchmark for adaptive network-based process control. In W. T. Miller, R. S. Sutton, and P. J. Werbos (Eds. ), Neural Networks for Control, pp. 387–402. MIT Press, Cambridge, MA. Unnikrishnan, K. P., Venugopal, K. P',\n",
       " '228d81b6-8cfb-47f7-843d-4d80eecfac8c': 'We therefore need to solve the inverse problem, which has two solutions as seen in Figure 5.18. Forward problems often corresponds to causality in a physical system and generally have a unique solution. For instance, a speciﬁc pattern of symptoms in the human body may be caused by the presence of a particular disease. In pattern recognition, however, we typically have to solve an inverse problem, such as trying to predict the presence of a disease given a set of symptoms. If the forward problem involves a many-to-one mapping, then the inverse problem will have multiple solutions. For instance, several different diseases may result in the same symptoms. In the robotics example, the kinematics is deﬁned by geometrical equations, and the multimodality is readily apparent. However, in many machine learning problems the presence of multimodality, particularly in problems involving spaces of high dimensionality, can be less obvious.\\n\\nFor tutorial purposes, however, we shall consider a simple toy problem for which we can easily visualize the multimodality',\n",
       " 'c20da818-b6a6-4e4a-8103-5697264c0acf': 'However, the greater ﬂexibility of the variational approximation leads to improved accuracy compared to the Laplace method.\\n\\nFurthermore (unlike the Laplace method), the variational approach is optimizing a well deﬁned objective function given by a rigourous bound on the model evidence. Logistic regression has also been treated by Dybowski and Roberts  from a Bayesian perspective using Monte Carlo sampling techniques. Here we shall make use of a variational approximation based on the local bounds introduced in Section 10.5. This allows the likelihood function for logistic regression, which is governed by the logistic sigmoid, to be approximated by the exponential of a quadratic form. It is therefore again convenient to choose a conjugate Gaussian prior of the form (4.140). For the moment, we shall treat the hyperparameters m0 and S0 as ﬁxed constants. In Section 10.6.3, we shall demonstrate how the variational formalism can be extended to the case where there are unknown hyperparameters whose values are to be inferred from the data. In the variational framework, we seek to maximize a lower bound on the marginal likelihood',\n",
       " '9b8786d7-b754-41c5-9d5f-1e2d4ecea5a4': 'Such a hybrid criterion had previously been introduced for RBMs  https://www.deeplearningbook.org/contents/generative_models.html    by Larochelle and Bengio . They show improved classification performance using this scheme. 20.13 Other Generation Schemes  The methods we have described so far use either MCMC sampling, ancestral sampling, or some mixture of the two to generate samples. While these are the most popular approaches to generative modeling, they are by no means the only approaches. Sohl-Dickstein ef al. developed a diffusion inversion training scheme for learning a generative model, based on nonequilibrium thermodynamics.\\n\\nThe approach is based on the idea that the probability distributions we wish to sample from have structure. This structure can gradually be destroyed by a diffusion process that incrementally changes the probability distribution to have more entropy. To form a generative model, we can run the process in reverse, by training a model that gradually restores the structure to an unstructured distribution. By iteratively applying a process that brings a distribution closer to the target one, we can gradually approach that target distribution',\n",
       " '00713bde-d951-4873-91d6-418b7af98227': 'However, this can result in much slower training because, instead of solving K separate optimization problems each over N data points with an overall cost of O(KN 2), a single optimization problem of size (K −1)N must be solved giving an overall cost of O(K2N 2). Another approach is to train K(K −1)/2 different 2-class SVMs on all possible pairs of classes, and then to classify test points according to which class has the highest number of ‘votes’, an approach that is sometimes called one-versus-one. Again, we saw in Figure 4.2 that this can lead to ambiguities in the resulting classiﬁcation. Also, for large K this approach requires signiﬁcantly more training time than the one-versus-the-rest approach. Similarly, to evaluate test points, signiﬁcantly more computation is required. The latter problem can be alleviated by organizing the pairwise classiﬁers into a directed acyclic graph (not to be confused with a probabilistic graphical model) leading to the DAGSVM',\n",
       " '766217a7-a6b4-4075-8442-224370c083fc': 'Then the associative strengths of the stimulus components change according to these expressions: where ↵AβY and ↵XβY are the step-size parameters, which depend on the identities of the CS components and the US, and RY is the asymptotic level of associative strength that the US Y can support. (Rescorla and Wagner used λ here instead of R, but we use R to avoid confusion with our use of λ and because we usually think of this as the magnitude of a reward signal, with the caveat that the US in classical conditioning is not necessarily rewarding or penalizing.) A key assumption of the model is that the aggregate associative strength VAX is equal to VA + VX. The associative strengths as changed by these ∆s become the associative strengths at the beginning of the next trial. To be complete, the model needs a response-generation mechanism, which is a way of mapping values of V s to CRs. Because this mapping would depend on details of the experimental situation, Rescorla and Wagner did not specify a mapping but simply assumed that larger V s would produce stronger or more likely CRs, and that negative V s would mean that there would be no CRs',\n",
       " 'f7b6d4c7-275a-433c-944c-c3c44186e5b2': 'This presumably is why the exhaustive, unfocused approach does better in the long run, at least for small problems.\\n\\nThese results are not conclusive because they are only for problems generated in a particular, random way, but they do suggest that sampling according to the on-policy distribution can be a great advantage for large problems, in particular for problems in which a small subset of the state–action space is visited under the on-policy distribution. Exercise 8.7 Some of the graphs in Figure 8.8 seem to be scalloped in their early portions, particularly the upper graph for b = 1 and the uniform distribution. Why do you think this is? What aspects of the data shown support your hypothesis? ⇤ Real-time dynamic programming, or RTDP, is an on-policy trajectory-sampling version of the value-iteration algorithm of dynamic programming (DP). Because it is closely related to conventional sweep-based policy iteration, RTDP illustrates in a particularly clear way some of the advantages that on-policy trajectory sampling can provide. RTDP updates the values of states visited in actual or simulated trajectories by means of expected tabular value-iteration updates as deﬁned by (4.10)',\n",
       " '123e4887-ce7d-4dc1-958e-fd388ff71e12': 'Another form of dynamic experience comes from the changing environments, such as the data stream in online learning  whose distribution can change over time, or the experience in lifelong learning  that diﬀers across a series of tasks. In particular, we consider an online setting: at each time τ ∈ {1, ..., T}, a predictor is given an input and is required to make a prediction (e.g., if the stock market will go up or down tomorrow where). We have access to the recommended prediction by each of the K experts t = {1, ..., K}, and make our prediction accordingly. As a result, the environment reveals a reward based on the discrepancy between the prediction and the true answer.\\n\\nThe sequence of data instances follows a dynamic that is unknown and can even be adversarially adaptive to the predictor’s behavior (e.g., in the problem of spam email ﬁltering, or other strategic game environments) . In such cases, we can only hope the predictor to achieve some relative performance guarantee, in particular w.r.t. the best single expert in hindsight. This is formally captured by regret, which is the diﬀerence between the cumulative reward of the predictor and that of the best single expert',\n",
       " 'f0854b40-eb46-4b8c-ac0a-ae492cb0c2d8': \"(20.4 The energy function for an RBM is given by E(v,h) =—b'v—clh—v' Wh, (20.5  and Z is the normalizing constant known as the partition function:  Z=S_Soexp{-E(v,h)}. (20.6 v oh  It is apparent from the definition of the partition function Z that the naive method of computing Z (exhaustively summing over all states) could be computationally intractable, unless a cleverly designed algorithm could exploit regularities in the probability distribution to compute Z faster. In the case of restricted Boltzmann machines, Long and Servedio  formally proved that the partition function Z is intractable. The intractable partition function Z implies that the normalized joint probability distribution P(v) is also intractable to evaluate\",\n",
       " '1d771a12-ccd1-4af3-aec3-2f13dda38699': 'MACHINE LEARNING BASICS  introduce some dependencies between the regions through additional assumptions  https://www.deeplearningbook.org/contents/ml.html    about the underlying data-generating distribution. In this way, we can actually generalize nonlocally . Many ifferent deep learning algorithms provide implicit or explicit assumptions that are reasonable for a broad range of AI tasks in order to capture these advantages. Other approaches to machine learning often make stronger, task-specific as- sumptions. For example, we could easily solve the checkerboard task by providing the assumption that the target function is periodic. Usually we do not include such strong, task-specific assumptions in neural networks so that they can generalize to a much wider variety of structures. AI tasks have structure that is much too complex to be limited to simple, manually specified properties such as periodicity, so we want learning algorithms that embody more general-purpose assumptions.\\n\\nThe core idea in deep learning is that we assume that the data was generated by the composition of factors, or features, potentially at multiple levels in a hierar- chy. Many other similarly generic assumptions can further improve deep learning algorithms. These apparently mild assumptions allow an exponential gain in the relationship between the number of examples and the number of regions that can be distinguished',\n",
       " '2a848beb-3728-4b13-bdf6-97618cbd3222': 'If the target and behavior policies are very di↵erent it probably needs some new algorithmic ideas before it can be eﬃcient and practical. The other, based on tree-backup updates, is the natural extension of Q-learning to the multi-step case with stochastic target policies. It involves no importance sampling but, again if the target and behavior policies are substantially di↵erent, the bootstrapping may span only a few steps even if n is large. The notion of n-step returns is due to Watkins , who also ﬁrst discussed their error reduction property. n-step algorithms were explored in the ﬁrst edition of this book, in which they were treated as of conceptual interest, but not feasible in practice. The work of Cichosz  and particularly van Seijen  showed that they are actually completely practical algorithms.\\n\\nGiven this, and their conceptual clarity and simplicity, we have chosen to highlight them here in the second edition. In particular, we now postpone all discussion of the backward view and of eligibility traces until Chapter 12. 7.1–2 The results in the random walk examples were made for this text based on work of Sutton  and Singh and Sutton . The use of backup diagrams to describe these and other algorithms in this chapter is new',\n",
       " '5a381a79-cd59-43db-906f-9f9733203fc4': 'This has the same column space as a 2 X 1 matrix containing only one copy. of the replicated column. In other words, the column space is still Just a line and fails to  encompass all of R?, even though there are two columns. Formally, this kind of redundancy is known as linear dependence. A set of vectors is linearly independent if no vector in the set is a linear combination of the other vectors. If we add a vector to a set that is a linear combination of the other vectors in the set, the new vector does not add any points to the set’s span. This means that for the column space of the matrix to encompass all of R”, the matrix must contain at least one set of m linearly independent columns. This condition is both necessary and sufficient for equation 2.11 to have a solution for every value of b. Note that the requirement is for a set to have exactly m linearly independent columns, not at least m',\n",
       " '085b447d-8a5f-42f9-8559-3267a7d33291': 'Dougal J Sutherland, Hsiao-Yu Tung, Heiko Strathmann, Soumyajit De, Aaditya Ramdas, Alex Smola, and Arthur Gretton. Generative models and model criticism via optimized maximum mean discrepancy. In International Conference on Learning Representations, 2017. Under review. T. Tieleman and G. Hinton. Lecture 6.5—RmsProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012. C´edric Villani. Optimal Transport: Old and New. Grundlehren der mathematischen Wissenschaften. Springer, Berlin, 2009. Fisher Yu, Yinda Zhang, Shuran Song, Ari Seﬀ, and Jianxiong Xiao. LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop. Corr, abs/1506.03365, 2015. We now introduce our notation',\n",
       " 'b440ff5d-a199-4dec-b589-5d04a3699b31': 'For example, if we use a two-dimensional image J as our input, we probably also want to use a two-dimensional kernel K:  S(i,j) = (I* K)(i, j) = I(m,n)K(i—m,j —n). (9.4)  mn  Convolution is commutative, meaiyngywe can equivalently write  S(i,j) = (K «I(t = SST 1 i—m,j —n)K(m,n). (9.5)  https://www.deeplearningbook.org/contents/convnets.html    Usually the latter formula is more straightforward to implement in a machine learning library, because there is less variation in the range of valid values of ™ and n.  The commutative property of convolution arises because we have flipped the kernel relative to the input, in the sense that as m increases, the index into the input increases, but the index into the kernel decreases. The only reason to flip the kernel is to obtain the commutative property. While the commutative property  328  CHAPTER 9',\n",
       " '762a0926-6569-40d0-bce1-f5782e2e38e4': 'In this essay, we use techniques such as the photochemical transfer military: arms defense battalion battalion cavalry This essay discusses three main themes:\\\\n\\\\n 1) Lack of uniformed soldiers is an unacceptable and unconscionable strategy for the Army.\\\\n 2) Poor and inadequate training does not compensate the soldiers, and may deprive them of the necessary and competitive training from their instructors legal: there liable injunction In summary, the court decided that defendants had a right to petition the high court to intervene, and therefore granted the injunction.\\n\\nHowever, a Court of Appeal decision in the US District Court ruled with no effect on the petitioned by the plaintiffs, citing two reasons.\\\\n\\\\n The US politics: the primary referendum was In summary, the majority of Russians would support the idea of banning all human rights, and that would be a major part of the government’s effort to build a new liberal economy, which President Vladimir Putin says would boost Russian tourism.\\\\n\\\\n The results were published in the computers: the macintoshintosh In summary, the kernel has the best quality of life, and kernel security is at the heart of any OS development. The ﬁrst time Linux released was when i386 released the Macintosh version, which had a rather low standard',\n",
       " 'b875c196-7059-4c4c-8662-893c3571ec19': 'The greedy policy takes the action that looks best in the short term—after one step of lookahead—according to v⇡. By construction, the greedy policy meets the conditions of the policy improvement theorem (4.7), so we know that it is as good as, or better than, the original policy. The process of making a new policy that improves on an original policy, by making it greedy with respect to the value function of the original policy, is called policy improvement. Suppose the new greedy policy, ⇡0, is as good as, but not better than, the old policy ⇡. Then v⇡ = v⇡0, and from (4.9) it follows that for all s 2 S: But this is the same as the Bellman optimality equation (4.1), and therefore, v⇡0 must be v⇤, and both ⇡ and ⇡0 must be optimal policies. Policy improvement thus must give us a strictly better policy except when the original policy is already optimal. So far in this section we have considered the special case of deterministic policies.\\n\\nIn the general case, a stochastic policy ⇡ speciﬁes probabilities, ⇡(a|s), for taking each action, a, in each state, s',\n",
       " '6858d19c-12b8-404b-8398-371c709b7a71': 'The divergence function can have a variety of choices, ranging from the family of f-divergence (e.g., KL divergence), or Bregman divergence, to optimal transport distance (e.g., Wasserstein distance), and so on. We discuss the divergence term in Section 5 in more detail. Uncertainty function. The uncertainty function H(q) describes the uncertainty of the auxiliary distribution q and thus controls the complexity of the learning system. It conforms with the maximum entropy principle discussed in Section 2 that one should pick the most uncertain solution among those that ﬁt all experience. Like other components in SE, the uncertainty measure H(·) can take diﬀerent forms, such as the popular Shannon entropy, as well as other generalized ones such as Tsallis entropy. In this article, we assume Shannon entropy by default. For the discussion in the following sections, it is often convenient to consider a special case of the SE in Equation 3.1. Speciﬁcally, we assume a common choice of the penalty U(ξ) = � which can be easily seen by optimizing Equation 3.1 over ξ',\n",
       " 'c27747ae-6ccf-433a-9def-b2d3c449e2a5': 'Show that the solution for the components an of the vector a can be expressed as a linear combination of the elements of the vector φ(xn).\\n\\nDenoting these coefﬁcients by the vector w, show that the dual of the dual formulation is given by the original representation in terms of the parameter vector w. 6.2 (⋆ ⋆) In this exercise, we develop a dual formulation of the perceptron learning algorithm. Using the perceptron learning rule (4.55), show that the learned weight vector w can be written as a linear combination of the vectors tnφ(xn) where tn ∈ {−1, +1}. Denote the coefﬁcients of this linear combination by αn and derive a formulation of the perceptron learning algorithm, and the predictive function for the perceptron, in terms of the αn. Show that the feature vector φ(x) enters only in the form of the kernel function k(x, x′) = φ(x)Tφ(x′)',\n",
       " '6da64f31-7f24-4103-a5da-afc6c6ce4438': \"For sequence prediction tasks, rather than modeling the future observations pr(Xtrk|Ce) directly (which could be fairly expensive), CPC models a density function to preserve the mutual information between x¢;% and cz:  D(X 4|C%)  + = WwW Fe (Xt+ks Ce) exp(Zi+4 Kez) OC D(Xt.6)  https://lilianweng.github.io/posts/2021-05-31-contrastive/  Contrastive Representation Learning | Lil'Log   where Z;,% is the encoded input and W, is a trainable weight matrix.\\n\\nSoft-Nearest Neighbors Loss  Soft-Nearest Neighbors Loss  extends it to include multiple positive samples\",\n",
       " '2a37aeab-9c80-47fc-8540-adf17d03f8f8': \"If the model were Gaussian, then these interactions could be modeled efficiently via the covariance matrix, but the sparse prior makes these interactions non-Gaussian. Because p( ) is intractable, so is the computation of the log-likelihood and  hla  https://www.deeplearningbook.org/contents/inference.html    its gradient. We thus cannot use exact maximum likelihood learning.\\n\\nInstead, we use MAP inference and learn the parameters by maximizing the ELBO defined by the Dirac distribution around the MAP estimate of h.  If we concatenate all the h vectors in the training set into a matrix H, and concatenate all the v vectors into a matrix V, then the sparse coding learning process consists of minimizing  = i|+o(v-aw') (19.16) a . tJ i,j a  Most applications of sparse coding also involve weight decay or a constraint on the norms of the columns of W, to prevent the pathological solution with extremely small H and large W.  We can minimize J by alternating between minimization with respect to H and minimization with respect to W. Both subproblems are convex\",\n",
       " '847fbab5-7550-488f-b7af-9e3f4482f7e9': 'this will in gencr~1 not be flO\\'slble, To see thl\\', OOIe Ihat the mapping 4\\'(x) maps the D-dimensional x space i\"t\" 0 D-dimensioo.l manijQiII in lhe M-dimemioo.l femure space <1>. TlIe: . \\'ector x i\\' koown a< lhe f\\'\",.imagr of lhe c\",\"\"\"ponding poi\"l 4\\'(x). However, fhe projec1ioo of poinl> in feature <J\\'3C\" \"\"to the linear rcA ,ub,p\"\"\" in that \\'pace will typically\"\\'\\'\\' lie On fhe nonlinear Ddimensional manifold and !iO will nul ha.,. a c\"\"\"\",pondlng p\",.lmo~ein dOlO spa<',\n",
       " '27165a61-6159-4359-af43-2686b2c9c643': 'This indicates that the algorithms have converged long before the end of the run for all ↵ values, since we do not see any effect of the initial learning phase. For Sarsa the performance comes close to the performance of Expected Sarsa only for ↵ = 0.1, while for large ↵, the performance for n = 100, 000 even drops below the performance for n = 100. The reason is that for large values of ↵ the Q values of Sarsa diverge. Although the policy is still improved over the initial random policy during the early stages of learning, divergence causes the policy to get worse in the long run. for n = 100 and n = 100, 000 using an ϵ-greedy policy with ϵ = 0.1. The big dots indicate the maximal values. We turn to the windy grid world task to further test Hypothesis 2. The windy grid world task is another navigation task, where the agent has to ﬁnd its way from start to goal. over Sarsa over a wide range of values for the step-size parameter ↵. In cli↵ walking the state transitions are all deterministic and all randomness comes from the policy',\n",
       " '4dd49dd8-7039-4890-b852-7209bfdb8cb0': 'Extend this result to the case of multiple outputs. where y(x, w) is a parametric function such as a neural network. The result (1.89) shows that the function y(x, w) that minimizes this error is given by the conditional expectation of t given x. Use this result to show that the second derivative of E with respect to two elements wr and ws of the vector w, is given by Note that, for a ﬁnite sample from p(x), we obtain (5.84). 5.18 (⋆) Consider a two-layer network of the form shown in Figure 5.1 with the addition of extra parameters corresponding to skip-layer connections that go directly from the inputs to the outputs. By extending the discussion of Section 5.3.2, write down the equations for the derivatives of the error function with respect to these additional parameters',\n",
       " '21ba0d7a-6bbb-4738-805e-e941ca1849bc': 'To make a more radical departure from the feedforward networks we have seen previously, we can also generalize the notion of an encoding function f(a) to an encoding distribution peycoder(h | x), as illustrated in figure 14.2. Any latent variable model pyodel(h, «) defines a stochastic encoder  Pencoder (Rh | x) = Pmodel (A | x) (14.12)  and a stochastic decoder  Paecoder(® | h) = Pmodel (© | h).\\n\\n(14.13)  In general, the encoder and decoder distributions are not necessarily conditional distributions compatible with a unique joint distribution pmodel(@,h). Alain ef al. showed that training the encoder and decoder as a denoising autoencoder will tend to make them compatible asymptotically (with enough capacity and examples). 14.5 Denoising Autoencoders  The denoising autoencoder (DAE) is an autoencoder that receives a corrupted data point as input and is trained to predict the original, uncorrupted data point as its output',\n",
       " '6b0f930f-1426-4653-bfa3-f83272ffd900': 'By using the standard results for the mean and variance of the gamma distribution given by (B.27) and (B.28), show that if we let N → ∞, this variational posterior distribution has a mean given by the inverse of the maximum likelihood estimator for the variance of the data, and a variance that goes to zero.\\n\\n10.9 (⋆ ⋆) By making use of the standard result E = aN/bN for the mean of a gamma distribution, together with (10.26), (10.27), (10.29), and (10.30), derive the result (10.33) for the reciprocal of the expected precision in the factorized variational treatment of a univariate Gaussian. 10.10 (⋆) www Derive the decomposition given by (10.34) that is used to ﬁnd approximate posterior distributions over models using variational inference. 10.12 (⋆ ⋆) Starting from the joint distribution (10.41), and applying the general result (10.9), show that the optimal variational distribution q⋆(Z) over the latent variables for the Bayesian mixture of Gaussians is given by (10.48) by verifying the steps given in the text',\n",
       " '87e64c79-dabb-4042-8d2b-83528614537a': '12.2.1.1 Contrast Normalization  One of the most obvious sources of variation that can be safely removed for many tasks is the amount of contrast in the image. Contrast simply refers to the magnitude of the difference between the bright and the dark pixels in an image. There are many ways of quantifying the contrast of an image. In the context of deep learning, contrast usually refers to the standard deviation of the pixels in an image or region of an image. Suppose we have an image represented by a tensor XE R™ 3, with Xj,j,1 being the red intensity at row i, and column j, X;,;,2 giving the green intensity, and X;,;,3 giving the blue intensity.\\n\\nThen the contrast of the entire image is given by  roc 3  LLd (Xi,je — X)”, (12.1)  i=1 j=1 k=1  where X is the mean intensity of the entire image: < 1 x= EYE xin 022)  Global contrast normalization (GCN) aims to prevent images from having varying amounts of contrast by subtracting the mean from each image, then rescaling it so that the standard deviation across its pixels is equal to some constant s',\n",
       " 'f55e81e1-87ae-43ef-8869-7acac272e1ec': 'These evaluation methods complemented one another: the value network evaluated the high-performance RL policy that was too slow to be used in live play, while rollouts using the weaker but much faster rollout policy were able to add precision to the value network’s evaluations for speciﬁc states that occurred during games.\\n\\nOverall, AlphaGo’s remarkable success fueled a new round of enthusiasm for the promise of artiﬁcial intelligence, speciﬁcally for systems combining reinforcement learning with deep ANNs, to address problems in other challenging domains. Building upon the experience with AlphaGo, a DeepMind team developed AlphaGo Zero . In contrast to AlphaGo, this program used no human data or guidance beyond the basic rules of the game (hence the Zero in its name). It learned exclusively from self-play reinforcement learning, with input giving just “raw” descriptions of the placements of stones on the Go board. AlphaGo Zero implemented a form of policy iteration (Section 4.3), interleaving policy evaluation with policy improvement. Figure 16.7 is an overview of AlphaGo Zero’s algorithm',\n",
       " '92cfc189-97ef-4de4-8eb2-6b8ee6cc2046': 'Each feature map in a subsampling layer consists of units that average over a receptive ﬁeld of units in the feature maps of the preceding convolutional layer. For example, each unit in each of the 6 feature maps in the ﬁrst subsampling layer of the network of Figure 9.15 averages over a 2 ⇥ 2 non-overlapping receptive ﬁeld over one of the feature maps produced by the ﬁrst convolutional layer, resulting in six 14 ⇥ 14 feature maps. Subsampling layers reduce the network’s sensitivity to the spatial locations of the features detected, that is, they help make the network’s responses spatially invariant. This is useful because a feature detected at one place in an image is likely to be useful at other places as well. Advances in the design and training of ANNs—of which we have only mentioned a few—all contribute to reinforcement learning.\\n\\nAlthough current reinforcement learning theory is mostly limited to methods using tabular or linear function approximation methods, the impressive performances of notable reinforcement learning applications owe much of their success to nonlinear function approximation by multi-layer ANNs. We discuss several of these applications in Chapter 16',\n",
       " 'e92fd635-071a-47c0-8471-87d7ddd769e6': 'And gradually all the other non-successful impulses will be stamped out and the particular impulse leading to the successful act will be stamped in by the resulting pleasure, until, after many trials, the cat will, when put in the box, immediately claw the button or loop in a deﬁnite way. (Thorndike 1898, p. 13) These and other experiments (some with dogs, chicks, monkeys, and even ﬁsh) led Thorndike to formulate a number of “laws” of learning, the most inﬂuential being the Law of E↵ect, a version of which we quoted in Chapter 1 (page 15). This law describes what is generally known as learning by trial and error. As mentioned in Chapter 1, many aspects of the Law of E↵ect have generated controversy, and its details have been modiﬁed over the years. Still the law—in one form or another—expresses an enduring principle of learning. Essential features of reinforcement learning algorithms correspond to features of animal learning described by the Law of E↵ect.\\n\\nFirst, reinforcement learning algorithms are selectional, meaning that they try alternatives and select among them by comparing their consequences',\n",
       " '35444eef-6e53-4648-b83b-9b622c2c4340': 'The oil ﬂow data set is generated using realistic known values for the absorption properties of oil, water, and gas at the two gamma energies used, and with a speciﬁc choice of integration time (10 seconds) chosen as characteristic of a typical practical setup. Each point in the data set is generated independently using the following steps: 1. Choose one of the three phase conﬁgurations at random with equal probability. 2.\\n\\nChoose three random numbers f1, f2 and f3 from the uniform distribution over (0, 1) and deﬁne This treats the three phases on an equal footing and ensures that the volume fractions add to one. 3. For each of the six beam lines, calculate the effective path lengths through oil and water for the given phase conﬁguration. 4. Perturb the path lengths using the Poisson distribution based on the known beam intensities and integration time to allow for the effect of photon statistics. Each point in the data set comprises the 12 path length measurements, together with the fractions of oil and water and a binary label describing the phase conﬁguration. The data set is divided into training, validation, and test sets, each of which comprises 1, 000 independent data points. Details of the data format are available from the book web site',\n",
       " '98085820-3664-4644-98b4-204e9e30a57e': 'The actor eligibility trace vector z✓ t is a trace (average of recent values) of r ln ⇡(At|St, ✓). To understand this eligibility trace refer to Exercise 13.5, which deﬁnes this kind of unit and asks you to give a learning rule for it.\\n\\nThat exercise asked you to express r ln ⇡(a|s, ✓) in terms of a, x(s), and ⇡(a|s, ✓) (for arbitrary state s and action a) by calculating the gradient. For the action and state actually occurring at time t, the answer is Unlike the non-contingent eligibility trace of a critic synapse that only accumulates the presynaptic activity x(St), the eligibility trace of an actor unit’s synapse in addition depends on the activity of the actor unit itself. We call this a contingent eligibility trace because it is contingent on this postsynaptic activity. The eligibility trace at each synapse continually decays, but increments or decrements depending on the activity of the presynaptic neuron and whether or not the postsynaptic neuron ﬁres',\n",
       " '508c6dcb-c5e3-4da2-a6c8-a5e9b0c41dbc': 'The z-axis is the coordinate of the initial state along a random direction in the 100-dimensional space. We can thus view this plot as a linear cross-section of a high-dimensional function. The plots show the function after each time step, or equivalently, after each number of times the transition function has been composed. 397  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  and lacking inputs a. As described in section 8.2.5, this recurrence relation essentially describes the power method. It may be simplified to  nd = (Ww)! pn, (10.37 and if W admits an eigendecomposition of the form W =QAQ’, (10.38  with orthogonal Q, the recurrence may be simplified further to  hn =Q’aAtQn®. (10.39  The eigenvalues are raised to the power of t, causing eigenvalues with magnitude less than one to decay to zero and eigenvalues with magnitude greater than one to explode',\n",
       " 'cbb09228-9e12-402f-83eb-18b2f9282e74': 'Actively supervised data instances.\\n\\nInstead of access to data instances x∗ with readily available labels y∗, in the active supervision setting, we are presented with a large pool of unlabeled instances D = {x∗} as well as a certain budget for querying an oracle (e.g., human annotators) for labeling a limited set of instances. To minimize the need for labeled instances, we need to strategically select queries from the pool according to an informativeness measure u(x) ∈ R. For example, u(x) can be the predictive uncertainty on the instance x, quantiﬁed by the Shannon entropy of the predictive distribution or the vote entropy based on a committee of predictors . Mapping the standard equation to this setting, we show the informativeness measure u(x) is subsumed as part of the experience. Intuitively, u(x) encodes our heuristic belief about sample ‘informativeness’. This heuristic is a form of information we inject into the learning system. Denote the oracle as o from which we can draw a label y∗ ∼ o(x∗)',\n",
       " '3d9f6035-e1db-47e5-9c36-e683f18f0dde': \"The tasks can be any well-defined family of machine learning problems: supervised learning, reinforcement learning, etc. For example, here are a couple concrete meta-learning tasks:  A classifier trained on non-cat images can tell whether a given image contains a cat after seeing a handful of cat pictures.\\n\\nA game bot is able to quickly master a new game. A mini robot completes the desired task on an uphill surface during test even through it was only trained in a flat surface environment. Define the Meta-Learning Problem  https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Log   In this post, we focus on the case when each desired task is a supervised learning problem like image classification. There is a lot of interesting literature on meta-learning with reinforcement learning problems (aka “Meta Reinforcement Learning”), but we would not cover them here. A Simple View  A good meta-learning model should be trained over a variety of learning tasks and optimized for the best performance on a distribution of tasks, including potentially unseen tasks. Each task is  associated with a dataset D, containing both feature vectors and true labels\",\n",
       " '1ff9cf74-fd75-4e4b-a145-1050d1011810': 'This representation allows the TD error to mimic the fact that dopamine neuron activity not only predicts a future reward, but that it is also sensitive to when after a predictive cue that reward is expected to arrive. There has to be some way to keep track of the time between sensory cues and the arrival of reward. If a stimulus initiates a sequence of internal signals that continues after the stimulus ends, and if there is a di↵erent signal for each time step following the stimulus, then each time step after the stimulus is represented by a distinct state. Thus, the TD error, being state-dependent, can be sensitive to the timing of events within a trial',\n",
       " '1cf48f04-635c-49b8-bff4-afd73f037172': 'By a shallow transformation, we mean a transformation that would be represented by a single layer within a deep MLP. Typically this is a transformation represented by a learned affine  c i eu WW ° 1 .o  https://www.deeplearningbook.org/contents/rnn.html    ULrallslormlatlou LOLLIOWEd DY a UxXed LOMINeAarity. Would it be advantageous to introduce depth in each of these operations? Experimental evidence  strongly suggests so. The experimental evidence is in agreement with the idea that we need enough  393  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  depth to perform the required mappings.\\n\\nSee also Schmidhuber , El Hihi and Bengio , or Jaeger  for earlier work on deep RNNs. Graves et al. were the first to show a significant benefit of decomposing the state of an RNN into multiple layers, as in figure 10.13 (left)',\n",
       " '3c863b1b-4c94-48b5-a589-944c845a9340': 'The naive algorithm could have exponential runtime due to these repeated subexpressions.\\n\\nNow that we have specified the back-propagation algorithm, we can understand its computational cost. If we assume that each operation evaluation has roughly the  212  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  https://www.deeplearningbook.org/contents/mlp.html    Algorithm 6.5 The outermost skeleton of the back-propagation algorithm. This portion does simple setup and cleanup work. Most of the important work happens  in the build_grad subroutine of algorithm 6.6  Require: T, the target set of variables whose gradients must be computed. Require: G, the computational graph Require: z, the variable to be differentiated Let G’ be G pruned to contain only nodes that are ancestors of z and descendents of nodes in T',\n",
       " '8a592813-f0c2-46bd-8177-3ba3b1643a93': 'We see that the weight parameters in the ﬁrst layer of the network are shared between the various outputs, whereas in the linear model each classiﬁcation problem is solved independently. The ﬁrst layer of the network can be viewed as performing a nonlinear feature extraction, and the sharing of features between the different outputs can save on computation and can also lead to improved generalization. Finally, we consider the standard multiclass classiﬁcation problem in which each input is assigned to one of K mutually exclusive classes.\\n\\nThe binary target variables tk ∈ {0, 1} have a 1-of-K coding scheme indicating the class, and the network outputs are interpreted as yk(x, w) = p(tk = 1|x), leading to the following error function Following the discussion of Section 4.3.4, we see that the output unit activation function, which corresponds to the canonical link, is given by the softmax function k yk = 1. Note that the yk(x, w) are unchanged if a constant is added to all of the ak(x, w), causing the error function to be constant for some directions in weight space. This degeneracy is removed if an appropriate regularization term (Section 5.5) is added to the error function',\n",
       " '2e916cfe-a02f-4905-b66d-f7f3ac964787': 'However, we can ﬁrst seek invertible transformations either of the function or of its argument which change it into a convex form. We then calculate the conjugate function and then transform back to the original variables. An important example, which arises frequently in pattern recognition, is the logistic sigmoid function deﬁned by As it stands this function is neither convex nor concave. However, if we take the logarithm we obtain a function which is concave, as is easily veriﬁed by ﬁnding the second derivative. From (10.133) the corresponding conjugate function then takes Exercise 10.30 which we recognize as the binary entropy function for a variable whose probability of having the value 1 is λ.\\n\\nUsing (10.132), we then obtain an upper bound on the log Appendix B with two examples of the exponential upper bound (10.137) shown in blue. The right-hand plot shows the logistic sigmoid again in red together with the Gaussian lower bound (10.144) shown in blue. Here the parameter ξ = 2.5, and the bound is exact at x = ξ and x = −ξ, denoted by the dashed green lines',\n",
       " 'fdc73231-0a5b-4fcf-8cdf-4e12648e447c': 'A Gaussian mixture output with n components is defined by the conditional probability distribution:  ply |x) = Yo rle= il Ny; nw? (x), B® (a). (6.35)  The neural network must have three outputs: a vector defining p(c =i| x), a matrix providing (a) for all i, and a tensor providing © (a) for all i. These outputs must satisfy different constraints:  185  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  1. Mixture components p(c = i | x): these form a multinoulli distribution over the n different components associated with latent variable! c, and can typically be obtained by a softmax over an n-dimensional vector, to guarantee that these outputs are positive and sum to 1. 2. Means pa): these indicate the center or mean associated with the i-th  https://www.deeplearningbook.org/contents/mlp.html    Gaussian component and are unconstrained (typically with no nonlinearity at all for these output units)',\n",
       " 'ab3c2c39-d8e3-42dd-9753-a5213a24a2f4': '(In the undiscounted episodic case, it is possible that there are some orderings of updates that do not result in convergence, but it is relatively easy to avoid these.) Similarly, it is possible to intermix policy evaluation and value iteration updates to produce a kind of asynchronous truncated policy iteration. Although the details of this and other more unusual DP algorithms are beyond the scope of this book, it is clear that a few di↵erent updates form building blocks that can be used ﬂexibly in a wide variety of sweepless DP algorithms. Of course, avoiding sweeps does not necessarily mean that we can get away with less computation. It just means that an algorithm does not need to get locked into any hopelessly long sweep before it can make progress improving a policy.\\n\\nWe can try to take advantage of this ﬂexibility by selecting the states to which we apply updates so as to improve the algorithm’s rate of progress. We can try to order the updates to let value information propagate from state to state in an eﬃcient way. Some states may not need their values updated as often as others. We might even try to skip updating some states entirely if they are not relevant to optimal behavior. Some ideas for doing this are discussed in Chapter 8',\n",
       " '63efb4ac-41ee-4e21-b0e5-f06a6f53d11d': 'The computational cost of this algorithm is proportional to the number of edges in  https://www.deeplearningbook.org/contents/mlp.html    the graph, een that the partial derivative associated with each edge requires a constant time. This is of t he ssagne order as the number of corp atten for the forward propagation. Each 9,@ is a function of the parents u~’ of u’’, thus linking the nodes of the forward graph to those added for the back-propagation  graph.\\n\\nRun forward propagation (algorithm 6.1 for this example) to obtain the activa- tions of the network. Initialize grad_table, a data structure that will store the derivatives that have  been computed. The entry grad_table — 1 for j = n—1 down to 1 do  The next line computes 4 Su = yijePa(ul) bury Burs using stored values:  iy) du  grad_table — Tejepaquoy erad_table 200 end for return {grad_table | i =1,..., ni}  207  CHAPTER 6',\n",
       " 'b62a1980-565f-4696-9932-639fc7437af9': 'A neuron’s background activity is its level of activity, usually its ﬁring rate, when the neuron does not appear to be driven by synaptic input related to the task of interest to the experimenter, for example, when the neuron’s activity is not correlated with a stimulus delivered to a subject as part of an experiment. Background activity can be irregular due to input from the wider network, or due to noise within the neuron or its synapses. Sometimes background activity is the result of dynamic processes intrinsic to the neuron. A neuron’s phasic activity, in contrast to its background activity, consists of bursts of spiking activity usually caused by synaptic input. Activity that varies slowly and often in a graded manner, whether as background activity or not, is called a neuron’s tonic activity',\n",
       " '224ee220-7ff7-483a-af4a-060555896040': 'The RNN in this figure is trained to put a specific output value into o, and o is the only information it is allowed to send to the future. There are no direct connections from h going forward. The previous h is connected to the present only indirectly, via the predictions it was used to produce. Unless o is very high-dimensional and rich, it will usually lack important information from the past. This makes the RNN in this figure less powerful, but it may be easier to train because each time step can be trained in isolation from the others, allowing greater parallelization during training, as described in section 10.2.1. 375  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  MN L IN.\\n\\nDNL O™..  https://www.deeplearningbook.org/contents/rnn.html    1(T)  Figure 10.5: Time-unfolded recurrent neural network with a single output at the end of the sequence. Such a network can be used to summarize a sequence and produce a fixed-size representation used as input for further processing',\n",
       " 'e5513a02-13e7-4ceb-8100-5181a4e1f286': 'Proper resolution of this issue awaits a more thorough understanding of the theory of function approximation in reinforcement learning.\\n\\nIn the multi-step generalizations of these algorithms, both the state-value and actionwt+n .= wt+n−1 + ↵⇢t+1 · · · ⇢t+n−1  rˆq(St, At, wt+n−1) Gt:t+n .= Rt+1 − ¯Rt + · · · + Rt+n − ¯Rt+n−1 + ˆq(St+n, At+n, wt+n−1), (continuing) where here we are being slightly informal in our treatment of the ends of episodes. In the ﬁrst equation, the ⇢ks for k ≥ T (where T is the last time step of the episode) should be taken to be 1, and Gt:n should be taken to be Gt if t + n ≥ T. Recall that we also presented in Chapter 7 an o↵-policy algorithm that does not involve with δt as deﬁned at the top of this page for Expected Sarsa',\n",
       " '72202496-bc68-41eb-a0a6-54de87c8bd29': 'LSTD is the most data-eﬃcient linear TD prediction method, but requires computation proportional to the square of the number of weights, whereas all the other methods are of complexity linear in the number of weights. Nonlinear methods include artiﬁcial neural networks trained by backpropagation and variations of SGD; these methods have become very popular in recent years under the name deep reinforcement learning. Linear semi-gradient n-step TD is guaranteed to converge under standard conditions, for all n, to a VE that is within a bound of the optimal error (achieved asymptotically by Monte Carlo methods). This bound is always tighter for higher n and approaches zero as n ! 1. However, in practice very high n results in very slow learning, and some degree of bootstrapping (n < 1) is usually preferrable, just as we saw in comparisons of tabular n-step methods in Chapter 7 and in comparisons of tabular TD and Monte Carlo methods in Chapter 6.\\n\\nGeneralization and function approximation have always been an integral part of reinforcement learning. Bertsekas and Tsitsiklis , Bertsekas , and Sugiyama et al',\n",
       " '8fac79c2-45f8-43e6-afa0-36b968e995d5': '(In reality, Alice’s performance probably influences Bob’s performance—depending on Bob’s personality, if Alice runs especially fast in a given race, this might encourage Bob to push hard and match her exceptional performance, or it might make him overconfident and lazy). Then the only effect Alice has on Bob’s finishing time is that we must add Alice’s finishing time to the total amount of time we think Bob needs to run. This observation allows us to define a model with O(k) parameters instead of O(k?). However, note that tg and t; are still directly dependent with this assumption, because t; represents the absolute time at which Bob finishes, not the total time he spends running. This means our graph must still contain an arrow from to to ti. The assumption that Bob’s personal running time is independent from all other factors cannot be encoded in a graph over to, t1, and tz. Instead, we encode this information in the definition of the conditional distribution itself',\n",
       " 'b801ae54-b07f-4af4-a3f1-2dfe0faaadf1': 'Within each row, the arithmetic is element-wise, so H;; is normalized by subtracting 1; and dividing by o;. The rest of the network then operates on H’ in exactly the same way that the original network operated on H.  At training time,  w= SH, (8.36)  and  o= 5+ Hn );, (8.37)  where 6 is a small positive value such as 10~8, imposed to avoid encountering the undefined gradient of \\\\/z at z = 0. Crucially, we back-propagate through these operations for computing the mean and the standard deviation, and for applying them to normalize H. This means that the gradient will never propose an operation that acts simply to increase the standard deviation or mean of  he a1 a at at ad roo c 1 at 1  https://www.deeplearningbook.org/contents/optimization.html    ree LUE HOLMAN Zavlou OPeLrabllOlUs LEMLOVE LIE CLLECL OL SUCLIL all ACLIOLI ALLU ZELO out its component in the gradient.\\n\\nThis was a major innovation of the batch normalization approach',\n",
       " '376b97aa-9354-4122-87a4-584ff3d6fe4a': 'We say that the earlier states are given less credit for the TD error. If λ = 1, then the credit given to earlier states falls only by γ per step. This turns out to be just the right thing to do to achieve Monte Carlo behavior. For example, remember that the TD error, δt, includes an undiscounted term of Rt+1. In passing this back k steps it needs to be discounted, like any reward in a return, by γk, which is just what the falling eligibility trace achieves. If λ = 1 and γ = 1, then the eligibility traces do not decay at all with time. In this case the method behaves like a Monte Carlo method for an undiscounted, episodic task. If λ = 1, the algorithm is also known as TD(1).\\n\\nTD(1) is a way of implementing Monte Carlo algorithms that is more general than those presented earlier and that signiﬁcantly increases their range of applicability. Whereas the earlier Monte Carlo methods were limited to episodic tasks, TD(1) can be applied to discounted continuing tasks as well. Moreover, TD(1) can be performed incrementally and online',\n",
       " '8f41f0bb-72f2-4473-aff8-5a15587c0639': 'Once an action is taken, the environment delivers a reward (r € 7) as feedback. The model defines the reward function and transition probabilities. We may or may not know how the model works and this differentiate two circumstances:  Know the model: planning with perfect information; do model-based RL. When we fully know the environment, we can find the optimal solution by Dynamic Programming (DP). Do you still remember “longest increasing subsequence” or “traveling salesmen problem\" from your Algorithms 101 class? LOL. This is not the focus of this post though.\\n\\nDoes not know the model: learning with incomplete information; do model-free RL or try to learn the model explicitly as part of the algorithm. Most of the following content serves the scenarios when the model is unknown. The agent\\'s policy 7(s) provides the guideline on what is the optimal action to take in a certain state with the goal to maximize the total rewards',\n",
       " 'e42c755e-e82a-4214-bb57-ece91f24bedd': 'It is convenient to consider low-level actions to be special cases of options—each action a corresponds to an option h⇡!, γ!i whose policy picks the action (⇡! (s)=a for all s 2 S) and whose termination function is zero (γ! (s) = 0 for all s 2 S+). Options e↵ectively extend the action space. The agent can either select a low-level action/option, terminating after one time step, or select an extended option that might execute for many time steps before terminating. Options are designed so that they are interchangable with low-level actions. For example, the notion of an action-value function q⇡ naturally generalizes to an optionvalue function that takes a state and option as input and returns the expected return starting from that state, executing that option to termination, and thereafter following the policy, ⇡.\\n\\nWe can also generalize the notion of policy to a hierarchical policy that selects from options rather than actions, where options, when selected, execute until termination. With these ideas, many of the algorithms in this book can be generalized to learn approximate option-value functions and hierarchical policies',\n",
       " 'da337795-ff23-48a9-8720-baa6f6534249': 'SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  https://www.deeplearningbook.org/contents/rnn.html    Figure 10.11: Computation of a typical bidirectional recurrent neural network, meant to learn to map input sequences « to target sequences y, with loss L® at each step t The h recurrence propagates information forward in time (toward the right), while theg recurrence propagates information backward in time (toward the left). Thus at each point t, the output units o”) can benefit from a relevant summary of the past in its ne) input and from a relevant summary of the future in its g”) input. 389  CHAPTER 10',\n",
       " 'c84b0714-251b-4be0-802f-e0b2652da8cf': 'In this sense, DP is exponentially faster than any direct search in policy space could be, because direct search would have to exhaustively examine each policy to provide the same guarantee. Linear programming methods can also be used to solve MDPs, and in some cases their worst-case convergence guarantees are better than those of DP methods. But linear programming methods become impractical at a much smaller number of states than do DP methods (by a factor of about 100).\\n\\nFor the largest problems, only DP methods are feasible. DP is sometimes thought to be of limited applicability because of the curse of dimensionality, the fact that the number of states often grows exponentially with the number of state variables. Large state sets do create diﬃculties, but these are inherent diﬃculties of the problem, not of DP as a solution method. In fact, DP is comparatively better suited to handling large state spaces than competing methods such as direct search and linear programming. In practice, DP methods can be used with today’s computers to solve MDPs with millions of states. Both policy iteration and value iteration are widely used, and it is not clear which, if either, is better in general',\n",
       " 'ce30dec8-12eb-42a0-ad68-78224e95a839': 'That’s why we’re committed to working collaboratively with the broader Al community to achieve our goal of, one day, building machines with human-level  https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/  Self-supervised learning: The dark matter of intelligence   intelligence. Our research has been made publicly available and published at top conferences. And we’ve organized workshops and released libraries to help accelerate the research in this area.\\n\\nWritten By  Yann LeCun Ishan Misra VP and Chief Al Scientist Research Scientist  https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/  Self-supervised learning: The dark matter of intelligence  Who We Are  About Meta Al People Careers Events  Latest Work  Research Infrastructure Blog Resources  Our Actions  Responsibilities  Newsletter  Sign Up  Privacy Policy Terms Cookies  https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/    as @O000  Meta © 2023',\n",
       " '4a1830c4-598e-42de-8540-2870adf3875a': 'This may seem paradoxical because a polynomial of given order contains all lower order polynomials as special cases.\\n\\nThe M = 9 polynomial is therefore capable of generating results at least as good as the M = 3 polynomial. Furthermore, we might suppose that the best predictor of new data would be the function sin(2πx) from which the data was generated (and we shall see later that this is indeed the case). We know that a power series expansion of the function sin(2πx) contains terms of all orders, so we might expect that results should improve monotonically as we increase M. We can gain some insight into the problem by examining the values of the coefﬁcients w⋆ obtained from polynomials of various order, as shown in Table 1.1. We see that, as M increases, the magnitude of the coefﬁcients typically gets larger. In particular for the M = 9 polynomial, the coefﬁcients have become ﬁnely tuned to the data by developing large positive and negative values so that the correspondpolynomial for N = 15 data points (left plot) and N = 100 data points (right plot). We see that increasing the size of the data set reduces the over-ﬁtting problem',\n",
       " '9b628e02-e2b5-4a52-818f-d15a10c70cca': 'S¸im¸sek, ¨O., Alg´orta, S., Kothiyal, A. Why most decisions are easy in tetris—And perhaps in other sequential decision problems, as well. In Proceedings of the 33rd International Conference on Machine Learning , pp. 1757-1765. Simon, H. Lecture at the Earthware Symposium, Carnegie Mellon University. https://www.youtube.com/w Singh, S. P. Reinforcement learning with a hierarchy of abstract models. In Proceedings of the Tenth National Conference on Artiﬁcial Intelligence (AAAI-92), pp. 202–207. AAAI/MIT Press, Menlo Park, CA. Singh, S. P. Scaling reinforcement learning algorithms by learning variable temporal resolution models. In Proceedings of the 9th International Workshop on Machine Learning, pp. 406–415. Morgan Kaufmann. Singh, S. P. Learning to Solve Markovian Decision Processes. Ph.D',\n",
       " '05e57253-6b2f-494d-9c16-3ecac6599608': 'Pseudocode for the full algorithm is shown in the box below.\\n\\nInput: an arbitrary behavior policy b such that b(a|s) > 0, for all s 2 S, a 2 A Initialize Q(s, a) arbitrarily, for all s 2 S, a 2 A Initialize ⇡ to be greedy with respect to Q, or as a ﬁxed given policy Algorithm parameters: step size ↵ 2 (0, 1], a positive integer n All store and access operations (for St, At, and Rt) can take their index mod n + 1 The o↵-policy version of n-step Expected Sarsa would use the same update as above for n-step Sarsa except that the importance sampling ratio would have one less factor in it. That is, the above equation would use ⇢t+1:t+n−1 instead of ⇢t+1:t+n, and of course it would use the Expected Sarsa version of the n-step return (7.7). This is because in Expected Sarsa all possible actions are taken into account in the last state; the one actually taken has no e↵ect and does not have to be corrected for',\n",
       " 'b63cb3d9-bcc5-4089-9be4-ee340a4dcde8': 'The back-propagation algorithm is designed to reduce the number of common subexpressions without regard to memory. Specifically, it performs on the order of one Jacobian product per node in the graph.\\n\\nThis can be seen from the fact that backprop (algorithm 6.2) visits each edge from node uD to node u™ of the graph exactly once in order to obtain the associated partial derivative au Back-propagation thus avoids the exponential explosion in repeated subexpres- sions. Other algorithms may be able to avoid more subexpressions by performing simplifications on the computational graph, or may be able to conserve memory by recomputing rather than storing some subexpressions. We revisit these ideas  after describing the back-propagation algorithm itself. Algorithm 6.2 Simplified version of the back-propagation algorithm for computing the derivatives of u!™ with respect to the variables in the graph. This example is intended to further understanding by showing a simplified case where all variables are scalars, and we wish to compute the derivatives with respect to uw, Lee Un),  This simplified version computes the derivatives of all nodes in the graph',\n",
       " '53cdf9b6-08fe-45ab-8d87-deb15e6ebb1d': \"Thus, contrastive loss takes a pair of inputs (x;, x;) and minimizes the embedding distance when they are from the same class but maximizes the distance otherwise. Leont (is X51 9) = 1 max(0, € — || fo(%:) — fa(x,) Ilo)?\\n\\nwhere € is a hyperparameter, defining the lower bound distance between samples of different classes. Triplet Loss  Triplet loss was originally proposed in the FaceNet  paper and was used to learn face recognition of the same person at different poses and angles. https://lilianweng.github.io/posts/2021-05-31-contrastive/  Contrastive Representation Learning | Lil'Log  Negative  Anchor LEARNING Negative Anchor  Positive Positive  Given one anchor input x, we select one positive sample x* and one negative x~, meaning that xt and x belong to the same class and x” is sampled from another different class\",\n",
       " '8a57bfcd-7e23-442f-8e72-ee07649f4a0c': 'In the second column there are M - 2 independent parameters, because the column must be normalized and also must be orthogonal to the previous column, and so on. Summing this arithmetic series, we see that R has a total of M(M -1)/2 independent parameters. Thus the number of degrees of freedom in the covariance matrix C is given by DM + 1 - M(M - 1)/2. (12.51) The number of independent parameters in this model therefore only grows linearly with D, for fixed M. If we take M = D - 1, then we recover the standard result for a full covariance Gaussian. In this case, the variance along D - 1 linearly independent directions is controlled by the columns of W, and the variance along the remaining direction is given by a 2 . If M = 0, the model is equivalent to the isotropic covariance case. As we have seen, the probabilistic PCA model can be expressed in terms of a marginalization over a continuous latent space z in which for each data point X n , there is a corresponding latent variable Zn',\n",
       " 'ea03962c-3389-4f9e-837d-357096685842': 'The cartoon view of a complex cell is that it computes the L? norm of the 2-D vector containing two simple cells’ responses: c(I) = \\\\/so(I)? + 5,(J)?. An important special case occurs when 5; has all the same parameters as s9 except for ¢, and ¢ is set such that s1 is one quarter cycle out of phase with so. In this case, so and s; form a quadrature pair. A complex cell defined in this way responds when the Gaussian reweighted image I(a,y) exp(—,x”\" — Byy”) contains a high-amplitude sinusoidal wave with frequency f in direction 7 near (29, yo), regardless of the phase offset of this wave. In other words, the complex cell is invariant to small translations of the image in direction 7, or to negating the image (replacing black with white and vice versa).\\n\\nSome of the most striking correspondences between neuroscience and machine learning come from visually comparing the features learned by machine learning models with those employed by V1',\n",
       " 'e761a861-5a60-4e4c-9d87-9ffd2edeb5f1': \"25 Illustration of style and content reconstructions in Neural Style Transfer   Style Target porelut2 pbrelu2.2 pdrelu3.3 y6,relud.3  style “style style style ce fw LEBY) ----FF---FE-----f]------F fh;  oeeea  Input | H Image Tage Transform Net | ' Loss Network vee-1)| Gg! Fe1u3.3 Content Target efit i. Fig. 26 Illustration of the Fast neural style algorithm by Johnson et al. (35,  data into a night-to-day scale, winter-to-summer, or rainy-to-sunny scale. However, in other application domains, the set of styles to transfer into is not so obvious. For ease of implementation, data augmentation via Neural Style Transfer could be done by selecting a set of k styles and applying them to all images in the training set. The work of Style Augmentation , avoids introducing a new form of style bias into the dataset by deriving styles at random from a distribution of 79,433 artistic images\",\n",
       " '2337db9f-c290-424e-8d24-c7c652517d66': 'Snorkel’s deployments in industry, research laboratories, and government agencies show that it has real-world impact, offering developers an improved way to build models. Acknowledgements Alison Callahan and Nigam Shah of Stanford, and Nicholas Giori of the US Dept. of Veterans Affairs developed the electronic health records application. Emily Mallory, Ambika Acharya, and Russ Altman of Stanford, and Roselie Bright and Elaine Johanson of the US Food and Drug Administration developed the scientiﬁc articles application. Joy Ku of the Mobilize Center organized the user study. Nishith Khandwala developed the radiograph application. We thank the contributors to Snorkel including Bryan He, Theodoros Rekatsinas, and Braden Hancock. We gratefully acknowledge the support of DARPA under Nos. N66001-15-C-4043 (SIMPLEX), FA875017-2-0095 (D3M), FA8750-12-2-0335, and FA8750-13-2-0039, DOE 108845, NIH U54EB020405, ONR under Nos',\n",
       " '05d93cf3-4ada-45b0-8ea2-aac088492bad': 'Dynamic programming has been extensively developed since the late 1950s, including extensions to partially observable MDPs , many applications , approximation methods , and asynchronous methods . Many excellent modern treatments of dynamic programming are available . Bryson  provides an authoritative history of optimal control. Connections between optimal control and dynamic programming, on the one hand, and learning, on the other, were slow to be recognized.\\n\\nWe cannot be sure about what accounted for this separation, but its main cause was likely the separation between the disciplines involved and their di↵erent goals. Also contributing may have been the prevalent view of dynamic programming as an o✏ine computation depending essentially on accurate system models and analytic solutions to the Bellman equation. Further, the simplest form of dynamic programming is a computation that proceeds backwards in time, making it diﬃcult to see how it could be involved in a learning process that must proceed in a forward direction. Some of the earliest work in dynamic programming, such as that by Bellman and Dreyfus , might now be classiﬁed as following a learning approach. Witten’s  work (discussed below) certainly qualiﬁes as a combination of learning and dynamic-programming ideas',\n",
       " '2a31de3f-4fb9-459a-9108-df795bb406e6': 'For example, in natural images, pixels that are widely separated in space also have weak correlation, so the generalized pseudolikelihood  can be applied with each S set being a small, spatially localized window. One weakness of the pseudolikelihood estimator is that it cannot be used with other approximations that provide only a lower bound on p(x), such as variational inference, which is covered in chapter 19. This is because p appears  614  https://www.deeplearningbook.org/contents/partition.html    CHAPTER 18, CONFRONTING THE PARTITION FUNCTION  in the denominator. A lower bound on the denominator provides only an upper bound on the expression as a whole, and there is no benefit to maximizing an upper bound. This makes it difficult to apply pseudolikelihood approaches to deep models such as deep Boltzmann machines, since variational methods are one of the dominant approaches to approximately marginalizing out the many layers of hidden variables that interact with each other.\\n\\nNonetheless, pseudolikelihood is still useful for deep learning, because it can be used to train single-layer models or deep models using approximate inference methods that are not based on lower bounds',\n",
       " 'c7737d9e-9d38-451c-ac63-4e44cf497a88': 'If we were to draw examples in order from this list, then each of our minibatches would  LA nvetannen alee LinnnA Lananeenn 14 wee nanan 4 wwe nntle Ann RAti nnd Ae04 Af ELA  https://www.deeplearningbook.org/contents/optimization.html    vo CAULLTLUCLY VilascuUu, VOCAUSE LL WUULU LEOPLESCLL pililllaliry VLLO PaviTilt UUL VIL LLCO many patients in the dataset. In cases such as these, where the order of the dataset holds some significance, it is necessary to shufle the examples before selecting minibatches. B or very large datasets, for example, datasets containing billions o  examples in a data center, it can be impractical to sample examples truly uniformly at random every time we want to construct a minibatch',\n",
       " 'ab6b33dd-d852-4a0e-a8bf-fe330c769622': 'These additional limitations, such as the imperfection of the optimization algorithm, mean that the learning algorithm’s effective capacity may be less than the representational capacity of the model family. Our modern ideas about improving the generalization of machine learning models are refinements of thought dating back to philosophers at least as early as Ptolemy. Many early scholars invoke a principle of parsimony that is now most widely known as Occam’s razor . This principle states that among competing hypotheses that explain known observations equally well, we should choose the “simplest” one. This idea was formalized and made more precise in the twentieth century by the founders of statistical learning theory . Statistical learning theory provides various means of quantifying model capacity.\\n\\nAmong these, the most well known is the Vapnik-Chervonenkis dimension, or VC dimension. The VC dimension measures the capacity of a binary classifier. The VC dimension is defined as being the largest possible value of m for which there exists a training set of m different 2 points that the classifier can label arbitrarily. Quantifying the capacity of the model enables statistical learning theory to make quantitative predictions',\n",
       " 'fa312780-a19b-4503-a99c-27213c8c25b1': 'The rough idea is that when a component of wt participates in producing an estimated value, then the corresponding component of zt is bumped up and then begins to fade away. Learning will then occur in that component of wt if a nonzero TD error occurs before the trace falls back to zero. The trace-decay parameter λ 2  determines the rate at which the trace falls. The primary computational advantage of eligibility traces over n-step methods is that only a single trace vector is required rather than a store of the last n feature vectors. Learning also occurs continually and uniformly in time rather than being delayed and then catching up at the end of the episode. In addition learning can occur and a↵ect behavior immediately after a state is encountered rather than being delayed n steps. Eligibility traces illustrate that a learning algorithm can sometimes be implemented in a di↵erent way to obtain computational advantages.\\n\\nMany algorithms are most naturally formulated and understood as an update of a state’s value based on events that follow that state over multiple future time steps',\n",
       " '1d088448-6601-4c63-b273-7eb115fda2f4': 'Let pα(θ) be some hyperprior for the parameters introduced above, parameterized by α. The marginal likelihood can be written as: where the ﬁrst RHS term denotes a KL divergence of the approximate from the true posterior, and where L(φ; X) denotes the variational lower bound to the marginal likelihood: Note that this is a lower bound since the KL divergence is non-negative; the bound equals the true marginal when the approximate and true posteriors match exactly. The term log pθ(X) is composed of a sum over the marginal likelihoods of individual datapoints log pθ(X) = �N i=1 log pθ(x(i)), which can each be rewritten as: where again the ﬁrst RHS term is the KL divergence of the approximate from the true posterior, and L(θ, φ; x) is the variational lower bound of the marginal likelihood of datapoint i: The expectations on the RHS of eqs (14) and (16) can obviously be written as a sum of three separate expectations, of which the second and third component can sometimes be analytically solved, e.g. when both pθ(x) and qφ(z|x) are Gaussian',\n",
       " 'e7a9abe8-c875-499c-a3eb-33a05b174f2e': 'The most serious limitation of the Laplace framework, however, is that it is based purely on the aspects of the true distribution at a speciﬁc value of the variable, and so can fail to capture important global properties. In Chapter 10 we shall consider alternative approaches which adopt a more global perspective. As well as approximating the distribution p(z) we can also obtain an approximation to the normalization constant Z.\\n\\nUsing the approximation (4.133) we have where we have noted that the integrand is Gaussian and made use of the standard result (2.43) for a normalized Gaussian distribution. We can use the result (4.135) to obtain an approximation to the model evidence which, as discussed in Section 3.4, plays a central role in Bayesian model comparison. Consider a data set D and a set of models {Mi} having parameters {θi}. For each model we deﬁne a likelihood function p(D|θi, Mi). If we introduce a prior p(θi|Mi) over the parameters, then we are interested in computing the model evidence p(D|Mi) for the various models',\n",
       " '7f3201ed-d19d-49a9-8083-7a46be1b6c6d': 'Now suppose we ﬁrst run K-means on the image data, and then instead of transmitting the original pixel intensity vectors we transmit the identity of the nearest vector µk. Because there are K such vectors, this requires log2 K bits per pixel. We must also transmit the K code book vectors µk, which requires 24K bits, and so the total number of bits required to transmit the image is 24K + N log2 K (rounding up to the nearest integer). The original image shown in Figure 9.3 has 240 × 180 = 43, 200 pixels and so requires 24 × 43, 200 = 1, 036, 800 bits to transmit directly. By comparison, the compressed images require 43, 248 bits (K = 2), 86, 472 bits (K = 3), and 173, 040 bits (K = 10), respectively, to transmit. These represent compression ratios compared to the original image of 4.2%, 8.3%, and 16.7%, respectively. We see that there is a trade-off between degree of compression and image quality',\n",
       " '7994b4e6-7dbf-4a97-8496-474d07c25385': 'The idea follows the same route of GPI. Within one episode, it works as follows:  Initialize t = 0. Start with So and choose action Ay = arg MaX,<4 Q(So, a), where e-greedy is commonly applied. At time t, after applying action A;, we observe reward R;,, and get into the next state S;,,. Then pick the next action in the same way as in step 2: Ay,; = argmax,cy Q(S1,1, 4). Update the Q-value function: Q(S:, Ar) — Q(S:, At) + a( Rey + YQ(Si41, Atv) — Q(S:, At). Set t = t+ 1 and repeat from step 3. In each step of SARSA, we need to choose the next action according to the current policy. Q-Learning: Off-policy TD control  The development of Q-learning  is a big breakout in the early days of Reinforcement Learning',\n",
       " 'b171da0c-7e9f-4d3b-947f-eed5bc43db24': 'Equation (13.6) involves an appropriate sum over actions, but each term is not weighted by ⇡(a|St, ✓) as is needed for an expectation under ⇡. So we introduce such a weighting, without changing the equality, by multiplying and then dividing the summed terms by ⇡(a|St, ✓). Continuing from (13.6), we have where Gt is the return as usual. The ﬁnal expression in brackets is exactly what is needed, a quantity that can be sampled on each time step whose expectation is equal to the gradient. Using this sample to instantiate our generic stochastic gradient ascent algorithm (13.1) yields the REINFORCE update: This update has an intuitive appeal. Each increment is proportional to the product of a return Gt and a vector, the gradient of the probability of taking the action actually taken divided by the probability of taking that action.\\n\\nThe vector is the direction in parameter space that most increases the probability of repeating the action At on future visits to state St. The update increases the parameter vector in this direction proportional to the return, and inversely proportional to the action probability',\n",
       " 'd5852cc6-2cd5-4c3e-8436-e45609130008': 'To demonstrate this, we consider the emerging task of prompting a large pretrained LM for controllable generation . The goal is to learn to generate text prompts that steer the LM to generate sentences of certain desired attributes (e.g., topics). The problem of controlling the generation of pretrained LMs was previously approached through specialized algorithms such as modifying the LM hidden states during decoding . Here we show that prompts offer an easier, faster, more effective way for controlled generation.\\n\\nLearning to generate/tune prompts is gaining increasing attention recently. It side-steps the needs for expensive LM ﬁne-tuning, and adapts LMs to new scenarios with prompt as the (computefriendly) interface. Most existing approaches  rely on gradient backpropagation and are applicable only when the whole training pipeline is differentiable. This does not hold for the text generation setting, as illustrated in Figure 5. In contrast, the RL framework is generally applicable to any differentiable or discrete pipelines. Setup (more in §A.2.3)',\n",
       " '80574f9d-f0fc-4fd0-8467-bb309e78644f': 'The result is a distribution that in general has longer ‘tails’ than a Gaussian, as was seen in Figure 2.15. This gives the tdistribution an important property called robustness, which means that it is much less sensitive than the Gaussian to the presence of a few data points which are outliers. The robustness of the t-distribution is illustrated in Figure 2.16, which compares the maximum likelihood solutions for a Gaussian and a t-distribution. Note that the maximum likelihood solution for the t-distribution can be found using the expectationmaximization (EM) algorithm.\\n\\nHere we see that the effect of a small number of Exercise 12.24 distribution of 30 data points drawn from a Gaussian distribution, together with the maximum likelihood ﬁt obtained from a t-distribution (red curve) and a Gaussian (green curve, largely hidden by the red curve). Because the t-distribution contains the Gaussian as a special case it gives almost the same solution as the Gaussian',\n",
       " 'e60a013b-570f-46c7-8295-6181a63453fe': '“Representation Learning with Contrastive Predictive Coding\" arXiv preprint arXiv:1807.03748 . 23] Jason Wei and Kai Zou. “EDA: Easy data augmentation techniques for boosting performance on text classification tasks.\" EMNLP-IJCNLP 2019. 24] Sosuke Kobayashi. “Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations.\" NAACL 2018  25] Hongchao Fang et al. \"CERT: Contrastive self-supervised learning for language understanding.\" arXiv preprint arXiv:2005.12766 . 26] Dinghan Shen et al. “A Simple but Tough-to-Beat Data Augmentation Approach for Natural Language Understanding and Generation.\" arXiv preprint arXiv:2009.13818    27] Tianyu Gao et al. “SimCSE: Simple Contrastive Learning of Sentence Embeddings.\" arXiv preprint arXiv:2104.08821 . 28] Nils Reimers and Iryna Gurevych',\n",
       " '1cd341bc-209a-46f6-ac55-40aa784dfd07': 'These constraints explain why the MDP has a NoOp action and why the reward signal is 0 except when a read or write command is issued. NoOp is issued when it is the sole legal action in a state.\\n\\nTo maximize utilization of the memory system, the controller’s task is to drive the system to states in which either a read or a write action can be selected: only these actions result in sending data over the external data bus, so it is only these that contribute to the throughput of the system. Although precharge and activate produce no immediate reward, the agent needs to select these actions to make it possible to later select the rewarded read and write actions. The scheduling agent used Sarsa (Section 6.4) to learn an action-value function. States were represented by six integer-valued features. To approximate the action-value function, the algorithm used linear function approximation implemented by tile coding with hashing (Section 9.5.4). The tile coding had 32 tilings, each storing 256 action values as 16-bit ﬁxed point numbers. Exploration was \"-greedy with \" = 0.05',\n",
       " '34c7072e-611b-4f10-9b5f-90c05b2ff80c': 'We might ask whether it is possible to deﬁne an alternative graphical semantics for probability distributions such that conditional independence is determined by simple graph separation.\\n\\nThis is indeed the case and corresponds to undirected graphical models. By removing the directionality from the links of the graph, the asymmetry between parent and child nodes is removed, and so the subtleties associated with head-to-head nodes no longer arise. Suppose that in an undirected graph we identify three sets of nodes, denoted A, B, and C, and that we consider the conditional independence property To test whether this property is satisﬁed by a probability distribution deﬁned by a graph we consider all possible paths that connect nodes in set A to nodes in set B. If all such paths pass through one or more nodes in set C, then all such paths are ‘blocked’ and so the conditional independence property holds. However, if there is at least one such path that is not blocked, then the property does not necessarily hold, or more precisely there will exist at least some distributions corresponding to the graph that do not satisfy this conditional independence relation. This is illustrated with an example in Figure 8.27',\n",
       " '55d1a023-4f50-4357-b951-3db437bdadb2': 'In this diagram we distinguish the “representation” part of the model (the “task network,” here a recurrent net in the bottom) from the “memory” part of the model (the set of cells), which can store facts. The task network learns to “control” the memory, deciding where to read from and where to write to within the memory (through the reading and writing mechanisms, indicated by bold arrows pointing at the reading and writing addresses). 414  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  Explicit memory seems to allow models to learn tasks that ordinary RNNs or LSTM RNNs cannot learn. One reason for this advantage may be that information and  roo 1 ,04¢ trout 1 1 trout ard  https://www.deeplearningbook.org/contents/rnn.html    TAUIELLLS Call DE propagated (4oL wald Ll Lune OF DaCKWalrU Il LUE, LESPECLLVEly ) or very long durations.\\n\\nAs an alternative to back-propagation through weighted averages of memory cells, we can interpret the memory addressing coefficients as probabilities and  stochastically read just one cell',\n",
       " '1c7bdd83-6733-4bb4-9726-728f7bc6c508': 'The geometrical interpretation of the sum-of-squares error function is illustrated in Figure 1.3. We can solve the curve ﬁtting problem by choosing the value of w for which E(w) is as small as possible. Because the error function is a quadratic function of the coefﬁcients w, its derivatives with respect to the coefﬁcients will be linear in the elements of w, and so the minimization of the error function has a unique solution, denoted by w⋆, which can be found in closed form. The resulting polynomial is Exercise 1.1 There remains the problem of choosing the order M of the polynomial, and as we shall see this will turn out to be an example of an important concept called model comparison or model selection. In Figure 1.4, we show four examples of the results of ﬁtting polynomials having orders M = 0, 1, 3, and 9 to the data set shown in Figure 1.2.\\n\\nWe notice that the constant (M = 0) and ﬁrst order (M = 1) polynomials give rather poor ﬁts to the data and consequently rather poor representations of the function sin(2πx). The third order (M = 3) polynomial seems to give the best ﬁt to the function sin(2πx) of the examples shown in Figure 1.4',\n",
       " '97c1523e-8da6-452e-a1c2-5c4d4449c59e': 'We recommend a terminology in which Figure 5.1 is called a two-layer network, because it is the number of layers of adaptive weights that is important for determining the network properties.\\n\\nAnother generalization of the network architecture is to include skip-layer connections, each of which is associated with a corresponding adaptive parameter. For instance, in a two-layer network these would go directly from inputs to outputs. In principle, a network with sigmoidal hidden units can always mimic skip layer connections (for bounded input values) by using a sufﬁciently small ﬁrst-layer weight that, over its operating range, the hidden unit is effectively linear, and then compensating with a large weight value from the hidden unit to the output. In practice, however, it may be advantageous to include skip-layer connections explicitly. Furthermore, the network can be sparse, with not all possible connections within a layer being present. We shall see an example of a sparse network architecture when we consider convolutional neural networks in Section 5.5.6. Because there is a direct correspondence between a network diagram and its mathematical function, we can develop more general network mappings by considering more complex network diagrams',\n",
       " 'fa5ea8c1-f32d-4959-aa7c-26be7c26f95b': 'It has been reported that gradient-based optimization of conditional Gaussian mixtures (on the output of neural networks) can be unreliable, in part because one gets divisions (by the variance) which can be numerically unstable (when some variance gets to be small for a particular example, yielding very large gradients). One solution is to clip gradients (see section 10.11.1), while another is to scale the gradients heuristically .\\n\\nGaussian mixture outputs are particularly effective in generative models of speech  and movements of physical objects . The mixture density strategy gives a way for the network to represent multiple output modes and to control the variance of its output, which is crucial for obtaining  1 We consider c to be latent because we do not observe it in the data: given input x and target y, it is not possible to know with certainty which Gaussian component was responsible fory, but we can imagine that y was generated by picking one of them, and we can make that unobserved choice a random variable. 186  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  cn  https://www.deeplearningbook.org/contents/mlp.html     Figure 6.4: Samples drawn from a neural network with a mixture density output layer',\n",
       " '2ef4ef6b-c378-4c37-bdca-76316b3c81f1': 'Comparing the predicted returns of simulated paths is a simple form of planning, which can be done in a variety of ways as discussed in Chapter 8. When the environment of a model-free agent changes the way it reacts to the agent’s actions, the agent has to acquire new experience in the changed environment during which it can update its policy and/or value function. In the model-free strategy shown in Figure 14.5 (lower left), for example, if one of the goal boxes were to somehow shift to delivering a di↵erent reward, the rat would have to traverse the maze, possibly many times, to experience the new reward upon reaching that goal box, all the while updating either its policy or its action-value function (or both) based on this experience.\\n\\nThe key point is that for a model-free agent to change the action its policy speciﬁes for a state, or to change an action value associated with a state, it has to move to that state, act from it, possibly many times, and experience the consequences of its actions. A model-based agent can accommodate changes in its environment without this kind of ‘personal experience’ with the states and actions a↵ected by the change. A change in its model automatically (through planning) changes its policy',\n",
       " '01074af7-a127-4859-8533-be5524f46e2b': \"They showed that the most important component is the element-wise difference | f(x) — f(x’). Regression objective: This is the regression loss on cos( f(x), f(x’)), in which the pooling strategy has a big impact. In the experiments, they observed that max performs much worse than mean and cis -token. https://lilianweng.github.io/posts/2021-05-31-contrastive/     Contrastive Representation Learning | Lil'Log  In the experiments, which objective function works the best depends on the datasets, so there is no  Triplet objective: max(0, | f(x)  Sf(x*)|  fx)  f(x~)| + €), where x,x*,x~ are  embeddings of the anchor, positive and negative sentences. universal winner\",\n",
       " '4fcbbd18-0ea5-4c18-9b5b-17a244f7d344': 'After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.\\n\\nThis is the original form of the k-armed bandit problem, so named by analogy to a slot machine, or “one-armed bandit,” except that it has k levers instead of one. Each action selection is like a play of one of the slot machine’s levers, and the rewards are the payo↵s for hitting the jackpot. Through repeated action selections you are to maximize your winnings by concentrating your actions on the best levers. Another analogy is that of a doctor choosing between experimental treatments for a series of seriously ill patients. Each action is the selection of a treatment, and each reward is the survival or well-being of the patient. Today the term “bandit problem” is sometimes used for a generalization of the problem described above, but in this book we use it to refer just to this simple case. In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action',\n",
       " 'd9693618-ef83-49df-b8fc-cc7383cc6cd0': 'Transferring style in training data has been tested on the transition from simulated environments to the real-world.\\n\\nThis is very useful for robotic manipulation tasks using Reinforcement Learning because of potential damages to hardware when train- ing in the real-world. Many constraints such as low-fidelity cameras cause these models to generalize poorly when trained in physics simulations and deployed in the real-world. Tobin et al. explore the effectiveness of using different styles in training simula- tion and achieve within 1.5 cm accuracy in the real-world on the task of object localiza-  tion. Their experiments randomize the position and texture of the objects to be detected Shorten and Khoshgoftaar J Big Data  6:60   Fig. 27 Examples of different styles simulated by Tobin et al. on the table in the simulation, as well as the texture, lighting, number of lights, and ran- dom noise in the background. They found that with enough variability in the training data style, the real-world simply appears as another variation to the model. Interestingly, they found that diversity in styles was more effective than simulating in as realistic of an environment as possible. This is in contrast to the work of Shrivastava et al',\n",
       " '94f0a705-25c8-4690-9e84-e1712ed42612': 'One option is to have a tree structure that does not depend on the data,  ?We suggest not abbreviating “recursive neural network” as “RNN” to avoid confusion with “recurrent neural network.”  394  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  Figure 10.14: A recursive network has a computational graph that generalizes that of the  my 9N “4 https://www.deeplearningbook.org/contents/rnn.html     recurrent network from a chain to a tree.\\n\\nA variable-size sequence ®*”’, @*’,...,@*” can be mapped to a fixed-size representation (the output ©), witha fixed set of parameters . Ideally, one would like the learner itself to discover and infer the tree structure that is appropriate for any given input, as suggested by Bottou . Many variants of the recursive net idea are possible. For example, Frasconi et al. and Frasconi et al. associate the data with a tree structure, and associate the inputs and targets with individual nodes of the tree',\n",
       " 'cd895f3e-f8ee-4326-ae5a-dda234c7a3e9': 'For example, if you are one cell to the right of the goal and you move left, then one-third of the time you move one cell above the goal, one-third of the time you move two cells above the goal, and one-third of the time you move to the goal.\\n\\n⇤ One of the early breakthroughs in reinforcement learning was the development of an o↵-policy TD control algorithm known as Q-learning , deﬁned by In this case, the learned action-value function, Q, directly approximates q⇤, the optimal action-value function, independent of the policy being followed. This dramatically simpliﬁes the analysis of the algorithm and enabled early convergence proofs. The policy still has an e↵ect in that it determines which state–action pairs are visited and updated. However, all that is required for correct convergence is that all pairs continue to be updated. As we observed in Chapter 5, this is a minimal requirement in the sense that any method guaranteed to ﬁnd optimal behavior in the general case must require it. Under this assumption and a variant of the usual stochastic approximation conditions on the sequence of step-size parameters, Q has been shown to converge with probability 1 to q⇤. The Q-learning algorithm is shown below in procedural form',\n",
       " '67ea78bc-a840-4119-a05d-0fac30508f1b': '“A Survey on Contrastive Self-Supervised Learning.\" arXiv preprint arXiv:2011.00362   17] Jure Zbontar et al. “Barlow Twins: Self-Supervised Learning via Redundancy Reduction.\" arXiv preprint arXiv:2103.03230   18  Alec Radford, et al. “Learning Transferable Visual Models From Natural Language Supervision” arXiv preprint arXiv:2103.00020   19] Mathilde Caron et al. “Unsupervised Learning of Visual Features by Contrasting Cluster  https://lilianweng.github.io/posts/2021-05-31-contrastive/   Contrastive Representation Learning | Lil\\'Log   Assignments (SWAV).\"\\n\\nNeuriPS 2020. 20] Mathilde Caron et al. “Deep Clustering for Unsupervised Learning of Visual Features.\" ECCV 2018. 21] Prannay Khosla et al. “Supervised Contrastive Learning.\" NeurlPS 2020. 22] Aaron van den Oord, Yazhe Li & Oriol Vinyals',\n",
       " '84b4cb3d-87d2-4fe6-bd29-c8e4f5037c11': 'However, the ﬁnal function learned was a↵ected only slightly by the width of the features. Receptive ﬁeld shape tends to have a strong e↵ect on generalization but little e↵ect on asymptotic solution quality. Tile coding is a form of coarse coding for multi-dimensional continuous spaces that is ﬂexible and computationally eﬃcient. It may be the most practical feature representation for modern sequential digital computers. In tile coding the receptive ﬁelds of the features are grouped into partitions of the state space. Each such partition is called a tiling, and each element of the partition is called a tile. For example, the simplest tiling of a two-dimensional state space is a uniform grid such as that shown on the left side of Figure 9.9. The tiles or receptive ﬁeld here are squares rather than the circles in Figure 9.6. If just this single tiling were used, then the state indicated by the white spot would be represented by the single feature whose tile it falls within; generalization would be complete to all states within the same tile and nonexistent to states outside it',\n",
       " 'c2abb443-651c-4f9b-88ad-9546c4163caf': 'In this case, the network can contain as many convolutional layers as the available hardware can support, since the operation of convolution does not modify the architectural possibilities  343  CHAPTER 9. CONVOLUTIONAL NETWORKS  ()  Strided convolution Downsampling  Convolution  https://www.deeplearningbook.org/contents/convnets.html    OCRORORO  Figure 9.12: Convolution with a stride. In this example, we use a stride of two. (Top)Convolution with a stride length of two implemented in a single operation. (Bot- tom)Convolution with a stride greater than one pixel is mathematically equivalent to convolution with unit stride followed by downsampling. Obviously, the two-step approach involving downsampling is computationally wasteful, because it computes many values that are then discarded. available to the next layer. The input pixels near the border, however, influence fewer output pixels than the input pixels near the center',\n",
       " '30a4018e-9371-4946-840a-4d278e52e486': 'Linear models, such as logistic regression and linear regression, are appealing because they can be fit efficiently and reliably, either in closed form or with convex optimization. Linear models also have the obvious defect that the model capacity is limited to linear functions, so  https://www.deeplearningbook.org/contents/mlp.html    the model cannot understand the interaction between any two input variables. To extend linear models to represent nonlinear functions of ©, we can apply the linear model not to 2 itself but to a transformed input ¢(#), where ¢ is a  165  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  nonlinear transformation. Equivalently, we can apply the kernel trick described in section 5.7.2, to obtain a nonlinear learning algorithm based on implicitly applying the @ mapping. We can think of ¢ as providing a set of features describing x, or as providing a new representation for a. The question is then how to choose the mapping @. 1. One option is to use a very generic ¢, such as the infinite-dimensional ¢ that is implicitly used by kernel machines based on the RBF kernel',\n",
       " '4ebe2b5f-8268-44f8-99ed-9c1dcecb1a04': 'This means that immediately neighboring locations will have different filters, as in a locally connected layer, but the memory requirements for storing the parameters will increase only by a factor of the size of this set of kernels, rather than by the size of the entire output feature map.\\n\\nSee figure 9.16 for a comparison of locally connected layers, tiled convolution, and standard convolution. To define tiled convolution algebraically, let K be a 6-D tensor, where two of the dimensions correspond to different locations in the output map. Rather than having a separate index for each location in the output map, output locations cycle through a set of t different choices of kernel stack in each direction. If t is equal to  346  CHAPTER 9. CONVOLUTIONAL NETWORKS  https://www.deeplearningbook.org/contents/convnets.html    Figure 9.14: Comparison of local connections, convolution, and full connections. (Top)A locally connected layer with a patch size of two pixels. Each edge is labeled with a unique letter to show that each edge is associated with its own weight parameter. (Center)A convolutional layer with a kernel width of two pixels',\n",
       " 'd7af4005-1754-4ff8-a7de-2f0a6bf3a49a': 'Because we cannot use the complete-data log likelihood, we consider instead its expected value under the posterior distribution of the latent variable, which corresponds (as we shall see) to the E step of the EM algorithm. In the subsequent M step, we maximize this expectation. If the current estimate for the parameters is denoted θold, then a pair of successive E and M steps gives rise to a revised estimate θnew. The algorithm is initialized by choosing some starting value for the parameters θ0. The use of the expectation may seem somewhat arbitrary. However, we shall see the motivation for this choice when we give a deeper treatment of EM in Section 9.4. In the E step, we use the current parameter values θold to ﬁnd the posterior distribution of the latent variables given by p(Z|X, θold)',\n",
       " 'ef07aa81-b565-4d91-8182-301bcb5612d1': ', xN+1 and corresponding target observations t1, . , tN. 6.24 (⋆) Show that a diagonal matrix W whose elements satisfy 0 < Wii < 1 is positive deﬁnite. Show that the sum of two positive deﬁnite matrices is itself positive deﬁnite. 6.27 (⋆ ⋆ ⋆) Derive the result (6.90) for the log likelihood function in the Laplace approximation framework for Gaussian process classiﬁcation. Similarly, derive the results (6.91), (6.92), and (6.94) for the terms in the gradient of the log likelihood. In the previous chapter, we explored a variety of learning algorithms based on nonlinear kernels.\\n\\nOne of the signiﬁcant limitations of many such algorithms is that the kernel function k(xn, xm) must be evaluated for all possible pairs xn and xm of training points, which can be computationally infeasible during training and can lead to excessive computation times when making predictions for new data points. In this chapter we shall look at kernel-based algorithms that have sparse solutions, so that predictions for new inputs depend only on the kernel function evaluated at a subset of the training data points',\n",
       " '2950a372-c591-4898-86c3-cb623cd8f337': 'In a traditional supervised learning setup, we would learn hθ by ﬁtting it to a training set of labeled data points. However, in our setting, we assume that we only have access to unlabeled data for training. We do assume access to a small set of labeled data used during development, called the development set, and a blind, held-out labeled test set for evaluation. These sets can be orders of magnitudes smaller than a training set, making them economical to obtain.\\n\\nThe user of Snorkel aims to generate training labels by providing a set of labeling functions, which are black-box functions, λ : X → Y ∪ {∅}, that take in a data point and output a label where we use ∅ to denote that the labeling function abstains. Given m unlabeled data points and n labeling functions, Snorkel applies the labeling functions over the unlabeled data to produce a matrix of labeling function outputs � ∈ (Y ∪ {∅})m×n. The goal of the remaining Fig. 4 Labeling functions take as input a Candidate object, representing a data point to be classiﬁed',\n",
       " '0cae7e82-cc92-496d-b248-19e71944c863': 'In section 3.3.2, we saw that the probability of a continuous vector-valued x lying in some set S is given by the integral of p(x) over the set S. Some choices of set S can produce paradoxes. For example, it is possible to construct two sets S; and Sg such that p@ € S1) + p(w € Sz) > 1 but S;MS2 = @.\\n\\nThese sets are generally constructed making very heavy use of the infinite precision of real numbers, for example by making fractal-shaped sets or sets that are defined  https://www.deeplearningbook.org/contents/prob.html    by transforming the set of rational numbers.” One of the key contributions of measure theory is to provide a characterization of the set of sets we can compute he probability of without encountering paradoxes. In this book, we integrate only  over sets with relatively simple descriptions, so this aspect of measure theory never becomes a relevant concern. For our purposes, measure theory is more useful for describing theorems that apply to most points in R” but do not apply to some corner cases. Measure theory provides a rigorous way of describing that a set of points is negligibly small. Such a set is said to have measure zero',\n",
       " '39f1857f-bd29-457a-b8bf-9489037a73e4': 'We can derive the EM algorithm for the linear dynamical system as follows. Let us denote the estimated parameter values at some particular cycle of the algorithm by θold. For these parameter values, we can run the inference algorithm to determine the posterior distribution of the latent variables p(Z|X, θold), or more precisely those local posterior marginals that are required in the M step. In particular, we shall require the following expectations where we have used (13.104). Now we consider the complete-data log likelihood function, which is obtained by taking the logarithm of (13.6) and is therefore given by in which we have made the dependence on the parameters explicit.\\n\\nWe now take the expectation of the complete-data log likelihood with respect to the posterior distribution p(Z|X, θold) which deﬁnes the function In the M step, this function is maximized with respect to the components of θ. Consider ﬁrst the parameters µ0 and V0. If we substitute for p(z1|µ0, V0) in (13.108) using (13.77), and then take the expectation with respect to Z, we obtain where all terms not dependent on µ0 or V0 have been absorbed into the additive constant',\n",
       " 'c8c8c0a9-a815-4e35-bc60-25fea37db1cf': 'Xinyun Chen, Chen Liang, Adams Wei Yu, Dawn Song, and Denny Zhou. 2020e. Compositional generalization via neural-symbolic stack machines. Advances in Neural Information Processing Systems, 33. Yong Cheng, Lu Jiang, and Wolfgang Macherey. 2019.\\n\\nRobust neural machine translation with doubly adversarial inputs. Proceedings of the 57th Annual Yong Cheng, Lu Jiang, Wolfgang Macherey, and Jacob Eisenstein. 2020b. AdvAug: Robust adversarial augmentation for neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5961– 5970, Online. Association for Computational Linguistics. Yong Cheng, Wei Xu, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. 2016. Semisupervised learning for neural machine translation. Kevin Clark, Minh-Thang Luong, Christopher D. Manning, and Quoc Le. 2018',\n",
       " 'ad8c4ba2-1c3b-40ab-9d0e-3538dd226723': 'This points to a direction in which the reinforcement learning framework might be developed in the future to exploit computational advantages of separate appetitive and aversive systems, but for now we are passing over these possibilities. Another discrepancy in terminology is how we use the word action. To many cognitive scientists, an action is purposeful in the sense of being the result of an animal’s knowledge about the relationship between the behavior in question and the consequences of that behavior. An action is goal-directed and the result of a decision, in contrast to a response, which is triggered by a stimulus; the result of a reﬂex or a habit.\\n\\nWe use the word action without di↵erentiating among what others call actions, decisions, and responses. These are important distinctions, but for us they are encompassed by di↵erences between model-free and model-based reinforcement learning algorithms, which we discussed above in relation to habitual and goal-directed behavior in Section 14.6. Dickinson  discusses the distinction between responses and actions. A term used a lot in this book is control. What we mean by control is entirely di↵erent from what it means to animal learning psychologists',\n",
       " '8f729e91-097b-40c1-b7b3-9dd8937ebf16': 'No triggering stimuli were presented, and after the monkey reached for and ate the food morsel, the experimenter usually (though not always), silently and unseen by the monkey, replaced food in the bin by sticking it onto a rigid wire.\\n\\nHere too, the activity of the dopamine neurons Romo and Schultz monitored was not related to the monkey’s movements, but a large percentage of these neurons produced phasic responses whenever the monkey ﬁrst touched a food morsel. These neurons did not respond when the monkey touched just the wire or explored the bin when no food was there. This was good evidence that the neurons were responding to the food and not to other aspects of the task. The purpose of Romo and Schultz’s second task was to see what happens when movements are triggered by stimuli. This task used a di↵erent bin with a movable cover. The sight and sound of the bin opening triggered reaching movements to the bin. In this case, Romo and Schultz found that after some period of training, the dopamine neurons no longer responded to the touch of the food but instead responded to the sight and sound of the opening cover of the food bin',\n",
       " 'fdcb51bb-f08b-4d3d-9c31-d313192d4cfc': 'This equation should include an expectation conditioned on following the policy, ⇡. Then give a second equation in which the expected value is written out explicitly in terms of ⇡(a|s) such that no expected value notation appears in the equation. ⇤ Exercise 3.19 The value of an action, q⇡(s, a), depends on the expected next reward and the expected sum of the remaining rewards. Again we can think of this in terms of a small backup diagram, this one rooted at an action (state–action pair) and branching to the possible next states: Give the equation corresponding to this intuition and diagram for the action value, q⇡(s, a), in terms of the expected next reward, Rt+1, and the expected next state value, v⇡(St+1), given that St =s and At =a. This equation should include an expectation but not one conditioned on following the policy.\\n\\nThen give a second equation, writing out the expected value explicitly in terms of p(s0, r|s, a) deﬁned by (3.2), such that no expected value notation appears in the equation. ⇤ Solving a reinforcement learning task means, roughly, ﬁnding a policy that achieves a lot of reward over the long run',\n",
       " '16c380d1-9e51-4437-8f0e-f2e5f7c23bf1': 'On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3-4):285–294. Thompson, W. R. On the theory of apportionment. American Journal of Mathematics, Thon, M. Spectral Learning of Sequential Systems. Ph.D. thesis, Jacobs University Thon, M., Jaeger, H. Links between multiplicity automata, observable operator models and predictive state representations: a uniﬁed learning framework. The Journal of Machine Learning Research, 16(1):103–147. Thorndike, E. L. Animal intelligence: An experimental study of the associative processes in animals. The Psychological Review, Series of Monograph Supplements, II(4). Tian, T. (in preparation) An Empirical Study of Sliding-Step Methods in Temporal Di↵erence Learning. M.Sc thesis, University of Alberta, Edmonton. Tieleman, T., Hinton, G. Lecture 6.5–RMSProp',\n",
       " '844df381-2354-4eb9-98c4-d6eebf1afb6b': 'Besides, compared to both the conventional synonym substitution and the approach that keeps the augmentation network ﬁxed, our adaptive method that ﬁne-tunes the augmentation network jointly with model training achieves superior results. Indeed, the heuristic-based synonym approach can sometimes harm the model performance (e.g., SST-5 and IMDB), as also observed in previous work . This can be because the heuristic rules do not ﬁt the task or datasets well. In contrast, learning-based augmentation has the advantage of adaptively generating useful samples to improve model training. It is interesting to see from Table 1 that our augmentation method consistently outperforms the weighting method, showing that data augmentation can be a more suitable technique than data weighting for manipulating small-size data.\\n\\nOur approach provides the generality to instantiate diverse manipulation types and learn with the same single procedure. To investigate the augmentation model and how the ﬁne-tuning affects the augmentation results, we show in Figure 2 the top-5 most probable word substitutions predicted by the augmentation model for two masked tokens, respectively. Comparing the results of epoch 1 and epoch 3, we can see the augmentation model evolves and dynamically adjusts the augmentation behavior as the training proceeds',\n",
       " '5817884d-b4a7-4fce-a478-d04fc852ce1b': 'Maximum likelihood thus becomes minimization of the negative log-likelihood NLL), or equivalently, minimization of the cross-entropy.\\n\\nThe perspective of maximum likelihood as minimum KL divergence becomes helpful in this case because the KL divergence has a known minimum value of zero. The negative og-likelihood can actually become negative when 2 is real-valued. 130  CHAPTER 5. MACHINE LEARNING BASICS  5.5.1 Conditional Log-Likelihood and Mean Squared Error  The maximum likelihood estimator can readily be generalized to estimate a condi- tional probability P( ;@) in order to predict y given x. This is actually the yx  https://www.deeplearningbook.org/contents/ml.html  most common situation because 1t forms the basis tor most supervised learning. It represents all our inputs and Y all our observed targets, then the conditional maximum likelihood estimator is  Oy, = arg max P(Y | X; 6)',\n",
       " 'ce63ddf9-52d6-4622-8fc2-0ee92ed4b7f6': \"input: batch size N, constant rT, structure of f, g, T. for sampled minibatch {a,}/’_, do for all k € {1,...,N} do draw two augmentation functions t~T, t!~T # the first augmentation  Fox-1 = t(xp)  hop-1 = f(®2x-1) # representation Zor—1 = g(hox-1) # projection # the second augmentation Ea. = t' (x) hoa, = f (Xx) # representation Zo\",\n",
       " '88ff02c0-3e32-4a5e-b0dc-ed1ed36887b5': 'If we call the ?P¥°P method to request the gradient with respect to  given that the gradient on the output is G, then the PPrOP method of the matrix multiplication operation must state that the gradient with respect to A  is given by GB\". Likewise, if we call the bprop method to request the gradient with respect to B, then the matrix operation is responsible for implementing the bprop method and specifying that the desired gradient is given by A\\'G. The back-propagation algorithm itself does not need to know any differentiation rules. It only needs to call each operation’s bprop rules with the right arguments. Formally, op.bprop(inputs, X,G) must return  S- (Vxop.f(inputs),;) G;, (6.54)  a  which is just an implementation of the chain rule as expressed in equation 6.47.\\n\\nHere, inputs is a list of inputs that are supplied to the operation, op.f is the mathematical function that the operation implements, X is the input whose gradient we wish to compute, and G is the gradient on the output of the operation',\n",
       " '9afdaaf7-eb91-41ca-8a67-d927b8137836': 'Note that this holds independently of the values of the πk so long as none of the πk is zero.\\n\\nThus, in this limit, we obtain a hard assignment of data points to clusters, just as in the K-means algorithm, so that γ(znk) → rnk where rnk is deﬁned by (9.2). Each data point is thereby assigned to the cluster having the closest mean. The EM re-estimation equation for the µk, given by (9.17), then reduces to the K-means result (9.4). Note that the re-estimation formula for the mixing coefﬁcients (9.22) simply re-sets the value of πk to be equal to the fraction of data points assigned to cluster k, although these parameters no longer play an active role in the algorithm. Finally, in the limit ϵ → 0 the expected complete-data log likelihood, given by (9.40), becomes Exercise 9.11 Thus we see that in this limit, maximizing the expected complete-data log likelihood is equivalent to minimizing the distortion measure J for the K-means algorithm given by (9.1). Note that the K-means algorithm does not estimate the covariances of the clusters but only the cluster means',\n",
       " 'd4e4fbe4-36a0-4452-b0ef-59a178ed3caf': 'In reward-modulated STDP, changes in synapses in addition depend on a neuromodulator, such as dopamine, arriving within a time window that can last up to 10 seconds after the conditions for STDP are met. Evidence is accumulating that reward-modulated STDP occurs at corticostriatal synapses, where the actor’s learning takes place in the hypothetical neural implementation of an actor–critic system, adds to the plausibility of the hypothesis that something like an actor–critic system exists in the brains of some animals. The idea of synaptic eligibility and basic features of the actor learning rule derive from Klopf’s hypothesis of the “hedonistic neuron” . He conjectured that individual neurons seek to obtain reward and to avoid punishment by adjusting the eﬃcacies of their synapses on the basis of rewarding or punishing consequences of their action potentials.\\n\\nA neuron’s activity can a↵ect its later input because the neuron is embedded in many feedback loops, some within the animal’s nervous system and body and others passing through the animal’s external environment',\n",
       " 'eadef837-8de7-4c4e-9094-2ee211d7c82c': 'Finally, there is o↵-policy learning; can we give that up? On-policy methods are often adequate. For model-free reinforcement learning, one can simply use Sarsa rather than Q-learning. O↵-policy methods free behavior from the target policy. This could be considered an appealing convenience but not a necessity. However, o↵-policy learning is essential to other anticipated use cases, cases that we have not yet mentioned in this book but may be important to the larger goal of creating a powerful intelligent agent. In these use cases, the agent learns not just a single value function and single policy, but large numbers of them in parallel. There is extensive psychological evidence that people and animals learn to predict many di↵erent sensory events, not just rewards. We can be surprised by unusual events, and correct our predictions about them, even if they are of neutral valence (neither good nor bad). This kind of prediction presumably underlies predictive models of the world such as are used in planning.\\n\\nWe predict what we will see after eye movements, how long it will take to walk home, the probability of making a jump shot in basketball, and the satisfaction we will get from taking on a new project',\n",
       " 'c8f15a5f-ed65-4cb3-9466-9d81b4d264dc': 'Write down the minimum misclassiﬁcation-rate decision rule assuming the two classes have equal prior probability.\\n\\nShow also that, if the kernel is chosen to be k(x, x′) = xTx′, then the classiﬁcation rule reduces to simply assigning a new input vector to the class having the closest mean. Finally, show that, if the kernel takes the form k(x, x′) = φ(x)Tφ(x′), that the classiﬁcation is based on the closest mean in the feature space φ(x). 7.3 (⋆ ⋆) Show that, irrespective of the dimensionality of the data space, a data set consisting of just two data points, one from each class, is sufﬁcient to determine the location of the maximum-margin hyperplane. where {an} are given by maximizing (7.10) subject to the constraints (7.11) and (7.12). 7.5 (⋆ ⋆) Show that the values of ρ and {an} in the previous exercise also satisfy where �L(a) is deﬁned by (7.10). Similarly, show that 7.6 (⋆) Consider the logistic regression model with a target variable t ∈ {−1, 1}',\n",
       " 'f9d06b52-3f76-4c75-a704-d6b44ce4ee3b': 'Once the SamplePairing images are added to the training set, they run in cycles between 8:2 epochs, 8 with SamplePairing images, 2 without. Jaderberg et al. train exclusively with synthetic data for natural scene text recognition. The synthetic data produced the training data by enumerating through different fonts and augmentations. This produced sets of training images for size 50 k and 90 k lexicons.\\n\\nMikolajczyk and Grochowski  draw comparisons from transfer learning. They suggest that training on augmented data to learn the initial weights of a deep convolutional network is similar to transferring weights trained on other datasets such as ImageNet. These weights are then fine-tuned only with the original training data. Curriculum learning decisions are especially important for One-Shot Learning sys- tems such as FaceNet, presented by Schroff et al. It is important to find faces which are somewhat similar to the new face such that the learned distance function is actually useful. In this sense, the concept of curriculum learning shares many similarities with adversarial search algorithms or learning only on hard examples',\n",
       " 'e306bddc-bfc0-4b3e-9b0f-ba0fcd46fa99': ', N, spaced uniformly in range , and the target data set t was obtained by ﬁrst computing the corresponding values of the function sin(2πx) and then adding a small level of random noise having a Gaussian distribution (the Gaussian distribution is discussed in Section 1.2.4) to each such point in order to obtain the corresponding value tn. By generating data in this way, we are capturing a property of many real data sets, namely that they possess an underlying regularity, which we wish to learn, but that individual observations are corrupted by random noise. This noise might arise from intrinsically stochastic (i.e. random) processes such as radioactive decay but more typically is due to there being sources of variability that are themselves unobserved. Our goal is to exploit this training set in order to make predictions of the value �t of the target variable for some new value �x of the input variable. As we shall see later, this involves implicitly trying to discover the underlying function sin(2πx)',\n",
       " 'e6258fbe-f5f2-4efb-9086-595b0e75fe21': 'The graphical models research community is large and has developed many different models, training algorithms, and inference algorithms.\\n\\nIn this chapter, we provide basic background on some of the most central ideas of graphical models, with an emphasis on the concepts that have proved most useful to the deep learning research community. If you already have a strong background in graphical models,  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    you May wisn to SKlp Most OI this Chapter. However, even a graphical Model 4 555  CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  expert may benefit from reading the final section of this chapter, section 16.7, in which we highlight some of the unique ways in which graphical models are used for deep learning algorithms. Deep learning practitioners tend to use very different model structures, learning algorithms and inference procedures than are commonly used by the rest of the graphical models research community. In this chapter, we identify these differences in preferences and explain the reasons for them. We first describe the challenges of building large-scale probabilistic models. Next, we describe how to use a graph to describe the structure of a probability distribution',\n",
       " 'b38dd58e-403a-452e-b9d2-70117e53ec24': 'CONVOLUTIONAL NETWORKS  Figure 9.19: Many machine learning algorithms learn features that detect edges or specific colors of edges when applied to natural images.\\n\\nThese feature detectors are reminiscent of the Gabor functions known to be present in the primary visual cortex. (Left)Weights learned by an unsupervised learning algorithm (spike and slab sparse coding) applied to small image patches. (Right)Convolution kernels learned by the first layer of a fully supervised convolutional maxout network. Neighboring pairs of filters drive the same maxout unit. 9.11 Convolutional Networks and the History of Deep Learning  Convolutional networks have played an important role in the history of deep learning. They are a key example of a successful application of insights obtained by studying the brain to machine learning applications. They were also some of the first deep models to perform well, long before arbitrary deep models were considered viable. Convolutional networks were also some of the first neural networks to solve important commercial applications and remain at the forefront of commercial applications of deep learning today. For example, in the 1990s, the neural network research group at AT&T developed a convolutional network for reading checks',\n",
       " 'f1d21b2e-e90a-4559-ab74-22159b4a5202': 'A~! is primarily useful as a theoretical tool, however, and should not actually be used in practice for most software applications. Because A 7! can be represented with only limited precision on a digital computer, algorithms that make use of the value of b can usually obtain more accurate estimates of x. 2.4 Linear Dependence and Span  For A7! to exist, equation 2.11 must have exactly one solution for every value of b. It is also possible for the system of equations to have no solutions or infinitely many solutions for some values of b. It is not possible, however, to have more than one but less than infinitely many solutions for a particular 6; if both aw and y are  https://www.deeplearningbook.org/contents/linear_algebra.html    solutions, then z=ax+(l—a)y (2.26)  is also a solution for any real a',\n",
       " 'c2804b9c-6c90-4c83-a5e4-9e477ca26613': 'L., Szepesvri, C. Convergence results for single-step on-policy reinforcement-learning algorithms. Machine Learning, 38(3):287–308. aggregation. In Advances in Neural Information Processing Systems 7 , pp. 359– 368. MIT Press, Cambridge, MA. Singh, S., Lewis, R. L., Barto, A. G. Where do rewards come from? In N. Taatgen and H. van Rijn (Eds. ), Proceedings of the 31st Annual Conference of the Cognitive Science Society, pp. 2601–2606. Cognitive Science Society. Singh, S., Lewis, R. L., Barto, A. G., Sorg, J. Intrinsically motivated reinforcement learning: An evolutionary perspective. IEEE Transactions on Autonomous Mental Development, 2(2):70–82. Special issue on Active Learning and Intrinsically Motivated Exploration in Robots: Advances and Challenges. Skinner, B',\n",
       " '78800acc-9e5f-4768-a005-7fa7d7745c8e': 'In many follow-up works, contrastive loss incorporating multiple negative samples is also broadly referred to as NCE. InfoNCE  The InfoNCE loss in CPC , inspired by NCE, uses categorical cross-entropy loss to identify the positive sample amongst a set of unrelated noise samples.\\n\\nGiven a context vector c, the positive sample should be drawn from the conditional distribution p(x|c), while VV — 1 negative samples are drawn from the proposal distribution p(x), independent from the context c. For brevity, let us label all the samples as X = {x;}%, among which only one of them Xpos is a positive sample',\n",
       " 'c22ea91f-84c9-44f5-a834-3f913e165004': 'https://www.deeplearningbook.org/contents/generative_models.html    Figure 20.9: A neural auto-regressive network predicts the i-th variable x; from the i — 1 previous ones, but is parametrized so that features (groups of hidden units denoted h; ) that are functions of x1,...,2; can be reused in predicting all the subsequent variables Vit, Vi42,..-,Xa. 704  CHAPTER 20. DEEP GENERATIVE MODELS  Each P(x; | %j-1,...,21) can represent a conditional distribution by having outputs of the neural network predict parameters of the conditional distribution of 2;, as discussed in section 6.2.1.1. Although the original neural auto-regressive networks were initially evaluated in the context of purely discrete multivariate data (with a sigmoid output for a Bernoulli variable or softmax output for a multinoulli variable), it is natural to extend such models to continuous variables or joint distributions involving both discrete and continuous variables. 20.10.10 NADE  The neural auto-regressive density estimator (NADE) is a very successful recent form of neural auto-regressive network',\n",
       " 'b94509bf-7076-4ae4-a4fe-e1a9a1c6bd3b': 'Note that the left-hand mode of the class-conditional density p(x|C1), shown in blue on the left plot, has no effect on the posterior probabilities. The vertical green line in the right plot shows the decision boundary in x that gives the minimum misclassiﬁcation rate. be of low accuracy, which is known as outlier detection or novelty detection . However, if we only wish to make classiﬁcation decisions, then it can be wasteful of computational resources, and excessively demanding of data, to ﬁnd the joint distribution p(x, Ck) when in fact we only really need the posterior probabilities p(Ck|x), which can be obtained directly through approach (b). Indeed, the classconditional densities may contain a lot of structure that has little effect on the posterior probabilities, as illustrated in Figure 1.27. There has been much interest in exploring the relative merits of generative and discriminative approaches to machine learning, and in ﬁnding ways to combine them',\n",
       " 'aabb213d-b9bc-47e3-9c64-0645dcf9bfe6': 'Similarly, the corresponding planning algorithms also have no γ.\\n\\nFor example, the value iteration algorithm with options, analogous to (4.10), is If ⌦(s) includes all the low-level actions available in each s, then this algorithm converges to the conventional v⇤, from which the optimal policy can be computed. However, it is particularly useful to plan with options when only a subset of the possible options are considered (in ⌦(s)) in each state. Value iteration will then converge to the best hierarchical policy limited to the restricted set of options. Although this policy may be sub-optimal, convergence can be much faster because fewer options are considered and because each option can jump over many time steps. To plan with options, one must either be given the option models, or learn them. One natural way to learn an option model is to formulate it as a collection of GVFs (as deﬁned in the preceding section) and then learn the GVFs using the methods presented in this book. It is not diﬃcult to see how this could be done for the reward part of the option model',\n",
       " '5a9e6d06-0e04-42ca-98c0-daa1d0fbae11': 'In many ways these categories respectively correspond to categories of learning extensively studied by psychologists: classical, or Pavlovian, conditioning and instrumental, or operant, conditioning. These correspondences are not completely accidental because of psychology’s inﬂuence on reinforcement learning, but they are nevertheless striking because they connect ideas arising from di↵erent objectives. The prediction algorithms presented in this book estimate quantities that depend on how features of an agent’s environment are expected to unfold over the future.\\n\\nWe speciﬁcally focus on estimating the amount of reward an agent can expect to receive over the future while it interacts with its environment. In this role, prediction algorithms are policy evaluation algorithms, which are integral components of algorithms for improving policies. But prediction algorithms are not limited to predicting future reward; they can predict any feature of the environment . The correspondence between prediction algorithms and classical conditioning rests on their common property of predicting upcoming stimuli, whether or not those stimuli are rewarding (or punishing). The situation in an instrumental, or operant, conditioning experiment is di↵erent. Here, the experimental apparatus is set up so that an animal is given something it likes (a reward) or something it dislikes (a penalty) depending on what the animal did',\n",
       " '994c00da-17b8-477d-b9f5-cda00af307d7': 'However, imagine using color aug- mentations exclusively. If the initial training dataset consists of 50 dogs and 50 cats, and each image is augmented with 100 color filters to produce 5000 dogs and 5000 cats, this dataset will be heavily biased towards the spatial characteristics of the original 50 dogs and 50 cats. This over-extensive color-augmented data will cause a deep model to overfit even worse than the original. From this anecdote, we can conceptualize the existence of an optimal size for post-augmented data. Additionally, there is no consensus about the best strategy for combining data warping and oversampling techniques. One important consideration is the intrinsic bias in the initial, limited dataset.\\n\\nThere are no existing augmentation techniques that can correct a dataset that has very poor diversity with respect to the testing data. All these augmenta- tion algorithms perform best under the assumption that the training data and testing data are both drawn from the same distribution. If this is not true, it is very unlikely that  these methods will be useful',\n",
       " 'f694f9a3-b3cd-4f95-b1c0-4da74e044723': 'In particular, squared error is a poor loss function for softmax units and can fail to train the model to change its output,  even when the model makes highly confident incorrect predictions . To understand why these other loss functions can fail, we need to examine the softmax function itself. Like the sigmoid, the softmax activation can saturate. The sigmoid function has a single output that saturates when its input is extremely negative or extremely positive. The softmax has multiple output values.\\n\\nThese output values can saturate when the differences between input values become extreme. When the softmax saturates, many cost functions based on the softmax also saturate, unless they are able to invert the saturating activating function. To see that the softmax function responds to the difference between its inputs, observe that the softmax output is invariant to adding the same scalar to all its  inputs: softmax(z) = softmax(z + c). (6.32) Using this property, we can derive a numerically stable variant of the softmax: softmax(z) = softmax(z — max z;)',\n",
       " 'c792c5c0-f3f7-43e8-a9f1-da83a286ce55': 'For any ﬁxed policy ⇡, TD(0) has been proved to converge to v⇡, in the mean for a constant step-size parameter if it is suﬃciently small, and with probability 1 if the step-size parameter decreases according to the usual stochastic approximation conditions (2.7). Most convergence proofs apply only to the table-based case of the algorithm presented above (6.2), but some also apply to the case of general linear function approximation. These results are discussed in a more general setting in Section 9.4. If both TD and Monte Carlo methods converge asymptotically to the correct predictions, then a natural next question is “Which gets there ﬁrst?” In other words, which method learns faster? Which makes the more eﬃcient use of limited data? At the current time this is an open question in the sense that no one has been able to prove mathematically that one method converges faster than the other. In fact, it is not even clear what is the most appropriate formal way to phrase this question!\\n\\nIn practice, however, TD methods have usually been found to converge faster than constant-↵ MC methods on stochastic tasks, as illustrated in Example 6.2',\n",
       " 'dbb7c6e0-62bc-4951-92d9-9cc7fcaeab88': 'Now´e, Vrancx, and De Hauwere  reviewed more recent developments in the wider ﬁeld of multi-agent reinforcement learning 15.11 Yin and Knowlton  reviewed ﬁndings from outcome-devaluation experiments with rodents supporting the view that habitual and goal-directed behavior (as psychologists use the phrase) are respectively most associated with processing in the dorsolateral striatum (DLS) and the dorsomedial striatum (DMS).\\n\\nResults of functional imaging experiments with human subjects in the outcomedevaluation setting by Valentin, Dickinson, and O’Doherty  suggest that the orbitofrontal cortex (OFC) is an important component of goal-directed choice. Single unit recordings in monkeys by Padoa-Schioppa and Assad  support the role of the OFC in encoding values guiding choice behavior. Rangel, Camerer, and Montague  and Rangel and Hare  reviewed ﬁndings from the perspective of neuroeconomics about how the brain makes goal-directed decisions. Pezzulo, van der Meer, Lansink, and Pennartz  reviewed the neuroscience of internally generated sequences and presented a model of how these mechanisms might be components of model-based planning',\n",
       " 'f9eacb38-a759-499a-a30a-d8b0778bf6f3': 'One of prioritized sweeping’s limitations is that it uses expected updates, which in stochastic environments may waste lots of computation on low-probability transitions.\\n\\nAs we show in the following section, sample updates right is shown the obstacles and the shortest solution from start to goal, found by prioritized sweeping. This problem is deterministic, but has four actions and 14,400 potential states (some of these are unreachable because of the obstacles). This problem is probably too large to be solved with unprioritized methods. Figure reprinted from Moore and Atkeson . can in many cases get closer to the true value function with less computation despite the variance introduced by sampling. Sample updates can win because they break the overall backing-up computation into smaller pieces—those corresponding to individual transitions—which then enables it to be focused more narrowly on the pieces that will have the largest impact. This idea was taken to what may be its logical limit in the “small backups” introduced by van Seijen and Sutton . These are updates along a single transition, like a sample update, but based on the probability of the transition without sampling, as in an expected update',\n",
       " '34553c65-1862-4afb-a57c-cb84168073e2': 'Smart Augmentation The Smart Augmentation  approach utilizes a similar con- cept as the Neural Augmentation technique presented above. However, the combina- tion of images is derived exclusively from the learned parameters of a prepended CNN, rather than using the Neural Style Transfer algorithm. Smart Augmentation is another approach to meta-learning augmentations. This is done by having two networks, Network-A and Network-B. Network-A is an augmen- tation network that takes in two or more input images and maps them into a new  image or images to train Network-B. The change in the error rate in Network-B is then Shorten and Khoshgoftaar J Big Data  6:60   Table 7 Results comparing augmentations   Quantitative results on dogs vs. goldfish  Dogs vs goldfish  Augmentation Val. acc.\\n\\nNone 0.855 Traditional 0.890 GANs 0.865 Neural + no loss 0.915 Neural + content loss 0.900 Neural + style 0.890 Control 0.840  Quantitative results on dogs vs cats  Dogs vs cat  Augmentation Val. acc',\n",
       " '1cc55bc8-2770-4193-bf49-f4047b345ac2': 'A Lipschitz continuous function is a function f whose rate of change is bounded by a Lipschitz constant CL:  Va,Vy,|f(@) — f(y)| < L\\\\|a— ylle- (4.13)  This property is useful because it enables us to quantify our assumption that a small change in the input made by an algorithm such as gradient descent will have  90  https://www.deeplearningbook.org/contents/numerical.html    CHAPTER 4.\\n\\nNUMERICAL COMPUTATION  a small change in the output. Lipschitz continuity is also a fairly weak constraint, and many optimization problems in deep learning can be made Lipschitz continuous with relatively minor modifications. Perhaps the most successful field of specialized optimization is convex op- timization. Convex optimization algorithms are able to provide many more guarantees by making stronger restrictions. These algorithms are applicable only to convex functions—functions for which the Hessian is positive semidefinite ev- erywhere. Such functions are well-behaved because they lack saddle points, and all their local minima are necessarily global minima. However, most problems in deep learning are difficult to express in terms of convex optimization',\n",
       " '4a9576ab-bd80-42a5-8128-de98f3f30fff': 'We turn now to the second major class of graphical models that are described by undirected graphs and that again specify both a factorization and a set of conditional independence relations. A Markov random ﬁeld, also known as a Markov network or an undirected graphical model , has a set of nodes each of which corresponds to a variable or group of variables, as well as a set of links each of which connects a pair of nodes. The links are undirected, that is they do not carry arrows. In the case of undirected graphs, it is convenient to begin with a discussion of conditional independence properties. In the case of directed graphs, we saw that it was possible to test whether a parSection 8.2 ticular conditional independence property holds by applying a graphical test called d-separation. This involved testing whether or not the paths connecting two sets of nodes were ‘blocked’. The deﬁnition of blocked, however, was somewhat subtle due to the presence of paths having head-to-head nodes',\n",
       " 'f83c8fa0-08c1-4cb2-b95c-417b675216cf': \"1 1 1 a wo soo  https://www.deeplearningbook.org/contents/applications.html    1oOKup.\\n\\n1 OlMer Words, IU Call De viewed as a local NOUparamevric preaicvor, sumuar to k-nearest neighbors. The statistical problems facing these extremely local pre- dictors are described in section 5.11.2. 'The problem for a language model is even more severe than usual, because any two different words have the same distance  from each other in one-hot vector space. It is thus difficult to leverage much information from any “neighbors’—only training examples that repeat literally the same context are useful for local generalization. To overcome these problems, a language model must be able to share knowledge between one word and other semantically similar words. To improve the statistical efficiency of n-gram models, class-based language models  introduce the notion of word categories and then share statistical strength between words that are in the same category. The idea is to use a clustering algorithm to partition the set of words into clusters or classes, based on their co-occurrence frequencies with other words\",\n",
       " '20019da6-4a91-4f06-a844-131a6f5b3bff': ', |T|, with leaf node τ representing a region Rτ of input space having Nτ data points, and |T| denoting the total number of leaf nodes.\\n\\nThe optimal prediction for region Rτ is then given by and the corresponding contribution to the residual sum-of-squares is then The pruning criterion is then given by The regularization parameter λ determines the trade-off between the overall residual sum-of-squares error and the complexity of the model as measured by the number |T| of leaf nodes, and its value is chosen by cross-validation. For classiﬁcation problems, the process of growing and pruning the tree is similar, except that the sum-of-squares error is replaced by a more appropriate measure of performance. If we deﬁne pτk to be the proportion of data points in region Rτ assigned to class k, where k = 1, . , K, then two commonly used choices are the cross-entropy These both vanish for pτk = 0 and pτk = 1 and have a maximum at pτk = 0.5. They encourage the formation of regions in which a high proportion of the data points are assigned to one class',\n",
       " '6a2f4b26-53fa-433f-b30c-9541cea22ddb': 'The dashed lines in Fig. 9 show that as ϵ decreases, the number of selected correlations follows a pattern. Generally, the number of correlations grows slowly at ﬁrst, then hits an “elbow point” beyond which the number explodes, which ﬁts the assumption that the correlation structure is sparse. In all three cases, setting ϵ to this elbow point is a safe trade-off between predictive performance and computational cost. In cases where performance grows consistently (left and right), the elbow point achieves most of the predictive performance gains at a small fraction of the computational cost. For example,onSpouses(right),choosingϵ = 0.08achievesascoreof 56.6 F1—within one point of the best score—but only takes 8 min for parameter estimation. In cases where predictive performance eventually degrades (middle), the elbow point also selects a relatively small number of correlations, giving an 0.7 F1 point improvement and avoiding overﬁtting',\n",
       " '961a190f-92f4-4e2f-ab7a-9c7ea38baffd': 'We can express any probability distribution of the form p(y; 0) or p( ;0)  https://www.deeplearningbook.org/contents/generative_models.html    as Ply |W), Where W 1s a varlable contaiming both parameters 9, and 11 applicanle, the inputs #. Given a value y sampled from distribution p(y | w), where W may in turn be a function of other variables, we can rewrite  y ~ vly | w) (20.56)  as y = f(z), (20.57)  where z is a source of randomness. We may then compute the derivatives of y with respect to w using traditional tools such as the back-propagation algorithm applied to f, as long as f is continuous and differentiable almost everywhere. Crucially, w must not be a function of z, and z must not be a function of w. This technique is often called the reparametrization trick, stochastic back-propagation, or perturbation analysis. The requirement that f be continuous and differentiable of course requires y to be continuous',\n",
       " '2a9d4901-f201-428f-87f0-d9fc68e7b55e': '\"\".Ioos, which \"\\'l\\'fesoots!he s.um-ol·SQ\"\",es distortlon J i<*~ by projecti<Xl the data onto a p<incipal componenl slll>spaee \\'\" dimensionalitv M. FIIIUr. 1:1:.5 An \",>gi\",,1 ~mpIe Irom lI>e 011·_ digils data ...ttOll\"1her with its PeA re<:onstnxlions oblair...:! by \\'e1aio\"li!Xl ,If j)<incipal ~n1S 10< various val,,\" 01 ,If. As ,II increason !tie re<:onst,uctiOfI ~s more ao::urate and woukl ~ portee! when .-If K D ~ 28 x 28 ~ . \"-1',\n",
       " '42eaf789-d459-4b13-bf52-09a569316a72': 'Gaussians, we ﬁrst chose one of the components at random with probability given by the mixing coefﬁcients πk and then generate a sample vector x from the corresponding Gaussian component.\\n\\nThis process is repeated N times to generate a data set of N independent samples. In the case of the hidden Markov model, this procedure is modiﬁed as follows. We ﬁrst choose the initial latent variable z1 with probabilities governed by the parameters πk and then sample the corresponding observation x1. Now we choose the state of the variable z2 according to the transition probabilities p(z2|z1) using the already instantiated value of z1. Thus suppose that the sample for z1 corresponds to state j. Then we choose the state k of z2 with probabilities Ajk for k = 1, . , K. Once we know z2 we can draw a sample for x2 and also sample the next latent variable z3 and so on. This is an example of ancestral sampling for a directed graphical model',\n",
       " '1668a9f2-5d7e-4cca-9db4-5796bfd48773': 'We can then interpret the average over these one-hot codes as giving a probability distribution over classes. As a nonparametric learning algorithm, k-nearest neighbor can achieve very high capacity.\\n\\nFor example, suppose we have a multiclass classification task and measure performance with 0-1 loss. In this setting, 1-nearest neighbor converges to double the Bayes error as the number of training examples approaches infinity. The error in excess of the Bayes error results from choosing a single neighbor by breaking ties between equally distant neighbors randomly. When there is infinite training data, all test points x will have infinitely many training set neighbors at distance zero. If we allow the algorithm to use all these neighbors to vote, rather than randomly choosing one of them, the procedure converges to the Bayes error rate. The high capacity of k-nearest neighbors enables it to obtain high accuracy given a large training set. It does so at high computational cost, however, and it may generalize very badly given a small finite training set. One weakness of k-nearest neighbors is that it cannot learn that one feature is more discriminative than another',\n",
       " '1e8e90d7-f261-4ee3-bd82-e9250ec61535': 'Whereas the tile coding input was derived from the contents of the transaction queue, the constraint sets depended on a host of other features related to timing and resource constraints that had to be satisﬁed by the hardware implementation of the entire system. In this way, the action constraints ensured that the learning algorithm’s exploration could not endanger the integrity of the physical system, while learning was e↵ectively limited to a “safe” region of the much larger state space of the hardware implementation. Because an objective of this work was that the learning controller could be implemented on a chip so that learning could occur online while a computer is running, hardware implementation details were important considerations.\\n\\nThe design included two ﬁve-stage pipelines to calculate and compare two action values at every processor clock cycle, and to update the appropriate action value. This included accessing the tile coding which was stored on-chip in static RAM. For the conﬁguration ˙Ipek et al. simulated, which was a 4GHz 4-core chip typical of high-end workstations at the time of their research, there were 10 processor cycles for every DRAM cycle. Considering the cycles needed to ﬁll the pipes, up to 12 actions could be evaluated in each DRAM cycle',\n",
       " '81e1c4fe-8ba5-445e-9d2a-9ce92c3bd21e': 'We develop them ﬁrst for the on-policy case then extend them to o↵-policy learning.\\n\\nOur treatment pays special attention to the case of linear function approximation, for which the results with eligibility traces are stronger. All these results apply also to the tabular and state aggregation cases because these are special cases of linear function approximation. In Chapter 7 we deﬁned an n-step return as the sum of the ﬁrst n rewards plus the estimated value of the state reached in n steps, each appropriately discounted (7.1). The general form of that equation, for any parameterized function approximator, is Gt:t+n .= Rt+1 +γRt+2 +· · ·+γn−1Rt+n +γnˆv(St+n,wt+n−1), 0 \\uf8ff t \\uf8ff T −n, (12.1) where ˆv(s,w) is the approximate value of state s given weight vector w (Chapter 9), and T is the time of episode termination, if any',\n",
       " 'c65f1f15-ed59-43ef-bced-af39234420e8': 'shows that increasing the number of parameters in layers of convolutional networks without increasing their depth is not nearly as effective at increasing test set performance, as illustrated in this figure. The legend indicates the depth of network used to make each curve and whether the curve represents variation in the size of the convolutional or the fully connected layers. We observe that shallow models in this context overfit at around 20 million parameters while deep ones can benefit from having over 60 million. This suggests that using a deep model expresses a useful preference over the space of functions the model can learn. Specifically, it expresses a belief that the function should consist of many simpler functions composed together. This could result either in learning a representation that is composed in turn of simpler representations (e.g., corners defined in terms of edges) or in learning a program with sequentially dependent steps (e.g., first locate a set of objects, then segment them from each other, then recognize them)',\n",
       " '382de4c2-a448-4b98-b39c-a86e87b5aa60': 'This multivariate probability density must satisfy in which the integral is taken over the whole of x space. We can also consider joint probability distributions over a combination of discrete and continuous variables. Note that if x is a discrete variable, then p(x) is sometimes called a probability mass function because it can be regarded as a set of ‘probability masses’ concentrated at the allowed values of x. The sum and product rules of probability, as well as Bayes’ theorem, apply equally to the case of probability densities, or to combinations of discrete and continuous variables.\\n\\nFor instance, if x and y are two real variables, then the sum and product rules take the form A formal justiﬁcation of the sum and product rules for continuous variables  requires a branch of mathematics called measure theory and lies outside the scope of this book. Its validity can be seen informally, however, by dividing each real variable into intervals of width ∆ and considering the discrete probability distribution over these intervals. Taking the limit ∆ → 0 then turns sums into integrals and gives the desired result. One of the most important operations involving probabilities is that of ﬁnding weighted averages of functions',\n",
       " '10372319-1b1b-48a7-86a2-3492dd840ea6': 'If color space transforms repeatedly change the color space such that the model cannot recognize red blood from green paint, the model will perform poorly on Image Sentiment Analysis. In effect, color space transformations will eliminate color biases present in the dataset in favor of spa- tial characteristics. However, for some tasks, color is a very important distinctive  feature.\\n\\nShorten and Khoshgoftaar J Big Data  6:60   Table 1 Results of Taylor and Nitschke’s Data Augmentation experiments on Caltech101   Top-1 accuracy (%) Top-5 accuracy (%)  Baseline 48.134042 64.50+0.65 Flipping 49.7341.13 67.36 + 138 Rotating 50.80 + 0.63 69.414048 Cropping 61.95+ 1.01 79.104 0.80 Color Jittering 49.57 £0.53 67.1840.42 Edge Enhancement 49.29 + 1.16 66.49 + 0.84 Fancy PCA 49.41 40.84 67.544 1.01  Their results find that the cropping geometric transformation results in the most accurate classifier The italic value denote high performance according to the comparative metrics  Geometric versus photometric transformations  Taylor and Nitschke  provide a comparative study on the effectiveness of geometric and photometric (color space) transformations',\n",
       " 'f543a538-3ddf-4dac-a786-8fa335cd0808': 'The resulting value F(ξ⋆) represents the tightest bound within this family of bounds and can be used as an approximation to I. This optimized bound, however, will in general not be exact. Although the bound σ(a) ⩾ f(a, ξ) on the logistic sigmoid can be optimized exactly, the required choice for ξ depends on the value of a, so that the bound is exact for one value of a only. Because the quantity F(ξ) is obtained by integrating over all values of a, the value of ξ⋆ represents a compromise, weighted by the distribution p(a). We now illustrate the use of local variational methods by returning to the Bayesian logistic regression model studied in Section 4.5. There we focussed on the use of the Laplace approximation, while here we consider a variational treatment based on the approach of Jaakkola and Jordan . Like the Laplace method, this also leads to a Gaussian approximation to the posterior distribution',\n",
       " '097bd2cf-88ff-4a55-825f-3ee86dcbe701': 'The other is to develop true gradient methods that do not rely on any special distribution for stability. We present methods based on both approaches. This is a cutting-edge research area, and it is not clear which of these approaches is most e↵ective in practice. We begin by describing how the methods developed in earlier chapters for the o↵policy case extend readily to function approximation as semi-gradient methods. These methods address the ﬁrst part of the challenge of o↵-policy learning (changing the update targets) but not the second part (changing the update distribution). Accordingly, these methods may diverge in some cases, and in that sense are not sound, but still they are often successfully used. Remember that these methods are guaranteed stable and asymptotically unbiased for the tabular case, which corresponds to a special case of function approximation. So it may still be possible to combine them with feature selection methods in such a way that the combined system could be assured stable. In any event, these methods are simple and thus a good place to start',\n",
       " 'db8a3af2-5e28-4b54-b535-4a4687b61fbd': 'In an artificial neural network, we can just display an image of the convolution kernel to see what the corresponding channel of a convolutional layer responds to. In a biological neural network, we do not have access to the weights themselves. Instead, we put an electrode in the neuron, display several samples of white noise images in front of the animal’s retina, and record how each of these samples causes the neuron to activate. We can then fit a linear model to these responses to obtain an approximation of the neuron’s weights. This approach is known as reverse correlation . Reverse correlation shows us that most V1 cells have weights that are described by Gabor functions. The Gabor function describes the weight at a 2-D point in the image. We can think of an image as being a function of 2-D coordinates, I(x,y). Likewise, we can think of a simple cell as sampling the image at a set of locations, defined by a set of x coordinates X and a set of y coordinates Y, then applying weights that are also a function of the location, w(z,y)',\n",
       " 'd5720a9f-77cd-4b5d-b4d4-30cadede7c98': 'TD errors like (6.5) are special kinds of RPEs that signal discrepancies between current and earlier expectations of reward over the long-term. When neuroscientists refer to RPEs they generally (though not always) mean TD RPEs, which we simply call TD errors throughout this chapter. Also in this chapter, a TD error is generally one that does not depend on actions, as opposed to TD errors used in learning action-values by algorithms like Sarsa and Q-learning. This is because the most well-known links to neuroscience are stated in terms of action-free TD errors, but we do not mean to rule out possible similar links involving action-dependent TD errors. (TD errors for predicting signals other than rewards are useful too, but that case will not concern us here. See, for example, Modayil, White, and Sutton, 2014.) One can ask many questions about links between neuroscience data and these theoreticallydeﬁned signals',\n",
       " '024f51c3-9e3b-47f2-b108-014f3b638012': '500 epochs 14) to obtain the optimal result, while ResNet-50 (4×) does not beneﬁt from longer training. B.4. Understanding The Non-Linear Projection Head 13It is 80.1% top-1 / 95.2% top-5 without broader augmentations for pretraining SimCLR. 14With AutoAugment , optimal test accuracy can be achieved between 900 and 500 epochs. A Simple Framework for Contrastive Learning of Visual Representations B.5. Semi-supervised Learning via Fine-Tuning Fine-tuning Procedure We ﬁne-tune using the Nesterov momentum optimizer with a batch size of 4096, momentum of 0.9, and a learning rate of 0.8 (following LearningRate = 0.05×BatchSize/256) without warmup. Only random cropping (with random left-to-right ﬂipping and resizing to 224x224) is used for preprocessing. We do not use any regularization (including weight decay). For 1% labeled data we ﬁne-tune for 60 epochs, and for 10% labeled data we ﬁne-tune for 30 epochs',\n",
       " '7e3f3dc6-5c3c-403a-8879-3f1876f4d04b': 'Di↵erential semi-gradient n-step Sarsa for estimating ˆq ⇡ q⇡ or q⇤ Input: a di↵erentiable function ˆq : S ⇥ A ⇥ Rd ! R, a policy ⇡ Initialize value-function weights w 2 Rd arbitrarily (e.g., w = 0) Initialize average-reward estimate ¯R 2 R arbitrarily (e.g., ¯R = 0) Algorithm parameters: step size ↵, β > 0, a positive integer n All store and access operations (St, At, and Rt) can take their index mod n + 1 parameter on the average reward, β, needs to be quite small so that ¯R becomes a good long-term estimate of the average reward. Unfortunately, ¯R will then be biased by its initial value for many steps, which may make learning ineﬃcient.\\n\\nAlternatively, one could use a sample average of the observed rewards for ¯R. That would initially adapt rapidly but in the long run would also adapt slowly. As the policy slowly changed, ¯R would also change; the potential for such long-term nonstationarity makes sample-average methods ill-suited',\n",
       " '2e19b7e2-a867-4bbb-98eb-c0485cfe7666': \"Policy Evaluation Policy Evaluation is to compute the state-value V_ for a given policy 7:  https://lilianweng.github.io/posts/2018-02-19-rl-overview/  A (Long) Peek into Reinforcement Learning | Lil'Log   Visa(s) = Eg = $0 a(a|s) $0 P(s',r|s,a)(r + 9Vi(s'))  a 3! Policy Improvement  Based on the value functions, Policy Improvement generates a better policy 7’ > 7 by acting greedily. Q,(8, a) = eRe + W-(St41) St = 8,Ar= a] = S> P(s',r\\\\s, a)(r + 1V,(8'))  3! Policy Iteration  The Generalized Policy Iteration (GPI) algorithm refers to an iterative procedure to improve the policy when combining policy evaluation and improvement. evaluation improve evaluation improve evaluation improve evaluation  Tr 70 > Ty > Vi, > 1, dee > Ts V,  In GPI, the value function is approximated repeatedly to be closer to the true value of the current policy and in the meantime, the policy is improved repeatedly to approach optimality\",\n",
       " 'c9ab1b84-d5bb-4585-bb9e-ed6d1b653257': 'For the given value of slope λ the contact point of the tangent line having the same slope is found by minimizing with respect to x the discrepancy (shown by the green dashed lines) given by f(x) − λx. This deﬁnes the dual function g(λ), which corresponds to the (negative of the) intercept of the tangent line having slope λ. exp(−x), we therefore obtain the tangent line in the form which is a linear function parameterized by ξ. For consistency with subsequent discussion, let us deﬁne λ = − exp(−ξ) so that Different values of λ correspond to different tangent lines, and because all such lines are lower bounds on the function, we have f(x) ⩾ y(x, λ). Thus we can write the function in the form We have succeeded in approximating the convex function f(x) by a simpler, linear function y(x, λ).\\n\\nThe price we have paid is that we have introduced a variational parameter λ, and to obtain the tightest bound we must optimize with respect to λ. We can formulate this approach more generally using the framework of convex duality',\n",
       " 'dec4ca5f-8acb-4882-b24d-d89fc4eb4588': 'Since the classifier should be invariant to the local factors of variation that correspond to movement on the manifold, it would make sense to use as nearest neighbor distance between points a, and a the distance between the manifolds M, and M) to which they respectively belong.\\n\\nAlthough that may be computationally difficult (it would require solving an optimization problem, to find the nearest pair of points on M, and Mp), a cheap alternative that makes sense locally is to approximate M; by its tangent plane at a; and measure the distance between the two tangents, or between  a banned Hla 2d 2 Ate MLA Ane Le 2 Ane dd Lee net 2 1a At tnd  https://www.deeplearningbook.org/contents/regularization.html    @ LALBCUL Plalle allLU a PUILLLL. Liab Call VC ACLUICVEU Vy SUILVILL @ 1OW-ULLEMSIONAL linear system (in the dimension of the manifolds). Of course, this algorithm requires one to specify the tangent vectors',\n",
       " '0e5fbd15-36ca-49e4-a028-d4ee2f30cbc1': 'Furthermore, the network must also exhibit invariance to more subtle transformations such as elastic deformations of the kind illustrated in Figure 5.14. One simple approach would be to treat the image as the input to a fully connected network, such as the kind shown in Figure 5.1. Given a sufﬁciently large training set, such a network could in principle yield a good solution to this problem and would learn the appropriate invariances by example.\\n\\nHowever, this approach ignores a key property of images, which is that nearby pixels are more strongly correlated than more distant pixels. Many of the modern approaches to computer vision exploit this property by extracting local features that depend only on small subregions of the image. Information from such features can then be merged in later stages of processing in order to detect higher-order features and ultimately to yield information about the image as whole. Also, local features that are useful in one region of the image are likely to be useful in other regions of the image, for instance if the object of interest is translated. These notions are incorporated into convolutional neural networks through three mechanisms: (i) local receptive ﬁelds, (ii) weight sharing, and (iii) subsampling. The structure of a convolutional network is illustrated in Figure 5.17',\n",
       " '9c6c2809-386d-407e-86ef-a05471a00912': 'The usual approach to problems of this kind is to put a grid over the area covered by the surface and solve for its height at the grid points by an iterative computation. Grid points at the boundary are forced to the wire frame, and all others are adjusted toward the average of the heights of their four nearest neighbors. This process then iterates, much like DP’s iterative policy evaluation, and ultimately converges to a close approximation to the desired surface. This is similar to the kind of problem for which Monte Carlo methods were originally designed. Instead of the iterative computation described above, imagine standing on the surface and taking a random walk, stepping randomly from grid point to neighboring grid point, with equal probability, until you reach the boundary. It turns out that the expected value of the height at the boundary is a close approximation to the height of the desired surface at the starting point (in fact, it is exactly the value computed by the iterative method described above).\\n\\nThus, one can closely approximate the height of the surface at a point by simply averaging the boundary heights of many walks started at the point',\n",
       " '387465a2-536c-453d-87a0-87ddfa92baa3': 'The gradient and the Hessian for the vector wk are given by Section 4.3.3 where ∇k denotes the gradient with respect to wk.\\n\\nFor ﬁxed γnk, these are independent of {wj} for j ̸= k and so we can solve for each wk separately using the IRLS algorithm. Thus the M-step equations for component k correspond simply to ﬁtting Section 4.3.3 a single logistic regression model to a weighted data set in which data point n carries a weight γnk. Figure 14.10 shows an example of the mixture of logistic regression models applied to a simple classiﬁcation problem. The extension of this model to a mixture of softmax models for more than two classes is straightforward. Exercise 14.16 In Section 14.5.1, we considered a mixture of linear regression models, and in Section 14.5.2 we discussed the analogous mixture of linear classiﬁers. Although these simple mixtures extend the ﬂexibility of linear models to include more complex (e.g., multimodal) predictive distributions, they are still very limited',\n",
       " 'c12177f8-66ec-4990-b09c-db307ed99471': 'An early important experiment of this type was conducted by Adams and Dickinson . They trained rats via instrumental conditioning until the rats energetically pressed a lever for sucrose pellets in a training chamber. The rats were then placed in the same chamber with the lever retracted and allowed non-contingent food, meaning that pellets were made available to them independently of their actions. After 15-minutes of this free-access to the pellets, rats in one group were injected with the nausea-inducing poison lithium chloride. This was repeated for three sessions, in the last of which none of the injected rats consumed any of the non-contingent pellets, indicating that the reward value of the pellets had been decreased—the pellets had been devalued. In the next stage taking place a day later, the rats were again placed in the chamber and given a session of extinction training, meaning that the response lever was back in place but disconnected from the pellet dispenser so that pressing it did not release pellets.\\n\\nThe question was whether the rats that had the reward value of the pellets decreased would lever-press less than rats that did not have the reward value of the pellets decreased, even without experiencing the devalued reward as a result of lever-pressing',\n",
       " 'f1a429e7-8a0b-448e-94d8-efba1f8242a3': 'We can think of the penalty Q(h) simply as a regularizer term added to a feedforward network whose primary task is to copy the input to the output (unsupervised learning objective) and possibly also perform some supervised task (with a supervised learning objective) that depends on these sparse features. Unlike other regularizers, such as weight decay, there is not a straightforward Bayesian interpretation to this regularizer.\\n\\nAs described in section 5.6.1, training with weight decay and other regularization penalties can be interpreted as a MAP approximation to Bayesian inference, with the added regularizing penalty  https://www.deeplearningbook.org/contents/autoencoders.html    corresponding to a prior probability distribution over the model parameters, In his view, regularized maximum likelihood corresponds to maximizing P x), which is equivalent to maximizing log p(x | 8) + logp(@). The log p(a | 6) term is the usual data log-likelihood term, and the log p(@) term, the log-prior over parameters, incorporates the preference over particular values of 0. This view is described in section 5.6',\n",
       " 'd557b405-330d-4dbc-96c9-17e2f34030d1': ', Gn−1, all starting in the same state and keep it up-to-date as we obtain a single additional return Gn. In addition to keeping track of Vn, we must maintain for each state the cumulative sum Cn of the weights given to the ﬁrst n returns. The update rule for Vn is where C0 .= 0 (and V1 is arbitrary and thus need not be speciﬁed). The box on the next page contains a complete episode-by-episode incremental algorithm for Monte Carlo policy evaluation. The algorithm is nominally for the o↵-policy case, using weighted importance sampling, but applies as well to the on-policy case just by choosing the target and behavior policies as the same (in which case (⇡ = b), W is always 1). The approximation Q converges to q⇡ (for all encountered state–action pairs) while actions are selected according to a potentially di↵erent policy, b',\n",
       " 'efb3df89-ed04-4ff8-88da-e7930d207d3c': 'Russell, S., Norvig, P. Artiﬁcial Intelligence: A Modern Approach, 3rd edition. PrenticeRusso, D. J., Van Roy, B., Kazerouni, A., Osband, I., Wen, Z. A tutorial on Thompson sampling, Foundations and Trends in Machine Learning. ArXiv:1707.02038. Rust, J. Numerical dynamic programming in economics. In H. Amman, D. Kendrick, and J. Rust (Eds. ), Handbook of Computational Economics, pp. 614–722. Elsevier, Amsterdam. dopamine release dynamics in the nucleus accumbens core and shell reveal complementary signals for error prediction and incentive motivation. The Journal of Neuroscience, 35(33):11572–11582. Saksida, L. M., Raymond, S. M., Touretzky, D. S. Shaping robot behavior using principles from instrumental conditioning',\n",
       " 'f3c9e099-50ac-4d5a-919b-814c925208e6': 'MA we dawnt nnd 4h AMR AF ALR AR RA HAAR RAR AI AAR ALR BI RIA RAR AE  https://www.deeplearningbook.org/contents/rnn.html    LU ULLUCLSLALLU LLC C1LCCL VI LLC Specular LAULUDS, CULLSIUCL LLC SUL pie Case VL back-propagation with a Jacobian matrix J that does not change with t. ‘This case happens, for example, when the network is purely linear. Suppose that J has an eigenvector v with corresponding eigenvalue 1. Consider what happens as we  propagate a gradient vector backward through time. If we begin with a gradient vector g, then after one step of back-propagation, we will have Jg, and after n steps we will have J”g.\\n\\nNow consider what happens if we instead back-propagate a perturbed version of g. If we begin with g + dv, then after one step, we will have J(g + dv). After n steps, we will have J\"(g + dv)',\n",
       " 'd367a149-8af3-434e-bb09-749c974af91c': 'The precise form of the function y(x) is determined during the training phase, also known as the learning phase, on the basis of the training data.\\n\\nOnce the model is trained it can then determine the identity of new digit images, which are said to comprise a test set. The ability to categorize correctly new examples that differ from those used for training is known as generalization. In practical applications, the variability of the input vectors will be such that the training data can comprise only a tiny fraction of all possible input vectors, and so generalization is a central goal in pattern recognition. For most practical applications, the original input variables are typically preprocessed to transform them into some new space of variables where, it is hoped, the pattern recognition problem will be easier to solve. For instance, in the digit recognition problem, the images of the digits are typically translated and scaled so that each digit is contained within a box of a ﬁxed size. This greatly reduces the variability within each digit class, because the location and scale of all the digits are now the same, which makes it much easier for a subsequent pattern recognition algorithm to distinguish between the different classes. This pre-processing stage is sometimes also called feature extraction',\n",
       " 'd40f7ef5-8545-4211-b88a-73041b73297f': 'What would the sequence of Rt+1 − ¯Rt errors be? What would the sequence of δt errors be (using (10.10))? Which error sequence would produce a more stable estimate of the average reward if the estimates were allowed to change in response to the errors? Why? ⇤ access control to a set of 10 servers. Customers of four di↵erent priorities arrive at a single queue. If given access to a server, the customers pay a reward of 1, 2, 4, or 8 to the server, depending on their priority, with higher priority customers paying more.\\n\\nIn each time step, the customer at the head of the queue is either accepted (assigned to one of the servers) or rejected (removed from the queue, with a reward of zero). In either case, on the next time step the next customer in the queue is considered. The queue never empties, and the priorities of the customers in the queue are equally randomly distributed. Of course a customer cannot be served if there is no free server; the customer is always rejected in this case. Each busy server becomes free with probability p = 0.06 on each time step. Although we have just described them for deﬁniteness, let us assume the statistics of arrivals and departures are unknown',\n",
       " '90d6a371-a3ec-4e43-aa1d-b979998ec39b': 'Section 15.4 describes how this view motivates the use of distributed representations, with separate directions in representation space corresponding to separate factors of variation.\\n\\ne Causal factors: The model is constructed in such a way that it treats the factors of variation described by the learned representation h as the causes of the observed data x, and not vice versa. As discussed in section 15.3, this  https://www.deeplearningbook.org/contents/representation.html    1s advantageous tor semi-supervised learning and makes the learned model more robust when the distribution over the underlying causes changes or when we use the model for a new task. e Depth, or a hierarchical organization of explanatory factors: High-level, abstract concepts can be defined in terms of simple concepts, forming a hierarchy. From another point of view, the use of a deep architecture expresses our belief that the task should be accomplished via a multistep program, with each step referring back to the output of the processing accomplished via previous steps',\n",
       " '1921098b-a381-4225-9aa0-4daa53aa2844': 'Any local minimum is  https://www.deeplearningbook.org/contents/optimization.html    uaranteed to be a global minimum. Some convex functions have a flat region at the bottom rather than a single global minimum point, but any point within such a flat region is an acceptable solution. When optimizing a convex function, we  know that we have reached a good solution if we find a critical point of any kind.\\n\\nWith nonconvex functions, such as neural nets, it is possible to have many local minima. Indeed, nearly any deep model is essentially guaranteed to have an extremely large number of local minima. As we will see, however, this is not necessarily a major problem. Neural networks and any models with multiple equivalently parametrized latent variables all have multiple local minima because of the model identifiability problem. A model is said to be identifiable if a sufficiently large training set can rule out all but one setting of the model’s parameters. Models with latent variables are often not identifiable because we can obtain equivalent models by exchanging latent variables with each other',\n",
       " 'a0ac8575-9338-4d32-b49e-e6b7ab1be114': 'The policy gradient theorem gives an exact expression proportional to the gradient; all that is needed is some way of sampling whose expectation equals or approximates this expression. Notice that the right-hand side of the policy gradient theorem is a sum over states weighted by how often the states occur under the target policy ⇡; if ⇡ is followed, then states will be encountered in these proportions. Thus We could stop here and instantiate our stochastic gradient-ascent algorithm (13.1) as where ˆq is some learned approximation to q⇡.\\n\\nThis algorithm, which has been called an all-actions method because its update involves all of the actions, is promising and deserving of further study, but our current interest is the classical REINFORCE algorithm  whose update at time t involves just At, the one action actually taken at time t. We continue our derivation of REINFORCE by introducing At in the same way as we introduced St in (13.6)—by replacing a sum over the random variable’s possible values by an expectation under ⇡, and then sampling the expectation',\n",
       " 'd93beefc-2746-4390-8a13-763c76e4ffc7': 'Frey and Morris  proposed the idea of a “synaptic tag” for the induction of long-lasting strengthening of synaptic eﬃcacy. Though not unlike Klopf’s eligibility, their tag was hypothesized to consist of a temporary strengthening of a synapse that could be transformed into a long-lasting strengthening by subsequent neuron activation. The model of O’Reilly and Frank  and O’Reilly, Frank, Hazy, and Watz  uses working memory to bridge temporal intervals instead of eligibility traces. Wickens and Kotter  discuss possible mechanisms for synaptic eligibility. He, Huertas, Hong, Tie, Hell, Shouval, Kirkwood  provide evidence supporting the existence of contingent eligibility traces in synapses of cortical neurons with time courses like those of the eligibility traces Klopf postulated. The metaphor of a neuron using a learning rule related to bacterial chemotaxis was discussed by Barto .\\n\\nKoshland’s extensive study of bacterial chemotaxis was in part motivated by similarities between features of bacteria and features of neurons . See also Berg',\n",
       " '9795695e-55ab-4efd-bdce-5a761f0c8559': 'It can readily be extended to multiple target variables represented by the vector t, in which case the optimal solution is the conditional average y(x) = Et. Exercise 1.25 We can also derive this result in a slightly different way, which will also shed light on the nature of the regression problem. Armed with the knowledge that the optimal solution is the conditional expectation, we can expand the square term as follows {y(x) − t}2 = {y(x) − E + E − t}2 where, to keep the notation uncluttered, we use E to denote Et. Substituting into the loss function and performing the integral over t, we see that the cross-term vanishes and we obtain an expression for the loss function in the form The function y(x) we seek to determine enters only in the ﬁrst term, which will be minimized when y(x) is equal to E, in which case this term will vanish. This is simply the result that we derived previously and that shows that the optimal least squares predictor is given by the conditional mean. The second term is the variance of the distribution of t, averaged over x.\\n\\nIt represents the intrinsic variability of the target data and can be regarded as noise',\n",
       " 'fadb4025-edfd-490a-9789-fdaf04e88b19': 'The value of this way of behaving is The key criterion is whether this is greater than or less than v⇡(s).\\n\\nIf it is greater—that is, if it is better to select a once in s and thereafter follow ⇡ than it would be to follow ⇡ all the time—then one would expect it to be better still to select a every time s is encountered, and that the new policy would in fact be a better one overall. That this is true is a special case of a general result called the policy improvement theorem. Let ⇡ and ⇡0 be any pair of deterministic policies such that, for all s 2 S, Then the policy ⇡0 must be as good as, or better than, ⇡. That is, it must obtain greater or equal expected return from all states s 2 S: Moreover, if there is strict inequality of (4.7) at any state, then there must be strict inequality of (4.8) at that state',\n",
       " '7b6bae98-e6cb-4663-8634-fc250fe04bf9': 'Taking the sentiment attribute, for example, given a sentence x (e.g., a customer’s review “the manager is a horrible person”) and a target sentiment a (e.g., positive), the goal of the problem is to generate a new sentence y that (1) possesses the target sentiment, (2) preserves all other characteristics of the original sentence, and (3) is ﬂuent (e.g., the transferred sentence “the manager is a perfect person”).\\n\\nTo learn an attribute transfer model pθ(y|x, a), a key challenge of the problem is the lack of direct supervision data (i.e., pairs of sentences that are exact the same except for sentiment), making it necessary to use other forms of experience. Here we brieﬂy describe an approach originally presented in Hu et al. and Yang et al. , highlighting how the approach can be built mechanically, by formulating relevant experience directly based on the problem deﬁnition and then plugging them into the SE. We can identify three types of experience, corresponding to the above three desiderata, respectively. First, the model needs to learn the concept of ‘sentiment’ to be able to modify the attribute of text',\n",
       " '98fc676b-c893-4a39-a3c1-71ea84841d60': 'The molecular mechanisms producing these traces, as well as the much shorter traces that likely underly STDP, are not yet understood, but research focusing on time-dependent and neuromodulator-dependent synaptic plasticity is continuing.\\n\\nThe neuron-like actor unit that we have described here, with its Law-of-E↵ect-style learning rule, appeared in somewhat simpler form in the actor–critic network of Barto et al. That network was inspired by the “hedonistic neuron” hypothesis proposed by physiologist A. H. Klopf . Not all the details of Klopf’s hypothesis are consistent with what has been learned about synaptic plasticity, but the discovery of STDP and the growing evidence for a reward-modulated form of STDP suggest that Klopf’s ideas may not have been far o↵ the mark. We discuss Klopf’s hedonistic neuron hypothesis next',\n",
       " 'a1248576-e6ae-4b81-88c2-b078f062d8ac': 'Let us simply assume that all models are given equal prior probability. The interesting term is the model evidence p(D|Mi) which expresses the preference shown by the data for different models, and we shall examine this term in more detail shortly. The model evidence is sometimes also called the marginal likelihood because it can be viewed as a likelihood function over the space of models, in which the parameters have been marginalized out. The ratio of model evidences p(D|Mi)/p(D|Mj) for two models is known as a Bayes factor',\n",
       " '0a3a07ce-228e-48be-8057-a1eeb49069d8': 'd., Li, Y., and Vinyals, O. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. Parkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar, C. Cats and dogs. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3498–3505. IEEE, 2012. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211–252, 2015. Schroff, F., Kalenichenko, D., and Philbin, J. Facenet: A uniﬁed embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp',\n",
       " 'baf4ba93-2421-4b55-8083-478fcf6d4ab3': 'PCA learns a representation that has lower dimensionality than the original input. It also learns a representation whose elements have no linear correlation with each other. This is a first step toward the criterion of learning representations whose elements are statistically independent. To achieve full independence, a representation learning algorithm must also remove the nonlinear relationships between variables. PCA learns an orthogonal, linear transformation of the data that projects an input x to a representation z as shown in figure 5.8. In section 2.12, we saw that we could learn a one-dimensional representation that best reconstructs the original data (in the sense of mean squared error) and that this representation actually corresponds to the first principal component of the data. Thus we can use PCA as a simple and effective dimensionality reduction method that preserves as much of the information in the data as possible (again, as measured by least-squares reconstruction error). In the following, we will study how the PCA representation decorrelates the original data representation X. Let us consider the m x n design matrix X',\n",
       " '085bba2d-fbe6-49a9-88c0-907b8e5114d6': 'Swee KL, Yi L, Ngoc-Trung T, Ngai-Man C, Gemma R, Yuval E. DOPING: generative data augmentation for unsuper- vised anomaly detection with GAN. arXiv preprint. 2018. Alireza M, Jonathon S, Navdeep J, lan G, Brendan F. Adversarial autoencoders. arXiv preprint. 2015.\\n\\nTim S, lan G, Wojciech Z, Vicki C, Alec R, Xi C. Improved techniques for training GANs. arXiv preprint. 2016. Yanghao L, Naiyan W, Jiaying L, Xiaodi H. Demistifying neural style transfer. arXiv preprint. 2017. Khizar H. Super-resolution via deep learning. arXiv preprint. 2017. Dmitry U, Andrea V, Victor L. Instance normalization: the missing ingredient for fast stylization',\n",
       " 'df14777b-a39d-4f4e-b862-e96e778ef011': 'Finally, in many of the application domains covered such as medical image analysis, the biases distancing the training data from the testing data are more complex than positional and transla- tional variances. Therefore, the scope of where and when geometric transformations  can be applied is relatively limited. Color space transformations  Image data is encoded into 3 stacked matrices, each of size height x width. These matri- ces represent pixel values for an individual RGB color value. Lighting biases are amongst the most frequently occurring challenges to image recognition problems. Therefore, the effectiveness of color space transformations, also known as photometric transforma- tions, is fairly intuitive to conceptualize. A quick fix to overly bright or dark images is to loop through the images and decrease or increase the pixel values by a constant value. Another quick color space manipulation is to splice out individual RGB color matrices. Another transformation consists of restricting pixel values to a certain min or max value.\\n\\nThe intrinsic representation of color in digital images lends itself to many strategies of augmentation. Color space transformations can also be derived from image-editing apps',\n",
       " '0d00de81-696a-4828-88ce-b0a4088cf5ef': 'Simulation, learning, and optimization techniques in Watson’s game strategies.\\n\\nIBM Journal of Research and Development, 56(3-4):16–1–16–11. Tesauro, G., Gondek, D. C., Lenchner, J., Fan, J., Prager, J. M. Analysis of Watson’s strategies for playing Jeopardy! Journal of Artiﬁcial Intelligence Research, 47:205–251. Tham, C. K. Modular On-Line Function Approximation for Scaling up Reinforcement Thathachar, M. A. L., Sastry, P. S. A new approach to the design of reinforcement Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, 36(6):711–722. Thathachar, M., Sastry, P. S. Networks of Learning Automata: Techniques for Online Stochastic Optimization. Springer Science & Business Media. Theocharous, G., Thomas, P. S., Ghavamzadeh, M',\n",
       " '8ddabb6b-9df3-48ee-a93c-68e82199303b': '(a) The restricted Boltzmann machine itself is an undirected graphical model based on a bipartite graph, with visible units in one part of the graph and hidden units in the other part.\\n\\nThere are no connections among the visible units, nor any connections among the hidden units. Typically every visible unit is connected to every hidden unit, but it is possible to construct sparsely connected RBMs such as convolutional RBMs. (b) A  https://www.deeplearningbook.org/contents/generative_models.html    deep beliet network 1s a hybrid graphical model involving both directed and undirected connections. Like an RBM, it has no intralayer connections. However, a. DBN has multiple hidden layers, and thus connections between hidden units that are in separate layers. All the local conditional probability distributions needed by the deep belief network are  copied directly from the local conditional probability distributions of its constituent RBMs. Alternatively, we could also represent the deep belief network with a completely undirected graph, but it would need intralayer connections to capture the dependencies between parents. (c) A deep Boltzmann machine is an undirected graphical model with several layers of latent variables',\n",
       " '81dd0688-fcbd-4862-b2fc-0ec207dada9b': 'Using the transformation rule (1.27) for densities we see that p(ln σ) = const. Thus, for this prior there is the same probability mass in the range 1 ⩽ σ ⩽ 10 as in the range 10 ⩽ σ ⩽ 100 and in 100 ⩽ σ ⩽ 1000. An example of a scale parameter would be the standard deviation σ of a Gaussian distribution, after we have taken account of the location parameter µ, because where �x = x − µ. As discussed earlier, it is often more convenient to work in terms of the precision λ = 1/σ2 rather than σ itself. Using the transformation rule for densities, we see that a distribution p(σ) ∝ 1/σ corresponds to a distribution over λ of the form p(λ) ∝ 1/λ. We have seen that the conjugate prior for λ was the gamma distribution Gam(λ|a0, b0) given by (2.146). The noninformative prior is obtained Section 2.3 as the special case a0 = b0 = 0',\n",
       " '95644829-e7ea-401c-8b31-6fddad14a66e': 'Before giving a proof, we ﬁrst discuss a generalization, known as the Metropolis-Hastings algorithm , to the case where the proposal distribution is no longer a symmetric function of its arguments. In particular at step τ of the algorithm, in which the current state is z(τ), we draw a sample z⋆ from the distribution qk(z|z(τ)) and then accept it with probability Ak(z⋆, zτ) where Here k labels the members of the set of possible transitions being considered. Again, the evaluation of the acceptance criterion does not require knowledge of the normalizing constant Zp in the probability distribution p(z) = �p(z)/Zp. For a symmetric proposal distribution the Metropolis-Hastings criterion (11.44) reduces to the standard Metropolis criterion given by (11.33).\\n\\nWe can show that p(z) is an invariant distribution of the Markov chain deﬁned by the Metropolis-Hastings algorithm by showing that detailed balance, deﬁned by (11.40), is satisﬁed. Using (11.44) we have as required. The speciﬁc choice of proposal distribution can have a marked effect on the performance of the algorithm',\n",
       " 'c37eb213-2a82-4ff7-ac02-850ed85d984b': 'Another option is to take only a single vector x as input. https://www.deeplearningbook.org/contents/rnn.html    When & 1s a nxed-size vector, we can simply make It an extra Input Of the HININ that generates the y sequence.\\n\\nSome common ways of providing an extra input to an RNN are  1. as an extra input at each time step, or 2. as the initial state h ©), or  3. both. The first and most common approach is illustrated in figure 10.9. The interaction between the input x and each hidden unit vector h) is parametrized by a newly introduced weight matrix R that was absent from the model of only the sequence of y values. The same product a! R is added as additional input to the hidden units at every time step. We can think of the choice of a as determining the value of a! R that is effectively a new bias parameter used for each of the hidden units. The weights remain independent of the input',\n",
       " '5303f67d-54a9-4ac7-88b1-f4e602a32baa': 'This approach has been successfully applied to neural language models .\\n\\n12.4.4 Combining Neural Language Models with n-grams  A major advantage of n-gram models over neural networks is that n-gram models achieve high model capacity (by storing the frequencies of very many tuples), while requiring very little computation to process an example (by looking up only a few tuples that match the current context). If we use hash tables or trees to access the counts, the computation used for n-grams is almost independent of capacity. In comparison, doubling a neural network’s number of parameters typically also roughly doubles its computation time. Exceptions include models that avoid using all parameters on each pass. Embedding layers index only a single embedding in each pass, so we can increase the vocabulary size without increasing the computation time per example. Some other models, such as tiled convolutional networks, can add parameters while reducing the degree of parameter sharing to maintain the same amount of computation. Typical neural network layers based on matrix multiplication, however, use an amount of computation proportional to  https://www.deeplearningbook.org/contents/applications.html    che number of parameters',\n",
       " '37b2671d-df93-440b-8980-6999127d180c': 'crafted a deep convolutional GAN (DCGAN) that performs very well for image synthesis tasks, and showed that its latent repre- sentation space captures important factors of variation, as shown in figure 15.9. See figure 20.7 for examples of images generated by a DCGAN generator. The GAN learning problem can also be simplified by breaking the generation process into many levels of detail.\\n\\nIt is possible to train conditional GANs  that learn to sample from a distribution p(x | y) rather than simply sampling from a marginal distribution p(a). Denton et al. showed that a series of conditional GANs can be trained to first generate a very low-resolution version of an image, then incrementally add details to the image. This technique is called the LAPGAN model, due to the use of a Laplacian pyramid to generate the images containing varying levels of detail. LAPGAN generators are able to fool not only discriminator networks but also human observers, with experimental subjects identifying up to 40 percent of the outputs of the network as being real data. See figure 20.7 for examples of images generated by a LAPGAN generator',\n",
       " '1a97a35d-2811-4202-b280-71a86b942a5f': 'If we change the sign of all of the weights and the bias feeding into a particular hidden unit, then, for a given input pattern, the sign of the activation of the hidden unit will be reversed, because ‘tanh’ is an odd function, so that tanh(−a) = − tanh(a). This transformation can be exactly compensated by changing the sign of all of the weights leading out of that hidden unit. Thus, by changing the signs of a particular group of weights (and a bias), the input–output mapping function represented by the network is unchanged, and so we have found two different weight vectors that give rise to the same mapping function. For M hidden units, there will be M such ‘sign-ﬂip’ symmetries, and thus any given weight vector will be one of a set 2M equivalent weight vectors .\\n\\nSimilarly, imagine that we interchange the values of all of the weights (and the bias) leading both into and out of a particular hidden unit with the corresponding values of the weights (and bias) associated with a different hidden unit. Again, this clearly leaves the network input–output mapping function unchanged, but it corresponds to a different choice of weight vector',\n",
       " 'e9068c75-7c8e-4c1c-abe6-056ceec4f4ff': 'We recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two models, as ELMo does.\\n\\nHowever: (a) this is twice as expensive as a single bidirectional model; (b) this is non-intuitive for tasks like QA, since the RTL model would not be able to condition the answer on the question; (c) this it is strictly less powerful than a deep bidirectional model, since it can use both left and right context at every layer. In this section, we explore the effect of model size on ﬁne-tuning task accuracy. We trained a number of BERT models with a differing number of layers, hidden units, and attention heads, while otherwise using the same hyperparameters and training procedure as described previously. Results on selected GLUE tasks are shown in Table 6. In this table, we report the average Dev Set accuracy from 5 random restarts of ﬁne-tuning. We can see that larger models lead to a strict accuracy improvement across all four datasets, even for MRPC which only has 3,600 labeled training examples, and is substantially different from the pre-training tasks',\n",
       " 'e0ebac1f-7775-4a4f-a359-d8e7c8e92ca2': 'Our driver is a bit unreliable, as expressed through the following probabilities Suppose that the driver tells us that the fuel gauge shows empty, in other words that we observe D = 0. Evaluate the probability that the tank is empty given only this observation.\\n\\nSimilarly, evaluate the corresponding probability given also the observation that the battery is ﬂat, and note that this second probability is lower. Discuss the intuition behind this result, and relate the result to Figure 8.54. M distinct random variables. Draw the 8 possibilities for the case of M = 3. function given by (8.42). Write down an expression for the difference in the values of the energy associated with the two states of a particular variable xj, with all other variables held ﬁxed, and show that it depends only on quantities that are local to xj in the graph. coefﬁcients β = h = 0. Show that the most probable conﬁguration of the latent variables is given by xi = yi for all i. nodes in the graph shown in Figure 8.38 is given by an expression of the form (8.58)',\n",
       " '946b3f3e-dd89-4abe-92f6-803b80618055': 'Suppose we have a joint distribution p(x, y) from which we draw pairs of values of x and y. If a value of x is already known, then the additional information needed to specify the corresponding value of y is given by − ln p(y|x). Thus the average additional information needed to specify y can be written as which is called the conditional entropy of y given x.\\n\\nIt is easily seen, using the product rule, that the conditional entropy satisﬁes the relation Exercise 1.37 where H is the differential entropy of p(x, y) and H is the differential entropy of the marginal distribution p(x). Thus the information needed to describe x and y is given by the sum of the information needed to describe x alone plus the additional information required to specify y given x. So far in this section, we have introduced a number of concepts from information theory, including the key notion of entropy. We now start to relate these ideas to pattern recognition. Consider some unknown distribution p(x), and suppose that we have modelled this using an approximating distribution q(x)',\n",
       " '8afcfa8c-2ab6-4123-bee7-1bbfa9d17d4c': 'In addition to accounting for many features of the phasic activity of dopamine neurons, the reward prediction error hypothesis links neuroscience to other aspects of reinforcement learning, in particular, to learning algorithms that use TD errors as reinforcement signals. Neuroscience is still far from reaching complete understanding of the circuits, molecular mechanisms, and functions of the phasic activity of dopamine neurons, but evidence supporting the reward prediction error hypothesis, along with evidence that phasic dopamine responses are reinforcement signals for learning, suggest that the brain might implement something like an actor–critic algorithm in which TD errors play critical roles.\\n\\nOther reinforcement learning algorithms are plausible candidates too, but actor–critic algorithms ﬁt the anatomy and physiology of the mammalian brain particularly well, as we describe in the following two sections. component that learns policies, and the ‘critic’ is the component that learns about whatever policy is currently being followed by the actor in order to ‘criticize’ the actor’s action choices. The critic uses a TD algorithm to learn the state-value function for the actor’s current policy. The value function allows the critic to critique the actor’s action choices by sending TD errors, δ, to the actor',\n",
       " 'a28979d5-4406-4ae5-a588-2979b8e10cec': 'Minimizing this KL divergence corresponds exactly to minimizing the cross- entropy between the distributions. Many authors use the term “cross-entropy” to identify specifically the negative log-likelihood of a Bernoulli or softmax distribution, but that is a misnomer. Any loss consisting of a negative log-likelihood is a cross- entropy between the empirical distribution defined by the training set and the probability distribution defined by model. For example, mean squared error is the cross-entropy between the empirical distribution and a Gaussian model. We can thus see maximum likelihood as an attempt to make the model dis- ribution match the empirical distribution pgata. Ideally, we would like to match he true data-generating distribution pgata, but we have no direct access to this distribution. While the optimal @ is the same regardless of whether we are maximizing the ikelihood or minimizing the KL divergence, the values of the objective functions are different. In software, we often phrase both as minimizing a cost function',\n",
       " '8b9b551a-12bb-49f1-8428-492713fc7547': '2.25 (⋆ ⋆) In Sections 2.3.1 and 2.3.2, we considered the conditional and marginal distributions for a multivariate Gaussian. More generally, we can consider a partitioning of the components of x into three groups xa, xb, and xc, with a corresponding partitioning of the mean vector µ and of the covariance matrix Σ in the form By making use of the results of Section 2.3, ﬁnd an expression for the conditional distribution p(xa|xb) in which xc has been marginalized out. By multiplying both sides by (A + BCD) prove the correctness of this result. 2.27 (⋆) Let x and z be two independent random vectors, so that p(x, z) = p(x)p(z). Show that the mean of their sum y = x + z is given by the sum of the means of each of the variable separately. Similarly, show that the covariance matrix of y is given by the sum of the covariance matrices of x and z. Conﬁrm that this result agrees with that of Exercise 1.10',\n",
       " 'dfffc46b-895c-4760-9ff7-d01067ad8618': 'This random variable can take one of two possible values, namely r (corresponding to the red box) or b (corresponding to the blue box).\\n\\nSimilarly, the identity of the fruit is also a random variable and will be denoted by F. It can take either of the values a (for apple) or o (for orange). To begin with, we shall deﬁne the probability of an event to be the fraction of times that event occurs out of the total number of trials, in the limit that the total number of trials goes to inﬁnity. Thus the probability of selecting the red box is 4/10 considering two random variables, X, which takes the values {xi} where i = 1, . , M, and Y , which takes the values {yj} where j = 1, . , L. In this illustration we have M = 5 and L = 3. If we consider a total number N of instances of these variables, then we denote the number of instances where X = xi and Y = yj by nij, which is the number of points in the corresponding cell of the array',\n",
       " '52b673a2-67fa-47bb-be55-82b86c413d3a': 'G., Veness, J., Bowling, M. Investigating contingency awareness using Intelligence (AAAI-12), pp. 864–871. AAAI Press, Menlo Park, CA. Bellman, R. E. A problem in the sequential design of experiments. Sankhya, 16:221–229. Bellman, R. E. Dynamic Programming. Princeton University Press, Princeton. Bellman, R. E. A Markov decision process. Journal of Mathematics and Mechanics, Bellman, R. E., Dreyfus, S. E. Functional approximations and dynamic programming. Mathematical Tables and Other Aids to Computation, 13:247–251. Bellman, R. E., Kalaba, R., Kotkin, B. Polynomial approximation—A new computational Bengio, Y. Learning deep architectures for AI',\n",
       " 'd82e3627-7875-4854-887e-e1a7d103a18e': 'PAC-Bayesian stochastic model selection. Machine Learning 51(1), 5–21. Minka, T. Divergence measures and message passing. Technical Report MSR-TR-2005173, Microsoft Research Cambridge. Minka, T. P. Automatic choice of dimensionality for PCA. In T. K. Leen, T. G. Dietterich, and V. Tresp (Eds. ), Advances in Neural Information Processing Systems, Volume 13, pp. 598–604. MIT Press. Minsky, M. L. and S. A. Papert . Perceptrons. MIT Press. Expanded edition 1990. Miskin, J. W. and D. J. C. MacKay . Ensemble learning for blind source separation. In S. J. Roberts and R. M',\n",
       " '46092513-7e62-4318-a9b3-105f701e3259': 'CoLA The Corpus of Linguistic Acceptability is a binary single-sentence classiﬁcation task, where the goal is to predict whether an English sentence is linguistically “acceptable” or not . STS-B The Semantic Textual Similarity Benchmark is a collection of sentence pairs drawn from news headlines and other sources . They were annotated with a score from 1 to 5 denoting how similar the two sentences are in terms of semantic meaning. MRPC Microsoft Research Paraphrase Corpus consists of sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in the pair are semantically equivalent . RTE Recognizing Textual Entailment is a binary entailment task similar to MNLI, but with much less training data .14 WNLI Winograd NLI is a small natural language inference dataset .\\n\\nThe GLUE webpage notes that there are issues with the construction of this dataset, 15 and every trained system that’s been submitted to GLUE has performed worse than the 65.1 baseline accuracy of predicting the majority class. We therefore exclude this set to be fair to OpenAI GPT',\n",
       " '4defa996-741d-418b-88d7-dc92d8ae9c3d': 'Both velocity components are restricted to be nonnegative and less than 5, and they cannot both be zero except at the starting line. Each episode begins in one of the randomly selected start states with both velocity components zero and ends when the car crosses the ﬁnish line. The rewards are −1 for each step until the car crosses the ﬁnish line. If the car hits the track boundary, it is moved back to a random position on the starting line, both velocity components are reduced to zero, and the episode continues. Before updating the car’s location at each time step, check to see if the projected path of the car intersects the track boundary. If it intersects the ﬁnish line, the episode ends; if it intersects anywhere else, the car is considered to have hit the track boundary and is sent back to the starting line. To make the task more challenging, with probability 0.1 at each time step the velocity increments are both zero, independently of the intended increments. Apply a Monte Carlo control method to this task to compute the optimal policy from each starting state',\n",
       " '8020404c-a40c-41bf-89cc-44bb663e9219': '| 84.13 | 91.4 71.13 84.66 InferSent - GloVe 81.57 | 86.54 | 92.50 | 90.38 | 84.18 | 88.2 75.77 85.59 Universal Sentence Encoder | 80.09 | 85.19 | 93.98 86.70 | 86.38 93.2 70.14 85.10 SBERT-NLI-base 83.64 | 89.43 | 94.39 | 89.86 | 88.96 | 89.6 76.00 87.41 SBERT-NLI-large 84.88 | 90.07 | 94.52 | 90.33 | 90.66 | 87.4 75.94 87.69  BERT-flow  The embedding representation space is deemed isotropic if embeddings are uniformly distributed on each dimension; otherwise, it is anisotropic. Li et al,  showed that a pre-trained BERT learns a non-smooth anisotropic semantic space of sentence embeddings and thus leads to poor  performance for text similarity tasks without fine-tuning',\n",
       " '53caab8a-3ee0-4275-8289-a097533f9621': 'We can then express the expectation in the form of a ﬁnite sum over The quantities rl = p(z(l))/q(z(l)) are known as importance weights, and they correct the bias introduced by sampling from the wrong distribution. Note that, unlike rejection sampling, all of the samples generated are retained. It will often be the case that the distribution p(z) can only be evaluated up to a normalization constant, so that p(z) = �p(z)/Zp where �p(z) can be evaluated easily, whereas Zp is unknown.\\n\\nSimilarly, we may wish to use an importance sampling distribution q(z) = �q(z)/Zq, which has the same property. We then have where �rl = �p(z(l))/�q(z(l)). We can use the same sample set to evaluate the ratio Zp/Zq with the result As with rejection sampling, the success of the importance sampling approach depends crucially on how well the sampling distribution q(z) matches the desired distribution p(z)',\n",
       " '80c50ac7-1a57-4c13-9fbc-e5580ed2b735': 'On the other hand, when q(x) < p(a )| f(a)|, whic  will happen more rarely, the ratio can be huge.\\n\\nBecause these latter events are rare, they may not show up in a typical sample, yielding typical underestimation of s, compensated rarely by gross overestimation. Such very large or very small numbers are typical when & is high dimensional, because in high dimension the dynamic range of joint probabilities can be very large. In spite of this danger, importance sampling and its variants have been found very useful in many machine learning algorithms, including deep learning algo- rithms. For example, see the use of importance sampling to accelerate training in neural language models with a large vocabulary (section 12.4.3.3) or other neural nets with a large number of outputs. See also how importance sampling has been used to estimate a partition function (the normalization constant of a probability  591  CHAPTER 17. MONTE CARLO METHODS  distribution) in section 18.7, and to estimate the log-likelihood in deep directed models, such as the variational autoencoder, in section 20.10.3',\n",
       " '8bedc8ec-1c1d-4306-9529-c76799e6e54f': \"x'|h, =h,);  The probability of observing a positive example for x is p* (x') = p( The probability of getting a negative sample for x is p; (x’) = p(x’|h, #h,). When we are sampling x~ , we cannot access the true p> (x-) and thus x~ may be sampled from the (undesired) anchor class c with probability 7+. The actual sampling data distribution becomes:  P(x’) = 1° pz (x’) + 7p; (x’')  Thus we can use p, (x’) = (p(x’) — nt p{(x’))/n~ for sampling x~ to debias the loss\",\n",
       " 'abeae0c6-aa37-47a9-ad4e-c409fbd3d817': 'The architectures were constructed such that many of the parameters in the classifier model could be paired to corresponding parameters in the unsupervised model.\\n\\nWhile a parameter norm penalty is one way to regularize parameters to be close to one another, the more popular way is to use constraints: to force sets of parameters to be equal. This method of regularization is often referred to as parameter sharing, because we interpret the various models or model components as sharing a unique set of parameters. A significant advantage of parameter sharing over regularizing the parameters to be close (via a norm penalty) is that only a subset of the parameters (the unique set) needs to be stored in memory. In certain models—such as the convolutional neural network—this can lead to significant reduction in the memory footprint of the model. 7.9.1 Convolutional Neural Networks By far the most popular and extensive use of parameter sharing occurs in convo- lutional neural networks (CNNs) applied to computer vision. Natural images have many statistical properties that are invariant to translation. For example, a photo of a cat remains a photo of a cat if it is translated one pixel to the right',\n",
       " '24f955bb-2939-4571-a36d-c8a158e6cfbe': 'We see that the solution for the maximum likelihood estimator depends on the data only through � n u(xn), which is therefore called the sufﬁcient statistic of the distribution (2.194). We do not need to store the entire data set itself but only the value of the sufﬁcient statistic. For the Bernoulli distribution, for example, the function u(x) is given just by x and so we need only keep the sum of the data points {xn}, whereas for the Gaussian u(x) = (x, x2)T, and so we should keep both the sum of {xn} and the sum of {x2 n}. If we consider the limit N → ∞, then the right-hand side of (2.228) becomes E, and so by comparing with (2.226) we see that in this limit ηML will equal the true value η',\n",
       " '1f12da11-11bd-405a-bb72-9d16fc3d0df6': 'The second estimates used to compute action values gave the “in-category DD conﬁdence,” pDD, which estimated the likelihood that Watson would respond correctly to the as-yet unrevealed DD clue. Tesauro et al. used the reinforcement learning approach of TD-Gammon described above to learn ˆv(·,w): a straightforward combination of nonlinear TD(λ) using a multilayer ANN with weights w trained by backpropagating TD errors during many simulated games. States were represented to the network by feature vectors speciﬁcally designed for Jeopardy!. Features included the current scores of the three players, how many DDs remained, the total dollar value of the remaining clues, and other information related to the amount of play left in the game. Unlike TD-Gammon, which learned by self-play, Watson’s ˆv was learned over millions of simulated games against carefully-crafted models of human players. In-category conﬁdence estimates were conditioned on the number of right responses r and wrong responses w that Watson gave in previously-played clues in the current category',\n",
       " '1c08494b-961a-441a-a1c4-976f4fb1f768': 'Sometimes we annotate the edges in this graph with the name of the parameters that describe the  relationship between two layers. Here, we indicate that a matrix W describes the mapping from x to h, and a vector w describes the mapping from h to y. We typically omit the  https://www.deeplearningbook.org/contents/mlp.html    intercept parameters associated with each layer when labeling this kind ot drawing. max{0, z}  9(2)  Figure 6.3: The rectified linear activation function. This activation function is the default activation function recommended for use with most feedforward neural networks. Applying this function to the output of a linear transformation yields a nonlinear transformation. The function remains very close to linear, however, in the sense that it is a piecewise linear function with two linear pieces. Because rectified linear units are nearly linear, they preserve many of the properties that make linear models easy to optimize with gradient-based methods. They also preserve many of the properties that make linear models generalize well.\\n\\nA common principle throughout computer science is that we can build complicated systems from minimal components',\n",
       " '6dec25d7-9053-4024-aac8-332bc7c6f6e9': 'An, P. C. E. .\\n\\nAn Improved Multi-dimensional CMAC Neural network: Receptive Field Function and Placement. Ph.D. thesis, University of New Hampshire, Durham. An, P. C. E., Miller, W. T., Parks, P. C. Design improvements in associative memories for cerebellar model articulation controllers (CMAC). Artiﬁcial Neural Networks, pp. 1207–1210, Elsevier North-Holland. http://www.incompleteideas.net/papers/AnMillerParks1991.pdf Anderson, C. W. Learning and Problem Solving with Multilayer Connectionist Systems. Anderson, C. W. Strategy learning with multilayer connectionist representations. In Anderson, C. W. Learning to control an inverted pendulum using neural networks. IEEE categorical perception, and probability learning: Some applications of a neural model. Psychological Review, 84(5):413–451',\n",
       " '6166f17d-c29c-475d-86c8-faf760261462': 'The existence of the ping pong ball and all its spatial coordinates are important underlying causal factors that generate the image and are relevant to the robotics task. Unfortunately, the autoencoder has limited capacity, and the training with mean squared error did not identify the ping pong ball as being salient enough to encode. Images graciously provided by Chelsea Finn. of a robotics task in which an autoencoder has failed to learn to encode a small  https://www.deeplearningbook.org/contents/representation.html    ping pong ball. This same robot is capable of successfully interacting with larger objects, such as baseballs, which are more salient according to mean squared error. Other definitions of salience are possible. For example, if a group of pixels follows a highly recognizable pattern, even if that pattern does not involve extreme brightness or darkness, then that pattern could be considered extremely salient.\\n\\nOne way to implement such a definition of salience is to use a recently developed approach called generative adversarial networks . In this approach, a generative model is trained to fool a feedforward classifier',\n",
       " '47573819-75ec-4a3b-92c4-ea722e1ca255': 'However, we see that while EHR and Chem have equivalent label densities, our optimizer correctly predicts that Chem can be modeled with majority vote, speeding up each pipeline execution by 1.8×. 3.1.3 Accelerating initial development cycles We ﬁnd in our applications that the optimizer can save execution time especially during the initial cycles of iterative development. To illustrate this empirically, in Fig. 8 we measure the modeling advantage of the generative model versus a majority vote of the labeling functions on increasingly large random subsets of the CDR labeling functions. We see that 11 Note that in Sect. 4, due to known negative class imbalance in relation extraction problems, we default to a negative value if majority vote yields a tie-vote label of 0. Thus, our reported F1 score metric hides instances in which the generative model learns to correctly (or incorrectly) break ties',\n",
       " 'e04dac51-f97e-4802-ba07-36a2db59bb6d': 'A typical way to do this is to introduce assumptions about how q factorizes. A common approach to variational learning is to impose the restriction that q  https://www.deeplearningbook.org/contents/inference.html    is a factorial distribution:  qh | v) = II (hi | v). (19.17)  This is called the mean field approach. More generally, we can impose any graphi- cal model structure we choose on gq, to flexibly determine how many interactions we want our approximation to capture. This fully general graphical model approach is called structured variational inference . The beauty of the variational approach is that we do not need to specify a specific parametric form for g. We specify how it should factorize, but then the optimization problem determines the optimal probability distribution within those factorization constraints. For discrete latent variables, this just means that we use traditional optimization techniques to optimize a finite number of variables describing the q distribution.\\n\\nFor continuous latent variables, this means that we use a branch of mathematics called calculus of variations to perform optimization over a space of functions and actually determine which function should be used to represent qg',\n",
       " 'b2f691ab-fb65-46e0-a26b-da426216951d': 'Besides, our data manipulation approach is derived based on a different perspective of reward learning, instead of meta-learning as in . Another popular type of data manipulation involves data synthesis, which creates entire artiﬁcial samples from scratch. GAN-based approaches have achieved impressive results for synthesizing conditional image data .\\n\\nIn the text domain, controllable text generation  presents a way of co-training the data generator and classiﬁer in a cyclic manner within a joint VAE  and wake-sleep  framework. It is interesting to explore the instantiation of the present approach for adaptive data synthesis in the future. We ﬁrst present the relevant work upon which our automated data manipulation is built. This section also establishes the notations used throughout the paper. Let x denote the input and y the output. For example, in text classiﬁcation, x can be a sentence and y is the sentence label. Denote the model of interest as pθ(y|x), where θ is the model parameters to be learned. In supervised setting, given a set of training examples D = {(x∗, y∗)}, we learn the model by maximizing the data log-likelihood',\n",
       " 'a964a698-f283-4636-97f8-d95701b128d4': 'An important class of such approximations, that can broadly be called variational methods, will be discussed in detail in Chapter 10. Complementing these deterministic approaches is a wide range of sampling methods, also called Monte Carlo methods, that are based on stochastic numerical sampling from distributions and that will be discussed at length in Chapter 11. Here we consider one simple approach to approximate inference in graphs with loops, which builds directly on the previous discussion of exact inference in trees. The idea is simply to apply the sum-product algorithm even though there is no guarantee that it will yield good results. This approach is known as loopy belief propagation  and is possible because the message passing rules (8.66) and (8.69) for the sum-product algorithm are purely local.\\n\\nHowever, because the graph now has cycles, information can ﬂow many times around the graph. For some models, the algorithm will converge, whereas for others it will not. In order to apply this approach, we need to deﬁne a message passing schedule. Let us assume that one message is passed at a time on any given link and in any given direction',\n",
       " '8b073643-2ccc-4334-baec-211a39393406': 'This heuristic serves as an upper bound to the true expected advantage, and thus, we can use it to determine when we can safely skip training the generative model (see Algorithm 1).\\n\\nLet cy(�i) = �n j=1 1 � �i, j = y � be the counts of labels of class y for xi, and assume that the true labeling function weights lie within a ﬁxed range, w j ∈  and have a mean ¯w.10 Then, deﬁne: where σ(·) is the sigmoid function, f ¯w is majority vote with all weights set to the mean ¯w, and ˜A∗(�) is the predicted modeling advantage used by our optimizer. Essentially, we are taking the expected counts of instances in which a weighted majority vote could possibly ﬂip the incorrect predictions of unweighted majority vote under best-case conditions, which is an upper bound for the expected advantage: Proposition 3 (Optimizer Upper Bound) Assume that the labeling functions have accuracy parameters (log-odds weights) w j ∈ , and have E = ¯w',\n",
       " '150b544a-02f5-496d-9113-61e68e856d25': 'We ﬁrst note that the leapfrog integration scheme (11.64), (11.65), and (11.66) is time-reversible, so that integration for L steps using step size −ϵ will exactly undo the effect of integration for L steps using step size ϵ.\\n\\nNext we show that the leapfrog integration preserves phase-space volume exactly. This follows from the fact that each step in the leapfrog scheme updates either a zi variable or an ri variable by an amount that is a function only of the other variable. As shown in Figure 11.14, this has the effect of shearing a region of phase space while not altering its volume. Finally, we use these results to show that detailed balance holds. Consider a small region R of phase space that, under a sequence of L leapfrog iterations of step size ϵ, maps to a region R′. Using conservation of volume under the leapfrog iteration, we see that if R has volume δV then so too will R′',\n",
       " '2ac30d57-88f8-402e-ae5d-7af13811c167': 'The development of goal-directed behavioral control was likely a major advance in the evolution of animal intelligence.\\n\\nstrategies in a hypothetical task in which a rat has to navigate a maze that has distinctive goal boxes, each delivering an associated reward of the magnitude shown (Figure 14.5 top). Starting at S1, the rat has to ﬁrst select left (L) or right (R) and then has to select L or R again at S2 or S3 to reach one of the goal boxes. The goal boxes are the terminal states of each episode of the rat’s episodic task. A model-free strategy (Figure 14.5 lower left) relies on stored values for state–action pairs. These action values are estimates of the highest return the rat can expect for each action taken from each (nonterminal) state. They are obtained over many trials of running the maze from start to ﬁnish. When the action values have become good enough estimates of the optimal returns, the rat just has to select at each state the action with the largest action value in order to make optimal decisions. In this case, when the action-value estimates become accurate enough, the rat selects L from S1 and R from S2 to obtain the maximum return of 4',\n",
       " '657d1b38-883c-4011-8148-13f8c99dccb1': \"An important class of such models, known as independent component analysis, or leA, arises when we consider a distribution over the latent variables that factorizes, so that M p(z) = IIp(Zj). j=l (12.89) To understand the role of such models, consider a situation in which two people are talking at the same time, and we record their voices using two microphones. If we ignore effects such as time delay and echoes, then the signals received by the microphones at any point in time will be given by linear combinations of the amplitudes of the two voices. The coefficients of this linear combination will be constant, and if we can infer their values from sample data, then we can invert the mixing process (assuming it is nonsingular) and thereby obtain two clean signals each of which contains the voice ofjust one person.\\n\\nThis is an example of a problem called blind source separation in which 'blind' refers to the fact that we are given only the mixed data, and neither the original sources nor the mixing coefficients are observed . This type of problem is sometimes addressed using the following approach  in which we ignore the temporal nature of the signals and treat the successive samples as i.i.d\",\n",
       " 'a8ed459e-43f8-4051-8013-2abeec206fb6': 'It could be tackled using handcrafted rules or heuristics for distinguishing the digits based on the shapes of the strokes, but in practice such an approach leads to a proliferation of rules and of exceptions to the rules and so on, and invariably gives poor results. Far better results can be obtained by adopting a machine learning approach in which a large set of N digits {x1, . , xN} called a training set is used to tune the parameters of an adaptive model. The categories of the digits in the training set are known in advance, typically by inspecting them individually and hand-labelling them. We can express the category of a digit using target vector t, which represents the identity of the corresponding digit. Suitable techniques for representing categories in terms of vectors will be discussed later. Note that there is one such target vector t for each digit image x. The result of running the machine learning algorithm can be expressed as a function y(x) which takes a new digit image x as input and that generates an output vector y, encoded in the same way as the target vectors',\n",
       " '77121de8-c220-492b-b786-abc6161d21fc': 'They find classifica- tion accuracy differences of 70.18% versus 74.42% on the CIFAR-10 dataset and 74.61% versus 80.35% on the problem of classifying dogs versus cats.\\n\\nFurther, they explore the  robustness of classifiers with respect to test-time augmentation and find that the model Shorten and Khoshgoftaar J Big Data  6:60  Table 8 AutoAugment augmentation policy found on the reduced CIFAR-10 dataset   Operation 1 Operation 2 Sub-policy 0 (Invert,0.1,7) (Contrast,0.2,6) Sub-policy 1 (Rotate,0.7,2) (TranslateX,0.3,9) Sub-policy 2 (Sharpness,0.8,1) (Sharpness,0.9,3) Sub-policy 3 (ShearY,0.5,8) (TranslateY,0.7,9) Sub-policy 4 (AutoContrast,0.5,8) (Equalize,0.9,2) Sub-policy 5 (ShearY,0.2,7) (Posterize,0.3,7) Sub-policy 6 (Color,0.4,3) (Brightness,0.6,7) Sub-policy 7 (Sharpness,0.3,9) (Brightness,0.7,9) Sub-policy 8& (Equalize,0.6,5) (Equalize,0.5,1) Sub-policy 9 (Contrast,0.6,7) (Sharpness,0.6,5) Sub-policy 10 (Color,0.7,7) (TranslateX,0.5,8) Sub-policy 11 (Equalize,0.3,7) (AutoContrast,0.4,8) Sub-policy 12 (Translatey,0.4,3) (Sharpness,0.2,6) Sub-policy 13 (Brightness,0.9,6) (Color,0.2,8)\\n\\nSub-policy 14 (Solarize,0.5,2) (Invert,0.0,3) Sub-policy 15 (Equalize,0.2,0) (AutoContrast,0.6,0) Sub-policy 16 (Equalize,0.2,8) (Equalize,0.6,4) Sub-policy 17 (Color,0.9,9) (Equalize,0.6,6) Sub-policy 18 (AutoContrast,0.8,4) (Solarize,0.2,8) Sub-policy 19 (Brightness,0.1,3) (Color,0.7,0) Sub-policy 20 (Solarize,0.4,5) (AutoContrast,0.9,3) Sub-policy 21 (TranslateY,0.9,9) (TranslateY,0.7,9) Sub-policy 22 (AutoContrast,0.9,2) (Solarize,0.8,3) Sub-policy 23 (Equalize,0.8,8) (Invert,0.1,3) Sub-policy 24 (Translatey,0.7,9) (AutoContrast,0.9,1)  Table 9 The performance of ARS on continuous space vs',\n",
       " 'e2d538fd-085e-4ee4-b716-485a484af395': 'G., Finn, C., Lee, L., Neiswanger, W., Qin, L., Berg-Kirkpatrick, T., Salakhutdinov, R., & Xing, E. P. The NeurIPS workshop on learning with rich experience: Integration of learning paradigms. https://sites.google.com/view/neurips2019lire Hu, Z., Yang, Z., Liang, X., Salakhutdinov, R., & Xing, E. P. Toward controlled generation of text. International Conference on Machine Learning, 1587–1596. Hu, Z., Yang, Z., Salakhutdinov, R., Liang, X., Qin, L., Dong, H., & Xing, E. P. Deep generative models with learnable knowledge constraints. Proceedings of the 32nd International Conference on Neural Information Processing Systems, 10522–10533. Jaakkola, T., Meila, M., & Jebara, T. Maximum entropy discrimination',\n",
       " '88e49456-7f0a-49cb-af96-7dd4838eff94': 'Repurposing Learning Algorithms for New Problems. The standardized formalism sheds new light on fundamental relationships between a number of learning problems in diﬀerent research areas, showing that they are essentially the same under the SE perspective. This opens up a wide range of opportunities for generalizing existing algorithms, which were originally designed for specialized problems, to a much broader set of new problems. It is also made easy to exchange between the diverse research areas in aspects of modeling, theoretical understanding, approximation, and optimization. For example, an earlier successful approach to challenges in one area can now be readily applied to address challenges in another. Similarly, a future progress made in one problem could immediately unlock progresses in many others',\n",
       " 'b5b90cc7-4414-4084-905a-88c3456e5c4d': 'Our discussion so far can be summarized by saying that the danger of instability and divergence arises whenever we combine all of the following three elements, making up what we call the deadly triad: Function approximation A powerful, scalable way of generalizing from a state space Bootstrapping Update targets that include existing estimates (as in dynamic programming or TD methods) rather than relying exclusively on actual rewards and complete returns (as in MC methods). O↵-policy training Training on a distribution of transitions other than that produced by the target policy.\\n\\nSweeping through the state space and updating all states uniformly, as in dynamic programming, does not respect the target policy and is an example of o↵-policy training. In particular, note that the danger is not due to control or to generalized policy iteration. Those cases are more complex to analyze, but the instability arises in the simpler prediction case whenever it includes all three elements of the deadly triad. The danger is also not due to learning or to uncertainties about the environment, because it occurs just as strongly in planning methods, such as dynamic programming, in which the environment is completely known. If any two elements of the deadly triad are present, but not all three, then instability can be avoided',\n",
       " '116c45eb-7e52-4ad6-ae43-9d1931647242': 'We also assume that the loss is the negative log-likelihood of the true target y given the input so far.\\n\\nThe gradient Vj) L on the outputs at time step ¢, for all 7,t, is as follows:  ott  aL AL OLY ow  (VoL); = ao = aL () ao) =U  1 (10.18)  jay *  We work our way backward, starting from the end of the sequence. At the final time step T, h 7) only has o(7) as a descendent, so its gradient is simple:  Vin b=V! Von L- 10.19)  We can then iterate backward in time to back-propagate gradients through time, from t =r — 1 down to t = 1, noting that AY (for t < 7) has as descendents both o) and A+)',\n",
       " 'a627efb6-04a1-4dd4-8b45-5864499293e0': 'In this case no β recursion is required, and we simply have Let us take a moment to interpret this result for p(X). Recall that to compute the likelihood we should take the joint distribution p(X, Z) and sum over all possible values of Z. Each such value represents a particular choice of hidden state for every time step, in other words every term in the summation is a path through the lattice diagram, and recall that there are exponentially many such paths.\\n\\nBy expressing the likelihood function in the form (13.42), we have reduced the computational cost from being exponential in the length of the chain to being linear by swapping the order of the summation and multiplications, so that at each time step n we sum the contributions from all paths passing through each of the states znk to give the intermediate quantities α(zn). Next we consider the evaluation of the quantities ξ(zn−1, zn), which correspond to the values of the conditional probabilities p(zn−1, zn|X) for each of the K × K settings for (zn−1, zn)',\n",
       " '0023e207-d2e5-4142-8618-6d04b9480661': 'Asynchronous algorithms also make it easier to intermix computation with real-time interaction. To solve a given MDP, we can run an iterative DP algorithm at the same time that an agent is actually experiencing the MDP. The agent’s experience can be used to determine the states to which the DP algorithm applies its updates. At the same time, the latest value and policy information from the DP algorithm can guide the agent’s decision making. For example, we can apply updates to states as the agent visits them. This makes it possible to focus the DP algorithm’s updates onto parts of the state set that are most relevant to the agent. This kind of focusing is a repeated theme in reinforcement learning. Policy iteration consists of two simultaneous, interacting processes, one making the value function consistent with the current policy (policy evaluation), and the other making the policy greedy with respect to the current value function (policy improvement).\\n\\nIn policy iteration, these two processes alternate, each completing before the other begins, but this is not really necessary. In value iteration, for example, only a single iteration of policy evaluation is performed in between each policy improvement',\n",
       " '42a6e364-7dac-4b07-8bb4-1050fe9ecb57': 'Having found the marginal and conditional distributions, we effectively expressed the joint distribution p(z) = p(x)p(y|x) in the form p(x|y)p(y). These results are summarized below. Given a marginal Gaussian distribution for x and a conditional Gaussian distribution for y given x in the form the marginal distribution of y and the conditional distribution of x given y are given by Given a data set X = (x1, . , xN)T in which the observations {xn} are assumed to be drawn independently from a multivariate Gaussian distribution, we can estimate the parameters of the distribution by maximum likelihood. The log likelihood function is given by By simple rearrangement, we see that the likelihood function depends on the data set only through the two quantities These are known as the sufﬁcient statistics for the Gaussian distribution.\\n\\nUsing (C.19), the derivative of the log likelihood with respect to µ is given by Appendix C and setting this derivative to zero, we obtain the solution for the maximum likelihood estimate of the mean given by which is the mean of the observed set of data points. The maximization of (2.118) with respect to Σ is rather more involved',\n",
       " 'ff099b3f-2a43-4b10-8e0b-c6ba5ae6ef34': 'Steinkrau ef al. implemented a two-layer fully connected neural network on a GPU and reported a three-times speedup over their CPU-based baseline. Shortly thereafter, Chellapilla et al. demonstrated that the same technique could be used to  1 1 ‘4 ras 1 1 1  https://www.deeplearningbook.org/contents/applications.html    accelerate SUPErVIsSed CONVOLULIONAL LLELWOIKS. The popularity of graphics cards for neural network training exploded after the advent of general purpose GPUs, These GP-GPUs could execute arbitrary  code, not just rendering subroutines. NVIDIA’s CUDA programming language provided a way to write this arbitrary code in a C-like language.\\n\\nWith their relatively convenient programming model, massive parallelism, and high memory bandwidth, GP-GPUs now offer an ideal platform for neural network programming. 440  CHAPTER 12. APPLICATIONS  This platform was rapidly adopted by deep learning researchers soon after it became available . Writing efficient code for GP-GPUs remains a difficult task best left to special- ists',\n",
       " '9ca64577-83bc-4ae8-9d36-859a62510b58': 'In this elegant approach, there is no need to construct explicit targets for the inference network. Instead, the inference network is simply used to define £, and then the parameters of the inference network are adapted to increase £. This model is described in depth in section 20.10.3. Using approximate inference, it is possible to train and use a wide variety of models. Many of these models are described in the next chapter.\\n\\n650  https://www.deeplearningbook.org/contents/inference.html',\n",
       " '4c531b26-b32e-4f40-8530-0557b397c072': 'Furthermore, we shall assume that the covariance of this Gaussian is small so that the network function is approximately linear with respect to the parameters over the region of parameter space for which the posterior probability is signiﬁcantly nonzero. With these two approximations, we will obtain models that are analogous to the linear regression and classiﬁcation models discussed in earlier chapters and so we can exploit the results obtained there. We can then make use of the evidence framework to provide point estimates for the hyperparameters and to compare alternative models (for example, networks having different numbers of hidden units). To start with, we shall discuss the regression case and then later consider the modiﬁcations needed for solving classiﬁcation tasks. Consider the problem of predicting a single continuous target variable t from a vector x of inputs (the extension to multiple targets is straightforward). We shall suppose that the conditional distribution p(t|x) is Gaussian, with an x-dependent mean given by the output of a neural network model y(x, w), and with precision (inverse variance) β For an i.i.d. data set of N observations x1, . , xN, with a corresponding set of target values D = {t1,',\n",
       " '5d408743-0207-4b1b-8c94-920697fb6e54': 'This gives rise to two terms, one of which cancels KL(q∥p) while the other gives the required log likelihood ln p(X|θ) after noting that q(Z) is a normalized distribution that sums to 1.\\n\\nFrom (9.72), we see that KL(q∥p) is the Kullback-Leibler divergence between q(Z) and the posterior distribution p(Z|X, θ). Recall that the Kullback-Leibler divergence satisﬁes KL(q∥p) ⩾ 0, with equality if, and only if, q(Z) = p(Z|X, θ). It Section 1.6.1 therefore follows from (9.70) that L(q, θ) ⩽ ln p(X|θ), in other words that L(q, θ) is a lower bound on ln p(X|θ). The decomposition (9.70) is illustrated in FigThe EM algorithm is a two-stage iterative optimization technique for ﬁnding maximum likelihood solutions. We can use the decomposition (9.70) to deﬁne the EM algorithm and to demonstrate that it does indeed maximize the log likelihood',\n",
       " '57cbd845-e564-4903-8caf-b3a1aa692b02': 'Throughout the book, we use two simple synthetic data sets to illustrate many of the algorithms. The ﬁrst of these is a regression problem, based on the sinusoidal function, shown in Figure A.6. The input values {xn} are generated uniformly in range (0, 1), and the corresponding target values {tn} are obtained by ﬁrst computing the corresponding values of the function sin(2πx), and then adding random noise with a Gaussian distribution having standard deviation 0.3. Various forms of this data set, having different numbers of data points, are used in the book. The second data set is a classiﬁcation problem having two classes, with equal prior probabilities, and is shown in Figure A.7. The blue class is generated from a single Gaussian while the red class comes from a mixture of two Gaussians.\\n\\nBecause we know the class priors and the class-conditional densities, it is straightforward to evaluate and plot the true posterior probabilities as well as the minimum misclassiﬁcation-rate decision boundary, as shown in Figure A.7. function from which the data points were generated',\n",
       " '65d29a12-679f-40e1-9351-4fcd2beb7e3a': 'In this case, we can let the variational approximate posterior be a multivariate Gaussian with a diagonal covariance structure2: where the mean and s.d. of the approximate posterior, µ(i) and σ(i), are outputs of the encoding MLP, i.e. nonlinear functions of datapoint x(i) and the variational parameters φ (see appendix C). As explained in section 2.4, we sample from the posterior z(i,l) ∼ qφ(z|x(i)) using z(i,l) = gφ(x(i), ϵ(l)) = µ(i) + σ(i) ⊙ ϵ(l) where ϵ(l) ∼ N(0, I). With ⊙ we signify an element-wise product. In this model both pθ(z) (the prior) and qφ(z|x) are Gaussian; in this case, we can use the estimator of eq. (7) where the KL divergence can be computed and differentiated without estimation (see appendix B)',\n",
       " '4be235eb-7caa-4c2c-95b1-7af72c94e1e0': 'tla tte t ie 1 ML Ny  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    a set of 1UCaL CULULUIULaL pruvaviily UIDLLIVULIOLIS P(x’ | ©UY(x\")), where P aol x!) gives the parents of x? in Y. The probability distribution over X is given  by p(x) = Tips | Pag(x)). (16.1) In our relay race example, this means that, using the graph drawn in figure 16.2, p(to, ti, t2) = p(to)p(ti | to)p(te | t1). (16.2)  This is our first time seeing a structured probabilistic model in action. We can examine the cost of using it, to observe how structured modeling has many advantages relative to unstructured modeling. Suppose we represented time by discretizing time ranging from minute 0 to minute 10 into 6-second chunks. This would make to, t; and t2 each be a discrete variable with 100 possible values',\n",
       " 'd28e6fc7-ce51-471e-8095-218db70df6e6': 'Our probabilistic model speciﬁes the joint distribution p(X, Z), and our goal is to ﬁnd an approximation for the posterior distribution p(Z|X) as well as for the model evidence p(X). As in our discussion of EM, we can decompose the log marginal probability using This differs from our discussion of EM only in that the parameter vector θ no longer appears, because the parameters are now stochastic variables and are absorbed into Z.\\n\\nSince in this chapter we will mainly be interested in continuous variables we have used integrations rather than summations in formulating this decomposition. However, the analysis goes through unchanged if some or all of the variables are discrete simply by replacing the integrations with summations as required. As before, we can maximize the lower bound L(q) by optimization with respect to the distribution q(Z), which is equivalent to minimizing the KL divergence. If we allow any possible choice for q(Z), then the maximum of the lower bound occurs when the KL divergence vanishes, which occurs when q(Z) equals the posterior distribution p(Z|X)',\n",
       " 'dece5c6b-b10b-44e7-a2ce-906a0719de61': 'Rather than training the model to maximize the likelihood, the model is trained to make each recurrent network obtain an accurate answer to the corresponding inference problem. The training process is illustrated in figure 20.5. It consists of randomly sampling a training example, randomly sampling a subset of inputs to the inference network, and then training the inference network to predict the values of the remaining units. This general principle of back-propagating through the computational graph for approximate inference has been applied to other models . In these models and in the MP-DBM, the final loss is not the lower bound on the likelihood. Instead, the final loss is typically based on  vd . 1 tae Pow. as sya ad . ote 1 1  https://www.deeplearningbook.org/contents/generative_models.html    Le Approximate CONCIUOLAL GISLLIDULION Lal Le approximate Wuerence LeEUWOrk imposes over the missing values. This means that the training of these models is somewhat heuristically motivated',\n",
       " '3b9a3e09-79c7-45e0-9dc1-6110456cbb56': 'In this  https://www.deeplearningbook.org/contents/optimization.html    case, the generalization error (equation 8.2) can be written as a sum  = So Paata(aw, y)L( f(a: 9), y), (8.7) ey with the exact gradient  g =VoJ*( = y Paatalx, y)V oL( f(x; 9), y). (8.8)  We have already seen the same fact demonstrated for the log-likelihood in equa- tion 8.5 and equation 8.6; we observe now that this holds for other functions L besides the likelihood.\\n\\nA similar result can be derived when x and y are continuous, under mild assumptions regarding pgata and L.  Hence, we can obtain an unbiased estimator of the exact gradient of the generalization error by sampling a minibatch of examples {a), Lee al} with cor- responding targets y from the data-generating distribution pgata, then computing the gradient of the loss with respect to the parameters for that minibatch:  an oe v6), y). (8.9)  Updating @ in the direction of g performs SGD on the generalization error',\n",
       " '42fc62d0-878e-4df4-b3c0-7cb12745169c': 'Conditional independence properties play an important role in using probabilistic models for pattern recognition by simplifying both the structure of a model and the computations needed to perform inference and learning under that model. We shall see examples of this shortly.\\n\\nIf we are given an expression for the joint distribution over a set of variables in terms of a product of conditional distributions (i.e., the mathematical representation underlying a directed graph), then we could in principle test whether any potential conditional independence property holds by repeated application of the sum and product rules of probability. In practice, such an approach would be very time consuming. An important and elegant feature of graphical models is that conditional independence properties of the joint distribution can be read directly from the graph without having to perform any analytical manipulations. The general framework for achieving this is called d-separation, where the ‘d’ stands for ‘directed’ . Here we shall motivate the concept of d-separation and give a general statement of the d-separation criterion. A formal proof can be found in Lauritzen . We begin our discussion of the conditional independence properties of directed graphs by considering three simple examples each involving graphs having just three nodes',\n",
       " '96799363-9e40-4335-8fb2-e954beb2746a': '441  CHAPTER 12. APPLICATIONS  12.1.3.\\n\\nLarge-Scale Distributed Implementations  In many cases, the computational resources available on a single machine are insufficient. We therefore want to distribute the workload of training and inference across many machines. Distributing inference is simple, because each input example we want to process can be run by a separate machine. This is known as data parallelism. It is also possible to get model parallelism, where multiple machines work together on a single data point, with each machine running a different part of the model. This is feasible for both inference and training. Data parallelism during training is somewhat harder. We can increase the size of the minibatch used for a single SGD step, but usually we get less than linear returns in terms of optimization performance. It would be better to allow multiple machines to compute multiple gradient descent steps in parallel. Unfortunately, the standard definition of gradient descent is as a completely sequential algorithm: the gradient at step t is a function of the parameters produced by step ¢t — 1. This can be solved using asynchronous stochastic gradient descent . In this approach, several processor cores share the memory representing the parameters',\n",
       " '8c953bf1-4f04-4c09-9f03-1051948854ba': 'For each data point xn, we introduce a corresponding set of binary indicator variables rnk ∈ {0, 1}, where k = 1, . , K describing which of the K clusters the data point xn is assigned to, so that if data point xn is assigned to cluster k then rnk = 1, and rnj = 0 for j ̸= k. This is known as the 1-of-K coding scheme. We can then deﬁne an objective function, sometimes called a distortion measure, given by which represents the sum of the squares of the distances of each data point to its assigned vector µk. Our goal is to ﬁnd values for the {rnk} and the {µk} so as to minimize J. We can do this through an iterative procedure in which each iteration involves two successive steps corresponding to successive optimizations with respect to the rnk and the µk. First we choose some initial values for the µk. Then in the ﬁrst phase we minimize J with respect to the rnk, keeping the µk ﬁxed.\\n\\nIn the second phase we minimize J with respect to the µk, keeping rnk ﬁxed',\n",
       " 'ad2820fc-6c76-4b55-9f76-955d4913f5b8': 'Having solved the quadratic programming problem and found a value for a, we can then determine the value of the threshold parameter b by noting that any support vector xn satisﬁes tny(xn) = 1. Using (7.13) this gives where S denotes the set of indices of the support vectors. Although we can solve this equation for b using an arbitrarily chosen support vector xn, a numerically more stable solution is obtained by ﬁrst multiplying through by tn, making use of t2 n = 1, and then averaging these equations over all support vectors and solving for b to give where NS is the total number of support vectors.\\n\\nFor later comparison with alternative models, we can express the maximummargin classiﬁer in terms of the minimization of an error function, with a simple quadratic regularizer, in the form where E∞(z) is a function that is zero if z ⩾ 0 and ∞ otherwise and ensures that the constraints (7.5) are satisﬁed. Note that as long as the regularization parameter satisﬁes λ > 0, its precise value plays no role',\n",
       " 'a4ee3b03-f0a8-457b-b241-122263364057': 'For example, in Goethe’s poem “The Sorcerer’s Apprentice” , the apprentice uses magic to enchant a broom to do his job of fetching water, but the result is an unintended ﬂood due to the apprentice’s inadequate knowledge of magic. In the engineering context, Norbert Wiener, the founder of cybernetics, warned of this problem more than half a century ago by relating the supernatural story of “The Monkey’s Paw” : “... it grants what you ask for, not what you should have asked for or what you intend” (p. 59). The problem has also been discussed at length in a modern context by Nick Bostrom . Anyone having experience with reinforcement learning has likely seen their systems discover unexpected ways to obtain a lot of reward. Sometimes the unexpected behavior is good: it solves a problem in a nice new way. In other instances, what the agent learns violates considerations that the system designer may never have thought about',\n",
       " '27948716-fdd5-4aea-ba62-57674de288ff': 'Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 2672–2680. Curran Associates, Inc. Demi Guo, Yoon Kim, and Alexander Rush. 2020. Sequence-level mixed sample data augmentation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5547–5552, Online. Association for Computational Linguistics. Ankush Gupta, Arvind Agarwal, Prawaan Singh, and Piyush Rai. 2017. A deep generative framework for paraphrase generation. Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, and Maosong Sun. 2018. FewRel: A large-scale supervised few-shot relation classiﬁcation dataset with state-of-the-art evaluation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4803– 4809, Brussels, Belgium',\n",
       " '8f54fa09-24c7-4535-90f0-a8ae2773d564': 'We can represent this data set as an N × D from the joint distribution p(z)p(x|z) in which the three states of z, corresponding to the three components of the mixture, are depicted in red, green, and blue, and (b) the corresponding samples from the marginal distribution p(x), which is obtained by simply ignoring the values of z and just plotting the x values. The data set in (a) is said to be complete, whereas that in (b) is incomplete.\\n\\n(c) The same samples in which the colours represent the value of the responsibilities γ(znk) associated with data point xn, obtained by plotting the corresponding point using proportions of red, blue, and green ink given by γ(znk) for k = 1, 2, 3, respectively matrix X in which the nth row is given by xT n. Similarly, the corresponding latent variables will be denoted by an N × K matrix Z with rows zT the data points are drawn independently from the distribution, then we can express the Gaussian mixture model for this i.i.d. data set using the graphical representation shown in Figure 9.6',\n",
       " '7f9f9ef9-16af-4da0-98da-1530a35d3d8f': 'Everson (Eds. ), Independent Component Analysis: Principles and Practice. Cambridge University Press. Moody, J. and C. J. Darken . Fast learning in networks of locally-tuned processing units. Neural Computation 1(2), 281–294. Moore, A. W. The anchors hierarch: using the triangle inequality to survive high dimensional data. In Proceedings of the Twelfth Conference on Uncertainty in Artiﬁcial Intelligence, pp. 397–405. M¨uller, K. R., S. Mika, G. R¨atsch, K. Tsuda, and B. Sch¨olkopf . An introduction to kernelbased learning algorithms. IEEE Transactions on Neural Networks 12(2), 181–202. Nag, R., K. Wong, and F. Fallside . Script recognition using hidden markov models. In ICASSP86, pp',\n",
       " '4a522759-f721-46e0-9dcb-3c04451da13e': \"Fast parameterization  ot = Fu (Voli, tee Vole) 7, = fo,o(2;)  Meta learner:  Slow weights @  Base learner:  Slow weights @  Fast parameterization $7 = Go(Vele™ Meta Info  fo,o+  “Key” memory R= {ri}hy  /  Input  Output  ‘Value” Memory M= {7 Ha  https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Log   Training Process  Sample a random pair of inputs at each time step t from the support set S, (x), y/) and (xi, Yj). Let Xq@1) = x, and X(t2) = xi. fort =1,...,K:  a\",\n",
       " 'fa3b1689-9240-4744-bac9-3a73dd0d2c4f': 'MCTS is largely responsible for the improvement in computer Go from a weak amateur level in 2005 to a grandmaster level (6 dan or more) in 2015. Many variations of the basic algorithm have been developed, including a variant that we discuss in Section 16.6 that was critical for the stunning 2016 victories of the program AlphaGo over an 18-time world champion Go player. MCTS has proved to be e↵ective in a wide variety of competitive settings, including general game playing , but it is not limited to games; it can be e↵ective for single-agent sequential decision problems if there is an environment model simple enough for fast multistep simulation.\\n\\nMCTS is executed after encountering each new state to select the agent’s action for that state; it is executed again to select the action for the next state, and so on. As in a rollout algorithm, each execution is an iterative process that simulates many trajectories starting from the current state and running to a terminal state (or until discounting makes any further reward negligible as a contribution to the return). The core idea of MCTS is to successively focus multiple simulations starting at the current state by extending the initial portions of trajectories that have received high evaluations from earlier simulations',\n",
       " '4fbdbffa-9839-4977-b21c-641fe815865c': '= argmax V,(s), 7. = argmaxQ,(s, a) And of course, we have V,,. (s) = V.(s) and Q,,(s,a) = Q. (s, a). Markov Decision Processes  In more formal terms, almost all the RL problems can be framed as Markov Decision Processes (MDPs)',\n",
       " 'd3e5b373-bb40-455d-9ec2-538e71bf83aa': 'The optimal test error is found by trading off these quantities.\\n\\nNeural networks typically perform best when the training error is very low (and thus, when capacity is high) and the test error is primarily driven by the gap between training and test error. Your goal is to reduce this gap without increasing training error faster than the gap decreases. To reduce the gap, change regularization hyperparameters to reduce effective model capacity, such as by adding dropout or weight decay. Usually the best performance comes from a large model that is regularized well, for example, by using dropout. Most hyperparameters can be set by reasoning about whether they increase or decrease model capacity. Some examples are included in table 11.1. While manually tuning hyperparameters, do not lose sight of your end goal: good performance on the test set. Adding regularization is only one way to achieve this goal. As long as you have low training error, you can always reduce general- ization error by collecting more training data. The brute force way to practically guarantee success is to continually increase model capacity and training set size until the task is solved. This approach does of course increase the computational cost of training and inference, so it is only feasible given appropriate resources',\n",
       " '75b63e14-48c2-4807-b781-742465373d3f': 'Another useful strategy for generative modeling worth mentioning is variational auto-encoders. The GAN framework can be extended to improve the quality of samples produced with variational auto-encoders . Variational auto-encoders learn a low- dimensional representation of data points. In the image domain, this translates an image tensor of size height x width x color channels down into a vector of size n x 1, identi- cal to what was discussed with respect to feature space augmentation.\\n\\nLow-dimensional constraints in vector representations will result in a poorer representation, although these constraints are better for visualization using methods such as t-SNE . Imag- ine a vector representation of size 5 x 1 created by an autoencoder. These autoencoders can take in a distribution of labeled data and map them into this space. These classes could include ‘head turned left; ‘centered head, and ‘head turned right: The auto-encoder learns a low-dimensional representation of these data points such that vector operations such as adding and subtracting can be used to simulate a front view-3D rotation of a new instance',\n",
       " 'dc0274c2-35f7-40a2-8f74-36e12f42a516': 'If a value 7 is missing, and all the other values, denoted ®-i, are given, then we know the distribution over it is given by p(2; | xi). In practice, density estimation does not always enable us to solve all these related tasks, because in many cases the required operations on p(a) are computationally intractable. Of course, many other tasks and types of tasks are possible. The types of tasks we list here are intended only to provide examples of what machine learning can do, not to define a rigid taxonomy of tasks. 5.1.2. The Performance Measure, P  To evaluate the abilities of a machine learning algorithm, we must design a quantitative measure of its performance. Usually this performance measure P is specific to the task T being carried out by the system. For tasks such as classification, classification with missing inputs, and tran- scription, we often measure the accuracy of the model. Accuracy is just the  101  CHAPTER 5. MACHINE LEARNING BASICS  proportion of examples for which the model produces the correct output.\\n\\nWe can also obtain equivalent information by measuring the error rate, the proportion of examples for which the model produces an incorrect output',\n",
       " '9431e36c-b87e-485f-a254-4f9ffb0d0aa7': 'It has become increasingly ubiquitous to manipulate data to improve learning, especially in low data regime or in presence of low-quality datasets (e.g., imbalanced labels).\\n\\nFor example, data augmentation applies label-preserving transformations on original data points to expand the data size; data weighting assigns an importance weight to each instance to adapt its effect on learning; and data synthesis generates entire artiﬁcial examples. Different types of manipulation can be suitable for different application settings. Common data manipulation methods are usually designed manually, e.g., augmenting by ﬂipping an image or replacing a word with synonyms, and weighting with inverse class frequency or loss values . Recent work has studied automated approaches, such as learning the composition of augmentation operators with reinforcement learning , deriving sample weights adaptively from a validation set via meta learning , or learning a weighting network by inducing a curriculum . These learning-based approaches have alleviated the engineering burden and produced impressive results. However, the algorithms are usually designed speciﬁcally for certain types of manipulation (e.g., either augmentation or weighting) and thus have limited application scope in practice. In this work, we propose a new approach that enables learning for different manipulation schemes with the same single algorithm',\n",
       " 'a2a8f339-3009-4432-a154-0b3824bc2616': \"Softmax classifier  4  MLP(f(z), f(2'),|f(@) — f(@’)))  siamese network: two towers have shared weights  f(x) f(e') eoropeooes E Sitaklake stage tetera!\\n\\nC siaietatolateieateeh ' pooling pooling | z x ' BERT BERT Sentence x Sentence x’  -1..  1  4 cos( f(z), f(2’))  f(x) f(z')  ry ry pooling pooling  r 3  BERT BERT Sentence © Sentence «’  The SentEval library  is commonly used for evaluating the quality of  earned sentence embedding. SBERT outperformed other baselines at that time  on 5  out of 7 tasks. Model MR CR | SUBJ | MPQA | SST | TREC | MRPC || Avg. Avg\",\n",
       " 'cc480821-6662-4937-ac15-1cf0615304a8': 'Figure 8.6 illustrates how the method of steepest descent, when applied in a quadratic bowl, progresses in a rather ineffective back-and-forth zig-zag pattern. This happens because each line search direction, when given by the gradient, is guaranteed to be orthogonal to the previous line search direction. Let the previous search direction be dt-1. At the minimum, where the line search terminates, the directional derivative is zero in direction dy: VeJ(0) - d,-1 = 0. Since the gradient at this point defines the current search direction, d, = VoJ(@) will have no contribution in the direction d,_,. Thus d; is orthogonal to d,_,;.\\n\\nThis relationship between d;_; and d; is illustrated in figure 8.6 for multiple iterations of steepest descent. As demonstrated in the figure, the choice of orthogonal directions of descent do not preserve the minimum along the previous search directions',\n",
       " '5ea0d289-c5d0-4985-b569-3d373adf9166': 'The use of cross-entropy losses greatly improved the performance of models with sigmoid and softmax outputs, which had previously suffered from saturation and slow learning when using the mean squared error loss. The other major algorithmic change that has greatly improved the performance  https://www.deeplearningbook.org/contents/mlp.html    of feedforward networks was the replacement of sigmoid hidden units with piecewis linear hidden units, such as rectified linear units. Rectification using the max{0, 2 function was introduced in early neural network models and dates back at least as far  as the cognitron and neocognitron . These early models did not use rectified linear units but instead applied rectification to nonlinear functions. Despite the early popularity of rectification, it was largely replaced by sigmoids in the 1980s, perhaps because sigmoids perform better when neural networks are very small. As of the early 2000s, rectified linear units were avoided because of a somewhat superstitious belief that activation functions with nondifferentiable points must be avoided. This began to change in about 2009',\n",
       " 'b9b42bd1-47b5-4c62-9b0f-592863b27f05': 'MACHINE LEARNING BASICS  oc exp (=H ~Xw)\\' (y— Xw)) exp (-3tw ~ po) AQ w — H0)) (5.75)  1 x exp (-3 (-2y\"Xw +wiX\\'Xw+ w! Ag w — 241) Ay\") ) : (5.76)  We now define Ay, = (XTX +A51) 1 and py = Am (XTy + Ap !uo). Us- ing these new variables, we find that the posterior may be rewritten as a Gaussian distribution:  1 _ 1 _ view | X.y) oc exp (= 5(0 = pn)\" ApH) + 5H Anlst) (5.7)  x exp (= 5(1 = Hn)\" AGM Hm) (5.78)  All terms that do not include the parameter vector w have been omitted; they are implied by the fact that the distribution must be normalized to integrate to 1.\\n\\nEquation 3.23 shows how to normalize a multivariate Gaussian distribution. Examining this posterior distribution enables us to gain some intuition for the effect of Bayesian inference',\n",
       " 'fddd4c55-8c47-4f8a-8b8d-50123b2ea726': 'An image can be quickly converted into its representation in one color channel by isolating that matrix and adding 2 zero matrices from the other color channels.\\n\\nAdditionally, the RGB values can be easily manipulated with simple matrix operations to increase or decrease the brightness of the image. More advanced color augmentations come from deriving a color histogram describing the image. Changing the intensity values in these histograms results in lighting alterations  such as what is used in photo editing applications. Cropping  Cropping images can be used as a practical processing step for image data with mixed height and width dimensions by cropping a central patch of each image. Additionally, random cropping can also be used to provide an effect very similar to translations. The contrast between random cropping and translations is that cropping will reduce the size of the input such as (256,256) — (224, 224), whereas translations preserve the spatial dimensions of the image. Depending on the reduction threshold chosen for  cropping, this might not be a label-preserving transformation. Rotation  Rotation augmentations are done by rotating the image right or left on an axis between 1° and 359°. The safety of rotation augmentations is heavily determined by the rotation degree parameter',\n",
       " '80d69e64-bda8-4df3-ae34-49406b94fac2': '⇤ The importance sampling that we have used in this section, the previous section, and in Chapter 5, enables sound o↵-policy learning, but also results in high variance updates, forcing the use of a small step-size parameter and thereby causing learning to be slow. It is probably inevitable that o↵-policy training is slower than on-policy training—after all, the data is less relevant to what is being learned. However, it is probably also true that these methods can be improved on. The control variates are one way of reducing the variance. Another is to rapidly adapt the step sizes to the observed variance, as in the Autostep method .\\n\\nYet another promising approach is the invariant updates of Karampatziakis and Langford  as extended to TD by Tian (in preparation). The usage technique of Mahmood (2017; Mahmood and Sutton, 2015) may also be part of the solution. In the next section we consider an o↵-policy learning method that does not use importance sampling',\n",
       " 'a90dec4c-1277-45d7-8438-cab4240dded5': 'Many real-world use cases of machine learning involve multiple related classiﬁcation tasks—both because there are multiple tasks of interest, and because available weak supervision sources may in fact label different related tasks. Handling this multi-task weak supervision setting has been the focus of recent work on a new version of Snorkel, Snorkel MeTaL,21 which handles labeling functions that label different tasks, and in turn can be used to supervise popular multi-task learning (MTL) discriminative models .\\n\\nFor example, we might be aiming to train a ﬁne-grained named entity recognition (NER) system which tags speciﬁc types of people, places, and things, and have access to both ﬁne-grained labeling functions—e.g., that label doctors versus lawyers—and coarse-grained ones, e.g., that label people versus organizations. By representing these as different logically-related tasks, we can model and combine these In addition to working on the core directions outlined—realworld deployment, higher-level interfaces, and multi-task supervision—severalotherdirectionsarenaturalandexciting extensions of Snorkel',\n",
       " 'd2777764-8389-49f1-937f-e4a16d29e512': '70.16 = 69.21 64.25 66.58 + SimCSE-BERT»ase 66.68 81.43 71.38 78.43 78.47 75.49 69.92 74.54 RoBERTapase (first-last avg.)\\n\\n40.88 58.74 49.07 65.63 61.48 58.55 61.63 56.57 RoBERTa,s¢-whitening 46.99 63.24 57.23 71.36 68.99 61.36 62.91 61.73 * SimCSE-RoBERTayase 68.68 82.62 73.56 81.49 80.82 80.48 6787 76.50 * SimCSE-RoBERTajarge 69.87 82.97 74.25 83.01 79.52 81.23 71.47 77.47 Supervised models InferSent-GloVe* 52.86 66.75 62.15 72.77 66.87 68.03 65.65 65.01 Universal Sentence Encoder* 64.49 67.80 64.61 76.83 73.18 74.92 76.69 71.22 SBERTyase* 70.97 76.53 73.19 79.09 74.30 77.03 72.91 74.89 SBERT>2s¢-flow 69.78 7127 74.35 82.01 7746 79.12 76.21 76.60 SBERT;,',\n",
       " 'd54e5854-c986-428e-b5ef-a68713994e0f': 'The promise of machine learning as a means for achieving this has been frustrated by the need to craft problem-speciﬁc representations. DeepMind’s DQN stands as a major step forward by demonstrating that a single agent can learn problem-speciﬁc features enabling it to acquire humancompetitive skills over a range of tasks. This demonstration did not produce one agent that simultaneously excelled at all the tasks (because learning occurred separately for each task), but it showed that deep learning can reduce, and possibly eliminate, the need for problem-speciﬁc design and tuning. As Mnih et al. point out, however, DQN is not a complete solution to the problem of task-independent learning. Although the skills needed to excel on the Atari games were markedly diverse, all the games were played by observing video images, which made a deep convolutional ANN a natural choice for this collection of tasks. In addition, DQN’s performance on some of the Atari 2600 games fell considerably short of human skill levels on these games',\n",
       " '94515580-343a-43c1-9f1d-5227db93218f': \"The input always sends a fixed message of v' 8W to the hidden units, but the hidden units constantly update the message they send to each other. Specifically, two units hi and h j inhibit each other when their weight vectors are aligned. This is a form of competition—between two hidden units that both explain the input, only the one that explains the input best will be allowed to remain active. This competition is the mean field approximation’s attempt to capture the explaining away interactions in the binary sparse coding posterior. The explaining away effect actually should cause a multimodal posterior, so that if we draw samples from the posterior, some samples will have one unit active, other samples will have the other unit active, but very few samples will have both active. Unfortunately, explaining away interactions cannot be modeled by the factorial g used for mean field, so the mean field approximation is forced to choose one mode to model. This is an instance of the behavior illustrated in figure 3.6.\\n\\nWe can rewrite equation 19.44 into an equivalent form that reveals some further  insights:  +  1 hj=o b+ v—  Wajh; BW.4— 5W. BW\",\n",
       " '34fdba1a-0137-4769-b41d-c125b7dd5bde': 'In (b), the joint distribution is given by a general form p(x) = f(x1, x2, x3), whereas in (c), it is given by the more speciﬁc factorization p(x) = fa(x1, x2)fb(x1, x3)fc(x2, x3).\\n\\nIt should be emphasized that the factorization in (c) does not correspond to any conditional independence properties. We shall now make use of the factor graph framework to derive a powerful class of efﬁcient, exact inference algorithms that are applicable to tree-structured graphs. Here we shall focus on the problem of evaluating local marginals over nodes or subsets of nodes, which will lead us to the sum-product algorithm. Later we shall modify the technique to allow the most probable state to be found, giving rise to the max-sum algorithm. Also we shall suppose that all of the variables in the model are discrete, and so marginalization corresponds to performing sums. The framework, however, is equally applicable to linear-Gaussian models in which case marginalization involves integration, and we shall consider an example of this in detail when we discuss linear dynamical systems',\n",
       " 'ac448a9e-93f8-4585-8138-446ddec0c0b5': 'Including higher-order interactions and average-pooling of the slab variables  enables the model to learn excellent features for a classifier when labeled data is scarce. Adding a term to the energy function that prevents the partition function from becoming  https://www.deeplearningbook.org/contents/generative_models.html    undefined results in a sparse coding model, spike and slab sparse coding , also known as S3C  20.6 Convolutional Boltzmann Machines  As we discuss in chapter 9, extremely high-dimensional inputs such as images place great strain on the computation, memory and statistical requirements of machine learning models. Replacing matrix multiplication by discrete convolution with a small kernel is the standard way of solving these problems for inputs that have translation invariant spatial or temporal structure. Desjardins and Bengio   679  CHAPTER 20. DEEP GENERATIVE MODELS  showed that this approach works well when applied to RBMs.\\n\\nDeep convolutional networks usually require a pooling operation so that the spatial size of each successive layer decreases. Feedforward convolutional networks often use a pooling function such as the maximum of the elements to be pooled',\n",
       " '973dbd16-1aca-49d0-bf9e-414c5dd3aba7': \"Let x be the target sample ~ P(x|C = 1; 6) = po(x) and X be the noise sample  ~ P(&|C = 0) = ¢q(X). Note that the logistic regression models the logit (i.e. log-odds) and in this case we would like to model the logit of a sample u from the target data distribution instead of the noise distribution:  £(u) = log ot = log pe(u) — log q(u)  https://lilianweng.github.io/posts/2021-05-31-contrastive/  Contrastive Representation Learning | Lil'Log   After converting logits into probabilities with sigmoid o(. ), we can apply cross entropy loss:  Cree = - xd log o(lo(x,)) + log(1 — o(E0(%:)))]  i=l  Hb  Po  where o(£) = 1+ exp(—d) = beng  Here | listed the original form of NCE loss which works with only one positive and one noise sample\",\n",
       " '650a01ea-197e-4398-872b-fb92b3a1aaa0': 'Section 13.3 The hidden Markov model can be viewed as a speciﬁc instance of the state space model of Figure 13.5 in which the latent variables are discrete. However, if we examine a single time slice of the model, we see that it corresponds to a mixture distribution, with component densities given by p(x|z). It can therefore also be interpreted as an extension of a mixture model in which the choice of mixture component for each observation is not selected independently but depends on the choice of component for the previous observation. The HMM is widely used in speech recognition , natural language modelling , on-line handwriting recognition , and for the analysis of biological sequences such as proteins and DNA . As in the case of a standard mixture model, the latent variables are the discrete multinomial variables zn describing which component of the mixture is responsible for generating the corresponding observation xn. Again, it is convenient to use a 1-of-K coding scheme, as used for mixture models in Chapter 9',\n",
       " 'a7bd6c13-1f8f-4db5-92a8-67a183a0f200': 'In the bandit algorithms the baseline was just a number (the average of the rewards seen so far), but for MDPs the baseline should vary with state.\\n\\nIn some states all actions have high values and we need a high baseline to di↵erentiate the higher valued actions from the less highly valued ones; in other states all actions will have low values and a low baseline is appropriate. One natural choice for the baseline is an estimate of the state value, ˆv(St,w), where w 2 Rd is a weight vector learned by one of the methods presented in previous chapters. Because REINFORCE is a Monte Carlo method for learning the policy parameter, ✓, it seems natural to also use a Monte Carlo method to learn the state-value weights, w. A complete pseudocode algorithm for REINFORCE with baseline using such a learned state-value function as the baseline is given in the box below',\n",
       " '4d2569ad-2247-444c-9627-c9bbc0916d5d': 'In relevant recent work on autoencoders  it was shown that the training criterion of unregularized autoencoders corresponds to maximization of a lower bound (see the infomax principle ) of the mutual information between input X and latent representation Z. Maximizing (w.r.t. parameters) of the mutual information is equivalent to maximizing the conditional entropy, which is lower bounded by the expected loglikelihood of the data under the autoencoding model , i.e. the negative reconstrution error. However, it is well known that this reconstruction criterion is in itself not sufﬁcient for learning useful representations . Regularization techniques have been proposed to make autoencoders learn useful representations, such as denoising, contractive and sparse autoencoder variants . The SGVB objective contains a regularization term dictated by the variational bound (e.g. eq. (10)), lacking the usual nuisance regularization hyperparameter required to learn useful representations',\n",
       " '87227887-868b-4720-812d-606ccf719916': 'The prediction of the ensemble is given by the arithmetic mean of all  these distributions, k  i p k SS (y|@). (7.52)  https://www.deeplearningbook.org/contents/regularization.html    —  In the case of dropout, each submodel defined by mask vector H defines a probability distribution p(y (y | wp) pu). The arithmetic mean over all masks is given by  rw) ply | x, 1), (7.53)  where p(s) is the probability distribution that was used to sample yz at training time. Because this sum includes an exponential number of terms, it is intractable to evaluate except when the structure of the model permits some form of simplification. So far, deep neural nets are not known to permit any tractable simplification. Instead, we can approximate the inference with sampling, by averaging together he output from many masks.\\n\\nEven 10-20 masks are often sufficient to obtain good performance. An even better approach, however, allows us to obtain a good approximation to he predictions of the entire ensemble, at the cost of only one forward propagation',\n",
       " '2a79d43d-5048-4c8a-aef7-6ea3028e5cff': 'What is the space of methods lying between Monte Carlo and TD methods? Consider estimating v⇡ from sample episodes generated using ⇡. Monte Carlo methods perform an update for each state based on the entire sequence of observed rewards from that state until the end of the episode. The update of one-step TD methods, on the other hand, is based on just the one next reward, bootstrapping from the value of the state one step later as a proxy for the remaining rewards. One kind of intermediate method, then, would perform an update based on an intermediate number of rewards: more than one, but less than all of them until termination. For example, a two-step update would be based on the ﬁrst two rewards and the estimated value of the state two steps later.\\n\\nSimilarly, we could have three-step updates, four-step updates, and so on. Figure 7.1 shows the backup diagrams of the spectrum of n-step updates for v⇡, with the one-step TD update on the left and the up-until-termination Monte Carlo update on the right. The methods that use n-step updates are still TD methods because they still change an earlier estimate based on how it di↵ers from a later estimate',\n",
       " '8aab867b-cb51-44fd-8c50-22444eb3afdb': 'To do so, we change to using the geometric mean rather than the arithmetic mean of he ensemble members’ predicted distributions. Warde-Farley et al. present arguments and empirical evidence that the geometric mean performs comparably (0 the arithmetic mean in this context. The geometric mean of multiple probability distributions is not guaranteed to be a probability distribution. To guarantee that the result is a probability distribution, we impose the requirement that none of the submodels assigns probability 0 to any event, and we renormalize the resulting distribution. The unnormalized probability distribution defined directly by the geometric mean is given by  Pensemble( y | x) = 2d Ifo y | x Ht); (7.54)  where d is the number of units that may be dropped. Here we use a uniform distribution over yz to simplify the presentation, but nonuniform distributions are also possible. To make predictions we must renormalize the ensemble:  Pensemble (y | x) Ly Pensemble(y’ | xv) 259  Pensemble(Y | x) = (7.55)  CHAPTER 7',\n",
       " 'eade1691-a381-4fdd-aaf6-0d4a57d356e8': 'it eventually makes $$\\\\text{SGD}(\\\\mathbb{E} |tau, \\\\theta, k)diverge from\\\\mathbb{E}|tau $$ when k > 1. The Optimization Assumption  https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil\\'Log   Assuming that a task T ~ p(T) has a manifold of optimal network configuration, W>. The model fg achieves the best performance for task 7 when @ lays on the surface of W*. To find a solution that is good across tasks, we would like to find a parameter close to all the optimal manifolds of all tasks:  0* = arg min En p(r) = Vol; dist(6, W:(6))?] 1 * = Vol 5 (0— W7(9))\" =60- W;. (0) ; See notes',\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01c21962",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/lichenghu/Desktop/embedding_finetune/training_data.json\", 'w+') as f:\n",
    "    json.dump(training_data, f)\n",
    "\n",
    "with open(\"/Users/lichenghu/Desktop/embedding_finetune/test_data.json\", 'w+') as f:\n",
    "    json.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dc23f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"ANTHROPIC_API_KEY\"]=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90c6eb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "series_of_training_data=pd.Series(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1bfff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_portion_1=series_of_training_data[:1000].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "040c196f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_portion_2=series_of_training_data[1000:2000].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f04c5635",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_portion_3=series_of_training_data[2000:3000].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e532117",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_portion_4=series_of_training_data[3000:].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1add22b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_portion_1)+len(train_data_portion_2)+len(train_data_portion_3)+len(train_data_portion_4)==len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "eecf768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_queries(\n",
    "    docs,\n",
    "    num_questions_per_chunk=2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Automatically generate hypothetical questions that could be answered with\n",
    "    doc in the corpus.\n",
    "    \"\"\"\n",
    "    chat = ChatAnthropic(model='claude-2')\n",
    "\n",
    "\n",
    "    queries = {}\n",
    "    relevant_docs = {}\n",
    "    for doc_id, text in tqdm(docs.items()):\n",
    "        context_str=text\n",
    "        massage= [HumanMessage(\n",
    "        content=(\n",
    "            f\"\"\"\\\n",
    "                    Context information is below.\n",
    "\n",
    "                    ---------------------\n",
    "                    {context_str}\n",
    "                    ---------------------\n",
    "\n",
    "                    Given the context information and not prior knowledge.generate only questions based on the below query.\n",
    "\n",
    "                    You are a Teacher/ Professor. Your task is to setup \\\n",
    "                    {num_questions_per_chunk} questions for an upcoming \\\n",
    "                    quiz/examination. The questions should be diverse in nature \\\n",
    "                    across the document. Restrict the questions to the \\\n",
    "                    context information provided.\"\n",
    "                    \"\"\"\n",
    "                )\n",
    "        )]\n",
    "        response = chat(massage)\n",
    " \n",
    "        result = response.content.strip().split(\"\\n\")\n",
    "        question = [\n",
    "            re.sub(r'^[0-9a-zA-Z]+\\)?.?', \"\", question).strip() for question in result\n",
    "        ]\n",
    "#         print(question)\n",
    "#         print(\"----------\")\n",
    "        questions = [q for q in question[1:] if len(q) > 0]\n",
    "        \n",
    "        for question in questions:\n",
    "            question_id = str(uuid.uuid4())\n",
    "            queries[question_id] = question\n",
    "            relevant_docs[question_id] = [doc_id]\n",
    "    return queries, relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8d608bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM']=\"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1c43be78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.002285003662109375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46dadac21bbd41f4beb6bd14bcd6c4d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_queries_portion_1, train_relevant_docs_portion_1 = generate_queries(train_data_portion_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7becea06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0028820037841796875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81b1c4fc645447668658d466e17202f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_queries_portion_2, train_relevant_docs_portion_2 = generate_queries(train_data_portion_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b4cd4f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d7050b0e-3f20-45d0-ae4e-9aca5aa278f8': 'According to the passage, what is the philosophy behind deep learning?',\n",
       " '45d5af18-19fd-4725-ac6e-08fbf59ae87a': 'The passage mentions that deep learning has been used to solve problems in areas like computer vision and speech recognition. Can you name one more application area where deep learning has been applied?',\n",
       " '9b4a36c0-6cd6-439b-98f0-c9cf5b0a2b63': 'How does the approximate value function simplify computation compared to a standard weighted sum, given that each component is either 0 or 1?',\n",
       " '11fb9594-9c72-4c71-ba8b-d914e930975d': 'What causes the qualitatively different generalization patterns shown in the upper half of Figure 9.11?',\n",
       " '3c708ae3-1f86-41a4-a551-5338e15278b1': 'What paper by Sutton proposed an incremental version of the delta-bar delta algorithm for adapting bias by gradient descent?',\n",
       " '0eefc08b-8124-409a-bd12-13b28f41cb03': 'In what publication did Sutton discuss modeling the world at different timescales using TD models?',\n",
       " '21ffcf69-e74d-4e3e-8764-f90d4cf69ce2': 'What are the two main types of machine learning algorithms described in the passage?',\n",
       " '750011ba-53ac-4c54-96e2-ff356b4a84e2': 'According to the passage, what is the key difference between supervised and unsupervised learning algorithms?',\n",
       " 'fe4d3a62-caad-4f40-92d0-59fdb5d22e1c': 'The paper by Mikolov et al. proposed an approach for estimating word representations in vector space. What was the name of the model they proposed?',\n",
       " '4a6b9e3a-ad64-4973-9cb4-37024b9cf93b': 'The paper by Noroozi and Favaro presented an unsupervised method for learning visual representations. What task did they use as a pretext task to learn these representations?',\n",
       " '6071c6f7-ac40-4056-99f0-2b9aef7b359c': 'According to the passage, what book discusses programming for feedback control?',\n",
       " '71077099-c05f-41a7-8dca-0ef0d6103086': \"The passage mentions Li et al.'s work on a contextual-bandit approach to personalized news article recommendation. What conference proceedings was this work published in?\",\n",
       " '6fd06e2b-d9c6-49e8-b759-4a4cfe764c7e': 'In directed graphical models, what constraint is placed on how we define the conditional distributions?',\n",
       " '8c23398f-7323-4ab6-9f12-0379e5adc3bc': 'What are two other common names for undirected graphical models?',\n",
       " 'c40b3284-80b7-4cb0-b336-43216d404df3': 'What is the issue that arises when using a high-order polynomial model on this data set? Why does this occur?',\n",
       " 'a82688ea-a8e8-4a88-8607-aacf62693720': 'The context discusses evaluating the model on both a training set and a test set. What is the purpose of evaluating performance on a separate test set, rather than just the training set?',\n",
       " '50738915-6519-4292-8da8-e035b22df6e6': 'What norm is commonly used in machine learning when it is important to discriminate between elements that are exactly zero versus elements that are small but nonzero?',\n",
       " '0f0ecff5-34f3-4127-922c-d10b4f1a41b0': 'True or False: The number of nonzero entries in a vector is a valid norm. Explain your answer.',\n",
       " '08dc8a0a-a2bc-493e-8fed-97c1a19bde82': 'According to the passage, why might it be desirable to use a separate penalty with a different α coefficient for each layer of a neural network?',\n",
       " '65bc2ab2-920f-4d96-b669-08b0ee946b3c': 'The passage mentions that using the same weight decay at all layers can help reduce the size of the search space. What is meant by \"search space\" in this context?',\n",
       " 'e5ae6e21-4699-472f-9c3c-24388d80908c': 'What is the key benefit of using a table-filling strategy like dynamic programming for implementing backpropagation?',\n",
       " '1dfbfc9a-7d7e-41fd-a1b7-6cfa0b83b6ff': 'What algorithm does backpropagation use to avoid recomputing intermediate results when calculating gradients for each node in the graph?',\n",
       " 'e2c16c6e-2cf9-48da-adbe-c61a46c226ca': 'What are the two categories of techniques used to avoid collapse in self-supervised learning models? Briefly explain how each category works to avoid collapse.',\n",
       " 'ada67c1e-2d13-49b4-9aeb-823993162e6f': 'Contrastive self-supervised learning methods work by constructing incompatible input pairs (x,y). Explain why constructing these incompatible pairs helps the model avoid collapse during training.',\n",
       " '21faf13c-805e-49b4-ba4d-c832e4e054f9': 'According to the passage, what is the steady state distribution?',\n",
       " 'c5bce295-6a81-4349-b281-cbd5d582636c': 'What is the difference between the average-reward value functions and the differential value functions discussed in the passage?',\n",
       " '6ae594b2-c29b-483d-9247-365f62ba36ba': 'What is a common issue that arises when performing continuous math on a digital computer?',\n",
       " '123d77f4-ee25-4262-9b07-a78503531e01': 'What type of rounding error can be particularly devastating for algorithms that work in theory but fail in practice?',\n",
       " '87ffda59-193c-4d16-9d8f-449d87b34601': 'What is a more realistic assumption for how often our laser sensor provides a measurement in the given example?',\n",
       " 'ed4d615c-c6e6-4e2e-b22c-f76870c6b962': 'When we work with multidimensional arrays of data and parameters in machine learning, what do we refer to these as?',\n",
       " 'ade9594c-d2bc-4d39-aa90-a1eb6fac6f27': 'What is the goal of the model-agnostic meta-learning algorithm presented?',\n",
       " '1a9e6557-da0f-4ce4-b7b5-4cf300a69214': 'True/False: The meta-update step in the algorithm uses the same examples as the inner loop gradient descent step.',\n",
       " 'c00beb49-394e-4cf6-a4d8-587a272eaea8': 'According to the passage, under what condition can a factor node send a message to a variable node?',\n",
       " 'd93aa0f1-0fb5-4f14-9a7c-d3b58c949427': 'The passage states that the term Gm(xm, Xsm) is given by the product of terms Fl(xm, Xml) associated with which nodes linked to node xm?',\n",
       " '47da1c59-11f7-4203-a1e3-fcad5bbf4129': 'What was one of the first explorations of backpropagation that introduced distributed representations for symbols?',\n",
       " 'c3832b85-92a2-477f-bc7a-c32f7e18a2d2': 'Who extended the idea of forming an embedding for a symbol to forming an embedding for a word?',\n",
       " 'cf7c7fae-53f1-4e6b-8fae-944bf2775be4': 'What property of the covariance matrix Σ must be satisfied when maximizing the log likelihood function with respect to Σ?',\n",
       " '0f279653-5317-4640-85e4-c54a93da8f73': 'Derive the expression for the sample covariance matrix (2.122) by maximizing the log likelihood function (2.118) with respect to Σ. You may ignore the constraints on Σ being symmetric and positive definite for the purposes of this derivation.',\n",
       " '45918455-f98e-4562-bb42-51f60421980a': 'According to the passage, how does the TD(λ) update rule reduce to the TD(0) update rule?',\n",
       " '8664a630-fe6e-48d1-8968-364c5d24d94c': 'The passage mentions \"shouting back\" TD errors to previously visited states. What mechanism causes the impact of the TD error to diminish for temporally distant states?',\n",
       " '6b90d659-8e85-48fb-b6a9-ec54c12a00e3': 'According to the context, if we do not have any prior knowledge about the mean of y(x), how do we choose it by symmetry?',\n",
       " '4534b171-2282-4f37-9ca4-ae7a4267b796': 'In the context, what two things fully specify the Gaussian process y(x)?',\n",
       " 'e5d33121-136f-4800-84ce-ca40f5e837ae': 'According to the context, maximizing the posterior distribution is equivalent to minimizing what function?',\n",
       " 'a8f4cc1f-0188-4bb2-8de2-1f5917d6241f': 'What key probability concept does a fully Bayesian approach require that is not done when simply making a point estimate of w?',\n",
       " '6552a63b-e9ab-4a78-b80a-97b0930fe46d': 'Explain the relationship between explanation-based learning and reinforcement learning as discussed in the Prieditis and Russell paper. What unified view do the authors present?',\n",
       " '6a86c7ea-1445-41a9-896c-72900620e683': 'According to the Dolan and Dayan paper, what are the differences between goals and habits in the brain? How do the neural systems underlying each process differ?',\n",
       " '5e262f67-f97b-4cda-8d79-f7d3d52f0cd3': 'The paper by Xie et al. in 2017 proposed using what technique to improve neural network language models?',\n",
       " '70866376-6a25-495e-aae9-f33597ba5c5d': 'Wu et al. in 2019 discussed easy data augmentation techniques for which type of tasks?',\n",
       " 'c82cd067-03bc-47a9-97d8-2dce9728adea': 'What are the two models that are simultaneously trained in the proposed adversarial framework?',\n",
       " 'e43586c3-3509-4c94-bf43-30ab6b1141f6': 'What is the training procedure for the generative model G in the adversarial framework?',\n",
       " '6244f342-f415-45c2-8e18-a715ef139c6f': 'The 2020 EMNLP paper by Malandrakis et al. proposes an adversarial augmentation technique for which two NLP tasks?',\n",
       " '8487c04c-c508-4851-92b3-859787c16b1e': 'The 2019 Hong Kong ACL paper by Malandrakis et al. describes a method for controlled text generation that could be used for what natural language processing purpose?',\n",
       " '33d45222-f969-45fc-b0ba-8de865254bf4': 'Assume all the data values {xn} are fully observed. Show that minimizing the cost function J in this case reduces to the EM algorithm for probabilistic PCA presented in Section 12.2.2.',\n",
       " 'f12ff4ad-20aa-4312-8987-a4d9e373f743': 'Let {zn} and {xn} be the latent variable vectors and observed data vectors respectively. Further assume the data matrix X has been mean-centered, i.e. Xn = xn - x̄. Derive the expression for the cost function J after minimizing it with respect to μ. Your expression should have terms involving only Xn and Zn.',\n",
       " 'b5a34a30-1800-428d-8261-1aa8c0655c4f': 'What is the basic structure of the learning process in the EM algorithm?',\n",
       " '11752b4f-b0b9-4a06-a03b-4adaa3746af1': 'What key insight in the EM algorithm allows us to continue using one value of θ even after we have moved to a different value of θ?',\n",
       " '30612b35-e6f5-4838-a77f-80f4be51f461': 'According to the passage, what are the two major threads leading to the modern field of reinforcement learning?',\n",
       " 'fdb945dc-c3d9-4c51-b02b-a140bcb9d755': 'The passage mentions Alexander Bain and Conway Lloyd Morgan. What key ideas did they contribute related to trial-and-error learning?',\n",
       " '736821e3-2c91-44e9-bbad-395110495c15': 'According to the passage, what is the requirement for the constant k when using rejection sampling?',\n",
       " '0c957e10-c627-44cb-83fb-4643fc1d8b51': 'Referring to Figure 11.5 in the passage, which distribution is being used as the proposal distribution for sampling from the gamma distribution, and why was it chosen?',\n",
       " '8a4d1cd4-8c7f-48bc-b5ba-1c21b2337593': \"The Student's t-distribution generalizes the Gaussian distribution and provides estimates that are more robust to outliers. How does the t-distribution behave as the degrees of freedom parameter ν approaches infinity?\",\n",
       " 'aa6bd9f8-ae1b-46d6-a891-337f70026f46': 'Consider a random variable X that follows a uniform distribution over the interval [a, b]. What is the distribution of the random variable Y = a + (b - a)X?',\n",
       " '3fba92f1-6f35-4797-923f-a3560daa7536': 'What is a V-structure in probability graphs and how does it relate variables?',\n",
       " '882118ac-8b43-449c-a022-4e26f318d958': 'Give an example of a V-structure using variables a, b, and s from the provided context. Explain the relationship between a and b when s is observed versus when it is not observed.',\n",
       " 'a08cd004-7c7e-4508-bedd-0646c27547f8': 'What property must the state have in order for a Markov decision process to satisfy the Markov property?',\n",
       " '2321bffd-4823-4d67-b2a6-884703b52efa': 'According to the context, what are the only two things that the probability distribution p in a Markov decision process depends on?',\n",
       " '8fbb65db-8a42-4885-87ae-4cc05cb17438': 'What method did Hengyi Cai et al. propose in their 2020 paper to improve neural dialogue generation?',\n",
       " 'fb3a13ec-4d04-49c0-a082-adb79843f6d7': 'What semi-supervised learning approach did Rui Cai and Mirella Lapata use in their 2019 paper to improve semantic role labeling?',\n",
       " 'f9f93124-a77d-4a54-a873-ebcc5cb21e88': 'What are the two networks that make up a generative adversarial network (GAN)?',\n",
       " 'abf2cdae-accf-44a4-8c24-e89c988b8426': 'In a GAN, what value does the discriminator network output, and what does this value indicate?',\n",
       " '2175afb6-ebf6-4d7c-a723-89276323e6e4': 'According to the variational distribution q(Z,η), how do the latent variables Z and parameters η factorize?',\n",
       " '784f18df-ca2e-4c1e-bf88-2f7871227d9e': 'What is the form of the optimized variational distribution q*(Z)? How does it factorize and why?',\n",
       " 'a4c039b4-c8ad-4a75-a09f-2a67e362db14': 'What happens to the mean squared error of an ensemble model as the errors of the individual models become more correlated?',\n",
       " '18453050-c87c-4846-8dd0-5c54667b95cc': 'Bagging involves training the same type of model on different datasets. True or False? Explain.',\n",
       " 'c8b64bcb-f231-4197-9ae0-de06045daa88': 'Which researchers proposed a framework for mesencephalic dopamine systems based on predictive Hebbian learning?',\n",
       " '5ca6be2f-5703-43ad-87eb-16d79ecf09de': 'What journal published the paper \"Computational Psychiatry\" by Montague, Dolan, and Friston?',\n",
       " '46a906db-d14b-4e0a-9f73-b1b8a2d1dafc': 'What issue with discounting in reinforcement learning did Singh, Jaakkola, and Jordan identify?',\n",
       " '8c54288e-7b24-4592-91a8-38b5bfc81fdb': 'According to the passage, why is extending off-policy learning with function approximation more challenging than extending on-policy learning with function approximation?',\n",
       " '36f8b6c9-27b0-479c-a014-5d1af6f688c0': 'What are the two main approaches for performing data augmentation discussed in the passage?',\n",
       " 'c08ebe83-1eb9-4a27-b64f-a5eef3696f6f': 'According to the passage, what are some of the key considerations when deciding whether to use online or offline data augmentation?',\n",
       " '36a2171f-cc24-443c-afe8-b59d338a078a': 'The passage mentions that AI agents need the capability to improve from heterogeneous signals in different forms, at varying granularities, from diverse sources, and with different intents. What are some examples of the different forms, granularities, sources, and intents of signals that could be useful for an AI agent to learn from?',\n",
       " 'ac009c24-bc86-4d23-a6c0-6ee5678fe021': 'The passage proposes combining multiple individual experience functions into a weighted composition as a way to enable versatile learning in an AI agent. What are some potential benefits and challenges of this proposed approach to combining diverse learning experiences?',\n",
       " '70107c75-989b-4640-87aa-6fff1e5a7018': 'In the calculus of variations, we seek to find which type of function y(x) that maximizes or minimizes a given functional F?',\n",
       " '10b26f5f-f63a-4eeb-b4df-9c02799fb435': 'What is an example of a physics problem that can be solved using the calculus of variations?',\n",
       " 'f4a530d5-3e09-47cf-a8f9-5b2279ec48c8': 'How can we control the size of the constraint region when using quadratic penalty functions?',\n",
       " '18621887-d6c5-44bb-89a2-79b8e4f38b3e': 'What are two reasons we may wish to use explicit constraints and reprojection rather than enforcing constraints with penalties when training neural networks?',\n",
       " '34278541-325a-47c7-8d39-fa935794928a': 'According to the passage, what is the key tension in artificial intelligence between breadth of applicability and mathematical tractability? What trade-offs does this tension imply?',\n",
       " '3e5a09b5-988e-4fd2-b667-97b4ca85c5a1': 'The passage discusses MDPs (Markov decision processes) as a way to frame the problem of reinforcement learning. What are the key components of an MDP as described in the passage? Specifically, what are the \"agent\" and \"environment\" and how do they interact?',\n",
       " '79cf308c-ed40-4326-8971-aa1c01099080': 'According to the passage, why is it usually sufficient to shuffle the dataset order only once before training machine learning models?',\n",
       " '3283dcdb-0b39-42ed-aa14-3e912d681427': 'The passage mentions that computing updates over different minibatches in parallel can help optimize machine learning algorithms. What term is used to describe these asynchronous parallel distributed approaches?',\n",
       " '6206b328-ec6f-4d62-bcc3-0f29a8d1f8ac': 'In the factor analysis model, what is the advantage of expressing the marginal distribution for the observed variable x in a form that involves inversion of matrices of size M x M rather than D x D?',\n",
       " '6a32728d-1481-4220-9c03-194a0b2ee449': 'How do the M-step equations for probabilistic PCA and factor analysis differ, and what does this imply about their behavior under transformations of the data set?',\n",
       " '57b14f21-da79-43c0-831a-253745cbdce6': 'What approach does the paper propose for learning different data manipulation schemes?',\n",
       " '8cc72b67-4b42-4f56-9447-8d1a857b7eb1': 'According to the paper, how can different parameterization of the \"data reward\" function instantiate different manipulation schemes?',\n",
       " '2ba2b06c-d46a-4a99-b250-edff47e0a141': 'True/False: When using max-sum message passing on a tree-structured factor graph, it is sufficient to just run a forward pass of message passing followed by a backward pass in order to find a globally optimal configuration.',\n",
       " 'c2d5c838-4d38-4cb5-8b96-8b5e1631c2ca': 'Fill in the blank:',\n",
       " '9ad0495f-8696-49d7-8c18-4297891a64fd': 'sending a message from a factor node f to a variable node x in max-sum message passing, a __________ is performed over all other variable nodes x1, ..., xM that are neighbors of that factor node. To find a globally optimal configuration, it is necessary to keep track of the ____________ states during the forward pass using the functions φ(xn).',\n",
       " '0a340a9f-93bd-4b80-8a01-6d4638ebba2d': \"How does BERT's pre-training approach differ from that of Radford et al.?\",\n",
       " '8b1901e7-75d4-4cf7-971c-52c94db8673e': 'What two pre-training tasks does BERT use, and what does each enable?',\n",
       " '692fc003-b5e0-4b1e-b482-b0690ce38b74': 'Which university is located in Pittsburgh, PA according to the passage?',\n",
       " 'b27e90b3-89b4-4d07-9e5f-f93bce1975f0': 'What does the paper by Gordon in Advances in Neural Information Processing Systems 13 discuss, according to the passage?',\n",
       " '8b0ee04c-1ea6-4b08-a073-a624f2c06f43': 'What are two problems that can arise with gradients during neural network training that make optimization difficult?',\n",
       " '6c42a92d-d845-44d0-b3bb-0a7a6b23efab': 'How can recurrent neural networks be more susceptible to vanishing/exploding gradients compared to feedforward networks?',\n",
       " '024bd58a-e1ae-4025-903e-34f981ac7dcb': \"Jensen's inequality can be used to show that the Kullback-Leibler divergence satisfies what property, given that -ln(x) is a convex function?\",\n",
       " 'c8a01af5-ee56-4b38-823c-75929f9382d9': \"For a continuous random variable x with probability distribution q(x), what is the form of Jensen's inequality that can be applied, as mentioned in the context?\",\n",
       " 'c6625685-ec0d-4933-8b8d-a8c0ee3084f9': 'What property of SGD allows it to converge even with very large datasets?',\n",
       " '8114ca5f-36d0-43f9-a2d9-42e756a8a2b6': 'How does the convergence rate of batch gradient descent compare theoretically to stochastic gradient descent for convex and strongly convex problems?',\n",
       " 'ab16ce01-f565-4444-9805-fdfc5eacf4b6': \"What are some benefits of training a student neural network to predict the outputs of a teacher network's middle layer?\",\n",
       " '3debe292-ec58-4991-8873-9a782933d7b4': 'According to the context, what are two objectives of the lower layers of a student network when trained using knowledge distillation?',\n",
       " '076f9f8f-c9d6-488a-892e-d8448da02b56': 'Using the formulas provided in the text, derive the expression for the marginal distribution p(zn|x) in a hidden Markov model in terms of the scaled variables.',\n",
       " 'a5bad4ed-a6d5-4dc2-a1ef-2bd363e59002': 'Starting from the joint distribution p(z1,...,zN,x1,...,xN), show step-by-step how to derive the Viterbi recursion equation (13.68) where the ω(zn) terms are defined by (13.70). Your derivation should clearly demonstrate how to obtain the initial condition given by (13.69).',\n",
       " 'ba500a87-4fc1-4cc1-b6b3-4d33313c5ab9': 'What are some early approaches that were used to model relations between entities? The context mentions using vectors for entities and matrices for relations as one early approach.',\n",
       " '8bb992a2-2304-430a-8929-6160d5639679': 'According to the passage, what are some practical short-term applications of models that learn relations between entities? The passage states that link prediction, predicting missing arcs in a knowledge graph, is one practical short-term application of such models.',\n",
       " '4ee0007b-c331-4c26-a0e5-c366b04a0f04': 'How does a distributed representation with n features assign codes to input regions, compared to a non-distributed representation like the nearest neighbor algorithm?',\n",
       " '7dc92949-e742-4fb3-a712-578a4f962628': 'If a distributed representation has d input dimensions, what shape does it divide R^d into when assigning codes to input regions?',\n",
       " '006bc71c-a8e4-4823-a4ec-5d19822ff49f': 'According to the passage, why is it best to wait until you reach your destination before revising your time estimate?',\n",
       " '676a85ae-8f8b-4965-bddc-9109c20bab65': 'The passage provides an example of being stuck in traffic on the way home from work. If your initial prediction was that the drive would take 30 minutes, but 25 minutes into your drive you were still stuck in traffic, how much longer did you estimate it would take to get home from that point?',\n",
       " '4a7c466d-11aa-4b09-becb-2d123e687ff2': 'What methods were used to develop labeling functions in the tutorial notebooks? How much time in total was provided for this task?',\n",
       " '95ad68cf-0cc3-42d5-aea7-309a5150c917': \"What was done to establish a baseline to compare against the performance of models trained on users' hand-labeled data? How did the performance compare?\",\n",
       " '223f9816-995e-4465-9e7e-221aed840313': 'What are two benefits of using the sequential minimal optimization (SMO) algorithm for training support vector machines compared to other decomposition methods like chunking?',\n",
       " 'ffba4baf-b111-4f8b-ae09-9ac762608c72': 'According to the passage, what are two ways that sequential minimal optimization (SMO) differs from the chunking technique for training support vector machines?',\n",
       " '7a1feacc-5f15-472f-9d26-464f47c559dc': 'In a first-order Markov chain model, how many parameters are needed to specify the conditional distribution p(x_n|x_{n-1}) if there are K possible states for each observation x_n?',\n",
       " '81b527e4-09cf-422e-b28d-870d91e514ea': 'How does moving to a higher-order Markov chain allow earlier observations to have more influence on the predictions? What is one disadvantage of using higher-order Markov chains?',\n",
       " 'c3e106c8-a822-4440-a4f5-dd7423fa2c66': 'Gibbs sampling can be used to draw samples from which type of graphical model?',\n",
       " '8cd17083-1217-42ae-bf93-89db74b46c2a': 'Directed',\n",
       " 'f2c43f25-f054-4d07-8cb5-35ec3c09c9c4': 'Undirected',\n",
       " 'c7ef1895-6510-42ff-85be-c247585c478e': 'Cyclical',\n",
       " '87ff93ad-0b79-4eee-a88a-a8faca7c57ef': 'Acyclical',\n",
       " 'e5d017c8-997f-4d3a-8223-2c6ce11072fd': 'Why is ancestral sampling not efficient for models where the posterior distributions are not explicitly specified and parameterized?',\n",
       " '886fb2a7-e2ab-4e7b-9b89-63fb9c377a5f': 'The posterior distributions need to be inferred which is costly',\n",
       " '9abcac83-764c-4f40-8deb-87b72df4ca3b': 'Ancestral sampling requires a directed model',\n",
       " '0b173f5c-a915-4a8f-96ab-83032cad156c': 'The root nodes are unknown',\n",
       " 'aebe0e43-a3c4-4351-83d5-28e322140653': 'There are too many edges',\n",
       " '871bfddf-2de7-462b-95a4-1ca00a92e147': 'What evaluation metric is used to report performance on the Food-101 dataset?',\n",
       " '93989d86-c017-456b-ba35-f8da11182dd8': 'According to the context, for which 2 datasets did the creators define multiple train/test splits, with results reported only for the first split?',\n",
       " 'd9d01b8b-fb96-496d-aed5-4fc6b7ff8928': 'Which function suffers from underfitting in the given examples?',\n",
       " '10b86a4c-1806-4655-85f5-83dd94255a55': 'What issue does the degree 9 polynomial function demonstrate in the provided example?',\n",
       " 'b1db79f0-0c50-490e-af45-750e5e04216f': 'Consider the two decision trees A and B described in the context. Show that the misclassification rates for trees A and B are equal. Also, show that the cross-entropy and Gini index are lower for tree B compared to tree A.',\n",
       " '4d0e87f7-f5b7-4315-aead-e8792a5c6cc1': 'Let t be a vector representing multiple target values. Extend the results in Section 14.5.1 for a mixture of linear regression models to handle multiple target values t, using the techniques from Section 3.1.5.',\n",
       " '49509bb1-b156-4df0-9d3a-9fcfdcaedc2c': 'What is the key advantage of the mixture density network approach over the hierarchical mixture of experts approach?',\n",
       " '4533f68f-782f-4a0a-b426-8839c502f025': 'How do the splits of the input space differ between the mixture density network and hierarchical mixture of experts? Specifically, how are they further relaxed in the mixture density network?',\n",
       " 'a1cc852d-6218-46e5-8d78-3e44ab0277f7': 'What was the primary performance metric that was optimized during the Street View transcription project, since a 98% accuracy goal was set?',\n",
       " 'de65c1a4-a0f6-4ec2-81a4-fbc0578d9d0c': 'What was the next step recommended after choosing quantitative goals for the Street View transcription project?',\n",
       " '60bbd2d1-2987-4977-aba3-9c0d0dcf165b': 'What method did Roweis and Ghahramani propose to unify various unsupervised learning algorithms?',\n",
       " '93e4fe84-17b5-44a1-9de7-5aebc89d6456': 'Which researchers presented Markov logic networks that combine Bayesian networks and first-order logic for performing uncertain inference?',\n",
       " 'c164c3bc-fa63-4987-8463-7be9493cc00a': 'Using the properties of the matrix U, what is the determinant |J| of the Jacobian matrix equal to?',\n",
       " '7d394213-e370-4974-bdda-3156431e9a05': 'What property of the eigenvectors defines a new set of shifted and rotated coordinates with respect to which the joint probability distribution factorizes into independent distributions?',\n",
       " 'ab5b1ed7-fa76-4e07-aa8c-6c07a764e96c': 'What is the term used in MATLAB for convolution with no zero padding, where the kernel is only allowed to visit positions fully contained within the image?',\n",
       " '41156c7e-5627-43dc-9574-0b12de8ca722': 'If the input image has width m and the convolution kernel has width k, what will be the width of the output feature map when using valid convolution with no zero padding?',\n",
       " 'da792c92-e294-4903-aece-3bdfd31d7639': \"What university press published Kakade's Ph.D. thesis on the sample complexity of reinforcement learning?\",\n",
       " '396c89ea-4fa6-4ac8-9241-f7438317a291': 'In what book chapter did Kamin discuss \"attention-like\" processes in classical conditioning?',\n",
       " '9cace48c-e004-4397-be28-90a867d89a5f': 'What is the final simplified objective that PCA optimizes for?',\n",
       " '9652d47d-e1f1-43f0-9359-21bcc9cf5354': 'Fill in the blanks:',\n",
       " '6c1a2e55-1736-4d5b-8974-9ead4a7653e1': 'finds the _____ that maximizes the _____ of the _____ matrix onto the _____. The constraint is that the _____ vectors must have _____ length.',\n",
       " 'c6e77946-a3a0-4021-b72b-072de80bb38f': 'What parameter governs the variance of the conditional distribution in the principal component analysis model described?',\n",
       " '245d44e2-6dba-4c88-bbd5-d224747ed84b': 'According to the passage, do the columns of W span the full data space or a subspace within the data space? Why?',\n",
       " 'd7099916-6364-4089-a437-1bcb03348734': 'According to the text, what does an infinitely strong prior place zero probability on?',\n",
       " 'e2118213-4cce-49ba-911a-f45b500cc468': 'The text states that an infinitely strong prior places zero probability on some parameters. What chapter is this statement from?',\n",
       " '4416e9d9-2efb-48f3-a881-4d8711d87097': 'Which author discussed bacterial chemotaxis as a model behavioral system?',\n",
       " '94c03683-60cf-4fa1-93a8-d0a898f4b750': \"What pressing published John R. Koza's book on Genetic Programming in 1991?\",\n",
       " '6cca126a-e1b9-47f0-8aea-482034e88720': 'According to the context, what is one reason applying PCA is not useful when M is greater than N?',\n",
       " '02a6a6fa-7b01-499a-bb17-418c6d6132e7': 'The context mentions the computational cost of typical algorithms for finding eigenvectors scales like O(D^3). For an image processing application with a large number of pixels D, what issue does this cause?',\n",
       " '4011ecd8-4104-40a6-b859-b75bb36022cb': 'Why is maximum likelihood often preferred for training sigmoid output units?',\n",
       " 'ae3968c4-8ad1-4380-9e30-6f2e2ed1f07e': 'What property of the logarithm of the sigmoid function makes it useful for gradient-based learning compared to other loss functions like mean squared error?',\n",
       " '1b3ace9a-d3f5-4679-a98d-9bdc9a0c8ef5': 'What are the two main Monte Carlo method variants discussed in the passage? How do they differ in terms of which state visits are used to compute the value function estimates?',\n",
       " '6001e096-a06d-41ce-b880-2bca76a9294d': 'The passage states that both first-visit MC and every-visit MC converge to vπ(s) as the number of visits (or first visits) to s increases. Explain why this convergence is guaranteed for first-visit MC based on the fact that each return is an independent, identically distributed estimate of vπ(s) with finite variance.',\n",
       " '39e20763-fc9d-4b4b-abe5-68a30cccc49a': 'What are two limitations of using a context vector C with fixed size in the encoder-decoder architecture for sequence-to-sequence models?',\n",
       " '29ce2a71-eb75-49f6-8612-8d0f0ea98416': 'What mechanism did Bahdanau et al. introduce to address the limitations of using a fixed-size context vector C in the encoder-decoder architecture?',\n",
       " 'd30b7737-cd58-4c7f-aa8f-ee828ec564f9': 'Derive the differential equation satisfied by f(x), given that f(x) represents a probability density function that undergoes the following transformation:',\n",
       " '566457a8-e033-450c-95d6-52924373f3d6': 'x) --> (1/3)f(3x)',\n",
       " 'f43d4b86-d687-4682-95b1-9a804709080c': 'Consider two random variables Y1 and Y2 where -1 ≤ Y1 ≤ 1 and Y2 = Y12. Show that the covariance matrix between Y1 and Y2 is diagonal, even though Y1 and Y2 are not independent random variables.',\n",
       " '44248a04-634c-488d-a2ef-c0f320c1db33': 'What are two key limitations of rejection sampling and importance sampling methods according to the text?',\n",
       " '1d2b2a4b-d2f6-4e81-9615-1fe249c2aa4c': 'Briefly explain the Markov chain Monte Carlo (MCMC) framework and how it addresses some of the limitations of rejection and importance sampling.',\n",
       " 'f3a63ace-9d1d-4f47-8f80-6f5ea7e25b52': 'In Jeopardy, what happens if the first contestant to buzz in responds correctly to a clue?',\n",
       " '80391597-8c5d-4a55-8ce9-61e6b91b11db': 'What is unique about the DD (Daily Double) squares in Jeopardy compared to regular squares?',\n",
       " '2ef50062-5380-4b60-93c3-6886e9f1be11': 'How does the framework proposed in the context decouple the prediction task and encoder architecture?',\n",
       " 'e0d1f764-390b-4bab-88ee-a33fb6dd98d8': 'What are two key differences between the contrastive learning framework discussed in the context and CPC v1 and v2?',\n",
       " '51c79b25-70fc-4e6b-b582-c36ba8f24af1': 'If training is halted after a finite number of steps τ, what do the components of the weight vector parallel to the eigenvectors of the Hessian satisfy?',\n",
       " 'b6c4a091-6a87-432c-a50b-c9cded54bacf': 'How is (ρτ)^-1 analogous to the regularization parameter λ when comparing early stopping to regularization with simple weight decay?',\n",
       " '093cf939-b9fa-4147-ab87-b2a9fb78dddf': 'What does the variable α(zn) represent in the equations for γ(zn)?',\n",
       " '97cf7d67-cb5b-4b12-9d04-b3b6e754049c': 'How does the denominator p(X) relate to the parameters θold of the HMM according to the context?',\n",
       " '7659f530-c44a-4e52-a7f5-e9252d0141de': 'Who first developed the UCB1 algorithm for utilizing upper confidence bounds to select actions in reinforcement learning?',\n",
       " '55923b63-565d-44f2-9522-fa819cf5a1f2': 'Softmax is a common action selection rule in reinforcement learning. Who first proposed using this rule?',\n",
       " '50c61730-47be-4ed8-8afc-47cccfaf47ae': 'According to the passage, what are the default values used for the weights (wmin,  ̄w, wmax) that correspond to assumed labeling function accuracies between 62% and 82%?',\n",
       " 'f031a333-85bf-4581-ada5-958f56c887c9': 'The passage states that at 9 labeling functions, the optimizer switches from choosing majority vote to choosing the generative model. What reason does the passage give to explain why this leads to faster modeling in early development cycles, and more accurate results in later cycles?',\n",
       " 'e1fa57bc-fe26-4c7e-9594-80066df6fe2b': 'The paper by Salimans and Knowles proposes an approach for variational posterior approximation that utilizes what technique?',\n",
       " '87c8b6bb-1c93-4193-9eee-46c47d5afcaf': 'The work by Vincent et al. introduces an unsupervised learning algorithm called stacked denoising autoencoders. What is the key idea behind this method?',\n",
       " '862484cf-0c3c-4fec-99f3-c9e3c13864d3': \"In the Nature paper by Silver et al., what game did the authors' system master through self-play reinforcement learning without human knowledge?\",\n",
       " '7b1b53f5-feac-4b43-97c2-dad1074dd647': 'What general reinforcement learning algorithm did Silver et al. use to master chess and shogi by self-play in their 2017 arXiv paper?',\n",
       " '4bd7b8ea-9bb3-411e-a0d5-1115107ab8b1': 'What norm calculates the absolute value of the element with the largest magnitude in a vector?',\n",
       " '17d09e62-c6bb-40ce-b467-33acd9dd465a': 'What is the formula for calculating the Frobenius norm of a matrix?',\n",
       " '14f8532b-6d90-4f41-81c0-5d9d4f0921a8': 'According to the passage, why do Fourier features have trouble with discontinuities?',\n",
       " '77cd097c-b9d5-4c26-b3f0-af8056544f82': 'The passage states that if the dimension of the state space is small enough (e.g. k ≤ 5), then what can be done with regards to selecting all of the order-n Fourier features?',\n",
       " '523a5cb3-7489-4a5a-8d56-f43f26ef9885': 'What is the issue with using exact importance sampling for this application of estimating the negative phase contribution?',\n",
       " '8ceeaca9-5ad9-40e9-9ff5-9a4ea11cad59': 'How are the importance weights for the negative samples modified in the method of biased importance sampling described in the passage?',\n",
       " '2479615a-f0a3-4817-a68c-ced90dda63a3': 'When developing new generative models like Boltzmann machines, what is one key challenge that requires more creativity compared to developing a new neural network layer?',\n",
       " 'e6954ee7-9e70-4768-9abb-748f00791df5': 'The reading describes a straightforward way to extend neural networks to implement stochastic transformations by augmenting them with what?',\n",
       " 'cc60decd-7b3c-4266-b27d-0b875474c7a9': 'What were some of the state features used by Ipek et al. in their MDP formulation of the DRAM scheduling problem?',\n",
       " 'dc178c84-4f85-40e0-abe7-a0c6dba024e9': 'How did Ipek et al. decide which state features to include in their MDP formulation? What process did they use to pare down their initial list of potential features?',\n",
       " 'cd4e930c-523c-494f-88f7-62659afaadc7': 'Consider the function f(x) = exp(-x). According to the context, what type of function is this with respect to x?',\n",
       " '3ecc34d1-60c5-4254-8d86-5335f6b5bb73': 'The context mentions obtaining a tangent line y(x) to the function f(x) = exp(-x) at a point x = ξ. This tangent line corresponds to making what type of expansion of f(x) so that y(x) ≤ f(x)?',\n",
       " '826a8c5a-9a83-420d-891f-ec6300b3deb5': 'Consider the contour plots of a multidimensional error function E(w). How are the lengths of the contour ellipse axes related to the eigenvalues λi of the Hessian matrix?',\n",
       " '0d04bc77-6f1b-41c6-85f3-142924df89ff': 'Derive the forward propagation equations for computing the Jacobian matrix J of a neural network. Your derivation should not utilize the backpropagation approach presented in Section 5.3.4.',\n",
       " '2b25141e-777c-4d06-8632-e9870cd7acda': 'According to the passage, why is a perfect reconstruction of all input vectors not generally possible when the number of hidden units is smaller than the number of inputs?',\n",
       " '6ec8d322-4ee4-4253-b37b-c68c1e9c91c9': 'The passage states that if the hidden units have linear activation functions, the error function has a unique global minimum. What does the network perform a projection onto at this minimum error?',\n",
       " '1d1d8582-6107-4ac0-81a3-1844ec8576ea': 'The re-estimation equations for the hyperparameters α and β were derived in two ways - through direct maximization of the marginal likelihood in Section 7.2.1, and through the EM algorithm in Section 9.3.4. Show that equations (7.87)-(7.88) and equations (9.67)-(9.68) are formally equivalent.',\n",
       " 'ce9cfba6-2d94-457d-ab3b-8d290b6096ba': 'In Section 9.3.6, incremental updates for the responsibilities and parameters of a Gaussian mixture model are derived. Starting from the batch update equations (9.17) and (9.18), show how the incremental update equations (9.78) and (9.79) can be obtained for updating the component means when only one data point xm is considered.',\n",
       " '71b4812a-fd96-4a8e-8535-aa7512256c1c': 'What is the term for the lower bound on the marginal likelihood of a datapoint i, according to the variational inference equation?',\n",
       " '3344ad88-ff1c-43a6-82ad-25b42f97f314': 'True or False: The KL divergence term in the variational inference equation is always zero. Explain your answer.',\n",
       " 'd2628938-086d-498a-9be8-28d85e46dc8b': 'According to the text, when using an extremely large training set, what becomes the predominant concern over overfitting?',\n",
       " 'f96802e9-6b7d-4428-8768-ff158830abf1': 'The text mentions that traditionally, machine learning has avoided the difficulty of general optimization by doing what?',\n",
       " '7db1d938-6605-4d51-8e83-b0dfb7a094d6': 'What are the two main historical threads that led to the development of modern reinforcement learning?',\n",
       " 'ab7654f3-de01-4953-8602-0b1bedeccba0': 'What is the key difference between reinforcement learning methods and evolutionary methods according to the passage?',\n",
       " '59c431eb-bf8f-4221-867c-615a87ff930e': 'What is one advantage of using hashing with tile coding?',\n",
       " 'b5e8fc24-6f88-40c7-9218-f62d98b7321e': 'Describe one way that tile coding enables flexibility in determining generalization.',\n",
       " '1f08ae72-4907-4405-8ec8-aa42662d9642': 'What happens to the posterior variance σ^2_N as the number of data points N approaches infinity?',\n",
       " '986f948c-1479-4653-b42e-50a062357898': 'How does the posterior mean from Bayesian inference reduce to the maximum likelihood estimate for the mean of a Gaussian distribution in two different limiting cases?',\n",
       " '3b469a01-77cf-401d-8180-c1c9888946d1': 'According to the text, what is required for the behavior policy in off-policy Monte Carlo control?',\n",
       " '51604bd1-78de-47ee-baef-50fbbd50fa3d': 'The text mentions a potential problem with the off-policy Monte Carlo control method discussed. What is this potential problem?',\n",
       " 'f8fdf407-88b1-400e-82af-c6b382ab102b': 'What are some of the key factors that affect the quality of a histogram density estimate?',\n",
       " '8561e39c-4249-4b5c-98c0-8203b43a8a79': 'How does the histogram density estimation method compare to some of the other density estimation methods that will be discussed later? What are some of its advantages and disadvantages?',\n",
       " '29d9f411-13d3-41c4-9f0d-f2457789dd0f': 'What is the effect of the eigenvalue λ being small compared to α on the MAP value of the parameter vector w?',\n",
       " 'fc7e31f7-fbd0-47b7-8baa-763b59b4aa53': 'How do the contours of the likelihood function and prior differ when plotted in the eigenbasis compared to the original parameter basis? What insight does this provide into the Bayesian solution?',\n",
       " '44b1ee05-2fe1-4d8e-8a48-8a5435053442': 'What are two kinds of catastrophic outcomes that can occur when the maximum likelihood estimate Pn is zero for an n-gram model?',\n",
       " '316f5243-5f9e-4453-bd84-893ca426590a': 'What is one basic smoothing technique that can be used with n-gram models to avoid catastrophic outcomes by shifting probability mass from observed to unobserved tuples?',\n",
       " '4e58b01a-ddba-4bfc-b816-b93dd33d6148': 'How did the width of the receptive fields affect generalization early in learning according to the passage?',\n",
       " 'efdc8571-ee69-408e-a227-017812bd232a': 'What three different sizes of intervals were used for the receptive fields in the experiment described in the passage?',\n",
       " '92cf6276-7620-44a9-bc26-95e7af9d1291': 'When training a deep Boltzmann machine using the joint PCD algorithm, what modification is made to the negative phase in order to obtain state-of-the-art results?',\n",
       " '10fcb169-f493-471a-b9ad-ce92ca14df4d': 'What does it mean when it states the top and bottom RBMs should be trained with two copies of the units? How does this impact the weights?',\n",
       " '8dcd28f0-33e5-4c86-ae81-b55ce8cc3491': 'What is value iteration and how does it differ from regular policy iteration?',\n",
       " '5a75ac71-b778-4a1f-8de4-6682e2c1b62e': 'The context describes two ways to truncate the policy evaluation step in policy iteration. What are these two truncation methods?',\n",
       " '3a3cb7aa-e2f9-446d-adb6-b8082c2edff3': 'According to the passage, what did Lucic et al. conclude about newly developed GAN loss functions after conducting a series of tests?',\n",
       " 'c6393871-b59b-456f-b0ae-b5e6748976bf': 'The passage states that most research applying GANs for data augmentation and reporting classification performance has been done in what field?',\n",
       " '79c8a17d-8168-47c1-97e2-b9d7664bb471': 'Which paper discusses using unrolled generative adversarial networks?',\n",
       " '209ecfa6-7cf2-493b-964a-da9e48ee4c0e': 'Which paper proposes asynchronous methods for deep reinforcement learning?',\n",
       " 'eda2eb18-0d18-4ae1-80ac-3390258a0add': 'What function does the softplus activation demonstrate according to the passage?',\n",
       " '20bea141-4b2c-4396-9f7c-65f29287b7f0': 'According to the passage, what are two key design considerations when developing the architecture for neural networks?',\n",
       " 'eb368454-fb5f-4870-9529-cb38bce6acd1': 'The original GMMN paper used a minibatch size of 1000, much larger than the common sizes of 32 or 64. What are two reasons stated in the passage for using this large minibatch size, despite its quadratic computational cost?',\n",
       " '4b2ed0af-16e5-4a28-bf9e-77ee73c2a8ca': 'The passage mentions recent work exploring the use of Wasserstein distances for Restricted Boltzmann Machines in discrete spaces. What are two motivations it gives for exploring Wasserstein distances, even though the manifold setting is restricted to continuous spaces?',\n",
       " 'b94a5ea5-e008-4c54-b692-08599cb380ca': 'If p(X,Y) = p(X)p(Y), then X and Y are said to be _______.',\n",
       " 'b647e19f-af46-4694-9ed1-814cd9d09569': 'Consider a case where each of 2 boxes (red and blue) contains the same fraction of apples and oranges. In this case, is p(Fruit|Box) independent or dependent on which box is chosen? Explain.',\n",
       " '556865af-8af7-4f8e-998d-3fbc836b598f': 'The passage mentions that \"The number of tilings, along with the size of the tiles, determines the resolution or fineness of the asymptotic approximation.\" What does it mean for the tilings to provide an \"asymptotic approximation\"?',\n",
       " 'da1174c7-c392-4f77-97d6-8ee4ac9c0ea4': 'The passage states that \"Tiles that are elongated along one dimension, such as the stripe tilings in Figure 9.12 (middle), will promote generalization along that dimension.\" How would using square tiles instead of elongated stripe tiles change the nature of generalization in the tilings?',\n",
       " 'ef08f8b5-f8c7-4b84-a2f1-55ab82e4ecbf': 'What is an example of an experience that does not have an analytic form, but is instead defined variationally?',\n",
       " '475b6b32-81ff-409e-87fc-4e7cea648ea5': 'The context describes using a discriminator or critic to measure the closeness of a configuration to a dataset. What machine learning technique does this describe?',\n",
       " 'f27090bb-fcc2-472b-a4bb-b82cd7a2424d': 'What strategy does the chapter mention is difficult to provide specific advice for when designing a generic neural network architecture?',\n",
       " '573f8d3a-d86d-42c6-bf09-6f222e2ccb1e': 'What is the term used to describe how information flows through a feedforward neural network, from the input to the hidden units and finally to the output?',\n",
       " 'b6bf31c0-673a-45a0-963d-86cc25cba6f0': 'What are two approaches discussed in the passage for performing inference on models with non-Gaussian transition or emission distributions?',\n",
       " 'f41f042a-473d-4e81-a1c7-a61b6c4d1897': 'The passage mentions that the switching state space model combines aspects of two models discussed earlier in the text. Which two models are those?',\n",
       " '4cb21b9d-0791-410d-a102-fc081353bda2': 'What are some key differences between deep graphical models and traditional graphical models in terms of interpretability, theoretical guarantees, scalability, and connectivity?',\n",
       " '5f48f6fa-d1aa-4922-973e-6af305f45dba': 'The context mentions that in traditional graphical models, the design of the model structure is tightly linked with the choice of which inference algorithm? Why is this the case?',\n",
       " '13edb9bf-9a51-44fd-9ef2-59d45ee98a4c': 'What is the key difference between the Laplace and EP approximations shown in the left-hand plot?',\n",
       " '3350cdd7-e214-481b-b77e-fd9090d2a628': 'According to the context, how are the parameters of the revised distribution q_new(θ) determined when using EP?',\n",
       " '21f09199-8f55-40ec-8763-d360c375f6bb': \"Using the mean value theorem, prove that a twice differentiable function f is convex on an interval if f''(x) ≥ 0 on that interval.\",\n",
       " '9a12c67b-befe-400c-886f-04e264a5cd89': 'Let f be a twice differentiable function. Given points x < y and 0 < λ < 1, with z = λy + (1-λ)x, show that f is convex on the interval [x, y] if and only if f(z) ≤ λf(y) + (1-λ)f(x).',\n",
       " 'b59df8d3-5cee-46af-af7c-89a08f667535': 'Consider a joint distribution p(x,y) over discrete variables x and y that both take on values in {0,1,2}. Construct a specific joint distribution for x and y such that the values x^ and y^ that maximize the marginals p(x) and p(y) respectively have p(x^,y^) = 0.',\n",
       " '2aea2c47-47d5-412e-9116-cbfa309727b6': 'Explain why, for a factor graph containing cycles, there will always be at least one pending message irrespective of how long the sum-product algorithm runs.',\n",
       " 'be45cdeb-6a3f-41b7-8334-5ce14ec04ad8': 'What are the two components that control the bias-variance tradeoff in the polynomial regression example discussed?',\n",
       " '51747db9-9b28-4d26-b699-2f1e474a5988': 'How does weight decay help prevent overfitting when using a high-degree polynomial model, as illustrated in the example with a degree 9 polynomial?',\n",
       " '88f0d0e2-687b-40d7-ad3a-e366a586b1df': 'What cost function term can be added to introduce the slowness principle when training a differentiable model with gradient descent?',\n",
       " '24d0c052-f4b7-402e-bccc-e9c968b3cec6': 'What is one particularly efficient application of the slowness principle mentioned in the passage?',\n",
       " '4e67e515-e101-4557-964a-32eb1a72ea94': 'According to the passage, what percentage of the hypotheses have an entailment probability of less than 20%?',\n",
       " '3c9325e0-c698-4c42-9320-a4bcf8ec5086': 'What three types of rewards are used in the system described in the passage?',\n",
       " '2d6a4774-d997-4b4c-aa13-d4f8013329a0': 'According to the passage, who does the author thank for contributing specifically to the second edition of the book?',\n",
       " '23079a82-cbf0-45a9-ba1c-c6509e23f76b': 'The passage mentions that the authors received help on the psychology and neuroscience chapters of the book. Which experts in those fields are specifically named as having provided guidance?',\n",
       " '8b006a25-8f6a-4246-8aca-215daff738b5': 'According to the passage, what is one way that reinforcement learning theory is being applied to understand a major health problem?',\n",
       " '48f1a13f-fa28-4b3e-8d24-f7046d2b9145': \"What does the passage say is progressing rapidly and revealing correspondences between the brain's reward system and reinforcement learning theory?\",\n",
       " '2ad8ffc5-18fb-4613-92b1-29f01fe6dec7': 'What is the main advantage of n-step TD methods over one-step TD methods?',\n",
       " '44c71381-eb58-45b2-ad35-38feff1c57d2': 'True/False: n-step TD methods perform a complete backup like Monte Carlo methods, looking ahead to the end of the episode.',\n",
       " '57b11cb6-cd68-4ab8-8650-566ffe564060': 'What makes learning undirected models by maximum likelihood particularly difficult?',\n",
       " '834b6d3a-6eff-401f-970a-4680faf7b7d4': 'What is the well-known decomposition of the gradient of the log-likelihood with respect to the parameters called?',\n",
       " '8324f9a8-eb72-4cc3-9a51-f322220faa57': 'What dataset did Ionue use when testing the SamplePairing data augmentation technique? How much did it reduce the error rate on this dataset?',\n",
       " '5b13e417-917a-4706-afe0-d164b77710d4': 'When Ionue tested SamplePairing on a reduced version of the CIFAR-10 dataset with only 1000 total images, how many images were there per class? How much did SamplePairing reduce the error rate on this reduced dataset?',\n",
       " '966444e8-cfef-45c4-b28c-f956d1ddd5f6': 'How does the data augmentation review provided in this work differ from that of Hedderich et al.?',\n",
       " 'a5d34833-0ea2-40c9-a9fa-8f883ffc687d': 'According to the passage, what are two ways that data augmentation helps deal with limited labeled data for machine learning models?',\n",
       " 'c6f3ca12-bd6f-401b-81e7-4ae84b1bdd0c': 'What is the purpose of the penalty term U(ξ) in the optimization objective?',\n",
       " 'a222acf6-062d-40a9-86f1-1a8e961ba7ee': 'How does the divergence function D(q, pθ) contribute to the overall learning process? Briefly explain.',\n",
       " 'b51aa348-15c5-46bd-b6cb-2151399755a7': 'If we want to calculate the marginal distribution p(x), we first write the joint distribution p(X) using which of the following expressions:',\n",
       " '038ac8f8-65fd-4155-9b16-55eabad9ae6d': 'p(X) = ∏s∈ne(x) Fs(x,Xs)',\n",
       " '4e6ac570-9237-47ed-991e-5b0f70fc9723': 'p(X) = ∏i Fi(Xi)',\n",
       " '5f47c119-f1ae-4d65-8487-3c33d04bb11c': 'p(X) = ∏i∈Vp(xi)',\n",
       " '14793840-67b2-4fde-8cbb-014f58fccd59': 'p(X) = ∏(i,j)∈Ep(xi,xj)',\n",
       " '01851a14-416c-45c9-8b17-22e8bbf093e1': 'After writing the joint distribution p(X) using the appropriate factorization, how do we calculate the marginal p(x)?',\n",
       " 'fc160045-56f4-4345-86bf-20c3d1cb87ed': 'By summing p(X) over all variables except x',\n",
       " '79078119-69b1-4ee9-a55e-6ded42453b90': 'By taking the derivative of p(X) with respect to x',\n",
       " '256e1923-595f-4b35-87e1-7a9ddf2aef5d': 'By calculating the normalization constant Z',\n",
       " 'f89c1127-f159-4386-b0a5-813154b6c6e3': 'By maximizing p(X) with respect to x',\n",
       " 'd8117d43-5bb5-4203-9048-8f0d2e0355f3': 'According to Kober and Peters (year), what are two main approaches for solving robotic reinforcement learning problems?',\n",
       " '8731baa7-6b1d-48f6-996f-59494550e70b': \"According to the Empowerment paper (year), what key concept do the authors propose for quantifying an agent's control over its environment?\",\n",
       " '3c293b7c-ed2d-4d09-80de-872c8ee31810': 'What property of the graph structure allows the autoregressive HMM to be solved efficiently using the forward-backward algorithm?',\n",
       " '445610d5-fcde-44f0-a56b-cdfc58e1625d': 'The text mentions two examples of graphical models that extend the standard HMM. Name these two models.',\n",
       " 'eccb3f36-06c2-4bbb-aef4-01c4e1ca0ed1': 'The variational approximation to the predictive distribution takes the same form as which other distribution, with the noise variance set to zero?',\n",
       " '7e2672b1-623a-4eea-b4b3-3be9bcb2c32d': 'What approximation can be made between the logistic sigmoid function σ(a) and the probit function Φ(a), in order to evaluate the integral over a that represents the convolution of a Gaussian with a logistic sigmoid?',\n",
       " 'c4b684cb-440b-4221-bd56-0c42fb2a913c': 'What is the name given to a critical point where the Hessian matrix has both positive and negative eigenvalues?',\n",
       " '7d9286ba-64ba-40aa-934a-c9e6e53ade74': 'Why does the name \"saddle point\" make sense for a critical point with both positive and negative Hessian eigenvalues?',\n",
       " '7a09bbd5-cc7e-4262-8624-816fb802ff7b': 'According to the passage, who first suggested an approach to approximating dynamic programming called \"heuristic dynamic programming\" that emphasizes gradient-descent methods for continuous-state problems?',\n",
       " 'e6b2589e-75ee-48ad-b945-3603012455ee': 'The passage mentions that Andreae discussed dynamic programming in the context of which reinforcement learning algorithm, although he did not make specific connections between the two?',\n",
       " 'c7e8307c-2d53-4e93-9cfd-fed34c6283da': 'What are two limitations of gradient-based optimization that meta-learning algorithms aim to address?',\n",
       " '78449fa1-834c-47f3-a9b0-f342e5dc0a0c': 'What are the two components of the LSTM Meta-Learner model proposed by Ravi & Larochelle? Briefly explain the role of each component.',\n",
       " '9d8554cb-2328-4626-9831-594d7f2ded19': 'According to the passage, in problems of incomplete knowledge, what is not available to the agent?',\n",
       " '38c486e5-fe6a-409f-828d-3406c07b71d4': 'The passage states that even with a complete environment model, agents are typically unable to do what per time step?',\n",
       " '881abda2-bf08-440c-afe4-37eae208ea26': 'When implementing neural networks, it is important to pay close attention to numerical stability. What is one common cause of numerical instability in neural network implementations?',\n",
       " '33439db8-806c-4bc4-bd2c-67c5f6336d82': 'The context discusses various methods for initializing the weights in a neural network, like random initialization and Xavier initialization. What is the main motivation behind using Xavier initialization compared to simple random initialization?',\n",
       " '45f60ee8-e86c-4db0-93ef-86def8a49f11': 'According to the passage, how do kernels arise naturally in a Bayesian setting?',\n",
       " 'a42df197-894c-407e-9069-243de1759782': 'The passage mentions that in Chapter 3, linear regression models of the form y(x,w) = w^Tφ(x) were considered. What is φ(x) in this context?',\n",
       " '163e2224-2cbe-4c6b-91e2-cb02097de4ff': 'According to the passage, how does a closed eye during a classical conditioning experiment influence the reinforcing value of an air puff to the eye?',\n",
       " 'f1d4f2a7-7abc-44fa-bdf9-0fbf629e3e78': 'What is the key distinction made in the passage between a reward signal and a reinforcement signal?',\n",
       " '637005a6-a905-4b6f-9973-6bdceb428457': 'When adapting Gaussian processes to classification problems, what function is used to transform the output of the Gaussian process?',\n",
       " '7432c1d2-231f-4887-9110-4f0525509057': 'In the example of a Gaussian process over a function a(x) that is transformed using a logistic sigmoid y = σ(a), what is the resulting distribution over the target variable t?',\n",
       " '8813fb06-06f1-4bd9-842c-6f70129f53ff': 'According to the passage, how do eligibility traces arise in Monte Carlo learning?',\n",
       " '5fdbf8c3-184c-45b0-8016-78e8fc8fd70a': 'The passage mentions that the linear MC algorithm can be used to derive an equivalent yet computationally cheaper backward-view algorithm using Dutch traces. What is the only equivalence of forward- and backward-views that is explicitly demonstrated in this book?',\n",
       " '86754692-7931-4dbe-98d1-bca7bba8d691': 'What algorithm does AutoAugment use to search for an optimal image augmentation policy?',\n",
       " '2c5d137f-ff5c-442c-8792-a1acd617bf0f': 'How does AutoAugment differ from Neural Augmentation and Smart Augmentation in its approach to image augmentation?',\n",
       " '07cc74ad-7720-41e6-90fd-d888cb263671': 'In the computational graph shown in part (c), what is the significance of the variable H?',\n",
       " 'ae2b56b6-f04b-444c-bb43-26bd61d7b7d2': 'Part (d) shows a computation graph that applies more than one operation to the same variable w. Describe what this demonstrates about computation graphs.',\n",
       " '92de8848-4d2c-44ea-bcf5-79eab788ad69': 'What is the key property that a probability density function p(x) must satisfy?',\n",
       " '5b383bf9-caad-486b-963f-d9244303076a': 'If X is a continuous random variable with probability density function p(x), how do you calculate the probability that X lies in the interval [a,b]?',\n",
       " '124b3c39-684b-4a8e-8bb0-9197bfbd3ac5': 'What allows machine learning methods to work effectively in high dimensional feature spaces without actually operating directly in that space?',\n",
       " 'ea5d6df7-3521-4c0e-9c71-5d953a1cdadd': 'The \"kernel trick\" takes advantage of what property to enable working with high dimensional feature spaces indirectly?',\n",
       " 'b51b76f6-cee3-4b71-9a1e-753839407b65': 'When performing a transcription task, what are two ways we can measure the performance of a system? Should we focus more on accuracy of entire sequences or give partial credit for getting some elements correct? What factors go into this design choice?',\n",
       " 'd9b0f6c7-3b00-4c78-89d6-487c689044cb': 'The passage states that in some density estimation models, computing the actual probability value assigned to a point is intractable. What does \"intractable\" mean in this context? Can you give an example of a model where this might occur? How can we still evaluate these models if we can\\'t directly compute probabilities?',\n",
       " '2c246bd0-fecb-4a4b-ad59-4d6aa9791df0': 'According to the text, what type of structure was generally believed to be present in neural net cost functions prior to 2012?',\n",
       " 'ed950231-1853-4707-9e6b-1be883306aaf': 'What do the visualizations of neural network cost functions show, according to the text?',\n",
       " '9b0d2b35-cd34-473d-810a-36e5197e5e4b': 'According to the passage, why can an autoencoder with too much capacity fail to learn useful information about the distribution of the data?',\n",
       " 'fdefd548-a3b8-4adb-84ee-e23b823c9e68': \"The passage mentions that in theory, an autoencoder with a 1-dimensional code but a very powerful nonlinear encoder could learn to represent each training example 'a' with the code 'i'. What does this hypothetical scenario illustrate about autoencoders trained to perform the copying task?\",\n",
       " '08806e37-a437-45f6-9daf-d5531334b0c9': 'According to the passage, what are two limitations of developing highly specific rules compared to more general rules?',\n",
       " '9dc05bc4-81d7-40fa-9daa-f4b9f81b16d8': 'The passage states that probability theory was originally developed to analyze the frequencies of events. Can you describe one example the passage gives of an event where probability theory is easily applied?',\n",
       " '87a38c0c-5c8c-4113-a59c-8bd904a5f0f2': 'According to the context, how does testing for conditional independence in undirected graphs differ from directed graphs?',\n",
       " '943db84b-cedb-4e2c-8db2-22e4c081c7cf': 'The context mentions an \"alternative way to view the conditional independence test\". What does it say is an alternative perspective to imagining removing all nodes in set C from the graph?',\n",
       " 'cc4e6cc8-f0e7-4052-93ca-8530236718fd': 'What book discusses representing knowledge as forecasts and state as knowledge?',\n",
       " '0b34bf69-4264-4850-8626-2b51b89a4919': 'Which author discusses optimizing memory controllers for web servers?',\n",
       " 'ebcf4d66-2fae-497b-a5c3-6c32ffb45106': 'Consider a mixture of K Bernoulli distributions with means μk and mixing proportions πk. What are the mean and covariance of this mixture distribution expressed in terms of μk, πk, and Σk?',\n",
       " '7c727d89-11af-452e-aa48-ddce92c120ed': 'In the EM algorithm for maximizing the likelihood of a mixture of Bernoulli distributions, latent variables zn are introduced for each data point xn. What does each znk represent for the nth data point and kth mixture component?',\n",
       " '5f3dab9b-62e6-48f9-9955-6f6fe7d4d4bf': 'What is done to compute the outgoing message from a factor node in the sum-product algorithm?',\n",
       " '378eff18-7d43-4927-bf72-98ec5fe51491': 'The text mentions two ways the expectation propagation algorithm can be generalized to potentially improve accuracy over the sum-product algorithm. What are these two generalizations?',\n",
       " '30eae274-e06d-4b16-931d-a1bb462c48d8': 'According to the passage, why do we typically choose to regularize the weights but not the biases in a neural network?',\n",
       " '57dad14a-5f4f-44f4-95b0-aca7683e6d09': 'The passage states that each weight in a neural network specifies the interaction between which two variables?',\n",
       " '33db873a-f3c2-4e27-a0bf-acc42f16722c': 'What algorithm can be used to perform exact inference efficiently in linear time for a chain of nodes?',\n",
       " '681d1b7b-3a09-4d2d-9aa5-2a35751909d3': 'How can we obtain the joint distributions over all the sets of variables in each of the potentials according to the passage?',\n",
       " '21db7302-a1c2-4ceb-ad41-63858a27a843': 'According to the passage, what are two advantages of parameterizing policies according to the soft-max over action preferences compared to using a soft-max distribution over action values?',\n",
       " '07509fe2-c2c0-44fb-98f7-db83c49c3cc0': 'The passage states that using a soft-max distribution over action values alone would not allow the policy to approach a deterministic policy. Why is this the case?',\n",
       " 'a746ad38-1e1d-4d86-98d7-f81dfa46d58b': 'According to the context, if the function fφ is parameterized as a neural network with a fixed architecture (e.g. ConvNet), is its space F necessarily convex? Explain your answer.',\n",
       " '29a3ef90-ab0c-4320-990b-0985c9b0ff3e': 'The context discusses using the KL divergence D(q, pθ) = KL(q||pθ) as the divergence D in GANs. What is the motivation provided for using the KL divergence? What is the closed-form solution for the auxiliary distribution q when using the KL divergence?',\n",
       " '3fb7d2c3-3c93-4ee8-9ed1-d49aed13291c': 'The context mentions that suitable analytically specified importance sampling distributions cannot readily be found for complex models. Why is this the case?',\n",
       " '383a55d6-2d06-4cdf-937e-a6a22f3e0d3a': 'The context discusses a technique called \"chaining\" to help estimate the ratio of partition functions for complex distributions. Can you briefly summarize how the chaining technique works?',\n",
       " '4e381a2e-eda2-46de-9bdd-3ac68797a191': 'What does SGVB stand for and what is its purpose according to the passage?',\n",
       " '8e79d1d9-eb8d-4639-9585-12ea504dbc9a': 'According to the passage, what are two benefits of using the AEVB algorithm for inference and learning?',\n",
       " 'de922e29-37f8-4f42-b512-754e8db1279a': 'Fill in the blank: Among \"-soft policies, \"-greedy policies are in some sense those that explore the ______ as much as possible.',\n",
       " '49ab83b6-19c2-44cd-8121-5538acf04793': 'True or False: On-policy Monte Carlo control uses first-visit MC methods to estimate the action-value function for the current policy.',\n",
       " '5afd82b8-5208-4ecb-b2b4-9ea575bf0f8d': 'What are the 3 technical contributions outlined in the paper?',\n",
       " 'a6f4eacc-46bc-4fee-a0e5-5cc863d401cd': 'According to the passage, how do labeling functions (LFs) express weak supervision sources?',\n",
       " '5d10253b-650d-43af-b3bd-035adc0fe11f': 'The Pascal VOC challenge workshop in 2008 focused on which task in computer vision?',\n",
       " 'b94ac882-3ef8-4030-a756-5a2157d7f9c8': 'PatchShuffle regularization is a technique used to improve what type of neural network models?',\n",
       " '429d4f79-5248-4d2b-98b8-ab9ae39c1e10': 'According to the passage, why did superfluous latent variables not result in overfitting for the autoencoder models tested?',\n",
       " '190d1ea7-c9ab-4c4a-874f-59485c03bdb0': 'What three algorithms were compared in terms of convergence speed using the MNIST dataset, and how many latent variables and hidden units were used for these tests?',\n",
       " 'f355bc2e-c4d3-4d1f-a71f-8782c45be2e7': 'According to the text, what is the standard deviation represented by?',\n",
       " '3b1d0405-52eb-4228-91db-21d36a4b104d': 'The text states that Laplace published Th ́eorie Analytique des Probabilit ́es in 1812. What quote about probability theory is attributed to Laplace from this published work?',\n",
       " 'e5b7a0c8-39c1-4d38-a1b4-28073ece7987': 'According to the passage, why does EP often out-perform variational Bayes and the Laplace approximation in logistic-type models?',\n",
       " '54d7ac69-edb1-4dda-b1ca-ae8f65f4f64d': 'The passage states that EP tries to capture all the modes of the posterior distribution when applied to mixture models. Why does this lead to poor approximations?',\n",
       " 'dd2c1b7a-1a6b-4b9a-95c5-1a92b2d5e459': 'What is a probability mass function (PMF)?',\n",
       " '511b8182-2652-4341-b208-33f15c6cae3f': 'True or False: A continuous random variable can be described using a probability mass function (PMF).',\n",
       " '85310baf-b31a-452c-95bd-298f3289a1e9': 'What are two approaches mentioned in the passage for restricting the family of q in variational inference?',\n",
       " '6ced7a62-b1a4-4c21-ada9-b1962d5cc4ed': 'What is one key difference between the mean-field approximation and the variational approach used in approximate Gaussian processes, according to the passage?',\n",
       " 'a837e41b-8874-4dc1-bce7-71933f771772': 'What is the shape of the resulting matrix C if matrix A has shape m x n and matrix B has shape n x p?',\n",
       " '75b5457b-46b4-4776-8b84-420ef59f03c8': 'What is broadcasting in the context of adding a matrix and a vector?',\n",
       " 'b450c69d-05b2-4aff-ba38-380d7d6941f8': 'According to the passage, what is the classical approach to unsupervised learning and what problem does it run into?',\n",
       " 'fdfd56a8-3449-4ad7-92b3-9e26bddba4fe': \"The passage states that if the model manifold and true distribution's support have a negligible intersection, the KL divergence is not defined or infinite. Why does this pose an issue for density estimation approaches to unsupervised learning?\",\n",
       " '95599825-b27c-45f1-81b9-9f20be672cb8': 'What is the relationship between Bellman errors (BE) and temporal difference errors (TDE) in deterministic problems?',\n",
       " '9eb52254-7c08-4153-a255-2bca89521fb1': 'According to the passage, what are two issues with using Bellman error as an objective for learning value functions with function approximation?',\n",
       " '86aa71c9-3f6a-4dc3-8016-979888b7c0f3': 'What two settings were used to evaluate the performance of the self-supervised representation for transfer learning?',\n",
       " 'd627b2a0-9695-467f-a48c-5e4ede4eeba5': 'What approach did the authors follow for both the linear evaluation and fine-tuning experiments, according to the passage?',\n",
       " '2e5d2ea8-4e35-4474-909a-571ab9c62bfa': 'What are the two parameters that define the probability distribution of y in the given Gaussian process model?',\n",
       " '79683a76-e16d-440f-bfa6-08cc692a7cbb': 'True or False: A Gaussian process defines a probability distribution over functions y(x) such that the set of values y(x1),...,y(xN) evaluated at N points have a joint Gaussian distribution.',\n",
       " '4976497f-d4db-4db0-b00b-635e18a570f8': 'According to the passage, how does the standard equation allow for rediscovering classical algorithms for learning with symbolic knowledge?',\n",
       " 'ea73a5b1-db8b-4367-a100-c04196a22254': 'The passage mentions that by setting α and β to 1 and f to a constraint function like frule, the structural equation with cross entropy leads to what framework that extends conventional Bayesian inference?',\n",
       " '7c9efa33-af9c-47f5-8718-4f0581b7a67a': 'Explain the two kinds of parts and associated parameters in the multitask learning model illustrated in Figure 7.2. How do these parameters benefit from the training data differently?',\n",
       " '4c399f01-8cad-4dda-9acd-3b9b506ee3c0': 'In the multitask learning setup shown in Figure 7.2, the model shares an intermediate-level representation p(shared) that captures common factors across the tasks. Why might sharing this intermediate representation be beneficial for the model? Describe one advantage it provides.',\n",
       " '0e01add1-09ae-491e-b234-f4d3cb9cc794': 'What is weight decay in linear regression and how does it work?',\n",
       " '40c59cd0-546f-41f5-af45-05e8380317fd': 'How can we modify the training criterion for linear regression to express a preference for smaller weights? Briefly explain the equation used for this purpose.',\n",
       " 'a55524b0-7b4b-4293-a3db-7e614d36f292': 'What is the goal when picking out the terms involving xb from the quadratic form for the joint distribution?',\n",
       " '92240ef1-110a-4f33-82ab-604f4a2e7e43': 'After completing the square in the terms involving xb, what form does the dependence on xb take, and how does this allow the integration over xb to be performed?',\n",
       " 'aaac58db-4cec-4ece-8ecb-99cb66e78622': 'In the described MDP, what is the average reward over time starting from state A?',\n",
       " '41181d28-0aed-443c-b7f9-42f40894b6ee': 'Consider states A and B in the described MDP. How do the reward sequences from these two states differ? Specifically, what is the first reward received from each state?',\n",
       " '39760344-89ce-4ce4-8b39-1f7cbcd5ed0c': 'What does the acronym PBE stand for in the context? Explain what a PBE represents.',\n",
       " '1adb9523-5d4a-4723-961f-773b78b67e8c': 'Referring to Figure 1 described in the passage, explain the process that minimizes the PBE. What is the PBE minimized to when the process leaves the value function in the same place after going up with the Bellman operator and back down with the projection operator?',\n",
       " '6533486f-681a-4c08-bc04-827f96f5ae08': 'What are the two outputs of the neural network described in the passage?',\n",
       " '09aed71e-1aef-420d-8863-e4c7102a4ad6': 'According to the passage, how did AlphaGo Zero use the output probabilities p from the neural network to direct each execution of MCTS?',\n",
       " '57f6795c-5eab-4d2e-8f42-d0803b43ce9a': 'What is the main strategy used by denoising autoencoders to prevent autoencoders from learning the identity function?',\n",
       " 'a0eda3b4-bac5-4433-8bd5-ff2fdd50a02a': 'What is the key difference between the regularization strategies used by sparse autoencoders versus contractive autoencoders?',\n",
       " '55ce008f-62d8-4c14-b491-dfb633084dbd': 'According to the text, measurements are taken from a pipeline containing a mixture of 3 materials. What are those 3 materials?',\n",
       " 'd33ce560-1974-473b-907c-87e0bbb68e9e': 'The text mentions that each data point consists of a 12-dimensional input vector. What type of measurement provides the input values for this vector?',\n",
       " 'ec7d8d37-2c42-4ead-8f4c-f51dc189aa20': 'According to the text, what are two limitations of using stochastic gradient descent to store facts in neural networks?',\n",
       " 'e5238ca2-fffe-4fc8-8016-8c16eaa38a4c': 'The text states that neural networks struggle to precisely memorize explicit facts. What system do humans have that enables us to rapidly store and retrieve specific facts to reason sequentially?',\n",
       " 'b03f4066-888e-4c67-8629-e8d3173af647': 'According to the passage, digit images have variability and deformations that arise from what two main factors?',\n",
       " '31380d16-7918-44a7-aa18-f90393ace825': 'The passage states that for the oil flow data set, given a particular geometry, how many degrees of freedom of variability are there and what do they correspond to?',\n",
       " '8aae22fa-2600-413c-bb3c-ae319d3fdaee': 'What is the generative process for the data x in a linear factor model? Explain the different components involved.',\n",
       " 'a3c79710-f2fa-495f-a6a6-c2171b341257': 'What are some of the differences between models like probabilistic PCA, factor analysis and ICA in terms of the assumptions they make about the noise term and prior over h in the linear factor model?',\n",
       " '0b75b066-79bb-4afa-8c7c-2d1d14a910b0': 'What are two desirable properties we want an estimator to have?',\n",
       " '556a25bf-f690-4a4d-bb89-2be19e598c60': 'If an estimator has high variance, how would we expect estimates computed from different samples of the same underlying data distribution to relate to each other?',\n",
       " '1ca41629-5959-4b25-911e-9de85fa1e549': 'According to the passage, why can a poor choice of q make the efficiency of Monte Carlo estimation much worse?',\n",
       " '92d18d87-9fe1-493e-8c39-3458f5f1ac3d': 'The passage states that when q(x) is much greater than p(x), importance sampling collects useless samples. Why does this occur?',\n",
       " '38967a5d-ef08-48cb-be21-365c3be7178c': 'What are the two key components of Monte Carlo Tree Search (MCTS) that allow it to be effective for selecting actions in games like Go?',\n",
       " '270ee4ec-21d9-44b9-98f1-fc087f3ef4b7': 'How does Monte Carlo Tree Search balance between exploiting actions that have worked well in past simulations and exploring new actions that may potentially be better?',\n",
       " '84d06b2b-e361-4acc-a7b5-dc8efea615e7': 'According to the context, if we compare equation 7.40 and 7.42, under what conditions can we see L2 regularization and early stopping as equivalent?',\n",
       " 'cccb6c55-d5ed-40be-b4be-1bdf5afe5e58': 'The context mentions that for neural networks, we cannot initialize all parameters to 0 to obtain symmetry breaking between hidden units. Which section of the book discusses this in more detail?',\n",
       " '088eb495-0551-48e6-837a-693e90e2f4e8': 'What is the goal of Matching Networks for the task of k-shot classification?',\n",
       " '040735d9-8f28-4643-8627-51547a0bdb9f': 'How does Matching Networks compare to using a pretrained model for a new task in terms of transfer learning?',\n",
       " 'f22a79f3-490e-4429-a919-102efa6d9373': 'The passage states that deep learning approaches to graphical modeling are characterized by a tolerance of unknowns. What does this tolerance refer to?',\n",
       " '9fdf7784-28a3-4123-8a6e-076481dce018': 'The passage mentions that deep learning models can be trained even when the objective function is intractable. How is this possible?',\n",
       " 'fd68e71b-d266-45d2-a18b-6d442779c982': 'How does max pooling provide invariance to translation in convolutional neural networks?',\n",
       " 'c52c7883-1371-4996-9cd1-737cbdd1be04': 'True or False: Invariance to translation is useful when we care more about whether some feature is present rather than its exact location. Explain your answer.',\n",
       " '294487fc-8b3e-4eae-8634-0925540f6ef9': 'What happens to the logistic sigmoid function in the case where the training data is linearly separable?',\n",
       " 'f7c71e5a-3e87-4e6b-89f3-167b233cc877': 'The maximum likelihood solution for a linear regression model leads to a closed-form solution. What property of the log likelihood function allows this?',\n",
       " '17283915-4f78-4513-affb-5685e2f2d677': 'What probability distribution do we use before observing any data to represent our initial knowledge and uncertainty about the true parameter θ?',\n",
       " 'abe9e22f-cd48-43e2-a591-92b71ae792f7': \"How can we update our knowledge about the parameter θ after observing some data samples {x1, ..., xn}? Describe the general process using Bayes' rule.\",\n",
       " '633a3ebf-c8ce-4f06-947f-e7c81214d761': 'What technique is used to break down the full quadratic programming problem into a series of smaller ones?',\n",
       " '8be2f673-44b8-4f82-9894-2a9db0d49072': 'According to the context, what is the form of the Gaussian kernels that have been used, and what is the value of γ?',\n",
       " '1ca13f22-43c9-460a-9f34-67ad626ebb80': 'One of the references mentioned is about error-correcting codes based on very sparse matrices. What publication venue was that work published in?',\n",
       " '2661e2e8-7fd0-4cd3-9b5a-50a3373186c9': 'The context mentions a reference by J.R. Magnus and H. Neudecker related to matrix differential calculus. What is the title of the book they authored on this topic?',\n",
       " '248f0a15-5a6a-4039-9a0c-4f2e1e902411': 'What method performs poorly on the task according to the results? Why does this method struggle on the task?',\n",
       " '13c7b491-4a74-4fab-bfdd-907485da8449': 'How does the performance of SQL compare to SQL(single)? What does this highlight about the SQL framework?',\n",
       " '78d9cdd2-10b6-4e7d-9b91-5080b7dc548e': 'What does the figure depicted in the passage show regarding the trajectory of stochastic gradient descent (SGD) training?',\n",
       " '2525d7f6-72dd-4652-824f-fa310664a504': \"How does the proliferation of saddle points in high-dimensional spaces help explain why second-order methods like Newton's method have not replaced gradient descent for training neural networks?\",\n",
       " '88574ee8-ed77-47e7-9921-fe33d6b388cb': 'According to the passage, what is the key difference between a factor graph and an undirected graph?',\n",
       " '599a2741-de55-41d7-9e61-93952196536f': 'The passage states that representation, inference, and learning are asymptotically cheaper in a factor graph with more factors, each over only two variables, compared to a factor graph with one factor over all three variables. Why is this the case?',\n",
       " '36dfed04-a5ff-4747-a3e5-7fe69bef63b3': 'In a left-to-right HMM, what is done to the initial state probabilities p(z1)?',\n",
       " '892836fe-a317-496a-9f82-e0fbd80bc107': 'How can the transition matrix A be constrained in a left-to-right HMM to prevent large changes in the state index?',\n",
       " '061a823d-2561-437e-b889-b3245e8456dc': 'According to the passage, how does a model-free strategy differ from a model-based strategy?',\n",
       " '260abf28-6946-4897-8e5e-d43ea1e9be16': \"The passage describes a model-based agent using a simulated decision tree. What two components make up this agent's model?\",\n",
       " '6006580e-db19-4102-b7e6-e6157d98a7ee': 'According to the passage, what is the definition of an \"active\" constraint?',\n",
       " 'cb2012e0-ea47-425e-a23d-d86d0f7dc562': 'The passage states that inactive constraints may still impact the solution space. Provide an example of how an inactive constraint could exclude better solutions for a nonconvex optimization problem.',\n",
       " '25b1f55d-61a0-4eb9-81e6-e03ae001931a': 'According to the passage, what are value functions estimates of?',\n",
       " '9669ff4e-94cc-4f31-8465-63779c986b1e': 'Fill in the blanks: Formally, a ________ is a mapping from ________ to probabilities of selecting each possible ________.',\n",
       " 'b8c41cc7-5eaa-4607-b548-7f856c0d4460': 'According to the passage, what are some examples of random local modifications that can preserve the meaning of a sentence?',\n",
       " '66f23ab7-f5fd-4fde-8d9c-28386eba3611': 'The passage mentions several studies that have explored different sampling strategies for word replacement when generating text. Which study proposed computing a weighted average over embeddings of possible words predicted by language models as the replaced input?',\n",
       " 'c40dd925-8a65-4ea9-927c-d389aa00540d': 'The POMDP approach to modeling partially observable environments makes assumptions that lead to poor scalability. What are two key assumptions of the POMDP approach that contribute to its poor scalability?',\n",
       " '2e910bc9-d1e5-4466-adf1-6729a5150b4a': 'The context mentions that Predictive State Representations (PSRs) address a weakness of POMDPs related to the semantics of the agent state. What is this weakness of POMDPs that PSRs aim to address?',\n",
       " 'ac359409-deab-40ce-b319-51755a397d16': 'According to the universal approximation theorem discussed in the passage, what is required for a feedforward network to be able to approximate any Borel measurable function from one finite-dimensional space to another?',\n",
       " '1a06182d-35d7-4d28-bd21-d30d04f152fe': 'The passage states that linear models have the advantage of being easy to train due to convex optimization problems when applied to certain loss functions. However, the passage also notes a key limitation of linear models. What is this limitation?',\n",
       " 'ec341c7a-6e1e-445b-a153-1b7299da3286': 'According to the passage, kernel functions can be constructed in two ways. What are these two approaches?',\n",
       " 'a51cdecf-1e4b-4a65-9873-1d6bff64fcca': 'The passage gives an example of a kernel function defined for a one-dimensional input space. What is the formula provided for this example kernel function?',\n",
       " '6efe81b3-7430-47ff-b6eb-6124c93f6f56': 'Who first introduced the use of k-d trees for efficient locally weighted regression?',\n",
       " 'd21a9ca2-1564-4e44-bc73-b68c5d5ad3b5': 'Whose work on the efficiency of local learning algorithms compared to non-local algorithms in pattern recognition tasks is discussed in the passage?',\n",
       " '7ac94775-46ef-4d54-adba-78e191523f68': 'According to the passage, what are some of the benefits of computing the posterior probabilities p(Ck|x), even if we use them to make decisions?',\n",
       " '41717110-76f0-429c-abe9-c01aca7b0c69': 'In the example of Figure 1.27 discussed in the passage, what does the vertical green line correspond to?',\n",
       " 'ba52e6be-3595-43f6-9ca8-ca8395671ec3': \"What two advantages does the authors' proposed SQL method have over previous specialized adversarial text attack methods?\",\n",
       " '33f107d7-010c-4473-9da4-717a7be5c7c5': 'How did the authors evaluate the performance of top-p sampling for generating attack hypotheses? What approximation did they use and why?',\n",
       " 'a3a72b80-2e01-4573-8f0f-301123f055bc': 'When using ancestral sampling to draw from a directed graphical model, in what order are the variables sampled?',\n",
       " 'b1b4b69a-6757-4b46-9ae1-10994230f11c': 'The context mentions that if f(z) is small in regions where p(z) is large, and vice versa, then large sample sizes may be required. What property of the distribution p(z) and function f(z) leads to this requirement?',\n",
       " 'a033c51b-f116-4d68-bb9d-649874467ead': 'What is the key difference between Markov chain Monte Carlo (MCMC) sampling and the rejection sampling and importance sampling methods discussed previously?',\n",
       " '481b6ab6-9144-493e-8852-11ebf3fad476': 'In MCMC, a proposal distribution q(z|z(τ)) is used to generate candidate samples. What properties must this proposal distribution have in order for the algorithm to work effectively?',\n",
       " 'b94edd5b-fc92-4564-ad47-bdeca16b8361': 'What is the notation used to represent the functional derivative of a functional F with respect to a function f(x)?',\n",
       " 'afc07583-5960-4fbc-9a4e-69be39dc3287': 'The principle that requires the functional derivative δF/δf(x) to vanish for a functional F to be stationary is known as what?',\n",
       " '0a12c39f-39d3-41e0-be14-b56507100ffe': 'What is the effect of max pooling on the activation of the pooling units, regardless of which detector unit has the largest activation?',\n",
       " 'bb35801e-7c98-4044-9a00-2d2a9cae3005': 'How does max pooling over spatial positions enable translation invariance in convolutional neural networks?',\n",
       " '608d992f-73ff-4ee8-9dce-e1b69bf00167': 'What method did D. Cubuk and Q.V. Le propose in their 2019 paper to augment speech recognition data?',\n",
       " 'f76bcc79-c7f3-451e-9eab-0495616cd74d': 'According to the 2017 NeurIPS paper by Ratner et al., what approach did they propose for generating domain-specific data augmentations?',\n",
       " '28c8056c-0be3-48fb-bfc1-9ae56c0fadeb': 'What two mechanisms do reinforcement learning algorithms use to enable learning when reinforcing stimuli are delayed?',\n",
       " 'eb20174b-8c3a-40b2-ab5b-b652d458cdee': 'How does the problem of delayed reinforcement relate to Minsky\\'s \"credit assignment problem for learning systems\"?',\n",
       " 'cea92297-877f-4e1a-b91f-8840a7b85efe': 'In the E-step of EM for Bayesian linear regression, what posterior distribution do we compute?',\n",
       " 'a0f74ed6-bfc5-45f9-8d25-f386e1b16a06': 'In the M-step of EM for Bayesian linear regression, what quantity do we maximize with respect to the parameters α and β?',\n",
       " 'b5f7c691-ae26-482a-81f2-4094a1fea216': 'How does the maximum entropy principle help reformulate Bayesian inference as a constraint optimization problem, similar to how it helps reformulate maximum likelihood estimation (MLE)?',\n",
       " 'ea74342d-d4a8-4d74-8b85-e69c5683ed6c': 'What are some of the optimization algorithms that arise as approximations to the intractable maximum likelihood estimation (MLE) problem after applying the maximum entropy principle and minimizing cross entropy?',\n",
       " '77f970a6-83fe-45f9-9b92-74564e7da0ad': 'What is the name given to functions like the polynomial function y(x,w) that are linear in the unknown parameters w?',\n",
       " '12e52a8c-e7db-49de-a875-98316be14ce9': 'What error function is introduced in the passage that measures the misfit between the function y(x,w) and the training data points? What property does it have that makes it a useful choice?',\n",
       " '43014107-371c-4ffc-8802-d8deccc9a194': 'According to the passage, why is it not necessary to explicitly consider all possible paths through the lattice at each time step?',\n",
       " '0374ab82-b1d7-4c00-8b47-e0431d034e0a': 'The passage describes an algorithm that keeps track of the K most probable paths at each time step. What data structure could be used to efficiently store these K paths at each time step?',\n",
       " '84a69c2b-4224-4b96-8ff6-5bfcf434d039': 'What is the purpose of introducing Lagrange multipliers {λj} and {μk} in the optimization problem described in the passage?',\n",
       " '65ebb76d-da1b-4872-bb35-207dc84efe87': 'The passage mentions \"Extensions to constrained functional derivatives are similarly straightforward.\" What does this statement imply about applying Lagrange multipliers to problems with constraints?',\n",
       " '3c5629db-22fe-4944-9440-f8624565ad47': 'What are two drawbacks of ancestral sampling for directed graphical models?',\n",
       " '7fc7f6e4-5247-4a5a-94ce-a2d5acb1ceba': 'Why might we need to use topological sorting when sampling from a graphical model?',\n",
       " '4d88c0f2-a3a6-4ffc-9b2c-a3680c7d3343': 'If M is the number of eigenvalues of a matrix A, how many orthogonal eigenvectors with unit length exist for A? Explain.',\n",
       " 'cb87597a-db36-4a07-93f8-d6cf3f9999b6': 'If U is a matrix whose columns are the orthogonal, unit length eigenvectors of A, what property does U satisfy? Give a brief explanation for why this property holds.',\n",
       " 'a31df963-c915-415e-86a4-e6d7bf342d13': 'Let y_m(x) be the predictions made by the mth committee member. Show that the combined prediction yCOM(x) will be bounded by the minimum and maximum predictions of the committee members if and only if the coefficients αm satisfy:',\n",
       " '3bfe84be-dda1-422e-a59b-99ecb7533571': '∑ αm = 1 and αm ≥ 0 ∀m',\n",
       " '5e012078-7f7d-48be-94d0-f5b0149cb3d9': 'Given the AdaBoost algorithm updates the parameters αm according to:',\n",
       " '4c161dad-337b-4040-827d-06b682edc57a': 'αm ∝ 1/2 ln[(1-εm)/εm]',\n",
       " 'dbf63401-f157-4fdd-a051-5c200597446a': 'εm is the weighted error rate of the mth committee member. Show that this update rule can be derived by differentiating the exponential error function with respect to αm.',\n",
       " '01e6c264-a8db-4eab-b708-c6f116d15277': 'According to the moment matching technique discussed, how can we minimize the Kullback-Leibler divergence between an approximating distribution q(z) and the true posterior distribution p(z)?',\n",
       " '7bf731e0-f2d0-4e71-b0cc-a15d4809e512': 'The text states that for many probabilistic models, the joint distribution of data D and hidden variables θ comprises a product of factors. Give an example of a model where this would occur and explain what each of the factors would represent.',\n",
       " 'ac47e6d6-08d2-4175-885c-2c4727f947c1': 'Why does discounting have no role to play in the definition of the control problem with function approximation, according to the passage?',\n",
       " '3f142316-0bdb-408a-af2c-d1ddd2eb8efd': 'What is the root cause of the difficulties with the discounted control setting, as stated in the passage?',\n",
       " 'bbf6f97e-28c0-4990-9d44-1c3f72098b0b': 'Who was the first to propose and implement a learning method involving temporal-difference ideas as part of a checkers-playing program?',\n",
       " '517b3dea-5268-45f9-a79a-90c32f33b111': 'What are secondary reinforcers according to the context, and who first realized their connection to artificial learning systems?',\n",
       " 'e9231396-2349-4c3a-ad5b-60fc298080a1': 'What technique was used to train the value network on simulated self-play games using the moves selected by the RL policy network?',\n",
       " '75ddc89d-6379-4e8d-baed-e547df949310': 'How many layers did the supervised learning (SL) policy network have that was initialized with weights learned via supervised learning before applying policy gradient reinforcement learning?',\n",
       " '6c5ae94b-fbd2-469c-9ac5-812988725ed7': 'In the TD learning model, how is the eligibility trace vector zt updated compared to the associative strength vector xt(St) in the Rescorla-Wagner model?',\n",
       " 'b7faf029-2848-4fc1-a786-3ddb9aa0062c': 'What is the key difference between the TD error δt and the Rescorla-Wagner error δt? How does the TD error incorporate the discount factor γ?',\n",
       " 'e41d15f4-127c-4c57-bbc0-d78c7189c971': 'According to the passage, how does Expected Sarsa differ from Sarsa in terms of setting the step size parameter α? What are the implications of this difference?',\n",
       " '34ad5738-6c2b-48cb-907b-038a82da2975': 'The passage states that Expected Sarsa subsumes and generalizes Q-learning. Explain this statement. How does Expected Sarsa relate to Q-learning algorithmically?',\n",
       " 'cddd4896-7be0-4d36-b51d-064fa8a7ce1c': 'What is the idea behind the proof of the policy improvement theorem?',\n",
       " '5c7936c8-009b-40a5-bb8d-45cff5fb52d8': \"If qπ(s,a) > vπ(s) for some state s and action a not taken by policy π, what can we conclude about the policy π'?\",\n",
       " '21e30384-4b86-4c93-a2c8-dc96b29bcd2a': 'According to the text, what two types of networks did MacKay study for ensemble learning?',\n",
       " 'db5f67d2-9aa4-4d60-8fbf-a01e777c6a03': 'Name one of the unpublished manuscripts by MacKay that was mentioned in the context information.',\n",
       " '72119277-83d3-4c8d-8892-801c9729d9dc': 'What are two key advances in artificial intelligence over the past 20 years that have driven progress in the field?',\n",
       " '51bc03e2-f5e0-4f94-a27e-e667529c707c': 'According to the text, what were the goals for the first and second editions of the book? How were they similar?',\n",
       " '78ec1d81-1fcc-43a7-b0d2-b0b42b1e69ad': 'What is the key idea behind Gibbs sampling?',\n",
       " '2c92b75b-0215-4ad3-9137-0a1c245ab944': 'What does the term \"z\\\\i\" refer to in the description of the Gibbs sampling procedure?',\n",
       " 'a5f14a21-15c4-4490-b3b5-f011fd081ef5': 'According to the information provided, how does our measure of information content depend on the probability distribution p(x)?',\n",
       " '389e5589-0fab-48ea-9e96-b591a6e00558': 'The context states that if we have two unrelated events x and y, then the information gain from observing both should be the sum of the information gained from each separately. What property of x and y leads to this relationship?',\n",
       " '85c507af-b817-43ff-91fc-b2a5c72c31d5': 'According to the passage, how does reinforcement learning relate to the Law of Effect and Thorndike\\'s idea of \"selecting and connecting\"?',\n",
       " 'b2ef0a16-97bd-4f1e-b59f-742d360500f9': 'The passage states that reinforcement learning algorithms involve both search and memory. What role does search play, and what role does memory play in reinforcement learning algorithms?',\n",
       " '02b98164-e545-473d-8c36-d81101ad79ad': 'Consider the graphical model in Figure 3.8. What is the proper normalized probability distribution over the variables a, b, c, d, and e?',\n",
       " 'df3a5807-20f6-4886-a98f-ad8638b7ecf0': 'What are two key properties that distinguish undirected graphical models from directed graphical models when describing probability distributions?',\n",
       " '2d0ca288-f313-4975-b476-9ae8887bae90': 'What are the two successive functional mappings that can be viewed from the network described?',\n",
       " '901eed0f-e194-47f2-ab27-26efcf27fb64': 'Describe the geometrical interpretation of the second half of the network mapping from the M-dimensional space back into the original D-dimensional input space.',\n",
       " 'e750dd6b-ad34-4cd0-9268-e3bae3eac208': 'Which author proposed an approach for contextual classification of multispectral pixel data?',\n",
       " 'e20420f9-43a2-4244-bf22-1502ed0e7e3a': 'What theorem did Karush prove regarding the minima of functions with inequalities as side constraints?',\n",
       " 'a4405a71-8c02-41f6-b90e-9df21c001304': 'What does the discriminator/critic represent in the context of likelihood-free inference and generative adversarial networks?',\n",
       " 'cf4fcce7-6145-4ba9-b850-47357a870c8e': 'How does the treatment of experience differ between the functional descent view of GANs presented in Section 5.2 versus the extended view presented in this section?',\n",
       " 'b08cb049-6045-494d-931c-21bc314dab96': \"What does Neal's work show regarding the distribution of functions generated by a neural network as M (the number of parameters) approaches infinity?\",\n",
       " 'ca5c3ddd-82ba-42ab-bd57-05653f39bb23': 'One of the merits of neural networks is that the output variables can share hidden units and \"borrow statistical strength\" from each other. What property of neural networks is lost as M approaches infinity, according to the given information?',\n",
       " '92edd6e3-b663-4a5d-b12a-57ceee74f07a': 'What is the key assumption that must hold for off-policy learning methods to work?',\n",
       " '3e72e187-10a5-4ece-b94a-64cf93861389': 'True or False: Off-policy learning methods require that the behavior policy takes the same actions in a state as the target policy with the same probabilities.',\n",
       " 'e5d10d7d-498d-49ed-ab5e-435290394e20': 'What is a symbolic representation an example of?',\n",
       " 'f1a8aa04-d856-4f68-9dd8-521f4050fc70': 'According to the passage, what are two examples of learning algorithms that are based on nondistributed representations?',\n",
       " '8e1b7da1-ea62-45b8-874d-1d4d5dcdf4db': 'What is the main problem that arises when dropping units from networks with narrow layers?',\n",
       " '7d56185b-5655-4712-8cf0-5fc228b0bf8c': 'What process does dropout training aim to approximate?',\n",
       " '19439df2-e578-451a-8d3a-b842778e3745': \"What did Olds and Milner discover in their famous 1954 paper on electrical stimulation of certain areas of a rat's brain?\",\n",
       " '6741f368-d423-4802-82f8-16c32dd54fb3': 'How did later research build upon the findings of Olds and Milner regarding the effects of electrical stimulation on the brain? What specifically did this later research reveal?',\n",
       " '41fd6a7d-8081-433a-a0ca-5d9db391433b': 'What allows the time scale of integration to be changed dynamically in an LSTM, even with fixed parameters?',\n",
       " '059a1a36-b60e-4a23-a092-a501256c16e0': 'What are some applications where LSTMs have been found to be highly successful?',\n",
       " '5e64243f-5dae-4952-b083-30662461b966': 'What technique did Liang et al. use to produce mixed images?',\n",
       " '622bdf41-6002-40e6-b884-19d0a5cfe1da': 'The context mentions the \"unreasonable effectiveness of big data with Deep Learning models\". What evidence does it provide to support this claim?',\n",
       " '3809069e-c6c0-4a57-bb3b-474353403fc2': \"What is the form of the posterior probability or responsibility γ(znk) that is evaluated in the E-step using Bayes' theorem?\",\n",
       " '6cdb105d-57be-4fc2-a2c8-c180e16b6a85': 'In the M-step, what two parameters are updated by maximizing the expected complete-data log likelihood?',\n",
       " '60628ec3-4bf0-4683-8b6b-d495d2213f1d': \"What are the two main steps in CLIP's pre-training approach?\",\n",
       " '0baac408-207f-46c2-8c0a-37687564623b': 'How does CLIP make zero-shot predictions on new images after pre-training?',\n",
       " '85442a81-c870-4888-91c0-c29d6cb576ac': 'What deep learning model was used for image classification of melanoma, nevus and seborrheic keratosis in the 2017 ISIC challenge?',\n",
       " 'a89f0ec3-5d20-44d7-8039-5cc003f3ffd9': 'What paper proposed FaceNet, a deep learning model for face recognition and clustering?',\n",
       " '9131452f-bc9c-4f4a-942c-55db35c5114b': 'The passage mentions that reinforcement learning can incorporate prior information in several ways. According to the passage, what are some examples of how prior information can be incorporated into reinforcement learning?',\n",
       " '0c7f3ab3-9d68-46ea-a7ed-a8abcfa2433e': 'The passage contrasts the tic-tac-toe reinforcement learning example with situations where part of the state may be hidden or different states appear the same. How might reinforcement learning be applied in cases where part of the state is hidden from the learning agent?',\n",
       " 'eba79c5e-aab2-4e2c-b8c1-9061ab648d43': 'True or False: If the mixture components are members of the exponential family, then the responsibilities can be updated efficiently because they enter only through simple sufficient statistics.',\n",
       " '1494d42d-789c-4ccb-80d0-4db622408d11': 'Fill in the blank: In the incremental EM algorithm described, during the E-step, instead of recomputing the responsibilities for _______ data points, we just re-evaluate the responsibilities for _______ data point.',\n",
       " '6595ee22-f689-4c57-a5d4-c70ef4daa230': 'How are greedy, epsilon-greedy, and UCB action-selection methods similar to heuristic search algorithms?',\n",
       " 'dfe53534-5750-4d57-8390-86059315b473': 'What is one key advantage of using deeper search compared to a one-step greedy policy when you have an imperfect action-value function?',\n",
       " 'dee87670-8241-4ace-a2ed-26e19415537a': 'What is one reason that an exact Bayesian treatment cannot be found for predicting the outputs of a multilayer neural network?',\n",
       " '557082b5-6c9f-484a-b3ec-2adbbe4aa27c': 'What technique has been applied to Bayesian neural networks using a full-covariance Gaussian approximation to the posterior distribution?',\n",
       " '5e9509e9-9dfe-4029-bf20-16bd6105f077': 'In truncated TD(λ), what is the maximum length of the component updates compared to regular TD(λ)?',\n",
       " '6fb51f56-6403-4c9c-97a6-f7f35a68c40c': 'How does the per-step computation scale in truncated TD(λ) compared to n-step TD methods?',\n",
       " 'c22728ff-d6c0-4f3f-a390-ff85d2ea4d88': 'According to the passage, what are two potential downsides of using a small clipping parameter for the critic in WGAN?',\n",
       " 'ef8b38bd-aaee-4bf2-9db4-01bcdd08533b': 'The passage states that the authors used weight clipping in WGAN due to its simplicity and already good performance. What alternative method did they try before settling on weight clipping?',\n",
       " 'd79c6794-ced8-4dfb-9ce4-8d6fffb6a06f': 'What is a key property of many commonly used kernel functions?',\n",
       " 'f56ec7c6-9db5-4376-9e9c-9d7413f08988': 'What are homogeneous kernels also known as, and what property do they depend on?',\n",
       " '0124edc5-209c-45aa-996f-526e7c38a143': 'According to the passage, current SSL techniques in computer vision have difficulty with what aspect of predicting missing information in images or videos?',\n",
       " '7ffe7d36-1b4b-490a-ac2d-3592c601748a': 'The passage mentions SEER as an example of a system that uses a convolutional network trained with a large number of examples. What does SEER demonstrate in relation to the limitations of SSL in computer vision?',\n",
       " 'c142f00b-e10b-4d44-9253-93654ec2d1cb': 'What is the form of the likelihood function for the 1-of-K coding scheme, where ynk = yk(φn)?',\n",
       " 'ca17dcf7-8d61-425e-bcbb-512782b8a730': 'What is the cross-entropy error function E(w1, ..., wK) equal to when taking the negative logarithm of the likelihood function?',\n",
       " '7407f9a4-d5c3-4a25-837f-e9cfb774bf49': 'What are the two distinct stages involved in most training algorithms for neural networks?',\n",
       " '8bf15cfc-2912-4e92-86bf-725abbb98aea': 'What specific contribution did the backpropagation technique make with regards to training neural networks?',\n",
       " 'a54f98ff-6627-4330-8d36-053ae6ced988': 'What technique contributed greatly to the popularization and research of multilayer neural networks in the 1980s?',\n",
       " 'f93c21b1-3efe-4e8b-8261-dee56a8dd975': 'What are two key ideas contributed by the authors of Parallel Distributed Processing regarding cognition, learning, and neural networks?',\n",
       " 'ee980ffc-22c4-4a98-98f9-ae4965a9fd59': 'What are two limitations of representing state s simply by the two-dimensional vector (s1, s2)?',\n",
       " '11828aeb-eb47-43fd-b483-a64bbf0abcc2': 'If we want to allow our state representation to account for interactions between dimensions s1 and s2, what is one way we could modify the representation x(s) = (s1, s2)?',\n",
       " '084af15b-0e90-41fb-8450-b69145c4b83a': \"How does the Gaussian weight prior in Williams' formulation affect the resulting kernel functions k(x, x')?\",\n",
       " '6bbeecab-cfe9-4a6a-969f-ac210b62b801': \"The context mentions we cannot marginalize out the hyperparameters analytically when using William's formulation. What technique does it suggest using instead, as mentioned in Section 6.4?\",\n",
       " 'cd750013-8a97-4b95-94ef-c462d123027b': 'According to the passage, what is the key difference between using a single tiling versus multiple offset tilings for tile coding?',\n",
       " 'fe9f4378-1934-449f-90f5-943629b40b28': 'The passage states that with multiple offset tilings, \"every state, such as that indicated by the white spot, falls in exactly one tile in each of the four tilings.\" How does this lead to coarse coding and what is the advantage over using just a single tiling?',\n",
       " '05793f6f-9265-49c9-aa0b-e9f9341210b7': 'What is one way to mitigate the problem that sampled values from a Markov chain may not be very representative of the equilibrium distribution?',\n",
       " 'f1ec3b17-0ca7-4fc8-b2e6-d77b570ebea9': 'Why are Markov chains expensive to use for Monte Carlo estimation?',\n",
       " 'c120880f-bd06-4e88-b5ec-f24b43bb804f': \"What publication is Curtiss' paper on comparing efficiencies of methods for computing solutions to linear algebraic equations published in?\",\n",
       " '89c32671-9dd5-41e9-b356-52526edf6f06': 'Which author proposed a stochastic learning model of economic behavior published in The Quarterly Journal?',\n",
       " '6d775380-6660-43fb-9766-a38ea799a770': 'Let f(x) = x^2 defined on the interval [0,3]. Show that f(x) is a convex function on this interval.',\n",
       " 'c8fe8508-354e-4fd1-866a-f362dac2fd93': 'Consider the function g(x) = -x^2 defined on the interval [0,3]. Show that g(x) is a concave function on this interval.',\n",
       " 'feeeffa4-a1df-4d58-9b02-03b962c7d34f': 'What 2016 conference paper introduced adversarial feature learning as a method to generate more robust machine learning features?',\n",
       " '60b1e725-2b1f-4803-ac45-aa361ace60ca': 'In 2017, which researchers proposed that generative adversarial networks (GANs) do not necessarily need to decrease a divergence at every step in order to reach equilibrium?',\n",
       " '996b7e74-3c94-4f6f-91dd-30ae825cd4ba': 'According to the abstract in Annals of Mathematical Statistics, what is discussed regarding adjusting an inverse matrix?',\n",
       " '00443470-ed4a-4f9e-a21f-f652394cece9': 'Which book discusses reinforcement learning and simulation based search in the game of Go?',\n",
       " '515bbf1b-c020-4708-ba47-fc0da72fd5c8': 'What are the three common categories of labeling functions examined in the CDR application?',\n",
       " '0cf9ffee-65de-492c-9f90-e95e9ae847d8': 'According to the results in Table 7, what labeling function category ultimately improves the F1 score the most?',\n",
       " '191eed9b-1d5e-4842-a980-4587eb5b0007': 'What is the name given to the way that contractive autoencoders (CAEs) warp space, where they map a neighborhood of input points to a smaller neighborhood of output points?',\n",
       " 'ec245d5b-2d64-4b0f-876a-cf651c118c61': 'How does a contractive penalty on the encoding function f(x) of a CAE relate to score matching?',\n",
       " 'b6806fd3-0c9b-4e91-bb39-626edf33f513': 'What formula allows us to evaluate the uncertainty in the parameter w after observing the data D, converting the prior probability p(w) into the posterior probability p(w|D)?',\n",
       " 'd252cff7-0111-47cd-9435-bd4ac4462c1b': 'How do we capture our assumptions about the parameter w before observing the data D?',\n",
       " '6a940944-4a9f-4f3f-98fb-cec5bb2d246f': 'Consider a classification problem with K classes where the feature vector φ has M independent components. If each component can take on L discrete states represented by a 1-of-L encoding, show that the quantities ak in the softmax function are linear functions of the components of φ.',\n",
       " 'a9d6e03d-5493-4d36-a55f-e975f2389da5': 'For logistic regression, show that the derivative of the error function E(w) involves the quantity y(n) - σ(wTφ(n)). Explain how this result arises from taking the derivative of E(w) and making use of the derivative of the logistic sigmoid σ(a).',\n",
       " 'ef43d758-934a-4116-8eaf-b294d8188e7f': 'What are two approaches mentioned in the passage for evaluating the partial derivatives of a function g?',\n",
       " '61ba682e-6c3a-4442-9459-75fc475b8ff3': 'True or false: Using random projections at the input and output of g to define a new function f can help test the implementation of the derivatives of g. Briefly explain your answer.',\n",
       " 'ac027369-b08a-4677-a973-95d8d1c324dc': 'What methods are guaranteed to converge to the value function that is generally different from those minimizing VE or BE, as discussed in the passage?',\n",
       " 'dd4b0563-4fb6-42fe-a825-78d695d05610': 'According to the passage, which algorithms investigated so far in the book are true stochastic gradient descent (SGD) methods? What are some key advantages of SGD methods mentioned?',\n",
       " '9a99f9a5-b8e5-4152-8f02-1bf476c3cc15': 'What method did Nye et al. propose in their 2020 paper to learn compositional rules?',\n",
       " '90ddfb4a-2938-43c4-ab85-77ed833df9eb': 'Pham et al. worked on improving zero-shot translation in their 2019 paper. What language-independent technique did they use to improve zero-shot translation performance?',\n",
       " '72f943be-d8c7-4eb5-b0c9-030b93f37623': 'According to the context, how are the parameters μ0 and V0 optimized in the linear dynamical system?',\n",
       " 'f30a62c3-cc04-4383-93e2-a0cb8f260d6a': 'The context mentions that to optimize A and Γ, p(zn|zn-1, A, Γ) is substituted into which equation?',\n",
       " 'd50dbb0e-d267-4592-b48b-b53e9f74b3dc': 'According to the passage, why does the marginal distribution p(X|θ) typically not belong to the exponential family, even if the joint distribution p(X,Z|θ) does?',\n",
       " 'fe1779e5-1b3b-4ba7-9ee5-55315e2c2a02': 'The passage mentions incomplete and complete data sets. What is the difference between the incomplete data set X and the complete data set {X,Z}?',\n",
       " 'a441fa82-bfea-4584-b3fb-e5d6bdfb8656': \"What technique did Geng et al. use to improve upon AutoAugment's search algorithm for finding better sub-policies for data augmentation?\",\n",
       " '85bbd29a-5fe9-409c-8371-7eef77f36d67': 'According to the passage, what did Minh et al. explore regarding learning transformations for data augmentation?',\n",
       " '3f04afad-fcc1-441c-bf09-adc38c902db2': 'What was a key difference between TD-Gammon 0.0 and TD-Gammon 1.0 in terms of their network inputs?',\n",
       " '6936359b-8ff4-471f-868e-03304641b046': 'What did Mnih et al. develop that combined Q-learning with a deep convolutional ANN?',\n",
       " 'ef9cb5c5-f9a5-4041-bd20-23def02de669': 'Which author proposed using the Nystrom method to speed up kernel machines?',\n",
       " '7c55d0cc-eb91-40e1-9419-10557d79254e': 'In which journal was the paper \"Bayesian classification with Gaussian processes\" published?',\n",
       " 'aaa05bbd-c5f7-4505-98ea-c8306b8312fc': 'Which 2017 paper proposed Texar, a modularized and extensible toolkit for text generation?',\n",
       " '43c8eb3e-eec8-4a5b-b5f0-f08448c6b494': 'The paper \"Toward controlled generation of text\" was presented at which machine learning conference in 2017?',\n",
       " '356c3b8e-13d6-41fb-9c8e-758fdb6517b9': 'What are the two key differences between basic MCTS and APV-MCTS as implemented in AlphaGo?',\n",
       " '166e9238-b0fb-4f32-a4b3-0b031b032c59': 'How does APV-MCTS evaluate new state nodes during the search, and what are the two components that contribute to this evaluation?',\n",
       " '7cebddd3-62c8-48fb-af32-e58f07581a37': 'Who were some of the early researchers that studied non-associative learning automata?',\n",
       " '55c8fff1-8070-4e57-8b62-68a6717e6678': 'What was the learning algorithm used by Barto, Sutton, and Brouwer in their experiments with associative stochastic learning automata?',\n",
       " 'aeda99be-9e17-4442-a3c7-54c7020cbbc6': 'If x is a random variable and we generate y = sx where s = 1 with probability 3 and s = -1 otherwise, what is the covariance between x and y?',\n",
       " '1c29ff3e-2538-480d-b46e-d323312c4708': 'True or False: The diagonal elements of a covariance matrix give the variance of each element in the random vector.',\n",
       " 'f4889aa9-5458-4896-83a9-c3b193a79686': 'What architecture did the DCGAN paper propose to increase the complexity and resolution of generated images compared to prior GAN models?',\n",
       " '50c68667-13b3-4940-a9d5-5a8a436bcb8a': 'What dataset was used to test the DCGAN architecture and what were the image dimensions (width, height, channels)?',\n",
       " '873523ce-d7b7-443c-ac76-df4e7ee03189': 'What does the E step of the EM algorithm involve in a hidden Markov model?',\n",
       " 'db6dabe4-1f04-4e83-9ebc-ffc07f83404e': 'Introduce notation for the marginal posterior distribution γ(zn) and the joint posterior distribution ξ(zn-1,zn). How can these distributions be represented?',\n",
       " '3b9e4c96-bcc3-47e8-9eac-57c5a2dd62fa': 'What are two problems that can occur when the force due to the gradient of the cost function is small but nonzero?',\n",
       " '28f8c1b8-8993-445c-85d7-931ddc2658b8': 'How does Nesterov momentum differ from standard momentum in terms of where the gradient is evaluated?',\n",
       " 'c08b7bf2-310f-4a8e-9952-936ad01719d9': 'According to the passage, what is one way that representation learning can help with semi-supervised learning when we have a large amount of unlabeled data and a small amount of labeled data?',\n",
       " '72847d7c-37d6-4245-8496-026d99452d96': 'The passage mentions there is a trade-off in representation learning between preserving information about the input and attaining nice properties like independence. Why might attaining independence in the representations be desirable?',\n",
       " '3b2c055b-e8ef-414f-9b04-6d525c42c4bb': 'According to the passage, what are the two main architectural considerations when designing a chain-based neural network architecture?',\n",
       " '0b95bb1d-165e-429f-9d74-e36d64ec89c8': 'The passage states that a network with even one hidden layer is sufficient to fit the training set. However, it also states that deeper networks can provide some advantages. What are two advantages of using a deeper neural network architecture?',\n",
       " '5adb0407-176d-435f-9786-2cef972715a2': 'Derive the update rule for the weight vector θ of a Bernoulli-logistic unit under the REINFORCE algorithm, given that the action preferences are modeled as h(s,a,θ) = θ⊤x(s). Show your work.',\n",
       " '9ae8fbe6-1c38-4a8a-b18d-0ae77a77aa2c': 'Consider a Bernoulli-logistic unit with action preferences given by h(s,a,θ) = θ⊤x(s). If the policy is defined using the exponential soft-max over the action preferences, express the eligibility trace term for this unit in terms of the input vector x(s), the action a, and the policy π(a|s,θ).',\n",
       " '6e087509-d9af-464b-93cc-3c59a6e19a5a': 'One of the applications of machine learning mentioned for denoising is removing scratches from an old photograph. What is one reason this task requires the model to understand the entire input image?',\n",
       " '13a12cbc-a349-4cf1-addb-dcf638862431': 'The context mentions that for sampling, if even one element of the generated sample is drawn from the wrong distribution, then the sampling process is incorrect. Why does this task require the model to have a good understanding of the entire input distribution?',\n",
       " '319ecc7d-a778-4f28-881e-c7d2e322a88d': 'Who first explicitly emphasized trajectory sampling as part of reinforcement learning?',\n",
       " '4bd1a686-c262-4224-804e-5e437127bdeb': 'What algorithm did Barto, Bradtke, and Singh introduce that includes the option of updating the values of many states between the execution of actions?',\n",
       " '560e64bd-d965-47fd-9361-3a9517e247a1': 'What does the Bengio and Delalleau paper show about contrastive divergence (CD)?',\n",
       " '28af8d35-a690-4e0a-9d79-dc5afc384629': 'According to the passage, why is CD not as useful for training deeper models like DBNs or DBMs directly, compared to training shallow models like RBMs?',\n",
       " '173e52ac-9c6e-4a16-b6f2-d671e6f289b1': 'What are two common definitions of Principal Component Analysis (PCA) that give rise to the same algorithm?',\n",
       " '4425ce7d-cea1-4a7b-9a88-97fd80ea983a': 'What are three applications of Principal Component Analysis (PCA) mentioned in the passage?',\n",
       " '81371153-ab6e-429e-ad99-9ad3a06a910d': \"When using function approximation and semi-gradient methods for control in the average reward setting, why can't we directly carry over the discounted formulation?\",\n",
       " '103b499d-8740-4ea7-9963-c47e000fe6f5': 'The text mentions a \"constant-step-size trick\" that can be used with the step-size parameter on the average reward. Briefly describe this trick and how it could be incorporated into the boxed algorithm for differential semi-gradient n-step Sarsa.',\n",
       " '86ad4ef5-2675-49c7-960f-3208670af084': 'True or False: In the generative process described, ptrain(y=1) is defined as the probability of sampling x from the data distribution Paata(x).',\n",
       " '3d80ae91-e621-440a-86e6-24f08ded4a30': 'Fill in the blanks: The distribution pjoint is modeled as a _________ _________ applied to the difference in log probabilities of the _______ and ________ distributions.',\n",
       " 'b298f806-17bc-435c-a50c-3bfdd88580d1': 'What is the return Gt defined as in the simplest case described in the passage?',\n",
       " '0f2cbacd-8489-42bb-af22-87375de228c6': 'The passage mentions episodes end in a special state called what?',\n",
       " '239a5d11-7474-4781-b348-c54864bbc019': \"According to the passage, what can cause uncertainty in a model's predictions?\",\n",
       " '0c01195a-fdc2-4a01-86e6-b20b01364762': \"The passage provides an example of a robot discretizing space when predicting the future location of objects. What effect does this discretization have on the robot's certainty about the precise position of objects?\",\n",
       " 'd56f997a-a083-4781-9337-c58668d4685b': 'What are two differences between the cross-entropy error function and the exponential error function for two-class classification?',\n",
       " 'ad54a2b9-2e4b-4b58-b9a9-0de83bb1a96f': 'The context mentions that the exponential error function penalizes large negative values of ty(x) more strongly than the cross-entropy error function. Why might this be considered a drawback of using the exponential error function?',\n",
       " '431c999e-3bec-4b96-933a-680ef3aac001': 'What does the variance of a random variable f(x) measure?',\n",
       " '90e428bd-ce4d-4792-a904-ce138b1c8aa5': 'How can we summarize the relationship between two random variables f(x) and g(y)? What does a high absolute value of their covariance indicate?',\n",
       " 'f47eeac7-de56-401f-80f8-4743c333d3c4': 'In the E-step of the EM algorithm for Gaussian mixture models, how are the posterior probabilities γnk of the components k evaluated for each data point n?',\n",
       " '719d481c-9c14-4fb4-ae1d-f83e4bac1ef6': 'What is the expectation maximization (EM) algorithm used for in the context of Gaussian mixture models? Briefly explain the E-step and M-step.',\n",
       " 'ba898605-c0d4-480e-8720-ea11c6c32ce5': 'What are the two key designs from SimCLR that were incorporated into MoCo v2 to achieve better transfer performance without requiring a very large batch size?',\n",
       " 'af5733c5-8b15-41a9-ba2e-333ed5804620': 'How does CURL handle the issue of temporal consistency between frames, which is important in RL but not in supervised visual tasks, when applying aggressive augmentation like random crops?',\n",
       " 'acc0df8f-4381-4d63-aeac-81bc59cde863': 'Consider the equation for the tangent vector τn given in the context. What is this equation used for?',\n",
       " '4b1468da-0c3b-4f14-bd47-616679575fa5': 'The context mentions modifying the standard error function E by adding a regularization function Ω. What is the purpose of adding this regularization function?',\n",
       " 'fc62f723-e55b-40d9-8af2-b8c72fec4d8e': 'What does the first-order Taylor series approximation predict about how the value of g will change when decreased by 0.1?',\n",
       " 'ee4328ac-a886-48ca-ad18-1f474647c3f8': 'According to the passage, why is it difficult to choose an appropriate learning rate when updating neural network parameters?',\n",
       " 'bdd966f6-6628-4029-8cef-21fa594b7790': 'According to the passage, why do many traditional machine learning algorithms make predictions by assuming the output at a new point should be similar to the output at the nearest training point?',\n",
       " '44d00134-c49b-44ef-99ca-2b2d38239047': 'The passage mentions that in high-dimensional spaces, the number of configurations is often much larger than the number of examples in the training data. What issue does this cause when trying to make predictions, and how does the passage suggest addressing it?',\n",
       " 'f1e00b0c-e075-43f9-83e5-42c8ca9794fd': 'Which researchers investigated dopamine neurons in monkeys and their relation to prediction and reward?',\n",
       " '6606c367-8e3c-4c38-89ee-20c7331a94f5': 'What type of reinforcement learning method did Schwartz propose for maximizing undiscounted rewards?',\n",
       " '3b02ca7b-28b2-4e58-b60e-59d06eac1756': 'What are the dimensions of the images in the Tiny-imagenet-200 dataset?',\n",
       " '5ea243f5-db43-407e-aa00-d779d5be2ebc': 'What are the two approaches compared to the proposed Neural Augmentation method in the experiment described?',\n",
       " '817161d6-9187-4527-b393-7129b971a491': 'According to the passage, what are two major drawbacks of using analytical regularization to make a model resist perturbation?',\n",
       " 'f4bf39aa-0ad1-4dba-99aa-33fd6fc36ad3': 'The passage states that dataset augmentation works well with rectified linear units. Why is this the case?',\n",
       " 'b356c3f3-0dbe-40bc-9d41-57e9a1d6dd01': 'According to the passage, why is local contrast normalization more likely to produce values with zero standard deviation compared to other forms of normalization?',\n",
       " '65146ee3-ed87-49d5-8efc-dc7be39bcb37': 'What are two examples of advanced transformations commonly used for dataset augmentation in specialized computer vision applications, as mentioned in the passage?',\n",
       " '0b408ff6-2a2c-4b66-9f41-8664b3dfb1fb': 'Which paper proposed a new off-policy TD learning algorithm called Q(λ) that has an interim forward view and Monte Carlo equivalence?',\n",
       " '5b27cb37-feb7-441f-9270-5ffba7148860': 'Which paper presented an approach to off-policy TD learning called emphatic TD learning that assigns importance weights to TD updates to address the problem of instability with off-policy learning?',\n",
       " '2ff587b4-ed20-449c-9ff8-f72b5a5bdb68': 'What does a denoising autoencoder minimize compared to a traditional autoencoder?',\n",
       " 'facde6ea-aef9-45e5-91bc-9aa0a58f9a9d': 'How does denoising training force the encoder and decoder to learn the structure of the data distribution?',\n",
       " '1209113b-ef8e-4c19-849f-5516c00bc029': 'What property is present in distributed representations of words that is absent from purely symbolic representations, according to the passage?',\n",
       " '76d1c118-6e04-4ae0-945a-0c6127265611': 'The passage states that neural language models operating on distributed representations generalize much better than models operating on one-hot representations. Why does using a distributed representation provide an advantage for generalization in this case?',\n",
       " '9178efc2-f5c0-4fa3-bf05-b09433dd22ac': 'Consider a multivariate Gaussian distribution with mean μ and covariance Σ partitioned as:',\n",
       " '7b7e3088-e2ff-4230-abcb-56007d2721d4': 'μ = (μx, μy)',\n",
       " '8b8f4d08-2985-47bc-b269-b9a1c4ff85ab': 'Σ = [Σxx Σxy]',\n",
       " 'e97daa59-cc72-4850-8231-4f9782e03245': '[Σyx Σyy]',\n",
       " 'ebf9978e-84d2-4415-95dd-308fd85fed49': 'is the expression for the mean of the marginal distribution p(y)?',\n",
       " '60b6a90d-82bc-4933-971d-7a7e509b3fe5': 'Again consider a multivariate Gaussian as above. What is the expression for the covariance matrix of the conditional distribution p(x|y)?',\n",
       " '996234c3-5708-443d-83fd-d2bd128c916b': 'What algorithm can be used to perform generative fine-tuning of a Deep Belief Network after greedy layer-wise pretraining?',\n",
       " '7d15eedd-ae00-478c-a861-a286da807007': 'After pretraining a Deep Belief Network (DBN), how can the learned weights be used to initialize a Multilayer Perceptron (MLP) for discriminative fine-tuning on a classification task?',\n",
       " '7b1bc4d6-7970-4b0b-98ce-5e08a90284c6': 'According to the information provided, how is information quantified in a way that satisfies the properties that likely events should have low information content and less likely events should have higher information content?',\n",
       " 'ab16971b-f3ba-4af4-831e-e6b991e189bc': 'The context mentions that independent events should have additive information. As an example, it states that finding out a tossed coin has come up heads twice should convey how much more information than finding out it came up heads once?',\n",
       " 'e47dc933-b5cd-4c83-89d6-89bd866252ab': 'According to the passage, what are the three main benefits of using principled path consistency learning for text generation?',\n",
       " '3743e99e-b815-40f4-9c1e-218e7b62ed45': 'The passage mentions that the proposed method was tested on three different text generation tasks. Can you name these three tasks?',\n",
       " '2b9a68a0-549a-408d-8894-70872acdf034': \"According to Bayes' theorem, what does p(Ck) represent?\",\n",
       " '88334813-d1c2-41db-a630-0cbf963bb4e0': 'If our aim is to minimize the chance of assigning x to the wrong class, what quantity should we choose the class having the higher value of?',\n",
       " 'edfed85c-4581-422b-afdf-fdf449a77608': 'According to the text, what are some of the limitations of Bayesian hyperparameter optimization for deep learning?',\n",
       " 'c58e25eb-9bd4-45a5-aa7f-27c020b72ede': 'The text states that hyperparameter optimization holds potential benefits beyond deep learning. What is one example of another field that could benefit from advancements in hyperparameter optimization?',\n",
       " '443496b0-7b77-4ecf-9581-e2d60076a35b': 'How does the diffusion inversion model differ from denoising autoencoders, even though both attempt to probabilistically undo the effect of added noise?',\n",
       " '22de65e6-8dee-43ff-952f-c340b2f3558a': 'What dilemma exists with the ordinary reconstruction log-likelihood objective used by denoising autoencoders? How does the diffusion inversion approach aim to address this dilemma?',\n",
       " 'f964f734-4394-4464-9c90-7cc94b10e669': 'What did Pavlov call the natural triggering stimuli, like food, that elicit innate, unconditional responses like salivation?',\n",
       " '5202af1b-3fa8-47df-b290-c48eed3c97b6': 'According to the passage, what is one example of a conditioned stimulus that is initially neutral but comes to produce a conditioned response after being paired with an unconditioned stimulus?',\n",
       " 'a9f117b4-f2e3-4777-b208-766e76213ff3': \"According to the passage, why is it usually not possible to simply compute an optimal policy by solving the Bellman optimality equation, even if we have a complete model of the environment's dynamics?\",\n",
       " '49695df7-e285-4d72-88d9-318e02039afa': 'The passage mentions that the amount of computation an agent can perform in a single time step is an important constraint. Why is this computational power an important factor in the problem facing the agent?',\n",
       " 'f78378c0-de36-473d-be33-eb95d012488c': 'The K-means algorithm is often used for what purpose before applying the EM algorithm to a Gaussian mixture model?',\n",
       " '37ea7e08-9493-481c-b2b4-4e0bca4f9547': 'What are two approaches discussed in the text for speeding up the K-means algorithm that avoid unnecessary distance calculations?',\n",
       " '4f64c9d2-4aa3-4f8c-942a-25dc826c90ca': 'According to the passage, what are some of the key differences between finite depths of SPNs that Martens and Medabalimi found in their research?',\n",
       " '74ee506c-655a-4c6f-bd75-cdf61dc672b2': 'The passage mentions that some theoretical results highlight the exponential advantage of deep circuits compared to shallow circuits, even when the shallow circuit is allowed to approximate the function of the deep circuit. How does this compare to previous theoretical work on this topic?',\n",
       " '8e78477c-7e46-4117-a059-e55b285ca22a': 'According to the passage, why can determining dimensionality via cross-validation become computationally costly for a probabilistic PCA model?',\n",
       " 'dedda9dc-4c1d-4b6f-bb0d-5146a7ff3e65': 'The passage mentions considering a probabilistic mixture of PCA models. What is one reason this could be beneficial, according to the text?',\n",
       " 'ba71a7f1-b316-43c6-92b9-61ffb8bbf13d': 'What are the two key ideas proposed to enable planning at higher levels using extended courses of action that correspond to many base-level time steps?',\n",
       " '2cb53eb8-7508-4a8e-9403-0b1b6364abe4': 'Explain the two components of an \"option\" and how an option is executed over multiple time steps.',\n",
       " '55e808be-ba72-4653-a3ff-b8b982ca76eb': 'According to the text, why can we not simply maximize the likelihood function to determine the appropriate model complexity?',\n",
       " 'a5a9195a-716b-452d-87c2-f017537ac8c4': 'The text mentions that a Bayesian treatment of linear regression can help avoid which problem that arises from maximum likelihood?',\n",
       " '025cc299-af30-4fc4-af33-0005db637ade': 'What theorem states that the averages of random variables tend to a Gaussian distribution?',\n",
       " 'a399ed34-3065-41b6-9d40-bf110f615104': 'If X and Y are Gaussian random variables, what distribution does X + Y follow?',\n",
       " '2b209935-72a9-4dd0-aa49-78e8c5da66b0': 'Who proved convergence of linear TD(0) in the mean to the minimal VE solution for the case of linearly independent feature vectors?',\n",
       " '4bd1583f-e094-426d-8004-866e6705b1bd': 'According to the passage, what basis did Konidaris, Osentoski, and Thomas introduce that is suitable for reinforcement learning problems with multi-dimensional continuous state spaces?',\n",
       " '16c0a027-589c-4b01-b0da-9d71737760bc': 'Let N1 and N2 denote the total number of data points in class C1 and C2 respectively. What is the maximum likelihood estimate for the prior probability π of class C1?',\n",
       " '10551408-3805-4406-9094-ee79b3ab1600': 'Consider a data point xn from class C1. What is the likelihood function p(xn|tn) in terms of π, N(xn|μ1, Σ), and N(xn|μ2, Σ)?',\n",
       " 'a2cb34df-11df-4685-a57e-084e375fe5df': 'What simple heuristic does the Dyna-Q+ agent use in the shortcut maze environment to encourage exploration of long-untried state-action pairs?',\n",
       " '490a5a76-5846-4366-bd03-3aec1da00b4a': 'What is the \"bonus reward\" given by the Dyna-Q+ agent during simulated experiences for long-untried state-action pairs, if the modeled reward for a transition is r and the pair has not been tried for τ time steps?',\n",
       " '021458ba-56a8-42ec-ae95-52b8f3e8a2aa': 'What does the generative model in Snorkel do?',\n",
       " '105cbd1e-def4-4b6e-a13b-ded7fe9f2bb6': 'According to the passage, what are two benefits of using modern discriminative models over the generative model in Snorkel?',\n",
       " 'f4733b60-0cf5-49d7-ac71-1629f004cee1': 'What is the difference between the action-value (Q-value) and the state-value V(s)?',\n",
       " '21ed6ae4-e1a0-4dfa-bbd3-a0422144ea90': 'How can we recover the state-value V(s) using the policy π and Q-values?',\n",
       " 'd47a08da-b604-44b7-9142-90486e88b14b': 'What were the step-size and discount rate parameters used in the reinforcement learning algorithm described in the passage?',\n",
       " '3ae83844-dc4e-499e-b8af-6fe3ad79f398': 'According to the passage, how long did each simulated flight episode last during the reinforcement learning experiments?',\n",
       " '1cf0515f-af9e-4a77-9df1-e3677093777b': 'How do Monte Carlo backup diagrams differ from DP backup diagrams?',\n",
       " '6ff4c6db-70de-448b-8620-c38869259591': 'What is an important fact about Monte Carlo methods regarding the estimates for each state?',\n",
       " '63d53781-c8d3-418f-8387-f8bae2733ebb': 'What is the key difference between \"sparse coding\" and \"sparse modeling\" according to the passage?',\n",
       " '35ecc448-bb0d-49fb-9cf9-45c95f7e6ffc': 'What type of noise distribution is commonly assumed for the linear factors in a sparse coding model?',\n",
       " '3af346dd-244e-44d2-bcee-1c2c0d23abb9': 'What are two limitations of using a least-squares approach for linear classification that are mentioned in the passage?',\n",
       " 'f2dc96a0-e239-4f51-98d0-0bc1d92c5866': 'According to the passage, why does adopting more appropriate probabilistic models yield classification techniques with better properties compared to least squares?',\n",
       " '84519674-ea66-4845-906b-290a70b7f926': 'What is the goal of the student step in the student-teacher optimization framework?',\n",
       " '1770fe3e-21f0-43e1-8ffe-222d95486358': 'What sampling methods can be used to draw samples from the teacher distribution q(n+1) during the student step?',\n",
       " '3b725503-d3cf-40b6-8e82-09dc81fcb16b': 'Consider the classical form of a dynamical system given by equation 10.1. What is s(t) called in this equation?',\n",
       " '666f446a-9f6b-4148-bbb6-f7395e0b93d6': 'The context describes unfolding a recurrent equation like 10.1 over 3 time steps. If we were to unfold the equation over 5 time steps instead of 3, how would the unfolded computational graph change compared to the example with 3 time steps?',\n",
       " '2bdc8cad-e3de-4f3f-9260-a71bde6c453e': \"What year was Arthur Samuel's first learning program completed and demonstrated on television?\",\n",
       " '68eb486b-d61d-4c53-8ee1-ce8665993971': 'According to the passage, why did Arthur Samuel choose to study checkers rather than chess when researching machine learning?',\n",
       " '9846827a-0c12-464e-bff3-c9066ff553d6': 'What does each node in the diagram represent?',\n",
       " '72090663-e772-4e5e-80bd-bcbbed3e434e': 'What process allows us to find the most probable state sequence after determining the most probable value of the final node xN?',\n",
       " '2290e7de-53da-4d2f-8595-652570dbe587': 'According to the text, what happens to variance and bias as model capacity increases?',\n",
       " 'd4f47a87-af47-4b0f-85d4-cb1258d29796': 'Refer to Figure 5.6. As model capacity increases, generalization error forms a U-shaped curve. Briefly explain why this occurs.',\n",
       " '93f9cefc-517c-46a6-ab91-d004123527a1': 'Who developed the formula eiπ = −1 that relates four of the most important numbers in mathematics?',\n",
       " 'c8485a42-bdb1-4fe6-a81f-cc847f34f6d6': 'What does the general expression for q⋆j(Zj) in equation 10.9 represent? How is it obtained?',\n",
       " 'e6b6785f-fdae-4123-9c30-44adb2fc0eac': 'What is an important difference between the gradient update in PCL training compared to vanilla training in conventional Q-learning?',\n",
       " 'bf2b3335-8a95-4aa0-ba3c-074ae1a4472b': 'How does the PCL training objective differ intuitively from the MLE objective? What does PCL encourage that is different from MLE?',\n",
       " 'd845a3bb-cc8b-449f-962f-c59fca193b9b': 'What are the four simple operations used by EDA to randomly transform a sentence?',\n",
       " '32d6fa28-15d4-448e-a379-4f05dc39dad5': 'According to the context, why does EDA allow more noise to be introduced into longer sentences compared to shorter ones?',\n",
       " '655b85a2-c03a-423f-bf11-ff752efd2626': 'What is the advantage of using the Expected Sarsa form of the general action-based λ-return (equation 12.20) over the recursive form (equation 12.19)?',\n",
       " '9c78bc00-1471-487f-b16e-3f3564f4cc43': 'In equation 12.27, what condition needs to be met for this equation to become exact?',\n",
       " '9fb19d26-158f-4b81-baf5-bc99eb4ca7ba': 'What is a critical point in a function?',\n",
       " '52304bd7-c57e-4992-80fe-9a36739fce9b': 'True or false: A saddle point is a type of critical point that is neither a local maximum nor local minimum.',\n",
       " '904ff0ae-196f-4e9c-a567-ef1566be9e18': 'What algorithm is discussed in the context that may fail to converge when the data set is not linearly separable?',\n",
       " '85c7f8e4-17ae-4605-aad0-787b4d31e6ab': 'The context mentions that the solution found by the perceptron algorithm depends on two factors. What are those two factors?',\n",
       " '9476c3bf-d6d5-4233-89a9-72e2d43d4f29': 'What is the notation Vk used to represent in the theorem discussed in the passage?',\n",
       " '7425d6ac-b506-4ee5-b06e-26f6677f9f9f': 'According to the passage, the truncated version of the general off-policy return can be denoted by which variable?',\n",
       " 'cb2b2f4b-93c2-40d9-b4cd-afb18ac11a35': 'What is the general form we have seen multiple times now for the gradient of the error function with respect to the model parameters?',\n",
       " 'ff6868a1-e2a9-4cbb-973c-78bdedcf5db0': 'For the multiclass softmax model with cross-entropy error, what was the form of the gradient that we obtained? How does this compare to the forms seen for linear regression and logistic regression?',\n",
       " 'cf67a847-29bb-41c6-9f98-447acf4629d1': 'The EM algorithm is useful for finding maximum likelihood solutions for what types of models?',\n",
       " '4e57e40a-2702-44cf-992d-490c0dab8aee': 'What makes maximizing the log likelihood function more complex for a Gaussian mixture model compared to a single Gaussian?',\n",
       " 'f0f371dc-2fdc-417e-a424-103f0e154aed': 'If we choose a prior distribution over a parameter λ to be constant, and then change variables to λ = η^2, what happens to the resulting distribution over η?',\n",
       " '6b4f2d48-1eeb-49fc-afe3-99d456aee6e6': 'What is an example of a parameter that leads to a family of densities exhibiting translation invariance? Explain why this family exhibits translation invariance.',\n",
       " '221b8ccc-3c92-472d-aecf-5c6490fae037': 'What technique does the stochastic gradient descent (SGD) algorithm use to help avoid getting stuck in local optima when training deep learning models?',\n",
       " '3e8d5148-15a4-4a18-b582-430f65c03b45': 'What are two key factors that the choice of initialization point impacts when training deep learning models using iterative optimization algorithms like SGD?',\n",
       " 'd6170c97-d73c-45ad-a3fa-4bbcb9b6d594': 'According to the context, there are two reasonable answers for the estimated value V(A). What are these two answers?',\n",
       " '1025bb7b-ad67-40ee-b5f1-b527dee6589b': 'The context mentions that the Monte Carlo answer gives minimum squared error on the training data but the model-based answer is expected to produce lower error on future data. Why is this the case?',\n",
       " '316a30f9-d59a-4673-89b9-ef81ece70a00': 'What is the constraint on the dual variables {an} that needs to be satisfied when minimizing (7.32)?',\n",
       " '1a0b01fc-fb58-4b82-9d92-bf23f750137a': 'If a data point xn has 0 < an < C, what does this imply about ξn and the position of xn with respect to the margin?',\n",
       " '4f449cc2-530b-48de-b916-955478447edb': 'What technique did Samuel use to encourage his checkers program to move along the most direct path to a win?',\n",
       " 'b4549fe3-7c55-46c1-80a7-8c2d121f8f74': \"According to the passage, what are two phases of a checkers game where rote learning produced slow but continual improvement in Samuel's checkers program?\",\n",
       " 'ed9cc3e2-b4e7-4381-aba9-e3f569454f20': 'What is the key difference between evolution strategies (ES) and MDP-based reinforcement learning approaches that was mentioned in the passage?',\n",
       " '323052b3-bdd6-45a7-804f-5794e134deaf': 'How does evolution strategies (ES) aim to learn the policy parameter θ without using value approximation, according to the context provided?',\n",
       " '96662625-9cf4-484b-a835-989d52822a91': 'What are some key design choices when creating feedforward neural networks?',\n",
       " 'de755943-4797-4eae-b0a5-62ce9c94e767': 'According to the text, rectified linear units are a good default choice for which component of a neural network?',\n",
       " 'a6f81389-6ced-4bf7-89d8-1a5292194621': 'What is the Bellman optimality equation for v∗? How does it relate to finding an optimal policy?',\n",
       " 'a3b4a0f0-7010-422b-b6f2-d9e1149b9059': 'The context describes two graphical representations related to the Bellman optimality equation. Briefly describe what each backup diagram is representing in relation to the Bellman optimality equation.',\n",
       " 'ab269139-e0e1-4c15-bc5f-953473dcb75b': 'What is the key innovation of the Snorkel system compared to previous ML systems according to the authors?',\n",
       " 'd9c48b89-9b8e-4562-ae7f-902dd50c5021': 'According to the results reported in the passage, by what percentage did Snorkel outperform heuristic baselines on average in the deployments with the US Department of Veterans Affairs, Stanford Hospital and Clinics, and the US Food and Drug Administration?',\n",
       " 'f820f6c9-207c-4d64-8fac-3c17bac11d75': 'What is a composite return and how can it be used to construct updates with guaranteed convergence properties?',\n",
       " '8bfb5c86-c427-4346-ba4f-088dc5fe3ab0': 'The text mentions averaging one-step and infinite-step returns as a way to interrelate TD and Monte Carlo methods. Can you explain how this averaging provides a connection between these two types of methods?',\n",
       " 'ca7c8c62-57f3-4f5b-b5c9-693815f49cea': 'According to the passage, why does averaging multiple models trained on bootstrapped data sets lead to improved predictions compared to using just a single model?',\n",
       " 'b260fc5b-854d-4677-b282-394518fc88e7': 'The passage mentions a \"trade-off between bias and variance\". Briefly explain what is meant by the \"bias\" and \"variance\" components when evaluating the performance of a predictive model.',\n",
       " '1a10af60-db02-4a7e-9fe2-cc2c3cfedf06': 'What is the key difference between supervised contrastive loss and cross entropy loss?',\n",
       " '73c7920e-a28c-4792-b757-12c7227245b4': 'Explain the process of creating training pairs from the original (image, label) pairs when using supervised contrastive loss. How many training pairs can be created from n original pairs?',\n",
       " 'c675a9ad-2862-4e21-8937-7e8716c691ac': 'According to the passage, what are two ways to interpret the use of deep architectures in neural networks?',\n",
       " '10c8ef04-62e9-47cc-89fc-61dee66d5936': 'The passage states that empirical results suggest greater depth results in better generalization. Can you describe two of the empirical results mentioned that support this?',\n",
       " '1f6e25e1-d311-4c29-b675-dce490a2c523': 'What is the key idea behind using importance sampling to estimate the partition function Z?',\n",
       " '2788a607-2ffa-4a7b-9b56-d0d564a70f52': 'If we draw K samples {x(1), ..., x(K)} from a proposal distribution po(x), how can we construct a Monte Carlo estimator Z1 for the integral in equation 18.44?',\n",
       " '90c54ec0-c4eb-4abc-b77f-2e85a0928e57': 'According to the passage, what are some of the limitations that our evolved reward signals help compensate for?',\n",
       " '6c861951-cedc-4c78-a495-d06919e42173': \"The passage mentions that a reinforcement learning agent's reward signals may depend on factors inside the larger behaving system it is part of. Can you give an example of what one of these internal factors might be?\",\n",
       " '5b1daea6-aeab-419a-9b56-25f58cb09e0f': 'Who generalized the results of Vincent on identifying a family of shallow autoencoders such that g(f(x)) ≈ x corresponds to a score?',\n",
       " '6728c055-ee42-4140-9baf-d8b591e5b5ce': 'What section of the text describes using the autoencoder as a generative model to draw samples from the learned distribution?',\n",
       " '792e5014-c9b6-4921-a758-dd31bf18ba0c': 'What is the key advantage of directed models like those discussed in section 16.3 over energy-based models (EBMs) when it comes to sampling?',\n",
       " '20cab45b-794c-4835-b2e0-813823de73ff': 'The text mentions using a Markov chain to avoid the \"chicken-and-egg problem\" of sampling from an EBM. Briefly summarize how a Markov chain allows one to obtain fair samples from the distribution defined by an EBM.',\n",
       " 'cdeb2f65-90cc-4dce-9c6d-efbb061122d0': 'According to the passage, what term did Tesauro and Galperin adopt to describe the use of rollout algorithms to evaluate backgammon positions?',\n",
       " '1bcda687-012c-4355-8b5d-e57adf41b3fc': 'The passage states that Abramson argued randomized rollout algorithms are a \"powerful heuristic\" for game playing. What properties did he claim made rollout algorithms effective?',\n",
       " 'e16f0c2f-c489-4c74-8a8d-e2f73af18087': 'If A is a symmetric matrix, which of the following must be true?',\n",
       " 'd299ff58-5ef6-4623-9fa0-a28b15799793': 'A is singular',\n",
       " 'a842f029-9764-475f-992d-70288bfd54fe': 'The eigenvalues of A are complex numbers',\n",
       " '7f30bbd6-2e27-4ead-8ca3-1eb879ba57c2': 'The eigenvalues of A are real numbers',\n",
       " 'a31d7c9c-cb7d-48c9-8f9c-257791863e0e': 'The rank of A equals the number of distinct eigenvalues',\n",
       " 'f1a8621b-e099-4ed0-a3bc-2f8e041155fc': 'Consider the equation (C.29) multiplied on the left by (ui*)T. What can we conclude about the eigenvalues λi of a symmetric matrix A from this manipulation?',\n",
       " '4cc025ee-b0b4-4b62-b73f-ce4c994cec92': 'The eigenvalues must be real numbers',\n",
       " '386caa4d-b219-44de-bc36-64c6797e28f3': 'The eigenvalues must be complex numbers',\n",
       " '5fab8b73-e23c-4828-a674-d4fcc59dc9c3': 'The rank of A equals the number of nonzero eigenvalues',\n",
       " '7a2eb182-ad66-430c-9a5c-7d624f342253': 'A must have M distinct eigenvalues',\n",
       " '21c5a968-7ed3-4dda-a876-63746d8db0e2': 'What is the main difference between how the EM algorithm is used in classical machine learning versus deep learning models?',\n",
       " 'e5cef4e0-4597-4e2f-bff2-5d0dbf281555': 'What does the acronym MAP stand for in the context of inference? Briefly explain what MAP inference computes.',\n",
       " '50fdec4c-3ba9-4dfe-9175-4c2756995fe5': 'According to the text, what is the only assumption we need to make in order to obtain a tractable practical solution to our Bayesian mixture model?',\n",
       " '5e2a448d-bca5-4be6-aa44-d52ff5ebe210': 'What does the variational distribution q(Z, π, μ, Λ) factorize between, according to the text?',\n",
       " '4ed0979f-3815-41c1-af2f-bd2ce0f1dc5b': 'What are the two key differences between the updates for TD(0) and episodic semi-gradient one-step Sarsa?',\n",
       " '389c0569-f83a-4f0b-9b70-e3115e98e7e1': 'According to the passage, what are some of the challenges faced when trying to couple action-value prediction methods with policy improvement techniques for continuous or large discrete action spaces?',\n",
       " 'a21f657d-2ca5-42a8-9d34-6559da80e970': \"According to the passage, how did Skinner's view of operant conditioning differ from Thorndike's law of effect?\",\n",
       " '9c5886e2-edb4-407d-91e1-cca009cfe706': 'What are two key components of the operant conditioning chamber or \"Skinner box\" that Skinner invented?',\n",
       " '701b3fcc-30e1-4a28-adfe-4748e96ed4ba': 'What are the two domains used as an example for the image-to-image translation task using CycleGAN in this work?',\n",
       " '92b4afef-0598-403d-9976-c4575fb526d0': 'What is the architecture of a CycleGAN comprised of according to Fig. 20?',\n",
       " 'f27216ff-ac1d-4f45-a83b-43c5fd03b614': 'What is the purpose of including a baseline Bt in the performance gradient formula?',\n",
       " '126181fa-44f3-4044-96ab-a44d6f4f411c': 'Walk through the steps that show the gradient bandit algorithm is an instance of stochastic gradient ascent.',\n",
       " 'f578b840-0309-4b02-ae6a-902bd9d45333': 'When using a line search method for constrained optimization, what are two ways we can ensure the search remains feasible?',\n",
       " 'f25301b8-7be1-4f4d-a194-d4c1a538b84f': 'The text mentions converting a constrained optimization problem into an unconstrained one. What example does it give of this technique?',\n",
       " '777fac96-4d7a-4e37-9371-96d77515f530': 'What strategy does score matching use to avoid difficulties associated with differentiating the partition function Z?',\n",
       " '402fc2f9-2a98-47c2-8f69-a0b4f7ebead2': 'True or False: Score matching requires knowledge of the true data distribution pdata(x) in order to compute the score of the data distribution.',\n",
       " '4b77f3b2-71fe-49e9-aa20-745ad1a435a3': 'What is the key difference between the batch and sequential algorithms discussed in the text?',\n",
       " '5edd02fc-75d2-4830-bd5c-e572dce4add7': 'The text mentions that the conditional expectation of z given θ defines what type of function f(θ)?',\n",
       " '4ef6086c-391b-447f-bb3d-4f6d0b70f045': 'What are some of the key objectives used in prior work to pretrain sentence representations, according to the passage?',\n",
       " '4dba7359-5400-490a-aa6d-575f24b16821': 'The passage mentions ElMo and its predecessor developed a different approach to traditional word embedding research. What was the key difference in their approach?',\n",
       " '452f3309-eb5f-4c3c-9bfc-c57f70ced53c': 'What are some of the key benefits of using test-time augmentation on image data?',\n",
       " '735a943d-1d82-4b3d-b1d0-f8b40585a80f': 'How does test-time augmentation relate to ensemble learning techniques? Explain the analogy that is made.',\n",
       " '35a4d071-39b2-4e3c-a8e3-79cebe475abf': 'According to the text, how does maximizing the log-likelihood relate to minimizing the mean squared error?',\n",
       " '3978ce41-8fec-42df-b6d2-4484756bbb35': 'What are the conditions required for the maximum likelihood estimator to be consistent, meaning it converges to the true parameter value as the number of examples approaches infinity?',\n",
       " 'b4ee1b1a-6c7b-42e6-a9ce-b603e0e229d9': 'What are two key differences between Memory-Augmented Neural Networks (MANNS) and recurrent neural networks like RNNs or LSTMs?',\n",
       " '64da3eae-63d1-43a7-8f24-4a345db1d1df': 'What modification did Santoro et al. make to the Neural Turing Machine architecture to make it more suitable for meta-learning tasks?',\n",
       " '504bdb76-2f93-4668-9aba-21b3efa7907d': 'According to the context, what are the main advantages of using the dual formulation over the original parameter space formulation?',\n",
       " '96f8a38b-348e-4052-908a-d654a53fc2e0': 'The context mentions that the existence of a dual representation based on the Gram matrix is a property of many linear models. Name at least 2 other linear models, besides the perceptron, where duality plays an important role.',\n",
       " 'd17a9575-995d-41e6-a310-5e59a599def0': 'According to the passage, what is one key benefit of using dimensionality reduction for information retrieval tasks?',\n",
       " 'f1895382-5f25-47aa-866a-39651665223f': 'The passage mentions \"semantic hashing\" as an approach that uses dimensionality reduction and binarization for efficient information retrieval. What are the two main components of semantic hashing that enable its efficiency, according to the passage?',\n",
       " '707eb48e-68b4-47d3-86f7-729915ec52da': 'Which institute published the technical report \"Potential-based shaping and Q-value initialization are equivalent\"?',\n",
       " '79f806c7-01c1-481e-80ba-df0303e4de0b': 'What conference proceedings published the paper \"On the use of backpropagation in associative reinforcement learning\"?',\n",
       " '8f42a151-9764-4f29-b0c8-465768394b3d': 'According to the passage, what is the role of the additional term γV(St) - V(St-1) in the TD error delta?',\n",
       " '7c25d29e-2608-477d-996e-6ce913d40c47': 'What do recent optogenetic experiments demonstrate about the role of phasic dopamine neuron responses?',\n",
       " '90e2604b-0df8-43b9-910f-81b517e656c9': 'What are the key differences between the Monte Carlo update, TD(0) update, n-step TD update, and DP policy evaluation update for value prediction? Focus specifically on what state is being updated in each case.',\n",
       " 'd72797d4-8e83-491c-82a3-8edc83a4425d': 'The text mentions that the update \"s <- u\" can be interpreted as the estimated value for state s should be more like the update target u. Up until now the updates have been trivial, just shifting the table entry for s towards u. Explain what is meant by permitting \"arbitrarily complex and sophisticated methods\" to implement the update, such that the values of many other states are changed as well.',\n",
       " 'ce00d130-2e98-444c-9b01-ec2a0fa930f1': 'According to the passage, what are some of the useful properties that probabilistic graphical models offer?',\n",
       " '398157aa-5228-4e93-9da7-9bfd9c64996e': 'The passage states that a graph comprises nodes connected by links. In a probabilistic graphical model, what does each node represent and what do the links express?',\n",
       " '70300e0f-5210-4894-b22e-3f916dda579b': 'According to the passage, what is the first experience function fsc defined as?',\n",
       " '327ea912-9e07-4e6e-b7b5-1cd2439e4732': 'What information do the reconstruction data instances D carry, according to the passage?',\n",
       " '45ad89ab-7108-4239-b6e6-a4ac9dd12291': 'According to the passage, why is finding a global optimum rarely possible for complex function approximators like neural networks?',\n",
       " '0b5b8c4d-fdd2-42b7-8a83-b27864548789': 'The passage mentions that complex function approximators may seek to converge to a local optimum instead of a global optimum. What guarantee does reaching a local optimum provide, according to the passage?',\n",
       " '8f46a941-65e2-4565-9aba-1cc6945bf5b2': 'What training procedure was used to improve training convergence of Generative Stochastic Networks (GSNs)?',\n",
       " '09e0e833-b57a-44a0-9325-b7a49ec4ced9': 'How did Zhou and Troyanskaya generalize GSNs to optimize p(y|x) instead of p(x)?',\n",
       " '7665e2d3-443a-440d-b01a-1bc3c9db9893': 'According to the context, what are two possible explanations if training error is low but test error is high for a machine learning model?',\n",
       " '47051822-6c42-485c-9d76-d0ec0a1563e7': 'The context mentions fitting a model on a very small dataset as a way to determine if high training error is due to underfitting or a software defect. What is given as an example of a small dataset that could be used for this purpose?',\n",
       " '3732209c-d537-4048-b641-d1c3f03efc4e': 'What is the form of the Gaussian approximation to the posterior distribution obtained from the MAP solution wMAP?',\n",
       " '402840ce-a1bf-40f3-86ab-3c768c2b48a1': 'What matrix gives the covariance for the Gaussian approximation to the posterior distribution?',\n",
       " '48f8f202-c205-45a2-99b0-cfcc92af1f7b': 'What are the two main approaches discussed for predicting user preferences?',\n",
       " '8cc6f2a3-8583-4e1e-b1c9-33348d9fcab6': 'The bilinear prediction model predicts ratings by taking the dot product between what two matrices?',\n",
       " 'a5266ccd-5e29-4db1-b2f6-d6b70c7a30ba': 'What does batch normalization provide an elegant way to do, according to the passage?',\n",
       " '9b5c44ca-6145-4c21-ae32-af552f4fe5e9': 'According to the passage, how is batch normalization applied in a network? Describe the mathematical details.',\n",
       " 'c862f7a5-15c9-4e5b-af57-e71e2f4afb68': 'In the example directed graph in Figure 8.32, how many nodes are there? How many edges? What is the joint distribution p(x1,x2,x3) expressed as in terms of the directed graph?',\n",
       " 'd789471b-a23f-48c7-a563-dffa84b76a3b': 'When converting a directed graphical model to an undirected graphical model, what needs to be done to the marginal distribution p(x1) for the first node? How does this relate to the form of the joint distribution expressed using maximal cliques in the undirected graph?',\n",
       " 'e43b5949-31d3-4b5d-be98-69fc110c1e44': 'What are two data augmentation techniques mentioned that can help learn good visual representations without changing the semantic meaning of images?',\n",
       " '4b376baf-8350-4e69-b1c4-b1bea67aaaf5': 'What are two key ingredients mentioned that have contributed to the success of contrastive learning methods like SimCLR and CLIP?',\n",
       " '23a9ef23-161d-4ded-9e20-b5547ebeb71d': \"According to the passage, why don't the samples generated by the spike and slab sparse coding model trained on MNIST resemble the original training examples, even though the model has learned useful features like strokes and digits?\",\n",
       " '6c918145-d419-4f09-bd16-df4e0a9da2e6': 'The passage states that the spike and slab sparse coding model has a problem because it uses a \"factorial prior over features\". What does this mean, and how does it lead to the model\\'s inability to form recognizable MNIST digits according to the combination of features it has learned?',\n",
       " '4d730b77-789d-4d0b-86af-1780316012bd': 'According to the context, who proposed the \"Law of Effect\" principle that behaviors followed by favorable consequences become more likely to recur?',\n",
       " '7be7c30b-0b8e-4945-acf6-6399413c19b1': 'Which researcher is credited in the context with developing a \"new type of behaviour theory\" published in the British Journal of Psychology?',\n",
       " '532cf21f-46d9-4da2-88f8-7a249c57e290': 'What are the two key differences between TD(λ) and HTD(λ)?',\n",
       " '94b1e381-4428-493c-a83d-16caa03d0979': 'How does Emphatic TD(λ) retain strong off-policy convergence guarantees while still enabling bootstrapping?',\n",
       " 'da393e1d-4075-4b4b-8384-ac2d41bc4bc7': 'According to the passage, why does the agent only need to learn predictions of future reward for the sequence of states it experiences, rather than learning the actions required to obtain the rewards?',\n",
       " '5a6ece4c-e434-47e9-9662-bc6121d0c004': 'What term does the passage use to refer to the process of the agent learning accurate predictions of future reward for the sequence of states it experiences?',\n",
       " 'bf813358-9261-4824-91b7-23ca7afdbc3c': 'According to Chen et al. (2020a), what technique did they propose for semi-supervised named entity recognition (NER)?',\n",
       " 'bdf10fd7-5783-468e-a6e4-970ab7f1a7a9': 'What publication venue was Local additivity based data augmentation for semi-supervised NER by Chen et al. (2020a) presented at?',\n",
       " '0e4652e4-688e-4a71-9492-e0e8f81eac02': 'What is the standard parameterization of the Gaussian distribution for a single variable x?',\n",
       " '57562871-257f-4a6a-a828-07620d038b25': 'What is the name of the joint conjugate prior distribution if both the mean μ and precision τ = 1/σ2 are unknown for a Gaussian distribution?',\n",
       " '5153e204-f4c2-41a0-b7f1-ca74614a1508': 'According to the Gaussian noise assumption described in the passage, what is the form of the conditional distribution of t given x?',\n",
       " 'd2d6ecd1-4e11-4c09-85a4-6be4534a2a95': 'In Section 1.5.5, what is the optimal prediction for a new value of x, assuming a Gaussian noise model and squared loss function?',\n",
       " 'eaafb5e0-4e71-475d-b851-bebb18dc0258': 'What are the two main classes of reinforcement learning algorithms discussed in the overview? Can you briefly explain the difference between them?',\n",
       " '0fac4205-3a83-4759-ae3a-ef81afde1125': 'The overview mentions that policy-based methods are more useful in continuous spaces compared to value-based methods. Why is this the case?',\n",
       " 'b55ef4af-c52c-4ee3-b6f4-69a44e23d3fc': 'According to the passage, how can modeling p(x) in an unsupervised way reveal the factor y?',\n",
       " '2a9df362-1fe7-4f6a-ab18-a9fa41715825': 'The passage states that \"observing a training set of w values alone gives us no information about p(y | x)\". Why is this the case?',\n",
       " '5781f8ff-7120-408b-b341-09daf1760ece': 'What are some of the data augmentation techniques discussed in the passage for creating multiple versions of a text sample during contrastive representation learning?',\n",
       " '4c77f441-6bc3-4067-8d0e-a07f3eee208c': 'The passage mentions two specific methods for contrastive learning on text - SimCSE and the cutoff augmentation technique. What are some key differences between these two methods in terms of how they generate augmented samples for contrastive learning?',\n",
       " '2b68fb1d-0e68-4b67-a924-4743ae248d50': 'The mcRBM has a nondiagonal conditional covariance structure. How does this impact contrastive divergence and persistent contrastive divergence training?',\n",
       " '13337537-d8ed-41f4-9d23-19abfcc0a717': \"Sampling from the conditional distribution p(x|h,h') in the mcRBM requires computing the inverse of a matrix at each iteration. What is this matrix and why does computing its inverse create a computational burden during learning?\",\n",
       " 'e5d86420-1d18-4126-91ef-e1ae7ff4c8aa': 'Consider the graph in Figure 8.32(b). This graph can be transformed from a directed chain into an equivalent undirected chain without needing to add any extra links. Why is this possible?',\n",
       " 'df43b14c-695c-4281-991b-1a4276ec6bee': \"In the chain of nodes shown in Figure 8.32, we can use the sum and product rules of probability along with Bayes' theorem to evaluate the joint distribution p(x,y). What property of the graph in Figure 8.32 allows us to evaluate the joint distribution in this way?\",\n",
       " '496725d6-2b78-41fe-90b0-ee9eba7ca0e8': 'According to Hinton and van Camp, what is the goal of their proposed approach for keeping neural networks simple?',\n",
       " '57768458-cfd0-4440-9a63-7ac60d7c16a8': 'What two techniques does the paper by Hofmann propose for document retrieval and classification?',\n",
       " '32107516-5bb4-4b32-b03c-3c1430885283': 'What is the energy function that defines the joint distribution in the ssRBM model?',\n",
       " 'b17dde4b-1f02-4931-9508-d3ac88732c67': 'What are the model parameters in the energy function for the ssRBM? Describe what each one represents.',\n",
       " '98db7bd5-e2fd-462f-add1-e1dd60b607f7': 'What assumption about the distribution of data points is made when using the likelihood function expressed as the product over all data points?',\n",
       " '5d6cadba-eafa-416b-b845-5149b1606257': 'Give two examples of types of sequential data discussed in the passage.',\n",
       " '4402e6ba-eb6e-442a-bcff-5012ffc8abb7': 'If we have a prior distribution p(σ) ∝ 1/σ for the scale parameter σ, which of the following statements is true?',\n",
       " '0890dab9-a8c7-40f4-9bad-7d8a751224f5': 'p(σ) is an improper prior',\n",
       " '66d627e2-0faf-48ab-b1a8-4abb16b0632c': 'p(σ) exhibits scale invariance',\n",
       " '048d1d80-936c-43e9-827a-75079bdc5250': 'The integral of p(σ) from 0 to ∞ converges',\n",
       " 'bbe0b2f7-7b43-4e48-9ffc-55e722acc89a': 'Both a and b',\n",
       " '3cdae8ce-2fab-4f8f-b80b-8e138d6d41c0': 'Consider the transformation x ↦ cx where c is a constant. How does the scale parameter σ transform under this change of scale?',\n",
       " '08484a06-fca6-4dbb-bbe5-8ae5461ffc02': 'σ ↦ cσ',\n",
       " 'd2902bc3-5a32-49ca-9d98-c95d15feeb35': 'σ ↦ σ/c',\n",
       " 'a67edb46-000a-42d0-a2e9-4b63dd4e2b1c': 'σ remains unchanged',\n",
       " '2fcf08c7-5ac7-4057-99fd-7dd8a42cfa29': 'The transformation of σ depends on the value of c',\n",
       " '840836d2-c663-4055-a39e-90bc55f3b753': 'What were the educational backgrounds of the 15 researchers invited to attend the Snorkel workshop?',\n",
       " '9597800f-7363-471c-9dc6-a858268ed6a6': 'What prior experience did 40% of the workshop participants have with machine learning?',\n",
       " '682066a5-b67d-42aa-bf15-14d6a96df4cb': 'The paper by Kumar et al. (2020) proposes using pre-trained transformer models for what task?',\n",
       " '282d0a38-ba53-4601-b744-9d94ed690d7e': 'What semi-supervised learning method do Laine and Aila (2017) propose in their paper?',\n",
       " 'a0a3cea5-dc57-4507-a29d-a631991dea6d': 'What are some of the specialized hardware implementations that have been developed over the years to speed up training and inference of neural networks?',\n",
       " 'd62c1a72-a38a-4b08-b025-66c59fd25adc': 'True or False: Software implementations of neural networks on CPUs and GPUs typically use lower precision (less than 32 bits) to represent floating point numbers, while specialized hardware implementations often use higher precision.',\n",
       " '796e5dfa-9ce4-4265-9b14-a15a0cbe018f': 'According to the text, what is the condition for a stationary point w* to be a minimum in a one-dimensional weight space?',\n",
       " 'd6ebbf97-d7df-4d27-a7b8-c074320290e3': 'The text states that using gradient information can lead to improvements in the speed at which the minima of the error function can be located. Briefly explain why this is the case.',\n",
       " '6428f168-6ed1-4a01-9fd4-75d4c9b9b2a4': 'What are some examples of data augmentations that would generally be considered \"safe\" for an image classification task like cat vs dog but \"unsafe\" for a task like digit recognition?',\n",
       " '0e20a58b-0d31-4809-ab6e-3075fed1e827': 'The context mentions that creating refined labels for data augmented with non-label preserving transformations would allow a model to learn more robust confidence predictions. However, this is computationally expensive. Why is constructing refined labels for every unsafe data augmentation computationally challenging?',\n",
       " '4702e104-130d-4c99-a593-4ef201386f0c': 'In equation (2.72), what matrix is being minimized in the Frobenius norm?',\n",
       " 'c49c088f-709f-41bd-94cf-e51357a43f33': 'According to the text, what can we do by disregarding the constraint for the moment in equation (2.72)?',\n",
       " '26cd46e6-4d48-43a2-8b6c-a365ba5cfa01': 'If a classifier cannot correctly label even a single example in the training set, what is a likely cause according to the guidelines?',\n",
       " 'f92f2196-e0f2-4456-8c7b-1efb61f291ec': 'When implementing your own gradient computations or adding a new operation to a differentiation library, what common source of error should you check for according to the guidelines?',\n",
       " 'c6600384-5acd-40ba-88f0-20b60e503504': 'What prior distribution is used for the mixing coefficients π in the Gaussian mixture model described?',\n",
       " 'b0ae12d1-a712-4ade-bed5-d4cd2592688f': 'What form of the covariance matrix is used for the component distributions in the Gaussian mixture model, and what is the motivation for using this form?',\n",
       " 'b3c2bbce-df9d-40c4-adb8-1907e74ee621': 'What two approaches were compared to evaluate the effect on the end predictive performance of the discriminative model?',\n",
       " '9b4cd202-47d2-48dd-9752-b1f4f434419d': \"By what percentage did the discriminative model trained on Snorkel's probabilistic labels improve on average compared to the simpler pipeline approach?\",\n",
       " '0441f990-2f0b-4a4a-8410-55016b26ea55': 'What framework do Lim et al. use to reduce false positive rates in anomaly detection by oversampling rare normal samples with GANs?',\n",
       " '08d5d1ef-0b19-4b53-9f5a-45ddd2699984': 'What are two key challenges mentioned with increasing the output size of images generated by GANs that can cause training instability and non-convergence?',\n",
       " '354431f9-c43d-4286-8959-6f18ca20262f': 'What was the goal of the system developed in collaboration with Stanford researchers to extract information from EHR notes?',\n",
       " 'd5541e32-3f63-4b93-af0c-4abe75eaea58': 'What dataset was used to compare against the regular expression-based labeling developed for extracting pain levels and locations from clinician notes?',\n",
       " 'c909db0a-bb0f-449d-aeca-e7f7977ac400': 'According to the passage, what are some applications of Gaussian mixture models that are often determined by maximum likelihood using the EM algorithm?',\n",
       " '85b87f3d-da5e-492b-81a4-a5534bbdaa61': 'The passage states that the K-means algorithm corresponds to what regarding EM applied to mixtures of Gaussians?',\n",
       " '220dda98-22e3-40e4-8131-2dd92477e595': 'How is dropout boosting analogous to traditional boosting methods?',\n",
       " '44da66f7-0ba9-4e40-8bde-51e730c18fff': 'What are some examples of other stochastic approaches inspired by dropout for training ensembles of models that share weights?',\n",
       " '09b06b92-c929-4bb5-a209-b95a6e891bbb': 'What does the RNADE model do to extend NADE to handle real-valued, continuous data?',\n",
       " 'd1858b12-1603-4d19-bbdd-c9492a031def': 'The text mentions two ways in which neural auto-regressive architectures have been extended beyond modeling categorical data in a predefined order. What are these two extensions?',\n",
       " 'f249cfbe-c692-4b79-9021-0154bf96ea0a': 'What is the one-step return target for Expected Sarsa and the tree-backup return target for n ≥ 2 steps?',\n",
       " '212a0de6-168c-42f9-bf51-921a50e57e42': 'What is the update rule for the action-value function Qt+n(St, At) in the n-step Sarsa algorithm?',\n",
       " '0cdb46c4-9bde-4ef1-b48e-06abaac49dc4': 'Consider the Dirichlet distribution over M variables, with the constraint that the sum of the variables is 1. By eliminating the Mth variable and integrating over the M-1th variable, derive an expression for the normalization constant CM in terms of CM-1.',\n",
       " 'f9bba7f1-71b4-4f65-9a10-6270338419b1': 'For the Dirichlet distribution, prove that the mean and variance of the kth variable μk are given by E[μk] = αk/C and Var[μk] = (αk/C)(1 - αk/C), where C = ∑k αk.',\n",
       " 'a4426f0f-d88a-4835-ab10-9cb0c3a5d508': 'In the experiment described, how many different digit images (‘2’, ‘3’, and ‘4’) were used in the data set of N images?',\n",
       " '3634d534-3580-4112-8353-e7a89fba8789': 'What distribution was used as the prior for the parameters of the Bernoulli distributions in the mixture model?',\n",
       " 'ee2ed880-3bd7-4669-9384-f4723ae955bf': 'What are the four networks and parameter sets that need to be learned in a meta-network?',\n",
       " '2caac8a8-fb96-4f72-bebc-4825a6e27237': 'In a meta-network, what are the support and test sets used for training comprised of?',\n",
       " '7c2d8341-2df6-4d52-8879-e9f1200ad63d': 'What is the disadvantage of back-propagating through the approximate inference graph in MP-DBMs?',\n",
       " 'd45f4232-f0e9-4651-8a1e-d2111c1e3d4a': 'How does dropout relate to MP-DBMs? What do they have in common?',\n",
       " '9576d37b-3717-4aa5-9a7e-8ef44d87798f': 'What is a key difference between the frequentist and Bayesian approaches to determining parameter estimates and uncertainty?',\n",
       " '2d475100-a008-4aed-aac7-ac4be9c7e2a6': 'The context describes two approaches for estimating parameters and uncertainty. Name these two approaches.',\n",
       " '76342b61-d8a6-4353-9247-04d859d36051': 'What is the parameterization of the target model pθ in the context of the expert distribution/weights?',\n",
       " 'e58c4249-76f7-4eaa-b2fb-64ec7f225a06': 'What is the update rule for the expert distribution/weights pθ at each time step τ? Specifically, how is pθ updated in the teacher step and student step?',\n",
       " 'e97bf7bd-8b13-4bd5-93c9-60015db8977e': 'How does the choice of the prior parameter α0 affect the number of components with nonzero mixing coefficients in the Bayesian mixture of Gaussians model?',\n",
       " '2ebf0970-f66c-4542-870a-8d14cbe0a245': 'What is the computational cost associated with the variational Bayesian algorithm for Gaussian mixture models and how does it compare to the maximum likelihood EM algorithm?',\n",
       " '157aab07-1190-4831-ad60-d765fcdd45f6': 'According to the passage, chain graphs can represent a broader class of distributions than either directed or undirected graphs alone. However, the passage states there are still some distributions for which even a chain graph cannot provide a perfect map. What type of graphical model would be required to represent any possible distribution over a set of random variables?',\n",
       " '4d3feb96-b6b8-4b4c-9dc2-e6e721e8aa37': 'The passage describes an example with 3 variables A, B, and C where the conditional independences A ⊥⊥ B | ∅ and A ⊥̸⊥ B | C hold. It states there is no undirected graph over these 3 variables that implies the same set of conditional independences. Draw a directed acyclic graph over variables A, B, and C that encodes these conditional independence properties.',\n",
       " 'd9be4acb-06ec-44fc-aa22-de863477c118': \"What are the 6 previous text-generation RL algorithms that the authors' approach is compared to in the experiments?\",\n",
       " '6d964b8a-efd2-4c72-829b-e75fcea2d747': 'According to the passage, what are the 3 evaluation metrics used to evaluate the generation results of the different models?',\n",
       " 'b5273e0e-c9e0-46a9-8919-303f134989ee': 'What is the key idea behind annealed importance sampling (AIS)?',\n",
       " '2a0bf768-1575-4130-9983-5515fef7c9b7': 'Walk through the process of generating samples from a target distribution p1 using AIS given a proposal distribution po. What are the main steps?',\n",
       " 'f3f14234-43e3-472d-a3d5-2575f2cdd2bb': 'Suppose an agent is moved to a new building with a different parking lot, but still enters the highway at the same place after leaving the parking lot. Why might TD updates be better than Monte Carlo initially in learning predictions for the new parking lot and building?',\n",
       " 'a51eeb00-002b-411b-a352-6d6b5d5bdde9': 'What are two key advantages TD methods have over Monte Carlo and DP methods according to the passage?',\n",
       " 'ae0e6a19-740f-4d00-82c9-1ca5233e49d7': 'What was the key finding from the results of using Snorkel compared to hand-labeling according to the passage?',\n",
       " '41eceb4f-2ad5-442d-950d-2ddf41bce6df': 'What metrics were used to evaluate and compare the performance of models trained using Snorkel labeling functions versus hand-labeled data according to the passage?',\n",
       " 'b2999c01-f365-411f-965c-13ee02a88463': 'According to the passage, domain adaptation in sentiment analysis can arise when moving from analyzing reviews of what types of products to analyzing reviews of what other type of product?',\n",
       " 'f33d8941-c31c-4fd9-beb7-95f198aebdd7': 'The passage mentions two techniques that have been found useful for sentiment analysis with domain adaptation. What are these two techniques?',\n",
       " 'd1205e26-a860-4e9d-8390-718bde4bdc3b': 'What type of neural network is used in the study by Hwang et al. (2018) to generate synthetic medical images?',\n",
       " '3c70788a-4011-4693-bd1d-99c097a5b6ce': \"According to the excerpt from the Publisher's Note, which benefits does publishing in a SpringerOpen journal provide?\",\n",
       " '54e21982-a0b5-4683-a015-8b71247b7265': 'According to the passage, how does predictive sparse decomposition (PSD) train its encoder f(a)?',\n",
       " '5d8df1b8-6a79-472d-a990-4a73b6721529': 'What are the three terms in the PSD training objective function shown in equation 14.19? What does each term accomplish?',\n",
       " 'c66e8b28-bb5c-483c-942e-241dc0d6357e': 'According to the passage, what was the impact of removing the next sentence prediction (NSP) task during pre-training?',\n",
       " '3cb44a19-9ecc-4e10-b3c4-5bd568996c3e': 'The passage mentions that a left-to-right (LTR) model performs worse than a masked language model (MLM) on several tasks. Why does the LTR model do particularly poorly on SQuAD?',\n",
       " '5b10e6ae-47a0-403e-91f1-7d1c592269b4': 'What breakthrough around 2006 enabled the successful training of deep neural networks with fully-connected architectures?',\n",
       " '8846194d-3778-4ac4-b97e-f5a919d00715': 'What is the name of the greedy layer-wise pretraining procedure that was used to initialize deep neural networks prior to joint end-to-end training?',\n",
       " '21672390-1448-45f0-868b-cbf41d845951': 'According to the passage, why does the evidence fall when going from M=1 to M=2 in the polynomial model?',\n",
       " '32472fa8-4111-43b9-89b0-71a85e57fc2f': 'The passage states that the generalization error is roughly constant between M=3 and M=8. Based on this, what additional information helps determine that M=3 is the preferred model?',\n",
       " '803e1c6f-987b-49ae-9150-db7e17fef2a8': 'What does the contractive autoencoder (CAE) do to the singular values of the Jacobian matrix J during training?',\n",
       " '447629c7-6026-4085-b8fd-bc59e5b91f28': 'How does the contractive autoencoder (CAE) form more accurate estimates of local tangents from limited training data compared to local PCA?',\n",
       " 'ce85e052-619f-42f3-ab7e-8d3c91a94df1': 'Consider a histogram density model where the space x is divided into fixed regions. If there are N total observations with ni observations falling in region i, which has a volume ∆i, derive an expression for the maximum likelihood estimator of the density hi in region i.',\n",
       " '26848e13-0465-4d97-925d-9e167beb07cd': 'For the exponential family of distributions, show that the negative gradient of the log-partition function ln g(η) is equal to the expectation of the sufficient statistic u(x). You may do this by taking second derivatives of the log-likelihood function in (2.195).',\n",
       " 'f3596d29-66c0-4972-9d26-3c333cadb743': 'What is the approximate location of the maximum of the radial probability density p(r) for a high-dimensional Gaussian distribution? Explain your reasoning.',\n",
       " '03c64528-3edf-4174-8d7e-b3ad8ba93ce4': 'How does the probability density at the origin compare to the probability density at the radius r̅ for a high-dimensional Gaussian distribution? What does this imply about where the probability mass is concentrated?',\n",
       " 'f02f14fb-ba92-49df-9d54-23f365e4ab86': 'According to the passage, why is learning from interaction considered a foundational idea underlying theories of learning and intelligence?',\n",
       " 'eaceb45c-f592-439c-ab4b-556ab03d97ca': 'What is the perspective adopted in the book being discussed, according to the passage?',\n",
       " 'f2d697a9-49b8-4cb8-a923-3d9b350dec9a': 'What is the term used to refer to the function we want to minimize or maximize in optimization problems?',\n",
       " '8cf36754-9bf2-4b22-8c97-0c879c872d61': 'True or False: Gradient descent algorithms are guaranteed to find the global minimum or maximum of a function. Explain your answer.',\n",
       " '4555ebaa-cf94-4fa3-a88f-48dd1fad35cd': 'What kind of graph structure allows a directed model to represent some independences that an undirected model cannot?',\n",
       " '3a4bb137-1cb1-4327-a47e-8a805f342591': 'What is the name of the substructure that exists when two random variables a and b are both parents of a third random variable c, with no direct edge between a and b?',\n",
       " '014ce852-9929-4111-b7db-6fb35f16da7e': 'What are some of the limitations of the original artistic style transfer algorithm proposed by Gatys et al.?',\n",
       " 'dbf6062b-109d-4785-87f9-512624a28675': 'According to the passage, how does the neural architecture search approach proposed by Real et al. differ from the approach proposed by Zoph and Le?',\n",
       " 'b8d9046d-c7c9-4f0f-85bd-46acb0e9fdd1': 'According to the passage, how many states are reachable from the start states by any policy in the example racetrack?',\n",
       " 'fed17597-90fe-4dc4-88e9-53bbeff13b78': 'What are the two different dynamic programming algorithms compared in solving the racetrack task? What are the key differences between them according to the passage?',\n",
       " '079f805b-a7f8-4eb4-91a2-a837c08b22c9': 'What are the two approaches discussed for estimating probability density functions from data?',\n",
       " '73bcb7dd-e3a2-4a84-bee5-248b8fc02223': 'True or False: Both the K-nearest-neighbor density estimator and the kernel density estimator converge to the true probability density as N goes to infinity, provided V shrinks suitably with N and K grows with N.',\n",
       " '698a3d96-d158-4e53-bd13-0d5cf9c72304': 'True or False: When the spectral radius (largest eigenvalue of the Jacobian matrix) is greater than 1, small perturbations to the initial input can grow exponentially large over repeated applications of the backpropagation algorithm.',\n",
       " '8a6b36e2-78c6-4056-a2e3-6488ea673a4c': 'Short Answer: What is one way the effect of exploding gradients due to a large spectral radius can be mitigated when training a neural network model using backpropagation?',\n",
       " 'c809b8da-dbee-45e7-b2a7-7380bd22b84c': 'According to the context, what is the formula used to update the model parameters for a single task?',\n",
       " 'b80b6360-0986-4dab-9dc2-a9ba987f2a0a': 'The context mentions that to achieve good generalization across tasks, we want to find the optimal theta. What principle does this follow in meta-learning?',\n",
       " 'eb5a3049-b064-431e-a377-874d42f9e795': 'According to the passage, what are the two categories of observations that provide evidence for the manifold assumption?',\n",
       " '0ecf93a9-280d-45a5-ac47-f932b899faac': 'The passage states that uniform noise essentially never resembles structured inputs from domains like images, text strings, and sounds. Why does this provide evidence for the manifold assumption?',\n",
       " '2fd57d49-27c3-48f2-a302-d7358c510445': 'What is the purpose of the threshold θ in the pseudocode?',\n",
       " '06c7f615-b298-4a5c-9490-e21c0d0e2e02': 'If an action would take the agent off the grid, what happens according to the context information?',\n",
       " '1a5b2d57-d586-441e-8f24-e5d1f979ca80': 'What are the key differences between GTD(λ) and the TDC algorithm presented in Section 11.7? How does GTD(λ) incorporate eligibility traces?',\n",
       " '7820acf3-f9ca-49af-9f7f-49a52d52a6cd': 'The context describes four important eligibility trace algorithms that achieve stability under off-policy training. Name at least two of these four algorithms. What are the key ideas or components on which they are based?',\n",
       " '8558f073-e25b-4fab-921b-bb1d511d7d09': 'What training method was used to update the weights of the AlphaGo neural network during training?',\n",
       " 'a3cc40e2-9c20-4b98-9cb3-e28e13d4875a': 'How many games of self-play were used to generate training data for the neural network before each checkpoint evaluation during training?',\n",
       " '29e99dfe-1ecd-40be-af7c-054105c51b8d': 'What is the conditional distribution of x in the probabilistic PCA model?',\n",
       " '7d931f40-3b4b-4357-9551-ac692219939a': 'What iterative algorithm can be used for estimating the parameters W and σ2 in the probabilistic PCA model?',\n",
       " 'd13a54d2-71cb-49f7-953c-b7aa1e1f3985': 'What was an early exploration in applying sparsity penalties to autoencoders? How did this connect intuitively to maximum likelihood applied to undirected probabilistic models?',\n",
       " '26d87c7a-c44f-4407-ad6d-666e1c395b23': 'What is one way that was introduced to achieve actual zeros in the hidden code layer h for sparse and denoising autoencoders? Describe the method and how it enables control over the average number of zeros.',\n",
       " '5c0d68a9-f88f-4709-94cb-76bfa1de6a05': 'What journal was the paper \"Maximum likelihood estimates of linear dynamical systems\" by Rauch, Tung, and Striebel published in?',\n",
       " 'bb7a1d3b-922a-40b2-8d94-28c89d1bac44': 'Which conference proceedings contains the paper \"Learning of word stress in a sub-optimal second order backpropagation neural network\" by Ricotti, Ragazzini, and Martinelli?',\n",
       " '0fd0fc17-dc74-4b20-ae14-d3fc009be263': 'Let Pr and Pg be two probability measures on the metric space X. What is a key property that both Pr and Pg must satisfy for the Kullback-Leibler (KL) divergence between them to be well-defined?',\n",
       " '05f0d35c-2400-421a-bcf6-20f6f9b1b7b2': 'True or False: If Pr and Pg are probability measures on X such that Pr is absolutely continuous with respect to Pg, then the KL divergence DKL(Pr||Pg) is guaranteed to be finite.',\n",
       " '3942b965-6259-47a7-8baf-9a944379d312': 'What is the role of each \"expert\" in an energy-based model?',\n",
       " 'bf5d8a2a-3fb7-4046-a95d-579a98201d79': 'What is the reason for the negative sign in the energy function of energy-based models, even though it serves no functional purpose?',\n",
       " 'd77d66d8-a796-4986-a206-a11c2cf1855d': 'What are two advantages memory-based local approximation methods have over parametric methods according to the passage?',\n",
       " 'e5bfcdc8-4579-44f7-a1d6-7298ec1f46c9': 'The passage states that locally weighted regression fits a surface to the values of a set of nearest states. What error measure does it try to minimize according to the passage?',\n",
       " 'fcc8e75a-e7a4-440c-aad6-0716c37b4c90': \"According to the passage, what is different about the units in a neural network's input layer compared to other layers?\",\n",
       " 'd750970c-adfa-4d9c-a8ff-947a576b0de7': 'The passage states that a neural network with no hidden layers can only represent a small fraction of possible input-output functions. Why is adding a single hidden layer with enough sigmoid units able to approximate any continuous function?',\n",
       " '9ada1b1f-53ec-4e27-a334-55e948c34197': 'Using equations (4.20), (4.23), and (4.24), derive the expression for the Fisher criterion given in (4.26). Show your work.',\n",
       " '7d3e9923-ab29-45d2-9f5a-fe20645c186b': 'Consider a two-class generative model with Gaussian class-conditional densities. Using equations (4.57) and (4.58), derive an expression for the posterior class probability in terms of the model parameters w and w0. Verify that your result matches equations (4.65)-(4.67).',\n",
       " '94fecb13-acd8-4d3f-8d74-95e27b9fd342': 'Which author proposed using reinforcement learning for self-optimizing memory controllers?',\n",
       " '466eaa4c-0a50-4589-84ea-23d5d04fdf6c': 'What publication by Hull discusses principles of behavior and behavior systems?',\n",
       " '9133e3ad-5802-48b7-8f15-cb74851384ca': 'According to the context, what happens when we apply the Markov chain update repeatedly?',\n",
       " '23afd7d1-0409-421c-97ea-1b22c3942fa9': 'The context mentions the Perron-Frobenius theorem. What does this theorem guarantee about the largest eigenvalue when there is a nonzero probability of transitioning between any two states?',\n",
       " 'e9d3e097-e3a6-4a10-b77e-cb3a6eb25b1e': 'What is the N-pair loss function defined as in the given context?',\n",
       " 'b70702f4-1718-4fbd-b92b-c0bedd5b74cb': 'How is N-pair loss related to softmax loss for multi-class classification, according to the given information?',\n",
       " '7877d003-e8d0-448d-a70d-b46fbb65cfe4': 'What is the goal of using variance reduction methods with the REINFORCE estimator?',\n",
       " 'a32e25d5-056f-450d-8eb1-4886da57b5f6': 'How can a baseline be used to modify the REINFORCE estimator to reduce its variance while keeping the expected value unchanged?',\n",
       " '9ba2efe3-506c-4532-9d7a-c2a2b14aa685': 'Which paper discusses using policy gradient methods for reinforcement learning with function approximation?',\n",
       " '44590ef8-9aa6-442f-9927-0a0189e1872a': 'Which paper introduces a generative modeling approach based on modeling data with stochastic differential equations?',\n",
       " ...}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_queries_portion_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "421ee9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.002312898635864258,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6cafcf779714895bdf378076352b6e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_queries_portion_3, train_relevant_docs_portion_3 = generate_queries(train_data_portion_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "21ce9858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0035390853881835938,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 455,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9559cfe3ba1c476086b873f3e3a46b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/455 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_queries_portion_4, train_relevant_docs_portion_4 = generate_queries(train_data_portion_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "db03938e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0037190914154052734,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1482,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "283dfa9108f143f3ba2d0f521f93cf38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_queries,test_relevant_docs=generate_queries(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b86a2177",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_queries={**train_queries_portion_1,**train_queries_portion_2,**train_queries_portion_3,**train_queries_portion_4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "91e7a3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_queries)==len(train_queries_portion_1)+len(train_queries_portion_2)+len(train_queries_portion_3)+len(train_queries_portion_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ffc15a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_relevant_docs={**train_relevant_docs_portion_1,**train_relevant_docs_portion_2,**train_relevant_docs_portion_3,**train_relevant_docs_portion_4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7412997d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_relevant_docs)==len(train_relevant_docs_portion_1)+len(train_relevant_docs_portion_2)+len(train_relevant_docs_portion_3)+len(train_relevant_docs_portion_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d64f1a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/lichenghu/Desktop/embedding_finetune/train_queries.json\", 'w+') as f:\n",
    "    json.dump(train_queries, f)\n",
    "\n",
    "with open(\"/Users/lichenghu/Desktop/embedding_finetune/train_relevant_docs.json\", 'w+') as f:\n",
    "    json.dump(train_relevant_docs, f)\n",
    "\n",
    "with open(\"/Users/lichenghu/Desktop/embedding_finetune/val_queries.json\", 'w+') as f:\n",
    "    json.dump(test_queries, f)\n",
    "\n",
    "with open(\"/Users/lichenghu/Desktop/embedding_finetune/val_relevant_docs.json\", 'w+') as f:\n",
    "    json.dump(test_relevant_docs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b3969491",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = {\n",
    "    'queries': train_queries,\n",
    "    'docs': training_data,\n",
    "    'relevant_docs': train_relevant_docs,\n",
    "}\n",
    "\n",
    "val_dataset = {\n",
    "    'queries': test_queries,\n",
    "    'docs': test_data,\n",
    "    'relevant_docs': test_relevant_docs,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b58a7888",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/lichenghu/Desktop/embedding_finetune/train_dataset.json\", 'w+') as f:\n",
    "    json.dump(train_dataset, f)\n",
    "\n",
    "with open(\"/Users/lichenghu/Desktop/embedding_finetune/val_dataset.json\", 'w+') as f:\n",
    "    json.dump(val_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651c0665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
