{"8990730e-f6b0-4831-8ac6-6b5510ee0f93": "There Section 10.1.4 is, however, one subtlety that needs to be addressed.\n\nFor any given setting of the parameters in a Gaussian mixture model (except for speci\ufb01c degenerate settings), there will exist other parameter settings for which the density over the observed variables will be identical. These parameter values differ only through a re-labelling of the components. For instance, consider a mixture of two Gaussians and a single observed variable x, in which the parameters have the values \u03c01 = a, \u03c02 = b, \u00b51 = c, \u00b52 = d, \u03c31 = e, \u03c32 = f. Then the parameter values \u03c01 = b, \u03c02 = a, \u00b51 = d, \u00b52 = c, \u03c31 = f, \u03c32 = e, in which the two components have been exchanged, will by symmetry give rise to the same value of p(x). If we have a mixture model comprising K components, then each parameter setting will be a member of a family of K! equivalent settings", "7c6c931f-1736-4ca8-b94f-b0cc2c037656": "Chapter 19  Approximate Inference  Many probabilistic models are difficult to train because it is difficult to perform inference in them. In the context of deep learning, we usually have a set of visible variables v and a set of latent variables h. The challenge of inference usually refers to the difficult problem of computing p(h | v) or taking expectations with respect to it. Such operations are often necessary for tasks like maximum likelihood learning. Many simple graphical models with only one hidden layer, such as restricted Boltzmann machines and probabilistic PCA, are defined in a way that makes inference operations like computing p(h | v), or taking expectations with respect o it, simple. Unfortunately, most graphical models with multiple layers of hidden variables have intractable posterior distributions. Exact inference requires an exponential amount of time in these models. Even some models with only a single ayer, such as sparse coding, have this problem. In this chapter, we introduce several of the techniques for confronting these intractable inference problems. In chapter 20, we describe how to use these echniques to train probabilistic models that would otherwise be intractable, such as deep belief networks and deep Boltzmann machines", "594b4184-e5da-465e-8a56-88cb10db4c90": "This algorithm is the most data e\ufb03cient form of linear TD(0), but it is also more expensive computationally.\n\nRecall that semi-gradient TD(0) requires memory and perstep computation that is only O(d). How complex is LSTD? As it is written above the complexity seems to increase with t, but the two approximations in (9.20) could be implemented incrementally using the techniques we have covered earlier (e.g., in Chapter 2) so that they can be done in constant time per step. Even so, the update for bAt would involve an outer product (a column vector times a row vector) and thus would be a matrix update; its computational complexity would be O(d2), and of course the memory required to hold the bAt matrix would be O(d2). A potentially greater problem is that our \ufb01nal computation (9.21) uses the inverse of bAt, and the computational complexity of a general inverse computation is O(d3). Fortunately, an inverse of a matrix of our special form\u2014a sum of outer products\u2014can also be updated incrementally with only O(d2) computations, as for t > 0, with bA0 .= \"I", "ce5d5992-7e9b-4d75-8f58-04e727e74318": "The next value of z is obtained by considering a region zmin \u2a7d z \u2a7d zmax that contains z(\u03c4). It is in the choice of this region that the adaptation to the characteristic length scales of the distribution takes place. We want the region to encompass as much of the slice as possible so as to allow large moves in z space while having as little as possible of this region lying outside the slice, because this makes the sampling less ef\ufb01cient.\n\nOne approach to the choice of region involves starting with a region containing z(\u03c4) having some width w and then testing each of the end points to see if they lie within the slice. If either end point does not, then the region is extended in that direction by increments of value w until the end point lies outside the region. A candidate value z\u2032 is then chosen uniformly from this region, and if it lies within the slice, then it forms z(\u03c4+1). If it lies outside the slice, then the region is shrunk such that z\u2032 forms an end point and such that the region still contains z(\u03c4). Then another candidate point is drawn uniformly from this reduced region and so on, until a value of z is found that lies within the slice", "dfa1df4a-2ed3-4538-a6bc-11b5cbd6bc43": "MACHINE LEARNING BASICS  affine functions means that the plot of the model\u2019s predictions still looks like a line, but it need not pass through the origin. Instead of adding the bias parameter b, one can continue to use the model with only weights but augment a with an extra entry that is always set to 1. The weight corresponding to the extra 1 entry plays the role of the bias parameter. We frequently use the term \u201clinear\u201d when referring to affine functions throughout this book. The intercept term 6 is often called the bias parameter of the affine transfor- mation. This terminology derives from the point of view that the output of the transformation is biased toward being b in the absence of any input. This term is different from the idea of a statistical bias, in which a statistical estimation algorithm\u2019s expected estimate of a quantity is not equal to the true quantity. Linear regression is of course an extremely simple and limited learning algorithm, but it provides an example of how a learning algorithm can work.\n\nIn subsequent sections we describe some of the basic principles underlying learning algorithm design and demonstrate how these principles can be used to build more complicated learning algorithms", "fb2989af-f20e-4e03-bd2d-2d5241464c91": "Finally, show that without loss of generality, the set of eigenvectors can be chosen to be orthonormal, so that they satisfy (2.46), even if some of the eigenvalues are zero. 2.19 (\u22c6 \u22c6) Show that a real, symmetric matrix \u03a3 having the eigenvector equation (2.45) can be expressed as an expansion in the eigenvectors, with coef\ufb01cients given by the eigenvalues, of the form (2.48). Similarly, show that the inverse matrix \u03a3\u22121 has a representation of the form (2.49). is positive for any real value of the vector a.\n\nShow that a necessary and suf\ufb01cient condition for \u03a3 to be positive de\ufb01nite is that all of the eigenvalues \u03bbi of \u03a3, de\ufb01ned by (2.45), are positive. 2.23 (\u22c6 \u22c6) By diagonalizing the coordinate system using the eigenvector expansion (2.45), show that the volume contained within the hyperellipsoid corresponding to a constant where VD is the volume of the unit sphere in D dimensions, and the Mahalanobis distance is de\ufb01ned by (2.44). and making use of the de\ufb01nition (2.77)", "1526fa80-cb7f-40a3-aa7b-7d7963af4176": "The network\u2019s weights were updated to make the network\u2019s policy output p more closely match the policy returned by MCTS, and to make its value output, v, more closely match the probability that the current best policy wins from the board position represented by the network\u2019s input. The DeepMind team trained AlphaGo Zero over 4.9 million games of self-play, which took about 3 days. Each move of each game was selected by running MCTS for 1,600 iterations, taking approximately 0.4 second per move. Network weights were updated over 700,000 batches each consisting of 2,048 board con\ufb01gurations. They then ran tournaments with the trained AlphaGo Zero playing against the version of AlphaGo that defeated Fan Hui by 5 games to 0, and against the version that defeated Lee Sedol by 4 games to 1. They used the Elo rating system to evaluate the relative performances of the programs. The di\u21b5erence between two Elo ratings is meant to predict the outcome of games between the players.\n\nThe Elo ratings of AlphaGo Zero, the version of AlphaGo that played against Fan Hui, and the version that played against Lee Sedol were respectively 4,308, 3,144, and 3,739", "6f37ae3d-e603-46b0-8773-ddce08b6dc82": "One advantage of SVMs is that, although the training involves nonlinear optimization, the objective function is convex, and so the solution of the optimization problem is relatively straightforward. The number of basis functions in the resulting models is generally much smaller than the number of training points, although it is often still relatively large and typically increases with the size of the training set. The relevance vector machine, discussed in Section 7.2, also chooses a subset from a \ufb01xed set of basis functions and typically results in much sparser models. Unlike the SVM it also produces probabilistic outputs, although this is at the expense of a nonconvex optimization during training. An alternative approach is to \ufb01x the number of basis functions in advance but allow them to be adaptive, in other words to use parametric forms for the basis functions in which the parameter values are adapted during training. The most successful model of this type in the context of pattern recognition is the feed-forward neural network, also known as the multilayer perceptron, discussed in this chapter.\n\nIn fact, \u2018multilayer perceptron\u2019 is really a misnomer, because the model comprises multiple layers of logistic regression models (with continuous nonlinearities) rather than multiple perceptrons (with discontinuous nonlinearities)", "45d89ed2-9a97-4a9c-b545-083447c40058": "DEEP GENERATIVE MODELS  For a deep Boltzmann machine with two hidden layers, \u00a3 is given by >\u00bb WED + Loh WOW PAL \u2014log Z(0) + H(Q). (20.35) fa  This expression still contains the log partition function, log Z(@). Because a deep Boltzmann machine contains restricted Boltzmann machines as components, the hardness results for computing the partition function and sampling that apply to restricted Boltzmann machines also apply to deep Boltzmann machines.\n\nThis means that evaluating the probability mass function of a Boltzmann machine requires approximate methods such as annealed importance sampling. Likewise, training the model requires approximations to the gradient of the log partition function. See chapter 18 for a general description of these methods. DBMs are typically trained using stochastic maximum likelihood. Many of the other techniques described in  chapter 18 are not applicable. Techniques such as pseudolikelihood require the ability to evaluate the unnormalized probabilities, rather than merely obtain a  variational lower bound on them", "f53f67e7-041f-48fe-a7d6-8e39e5d419ba": "Only if the target and behavior policies are related, if they visit similar states and take similar actions, should one be able to make signi\ufb01cant progress in o\u21b5-policy training. On the other hand, any policy has many neighbors, many similar policies with considerable overlap in states visited and actions chosen, and yet which are not identical. The raison d\u2019\u02c6etre of o\u21b5-policy learning is to enable generalization to this vast number of related-but-not-identical policies. The problem remains of how to make the best use of the experience. Now that we have some methods that are stable in expected value (if the step sizes are set right), attention naturally turns to reducing the variance of the estimates.\n\nThere are many possible ideas, and we can just touch on a few of them in this introductory text. Why is controlling variance especially critical in o\u21b5-policy methods based on importance sampling? As we have seen, importance sampling often involves products of policy ratios. The ratios are always one in expectation (5.13), but their actual values may be very high or as low as zero. Successive ratios are uncorrelated, so their products are also always one in expected value, but they can be of very high variance", "f7fdb36d-64d9-4365-87f3-fd9ad9f49088": "The constraint that the n outputs must sum to 1 means that only 1 parameters are necessary; the probability of the n-th value may be obtained, by subtracting the  https://www.deeplearningbook.org/contents/mlp.html    first 2 \u2014 1 probabilities from 1. We can thus impose a requirement that one element of 2 be fixed. For example, we can require that, 2\u201d = 0. Indeed, this is exactly what the sigmoid unit does. Defining Ply \u2014 =l|a)= = (2) i is equivalent to defining  P(y=1 | x) = softmax(z), with a two- \u2018hamensiova zand z% =0. Both the n\u2014 1 argument and the n argument approaches to the softmax can describe the same set of probability distributions but have different learning dynamics. In practice, there is rarely much difference between using the overparametrized version or the restricted version, and it is simpler to implement the overparametrized version", "f947c650-ea88-48ff-a1e4-e46dc20fb662": "The di\u21b5erential return (10.9) is not well de\ufb01ned for this case as the limit does not exist. To repair this, one could alternately de\ufb01ne the value of a state as Exercise 10.7 Consider a Markov reward process consisting of a ring of three states A, B, and C, with state transitions going deterministically around the ring. A reward of +1 is received upon arrival in A and otherwise the reward is 0. What are the di\u21b5erential values of the three states using (10.13)? \u21e4 Exercise 10.8 The pseudocode in the box on page 251 updates \u00afRt using \u03b4t as an error rather than simply Rt+1 \u2212 \u00afRt. Both errors work, but using \u03b4t is better. To see why, consider the ring MRP of three states from Exercise 10.7. The estimate of the average reward should tend towards its true value of 1 held stuck there", "2e0ab83f-dfcf-4bc6-bc18-3d249c254422": "E\ufb03cient implementation relies on the fact that the k-step \u03bb-return can be written exactly as Exercise 12.5 Several times in this book (often in exercises) we have established that returns can be written as sums of TD errors if the value function is held constant. Why is (12.10) another instance of this? Prove (12.10). \u21e4 Choosing the truncation parameter n in truncated TD(\u03bb) involves a tradeo\u21b5. n should be large so that the method closely approximates the o\u270fine \u03bb-return algorithm, but it should also be small so that the updates can be made sooner and can in\ufb02uence behavior sooner. Can we get the best of both? Well, yes, in principle we can, albeit at the cost of computational complexity. The idea is that, on each time step as you gather a new increment of data, you go back and redo all the updates since the beginning of the current episode.\n\nThe new updates will be better than the ones you previously made because now they can take into account the time step\u2019s new data. That is, the updates are always towards an n-step truncated \u03bb-return target, but they always use the latest horizon", "c37d7a90-0a63-438e-b9a4-945dab201d98": "Cross Entropy and Kullback\u2013Leibler (KL) Divergence.\n\nThe diverse algorithms discussed in Section 4 have all been based on the cross entropy as the divergence function in SE, namely, A nice advantage of using the cross entropy is the close-form solution of q in the teacher-student procedure, as shown in Equation 3.3, which makes the optimization and analysis easier. In the case where the uncertainty function is the Shannon entropy H(q) = \u2212Eq (as is commonly assumed in this article), one could alternatively see the above algorithms as using the KL divergence for D, by noticing that KL(q, p\u03b8) = Eq\u2212Eq. That is, given speci\ufb01c balancing weights (\u03b10, \u03b20), the divergence and uncertainty terms in SE can be rearranged as: where the KL divergence term corresponds to D in SE if we see \u03b1 = \u03b10 \u2212 \u03b20 and \u03b2 = \u03b20. 2(q+p\u03b8) is the mean distribution", "5d2727e1-c21b-42c2-9ebe-2aeba2fb8ad2": "For example, the US generally must begin after the onset of a neutral stimulus for conditioning to occur, with the rate and e\u21b5ectiveness of learning depending on the inter-stimulus interval, or ISI, the interval between the onsets of the CS and the US. When CRs appear, they generally begin before the appearance of the US and their temporal pro\ufb01les change during learning.\n\nIn conditioning with compound CSs, the component stimuli of the compound CSs may not all begin and end at the same time, sometimes forming what is called a serial compound in which the component stimuli occur in a sequence over time. Timing considerations like these make it important to consider how stimuli are represented, how these representations unfold over time during and between trials, and how they interact with discounting and eligibility traces. the behavior of the TD model: the complete serial compound (CSC), the microstimulus (MS), and the presence representations . These representations di\u21b5er in the degree to which they force generalization among nearby time points during which a stimulus is present. The simplest of the representations shown in Figure 14.1 is the presence representation in the \ufb01gure\u2019s right column", "4473d162-63ee-4cbc-8a82-193a86ed92c4": "For K classes, the DAGSVM has a total of K(K \u2212 1)/2 classi\ufb01ers, and to classify a new test point only K \u2212 1 pairwise classi\ufb01ers need to be evaluated, with the particular classi\ufb01ers used depending on which path through the graph is traversed.\n\nA different approach to multiclass classi\ufb01cation, based on error-correcting output codes, was developed by Dietterich and Bakiri  and applied to support vector machines by Allwein et al. This can be viewed as a generalization of the voting scheme of the one-versus-one approach in which more general partitions of the classes are used to train the individual classi\ufb01ers. The K classes themselves are represented as particular sets of responses from the two-class classi\ufb01ers chosen, and together with a suitable decoding scheme, this gives robustness to errors and to ambiguity in the outputs of the individual classi\ufb01ers. Although the application of SVMs to multiclass classi\ufb01cation problems remains an open issue, in practice the one-versus-the-rest approach is the most widely used in spite of its ad-hoc formulation and its practical limitations. There are also single-class support vector machines, which solve an unsupervised learning problem related to probability density estimation", "917e5fba-5285-4f2a-aaf9-548be0b5fc6c": "Because the Gibbs sampling is performed in a deep graphical model, this similarity is based more on semantic than raw visual features, but it is still difficult for the Gibbs chain to transition from one mode of the distribution to another, for example, by changing the digit identity. (Right)Consecutive ancestral samples from a generative adversarial network. Because ancestral sampling  https://www.deeplearningbook.org/contents/monte_carlo.html   generates each sample independently from the others, there is no mixing problem. 600  CHAPTER 17. MONTE CARLO METHODS  deterministic. When the temperature rises to infinity, and 6 falls to zero, the distribution (for discrete x) becomes uniform. Typically, a model is trained to be evaluated at 6 = 1. However, we can make use of other temperatures, particularly those where 3 < 1. Tempering is a general strategy of mixing between modes of p, rapidly by drawing samples with 8 < 1. Markov chains based on tempered transitions  temporarily sample from higher-temperature distributions to mix to different modes, then resume sampling from the unit temperature distribution", "c18cf274-53ef-4b38-ae19-032ee1f1693f": "So far in this book almost all the methods have been action-value methods; they learned the values of actions and then selected actions based on their estimated action values1; their policies would not even exist without the action-value estimates. In this chapter we consider methods that instead learn a parameterized policy that can select actions without consulting a value function.\n\nA value function may still be used to learn the policy parameter, but is not required for action selection. We use the notation \u2713 2 Rd0 for the policy\u2019s parameter vector. Thus we write \u21e1(a|s, \u2713) = Pr{At =a | St =s, \u2713t =\u2713} for the probability that action a is taken at time t given that the environment is in state s at time t with parameter \u2713. If a method uses a learned value function as well, then the value function\u2019s weight vector is denoted w 2 Rd In this chapter we consider methods for learning the policy parameter based on the gradient of some scalar performance measure J(\u2713) with respect to the policy parameter. These methods seek to maximize performance, so their updates approximate gradient ascent in J: of the performance measure with respect to its argument \u2713t", "af8f35e0-d60c-46f6-af02-b9cdf80edc4e": "Next, we present several of the concrete challenges that make optimization of neural networks difficult.\n\nWe then define several practical algorithms, including both optimization algorithms themselves and strategies for initializing the parameters. More advanced algorithms adapt their learning rates during training or leverage information contained in  https://www.deeplearningbook.org/contents/optimization.html    271  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  the second derivatives of the cost function. Finally, we conclude with a review of several optimization strategies that are formed by combining simple optimization algorithms into higher-level procedures. 8.1 How Learning Differs from Pure Optimization  Optimization algorithms used for training of deep models differ from traditional optimization algorithms in several ways. Machine learning usually acts indirectly. In most machine learning scenarios, we care about some performance measure P, that is defined with respect to the test set and may also be intractable. We therefore optimize P only indirectly. We reduce a different cost function J(@) in the hope that doing so will improve P. This is in contrast to pure optimization, where minimizing J is a goal in and of itself", "8db1bb19-760a-4e1f-be8d-ef295a1502f9": "A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., Hassabis, D. Humanlevel control through deep reinforcement learning. Nature, 518:529\u2013533. Modayil, J., Sutton, R. S. Prediction driven behavior: Learning predictions that drive Modayil, J., White, A., Sutton, R. S. .\n\nMulti-timescale nexting in a reinforcement learning Monahan, G. E. State of the art\u2014a survey of partially observable Markov decision processes: theory, models, and algorithms. Management Science, 28(1):1\u201316. Montague, P. R., Dayan, P., Nowlan, S", "00003785-54ce-4b39-888e-454bdb4388b1": "The normalization condition for the coefficients ai is obtained by requiring that the eigenvectors in feature space be normalized. Using (12.76) and (12.80), we have Having solved the eigenvector problem, the resulting principal component projections can then also be cast in terms of the kernel function so that, using (12.76), the projection of a point x onto eigenvector i is given by and so again is expressed in terms of the kernel function. In the original D-dimensional x space there are D orthogonal eigenvectors and hence we can find at most D linear principal components.\n\nThe dimensionality M of the feature space, however, can be much larger than D (even infinite), and thus we can find a number of nonlinear principal components that can exceed D. Note, however, that the number of nonzero eigenvalues cannot exceed the number N of data points, because (even if M > N) the covariance matrix in feature space has rank at most equal to N. This is reflected in the fact that kernel PCA involves the eigenvector expansion of the N x N matrix K", "45362b31-92f0-4841-a0d0-b58281712c4c": "It enables us to draw from the latest RL advances, such as path consistency learning, to combine the best of on-/off-policy updates, and learn effectively from sparse reward. We apply the approach to a wide range of novel text generation tasks, including learning from noisy/negative examples, adversarial attacks, and prompt generation.\n\nExperiments show our approach consistently outperforms both task-specialized algorithms and the previous RL methods.1 Recent natural language generation systems have made remarkable progress in producing wellformed text, especially with massive pretrained language models. Those models are typically trained using maximum likelihood estimation (MLE) with a large amount of data supervisions. Despite its successes, the standard training method suffers from limited applicability to many emerging text generation problems, where little or no supervised data is available. Prominent examples of such low-data problems include generating prompts to control the massive LMs , learning text generation from noisy or even negative data, generating adversarial text attacks for robustness study , and others (Figure 1, right). Due to the failure of standard MLE, people have had to devise specialized algorithms for those problems respectively", "d2e471f0-71ff-4f09-8fd6-88bc21306600": "For our GLUE submission, we always predicted the ma14Note that we only report single-task \ufb01ne-tuning results in this paper. A multitask \ufb01ne-tuning approach could potentially push the performance even further. For example, we did observe substantial improvements on RTE from multitask training with MNLI. 15https://gluebenchmark.com/faq 1. Question: Does BERT really need such a large amount of pre-training (128,000 words/batch * 1,000,000 steps) to achieve high \ufb01ne-tuning accuracy? Answer: Yes, BERTBASE achieves almost 1.0% additional accuracy on MNLI when trained on 1M steps compared to 500k steps. 2. Question: Does MLM pre-training converge slower than LTR pre-training, since only 15% of words are predicted in each batch rather than every word? Answer: The MLM model does converge slightly slower than the LTR model. However, in terms of absolute accuracy the MLM model begins to outperform the LTR model almost immediately. In Section 3.1, we mention that BERT uses a mixed strategy for masking the target tokens when pre-training with the masked language model (MLM) objective", "3ad0dab1-e21b-4112-b0bf-367ed361ebc6": "Chapter 3  Probability and Information Theory  In this chapter, we describe probability theory and information theory. Probability theory is a mathematical framework for representing uncertain statements. It provides a means of quantifying uncertainty as well as axioms for deriving new uncertain statements. In artificial intelligence applications, we use probability theory in two major ways. First, the laws of probability tell us how AI systems should reason, so we design our algorithms to compute or approximate various expressions derived using probability theory. Second, we can use probability and statistics to theoretically analyze the behavior of proposed AI systems. Probability theory is a fundamental tool of many disciplines of science and engineering. We provide this chapter to ensure that readers whose background is primarily in software engineering, with limited exposure to probability theory, can understand the material in this book. While probability theory allows us to make uncertain statements and to reason in the presence of uncertainty, information theory enables us to quantify the amount of uncertainty in a probability distribution", "f8e80582-a95b-49ac-ae79-182aa95b77e0": "A fundamental difficulty with such local nonparametric approaches to manifold learning is raised in Bengio and Monperrus : if the manifolds are not very smooth (they have many peaks and troughs and twists), one may need a very large number of training examples to cover each one of these variations, with no chance to generalize to unseen variations. Indeed, these methods can only generalize the shape of the manifold by interpolating between neighboring examples. Unfortunately, the manifolds involved in AI problems can have very complicated structures that can be difficult to capture from only local interpolation.\n\nConsider for example the manifold resulting from translation shown in figure 14.6. If we watch just one coordinate within the input vector, 2;, as the image is translated, we will observe that one coordinate encounters a peak or a trough in its value once for every peak or trough in brightness in the image. In other words, the complexity of the patterns of brightness in an underlying image template drives the complexity of the manifolds that are generated by performing simple image  https://www.deeplearningbook.org/contents/autoencoders.html    transformations. This motivates the use of distributed representations and deep learning for capturing manifold structure", "69d4a253-f0ea-4eb7-8932-26e33c2b2de5": "Among tested transfer tasks,  https://lilianweng.github.io/posts/2021-05-31-contrastive/     Contrastive Representation Learning | Lil'Log   CLIP struggles with very fine-grained classification, as well as abstract or systematic tasks such as counting the number of objects. The transfer performance of CLIP models is smoothly correlated with the amount of model compute. Supervised Contrastive Learning  There are several known issues with cross entropy loss, such as the lack of robustness to noisy labels and the possibility of poor margins. Existing improvement for cross entropy loss involves the curation of better training data, such as label smoothing and data augmentation", "416ce8f9-77cd-41d8-9624-8caf65cf326a": "Such a point must have the property that the vector \u2207f(x) is also orthogonal to the constraint surface, as illustrated in Figure E.1, because otherwise we could increase the value of f(x) by moving a short distance along the constraint surface. Thus \u2207f and \u2207g are parallel (or anti-parallel) vectors, and so there must exist a parameter \u03bb such that where \u03bb \u0338= 0 is known as a Lagrange multiplier. Note that \u03bb can have either sign. At this point, it is convenient to introduce the Lagrangian function de\ufb01ned by The constrained stationarity condition (E.3) is obtained by setting \u2207xL = 0. Furthermore, the condition \u2202L/\u2202\u03bb = 0 leads to the constraint equation g(x) = 0.\n\nThus to \ufb01nd the maximum of a function f(x) subject to the constraint g(x) = 0, we de\ufb01ne the Lagrangian function given by (E.4) and we then \ufb01nd the stationary point of L(x, \u03bb) with respect to both x and \u03bb. For a D-dimensional vector x, this gives D +1 equations that determine both the stationary point x\u22c6 and the value of \u03bb", "6906bacf-5c2c-404f-8255-35c65b9b9cd2": "Both kinds of graphical models use a graph G in which each node in the graph corresponds to a random variable, and an edge connecting two random variables means that the probability distribution is able to represent direct interactions between those two random variables. Directed models use graphs with directed edges, and they represent fac- torizations into conditional probability distributions, as in the example above.\n\nSpecifically, a directed model contains one factor for every random variable x; in the distribution, and that factor consists of the conditional distribution over x; given the parents of x;, denoted Pag(x,):  p(x) = oc | Pag(x:)) - (3.53)  See figure 3.7 for an example of a directed graph and the factorization of probability distributions it represents. Undirected models use graphs with undirected edges, and they represent factorizations into a set of functions; unlike in the directed case, these functions are usually not probability distributions of any kind. Any set of nodes that are all connected to each other in G is called a clique. Each clique C\u00ae in an undirected model is associated with a factor \u00a2(C)", "d481f4b9-9a79-4591-a6c9-d5cbe7a95026": "In each training episode, the truth label y; is presented with one step offset, (Xt41, Yt): it is the true label for the input at the previous time step t, but presented as part of the input at time step t+1. https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Log   External Memory External Memory  Class Prediction 2 | c be ---\u2014> 68) seen > | fe > : > Backpropagated  * 4 Shuffle: * 4 * * Signal (Xi, Yer)(Kev1,y) Labels (1,0) (x2,%) Xi Xp Xpand Classes  i=l where M; is the memory matrix at time t and M;(2) is the i-th row in this matrix.\n\n\u00bb How to write into memory? The addressing mechanism for writing newly received information into memory operates a lot like the cache replacement policy", "5e44c072-f37f-4905-9af8-3992df7b0e8e": "This can be a difficult and subtle task. Often, we cannot actually evaluate the log-probability of the data under the model, but can evaluate only an approximation. In these cases, it is important to think and communicate clearly about what exactly is being measured. For example, suppose we can evaluate a stochastic estimate of the log-likelihood for model A, and a deterministic lower bound on the log-likelihood for model B. If model A gets a higher score than model B, which is better? If we care about determining which model has a better internal representation of the distribution, we actually cannot tell, unless we have some way of determining how  loose the bound for model B is. However, if we care about how well we can use he model in practice, for example to perform anomaly detection, then it is fair to say that a model is preferable based on a criterion specific to the practical task of interest, for example, based on ranking test examples and ranking criteria such as precision and recall. Another subtlety of evaluating generative models is that the evaluation metrics are often hard research problems in and of themselves", "a592bd5c-615f-45d3-8d34-d8e5f86a1329": "In Chapter 7, however, we distinguished n-step Expected Sarsa from n-step Tree Backup, where the latter retained the property of not using importance sampling. It remains then to present the eligibility trace version of Tree Backup, which we will call Tree-Backup(\u03bb), or TB(\u03bb) for short. This is arguably the true successor to Q-learning because it retains its appealing absence of importance sampling even though it can be applied to o\u21b5-policy data. The concept of TB(\u03bb) is straightforward. As shown in its backup diagram in Figure 12.13, the tree-backup updates of each length (from Section 7.5) are weighted in the usual way dependent on the bootstrapping parameter \u03bb", "7c4e53b1-a351-4537-89c7-db3142562d9b": "The conjugate prior for \u00b5 is the Gaussian, the conjugate prior for \u039b is the Wishart, and the conjugate prior for (\u00b5, \u039b) is the Gaussian-Wishart.\n\nIf we have a marginal Gaussian distribution for x and a conditional Gaussian distribution for y given x in the form then the marginal distribution of y, and the conditional distribution of x given y, are given by If we have a joint Gaussian distribution N(x|\u00b5, \u03a3) with \u039b \u2261 \u03a3\u22121 and we de\ufb01ne the following partitions then the conditional distribution p(xa|xb) is given by and the marginal distribution p(xa) is given by This is the conjugate prior distribution for a univariate Gaussian N(x|\u00b5, \u03bb\u22121) in which the mean \u00b5 and the precision \u03bb are both unknown and is also called the normal-gamma distribution. It comprises the product of a Gaussian distribution for \u00b5, whose precision is proportional to \u03bb, and a gamma distribution over \u03bb", "0796c277-469d-4bf9-9a63-8664cb19d970": "The decision surface, shown in red, is perpendicular to w, and its displacement from the origin is controlled by the bias parameter w0. Also, the signed orthogonal distance of a general point x from the decision surface is given by y(x)/\u2225w\u2225. an arbitrary point x and let x\u22a5 be its orthogonal projection onto the decision surface, so that Multiplying both sides of this result by wT and adding w0, and making use of y(x) = wTx + w0 and y(x\u22a5) = wTx\u22a5 + w0 = 0, we have This result is illustrated in Figure 4.1. As with the linear regression models in Chapter 3, it is sometimes convenient to use a more compact notation in which we introduce an additional dummy \u2018input\u2019 value x0 = 1 and then de\ufb01ne \ufffdw = (w0, w) and \ufffdx = (x0, x) so that In this case, the decision surfaces are D-dimensional hyperplanes passing through the origin of the D + 1-dimensional expanded input space. Now consider the extension of linear discriminants to K > 2 classes", "494d81a3-6574-4cd4-9eb6-9a170e37f928": "F. Skinner called shaping in which reward contingencies are progressively altered to train an animal to successively approximate a desired behavior. Shaping is not only indispensable for animal training, it is also an e\u21b5ective tool for training reinforcement learning agents.\n\nThere is also a connection to the idea of an animal\u2019s motivational state, which in\ufb02uences what an animal will approach or avoid and what events are rewarding or punishing for the animal. The reinforcement learning algorithms presented in this book include two basic mechanisms for addressing the problem of delayed reinforcement: eligibility traces and value functions learned via TD algorithms. Both mechanisms have antecedents in theories of animal learning. Eligibility traces are similar to stimulus traces of early theories, and value functions correspond to the role of secondary reinforcement in providing nearly immediate evaluative feedback. The next correspondence the chapter addressed is that between reinforcement learning\u2019s environment models and what psychologists call cognitive maps. Experiments conducted in the mid 20th century purported to demonstrate the ability of animals to learn cognitive maps as alternatives to, or as additions to, state\u2013action associations, and later use them to guide behavior, especially when the environment changes unexpectedly", "7eb1e5b0-9030-482c-a69f-98fc3f97d903": "Other views of learning under delayed reinforcement invoke roles for awareness and working memory .\n\n14.5 Thistlethwaite  provides an extensive review of latent learning experiments up to the time of its publication. Ljung  provides an overview of model learning, or system identi\ufb01cation, techniques in engineering. Gopnik, Glymour, Sobel, Schulz, Kushnir, and Danks  present a Bayesian theory about how children learn models. 14.6 Connections between habitual and goal-directed behavior and model-free and model-based reinforcement learning were \ufb01rst proposed by Daw, Niv, and Dayan . The hypothetical maze task used to explain habitual and goal-directed behavioral control is based on the explanation of Niv, Joel, and Dayan . Dolan and Dayan  review four generations of experimental research related to this issue and discuss how it can move forward on the basis of reinforcement learning\u2019s model-free/model-based distinction. Dickinson  and Dickinson and Balleine  discuss experimental evidence related to this distinction. Donahoe and Burgos  alternatively argue that model-free processes can account for the results of outcome-devaluation experiments. Dayan and Berridge  argue that classical conditioning involves model-based processes", "6621c53d-f7d7-4ede-9b0e-058ef66310aa": "Often, we consider functionals de\ufb01ned by integrals whose integrands take the form G(y, x) and that do not depend on the derivatives of y(x). In this case, stationarity simply requires that \u2202G/\u2202y(x) = 0 for all values of x. If we are optimizing a functional with respect to a probability distribution, then we need to maintain the normalization constraint on the probabilities. This is often most conveniently done using a Lagrange multiplier, which then allows an unconAppendix E strained optimization to be performed. The extension of the above results to a multidimensional variable x is straightforward. For a more comprehensive discussion of the calculus of variations, see Sagan .\n\nLagrange multipliers, also sometimes called undetermined multipliers, are used to \ufb01nd the stationary points of a function of several variables subject to one or more constraints", "4c2a767d-71d4-4dc1-8070-1ed2d5718376": "This is plausible given that many\u2014though not all\u2014 drugs of abuse increase levels of dopamine either directly or indirectly in regions around terminals of dopamine neuron axons in the striatum, a brain structure \ufb01rmly implicated in normal reward-based learning (Section 15.7). But the self-destructive behavior associated with drug addiction is not characteristic of normal learning. What is di\u21b5erent about dopamine-mediated learning when the reward is the result of an addictive drug? Is addiction the result of normal learning in response to substances that were largely unavailable throughout our evolutionary history, so that evolution could not select against their damaging e\u21b5ects? Or do addictive substances somehow interfere with normal dopamine-mediated learning? The reward prediction error hypothesis of dopamine neuron activity and its connection to TD learning are the basis of a model due to Redish  of some\u2014but certainly not all\u2014features of addiction.\n\nThe model is based on the observation that administration of cocaine and some other addictive drugs produces a transient increase in dopamine. In the model, this dopamine surge is assumed to increase the TD error, \u03b4, in a way that cannot be cancelled out by changes in the value function", "95c81d0a-78b6-4502-af35-aee17316cb0a": "Authors\u2019 contributions  CS performed the primary literature review and analysis for this work, and also drafted the manuscript. TMK, JLL, RAB, RZ, KW, NS, and RK worked with CS to develop the article's framework and focus. TMK introduced this topic to CS, and helped to complete and finalize this work. All authors read and approved the final manuscript. Funding Not applicable. Availability of data and materials Not applicable. Competing interests The authors declare that they have no competing interests. Consent for publication Not applicable. Ethics approval and consent to participate Not applicable. Received: 9 January 2019 Accepted: 22 April 2019 Published online: 06 July 2019  References 1. Krizhevsky A, Sutskever |, Hinton GE. ImageNet classification with deep convolutional neural networks. Adv Neural Inf Process Syst. 2012;25:1 106-14. 2. Karen S, Andrew Z. Very deep convolutional networks for large-scale image recognition. arXiv e-prints. 2014. 3", "21641d4d-c8a5-41d6-9e3a-6857d1053f8c": "An important limitation of these early works is that they did not treat the o\u21b5-policy case with function approximation. Intra-option learning in general requires o\u21b5-policy learning, which could not be done reliably with function approximation at that time. Although now we have a variety of stable o\u21b5-policy learning methods using function approximation, their combination with option ideas had not been significantly explored at the time of publication of this book. Barto and Mahadevan  and Hengst  review the options formalism and other approaches to temporal abstraction.\n\nUsing GVFs to implement option models has not previously been described. Our presentation uses the trick introduced by Modayil, White and Sutton  for predicting signals at the termination of policies. The extension of options and option models to the average-reward setting has not yet been developed in the literature. 17.3 A good presentation of the POMDP approach is given by Monahan . PSRs and tests were introduced by Littman, Sutton and Singh . OOMs were introduced by Jaeger . Sequential Systems, which unify PSRs, OOMs, and many other works, were introduced in the PhD thesis of Michael Thon", "d152b01c-0c8e-479a-982a-de9363c4ecc9": "Initialize the time step t = 1 While T <= T_MAX:  Reset gradient: d@ = 0 and dw = 0.\n\nSynchronize thread-specific parameters with global ones: 8' = 8 and w' = w.  tstart = t and get s;. While (s\u00a2 #4 TERMINAL) and (\u00a2 \u2014 tstart <= tax):  Pick the action ay ~ 7(a;|sz; 6\u2019) and receive a new reward r; and a new state $4, ,. Update t=t+1andT=T+1. Initialize the variable that holds the return estimation  R={0 if s,is TERMINAL V(s,;w\u2019) otherwise  Fori =t\u20141,..-,tstart! R\u00abvr, + 7R; here R is a MC measure of G;. Accumulate gradients w.r.t. 6': dO \u2014 d\u00e9 + Vo log n(a;\\s;; 0\u2019)(R \u2014 V(s;; w\u2019)); Accumulate gradients w.r.t", "93079a0d-018b-463e-90e7-9952a8565258": "However, we see from Figure 5.21(c) that the model is able to produce a conditional density that is unimodal for some values of x and trimodal for other values by modulating the amplitudes of the mixing components \u03c0k(x). Once a mixture density network has been trained, it can predict the conditional density function of the target data for any given value of the input vector. This conditional density represents a complete description of the generator of the data, so far as the problem of predicting the value of the output vector is concerned.\n\nFrom this density function we can calculate more speci\ufb01c quantities that may be of interest in different applications. One of the simplest of these is the mean, corresponding to the conditional average of the target data, and is given by where we have used (5.148). Because a standard network trained by least squares is approximating the conditional mean, we see that a mixture density network can reproduce the conventional least-squares result as a special case. Of course, as we have already noted, for a multimodal distribution the conditional mean is of limited value. We can similarly evaluate the variance of the density function about the conditional average, to give Exercise 5.37 where we have used (5.148) and (5.158)", "d608d9a5-bf51-4e5a-a139-09142a1816dc": "maximize similarity between augmented and unaugmented copies of the same image, we apply data augmentation symmetrically to both branches of our framework (Figure 2). We also apply a nonlinear projection on the output of base feature network, and use the representation before projection network, whereas Ye et al. use the linearly projected \ufb01nal hidden vector as the representation. When training with large batch sizes using multiple accelerators, we use global BN to avoid shortcuts that can greatly decrease representation quality.", "1667019a-203f-4110-a966-e6c761618978": "Note that the effect of marginalization is to spread out the contours and to make the predictions less con\ufb01dent, so that at each input point x, the posterior probabilities are shifted towards 0.5, while the y = 0.5 contour itself is unaffected. The convolution of a Gaussian with a logistic sigmoid is intractable. We therefore apply the approximation (4.153) to (5.189) giving where \u03ba(\u00b7) is de\ufb01ned by (4.154). Recall that both \u03c32 \ufb01cation data set described in Appendix A.\n\n5.1 (\u22c6 \u22c6) Consider a two-layer network function of the form (5.7) in which the hiddenunit nonlinear activation functions g(\u00b7) are given by logistic sigmoid functions of the form Show that there exists an equivalent network, which computes exactly the same function, but with hidden unit activation functions given by tanh(a) where the tanh function is de\ufb01ned by (5.59). Hint: \ufb01rst \ufb01nd the relation between \u03c3(a) and tanh(a), and then show that the parameters of the two networks differ by linear transformations", "05a11d38-b49b-4641-9043-ce8b3a3ca139": "Ahhough the data space comprises 12 measuremenlS, a data set of points will lie close to a Iwo-dimensional manifold embedded within this space. In this case, the manifold comprises scveral distinct segments corresponding to different flow regimes. each such segment being a (noisy) continuous two-dimensional manifold. If our goal is data compression. or density modelling, then there can be benefits in exploiling this manifold struclUre. In praclice. the data points will not be confined precisely to a smooth lowdimensional manifold, and we can interpret the departures of data points from the manifold as \u00b7noise'.\n\nThis leads naturally to a generative view of such models in which we first select a poinl within the manifold according to some latent variable distribution and then generate an observed data point by :ldding noise, drawn from some conditional distribution of the data varillbles given the latent varillbles", "3aa3d0bb-7edd-495e-a988-187119f09530": "By basing a regression model on a heavy-tailed distribution such as a t-distribution, we obtain a more robust model.\n\nIf we go back to (2.158) and substitute the alternative parameters \u03bd = 2a, \u03bb = a/b, and \u03b7 = \u03c4b/a, we see that the t-distribution can be written in the form We can then generalize this to a multivariate Gaussian N(x|\u00b5, \u039b) to obtain the corresponding multivariate Student\u2019s t-distribution in the form with corresponding results for the univariate case. Although Gaussian distributions are of great practical signi\ufb01cance, both in their own right and as building blocks for more complex probabilistic models, there are situations in which they are inappropriate as density models for continuous variables. One important case, which arises in practical applications, is that of periodic variables. An example of a periodic variable would be the wind direction at a particular geographical location. We might, for instance, measure values of wind direction on a number of days and wish to summarize this using a parametric distribution. Another example is calendar time, where we may be interested in modelling quantities that are believed to be periodic over 24 hours or over an annual cycle", "3a171e47-94de-49f4-a3e4-14c393a6d9ec": "A major drawback to kernel machines is that the cost of evaluating the decision function is linear in the number of training examples, because the i-th example contributes a term a;k(a, al)) to the decision function. Support vector machines are able to mitigate this by learning an @ vector that contains mostly zeros. Classifying a new example then requires evaluating the kernel function only for the training examples that have nonzero a;. These training examples are known as support vectors. Kernel machines also suffer from a high computational cost of training when the dataset is large. We revisit this idea in section 5.9. Kernel machines with  140  CHAPTER 5. MACHINE LEARNING BASICS  generic kernels struggle to generalize well. We explain why in section 5.11. The modern incarnation of deep learning was designed to overcome these limitations of kernel machines.\n\nThe current deep learning renaissance began when Hinton e\u00a2 al. demonstrated that a neural network could outperform the RBF kernel SVM on the MNIST benchmark", "dfd676a4-2fa9-4133-9240-1860b9ae0f72": "In particular, model innovations like the LSTM, rectified linear units and maxout units have all moved toward using more linear functions than previous models like deep networks based on sigmoidal units. These models have nice properties that make optimization easier. The gradient flows through many layers provided that the Jacobian of the linear transformation has reasonable singular values. Moreover, linear functions consistently increase in a single direction, so even if the model\u2019s output is very far from correct, it is clear simply from computing the gradient which direction its output should move to reduce the loss function.\n\nIn other words, modern neural nets have been designed so that their local gradient information corresponds reasonably well to moving toward a distant solution. Other model design strategies can help to make optimization easier. For example, linear paths or skip connections between layers reduce the length of he shortest path from the lower layer\u2019s parameters to the output, and thus mitigate the vanishing gradient problem . A related idea o skip connections is adding extra copies of the output that are attached to the intermediate hidden layers of the network, as in GoogLeNet  and deeply supervised nets . These \u201cauxiliary heads\u201d are trained o perform the same task as the primary output at the top of the network to ensure that the lower layers receive a large gradient", "bd774cb6-75dc-4d57-83d4-3663ab91705c": "Clearly this is arti\ufb01cial. A more general way to think of an earliest reward-predicting state is that it is an unpredicted predictor of reward, and there can be many such states. In an animal\u2019s life, many di\u21b5erent states may precede an earliest reward-predicting state. However, because these states are more often followed by other states that do not predict reward, their reward-predicting powers, that is, their values, remain low. A TD algorithm, if operating throughout the animal\u2019s life, would update the values of these states too, but the updates would not consistently accumulate because, by assumption, none of these states reliably precedes an earliest reward-predicting state. If any of them did, they would be reward-predicting states as well. This might explain why with overtraining, dopamine responses decrease to even the earliest reward-predicting stimulus in a trial", "8f687df8-879b-4f5a-a1cf-10daf07bf999": "Thus x1 is a good predictor of t, x2 is a more noisy predictor of t, and x3 has only chance correlations with t. The marginal likelihood for a Gaussian process with ARD parameters \u03b71, \u03b72, \u03b73 is optimized using the scaled conjugate gradients algorithm. We see from Figure 6.10 that \u03b71 converges to a relatively large value, \u03b72 converges to a much smaller value, and \u03b73 becomes very small indicating that x3 is irrelevant for predicting t. The ARD framework is easily incorporated into the exponential-quadratic kernel (6.63) to give the following form of kernel function, which has been found useful for applications of Gaussian processes to a range of regression problems where D is the dimensionality of the input space. In a probabilistic approach to classi\ufb01cation, our goal is to model the posterior probabilities of the target variable for a new input vector, given a set of training data. These probabilities must lie in the interval (0, 1), whereas a Gaussian process model makes predictions that lie on the entire real axis", "4d178e19-9602-460c-aa0c-3c9066819e29": "These represent error signals \u03b4 for each pattern and for each output unit, and can be backpropagated to the hidden units and the error function derivatives evaluated in the usual way. Because the error function (5.153) is composed of a sum of terms, one for each training data point, we can consider the derivatives for a particular pattern n and then \ufb01nd the derivatives of E by summing over all patterns.\n\nBecause we are dealing with mixture distributions, it is convenient to view the mixing coef\ufb01cients \u03c0k(x) as x-dependent prior probabilities and to introduce the corresponding posterior probabilities given by Similarly, the derivatives with respect to the output activations controlling the component means are given by Exercise 5.35 Finally, the derivatives with respect to the output activations controlling the component variances are given by Exercise 5.36 We illustrate the use of a mixture density network by returning to the toy example of an inverse problem shown in Figure 5.19. Plots of the mixing coef\ufb01cients \u03c0k(x), the means \u00b5k(x), and the conditional density contours corresponding to p(t|x), are shown in Figure 5.21. The outputs of the neural network, and hence the parameters in the mixture model, are necessarily continuous single-valued functions of the input variables", "b8e5a54e-fc0f-4446-be70-0a3441164e19": "For example, we may have a predictor f(x;@) that we wish to employ to predict the mean of y. If we use a sufficiently powerful neural network, we can think of the neural network as being able to represent any function f from a wide class of functions, with this class being limited only by features such as continuity and boundedness  175  https://www.deeplearningbook.org/contents/mlp.html    CHAPTER 6 DEEP FEEDFORWARD NETWORKS  rather than by having a specific parametric form. From this point of view, we can view the cost function as being a functional rather than just a function. A functional is a mapping from functions to real numbers. We can thus think of earning as choosing a function rather than merely choosing a set of parameters.\n\nWe can design our cost functional to have its minimum occur at some specific function we desire. For example, we can design the cost functional to have its minimum lie on the function that maps a to the expected value of y given a. Solving an optimization problem with respect to a function requires a mathematical ool called calculus of variations, described in section 19.4.2", "adb7a7bc-2198-49e6-a338-4d7239cdf221": "So if c = (0, 0)>, the feature is constant over both dimensions; if c = (c1, 0)> the feature is constant over the second dimension and varies over the \ufb01rst with frequency depending on c1; and similarly, for c = (0, c2)>. When c = (c1, c2)> with neither cj = 0, the feature varies along both dimensions and represents an interaction between the two state variables. The values of c1 and c2 determine the frequency along each dimension, and their ratio gives the direction of the interaction. When using Fourier cosine features with a learning algorithm such as (9.7), semigradient TD(0), or semi-gradient Sarsa, it may be helpful to use a di\u21b5erent step-size parameter for each feature.\n\nIf \u21b5 is the basic step-size parameter, then Konidaris, Osentoski, and Thomas  suggest setting the step-size parameter for feature xi to \u21b5i = \u21b5/ Fourier cosine features with Sarsa can produce good performance compared to several other collections of basis functions, including polynomial and radial basis functions", "e94d44e6-33e6-4c15-9024-c3852be4559b": "A deep RNN  has state variables from several layers at each time step, giving the unfolded graph two kinds of depth: ordinary depth due to a stack of layers, and depth due to time unfolding. This work brought the phoneme error rate on TIMIT to a record low of 17.7 percent. See Pascanu ef al. and Chung ef al. for other variants of deep RNNs, applied in other settings. Another contemporary step toward end-to-end deep learning ASR is to let the system learn how to \u201calign\u201d the acoustic-level information with the phonetic-level information . 455  CHAPTER 12. APPLICATIONS  12.4 Natural Language Processing  Natural language processing (NLP) is the use of human languages, such as English or French, by a computer. Computer programs typically read and emit specialized languages designed to allow efficient and unambiguous parsing by simple programs.\n\nMore naturally occurring languages are often ambiguous and defy formal description. Natural language processing includes applications such as machine translation, in which the learner must read a sentence in one human language and emit an equivalent sentence in another human language", "79462613-9454-49b4-9fa1-351245cd517c": "AAAI Conference on Arti\ufb01cial Intelligence, 8, 1433\u20131438.", "7eb449e2-2a26-433f-828f-a4775fdf55d6": "Both types of multimodality were encountered in Chapter 9 in the context of Gaussian mixtures, where they manifested themselves as multiple maxima in the likelihood function, and a variational treatment based on the minimization of KL(q\u2225p) will tend to \ufb01nd one of these modes.\n\nBy contrast, if we were to minimize KL(p\u2225q), the resulting approximations would average across all of the modes and, in the context of the mixture model, would lead to poor predictive distributions (because the average of two good parameter values is typically itself not a good parameter value). It is possible to make use of KL(p\u2225q) to de\ufb01ne a useful inference procedure, but this requires a rather different approach to the one discussed here, and will be considered in detail when we discuss expectation propagation. Section 10.7 The two forms of Kullback-Leibler divergence are members of the alpha family of divergences  de\ufb01ned by where \u2212\u221e < \u03b1 < \u221e is a continuous parameter. The Kullback-Leibler divergence KL(p\u2225q) corresponds to the limit \u03b1 \u2192 1, whereas KL(q\u2225p) corresponds to the limit \u03b1 \u2192 \u22121", "30047d89-5f0e-4c5f-a3f1-dd1b7e6a9dc8": "The backward view of the \u03bb-return algorithm is also straightforward, using separate eligibility traces for the actor and critic, each after the patterns in Chapter 12. Pseudocode for the complete algorithm is given in the box below", "92ca6458-43e1-4ab3-adc3-b85a8789a996": "(8.25)  i=1  J(9) = Exy~ toate (wy) lE(F (as 8), 9] =  The methods we discuss here extend readily, however, to more general objective functions, such as those that include parameter regularization terms, as discussed in chapter 7. 8.6.1 Newton\u2019s Method  In section 4.3, we introduced second-order gradient methods. In contrast to first- order methods, second-order methods make use of second derivatives to improve optimization. The most widely used second-order method is Newton\u2019s method. We now describe Newton\u2019s method in more detail, with emphasis on its application to neural network training. Newton\u2019s method is an optimization scheme based on using a second-order Tay- lor series expansion to approximate J(@) near some point 00, ignoring derivatives of higher order:  J(0) ~ (00) + (9 ~ 80)\" Vo (8) + 5(0~ 00)\" H(O\u2014 0), (8.26)  where H is the Hessian of J with respect to @ evaluated at 09. If we then solve for the critical point of this function, we obtain the Newton parameter update rule:  0* = 0) \u2014 HV J(0,)", "a851044c-524a-4c7a-a79a-e460c4b0dc5a": "In this case, however, these policies are not just better, but optimal, proceeding to the terminal states in the minimum number of steps. In this example, policy iteration would \ufb01nd the optimal policy after just one iteration. Exercise 4.4 The policy iteration algorithm on page 80 has a subtle bug in that it may never terminate if the policy continually switches between two or more policies that are equally good. This is ok for pedagogy, but not for actual use. Modify the pseudocode so that convergence is guaranteed. \u21e4 Exercise 4.5 How would policy iteration be de\ufb01ned for action values? Give a complete algorithm for computing q\u21e4, analogous to that on page 80 for computing v\u21e4. Please pay special attention to this exercise, because the ideas involved will be used throughout the rest of the book. \u21e4 meaning that the probability of selecting each action in each state, s, is at least \"/|A(s)|. Describe qualitatively the changes that would be required in each of the steps 3, 2, and 1, in that order, of the policy iteration algorithm for v\u21e4 on page 80", "63728cad-bbae-4c12-b9c8-b95509402a8a": "5.38 (\u22c6) Using the general result (2.115), derive the predictive distribution (5.172) for the Laplace approximation to the Bayesian neural network model. 5.39 (\u22c6) www Make use of the Laplace approximation result (4.135) to show that the evidence function for the hyperparameters \u03b1 and \u03b2 in the Bayesian neural network model can be approximated by (5.175). 5.40 (\u22c6) www Outline the modi\ufb01cations needed to the framework for Bayesian neural networks, discussed in Section 5.7.3, to handle multiclass problems using networks having softmax output-unit activation functions.\n\n5.41 (\u22c6 \u22c6) By following analogous steps to those given in Section 5.7.1 for regression networks, derive the result (5.183) for the marginal likelihood in the case of a network having a cross-entropy error function and logistic-sigmoid output-unit activation function. In Chapters 3 and 4, we considered linear parametric models for regression and classi\ufb01cation in which the form of the mapping y(x, w) from input x to output y is governed by a vector w of adaptive parameters", "157e2d0b-261d-4825-8bff-aeef4deb70ed": "In some cases when the auxiliary q is assumed to have a certain form (e.g., a deep network), the approximate E-step in Equation 2.12 may still be too complex to be tractable, or the gradient estimator (w.r.t.\n\nthe parameters of q) can su\ufb00er from high variance . To tackle the challenge, more approximations are introduced. The wake-sleep algorithm  is one of such methods. In the E-step w.r.t. q, rather than minimizing KL(q(y)\u2225p\u03b8(y|x\u2217)) (Equation 2.9) as in EM and variational EM, the wakesleep algorithm makes an approximation by minimizing the Kullback\u2013Leibler (KL) divergence in which can be optimized e\ufb03ciently with gradient descent when q is parameterized. Besides wakesleep, one can also use other methods for low-variance gradient estimation in Equation 2.12, such as reparameterization gradient  and score gradient . In sum, the entropy maximization perspective has formulated unsupervised MLE as an optimizationtheoretic framework that permits simple alternating minimization solvers", "ce114ad2-e737-4c29-8f7e-9b37cc965986": "Obtaining this goal was rewarding to them and reinforced the actions allowing them to escape. It is di\ufb03cult to link the concept of motivation, which has many dimensions, in a precise way to reinforcement learning\u2019s computational perspective, but there are clear links with some of its dimensions. In one sense, a reinforcement learning agent\u2019s reward signal is at the base of its motivation: the agent is motivated to maximize the total reward it receives over the long run. A key facet of motivation, then, is what makes an agent\u2019s experience rewarding. In reinforcement learning, reward signals depend on the state of the reinforcement learning agent\u2019s environment and the agent\u2019s actions. Further, as pointed out in Chapter 1, the state of the agent\u2019s environment not only includes information about what is external to the machine, like an organism or a robot, that houses the agent, but also what is internal to this machine. Some internal state components correspond to what psychologists call an animal\u2019s motivational state, which in\ufb02uences what is rewarding to the animal. For example, an animal will be more rewarded by eating when it is hungry than when it has just \ufb01nished a satisfying meal", "a2d04503-8d83-4cf3-a782-a5e5a86b6824": "The distribution \u00b5 in the VE (9.1) is then de\ufb01ned as the distribution of states encountered while following the target policy, weighted by the interest. Second, we introduce another non-negative scalar random variable, the emphasis Mt. This scalar multiplies the learning update and thus emphasizes or de-emphasizes the learning done at time t. The general n-step learning rule, replacing (9.15), is with Mt .= 0, for all t < 0. These equations are taken to include the Monte Carlo case, for which Gt:t+n = Gt, all the updates are made at end of the episode, n = T \u2212 t, and Mt = It. Example 9.4 illustrates how interest and emphasis can result in more accurate value To see the potential bene\ufb01ts of using interest and emphasis, consider the four-state Markov reward process shown below: Episodes start in the leftmost state, then transition one state to the right, with a reward of +1, on each step until the terminal state is reached.\n\nThe true value of the \ufb01rst state is thus 4, of the second state 3, and so on as shown below each state", "55e7ff70-bd43-42e6-8788-f1995a39dfa4": "These two error functions are compared in Figure 14.4. There are various simple, but widely used, models that work by partitioning the input space into cuboid regions, whose edges are aligned with the axes, and then assigning a simple model (for example, a constant) to each region.\n\nThey can be viewed as a model combination method in which only one model is responsible for making predictions at any given point in input space. The process of selecting a speci\ufb01c model, given a new input x, can be described by a sequential decision making process corresponding to the traversal of a binary tree (one that splits into two branches at each node). Here we focus on a particular tree-based framework called classi\ufb01cation and regression trees, or CART , although there are many other variants going by such names as ID3 and C4.5 . space, along with the corresponding tree structure. In this example, the \ufb01rst step divides the whole of the input space into two regions according to whether x1 \u2a7d \u03b81 or x1 > \u03b81 where \u03b81 is a parameter of the model. This creates two subregions, each of which can then be subdivided independently", "10d4c4f6-7f1d-43a1-b7cb-e69069e05dab": "In its original form, this applies to problems for which the conditional distributions are Gaussian, which represents a more general class of distributions than the multivariate Gaussian because, for example, the non-Gaussian distribution p(z, y) \u221d exp(\u2212z2y2) has Gaussian conditional distributions. At each step of the Gibbs sampling algorithm, the conditional distribution for a particular component zi has some mean \u00b5i and some variance \u03c32 work, the value of zi is replaced with where \u03bd is a Gaussian random variable with zero mean and unit variance, and \u03b1 is a parameter such that \u22121 < \u03b1 < 1.\n\nFor \u03b1 = 0, the method is equivalent to standard Gibbs sampling, and for \u03b1 < 0 the step is biased to the opposite side of the mean. This step leaves the desired distribution invariant because if zi has mean \u00b5i and variance \u03c32 i. The effect of over-relaxation is to encourage directed motion through state space when the variables are highly correlated. The framework of ordered over-relaxation  generalizes this approach to nonGaussian distributions", "7326fb57-9f53-4da3-882d-452fe2bdaa4d": "Suppose, however, that it is impractical to sample directly from p(z) but that we can evaluate p(z) easily for any given value of z.\n\nOne simplistic strategy for evaluating expectations would be to discretize z-space into a uniform grid and to evaluate the integrand as a sum of the form An obvious problem with this approach is that the number of terms in the summation grows exponentially with the dimensionality of z. Furthermore, as we have already noted, the kinds of probability distributions of interest will often have much of their mass con\ufb01ned to relatively small regions of z space and so uniform sampling will be very inef\ufb01cient because in high-dimensional problems, only a very small proportion of the samples will make a signi\ufb01cant contribution to the sum. We would really like to choose the sample points to fall in regions where p(z) is large, or ideally where the product p(z)f(z) is large. As in the case of rejection sampling, importance sampling is based on the use of a proposal distribution q(z) from which it is easy to draw samples, as illustrated in Figure 11.8", "38e40825-d112-4157-a66c-579a13ddf523": "Dauphin et al. introduced a saddle-free Newton method for second-order optimization and showed that it improves significantly  https://www.deeplearningbook.org/contents/optimization.html   over the traditional version. Second-order methods remain difficult to scale to large  neural networks, but this saddle-free approach holds promise if it can be scaled. 284  CHAPTER 8.\n\nOPTIMIZATION FOR TRAINING DEEP MODELS  J(w,b)  Figure 8.3: The objective function for highly nonlinear deep neural networks or for recurrent neural networks often contains sharp nonlinearities in parameter space resulting from the multiplication of several parameters. These nonlinearities give rise to very high derivatives in some places. When the parameters get close to such a cliff region, a gradient descent update can catapult the parameters very far, possibly losing most of the optimization work that has been done. Figure adapted with permission from Pascanu  et al. There are other kinds of points with zero gradient besides minima and saddle points. Maxima are much like saddle points from the perspective of optimization\u2014 many algorithms are not attracted to them, but unmodified Newton\u2019s method is", "794e70ed-0944-46d7-9555-191a7a67024e": "With various teams at the Stanford School of Medicine, we have worked to extend the cross-modal radiology application described in Sect. 4 to a range of other similar cross-modal medical problems, which has involved building robust interfaces for various multi-modal clinical data and formats . In collaboration with several teams at Google, we recently developed a new version of Snorkel, Snorkel DryBell, to interface with Google\u2019s organizational weak supervision resources and compute infrastructure, and enable weak supervision at industrial scale . Another focus has been extending Snorkel to handle richly-formatted data, de\ufb01ned in  as data with multi-modal, semi-structured components such as PDF forms, tables, and HTML pages. Supportforthisrichbutchallengingdatatypehasbeenimplemented in a system built on top of Snorkel, Fonduer , which has been applied to domains such as anti-human traf\ufb01cking efforts via DARPA\u2019s MEMEX project and extraction of genome-wide association (GWA) studies from the scienti\ufb01c literature . The goal of Snorkel is to enable users to program the modern machine learning stack, by labeling training data with labeling functions rather than manual annotation", "ca79fe00-146d-4f59-9d52-a7a796490944": "We have seen that the joint probability of two independent events is given by the product of the marginal probabilities for each event separately.\n\nBecause our data set x is i.i.d., we can therefore write the probability of the data set, given \u00b5 and \u03c32, in the form When viewed as a function of \u00b5 and \u03c32, this is the likelihood function for the Gaussian and is interpreted diagrammatically in Figure 1.14. One common criterion for determining the parameters in a probability distribution using an observed data set is to \ufb01nd the parameter values that maximize the likelihood function. This might seem like a strange criterion because, from our foregoing discussion of probability theory, it would seem more natural to maximize the probability of the parameters given the data, not the probability of the data given the parameters. In fact, these two criteria are related, as we shall discuss in the context of curve \ufb01tting. Section 1.2.5 For the moment, however, we shall determine values for the unknown parameters \u00b5 and \u03c32 in the Gaussian by maximizing the likelihood function (1.53). In practice, it is more convenient to maximize the log of the likelihood function", "36f04b7f-ec42-473a-8631-ba3703a8c625": "They applied this successfully to model sequences (protein secondary structure) and introduced a (one-dimensional) convolutional structure in the transition operator of the Markov chain. It is important to remember that, for each step of the Markov chain, one generates a new sequence for each layer, and that sequence is the input for computing other layer values (say the one below and the one above) at the next time step.\n\nHence, the Markov chain is really over the output variable (and associated higher-level hidden layers), and the input sequence only serves to condition that chain, with back-propagation enabling it to learn how the input sequence can condition the output distribution implicitly represented by the Markov chain. It is therefore a case of using the GSN in the context of structured outputs. Zohrer and Pernkopf  introduced a hybrid model that combines a su- pervised objective (as in the above work) and an unsupervised objective (as in the original GSN work) by simply adding (with a different weight) the supervised and unsupervised costs, that is, the reconstruction log-probabilities of y and x  711  CHAPTER 20. DEEP GENERATIVE MODELS  respectively", "027772b8-6947-4e7d-847b-3274819c58d3": "lhe .ill\"\"\"\"\"l\"'\" equalions lell' U, thaI Y, !-ali,fies.\" s L <bC\".) {<b(x.lTv,} - )\"Y, .., Substituting this expansion back into the eigenvector equation, we obtain The key step is now to express this in terms of the kernel function k(xn , xm ) = \u00a2(Xn)T\u00a2(xm ), which we do by multiplying both sides by \u00a2(xZ)T to give This can be written in matrix notation as where ai is an N-dimensional column vector with elements ani for n = 1, ... ,N. We can find solutions for ai by solving the following eigenvalue problem in which we have removed a factor of K from both sides of (12.79). Note that the solutions of (12.79) and (12.80) differ only by eigenvectors of K having zero eigenvalues that do not affect the principal components projection", "8f631beb-7d6d-4846-88fb-cf963d134379": "We train our model with Cloud TPUs, using 32 to 128 cores depending on the batch size.2 Global BN. Standard ResNets use batch normalization . In distributed training with data parallelism, the BN mean and variance are typically aggregated locally per device. In our contrastive learning, as positive pairs are computed in the same device, the model can exploit the local information leakage to improve prediction accuracy without improving representations. We address this issue by aggregating BN mean and variance over all devices during the training. Other approaches include shuf\ufb02ing data examples across devices , or replacing BN with layer norm . 2With 128 TPU v3 cores, it takes \u223c1.5 hours to train our ResNet-50 with a batch size of 4096 for 100 epochs. Here we lay out the protocol for our empirical studies, which aim to understand different design choices in our framework. Dataset and Metrics. Most of our study for unsupervised pretraining (learning encoder network f without labels) is done using the ImageNet ILSVRC-2012 dataset .\n\nSome additional pretraining experiments on CIFAR-10  can be found in Appendix B.9", "5b9aeb38-ad73-4408-abaa-12e14dddc788": "In certain important special cases, this computation is tractable and leads directly to optimal solutions, although it does require complete knowledge of the prior distribution of possible problems, which we generally assume is not available.\n\nIn addition, neither the theory nor the computational tractability of this approach appear to generalize to the full reinforcement learning problem that we consider in the rest of the book. The Gittins-index approach is an instance of Bayesian methods, which assume a known initial distribution over the action values and then update the distribution exactly after each step (assuming that the true action values are stationary). In general, the update computations can be very complex, but for certain special distributions (called conjugate priors) they are easy. One possibility is to then select actions at each step according to their posterior probability of being the best action. This method, sometimes called posterior sampling or Thompson sampling, often performs similarly to the best of the distribution-free methods we have presented in this chapter. In the Bayesian setting it is even conceivable to compute the optimal balance between exploration and exploitation. One can compute for any possible action the probability of each possible immediate reward and the resultant posterior distributions over action values. This evolving distribution becomes the information state of the problem", "ae883bcb-f08c-4399-ae67-fd88b87c70c3": "Following , we aim to control the generation to have one of 7 topics (e.g., \u201cscience\u201d); the generated prompt is prepended to one of 20 input sentences (Figure 5) for the pretrained LM to generate continuation sentences. There is no direct supervision data available for training the prompt generator. We randomly create some noisy text as the training data for MLE baselines below and for off-policy updates for our algorithm.\n\nSpeci\ufb01cally, the noisy text is created by sampling keywords and topics from the list used in  and a paraphrase generation model. A.3.1 Comparison with MLE Objective It is interesting to take a closer look at the above objective and compare with the common MLE training. Speci\ufb01cally, we notice the relations between the optimal Q\u2217, V \u2217, and A\u2217 functions: A\u2217 (st, at) = Q\u2217(st, at) \u2212 V \u2217(st) = rt + \u03b3V \u2217(st+1) \u2212 V \u2217 (st), where the \ufb01rst equation is the de\ufb01nition of A\u2217 (see Eq.5) and the second equation is due to Eqs. (10) and (5)", "b00ca20e-4ed4-426b-afdc-7a4646051ba1": "Q-values as Generation Model Logits. We show the connection of the Q-values with the logits, i.e., outputs right before the softmax layer. Concretely, with the SQL objective, the following relationship between optimal policy \u03c0\u2217 and action-value Q\u2217 holds : This form is highly reminiscent of the softmax layer of the generation model in Eq.(1). The con3WLOG, we can assume \u03b1=1, as it can be folded into the reward function by scaling the latter with 1/\u03b1. nection suggests that we can naturally parameterize the Q-function in SQL as the generation model logit function, i.e., Q\u03b8(s, a) \u2261 f\u03b8(a|s). In other words, the model output f\u03b8(a|s), originally interpretted as the \u201clogit\u201d of token a given the preceding tokens s, is now re-interpretted as the Q-value of action a in state s. When achieving optimality, f\u03b8\u2217(a|s), namely Q\u2217(s, a), represents the best possible future reward achievable by generating token a in state s", "490af8bb-2fea-4162-9f44-349fd8ccf91d": "We now explore three approaches to learning the parameters of linear discriminant functions, based on least squares, Fisher\u2019s linear discriminant, and the perceptron algorithm. In Chapter 3, we considered models that were linear functions of the parameters, and we saw that the minimization of a sum-of-squares error function led to a simple closed-form solution for the parameter values. It is therefore tempting to see if we can apply the same formalism to classi\ufb01cation problems. Consider a general classi\ufb01cation problem with K classes, with a 1-of-K binary coding scheme for the target vector t. One justi\ufb01cation for using least squares in such a context is that it approximates the conditional expectation E of the target values given the input vector. For the binary coding scheme, this conditional expectation is given by the vector of posterior class probabilities. Unfortunately, however, these probabilities are typically approximated rather poorly, indeed the approximations can have values outside the range (0, 1), due to the limited \ufb02exibility of a linear model as we shall see shortly. Each class Ck is described by its own linear model so that where k = 1, . .\n\n, K", "cd26b833-52f2-46fa-9df1-e4d8e2242184": "In the same paper, the authors propose deep versions of the architecture, but unfortunately that immediately makes computation as expensive as in the original neural auto-regressive network .\n\nThe first layer and the output layer can still be computed in O(nh) multiply-add operations, as in the regular NADE, where h is the number of hidden units (the size of the groups h;, in figures 20.10 and 20.9), whereas it is O(n?h) in Bengio and Bengio . For the other hidden layers, however, the computation is O(n? h?) if every \u201cprevious\u201d group at layer | participates in predicting the \u201cnext\u201d group at layer ]+ 1, assuming n groups of h, hidden units at each layer. Making the 7-th group at layer /+ 1 only depend on the i-th group, as in Uria et al. , at layer | reduces it to O(nh? ),  https://www.deeplearningbook.org/contents/generative_models.html    which 1s still A times worse than the regular NADE. 20.11 Drawing Samples from Autoencoders  In chapter 14, we saw that many kinds of autoencoders learn the data distribution", "e2247ab8-605e-4987-b253-23f1c917193a": "The reduction in variance is greater if the variance in the posterior mean is greater. Note, however, that this result only holds on average, and that for a particular observed data set it is possible for the posterior variance to be larger than the prior variance. Binary variables can be used to describe quantities that can take one of two possible values. Often, however, we encounter discrete variables that can take on one of K possible mutually exclusive states.\n\nAlthough there are various alternative ways to express such variables, we shall see shortly that a particularly convenient representation is the 1-of-K scheme in which the variable is represented by a K-dimensional vector x in which one of the elements xk equals 1, and all remaining elements equal 0. So, for instance if we have a variable that can take K = 6 states and a particular observation of the variable happens to correspond to the state where x3 = 1, then x will be represented by x = (0, 0, 1, 0, 0, 0)T. (2.25) Note that such vectors satisfy \ufffdK k=1 xk = 1. If we denote the probability of xk = 1 by the parameter \u00b5k, then the distribution of x is given where \u00b5 = (\u00b51,", "edf7c46e-98e0-486d-9ac1-682a0418aa8b": " https://lilianweng.github.io/posts/2021-05-31-contrastive/  Contrastive Representation Learning | Lil'Log  St tt tt tt te He  image_encoder - ResNet or Vision Transformer text_encoder - CBOW or Text Transformer  I - minibatch of aligned images T - learned proj of image to embed W_t - learned proj of text to embed t - learned temperature parameter  xtract feature representations of each modality  # e I_f = image_encoder(I) # T_f = text_encoder(T) #  # joint multimodal embedding  np.linalg.norm(np.dot(I_f, W_i), Wt),  axis=1) axis=1)  # scaled pairwise cosine similarities   logits = np.dot(I_e, T_e.T) * np.exp(t)  # symmetric loss function  labels = np.arange(n)  loss_i = cross_entropy_loss(logits, labels, axis=@) loss_t = cross_entropy_loss(logits, labels, axis=1) loss = (loss_i + loss_t)/2  Compared to other methods above for learning good visual representation, what makes CLIP really special is \u201cthe appreciation of using natural language as a training signal\u201d", "a684a92f-04d3-468a-adc5-ea72e0b8448a": "They therefore have the same  702  CHAPTER 20.\n\nDEEP GENERATIVE MODELS  Figure 20.8: A fully visible belief network predicts the i-th variable from the i \u2014 1 previous ones. (Top)The directed graphical model for an FVBN. (Bottom)Corresponding computational graph for the logistic FVBN, where each prediction is made by a linear predictor. advantages and disadvantages as linear classifiers. Like linear classifiers, they may be trained with convex loss functions and sometimes admit closed form solutions (as in the Gaussian case). Like linear classifiers, the model itself does not offer a way of increasing its capacity, so capacity must be raised using techniques like basis expansions of the input or the kernel trick", "6cd5b572-4870-43af-a417-8a78eb8082d3": "The geometric transformations stud- ied were flipping, \u2014 30\u00b0 to 30\u00b0 rotations, and cropping. The color space transformations studied were color jittering, (random color manipulation), edge enhancement, and PCA. They tested these augmentations with 4-fold cross-validation on the Caltech101 dataset filtered to 8421 images of size 256 x 256 (Table 1). Kernel filters  Kernel filters are a very popular technique in image processing to sharpen and blur images. These filters work by sliding an n x n matrix across an image with either a Gauss- ian blur filter, which will result in a blurrier image, or a high contrast vertical or hori- zontal edge filter which will result in a sharper image along edges.\n\nIntuitively, blurring images for Data Augmentation could lead to higher resistance to motion blur during testing. Additionally, sharpening images for Data Augmentation could result in encapsu- lating more details about objects of interest. Sharpening and blurring are some of the classical ways of applying kernel filters to images. Kang et al", "ed934a3c-dc06-4223-a145-51c8c2cd2e6f": "Triplet loss learns to minimize the distance between the anchor x and positive x* and maximize the distance between the anchor x and negative x~ at the same time with the following equation:  Leripler(x,*, x) = SP max (0, || f(x) \u2014 f&*)I3 \u2014 F() \u2014 FOO )II3 + \u20ac)  xer  where the margin parameter \u20ac is configured as the minimum offset between distances of similar vs dissimilar pairs. It is crucial to select challenging x~ to truly improve the model", "a614b423-ceba-4dda-8ffa-87fb3cf70a0c": "This type of policy can result in more clicks by a user over repeated visits to the site, and if the policy is suitably designed, more eventual sales. Working at Adobe Systems Incorporated, Theocharous et al. conducted experiments to see if policies designed to maximize clicks over the long term could in fact improve over short-term greedy policies. The Adobe Marketing Cloud, a set of tools that many companies use to run digital marketing campaigns, provides infrastructure for automating user-targeted advertising and fund-raising campaigns. Actually deploying novel policies using these tools entails signi\ufb01cant risk because a new policy may end up performing poorly. For this reason, the research team needed to assess what a policy\u2019s performance would be if it were to be actually deployed, but to do so on the basis of data collected under the execution of other policies. A critical aspect of this research, then, was o\u21b5-policy evaluation. Further, the team wanted to do this with high con\ufb01dence to reduce the risk of deploying a new policy. Although high con\ufb01dence o\u21b5-policy evaluation was a central component of this research , here we focus only on the algorithms and their results.\n\nTheocharous et al", "0bc648b0-78ef-4b39-8f49-0f7ddd0de857": "For a given \ufb01nite data set, it is possible for the Bayes factor to be larger for the incorrect model.\n\nHowever, if we average the Bayes factor over the distribution of data sets, we obtain the expected Bayes factor in the form where the average has been taken with respect to the true distribution of the data. This quantity is an example of the Kullback-Leibler divergence and satis\ufb01es the propSection 1.6.1 erty of always being positive unless the two distributions are equal in which case it is zero. Thus on average the Bayes factor will always favour the correct model. We have seen that the Bayesian framework avoids the problem of over-\ufb01tting and allows models to be compared on the basis of the training data alone. However, a Bayesian approach, like any approach to pattern recognition, needs to make assumptions about the form of the model, and if these are invalid then the results can be misleading. In particular, we see from Figure 3.12 that the model evidence can be sensitive to many aspects of the prior, such as the behaviour in the tails", "c14b0ab4-6c74-4f18-a6cc-ca3a2624dfaf": "As proved on  we know that MMD is a proper metric and not only a pseudometric when the kernel is universal.\n\nIn the speci\ufb01c case where H = L2(X, m) for m the normalized Lebesgue measure on X, we know that {f \u2208 Cb(X), \u2225f\u2225\u221e \u2264 1} will be contained in F, and therefore dF(Pr, P\u03b8) \u2264 \u03b4(Pr, P\u03b8) so the regularity of the MMD distance as a loss function will be at least as bad as the one of the total variation. Nevertheless this is a very extreme case, since we would need a very powerful kernel to approximate the whole L2. However, even Gaussian kernels are able to detect tiny noise patterns as recently evidenced by . This points to the fact that especially with low bandwidth kernels, the distance might be close to a saturating regime similar as with total variation or the JS. This obviously doesn\u2019t need to be the case for every kernel, and \ufb01guring out how and which di\ufb00erent MMDs are closer to Wasserstein or total variation distances is an interesting topic of research", "09637815-5c2c-4e24-8a5b-38088213bc89": "The states might be represented only by feature vectors, which may do little to distinguish the states from each other. As a special case, all of the feature vectors may be the same. Thus one really has only the reward sequence (and the actions), and performance has to be assessed purely from these. How could it be done? One way is by averaging the rewards over a long interval\u2014this is the idea of the average-reward setting. How could discounting be used? Well, for each time step we could measure the discounted return. Some returns would be small and some big, so again we would have to average them over a su\ufb03ciently large time interval. In the continuing setting there are no starts and ends, and no special time steps, so there is nothing else that could be done. However, if you do this, it turns out that the average of the discounted returns is proportional to the average reward. In fact, for policy \u21e1, the average of the discounted returns is always r(\u21e1)/(1 \u2212 \u03b3), that is, it is essentially the average reward, r(\u21e1).\n\nIn particular, the ordering of all policies in the average discounted return setting would be exactly the same as in the average-reward setting", "aa070f83-bf3b-4c83-b3e6-ab7e10e626d3": "The eigenvalues \u03bbi measure the curvature of the likelihood function, and so in Figure 3.15 the eigenvalue \u03bb1 is small compared with \u03bb2 (because a smaller curvature corresponds to a greater elongation of the contours of the likelihood function). Because \u03b2\u03a6T\u03a6 is a positive de\ufb01nite matrix, it will have positive eigenvalues, and so the ratio \u03bbi/(\u03bbi + \u03b1) will lie between 0 and 1. Consequently, the quantity \u03b3 de\ufb01ned by (3.91) will lie in the range 0 \u2a7d \u03b3 \u2a7d M. For directions in which \u03bbi \u226b \u03b1, the corresponding parameter wi will be close to its maximum likelihood value, and the ratio \u03bbi/(\u03bbi + \u03b1) will be close to 1. Such parameters are called well determined because their values are tightly constrained by the data. Conversely, for directions in which \u03bbi \u226a \u03b1, the corresponding parameters wi will be close to zero, as will the ratios \u03bbi/(\u03bbi +\u03b1). These are directions in which the likelihood function is relatively insensitive to the parameter value and so the parameter has been set to a small value by the prior", "1b41f4e5-1592-4236-be33-ef7a0cfd7449": "The only difference is how the numbers are ar- ranged in a grid to form a tensor.\n\nWe could imagine flattening each tensor into a vector before we run back-propagation, computing a vector-valued gradient, and then reshaping the gradient back into a tensor. In this rearranged view, back-propagation is still just multiplying Jacobians by gradients. To denote the gradient of a value z with respect to a tensor X, we write V xz, just as if X were a vector. The indices into X now have multiple coordinates\u2014for example, a 3-D tensor is indexed by three coordinates. We can abstract this away by using a single variable i to represent the complete tuple of indices. For all possible index tuples i, (Vx z); gives e- This is exactly the same as how for all  possible integer indices 7 into a vector, (Vzz); gives ae Using this notation, we can write the chain rule as it applies to tensors", "2d12186f-7c01-49d7-a114-16c864fc0892": "1.21 (\u22c6 \u22c6) Consider two nonnegative numbers a and b, and show that, if a \u2a7d b, then a \u2a7d (ab)1/2. Use this result to show that, if the decision regions of a two-class classi\ufb01cation problem are chosen to minimize the probability of misclassi\ufb01cation, this probability will satisfy 1.22 (\u22c6) www Given a loss matrix with elements Lkj, the expected risk is minimized if, for each x, we choose the class that minimizes (1.81). Verify that, when the loss matrix is given by Lkj = 1 \u2212 Ikj, where Ikj are the elements of the identity matrix, this reduces to the criterion of choosing the class having the largest posterior probability. What is the interpretation of this form of loss matrix? 1.24 (\u22c6 \u22c6) www Consider a classi\ufb01cation problem in which the loss incurred when an input vector from class Ck is classi\ufb01ed as belonging to class Cj is given by the loss matrix Lkj, and for which the loss incurred in selecting the reject option is \u03bb. Find the decision criterion that will give the minimum expected loss", "c74584a5-79e9-48c4-9950-1ea529f26f2e": "DBNs only need to use MCMC sampling in their top pair of layers. The other layers are used only at the end of the sampling process, in one efficient ancestral sampling pass. To generate a sample from a DBM, it is necessary to use MCMC across all layers, with every layer of the model participating in every Markov chain transition. 20.4.2 DBM Mean Field Inference  The conditional distribution over one DBM layer given the neighboring layers is factorial. In the example of the DBM with two hidden layers, these distributions are Pv | AY), P(A | v,h@), and P(h@) | Ah). The distribution over ail hidden layers generally does not factorize because of interactions between layers.\n\nIn he example with two hidden layers, P(AY), hn?) | v) does not factorize because of che interaction weights W\u00ae) between Ah\u201c and h@), which render these variables mutually dependent. As was the case with the DBN, we are left to seek out methods to approximate he DBM posterior distribution", "f6913ad0-4997-44f6-8fb2-736033af06ed": "The factorial nature of the conditional P(h | v) follows immediately from our ability to write the joint probability over the vector h as the product of (unnormalized) distributions over the individual elements, h;. It is now a simple matter of normalizing the distributions over the individual binary h,. P(hj = P(h; =1\\v) == (hj = 1] \u00bb) (20.12) P(hj =0|v)+ P(h; =1|v) = exp 9 +015 (20.13) exp {0} + exp {ej + 0! W. 5} , =o c+uew.;. \u2014 f (20.14)  We can now express the full condifional over tHe hidden layer as the factorial  https://www.deeplearningbook.org/contents/generative_models.html    distribution: nh  P(h| v) = TJ o(@h- 0 (e+W'e)),", "98841e6d-b5c3-4687-9ce2-0808f4648d26": "Dynamic programming requires a distribution model because it uses expected updates, which involve computing expectations over all the possible next states and rewards. A sample model, on the other hand, is what is needed to simulate interacting with the environment during which sample updates, like those used by many reinforcement learning algorithms, can be used. Sample models are generally much easier to obtain than distribution models.\n\nWe have presented a perspective emphasizing the surprisingly close relationships between planning optimal behavior and learning optimal behavior. Both involve estimating the same value functions, and in both cases it is natural to update the estimates incrementally, in a long series of small backing-up operations. This makes it straightforward to integrate learning and planning processes simply by allowing both to update the same estimated value function. In addition, any of the learning methods can be converted into planning methods simply by applying them to simulated (model-generated) experience rather than to real experience. In this case learning and planning become even more similar; they are possibly identical algorithms operating on two di\u21b5erent sources of experience. It is straightforward to integrate incremental planning methods with acting and modelthe diagram on page 162), each producing what the other needs to improve; no other interaction among them is either required or prohibited. The most natural approach is for all processes to proceed asynchronously and in parallel", "23a3e893-7c29-4b76-b6f6-046e3a22c6d1": "This demonstrates the data-specific design of augmen- tations and the challenge of developing generalizable augmentation policies. This is an  important consideration with respect to the geometric augmentations listed below. Flipping  Horizontal axis flipping is much more common than flipping the vertical axis. This aug- mentation is one of the easiest to implement and has proven useful on datasets such as CIFAR-10 and ImageNet. On datasets involving text recognition such as MNIST or  SVHN, this is not a label-preserving transformation. Shorten and Khoshgoftaar J Big Data  6:60   Color space  Digital image data is usually encoded as a tensor of the dimension (height x width x color channels). Performing augmentations in the color channels space is another strategy that is very practical to implement. Very simple color augmentations include isolating a single color channel such as R, G, or B", "4b6437c3-110b-4dd7-9fbe-326613bdc364": "S., Wheeler, R. M. An N-player sequential stochastic game with identical payo\u21b5s.\n\nIEEE Transactions on Systems, Man, and Cybernetics, 6:1154\u20131158. Narendra, K. S., Wheeler, R. M. Decentralized learning in \ufb01nite Markov chains. IEEE Nedi\u00b4c, A., Bertsekas, D. P. Least squares policy evaluation algorithms with linear function approximation. Discrete Event Dynamic Systems, 13(1-2):79\u2013110. Ng, A. Y. Shaping and Policy Search in Reinforcement Learning. Ph.D. thesis, University Theory and application to reward shaping. In I. Bratko and S. Dzeroski (Eds. ), Proceedings of the 16th International Conference on Machine Learning , pp. 278\u2013287. Ng, A. Y., Russell, S. J. Algorithms for inverse reinforcement learning", "2bcc5ad9-2116-4f2e-9af4-59ed47b71d57": "The distinction between model-free and model-based algorithms is proving to be useful for this research. One can examine the computational implications of these types of algorithms in abstract settings that expose basic advantages and limitations of each type.\n\nThis serves both to suggest and to sharpen questions that guide the design of experiments necessary for increasing psychologists\u2019 understanding of habitual and goal-directed behavioral control. Our goal in this chapter has been to discuss correspondences between reinforcement learning and the experimental study of animal learning in psychology. We emphasized at the outset that reinforcement learning as described in this book is not intended to model details of animal behavior. It is an abstract computational framework that explores idealized situations from the perspective of arti\ufb01cial intelligence and engineering. But many of the basic reinforcement learning algorithms were inspired by psychological theories, and in some cases, these algorithms have contributed to the development of new animal learning models. This chapter described the most conspicuous of these correspondences. The distinction in reinforcement learning between algorithms for prediction and algorithms for control parallels animal learning theory\u2019s distinction between classical, or Pavlovian, conditioning and instrumental conditioning", "79d8511e-6f77-45c9-9430-6c2e4c98b1a5": "Figure 4 shows the topic accuracy of the controlled LM outputs averaged across the 7 topics, and Table 1 shows the respective language quality results. More detailed topic accuracy results and samples are provided in the appendix (\u00a7A.1.3) (where GeDi obtained low accuracy on 2 of the 7 topics, possibly because the topic tokens are tokenized into two subwords for which the model released by the authors was not speci\ufb01cally trained). 5https://huggingface.co/distilgpt2 6Note that the language quality emphasis is on the generated sentences. Prompts themselves do not necessarily have to be human-readable . We can see that the prompts generated by our SQL cause the LM to generate sentences with high topic accuracy while maintaining low perplexity in most settings. Increasing the prompt length positively impacts the topic accuracy, which makes sense because longer prompts give more \ufb02exible for steering the LM", "028d4bdf-baa7-4d12-947d-4d79f3c4e3eb": "Other models, presented later, apply these principles to learning stochastic mappings, functions with feedback, and probability distributions over a single vector. We begin this chapter with a simple example of a feedforward network. Next, we address each of the design decisions needed to deploy a feedforward network. First, training a feedforward network requires making many of the same design decisions as are necessary for a linear model: choosing the optimizer, the cost function, and the form of the output units. We review these basics of gradient-based learning, then proceed to confront some of the design decisions that are unique to feedforward networks.\n\nFeedforward networks have introduced the concept of a hidden layer, and this requires us to choose the activation functions that will be used to compute the hidden layer values. We must also design the architecture of the network, including how many layers the network should contain, how these layers should be connected to each other, and how many units should be in each layer. Learning in deep neural networks requires computing the gradients of complicated functions. We present the back-propagation algorithm and its modern generalizations, which can be used to efficiently compute these gradients. Finally, we close with some historical perspective", "2c00295a-203e-4ede-9699-ffcec6610948": "Kuleshov, V., Hancock, B., Ratner, A., R\u00e9 C, Batzaglou, S., Snyder, M.: A machine-compiled database of genome-wide association studies. NIPS ML4H Workshop  30.\n\nLehmann, J., Isele, R., Jakob, M., Jentzsch, A., Kontokostas, D., Mendes,P.,Hellmann,S.,Morsey,M.,vanKleef,P.,Auer,S.,Bizer, C.: DBpedia\u2013A large-scale, multilingual knowledge base extracted from Wikipedia. Semantic Web Journal  31. Li, H., Yu, B., Zhou, D.: Error rate analysis of labeling by crowdsourcing. In: ICML Workshop: Machine Learning Meets Crowdsourcing. Atalanta, Georgia, USA  32. Li, Y., Gao, J., Meng, C., Li, Q., Su, L., Zhao, B., Fan, W., Han, J.: A survey on truth discovery. SIGKDD Explor. Newsl", "9b7cde84-5c7a-4716-ab63-a9d532905bfc": "For example, suppose one wants to o\u21b5er v0 : S ! R as an initial guess at the true optimal value function v\u21e4, and that one is using linear function approximation with features x : S ! Rd. Then one would de\ufb01ne the initial value function approximation to be and update the weights w as usual. If the initial weight vector is 0, then the initial value function will be v0, but the asymptotic solution quality will be determined by the feature vectors as usual. This initialization can be done for arbitrary nonlinear approximators and arbitrary forms of v0, though it is not guaranteed to always accelerate learning. A particularly e\u21b5ective approach to the sparse reward problem is the shaping technique introduced by the psychologist B. F. Skinner and described in Section 14.3", "46b37fbc-03db-46c4-a471-ea4da9e969b1": "3.5 (\u22c6) www Using the technique of Lagrange multipliers, discussed in Appendix E, show that minimization of the regularized error function (3.29) is equivalent to minimizing the unregularized sum-of-squares error (3.12) subject to the constraint (3.30).\n\nDiscuss the relationship between the parameters \u03b7 and \u03bb. together with a training data set comprising input basis vectors \u03c6(xn) and corresponding target vectors tn, with n = 1, . , N. Show that the maximum likelihood solution WML for the parameter matrix W has the property that each column is given by an expression of the form (3.15), which was the solution for an isotropic noise distribution. Note that this is independent of the covariance matrix \u03a3. Show that the maximum likelihood solution for \u03a3 is given by 3.7 (\u22c6) By using the technique of completing the square, verify the result (3.49) for the posterior distribution of the parameters w in the linear basis function model in which mN and SN are de\ufb01ned by (3.50) and (3.51) respectively", "17f0a051-235b-4570-b1b8-ac611a3323bf": "https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Log   The training process in Matching Networks is designed to match inference at test time, see the details in the earlier section. It is worthy of mentioning that the Matching Networks paper refined the idea that training and testing conditions should match. o* = arg max Eycc|Estcp,pt-p| S Pa(yl|x, $\u201d)]]  (x,y)EBL  Relation Network  Relation Network (RN)  is similar to siamese network but with a few differences:  The relationship is not captured by a simple L1 distance in the feature space, but predicted by a CNN classifier gy.\n\nThe relation score between a pair of inputs, x; and x,, isr;; = go() where  is concatenation", "60c86c7a-2fe6-4c52-ad56-5e2fa1656474": "Optimization of this objective is equivalent to approximate MAP estimation, where the likelihood gradient is approximated by the gradient of the lower bound. We compared performance of AEVB to the wake-sleep algorithm . We employed the same encoder (also called recognition model) for the wake-sleep algorithm and the variational autoencoder. All parameters, both variational and generative, were initialized by random sampling from N(0, 0.01), and were jointly stochastically optimized using the MAP criterion. Stepsizes were adapted with Adagrad ; the Adagrad global stepsize parameters were chosen from {0.01, 0.02, 0.1} based on performance on the training set in the \ufb01rst few iterations. Minibatches of size M = 100 were used, with L = 1 samples per datapoint. Likelihood lower bound We trained generative models (decoders) and corresponding encoders (a.k.a.\n\nrecognition models) having 500 hidden units in case of MNIST, and 200 hidden units in case of the Frey Face dataset (to prevent over\ufb01tting, since it is a considerably smaller dataset)", "3ce9e25b-b8c9-4cc2-a66e-5000b3a1f70b": "We decouple the prediction task and the encoder architecture, so we do not require a context aggregation network, and our encoder can look at the images of wider spectrum of resolutions.\n\nIn addition, we use the NT-Xent loss function, which leverages normalization and temperature, whereas they use an unnormalized cross-entropy-based objective. We use simpler data augmentation. \u2022 InstDisc, MoCo, PIRL  generalize the Exemplar approach originally proposed by Dosovitskiy et al. and leverage an explicit memory bank. We do not use a memory bank; we \ufb01nd that, with a larger batch size, in-batch negative example sampling suf\ufb01ces. We also utilize a nonlinear projection head, and use the representation before the projection head. Although we use similar types of augmentations (e.g., random crop and color distortion), we expect speci\ufb01c parameters may be different. \u2022 CMC  uses a separated network for each view, while we simply use a single network shared for all randomly augmented views. The data augmentation, projection head and loss function are also different. We use larger batch size instead of a memory bank. \u2022 Whereas Ye et al", "89e13e26-6efc-4f97-a17d-56a2ab533f6f": "Instead, there is a general equation for the mean field fixed-point updates.\n\nIf we make the mean field approximation  a(h |v) =T] ahs |\u00bb), (19.55)  and fix g(hj | v) for all 7 A i, then the optimal q(h; | v) may be obtained by normalizing the unnormalized distribution G(hi |v) = exp Ep_jvq(n_,|vylogp(v,h) , (19.56)  as long as p does not assign 0 probability to any joint configuration of variables. Carrying out the expectation insidd the equation will yield dhe correct functional form of g(h; __). Deriving functional forms of q directly using calculus of variations is only necepgary if one wishes to develop a new form of variational learning;  https://www.deeplearningbook.org/contents/inference.html    equation 19.56 yields the mean field approximation for any probabilistic model. Equation 19.56 is a fixed-point equation, designed to be iteratively applied for each value of 7 repeatedly until convergence. However, it also tells us more than  that", "4a3e88e3-ce54-4ff2-aa06-28d0d844dba5": "For example, we can add weight decay to the linear regression cost function  https://www.deeplearningbook.org/contents/ml.html   LO ODLaLLL J(w, b) = AI|w||2 \u2014 Ex.v~paata log pmodel(y | x), (5.101) This still allows closed form optimization. If we change the model to be nonlinear, then most cost functions can no longer be optimized in closed form. This requires us to choose an iterative numerical optimization procedure, such as gradient descent. The recipe for constructing a learning algorithm by combining models, costs, and optimization algorithms supports both supervised and unsupervised learning.\n\nThe linear regression example shows how to support supervised learning. Unsupervised learning can be supported by defining a dataset that contains only X and providing an appropriate unsupervised cost and model. For example, we can obtain the first PCA vector by specifying that our loss function is  J(w) =E \\|x \u2014 r(w; w)||3 (5.102)  X~Paata  while our model is defined to have w with norm one and reconstruction function  r(a) = wlaew. 151  CHAPTER 5", "cf1f714d-eae1-4291-9648-5b1a880b5eea": "Similarly, the optimal solution for the factor q\u03c4(\u03c4) is given by and hence q\u03c4(\u03c4) is a gamma distribution Gam(\u03c4|aN, bN) with parameters It should be emphasized that we did not assume these speci\ufb01c functional forms for the optimal distributions q\u00b5(\u00b5) and q\u03c4(\u03c4).\n\nThey arose naturally from the structure of the likelihood function and the corresponding conjugate priors. Section 10.4.1 Thus we have expressions for the optimal distributions q\u00b5(\u00b5) and q\u03c4(\u03c4) each of which depends on moments evaluated with respect to the other distribution. One approach to \ufb01nding a solution is therefore to make an initial guess for, say, the moment E and use this to re-compute the distribution q\u00b5(\u00b5). Given this revised distribution we can then extract the required moments E and E, and use these to recompute the distribution q\u03c4(\u03c4), and so on. Since the space of hidden variables for this example is only two dimensional, we can illustrate the variational approximation to the posterior distribution by plotting contours of both the true posterior and the factorized approximation, as illustrated in Figure 10.4. tion", "379287ca-8b21-46f9-b902-ca1fc7c86df3": "N000141210041 and N000141310129, the Moore Foundation, the Okawa Research Grant, American Family Insurance, Accenture, Toshiba, the Stanford Interdisciplinary Graduate and Bio-X fellowships, and members of the Stanford DAWN project: Intel, Microsoft, Teradata, and VMware.\n\nThe US Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, \ufb01ndings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily re\ufb02ect the views, policies, or endorsements, either expressed or implied, of DARPA, DOE, NIH, ONR, or the US Government. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecomm ons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. 1", "bc012ce0-e56c-4af2-b73f-508a6ed17d6e": "The linear and quadratic decision boundaries are illustrated in Figure 4.11. distribution, coloured red, green, and blue, in which the red and green classes have the same covariance matrix. The right-hand plot shows the corresponding posterior probabilities, in which the RGB colour vector represents the posterior probabilities for the respective three classes. The decision boundaries are also shown. Notice that the boundary between the red and green classes, which have the same covariance matrix, is linear, whereas those between the other pairs of classes are quadratic. Once we have speci\ufb01ed a parametric functional form for the class-conditional densities p(x|Ck), we can then determine the values of the parameters, together with the prior class probabilities p(Ck), using maximum likelihood. This requires a data set comprising observations of x along with their corresponding class labels. Consider \ufb01rst the case of two classes, each having a Gaussian class-conditional density with a shared covariance matrix, and suppose we have a data set {xn, tn} where n = 1, . , N. Here tn = 1 denotes class C1 and tn = 0 denotes class C2", "12984491-fdb8-4749-be81-db71c794c2f2": "Thus, if the kernel function k(xn, xm) depends only on the distance \u2225xn \u2212 xm\u2225, then we obtain an expansion in radial basis functions. The results (6.66) and (6.67) de\ufb01ne the predictive distribution for Gaussian process regression with an arbitrary kernel function k(xn, xm). In the particular case in which the kernel function k(x, x\u2032) is de\ufb01ned in terms of a \ufb01nite set of basis functions, we can derive the results obtained previously in Section 3.3.2 for linear regression starting from the Gaussian process viewpoint. Exercise 6.21 For such models, we can therefore obtain the predictive distribution either by taking a parameter space viewpoint and using the linear regression result or by taking a function space viewpoint and using the Gaussian process result. The central computational operation in using Gaussian processes will involve the inversion of a matrix of size N \u00d7 N, for which standard methods require O(N 3) computations. By contrast, in the basis function model we have to invert a matrix SN of size M \u00d7 M, which has O(M 3) computational complexity", "0bc7106a-e97d-4983-9977-78f151cc37ea": "We did not reach for the highest possible level of mathematical abstraction and did not rely on a theorem\u2013proof format.\n\nWe tried to choose a level of mathematical detail that points the mathematically inclined in the right directions without distracting from the simplicity and potential generality of the underlying ideas. of people to thank. First, we thank those who have personally helped us develop the overall view presented in this book: Harry Klopf, for helping us recognize that reinforcement learning needed to be revived; Chris Watkins, Dimitri Bertsekas, John Tsitsiklis, and Paul Werbos, for helping us see the value of the relationships to dynamic programming; John Moore and Jim Kehoe, for insights and inspirations from animal learning theory; Oliver Selfridge, for emphasizing the breadth and importance of adaptation; and, more generally, our colleagues and students who have contributed in countless ways: Ron Williams, Charles Anderson, Satinder Singh, Sridhar Mahadevan, Steve Bradtke, Bob Crites, Peter Dayan, and Leemon Baird", "f9c2498d-dbaa-413e-917a-a32fe789632b": "In principle, we could also treat 6 as a parameter of the model and learn it. Our presentation here has discarded some terms that do not depend on h but do depend on 8. To learn 3, these terms must be included, or ( will collapse to 0. Not all approaches to sparse coding explicitly build a p(h) and a p(a | h). Often we are just interested in learning a dictionary of features with activation values that will often be zero when extracted using this inference procedure. If we sample h from a Laplace prior, it is in fact a zero probability event for an element of h to actually be zero. The generative model itself is not especially sparse; only the feature extractor is.\n\nGoodfellow et al. describe approximate inference in a different model family, the spike and slab sparse coding model, for which samples from the prior usually contain true zeros. 493  CHAPTER 13. LINEAR FACTOR MODELS  The sparse coding approach combined with the use of the nonparametric encoder can in principle minimize the combination of reconstruction error and log-prior better than any specific parametric encoder", "daca351c-25ff-4795-b7ca-75e1d16efd67": "In the context of models with latent variables, which define a joint distribution Pmodel(x, h), we often draw samples of x by alternating between sampling from Pmodel(& | A) and sampling from pmodei(h | x). From the point of view of mixing rapidly, we would like pmode( | #) to have high entropy. From the point of view of learning a useful representation of h, we would like h to encode enough information about x to reconstruct it well, which implies that h and a should have high mutual information. These two goals are at odds with each other. We often learn generative models that very precisely encode a into h but are not able to mix very well. This situation arises frequently with Boltzmann machines\u2014the sharper the distribution a Boltzmann machine learns, the harder it is for a Markov chain sampling from the model distribution to mix well. This problem is illustrated in figure 17.2.\n\nAll this could make MCMC methods less useful when the distribution of interest has a manifold structure with a separate manifold for each class: the distribution is concentrated around many modes, and these modes are separated by vast regions of high energy", "30f4d581-c211-4dd1-a105-68bade3fbf55": "Linked importance sampling Both AIS and bridge sampling have their ad- vantages. If Dxy(pollpi) is not too large (because po and py, are sufficiently close), bridge sampling can be a more effective means of estimating the ratio of partition functions than AIS. If, however, the two distributions are too far apart for a single distribution p, to bridge the gap, then one can at least use AIS with potentially many intermediate distributions to span the distance between pg and p,. Neal  showed how his linked importance sampling method leveraged the power of the bridge sampling strategy to bridge the intermediate distributions used in AIS and significantly improve the overall partition function estimates. 627  https://www.deeplearningbook.org/contents/partition.html    CHAPTER 18. CONFRONTING THE PARTITION FUNCTION  Estimating the partition function while training While AIS has become accepted as the standard method for estimating the partition function for many undirected models, it is sufficiently computationally intensive that it remains infeasible to use during training.\n\nAlternative strategies have been explored to maintain an estimate of the partition function throughout training", "4cf5b680-ca58-45ce-840d-483a150ce0dc": "The Experience, E  Machine learning algorithms can be broadly categorized as unsupervised or supervised by what kind of experience they are allowed to have during the learning process. Most of the learning algorithms in this book can be understood as being allowed  102  CHAPTER 5. MACHINE LEARNING BASICS  to experience an entire dataset. A dataset is a collection of many examples, as defined in section 5.1.1. Sometimes we call examples data points.\n\nOne of the oldest datasets studied by statisticians and machine learning re- searchers is the Iris dataset . It is a collection of measurements of different parts of 150 iris plants. Each individual plant corresponds to one example. The features within each example are the measurements of each part of the plant: the sepal length, sepal width, petal length and petal width. The dataset also records which species each plant belonged to. Three different species are represented in the dataset. Unsupervised learning algorithms experience a dataset containing many features, then learn useful properties of the structure of this dataset. In the context of deep learning, we usually want to learn the entire probability distribution that generated a dataset, whether explicitly, as in density estimation, or implicitly, for tasks like synthesis or denoising", "0fbee514-c110-4443-b3c2-a7fb8cae26aa": "For the moment, Section 10.1 however, we simply note that in applying maximum likelihood to Gaussian mixture models we must take steps to avoid \ufb01nding such pathological solutions and instead seek local maxima of the likelihood function that are well behaved.\n\nWe can hope to avoid the singularities by using suitable heuristics, for instance by detecting when a Gaussian component is collapsing and resetting its mean to a randomly chosen value while also resetting its covariance to some large value, and then continuing with the optimization. A further issue in \ufb01nding maximum likelihood solutions arises from the fact that for any given maximum likelihood solution, a K-component mixture will have a total of K! equivalent solutions corresponding to the K! ways of assigning K sets of parameters to K components. In other words, for any given (nondegenerate) point in the space of parameter values there will be a further K!\u22121 additional points all of which give rise to exactly the same distribution. This problem is known as identi\ufb01ability  and is an important issue when we wish to interpret the parameter values discovered by a model. Identi\ufb01ability will also arise when we discuss models having continuous latent variables in Chapter 12", "0a71defe-2c69-4300-acf5-34e4a2d03cb6": "(Right)With weight decay approaching zero (i.e., using the Moore-Penrose pseudoinverse to solve the underdetermined problem with minimal regularization), the degree-9 polynomial overfits significantly, as we saw in figure 5.2. 118  CHAPTER 5. MACHINE LEARNING BASICS  Sometimes a setting is chosen to be a hyperparameter that the learning algo- rithm does not learn because the setting is difficult to optimize. More frequently,  https://www.deeplearningbook.org/contents/ml.html    the setting must be a hyperparameter because it is not appropriate to learn that hyperparameter on the training set. This applies to all -hyperparameters that control model capacity.\n\nIf learned on the training set, such hyperparameters would always choose the maximum possible model capacity, resulting in overfitting (refer to figure 5.3). For example, we can always fit the training set better with a higher-degree polynomial and a weight decay setting of \\ = 0 than we could with a lower-degree polynomial and a positive weight decay setting. To solve this problem, we need a validation set of examples that the training algorithm does not observe", "2c9594a6-79be-4668-b1ce-fece29ed44f6": "The convergence point of gradient descent depends on the initial values of the parameters.\n\nIn practice, gradient descent would usually not find clean, easily understood, integer-valued solutions like the one we presented here. 6.2 Gradient-Based Learning  Designing and training a neural network is not much different from training any other machine learning model with gradient descent. In section 5.10, we described how to build a machine learning algorithm by specifying an optimization procedure, a cost function, and a model family. 172  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  The largest difference between the linear models we have seen so far and neural networks is that the nonlinearity of a neural network causes most interesting loss functions to become nonconvex. This means that neural networks are usually trained by using iterative, gradient-based optimizers that merely drive the cost function to a very low value, rather than the linear equation solvers used to train linear regression models or the convex optimization algorithms with global conver- gence guarantees used to train logistic regression or SVMs", "a5c0e321-b4b2-438b-b32f-9919a88b5626": "A successful example of such a method is the Variational Auto-Encoder (VAE),  https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/  Self-supervised learning: The dark matter of intelligence   in which the latent variable is made \u201cfuzzy\u201d, which limits its capacity. But VAE have not yet been shown to produce good representations for downstream visual tasks. Another successful example is sparse modeling, but its use has been limited to simple architectures. No perfect recipe seems to exist to limit the capacity of latent variables. The challenge of the next few years may be to devise non-contrastive methods for latent-variable energy- based model that successfully produce good representations of image, video, speech, and other signals and yield top performance in downstream supervised tasks without requiring large amounts of labeled data", "dc17bf29-b83a-4370-bdec-6f2eed469e71": "If we de\ufb01ne p(t = 1|y) = \u03c3(y) where y(x) is given by (7.1), show that the negative log likelihood, with the addition of a quadratic regularization term, takes the form (7.47). 7.7 (\u22c6) Consider the Lagrangian (7.56) for the regression support vector machine. By setting the derivatives of the Lagrangian with respect to w, b, \u03ben, and \ufffd\u03ben to zero and then back substituting to eliminate the corresponding variables, show that the dual Lagrangian is given by (7.61).\n\n7.8 (\u22c6) www For the regression support vector machine considered in Section 7.1.4, show that all training data points for which \u03ben > 0 will have an = C, and similarly all points for which \ufffd\u03ben > 0 will have \ufffdan = C. 7.9 (\u22c6) Verify the results (7.82) and (7.83) for the mean and covariance of the posterior distribution over weights in the regression RVM. 7.10 (\u22c6 \u22c6) www Derive the result (7.85) for the marginal likelihood function in the regression RVM, by performing the Gaussian integral over w in (7.84) using the technique of completing the square in the exponential", "83d5635a-82f0-4d2b-bfb9-48b0f1298e78": "(19.61)  Because of the presence of the terms multiplying h; and hg together, we can see that the true posterior does not factorize over h, and hy. Applying equation 19.56, we find that  G(hy | v) (19.62) =exp Ey, cg(ho|v) log B(v, h) (19.63) 1 =exp 5 Ey ~q(ho|v) hi + fp +y2+ hiwi + h3ws (19.64) (s\u00ab owolt Ihiwihowy) . (19.65) \u20142v \u2014 2vh  https://www.deeplearningbook.org/contents/inference.html    From t \u00a5i8, we can see that. there arg Bictectivelyy nly two values we need to obtain from 4\\ h2 | v): Eho~g(h|v)  and Eho~q(h|v) s", "400d5878-35b8-4081-ab84-10c673f62b3a": "An immediate practical advantage of tile coding is that, because it works with partitions, the overall number of features that are active at one time is the same for any state. Exactly one feature is present in each tiling, so the total number of features present is always the same as the number of tilings. This allows the step-size parameter, \u21b5, to be set in an easy, intuitive way. For example, choosing \u21b5 = 1 of tilings, results in exact one-trial learning. If the example s 7!\n\nv is trained on, then whatever the prior estimate, \u02c6v(s,wt), the new estimate will be \u02c6v(s,wt+1) = v. Usually one wishes to change more slowly than this, to allow for generalization and stochastic variation in target outputs. For example, one might choose \u21b5 = 1 estimate for the trained state would move one-tenth of the way to the target in one update, and neighboring states will be moved less, proportional to the number of tiles they have in common. Tile coding also gains computational advantages from its use of binary feature vectors", "254b503c-9706-48d5-b8b3-756325a5dbfe": "We want to establish bench- marks for different ensembles of test-time augmentations and investigate the solution algorithms used.\n\nCurrently, majority voting seems to be the dominant solution algo- rithm for test-time augmentation. It seems highly likely that test-time augmentation can be further improved if the weight of each augmented images prediction is further parameterized and learned. Additionally, we will explore the effectiveness of test-time augmentation on object detection, comparing color space augmentations and the Neural Style Transfer algorithm. Meta-learning GAN architectures is another exciting area of interest. Using Reinforce- ment Learning algorithms such as NAS on the generator and discriminator architectures seem very promising. Another interesting area of further research is to use an evolu- tionary approach to speed up the training of GANs through parallelization and cluster computing. Another important area of future work for practical integration of Data Augmentation into Deep Learning workflows is the development of software tools. Similar to how the Tensorflow  system automates the back-end processes of gradient-descent learn- ing, Data Augmentation libraries will automate preprocessing functions", "050f0e69-bd61-4f45-b04d-ed2bf4254c13": "Klopf recognized that essential aspects of adaptive behavior were being lost as learning researchers came to focus almost exclusively on supervised learning. What was missing, according to Klopf, were the hedonic aspects of behavior, the drive to achieve some result from the environment, to control the environment toward desired ends and away from undesired ends (see Section 15.9). This is the essential idea of trial-and-error learning. Klopf\u2019s ideas were especially in\ufb02uential on the authors because our assessment of them  led to our appreciation of the distinction between supervised and reinforcement learning, and to our eventual focus on reinforcement learning. Much of the early work that we and colleagues accomplished was directed toward showing that reinforcement learning and supervised learning were indeed di\u21b5erent .\n\nOther studies showed how reinforcement learning could address important problems in arti\ufb01cial neural network learning, in particular, how it could produce learning algorithms for multilayer networks (Barto, Anderson, and Sutton, 1982; Barto and Anderson, 1985; Barto, 1985, 1986; Barto and Jordan, 1987; see Section 15.10). We turn now to the third thread to the history of reinforcement learning, that concerning temporal-di\u21b5erence learning", "57cfa45d-c6cf-470a-9e4d-92e60bc3fbd3": "8.1 The overall view of planning and learning presented here has developed gradually over a number of years, in part by the authors ; it has been strongly in\ufb02uenced by Agre and Chapman , Bertsekas and Tsitsiklis , Singh , and others. The authors were also strongly in\ufb02uenced by psychological studies of latent learning  and by psychological views of the nature of thought . In Part III of the book, Section 14.6 relates model-based and model-free methods to psychological theories of learning and behavior, and Section 15.11 discusses ideas about how the brain might implement these types of methods.\n\n8.2 The terms direct and indirect, which we use to describe di\u21b5erent kinds of reinforcement learning, are from the adaptive control literature , where they are used to make the same kind of distinction. The term system identi\ufb01cation is used in adaptive control for what we call modellearning . The Dyna architecture is due to Sutton , and the results in this and the next section are based on results reported there. Barto and Singh  consider some of the issues in comparing direct and indirect reinforcement learning methods", "ecddb9ef-5ad1-4793-ad3a-6144d6a736de": "In our discussion of inference in graphical models, we have assumed that the structure of the graph is known and \ufb01xed. However, there is also interest in going beyond the inference problem and learning the graph structure itself from data . This requires that we de\ufb01ne a space of possible structures as well as a measure that can be used to score each structure. From a Bayesian viewpoint, we would ideally like to compute a posterior distribution over graph structures and to make predictions by averaging with respect to this distribution. If we have a prior p(m) over graphs indexed by m, then the posterior distribution is given by where D is the observed data set. The model evidence p(D|m) then provides the score for each model. However, evaluation of the evidence involves marginalization over the latent variables and presents a challenging computational problem for many models. Exploring the space of structures can also be problematic. Because the number of different graph structures grows exponentially with the number of nodes, it is often necessary to resort to heuristics to \ufb01nd good candidates", "59ff2495-a6a7-48b8-93fd-2d397c119097": "\u21e4Exercise 5.14 Modify the algorithm for o\u21b5-policy Monte Carlo control (page 111) to use the idea of the truncated weighted-average estimator (5.10). Note that you will \ufb01rst need to convert this equation to action values. \u21e4 The Monte Carlo methods presented in this chapter learn value functions and optimal policies from experience in the form of sample episodes. This gives them at least three kinds of advantages over DP methods. First, they can be used to learn optimal behavior directly from interaction with the environment, with no model of the environment\u2019s dynamics. Second, they can be used with simulation or sample models. For surprisingly many applications it is easy to simulate sample episodes even though it is di\ufb03cult to construct the kind of explicit model of transition probabilities required by DP methods. Third, it is easy and e\ufb03cient to focus Monte Carlo methods on a small subset of the states.\n\nA region of special interest can be accurately evaluated without going to the expense of accurately evaluating the rest of the state set (we explore this further in Chapter 8). A fourth advantage of Monte Carlo methods, which we discuss later in the book, is that they may be less harmed by violations of the Markov property", "4b424c0e-a5f9-40bb-b9ec-ab4496390129": "It is natural to ask which of these methods is best. Although this is a di\ufb03cult question to answer in general, we can certainly run them all on the 10-armed testbed that we have used throughout this chapter and compare their performances. A complication is that they all have a parameter; to get a meaningful comparison we have to consider their performance as a function of their parameter. Our graphs so far have shown the course of learning over time for each algorithm and parameter setting, to produce a learning curve for that algorithm and parameter setting. If we plotted learning curves for all algorithms and all parameter settings, then the graph would be too complex and crowded to make clear comparisons. Instead we summarize a complete learning curve by its average value over the 1000 steps; this value is proportional to the area under the learning curve. Figure 2.6 shows this measure for the various bandit algorithms from this chapter, each as a function of its own parameter shown on a single scale on the x-axis. This kind of graph is called a parameter study. Note that the parameter values are varied by factors of two and presented on a log scale", "2ecc803f-ffa0-4a22-ab67-6c9c13ba87a1": "Residual algorithms: Reinforcement learning with function approximation. In Proceedings of the 12th International Conference on Machine Learning , pp. 30\u201337. Morgan Kaufmann. Baird, L. C. Reinforcement Learning through Gradient Descent. Ph.D. thesis, Carnegie Baird, L. C., Klopf, A. H. Reinforcement learning with high-dimensional, continuous actions. Wright Laboratory, Wright-Patterson Air Force Base, Tech. Rep. WL-TR-93-1147. Baldassarre, G., Mirolli, M. (Eds.) . Intrinsically Motivated Learning in Natural and Arti\ufb01cial Systems. Springer-Verlag, Berlin Heidelberg. Balke, A., Pearl, J. Counterfactual probabilities: Computational methods, bounds and applications. In Proceedings of the Tenth International Conference on Uncertainty in Arti\ufb01cial Intelligence (UAI-1994, pp. 46\u201354. Morgan Kaufmann", "13b7f95b-680c-4a2e-ae51-52ae512945c1": "But like the RBM, within each layer, each of the variables are mutually independent, conditioned on the variables in  https://www.deeplearningbook.org/contents/generative_models.html    he neighboring layers.\n\nSee figure 20.2 for the graph structure. Deep Boltzmann machines have been applied to a variety of tasks, including document modeling Srivastava et al., 3013). Like RBMs and DBNs, DBMs typically contain only binary units\u2014as we assume for simplicity of our presentation of the model\u2014but it is straightforward (o include real-valued visible units. A DBM is an energy-based model, meaning that the joint probability distribu- ion over the model variables is parametrized by an energy function EF. In the case of a deep Boltzmann machine with one visible layer, v, and three hidden layers, AQ), A), and A), the joint probability is given by  P (vA, n,n) _ Fo  To simplify our presentation, we omit the bias parameters below. The DBM energy function is then defined as follows:  exp (\u2014B(v,h\\,h\u00ae,W;6))", "0c8578a4-6c04-4b12-8873-b5dffae0cf94": "We would also like to thank CIFAR, and Canada Research Chairs for funding, and Compute Canada, and Calcul Qu\u00b4ebec for providing computational resources. Ian Goodfellow is supported by the 2013 Google Fellowship in Deep Learning. Finally, we would like to thank Les Trois Brasseurs for stimulating our creativity. Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I. J., Bergeron, A., Bouchard, N., and Bengio, Y. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop. Bengio, Y. Learning deep architectures for AI. Now Publishers. Bengio, Y., Mesnil, G., Dauphin, Y., and Rifai, S. Better mixing via deep representations. In ICML\u201913. Bengio, Y., Thibodeau-Laufer, E., and Yosinski, J. Deep generative stochastic networks trainable by backprop. In ICML\u201914", "59dca559-9201-469b-abb7-9f3d4ca72b3b": "Then by construction this distribution will be periodic, although it will not be normalized.\n\nWe can determine the form of this distribution by transforming from Cartesian coordinates (x1, x2) to polar coordinates (r, \u03b8) so that We also map the mean \u00b5 into polar coordinates by writing Next we substitute these transformations into the two-dimensional Gaussian distribution (2.173), and then condition on the unit circle r = 1, noting that we are interested only in the dependence on \u03b8. Focussing on the exponent in the Gaussian distribution we have (r cos \u03b8 \u2212 r0 cos \u03b80)2 + (r sin \u03b8 \u2212 r0 sin \u03b80)2\ufffd 0 \u2212 2r0 cos \u03b8 cos \u03b80 \u2212 2r0 sin \u03b8 sin \u03b80 on the left and as the corresponding polar plot on the right. where \u2018const\u2019 denotes terms independent of \u03b8, and we have made use of the following trigonometrical identities Exercise 2.51 If we now de\ufb01ne m = r0/\u03c32, we obtain our \ufb01nal expression for the distribution of p(\u03b8) along the unit circle r = 1 in the form which is called the von Mises distribution, or the circular normal", "9613ac93-2cbd-4ea3-92a8-4f4bdb5fd909": "DEEP GENERATIVE MODELS  a tensor of zeros, then copies each value from spatial coordinate i of the input to spatial coordinate i x k of the output. The integer value k defines the size of the pooling region. Even though the assumptions motivating the definition of the un-pooling operator are unrealistic, the subsequent layers are able to learn to compensate for its unusual output, so the samples generated by the model as a whole are visually pleasing. 20.10.7 Auto-Regressive Networks  Auto-regressive networks are directed probabilistic models with no latent random variables. The conditional probability distributions in these models are represented by neural networks (sometimes extremely simple neural networks, such as logistic regression). The graph structure of these models is the complete graph. They decompose a joint probability over the observed variables using the chain rule of probability to obtain a product of conditionals of the form P(aq | a\u20141,.--,%1).\n\nSuch models have been called fully-visible Bayes networks (FVBNs) and used successfully in many forms, first with logistic regression for each conditional distribution , and then with neural networks with hidden units", "52de38b7-f048-4633-9dc3-22085a5899df": "Then show that minimizing J with respect to Zn, where W is kept fixed, gives rise to the PCA Estep (12.58), and that minimizing J with respect to W, where {zn} is kept fixed, gives rise to the PCA M step (12.59). 12.20 (**) By considering second derivatives, show that the only stationary point of the log likelihood function for the factor analysis model discussed in Section 12.2.4 with respect to the parameter J1 is given by the sample mean defined by (12.1). Furthermore, show that this stationary point is a maximum. 12.21 (**) Derive the formulae (12.66) and (12.67) for the E step of the EM algorithm for factor analysis. Note that from the result of Exercise 12.20, the parameter J1 can be replaced by the sample mean x.\n\n12.22 (* *) Write down an expression for the expected complete-data log likelihood function for the factor analysis model, and hence derive the corresponding M step equations (12.69) and (12.70)", "95558677-8820-4bfb-9c13-67aa0b2d8677": "On the other hand, inactivating the dorsomedial striatum (DMS) impairs goal-directed processes, requiring the animal to rely more on habit learning. Results like these support the view that the DLS in rodents is more involved in model-free processes, whereas their DMS is more involved in model-based processes. Results of studies with human subjects in similar experiments using functional neuroimaging, and with non-human primates, support the view that the analogous structures in the primate brain are di\u21b5erentially involved in habitual and goal-directed modes of behavior.\n\nOther studies identify activity associated with model-based processes in the prefrontal cortex of the human brain, the front-most part of the frontal cortex implicated in executive function, including planning and decision making. Speci\ufb01cally implicated is the orbitofrontal cortex (OFC), the part of the prefrontal cortex immediately above the eyes. Functional neuroimaging in humans, and also recordings of the activities of single neurons in monkeys, reveals strong activity in the OFC related to the subjective reward value of biologically signi\ufb01cant stimuli, as well as activity related to the reward expected as a consequence of actions. Although not free of controversy, these results suggest signi\ufb01cant involvement of the OFC in goal-directed choice", "4a2d1836-5a31-4f45-957c-01c282b42081": "Sample a corrupted version @ from C(x | x = x). 3. Use (a, %) as a training example for estimating the autoencoder reconstruction distribution preconstruct(& | &) = Paecoder(& | A) with h the output of encoder f(\u20ac) and paecoder typically defined by a decoder g(h). Typically we can simply perform gradient-based approximate minimization (such as minibatch gradient descent) on the negative log-likelihood \u2014 log p decoder(& | h). As long as the encoder is deterministic, the denoising autoencoder is a feedforward network and may be trained with exactly the same techniques as any other feedforward network. We can therefore view the DAE as performing stochastic gradient descent on the following expectation:  _ Exn Bata (x) xe C(%| ar) 108 Pdecoder (x | h= f(&)), (14.14) where Pgata(X) is the training distribution. 508  CHAPTER 14. AUTOENCODERS  14.5.1 Estimating the Score  Score matching  is an alternative to maximum likelihood", "3970bb5b-8ac6-42cb-9020-b5fe6b67bb51": "Similar to other metric-based models, the classifier output is defined as a sum of labels of support samples weighted by attention kernel a(x, x;) - which should be proportional to the similarity between x and x;. cs(x) = P(y|x, S) = So a(x, x;)yi, where S = {(xi5 yi) a  i=1  The attention kernel depends on two embedding functions, f and g, for encoding the test sample and the support set samples respectively. The attention weight between two data points is the  https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Log  cosine similarity, cosine(. ), between their embedding vectors, normalized by softmax:  exp(cosine( f(x), 9(x:)) Doj-1 exp(cosine( f(x), 9(x,;))  a(x, x;) =  Simple Embedding  In the simple version, an embedding function is a neural network with a single data sample as input", "f6be9335-db4f-4eab-8b35-e24c09307177": "Such feature vectors enable approximations as arbitrary quadratic functions of the state numbers\u2014even though the approximation is still linear in the weights that have to be learned.\n\nGeneralizing this example from two to k numbers, we can represent highly-complex interactions among a problem\u2019s state dimensions: Suppose each state s corresponds to k numbers, s1, s2, ..., sk, with each si 2 R. For this k-dimensional state space, each order-n polynomial-basis feature xi can be written as where each ci,j is an integer in the set {0, 1, . , n} for an integer n \u2265 0. These features make up the order-n polynomial basis for dimension k, which contains (n + 1)k di\u21b5erent features. Higher-order polynomial bases allow for more accurate approximations of more complicated functions. But because the number of features in an order-n polynomial basis grows exponentially with the dimension k of the natural state space (if n>0), it is generally necessary to select a subset of them for function approximation", "f780e724-48cb-463e-976a-895de2086876": "On extremely small datasets, such as the alternative splicing dataset, Bayesian methods outperform methods based on unsupervised pretraining . For these reasons, the popularity of unsupervised pretraining has declined. Nevertheless, unsupervised pretraining remains an important milestone in the history of deep learning research  533  CHAPTER 15. REPRESENTATION LEARNING  and continues to influence contemporary approaches. The idea of pretraining has been generalized to supervised pretraining, discussed in section 8.7.4, as a very common approach for transfer learning.\n\nSupervised pretraining for transfer learning is popular  for use with convolutional  https://www.deeplearningbook.org/contents/representation.html    networks pretrained on the ImageNet dataset. Practitioners publish the parameters of these trained networks for this purpose, just as pretrained word vectors are published for natural language tasks . 15.2 Transfer Learning and Domain Adaptation  Transfer learning and domain adaptation refer to the situation where what has been learned in one setting (e.g., distribution P;) is exploited to improve generalization in another setting (say, distribution P)). This generalizes the idea presented in the previous section, where we transferred representations between an unsupervised learning task and a supervised learning task", "72a65a16-f816-4482-ba85-c16c96aaae4f": "Clearly, each example conveys less information than in the supervised case, where the true label y is directly accessible, so more examples are necessary.\n\nWorse, if we are not careful, we could end up with a system that continues picking the wrong decisions even as more and more data is collected, because the correct decision initially had a very low probability: until the learner picks that correct decision, it does not learn about the correct decision. This is similar to the situation in reinforcement learning where only the reward for the selected action is observed. In general, reinforcement learning can involve a sequence of many actions and many rewards. The bandits scenario is a special case of reinforcement learning, in which the learner takes only a single action and receives a single reward. The bandit problem is easier in the sense that the learner knows which reward is associated with which action. In the general reinforcement learning scenario, a high reward or a low reward might have been caused by a recent action or by an action in the distant past. The term contextual bandits refers to the case where the action is taken in the context of some input variable that can inform the decision. For example, we at least know the user identity, and we want to pick an item", "d5f83a24-e1e3-4147-8b1d-c8ccbd22700b": "The entropy is given by (p \u2014 1) log(1 \u2014 p) \u2014 plogp. When p is near 0, the distribution is nearly deterministic, because the random variable is nearly always 0. Whenp is near 1, the distribution is nearly deterministic, because the random variable is nearly always 1. Whenp = 0.5, the entropy is maximal, because the distribution is uniform over the two outcomes. A quantity that is closely related to the KL divergence is the cross-entropy H(P,Q) = H(P)+ Dgr(P||Q), which is similar to the KL divergence but lacking the term on the left:  H(P,Q) = \u2014Exxp log Q(2). (3.51) Minimizing the cross-entropy with respect to Q is equivalent to minimizing the KL divergence, because @ does not participate in the omitted term. When computing many of these quantities, it is common to encounter expres- sions of the form 0 log0", "50db6259-4ec7-431d-b9e1-61fbeb4c4833": "For example, when processing images, it is useful to detect edges in the first layer of a convolutional network. The same edges appear more or less everywhere in the image, so it is practical to share parameters across the entire image. In some cases, we may not wish to share parameters across the entire image. For example, if we are processing images that are cropped to be centered on an individual\u2019s face, we probably want to extract different features at different locations\u2014the part of the network processing the top of the face needs to look for eyebrows, while the part of the network processing the bottom of the face needs to look for a chin. Convolution is not naturally equivariant to some other transformations, such as changes in the scale or rotation of an image. Other mechanisms are necessary for handling these kinds of transformations. Finally, some kinds of data cannot be processed by neural networks defined by matrix multiplication with a fixed-shape matrix. Convolution enables processing of some of these kinds of data. We discuss this further in section 9.7. 9.3 Pooling  A typical layer of a convolutional network consists of three stages (see figure 9.7)", "330a28ac-b725-4e84-b9c8-4fd60a73a8a1": "From a neuroscientific point of view, it is interesting to think of the softmax as a way to create a form of competition between the units that participate in it: the softmax outputs always sum to | so an increase in the value of one unit necessarily corresponds to a decrease in the value of others.\n\nThis is analogous to the lateral inhibition that is believed to exist between nearby neurons in the cortex. At the extreme (when the difference between the maximal qa; and the others is large in magnitude) it becomes a form of winner-take-all (one of the outputs is nearly 1, and the others are nearly 0). The name \u201csoftmax\u201d can be somewhat confusing. The function is more closely related to the arg max function than the max function. The term \u201csoft\u201d derives from the fact that the softmax function is continuous and differentiable. The arg max function, with its result represented as a one-hot vector, is not continuous or differentiable. The softmax function thus provides a \u201csoftened\u201d version of the arg max. The corresponding soft version of the maximum function is softmax(z) ! z. 183  CHAPTER 6", "91595849-8387-4eca-aa72-bc10bc7583e6": "None 0.705 Traditional 0.775 GANs 0.720 Neural + no loss 0.765 Neural + content loss 0.770 Neural + style 0.740 Control 0.710  MNIST 0's and 8\u2019s  Augmentation Val. acc. None 0.972 Neural + no loss 0.975 Neural + content loss 0.968  backpropagated to update Network-A. Additionally another loss function is incorpo- rated into Network-A to ensure that its outputs are similar to others within the class. Network-A uses a series of convolutional layers to produce the augmented image. The conceptual framework of Network-A can be expanded to use several Networks trained in parallel. Multiple Network-As could be very useful for learning class-specific aug- mentations via meta-learning (Fig. 30). Smart Augmentation is similar to SamplePairing  or mixed-examples in the sense that a combination of existing examples produces new ones. However, the mechanism of Smart Augmentation is much more sophisticated, using an adaptive CNN to derive new images rather than averaging pixels or hand-engineered image combinations. The Smart Augmentation technique was tested on the task of gender recognition", "8d568886-ab3d-46fb-86f0-d9de19cff818": "Ioffe and Szegedy  recommend  316  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  the latter. More specifically, XW + 6 should be replaced by a normalized version of XW. The bias term should be omitted because it becomes redundant with the @ parameter applied by the batch normalization reparametrization. The input to a layer is usually the output of a nonlinear activation function such as the rectified linear function in a previous layer. The statistics of the input are thus more non-Gaussian and less amenable to standardization by linear operations.\n\nIn convolutional networks, described in chapter 9, it is important to apply the same normalizing y, and o at every spatial location within a feature map, so that the statistics of the feature map remain the same regardless of spatial location. https://www.deeplearningbook.org/contents/optimization.html    8.7.2 Coordinate Descent  In some cases, it may be possible to solve an optimization problem quickly by breaking it into separate pieces", "96bf8f9a-f107-4680-9390-c0b9edf56891": "The following general procedure for solving such problems was given by and we shall also, without loss of generality, consider the case where f(\u03b8) > 0 for \u03b8 > \u03b8\u22c6 and f(\u03b8) < 0 for \u03b8 < \u03b8\u22c6, as is the case in Figure 2.10. The Robbins-Monro procedure then de\ufb01nes a sequence of successive estimates of the root \u03b8\u22c6 given by where z(\u03b8(N)) is an observed value of z when \u03b8 takes the value \u03b8(N). The coef\ufb01cients {aN} represent a sequence of positive numbers that satisfy the conditions It can then be shown  that the sequence of estimates given by (2.129) does indeed converge to the root with probability one. Note that the \ufb01rst condition (2.130) ensures that the successive corrections decrease in magnitude so that the process can converge to a limiting value.\n\nThe second condition (2.131) is required to ensure that the algorithm does not converge short of the root, and the third condition (2.132) is needed to ensure that the accumulated noise has \ufb01nite variance and hence does not spoil convergence. Now let us consider how a general maximum likelihood problem can be solved sequentially using the Robbins-Monro algorithm", "83c020d6-57d4-4730-962f-5b542fa2d56c": "However, adapting weighted importance sampling to function approximation is challenging and can probably only be done approximately with O(d) complexity . The Tree Backup algorithm (Section 7.5) shows that it is possible to perform some o\u21b5-policy learning without using importance sampling. This idea has been extended to the o\u21b5-policy case to produce stable and more e\ufb03cient methods by Munos, Stepleton, Harutyunyan, and Bellemare  and by Mahmood, Yu and Sutton . Another, complementary strategy is to allow the target policy to be determined in part by the behavior policy, in such a way that it never can be so di\u21b5erent from it to create large importance sampling ratios. For example, the target policy can be de\ufb01ned by reference to the behavior policy, as in the \u201crecognizers\u201d proposed by Precup et al. O\u21b5-policy learning is a tempting challenge, testing our ingenuity in designing stable and e\ufb03cient learning algorithms. Tabular Q-learning makes o\u21b5-policy learning seem easy, and it has natural generalizations to Expected Sarsa and to the Tree Backup algorithm", "d477723f-a850-4cc9-b555-fc21f1c2118e": "T., Glanz, F. H. UNH CMAC verison 2.1: The University of New Hampshire Implementation of the Cerebellar Model Arithmetic Computer - CMAC. Robotics Laboratory Technical Report, University of New Hampshire, Durham. Miller, S., Williams, R. J. Learning to control a bioreactor using a neural net Dyna-Q system. In Proceedings of the Seventh Yale Workshop on Adaptive and Learning Systems, pp. 167\u2013172. Center for Systems Science, Dunham Laboratory, Yale University, New Haven.\n\nMiller, W. T., Scalera, S. M., Kim, A. Neural network control of dynamic balance for a biped walking robot. In Proceedings of the Eighth Yale Workshop on Adaptive and Learning Systems, pp. 156\u2013161. Center for Systems Science, Dunham Laboratory, Yale University, New Haven. Minton, S. Quantitative results concerning the utility of explanation-based learning. Minsky, M. L", "4b297e9e-b1e4-48f8-9dc2-59894aa7d1c5": "In this book, a generative model either represents p(a) or can draw samples from it. Many variants of ICA only know how to transform between x and h but do not have any way of representing p(h), and thus do not impose a distribution over p(a). For example, many ICA variants aim to increase the sample kurtosis of h = W'z, because high kurtosis indicates that p(h) is non-Gaussian, but this is accomplished without explicitly representing p(h). This is because ICA is more often used as an analysis tool for separating signals, rather than for generating data or estimating its density. Just as PCA can be generalized to the nonlinear autoencoders described in chapter 14, ICA can be generalized to a nonlinear generative model, in which we use a nonlinear function f to generate the observed data. See Hyvarinen and Pajunen  for the initial work on nonlinear ICA and its successful use with ensemble learning by Roberts and Everson  and Lappalainen ef al", "75e7c740-1844-47c6-9605-54a55d3e4d2d": "The approach for setting the biases must be coordinated with the approach for setting the weights. Setting the biases to zero is compatible with most weight initialization schemes. There are a few situations where we may set some biases to nonzero values:  https://www.deeplearningbook.org/contents/optimization.html  e Ifa bias is for an output unit, then it is often beneficial to initialize the bias to obtain the right marginal statistics of the output. To do this, we assume that the initial weights are small enough that the output of the unit is determined  only by the bias. This justifies setting the bias to the inverse of the activation function applied to the marginal statistics of the output in the training set. For example, if the output is a distribution over classes, and this distribution is a highly skewed distribution with the marginal probability of class 7 given by element c; of some vector c, then we can set the bias vector b by solving the equation softmax(b) = c", "7a46707b-56d9-442b-9017-79467b855826": "So far, we have considered various approximation schemes for evaluating the Hessian matrix or its inverse.\n\nThe Hessian can also be evaluated exactly, for a network of arbitrary feed-forward topology, using extension of the technique of backpropagation used to evaluate \ufb01rst derivatives, which shares many of its desirable features including computational ef\ufb01ciency . It can be applied to any differentiable error function that can be expressed as a function of the network outputs and to networks having arbitrary differentiable activation functions. The number of computational steps needed to evaluate the Hessian scales like O(W 2). Similar algorithms have also been considered by Buntine and Weigend . Here we consider the speci\ufb01c case of a network having two layers of weights, for which the required equations are easily derived. We shall use indices i and i\u2032 Exercise 5.22 to denote inputs, indices j and j\u2032 to denoted hidden units, and indices k and k\u2032 to denote outputs. We \ufb01rst de\ufb01ne where En is the contribution to the error from data point n. The Hessian matrix for this network can then be considered in three separate blocks as follows", "da6ab905-bb6b-404c-b335-21f8be836737": "One way to reduce the cost of convolutional network training is to use features that are not trained in a supervised fashion. There are three basic strategies for obtaining convolution kernels without supervised training. One is to simply initialize them randomly. Another is to design them by hand, for example, by setting each kernel to detect edges at a certain orientation or scale. Finally, one can learn the kernels with an unsupervised criterion. For example, Coates et al. apply k-means clustering to small image patches, then use each learned centroid as a convolution kernel.\n\nIn Part III we describe many more unsupervised learning approaches. Learning the features with an unsupervised criterion allows them to be determined separately from the classifier layer at the top of the architecture. One can then extract the features for the entire training set just once, essentially constructing a new training set for the last layer. Learning the last layer is then typically a convex optimization problem, assuming the last layer is something like logistic regression or an SVM. Random filters often work surprisingly well in convolutional networks . Saxe et al", "f66d0ba6-754f-4c3f-bd39-793784a39b3b": "If the transition occurs again, then it will be from a state of estimated value \u21e111 to a state of estimated value \u21e122, for a TD error of \u21e111\u2014larger, not smaller, than before.\n\nIt will look even more like the \ufb01rst state is undervalued, and its value will be increased again, this time to \u21e112.1. This looks bad, and in fact with further updates w will diverge to in\ufb01nity. To see this de\ufb01nitively we have to look more carefully at the sequence of updates. The \u03b4t = Rt+1 + \u03b3\u02c6v(St+1,wt) \u2212 \u02c6v(St,wt) = 0 + \u03b32wt \u2212 wt = (2\u03b3 \u2212 1)wt, and the o\u21b5-policy semi-gradient TD(0) update (from (11.2)) is wt+1 = wt + \u21b5\u21e2t\u03b4tr\u02c6v(St,wt) = wt + \u21b5 \u00b7 1 \u00b7 (2\u03b3 \u2212 1)wt \u00b7 1 = Note that the importance sampling ratio, \u21e2t, is 1 on this transition because there is only one action available from the \ufb01rst state, so its probabilities of being taken under the target and behavior policies must both be 1", "0674fc6a-a85d-4e38-bbde-25b465c5755d": "In practice, a numerical  235  https://www.deeplearningbook.org/contents/regularization.html    CHAPTER 7 REGULARIZATION FOR DEEP LEARNING  implementation of gradient descent will eventually reach sufficiently large weights to cause numerical overflow, at which point its behavior will depend on how the programmer has decided to handle values that are not real numbers. Most forms of regularization are able to guarantee the convergence of iterative methods applied to underdetermined problems. For example, weight decay will cause gradient descent to quit increasing the magnitude of the weights when the slope of the likelihood is equal to the weight decay coefficient. The idea of using regularization to solve underdetermined problems extends beyond machine learning. The same idea is useful for several basic linear algebra problems. As we saw in section 2.9, we can solve underdetermined linear equations using the Moore-Penrose pseudoinverse. Recall that one definition of the pseudoinverse X* of a matrix X is  Xt= lim (XX tol) tx, (7.29)  We can now recognize equation 7.29 as performing linear regression with weight decay. Specifically, equation 7.29 is the limit of equation 7.17 as the regularization coefficient shrinks to zero", "e48f7302-5ced-44e3-b087-9e7d13209634": "Again, we can identify two approaches. In the \ufb01rst, we simply set the required derivatives of the marginal likelihood to zero and obtain the following re-estimation equations Exercise 7.12 where mi is the ith component of the posterior mean m de\ufb01ned by (7.82). The quantity \u03b3i measures how well the corresponding parameter wi is determined by the data and is de\ufb01ned by Section 3.5.3 in which \u03a3ii is the ith diagonal component of the posterior covariance \u03a3 given by (7.83).\n\nLearning therefore proceeds by choosing initial values for \u03b1 and \u03b2, evaluating the mean and covariance of the posterior using (7.82) and (7.83), respectively, and then alternately re-estimating the hyperparameters, using (7.87) and (7.88), and re-estimating the posterior mean and covariance, using (7.82) and (7.83), until a suitable convergence criterion is satis\ufb01ed. The second approach is to use the EM algorithm, and is discussed in Section 9.3.4. These two approaches to \ufb01nding the values of the hyperparameters that maximize the evidence are formally equivalent. Numerically, however, it is found Exercise 9.23 that the direct optimization approach corresponding to (7.87) and (7.88) gives somewhat faster convergence", "dc2a8e6d-beba-4ce4-8aca-72346fdde224": "In some forms of auto- regressive networks, such as NADE , described in section 20.10.10, we can introduce a form of parameter sharing that brings both a statistical advantage (fewer unique parameters) and a computational advantage (less computation). This is one more instance of the recurring deep learning motif of reuse of features. 20.10.8 Linear Auto-Regressive Networks  The simplest form of auto-regressive network has no hidden units and no sharing of parameters or features. Each P(x; | 2j_1,..., 21) is parametrized as a linear model (linear regression for real-valued data, logistic regression for binary data, softmax regression for discrete data). This model was introduced by Frey  and has O(d?) parameters when there are d variables to model. It is illustrated in figure 20.8. If the variables are continuous, a linear auto-regressive model is merely another way to formulate a multivariate Gaussian distribution, capturing linear pairwise interactions between the observed variables. https://www.deeplearningbook.org/contents/generative_models.html    Linear auto-regressive networks are essentially the generalization of linear classification methods to generative modeling", "428720bf-5b67-4396-b9e8-87cf73f7156d": "We \ufb01nd that BERTLARGE signi\ufb01cantly outperforms BERTBASE across all tasks, especially those with very little training data. The effect of model size is explored more thoroughly in Section 5.2. 9The GLUE data set distribution does not include the Test labels, and we only made a single GLUE evaluation server submission for each of BERTBASE and BERTLARGE. 10https://gluebenchmark.com/leaderboard Wikipedia containing the answer, the task is to predict the answer text span in the passage. As shown in Figure 1, in the question answering task, we represent the input question and passage as a single packed sequence, with the question using the A embedding and the passage using the B embedding. We only introduce a start vector S \u2208 RH and an end vector E \u2208 RH during \ufb01ne-tuning.\n\nThe probability of word i being the start of the answer span is computed as a dot product between Ti and S followed by a softmax over all of the words in the paragraph: Pi = eS\u00b7Ti \ufffd j eS\u00b7Tj . The analogous formula is used for the end of the answer span", "6b6ac2e5-05b7-4bfa-b213-979290ba8f34": "We saw that in going from a directed to an undirected representation we had to discard some conditional independence properties from the graph. Of course, we could always trivially convert any distribution over a directed graph into one over an undirected graph by simply using a fully connected undirected graph. This would, however, discard all conditional independence properties and so would be vacuous. The process of moralization adds the fewest extra links and so retains the maximum number of independence properties.\n\nWe have seen that the procedure for determining the conditional independence properties is different between directed and undirected graphs. It turns out that the two types of graph can express different conditional independence properties, and it is worth exploring this issue in more detail. To do so, we return to the view of a speci\ufb01c (directed or undirected) graph as a \ufb01lter, so that the set of all possible Section 8.2 distributions over the given variables could be reduced to a subset that respects the conditional independencies implied by the graph. A graph is said to be a D map (for \u2018dependency map\u2019) of a distribution if every conditional independence statement satis\ufb01ed by the distribution is re\ufb02ected in the graph", "eb473eb2-24e5-4720-810a-884f1e642e9b": "Its input at time t is a feature vector x(St); its output, At, is a random variable having two values, 0 and 1, with Pr{At = 1} = Pt and Pr{At = 0} = 1 \u2212 Pt (the Bernoulli distribution). Let h(s, 0, \u2713) and h(s, 1, \u2713) be the preferences in state s for the unit\u2019s two actions given policy parameter \u2713", "5a15a49f-04e0-4e5a-b5f2-d23f1a305365": "Using this synthetically generated data, a convolutional network is able to learn to map z descriptions of the content of an image to # approximations of rendered images. This suggests that contemporary differentiable generator networks have sufficient model capacity to be good generative models, and that contemporary optimization algorithms have the ability to fit them.\n\nThe difficulty lies in determining how to train generator networks when the value of z for each x is not fixed and known ahead of each time. 692  CHAPTER 20. DEEP GENERATIVE MODELS  The following sections describe several approaches to training differentiable generator nets given only training samples of a. 20.10.3. Variational Autoencoders  The variational autoencoder, or VAE , is a directed model that uses learned approximate inference and can be trained purely with gradient-based methods. To generate a sample from the model, the VAE first draws a sample z from the code distribution pmodei (2). The sample is then run through a differentiable generator network g(z). Finally, w is sampled from a distribution Pmodei(x; g(z)) = Pmodel(x | z)", "497987a1-9669-485d-8292-c61e3e998be4": "Given an observed data set {xn}, where n = 1, . , N, derive the E and M step equations of the EM algorithm for optimizing the mixing coef\ufb01cients \u03c0k and the component parameters \u00b5kij of this distribution by maximum likelihood. 9.20 (\u22c6) www Show that maximization of the expected complete-data log likelihood function (9.62) for the Bayesian linear regression model leads to the M step reestimation result (9.63) for \u03b1. 9.21 (\u22c6 \u22c6) Using the evidence framework of Section 3.5, derive the M-step re-estimation equations for the parameter \u03b2 in the Bayesian linear regression model, analogous to the result (9.63) for \u03b1. 9.22 (\u22c6 \u22c6) By maximization of the expected complete-data log likelihood de\ufb01ned by (9.66), derive the M step equations (9.67) and (9.68) for re-estimating the hyperparameters of the relevance vector machine for regression", "a7f93cf1-2c2e-48e6-9dfe-cf64197ea5a9": "This is unrealistic as a model of the monkey\u2019s situation because the monkey would likely learn these predictions at the same time that it is learning to act correctly (as would a reinforcement learning algorithm that learns policies as well as value functions, such as an actor\u2013critic algorithm), but this scenario is simpler to describe than one in which a policy and a value function are learned simultaneously.\n\nNow imagine that the agent\u2019s experience divides into multiple trials, in each of which the same sequence of states repeats, with a distinct state occurring on each time step during the trial. Further imagine that the return being predicted is limited to the return over a trial, which makes a trial analogous to a reinforcement learning episode as we have de\ufb01ned it. In reality, of course, the returns being predicted are not con\ufb01ned to single trials, and the time interval between trials is an important factor in determining what an animal learns. This is true for TD learning as well, but here we assume that returns do not accumulate over multiple trials. Given this, then, a trial in experiments like those conducted by Schultz and colleagues is equivalent to an episode of reinforcement learning", "38ca7a30-5160-4403-b9c6-bf1b465d683e": "This is much like the classical problem of tomographic reconstruction, used in medical imaging for example, in which a two-dimensional distribution is to be reconstructed from an number of one-dimensional averages.\n\nHere there are far fewer line measurements than in a typical tomography application. On the other hand the range of geometrical con\ufb01gurations is much more limited, and so the con\ufb01guration, as well as the phase fractions, can be predicted with reasonable accuracy from the densitometer data. For safety reasons, the intensity of the gamma beams is kept relatively weak and so to obtain an accurate measurement of the attenuation, the measured beam intensity is integrated over a speci\ufb01c time interval. For a \ufb01nite integration time, there are random \ufb02uctuations in the measured intensity due to the fact that the gamma beams comprise discrete packets of energy called photons. In practice, the integration time is chosen as a compromise between reducing the noise level (which requires a long integration time) and detecting temporal variations in the \ufb02ow (which requires a short integration time)", "d9684eda-fb8e-48b8-99a2-a1be50735812": "More generally, it is straightforward to obtain the required expectations for any member of the exponential family, provided it can be normalized, because the expected statistics can be related to the derivatives of the normalization coef\ufb01cient, as given by (2.226). The EP approximation is illustrated in Figure 10.14. From (10.193), we see that the revised factor \ufffdfj(\u03b8) can be found by taking qnew(\u03b8) and dividing out the remaining factors so that where we have used (10.195). The coef\ufb01cient K is determined by multiplying both sides of (10.199) by q\\i(\u03b8) and integrating to give Combining this with (10.197), we then see that K = Zj and so can be found by evaluating the integral in (10.197).\n\nIn practice, several passes are made through the set of factors, revising each factor in turn. The posterior distribution p(\u03b8|D) is then approximated using (10.191), and the model evidence p(D) can be approximated by using (10.190) with the factors fi(\u03b8) replaced by their approximations \ufffdfi(\u03b8)", "7db1a833-846f-4565-9b2e-3b2bc082d77b": "Bridge sampling estimates the ratio Z,/Z as the ratio of the expected impor- tance weights between po and pe and between a and px:  Px a} P(x ?) ay? is S . (18.62) (k) ta Bole ; kai Pi(@j) If the bridge distribution p, is chosen carefully to have a large overlap of support with both pp and p,, then bridge sampling can allow the distance between two distributions (or more formally, Dx, (po||p1)) to be much larger than with standard  importance sampling. It can be shown that the optimal bridging distribution is given by ph?) (x) ine ay).\n\nwhere r = Z/Z. At first, this appears to be an unworkable solution as it would seem to require the very quantity we are trying to estimate, Z,/Z. However, it is possible to start with a coarse estimate of r and use the resulting bridge distribution to refine our estimate iteratively . That is, we  iteratively reestimate the ratio and use each iteration to update the value of r", "535d4e64-eb40-41fe-8b32-7faa111703a4": "Now, we can see Ez\u223cp(z) \u2212 Ez\u223cp(z) \u2212 \u27e8(\u03b8 \u2212 \u03b80), Ez\u223cp(z)\u27e9 \ufffdf(g\u03b8(z)) \u2212 f(g\u03b80(z)) \u2212 \u27e8(\u03b8 \u2212 \u03b80), \u2207\u03b8f(g\u03b8(z))|\u03b80\u27e9 By di\ufb00erentiability, the term inside the integral converges p(z)-a.e. to 0 as \u03b8 \u2192 \u03b80. Furthermore, \u2225f(g\u03b8(z)) \u2212 f(g\u03b80(z)) \u2212 \u27e8(\u03b8 \u2212 \u03b80), \u2207\u03b8f(g\u03b8(z))|\u03b80\u27e9 and since Ez\u223cp(z) < +\u221e by assumption 1, we get by dominated convergence that Equation 6 converges to 0 as \u03b8 \u2192 \u03b80 so for almost every \u03b8, and in particular when the right hand side is well de\ufb01ned. Note that the mere existance of the left hand side (meaning the di\ufb00erentiability a.e. of Ez\u223cp(z)) had to be proven, which we just did", "70b28632-18b1-43e4-a4cd-6179b5231e1f": "found that using Exercise 5.4 the cross-entropy error function instead of the sum-of-squares for a classi\ufb01cation problem leads to faster training as well as improved generalization. If we have K separate binary classi\ufb01cations to perform, then we can use a network having K outputs each of which has a logistic sigmoid activation function. Associated with each output is a binary class label tk \u2208 {0, 1}, where k = 1, . .\n\n, K. If we assume that the class labels are independent, given the input vector, then the conditional distribution of the targets is Taking the negative logarithm of the corresponding likelihood function then gives the following error function Exercise 5.5 where ynk denotes yk(xn, w). Again, the derivative of the error function with respect to the activation for a particular output unit takes the form (5.18) just as in the Exercise 5.6 regression case. It is interesting to contrast the neural network solution to this problem with the corresponding approach based on a linear classi\ufb01cation model of the kind discussed in Chapter 4. Suppose that we are using a standard two-layer network of the kind shown in Figure 5.1", "654b1c43-4ec1-42eb-9bf1-17f44a3e0d2a": "With slight modification, this approach can also work using an extra output value in the neural language model\u2019s softmax layer, rather than a separate sigmoid unit.\n\nAn obvious disadvantage of the short list approach is that the potential gener- alization advantage of the neural language models is limited to the most frequent words, where, arguably, it is the least useful. This disadvantage has stimulated the exploration of alternative methods to deal with high-dimensional outputs, described below. 461  CHAPTER 12. APPLICATIONS  https://www.deeplearningbook.org/contents/applications.html    12.4.3.2 Hierarchical Softmax  A classical approach  to reducing the computational burden of high-dimensional output layers over large vocabulary sets V is to decompose probabilities hierarchically. Instead of necessitating a number of computations proportional to |Y| (and also proportional to the number of hidden units, n;), the |V| factor can be reduced to as low as log |V|. Bengio  and Morin and Bengio  introduced this factorized approach to the context of neural language models. One can think of this hierarchy as building categories of words, then categories of categories of words, then categories of categories of categories of words, and so on", "7d70ede0-f81b-4ddd-9e6c-702ab8cc9f8f": "If this random walk is tuned to preserve norms, then feedforward networks can mostly avoid the vanishing and exploding gradients problem that arises when the same weight matrix is used at each step, as described in section 8.2.5. Unfortunately, these optimal criteria for initial weights often do not lead to optimal performance. This may be for three different reasons. First, we may be using the wrong criteria\u2014it may not actually be beneficial to preserve the norm of a signal throughout the entire network. Second, the properties imposed at initialization may not persist after learning has begun to proceed. Third, the criteria might succeed at improving the speed of optimization but inadvertently increase generalization error.\n\nIn practice, we usually need to treat the scale of the weights as a hyperparameter whose optimal value lies somewhere roughly near but not exactly equal to the theoretical predictions. One drawback to scaling rules that set all the initial weights to have the same  standard deviation, such as +, is that every individual weight becomes extremely  Vv  ae fAn4nN +  https://www.deeplearningbook.org/contents/optimization.html    small when the layers become large", "02b3c520-4f61-4ba1-b7ed-8677c689a8d4": "(We can assume the right MRP starts in one of two states at random with equal probability.) Thus, even given even an in\ufb01nite amount of data, it would not be possible to tell which of these two MRPs was generating it. In particular, we could not tell if the MRP has one state or two, is stochastic or deterministic. These things are not learnable. This pair of MRPs also illustrates that the VE objective (9.1) is not learnable. If \u03b3 = 0, then the true values of the three states (in both MRPs), left to right, are 1, 0, and 2. Suppose w = 1. Then the VE is 0 for the left MRP and 1 for the right MRP. Because the VE is di\u21b5erent in the two problems, yet the data generated has the same distribution, the VE cannot be learned. The VE is not a unique function of the data distribution. And if it cannot be learned, then how could the VE possibly be useful as an objective for learning?\n\nIf an objective cannot be learned, it does indeed draw its utility into question. In the case of the VE, however, there is a way out", "ae904337-d5ee-487d-bc53-a88d468bf6ea": "use reinforcement learning techniques (policy gradient) to learn a form of conditional dropout on blocks of hidden units and get an actual reduction in computational cost without negatively affecting the quality of the approximation. Another kind of dynamic structure is a switch, where a hidden unit can receive input from different units depending on the context. This dynamic routing approach can be interpreted as an attention mechanism . So far, the use of a hard switch has not proven effective on large-scale applications. Contemporary approaches instead use a weighted average over many possible inputs, and thus do not achieve all the possible computational benefits of dynamic structure. Contemporary attention mechanisms are described in section 12.4.5.1. One major obstacle to using dynamically structured systems is the decreased degree of parallelism that results from the system following different code branches for different inputs. This means that few operations in the network can be described as matrix multiplication or batch convolution on a minibatch of examples. We can write more specialized subroutines that convolve each example with different kernels or multiply each row of a design matrix by a different set of columns of weights. Unfortunately, these more specialized subroutines are difficult to implement efficiently", "7196072f-0520-4df4-b60e-a2d64ad7ad68": "These results demonstrate that the discriminative model effectively learns from the additional signal contained in Snorkel\u2019s probabilistic training labels over simpler modeling strategies. One of the most exciting potential advantages of using a programmatic supervision approach as in Snorkel is the ability to incorporate additional unlabeled data, which is often cheaply available. Recently, proposed theory characterizing the data programming approach used predicts that discriminative model generalization risk (i.e., predictive performance on the held-out test set) should improve with additional unlabeled data, at the same asymptotic rate as in traditional supervised methods with respect to labeled data . That is, with a \ufb01xed amount of effort writing labeling functions, we could then get improved discriminative model performance simply by adding more unlabeled data. Wevalidatethistheoreticalpredictionempiricallyonthree of our datasets (Fig. 11). We see that by adding additional unlabeled data\u2014in these datasets, candidates from addiFig.\n\n11 The increase in end model performance (measured in F1 score) for different amounts of unlabeled data, measured in the number of candidates", "537b113a-b928-4f01-b7a0-1f27f34dfd9f": "We begin by considering the binomial and multinomial distributions for discrete random variables and the Gaussian distribution for continuous random variables. These are speci\ufb01c examples of parametric distributions, so-called because they are governed by a small number of adaptive parameters, such as the mean and variance in the case of a Gaussian for example. To apply such models to the problem of density estimation, we need a procedure for determining suitable values for the parameters, given an observed data set. In a frequentist treatment, we choose speci\ufb01c values for the parameters by optimizing some criterion, such as the likelihood function. By contrast, in a Bayesian treatment we introduce prior distributions over the parameters and then use Bayes\u2019 theorem to compute the corresponding posterior distribution given the observed data.\n\nWe shall see that an important role is played by conjugate priors, that lead to posterior distributions having the same functional form as the prior, and that therefore lead to a greatly simpli\ufb01ed Bayesian analysis. For example, the conjugate prior for the parameters of the multinomial distribution is called the Dirichlet distribution, while the conjugate prior for the mean of a Gaussian is another Gaussian", "a99b501a-94a7-427d-8a65-d0168ef4742a": "12.9 I\\n ~I\"'tfat\"\" oIlt>e II\"\"\"fative vi&w oI1t>e p<ot>abi! ;st\", PeA modeIfof\" two-dimensiooal <!ala space and a on&-<lirnent.ionallat/l<1t space, An Ob&erved <!ala point x Is generated by first drawing a value i fof 1t>e Iat&n1 vafiatlle /f(lm ~s prior dist,~t\"\" P(~) and Itlen drawing a val\"\" fof x lrom an iSO/fopK: Gaussian distr~t\"\" (iijust,al&(l by the red cir<:ie's) having mean wi+\" and coY8r1.once ,,'1 The l/f&er\\ ellips.&$ show l!le density \"\"\"toors!of the marg'''''1 dis1r1bulion PIx)", "d84e1f0e-61f2-4758-8fd9-4e02607cb632": "1llt.1f \" .If \"'\"\"\"* CO\\-.ullCC mMfU ,n (~ .If*'e ,~l\"\" by , and ,l~ \",,,,n'\"MOl\" opan,ion i' \u00ablined by ; = 1... ,. M. Our goal is 10 soh'\" lhis eigen\"lIlue problem WilhoUl ha\"inlllO work . \"plici,ly in ,he f.lIture 'pace.\n\nFrom !he definilion of C", "ca13775c-bed9-4ee8-b035-d89bea11c9e6": "They observed without proof that these results extend to deeper networks without nonlinearities. The output of such networks is a linear function of their input, but they are useful o study as a model of nonlinear neural networks because their loss function is a nonconvex function of their parameters. Such networks are essentially just multiple matrices composed together. Saxe et al. provided exact solutions  showed experimentally that real neural networks also have loss functions that contain very many high-cost saddle points. Choromanska et al. provided additional theoretical arguments, showing that another class of high-dimensional random functions related to neural networks does so as well. What are the implications of the proliferation of saddle points for training algorithms? For first-order optimization, algorithms that use only gradient infor- mation, the situation is unclear.\n\nThe gradient can often become very small near a saddle point. On the other hand, gradient descent empirically seems able to escape saddle points in many cases. Goodfellow e\u00a2 al. provided visualizations of several learning trajectories of state-of-the-art neural networks, with an example given in figure 8.2", "6a0494fe-7818-43e6-851f-4837ce1deda1": "This work may not be translated or copied in whole or in part without the written permission of the publisher (Springer Science+Business Media, LLC, 233 Spring Street, New York, NY 10013, USA), except for brief excerpts in connection with reviews or scholarly analysis. Use in connection with any form of information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed is forbidden.\n\nThe use in this publication of trade names, trademarks, service marks, and similar terms, even if they are not identified as such, is not to be taken as an expression of opinion as to whether or not they are subject to proprietary rights. Total eclipse of the sun, Antalya, Turkey, 29 March 2006. Pattern recognition has its origins in engineering, whereas machine learning grew out of computer science. However, these activities can be viewed as two facets of the same \ufb01eld, and together they have undergone substantial development over the past ten years. In particular, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic models", "0bbfffcc-528b-4228-ab89-e1e2c91bd038": "After warming up with the supervised MLE for n iterations, the approach then changes the experience function to the data augmentation-based one f\u03c4=n = fdata-aug de\ufb01ned in Equation 4.8, which introduces noise and task-speci\ufb01c evaluation information into the training. As revealed in Section 4.1.4, this stage in effect corresponds to the reward-augmented maximum likelihood (RAML) algorithm . The approach proceeds by further annealing the balancing weights, speci\ufb01cally by gradually increasing \u03b2\u03c4 from \u03f5 to 1 as \u03c4 increases. The increase of \u03b2 e\ufb00ectively gets the learning closer to a reinforcement-style learning (notice that the policy gradient algorithm has \u03b1 = \u03b2 = 1 as described in Section 4.3.1). Intuitively, the target model p\u03b8(t), as part of the q(\u03c4+1) solution in the teacher step weighted by the increasing weight \u03b2 (see Equation 3.3), serves to produce more data samples. Those samples are weighted by the experience function and used for updating the target model further, simulating the policy gradients.\n\nTherefore, the whole learning procedure spans multiple paradigms of learning (from supervised MLE, RAML, to reinforcement learning) that increasingly introduces more noise and exploration for improved robustness", "d5283fc1-bb5f-4582-bb7c-982ced7ae888": "Stochastic learning automata were foreshadowed by earlier work in psychology, beginning with William Estes\u2019  e\u21b5ort toward a statistical theory of learning and further developed by others . The statistical learning theories developed in psychology were adopted by researchers in economics, leading to a thread of research in that \ufb01eld devoted to reinforcement learning. This work began in 1973 with the application of Bush and Mosteller\u2019s learning theory to a collection of classical economic models .\n\nOne goal of this research was to study arti\ufb01cial agents that act more like real people than do traditional idealized economic agents . This approach expanded to the study of reinforcement learning in the context of game theory. Reinforcement learning in economics developed largely independently of the early work in reinforcement learning in arti\ufb01cial intelligence, though game theory remains a topic of interest in both \ufb01elds (beyond the scope of this book). Camerer  discusses the reinforcement learning tradition in economics, and Now\u00b4e, Vrancx, and De Hauwere  provide an overview of the subject from the point of view of multi-agent extensions to the approach that we introduce in this book", "0db457fc-f9fd-4732-9ea7-226b74c47f10": "Context- specific independences are independences that are present dependent on the value of some variables in the network.\n\nFor example, consider a model of three binary variables: a, b and c. Suppose that when a is 0, b and c are independent, but when a is 1, b is deterministically equal to c. Encoding the behavior when a = 1 requires an edge connecting b and c. The graph then fails to indicate that b and c are independent when a = 0. In general, a graph will never imply that an independence exists when it does  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    not. However, a graph may fail to encode an independence. 16.2.6 Converting between Undirected and Directed Graphs  We often refer to a specific machine learning model as being undirected or directed. For example, we typically refer to RBMs as undirected and sparse coding as directed. This choice of wording can be somewhat misleading, because no probabilistic model is inherently directed or undirected", "a7a027e0-630c-47aa-b03b-0351f15342ea": "In general, we can think of a sigmoid belief network as having a vector of binary states s, with each element of the state influenced by its ancestors:  p(si) =o | S > Wyisj +h: | - (20.70)  The most common structure of sigmoid belief network is one that is divided into many layers, with ancestral sampling proceeding through a series of many hidden layers and then ultimately generating the visible layer. This structure is very similar to the deep belief network, except that the units at the beginning of the sampling process are independent from each other, rather than sampled from a restricted Boltzmann machine. Such a structure is interesting for a variety of reasons.\n\nOne is that the structure is a universal approximator of probability distributions over the visible units, in the sense that it can approximate any probability distribution over binary variables arbitrarily well, given enough depth, even if the width of the individual layers is restricted to the dimensionality of the visible layer . While generating a sample of the visible units is very efficient in a sigmoid belief network, most other operations are not. Inference over the hidden units given the visible units is intractable", "822b1803-e49f-49a6-b62b-aa6dad67f2bd": "In this approach the sigmoid i  trained to maximize the log-probability of the correct prediction as to whether the  sequence ends or continues at each time step. Another way to determine the sequence length 7 is to add an extra output to the model that predicts the integer 7 itself. The model can sample a value of 7 and then sample 7 steps worth of data. This approach requires adding an extra input to the recurrent update at each time step so that the recurrent update is aware of whether it is near the end of the generated sequence. This extra input can either consist of the value of + or can consist of + \u2014 t, the number of remaining time steps. Without this extra input, the RNN might generate sequences that end abruptly, such as a sentence that ends before it is complete.\n\nThis approach is based on the decomposition  P(a\u2122,...,a) = P(r)P(a\u2122,...,2 | 7). (10.34)  384  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  The strategy of predicting 7 directly is used, for example, by Goodfellow e# al", "79421cdc-eeda-451f-adcf-f12e3ffc4f61": "Another very popular idea is to form a mixture model containing higher-order and lower-order n-gram models, with the higher-order models providing more capacity and the lower-order models being more likely to avoid counts of zero. Back-off methods look up the lower-order n-grams if the frequency of the context x~1,...,2+~-n+41 is too small to use the higher-order model. More formally, they estimate the distribution over x; by using contexts Y4-n+k,---;@+-1, for increasing k, until a sufficiently reliable estimate is found. Classical n-gram models are particularly vulnerable to the curse of dimension- ality. There are |V|\" possible n-grams and |V| is often very large. Even with a massive training set and modest n, most n-grams will not occur in the training set. One way to view a classical n-gram model is that it is performing nearest neighbor  yo4 T ad ton 1", "3e16f078-851e-4aed-be1f-60ec856014fe": "Shorten and Khoshgoftaar J Big Data  6:60   \u201cdeconv\u201d conv project & |, \u201cdeconv\u201d = conv reshape \u201cdeconv\u201d Leesa om Real | I} a axs xe oS : 100 8x8 16x16 @si2 e104 @s12 mone 32x32 @128 ene @2s6 32X32 64X64X1 @128 @6a lesion ie J J Y T Generator Discriminator Fig. 18 Complete DCGAN architecture used by Frid-Adar et al. to generate liver lesion images  Frid-Adar et al. tested the effectiveness of using DCGANs to generate liver lesion medical images. They use the architecture pictured above to generate 64 x 64 x 1 size images of liver lesion CT scans", "881298e6-bd12-4d25-aef2-ac22bdee79be": "The PCL-based training updates Q-values of all tokens at once through a connection between the value function and the induced policy. More speci\ufb01cally, Nachum et al. showed that the optimal policy \u03c0\u2217 (Eq.3) and the optimal state value function V \u2217 (Eq.5) in SQL must satisfy the following consistency property for all states and actions: V \u2217 (st) \u2212 \u03b3V \u2217 (st+1) = rt \u2212 log \u03c0\u2217 (at|st) , \u2200st, at. (6) Accordingly, the PCL-based training attempts to encourage the satisfaction of the consistency with the following regression objective LSQL, PCL(\u03b8): (7) where \u03c0\u03b8 is the induced policy de\ufb01ned in Eq. (4); V\u00af\u03b8 is de\ufb01ned similarly as in Eq.\n\n(5) but depends on the target Q\u00af\u03b8 network (i.e., a slow copy of the Q\u03b8 to be learned), and recall that \u03c0\u2032 is an arbitrary behavior policy (e.g., data distribution). Please see Figure 2 (left) for an illustration", "960b75e8-01bf-445d-8ee0-4953f2342e5e": "(Right)Regularized GCN, with \\ > 0, draws examples toward the sphere but does not completely discard the variation in their norm. We leave s and \u20ac the same as before.\n\nmotivates introducing a small positive regularization parameter to bias the estimate of the standard deviation. Alternately, one can constrain the denominator to be at least \u00ab. Given an input image X, GCN produces an output image X\u2019, defined such that  Xi j,k \u2014X  7  ijk \u2014 5 \u2014 max {6 At Bre Lint Dj Lok (Xi,j,k\u2014 X) \\  (12.3)  Datasets consisting of large images cropped to interesting objects are unlikely to contain any images with nearly constant intensity. In these cases, it is safe to practically ignore the small denominator problem by setting \\ = 0 and avoid division by 0 in extremely rare cases by setting \u20ac to an extremely low value like 10-8. This is the approach used by Goodfellow ef al. on the CIFAR-10 dataset. Small images cropped randomly are more likely to have nearly constant intensity, making aggressive regularization more useful. Coates ef al", "6f28efaa-0dbf-49b7-87d2-095a5a9cbf83": "Constructing d-dimensional feature vectors to represent states is the same as selecting a set of d basis functions.\n\nFeatures may be de\ufb01ned in many di\u21b5erent ways; we cover a few possibilities in the next sections. It is natural to use SGD updates with linear function approximation. The gradient of the approximate value function with respect to w in this case is Because it is so simple, the linear SGD case is one of the most favorable for mathematical analysis. Almost all useful convergence results for learning systems of all kinds are for linear (or simpler) function approximation methods. In particular, in the linear case there is only one optimum (or, in degenerate cases, one set of equally good optima), and thus any method that is guaranteed to converge to or near a local optimum is automatically guaranteed to converge to or near the global optimum. For example, the gradient Monte Carlo algorithm presented in the previous section converges to the global optimum of the VE under linear function approximation if \u21b5 is reduced over time according to the usual conditions", "d4c1b974-8b3e-4276-b426-c3684076caef": "Thus x will always appear in the set of conditioning variables, and so from now on we will drop the explicit x from expressions such as p(t|x, w, \u03b2) in order to keep the notation uncluttered.\n\nTaking the logarithm of the likelihood function, and making use of the standard form (1.46) for the univariate Gaussian, we have where the sum-of-squares error function is de\ufb01ned by Having written down the likelihood function, we can use maximum likelihood to determine w and \u03b2. Consider \ufb01rst the maximization with respect to w. As observed already in Section 1.2.5, we see that maximization of the likelihood function under a conditional Gaussian noise distribution for a linear model is equivalent to minimizing a sum-of-squares error function given by ED(w). The gradient of the log likelihood function (3.11) takes the form which are known as the normal equations for the least squares problem. Here \u03a6 is an N\u00d7M matrix, called the design matrix, whose elements are given by \u03a6nj = \u03c6j(xn), so that is known as the Moore-Penrose pseudo-inverse of the matrix \u03a6", "dcc9b6bd-5857-431c-9540-221f65595a91": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3606\u2013 3613. IEEE, 2014. Cubuk, E. D., Zoph, B., Mane, D., Vasudevan, V., and Le, Q. V. Autoaugment: Learning augmentation strategies from data. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 113\u2013123, 2019. Doersch, C. and Zisserman, A. Multi-task self-supervised visual learning. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2051\u20132060, 2017. Doersch, C., Gupta, A., and Efros, A. A. Unsupervised visual representation learning by context prediction. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1422\u20131430, 2015. Donahue, J. and Simonyan, K. Large scale adversarial representation learning. In Advances in Neural Information Processing Systems, pp", "c13cab25-a3f1-4b08-8a04-73db83427cfd": "The \ufb01rst stage in applying the Laplace framework to this model is to initialize the hyperparameter \u03b1, and then to determine the parameter vector w by maximizing the log posterior distribution. This is equivalent to minimizing the regularized error function E(w) = \u2212 ln p(D|w) + \u03b1 and can be achieved using error backpropagation combined with standard optimization algorithms, as discussed in Section 5.3. Having found a solution wMAP for the weight vector, the next step is to evaluate the Hessian matrix H comprising the second derivatives of the negative log likelihood function. This can be done, for instance, using the exact method of Section 5.4.5, or using the outer product approximation given by (5.85). The second derivatives of the negative log posterior can again be written in the form (5.166), and the Gaussian approximation to the posterior is then given by (5.167).\n\nTo optimize the hyperparameter \u03b1, we again maximize the marginal likelihood, which is easily shown to take the form Exercise 5.41 where the regularized error function is de\ufb01ned by in which yn \u2261 y(xn, wMAP)", "2c245e88-17b6-4ada-b21c-ac9c2c0fef0d": "2th image => \u2014> | |-\u2014 fh \u2014\u2014> J triage 128D 128D Be 1 th image ae 2048D Se \\-th image \u2014 EE  fo(x) 128D Unit Sphere  Let v = f(x) be an embedding function to learn and the vector is normalized to have |v| = 1.A non-parametric classifier predicts the probability of a sample v belonging to class 2 with a temperature parameter T:  P(C =i|\\v) =  Instead of computing the representations for all the samples every time, they implement an Memory Bank for storing sample representation in the database from past iterations. Let  V = {v,} be the memory bank and f; = f(x;) be the feature generated by forwarding the network. We can use the representation from the memory bank v; instead of the feature forwarded from the network f\u00a3; when comparing pairwise similarity. The denominator theoretically requires access to the representations of all the samples, but that is too expensive in practice. Instead we can estimate it via Monte Carlo approximation using a random subset of M indices {j,", "9b84d343-dfb2-4742-be76-575ee9deddb5": "Would the variance of the estimator still be in\ufb01nite? Why or why not? \u21e4 Monte Carlo prediction methods can be implemented incrementally, on an episode-byepisode basis, using extensions of the techniques described in Chapter 2 (Section 2.4). Whereas in Chapter 2 we averaged rewards, in Monte Carlo methods we average returns. In all other respects exactly the same methods as used in Chapter 2 can be used for onpolicy Monte Carlo methods. For o\u21b5-policy Monte Carlo methods, we need to separately consider those that use ordinary importance sampling and those that use weighted importance sampling. In ordinary importance sampling, the returns are scaled by the importance sampling ratio \u21e2t:T (t)\u22121 (5.3), then simply averaged, as in (5.5).\n\nFor these methods we can again use the incremental methods of Chapter 2, but using the scaled returns in place of the rewards of that chapter. This leaves the case of o\u21b5-policy methods using weighted importance sampling. Here we have to form a weighted average of the returns, and a slightly di\u21b5erent incremental algorithm is required. Suppose we have a sequence of returns G1, G2,", "3f50b4c4-70c8-4730-b21c-f4a9898b3436": "Chapter 16  Structured Probabilistic Models for Deep Learning  Deep learning draws on many modeling formalisms that researchers can use to guide their design efforts and describe their algorithms. One of these formalisms is the idea of structured probabilistic models. We discuss structured probabilistic models briefly in section 3.14. That brief presentation is sufficient to understand how to use structured probabilistic models as a language to describe some of the algorithms in part II. Now, in part III, structured probabilistic models are a key ingredient of many of the most important research topics in deep learning. To prepare to discuss these research ideas, in this chapter, we describe structured probabilistic models in much greater detail. This chapter is intended to be self- contained; the reader does not need to review the earlier introduction before continuing with this chapter. A structured probabilistic model is a way of describing a probability distribution, using a graph to describe which random variables in the probability distribution interact with each other directly. Here we use \u201cgraph\u201d in the graph theory sense\u2014a set of vertices connected to one another by a set of edges. Because the structure of the model is defined by a graph, these models are often also referred to as graphical models", "254353a9-0dec-43e0-99d7-a2e4b5555dda": "As with the k-nearest neighbors algorithm, each input is represented with multiple values, but  https://www.deeplearningbook.org/contents/representation.html    those values cannot readily be controlled separately from each other. e Kernel machines with a Gaussian kernel (or other similarly local kernel): although the degree of activation of each \u201csupport vector\u201d or template example is now continuous-valued, the same issue arises as with Gaussian mixtures. e Language or translation models based on n-grams: the set of contexts (sequences of symbols) is partitioned according to a tree structure of suffixes. A leaf may correspond to the last two words being w, and wy, for example.\n\nSeparate parameters are estimated for each leaf of the tree (with some sharing being possible). For some of these nondistributed algorithms, the output is not constant by parts but instead interpolates between neighboring regions. The relationship between the number of parameters (or examples) and the number of regions they can define remains linear. An important related concept that distinguishes a distributed representation from a symbolic one is that generalization arises due to shared attributes between different concepts", "a44b1a3b-72d1-4d50-b579-4a58afd25fb8": "A signi\ufb01cant di\u21b5erence between AlphaGo Zero and AlphaGo is that AlphaGo Zero used MCTS to select moves throughout self-play reinforcement learning, whereas AlphaGo used MCTS for live play after\u2014but not during\u2014learning. Other di\u21b5erences besides not using any human data or human-crafted features are that AlphaGo Zero used only one deep convolutional ANN and used a simpler version of MCTS. AlphaGo Zero\u2019s MCTS was simpler than the version used by AlphaGo in that it did not include rollouts of complete games, and therefore did not need a rollout policy.\n\nEach iteration of AlphaGo Zero\u2019s MCTS ran a simulation that ended at a leaf node of the current search tree instead of at the terminal position of a complete game simulation. But as in AlphaGo, each iteration of MCTS in AlphaGo Zero was guided by the output of a deep convolutional network, labeled f\u2713 in Figure 16.7, where \u2713 is the network\u2019s weight vector", "945047e0-0b9e-4ae1-ae9d-267438be4d1b": "1 1 soa od sad wd 1  https://www.deeplearningbook.org/contents/convnets.html    * ine numan visual sysven IS lvegraved WIL Waly OlWer senses, SUC as hearing, and factors like our moods and thoughts.\n\nConvolutional networks so far are purely visual. e The human visual system does much more than just recognize objects. It is able to understand entire scenes, including many objects and relationships between objects, and it processes rich 3-D geometric information needed for our bodies to interface with the world. Convolutional networks have been applied to some of these problems, but these applications are in their infancy. e Even simple brain areas like V1 are heavily affected by feedback from higher levels. Feedback has been explored extensively in neural network models but has not yet been shown to offer a compelling improvement. e While feedforward IT firing rates capture much of the same information as convolutional network features, it is not clear how similar the intermediate computations are. The brain probably uses very different activation and pooling functions. An individual neuron\u2019s activation probably is not well characterized by a single linear filter response", "3551bdc7-e1f0-4abd-8d4c-42964335fd72": "Hashing frees us from the curse of dimensionality in the sense that memory requirements need not be exponential in the number of dimensions, but need merely match the real demands of the task. Open-source implementations of tile Exercise 9.4 Suppose we believe that one of two state dimensions is more likely to have an e\u21b5ect on the value function than is the other, that generalization should be primarily across this dimension rather than along it.\n\nWhat kind of tilings could be used to take advantage of this prior knowledge? \u21e4 Radial basis functions (RBFs) are the natural generalization of coarse coding to continuousvalued features. Rather than each feature being either 0 or 1, it can be anything in the interval , re\ufb02ecting various degrees to which the feature is present. A typical RBF feature, xi, has a Gaussian (bell-shaped) response xi(s) dependent only on the distance between the state, s, and the feature\u2019s prototypical or center state, ci, and relative to the feature\u2019s width, \u03c3i: The norm or distance metric of course can be chosen in whatever way seems most appropriate to the states and task at hand. The \ufb01gure below shows a one-dimensional example with a Euclidean distance metric", "6f9cc1e3-74f6-4eea-ba46-9e1b7f9f9d09": "Because there is one basis function associated with every data point, the corresponding model can be computationally costly to evaluate when making predictions for new data points. Models have therefore been proposed , which retain the expansion in radial basis functions but where the number M of basis functions is smaller than the number N of data points. Typically, the number of basis functions, and the locations \u00b5i of their centres, are determined based on the input data {xn} alone. The basis functions are then kept \ufb01xed and the coef\ufb01cients {wi} are determined by least squares by solving the usual set of linear equations, as discussed in Section 3.1.1. One of the simplest ways of choosing basis function centres is to use a randomly chosen subset of the data points. A more systematic approach is called orthogonal least squares .\n\nThis is a sequential selection process in which at each step the next data point to be chosen as a basis function centre corresponds to the one that gives the greatest reduction in the sum-of-squares error. Values for the expansion coef\ufb01cients are determined as part of the algorithm. Clustering algorithms such as K-means have also been used, which give a set of basis function centres that Section 9.1 no longer coincide with training data points", "916d2d9f-591f-4c7c-83c0-59628f0c5d08": "Evaluating or maximizing the log-likelihood requires confronting not just the problem of intractable inference to marginalize out the latent variables, but also  the problem of an intractable partition function within the undirected model of the top two layers.\n\nTo train a deep belief network, one begins by training an RBM to maximize Ev~paata log p(v) using contrastive divergence or stochastic maximum likelihood. The parameters of the RBM then define the parameters of the first layer of the DBN. Next, a second RBM is trained to approximately maximize  Sy Paaa np) (nv) log p (A), (20.21)  where p) is the probability distribution represented by the first RBM, and p() is the probability distribution represented by the second RBM. In other words, the second RBM is trained to model the distribution defined by sampling the hidden units of the first RBM, when the first RBM is driven by the data. This procedure can be repeated indefinitely, to add as many layers to the DBN as desired, with each new RBM modeling the samples of the previous one. Each RBM defines another layer of the DBN", "531ee2ef-f005-43eb-a789-4a4afb03beed": "Timing in simple conditioning and occasion setting: A neural network approach. Behavioural Processes, 45(1):33\u201357. Bu\u00b8soniu, L., Lazaric, A., Ghavamzadeh, M., Munos, R., Babu\u02d8ska, R., De Schutter, B. Reinforcement Learning: State-of-the-Art, pp. 75\u2013109. Springer-Verlag Berlin Heidelberg. Bush, R. R., Mosteller, F. Stochastic Models for Learning. Wiley, New York. Byrne, J. H., Gingrich, K. J., Baxter, D. A. Computational capabilities of single neurons: Relationship to simple forms of associative and nonassociative learning in aplysia. In R. D. Hawkins and G. H. Bower (Eds. ), Computational Models of Learning, pp. 31\u201363", "36b10b7c-7df2-4787-8462-7669a0f5bbe0": "If \u00a2(a) is of high enough dimension, we can always have enough capacity to fit the training set, but generalization to the test set often remains poor.\n\nVery generic feature mappings are usually based only on the principle of local smoothness and do not encode enough prior information to solve advanced problems. 2. Another option is to manually engineer \u00a2. Until the advent of deep learning, this was the dominant approach. It requires decades of human effort for each separate task, with practitioners specializing in different domains, such as speech recognition or computer vision, and with little transfer between domains. 3. The strategy of deep learning is to learn \u00a2. In this approach, we have a model y = f(x; 0, w) = 6(@; 0)\" w. We now have parameters @ that we use to learn \u00a2 from a broad class of functions, and parameters w that map from \u00a2(a) to the desired output. This is an example of a deep feedforward network, with \u00a2 defining a hidden layer. This approach is the only one of the three that gives up on the convexity of the training problem, but the benefits outweigh the harms", "c4b9e57c-0050-41e1-8250-3e9bb0cfc24b": "Correct application of an algorithm depends on mastering some fairly simple methodology.\n\nMany of the recommendations in this chapter are adapted from Ng . We recommend the following practical design process:  e Determine your goals\u2014what error metric to use, and your target value for this error metric. These goals and error metrics should be driven by the problem that the application is intended to solve. https://www.deeplearningbook.org/contents/guidelines.html    \u00b0\u00ae Hstablish a working end-to-end pipeline as soon as possible, including the 416  CHAPTER 11. PRACTICAL METHODOLOGY  estimation of the appropriate performance metrics. e Instrument the system well to determine bottlenecks in performance. Diag- nose which components are performing worse than expected and whether poor performance is due to overfitting, underfitting, or a defect in the data or software. e Repeatedly make incremental changes such as gathering new data, adjusting hyperparameters, or changing algorithms, based on specific findings from your instrumentation. As a running example, we will use the Street View address number transcription system . The purpose of this application is to add buildings to Google Maps", "66330527-b680-47cd-be13-c021044d9919": "We now take the various parameters of the mixture model, namely the mixing coef\ufb01cients \u03c0k(x), the means \u00b5k(x), and the variances \u03c32 the outputs of a conventional neural network that takes x as its input. The structure of this mixture density network is illustrated in Figure 5.20. The mixture density network is closely related to the mixture of experts discussed in Section 14.5.3.\n\nThe principle difference is that in the mixture density network the same function is used to predict the parameters of all of the component densities as well as the mixing coef\ufb01cients, and so the nonlinear hidden units are shared amongst the input-dependent functions. The neural network in Figure 5.20 can, for example, be a two-layer network having sigmoidal (\u2018tanh\u2019) hidden units. If there are L components in the mixture model (5.148), and if t has K components, then the network will have L output unit activations denoted by a\u03c0 k that determine the mixing coef\ufb01cients \u03c0k(x), K outputs k that determine the kernel widths \u03c3k(x), and L \u00d7 K outputs denoted kj that determine the components \u00b5kj(x) of the kernel centres \u00b5k(x)", "3cdcbf2b-b64f-4d31-9003-619f162649d2": "When y = f(a), the model assigns an input described by vector x to a category identified by numeric code y. There are other variants of the classification task, for example, where f outputs a probability distribution over classes. An example of a classification task is object recognition, where the input is an image (usually described as a set of pixel brightness values), and the output is a numeric code identifying the object in the image.\n\nFor example, the Willow Garage PR2 robot is able to act as a waiter that can recognize different kinds of drinks and deliver them to people on command . Modern object recognition is best accomplished with deep learning . Object recognition is the same basic technology that enables computers to recognize faces , which can be used to automatically tag people in photo collections and for computers to interact more naturally with their users. e Classification with missing inputs: Classification becomes more chal- lenging if the computer program is not guaranteed that every measurement in its input vector will always be provided. To solve the classification task, the learning algorithm only has to define a single function mapping from a vector input to a categorical output", "706a50e8-5dff-48c1-a950-34fc987f3e81": "We have discussed the importance of probability distributions that are members of the exponential family, and we have seen that this family includes many wellSection 2.4 known distributions as particular cases.\n\nAlthough such distributions are relatively simple, they form useful building blocks for constructing more complex probability distributions, and the framework of graphical models is very useful in expressing the way in which these building blocks are linked together. Such models have particularly nice properties if we choose the relationship between each parent-child pair in a directed graph to be conjugate, and we shall explore several examples of this shortly. Two cases are particularly worthy of note, namely when the parent and child node each correspond to discrete variables and when they each correspond to Gaussian variables, because in these two cases the relationship can be extended hierarchically to construct arbitrarily complex directed acyclic graphs. We begin by examining the discrete case. The probability distribution p(x|\u00b5) for a single discrete variable x having K possible states (using the 1-of-K representation) is given by k \u00b5k = 1, only K \u2212 1 values for \u00b5k need to be speci\ufb01ed in order to de\ufb01ne the Now suppose that we have two discrete variables, x1 and x2, each of which has K states, and we wish to model their joint distribution", "89dcee4a-fdf2-4110-82e4-5630d4feb601": "For instance, the region x1 \u2a7d \u03b81 is further subdivided according to whether x2 \u2a7d \u03b82 or x2 > \u03b82, giving rise to the regions denoted A and B. The recursive subdivision can be described by the traversal of the binary tree shown in Figure 14.6. For any new input x, we determine which region it falls into by starting at the top of the tree at the root node and following a path down to a speci\ufb01c leaf node according to the decision criteria at each node. Note that such decision trees are not probabilistic graphical models. Within each region, there is a separate model to predict the target variable.\n\nFor instance, in regression we might simply predict a constant over each region, or in classi\ufb01cation we might assign each region to a speci\ufb01c class. A key property of treebased models, which makes them popular in \ufb01elds such as medical diagnosis, for example, is that they are readily interpretable by humans because they correspond to a sequence of binary decisions applied to the individual input variables. For instance, to predict a patient\u2019s disease, we might \ufb01rst ask \u201cis their temperature greater than some threshold?\u201d. If the answer is yes, then we might next ask \u201cis their blood pressure less than some threshold?\u201d", "fa82c282-2ddc-47c5-9520-502072e56827": "Like some variants of ICA, SFA is not quite a generative model per se, in the sense that it defines a linear map between input space and feature space but does not define a prior over feature space and thus does not impose a distribution p(a) on input space. The SFA algorithm  consists of defining f(a; 0) to be a linear transformation, then solving the optimization problem  min E,(f(al*), \u2014 f(al),)? (13.8) subject to the constraints Bef (a); =0 (13.9) and  = 1. (13.10)  The constraint that the learned feature have zero mean is necessary to make the problem have a unique solution; otherwise we could add a constant to all feature  490  CHAPTER 13. LINEAR FACTOR MODELS  values and obtain a different solution with equal value of the slowness objective.\n\nThe constraint that the features have unit variance is necessary to prevent the pathological solution where all features collapse to 0. Like PCA, the SFA features are ordered, with the first feature being the slowest. To learn multiple features, we must also add the constraint  Vi <j, Elf(@\u2122) f(e);] = 0", "bf183d0a-7ca7-48b6-a8a8-24fc979388fc": "Restricted Boltzmann Machines  Invented under the name harmonium , restricted Boltzmann machines are some of the most common building blocks of deep probabilistic models. We briefly describe RBMs in section 16.7.1. Here we review the previous information and go into more detail. RBMs are undirected probabilistic graphical  https://www.deeplearningbook.org/contents/generative_models.html    models containing a layer of observable variables and a single layer of latent variables. RBMs may be stacked (one on top of the other) to form deeper models. See figure 20.1 for some examples. In particular, figure 20.la shows the graph  structure of the RBM itself. It is a bipartite graph, with no connections permitted between any variables in the observed layer or between any units in the latent layer. We begin with the binary version of the restricted Boltzmann machine, but as  653  CHAPTER 20. DEEP GENERATIVE MODELS  Figure 20.1: Examples of models that may be built with restricted Boltzmann machines", "bff9d4b7-5a8f-415b-bd17-5086b8d63772": "Since the objective is to maximize L(\u03b8), we wish to compute an updated estimate \u03b8 such that, L(\u03b8) > L(\u03b8n) (8) Equivalently we want to maximize the di\ufb00erence, So far, we have not considered any unobserved or missing variables. In problems where such data exist, the EM algorithm provides a natural framework for their inclusion. Alternately, hidden variables may be introduced purely as an arti\ufb01ce for making the maximum likelihood estimation of \u03b8 tractable. In this case, it is assumed that knowledge of the hidden variables will make the maximization of the likelihood function easier. Either way, denote the hidden random vector by Z and a given realization by z. The total probability P(X|\u03b8) may be written in terms of the hidden variables z as, We may then rewrite Equation (9) as, Notice that this expression involves the logarithm of a sum. In Section (2) using Jensen\u2019s inequality, it was shown that, for constants \u03bbi \u2265 0 with \ufffdn i=1 \u03bbi = 1.\n\nThis result may be applied to Equation (11) which involves the logarithm of a sum provided that the constants \u03bbi can be identi\ufb01ed", "83ab9bbe-a859-4a79-bf29-05e45186c828": "Whereas rewards determine the immediate, intrinsic desirability of environmental states, values indicate the long-term desirability of states after taking into account the states that are likely to follow and the rewards available in those states. For example, a state might always yield a low immediate reward but still have a high value because it is regularly followed by other states that yield high rewards.\n\nOr the reverse could be true. To make a human analogy, rewards are somewhat like pleasure (if high) and pain (if low), whereas values correspond to a more re\ufb01ned and farsighted judgment of how pleased or displeased we are that our environment is in a particular state. Rewards are in a sense primary, whereas values, as predictions of rewards, are secondary. Without rewards there could be no values, and the only purpose of estimating values is to achieve more reward. Nevertheless, it is values with which we are most concerned when making and evaluating decisions. Action choices are made based on value judgments. We seek actions that bring about states of highest value, not highest reward, because these actions obtain the greatest amount of reward for us over the long run. Unfortunately, it is much harder to determine values than it is to determine rewards", "04af7524-ccee-45b6-ad81-ef64602f8301": "The marginal likelihood function p(t|\u03b1, \u03b2) is obtained by integrating over the weight parameters w, so that One way to evaluate this integral is to make use once again of the result (2.115) for the conditional distribution in a linear-Gaussian model. Here we shall evaluate Exercise 3.16 the integral instead by completing the square in the exponent and making use of the standard form for the normalization coef\ufb01cient of a Gaussian. From (3.11), (3.12), and (3.52), we can write the evidence function in the form Exercise 3.17 where M is the dimensionality of w, and we have de\ufb01ned We recognize (3.79) as being equal, up to a constant of proportionality, to the regularized sum-of-squares error function (3.27).\n\nWe now complete the square over w Exercise 3.18 Note that A corresponds to the matrix of second derivatives of the error function and is known as the Hessian matrix. Here we have also de\ufb01ned mN given by Using (3.54), we see that A = S\u22121 N , and hence (3.84) is equivalent to the previous de\ufb01nition (3.53), and therefore represents the mean of the posterior distribution", "f622f62a-5f46-48a9-bb25-59b74cbd1361": "The general solution to the minimization of J for arbitrary D and arbitrary M < D is obtained by choosing the {Ui} to be eigenvectors of the covariance matrix given by SUi = AiUi (12.17) where i = 1, ... ,D, and as usual the eigenvectors {Ui} are chosen to be orthonormal. The corresponding value of the distortion measure is then given by which is simply the sum of the eigenvalues of those eigenvectors that are orthogonal to the principal subspace. We therefore obtain the minimum value of J by selecting these eigenvectors to be those having the D - M smallest eigenvalues, and hence the eigenvectors defining the principal subspace are those corresponding to the M largest eigenvalues. Although we have considered M < D, the PCA analysis still holds if M = D, in which case there is no dimensionality reduction but simply a rotation of the coordinate axes to align with principal components. Finally, it is worth noting that there exists a closely related linear dimensionality reduction technique called canonical correlation analysis, or CCA", "2d758864-86df-4436-8961-b2a398c81e4e": ", xM)T is an (M + 1)-dimensional vector of parent states augmented with an additional variable x0 whose value is clamped to 1, and w = (w0, w1, . , wM)T is a vector of M + 1 parameters. This is a more restricted form of conditional distribution than the general case but is now governed by a number of parameters that grows linearly with M. In this sense, it is analogous to the choice of a restrictive form of covariance matrix (for example, a diagonal matrix) in a multivariate Gaussian distribution. The motivation for the logistic sigmoid representation was discussed in Section 4.2. In the previous section, we saw how to construct joint probability distributions over a set of discrete variables by expressing the variables as nodes in a directed acyclic graph. Here we show how a multivariate Gaussian can be expressed as a directed graph corresponding to a linear-Gaussian model over the component variables.\n\nThis allows us to impose interesting structure on the distribution, with the general Gaussian and the diagonal covariance Gaussian representing opposite extremes", "137e410b-8a73-4604-9a72-58e512823cb2": "We denote the probability of observing both x1k = 1 and x2l = 1 by the parameter \u00b5kl, where x1k denotes the kth component of x1, and similarly for x2l. The joint distribution can be written Because the parameters \u00b5kl are subject to the constraint \ufffd bution is governed by K2 \u2212 1 parameters.\n\nIt is easily seen that the total number of parameters that must be speci\ufb01ed for an arbitrary joint distribution over M variables is KM \u2212 1 and therefore grows exponentially with the number M of variables. Using the product rule, we can factor the joint distribution p(x1, x2) in the form p(x2|x1)p(x1), which corresponds to a two-node graph with a link going from the x1 node to the x2 node as shown in Figure 8.9(a). The marginal distribution p(x1) is governed by K \u2212 1 parameters, as before, Similarly, the conditional distribution p(x2|x1) requires the speci\ufb01cation of K \u2212 1 parameters for each of the K possible values of x1", "d21cca0e-fc74-4b8d-a3a0-7cff5e8c670e": "Textmix < My \u00a9 I, + (1 \u2014 Mp) \u00a9 In, where My \u20ac {0, 1} is a binary mask and \u00a9 is element-wise multiplication. It is equivalent to filling the cutout  region with the same region from another image. MoCHi : Given a query q, MoCHi maintains a queue of K negative features Q = {n, sey nx} and sorts these negative features by similarity to the query, q'n, in descending order. The first VV items in the queue are considered as the hardest negatives, Qn. Then synthetic hard examples can be generated by  h = h/|h| where h = an, + (1 \u2014 @)n, and a \u20ac (0, 1). Even harder examples can be created by mixing with the query feature, h\u2019 = h\u2019/|h\u2019|y where h\u2019 = Bq + (1 \u2014 B)n; and 6 \u20ac (0, 0.5)", "430f1a33-6a00-4191-8209-90f416b32d30": "If n = |S| is the number of states, then just forming the maximum-likelihood estimate of the process may require on the order of n2 memory, and computing the corresponding value function requires on the order of n3 computational steps if done conventionally. In these terms it is indeed striking that TD methods can approximate the same solution using memory no more than order n and repeated computations over the training set. On tasks with large state spaces, TD methods may be the only feasible way of approximating the certainty-equivalence solution. \u21e4Exercise 6.7 Design an o\u21b5-policy version of the TD(0) update that can be used with We turn now to the use of TD prediction methods for the control problem. As usual, we follow the pattern of generalized policy iteration (GPI), only this time using TD methods for the evaluation or prediction part.\n\nAs with Monte Carlo methods, we face the need to trade o\u21b5 exploration and exploitation, and again approaches fall into two main classes: on-policy and o\u21b5-policy. In this section we present an on-policy TD control method. The \ufb01rst step is to learn an action-value function rather than a state-value function", "19688d58-1e49-485c-a074-dbb451c4d3b6": "Maximum entropy rl (provably) solves some robust rl problems. arXiv preprint arXiv:2103.06257. Roy Fox, Ari Pakman, and Naftali Tishby. 2016. Taming the noise in reinforcement learning via soft updates. In Proceedings of the Thirty-Second Conference on Uncertainty in Arti\ufb01cial Intelligence, pages 202\u2013211. Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, and Jun Wang. 2018. Long text generation via adversarial training with leaked information. In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence. Tatsunori Hashimoto, Hugh Zhang, and Percy Liang. 2019. Unifying human and statistical evaluation for natural language generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1689\u20131701.\n\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019", "b12c0345-ed68-49af-a1a7-eba4716aefbe": "In the E step of the EM algorithm, we then use these parameter values to \ufb01nd the posterior distribution over w, which is given by (10.156). In the M step, we then maximize the expected complete-data log likelihood which is given by where the expectation is taken with respect to the posterior distribution q(w) evaluated using \u03beold. Noting that p(w) does not depend on \u03be, and substituting for h(w, \u03be) we obtain (10.161) where \u2018const\u2019 denotes terms that are independent of \u03be. We now set the derivative with respect to \u03ben equal to zero.\n\nA few lines of algebra, making use of the de\ufb01nitions of \u03c3(\u03be) and \u03bb(\u03be), then gives We now note that \u03bb\u2032(\u03be) is a monotonic function of \u03be for \u03be \u2a7e 0, and that we can restrict attention to nonnegative values of \u03be without loss of generality due to the symmetry of the bound around \u03be = 0. Thus \u03bb\u2032(\u03be) \u0338= 0, and hence we obtain the following re-estimation equations Exercise 10.33 where we have used (10.156). Let us summarize the EM algorithm for \ufb01nding the variational posterior distribution", "ad9a0905-b809-47e1-8e9a-1edcff747707": "The use of a reward signal to formalize the idea of a goal is one of the most distinctive features of reinforcement learning. Although formulating goals in terms of reward signals might at \ufb01rst appear limiting, in practice it has proved to be \ufb02exible and widely applicable. The best way to see this is to consider examples of how it has been, or could be, used. For example, to make a robot learn to walk, researchers have provided reward on each time step proportional to the robot\u2019s forward motion. In making a robot learn how to escape from a maze, the reward is often \u22121 for every time step that passes prior to escape; this encourages the agent to escape as quickly as possible. To make a robot learn to \ufb01nd and collect empty soda cans for recycling, one might give it a reward of zero most of the time, and then a reward of +1 for each can collected. One might also want to give the robot negative rewards when it bumps into things or when somebody yells at it.\n\nFor an agent to learn to play checkers or chess, the natural rewards are +1 for winning, \u22121 for losing, and 0 for drawing and for all nonterminal positions. You can see what is happening in all of these examples", "287f6f3d-d574-4be3-ad31-0bc1316bb335": "The knowledge the agent brings to the task at the start\u2014either from previous experience with related tasks or built into it by design or evolution\u2014in\ufb02uences what is useful or easy to learn, but interaction with the environment is essential for adjusting behavior to exploit speci\ufb01c features of the task. Beyond the agent and the environment, one can identify four main subelements of a reinforcement learning system: a policy, a reward signal, a value function, and, optionally, a model of the environment. A policy de\ufb01nes the learning agent\u2019s way of behaving at a given time. Roughly speaking, a policy is a mapping from perceived states of the environment to actions to be taken when in those states. It corresponds to what in psychology would be called a set of stimulus\u2013response rules or associations. In some cases the policy may be a simple function or lookup table, whereas in others it may involve extensive computation such as a search process. The policy is the core of a reinforcement learning agent in the sense that it alone is su\ufb03cient to determine behavior. In general, policies may be stochastic, specifying probabilities for each action", "b71f7681-dad1-4e1d-9578-246102013581": "Any value of x in the interval from x = a to x = b can be written in the form \u03bba + (1 \u2212 \u03bb)b where 0 \u2a7d \u03bb \u2a7d 1. The corresponding point on the chord is given by \u03bbf(a) + (1 \u2212 \u03bb)f(b), ory. This paper introduced the word \u2018bit\u2019, and his concept that information could be sent as a stream of 1s and 0s paved the way for the communications revolution. It is said that von Neumann recommended to Shannon that he use the term entropy, not only because of its similarity to the quantity used in physics, but also because \u201cnobody knows what entropy really is, so in any discussion you will always have an advantage\u201d. and the corresponding value of the function is f (\u03bba + (1 \u2212 \u03bb)b). Convexity then implies This is equivalent to the requirement that the second derivative of the function be everywhere positive. Examples of convex functions are x ln x (for x > 0) and x2", "f1b57013-51f3-48df-a6c1-0f524be7f88b": "They simply save training examples in memory as they arrive (or at least save a subset of the examples) without updating any parameters. Then, whenever a query state\u2019s value estimate is needed, a set of examples is retrieved from memory and used to compute a value estimate for the query state. This approach is sometimes called lazy learning because processing training examples is postponed until the system is queried to provide an output. Memory-based function approximation methods are prime examples of nonparametric methods. Unlike parametric methods, the approximating function\u2019s form is not limited to a \ufb01xed parameterized class of functions, such as linear functions or polynomials, but is instead determined by the training examples themselves, together with some means for combining them to output estimated values for query states. As more training examples accumulate in memory, one expects nonparametric methods to produce increasingly accurate approximations of any target function. There are many di\u21b5erent memory-based methods depending on how the stored training examples are selected and how they are used to respond to a query.\n\nHere, we focus on local-learning methods that approximate a value function only locally in the neighborhood of the current query state", "835a360c-ca36-4681-9e85-60d45396f776": "lan JG, David W-F, Mehdi M, Aaron C, Yoshua B. Maxout networks. arXiv preprint. 2013. Shuangtao L, Yuanke C, Yanlin P, Lin B. Learning more robust features with adversarial training. ArXiv preprints. 2018. Lingxi X, Jingdong W, Zhen W, Meng W, QiT. DisturbLabel: regularizing CNN on the loss layer. arXiv preprint. 2016. Christopher B, Liang C, Ricardo GPB, Roger G, Alexander H, David AD, Maria VH, Joanna W, Daniel R. GAN augmen- tation: augmenting training data using generative adversarial networks. arXiv preprint. 2018. Doersch C. Tutorial on Variational Autoencoders. ArXiv e-prints. 2016. Laurens M, Geoffrey H. Visualizing data using t-SNE", "3b5e3cd9-fea8-4783-b124-3ba68081be88": "Also show that the minimum expected Lq loss for q \u2192 0 is given by the conditional mode, i.e., by the function y(x) equal to the value of t that maximizes p(t|x) for each x. 1.28 (\u22c6) In Section 1.6, we introduced the idea of entropy h(x) as the information gained on observing the value of a random variable x having distribution p(x). We saw that, for independent variables x and y for which p(x, y) = p(x)p(y), the entropy functions are additive, so that h(x, y) = h(x) + h(y). In this exercise, we derive the relation between h and p in the form of a function h(p).\n\nFirst show that h(p2) = 2h(p), and hence by induction that h(pn) = nh(p) where n is a positive integer. Hence show that h(pn/m) = (n/m)h(p) where m is also a positive integer", "6d5c94a6-c625-452d-9db1-719ca3769f8c": "When the dataset has hundreds of thousands of examples or more, this is not a serious issue. When the dataset is too small, alternative procedures enable one to use all the examples in the estimation of the mean test error, at the price of increased computational cost. These procedures are based on the idea of repeating the training and testing computation on different randomly chosen subsets or splits of the original dataset. The most common of these is the k-fold cross-validation procedure, shown in algorithm 5.1, in which a partition of the dataset is formed by splitting it into k nonoverlapping subsets. The test error may then be estimated by taking the average test error across k trials. On trial 7, the i-th subset of the data is used as the test set, and the rest of the data is used as the training set. One problem is that no unbiased estimators of the variance of such average error estimators exist , but approximations are typically used. 5.4 Estimators, Bias and Variance  The field of statistics gives us many tools to achieve the machine learning goal of solving a task not only on the training set but also to generalize", "f64afd8a-6dc8-4f80-8677-7921c31a4fed": "Adaptive Filtering Prediction and Control. Prentice-Hall, Gopnik, A., Glymour, C., Sobel, D., Schulz, L. E., Kushnir, T., Danks, D. A theory of causal learning in children: Causal maps and Bayes nets. Psychological Review, 111(1):3\u201332. Gordon, G. J. Stable function approximation in dynamic programming. In A. Prieditis and S. Russell (Eds. ), Proceedings of the 12th International Conference on Machine Learning , pp. 261\u2013268. Morgan Kaufmann. An expanded version was published as Technical Report CMU-CS-95-103. Carnegie Mellon University, Pittsburgh, PA, 1995. Gordon, G. J. Chattering in SARSA(\u03bb). CMU learning lab internal report. Gordon, G. J. Stable \ufb01tted reinforcement learning.\n\nIn Advances in Neural Information Processing Systems 8 , pp", "ccea5816-ce10-404c-8c2e-10f2f4c4ac71": "used \u20ac = 0 and A = 10 on small, randomly selected patches drawn from CIFAR-10. The scale parameter s can usually be set to 1, as done by Coates et al. , or chosen to make each individual pixel have standard deviation across examples close to 1, as done by Goodfellow et al. The standard deviation in equation 12.3 is just a rescaling of the L? norm  450  https://www.deeplearningbook.org/contents/applications.html    CHAPTER 12. APPLICATIONS  of the image (assuming the mean of the image has already been removed). It is preferable to define GCN in terms of standard deviation rather than L?\n\nnorm because the standard deviation includes division by the number of pixels, so GCN based on standard deviation allows the same s to be used regardless of image size. However, the observation that the L? norm is proportional to the standard deviation can help build a useful intuition. One can understand GCN as mapping examples to a spherical shell. See figure 12.1 for an illustration. This can be a useful property because neural networks are often better at responding to directions in space rather than to exact locations", "1ef863f0-3809-4cac-a396-5435f41e5083": "More generally, we can consider priors in which the weights are divided into any number of groups Wk so that As a special case of this prior, if we choose the groups to correspond to the sets of weights associated with each of the input units, and we optimize the marginal likelihood with respect to the corresponding parameters \u03b1k, we obtain automatic relevance determination as discussed in Section 7.2.2. An alternative to regularization as a way of controlling the effective complexity of a network is the procedure of early stopping. The training of nonlinear network models corresponds to an iterative reduction of the error function de\ufb01ned with respect to a set of training data. For many of the optimization algorithms used for network training, such as conjugate gradients, the error is a nonincreasing function of the iteration index. However, the error measured with respect to independent data, generally called a validation set, often shows a decrease at \ufb01rst, followed by an increase as the network starts to over-\ufb01t", "ba186160-c0da-4d3a-a08f-1b5fedfe93ff": "Text data augmentation has also achieved impressive success, such as contextual augmentation , back-translation , and manual approaches . In addition to perturbing the input text as in classi\ufb01cation tasks, text generation problems expose opportunities to adding noise also in the output text, such as .\n\nRecent work  shows output nosing in sequence generation can be treated as an intermediate approach in between supervised learning and reinforcement learning, and developed a new sequence learning algorithm that interpolates between the spectrum of existing algorithms. We instantiate our approach for text contextual augmentation as in , but enhance the previous work by additionally \ufb01ne-tuning the augmentation network jointly with the target model. Data weighting has been used in various algorithms, such as AdaBoost , self-paced learning , hard-example mining , and others . These algorithms largely de\ufb01ne sample weights based on training loss. Recent work  learns a separate network to predict sample weights. Of particular relevance to our work is  which induces sample weights using a validation set. The data weighting mechanism instantiated by our framework has a key difference in that samples weights are treated as parameters that are updated iteratively, instead of re-estimated from scratch at each step. We show improved performance of our approach", "b84bc755-a62b-405f-958e-dbaa0b83e1b5": "Several different performance metrics may be used to measure the effectiveness of a complete application that includes machine learning components. These performance metrics are usually different from the cost function used to train the model. As described in section 5.1.2, it is common to measure the accuracy, or equivalently, the error rate, of a system. However, many applications require more advanced metrics. Sometimes it is much more costly to make one kind of a mistake than another. For example, an e-mail spam detection system can make two kinds of mistakes: incorrectly classifying a legitimate message as spam, and incorrectly allowing a spam message to appear in the inbox.\n\nIt is much worse to block a legitimate message than to allow a questionable message to pass through. Rather than measuring the error rate of a spam classifier, we may wish to measure some form of total cost, where the cost of blocking legitimate messages is higher than the cost of allowing spam messages. Sometimes we wish to train a binary classifier that is intended to detect some rare event. For example, we might design a medical test for a rare disease. Suppose that only one in every million people has this disease", "97c88710-c7e5-4970-9e01-31b8a3f61230": "Hull hypothesized that an animal\u2019s actions leave internal stimuli whose traces decay exponentially as functions of time since an action was taken. Looking at the animal learning data available at the time, he hypothesized that the traces e\u21b5ectively reach zero after 30 to 40 seconds. The eligibility traces used in the algorithms described in this book are like Hull\u2019s traces: they are decaying traces of past state visitations, or of past state\u2013action pairs.\n\nEligibility traces were introduced by Klopf  in his neuronal theory in which they are temporally-extended traces of past activity at synapses, the connections between neurons. Klopf\u2019s traces are more complex than the exponentially-decaying traces our algorithms use, and we discuss this more when we take up his theory in Section 15.9. To account for goal gradients that extend over longer time periods than spanned by stimulus traces, Hull  proposed that longer gradients result from conditioned reinforcement passing backwards from the goal, a process acting in conjunction with his molar stimulus traces", "727203c6-3bf8-4fac-a246-49647f8907cb": "+ won + wan  https://www.deeplearningbook.org/contents/partition.html  gradient ot ee pwhen experienc! ng real events while awake and follows the negative radient of log p to minimize log Z while sleeping and experiencing events sampled rom the current model. This view explains much of the language used to describe algorithms with a positive and a negative phase, but it has not been proved to be  correct with neuroscientific experiments.\n\nIn machine learning models, it is usually necessary to use the positive and negative phase simultaneously, rather than in separate periods of wakefulness and REM sleep. As we will see in section 19.5, other machine learning algorithms draw samples from the model distribution for other purposes, and such algorithms could also provide an account for the function of dream sleep. Given this understanding of the role of the positive and the negative phase of learning, we can attempt to design a less expensive alternative to algorithm 18.1. The main cost of the naive MCMC algorithm is the cost of burning in the Markov chains from a random initialization at each step. A natural solution is to initialize  607  CHAPTER 18", "1aa7f387-51a5-4524-951e-16985a565bce": "The development of sampling methods, such as Markov chain Monte Carlo (discussed in Chapter 11) along with dramatic improvements in the speed and memory capacity of computers, opened the door to the practical use of Bayesian techniques in an impressive range of problem domains. Monte Carlo methods are very \ufb02exible and can be applied to a wide range of models.\n\nHowever, they are computationally intensive and have mainly been used for small-scale problems. More recently, highly ef\ufb01cient deterministic approximation schemes such as variational Bayes and expectation propagation (discussed in Chapter 10) have been developed. These offer a complementary alternative to sampling methods and have allowed Bayesian techniques to be used in large-scale applications . We shall devote the whole of Chapter 2 to a study of various probability distributions and their key properties. It is convenient, however, to introduce here one of the most important probability distributions for continuous variables, called the normal or Gaussian distribution. We shall make extensive use of this distribution in the remainder of this chapter and indeed throughout much of the book. For the case of a single real-valued variable x, the Gaussian distribution is de\ufb01ned by which is governed by two parameters: \u00b5, called the mean, and \u03c32, called the variance", "9de2bcdf-9963-44d7-a45c-6cf7a0bb450d": "One key benefit to the MCMC-based methods described in this section is that hey provide an estimate of the gradient of log Z, and thus we can essentially decompose the problem into the logp contribution and the log Z contribution.\n\nWe can then use any other method to tackle log p(x) and just add our negative phase gradient onto the other method\u2019s gradient. In particular, this means that  612  CHAPTER 18. CONFRONTING THE PARTITION FUNCTION  our positive phase can make use of methods that provide only a lower bound on p. Most of the other methods of dealing with log Z presented in this chapter are incompatible with bound-based positive phase methods. https://www.deeplearningbook.org/contents/partition.html    18.3. Pseudolikelihood  Monte Carlo approximations to the partition function and its gradient directly confront the partition function. Other approaches sidestep the issue, by training the model without computing the partition function. Most of these approaches are based on the observation that it is easy to compute ratios of probabilities in an undirected probabilistic model", "100d35e7-df06-4aeb-9856-277be8cd6379": "The key advantage is that samples from other sources, e.g., human-written text, can be used, making them more data ef\ufb01cient than on-policy metharXiv:2106.07704v4    22 Oct 2022 Prompt Generation for Controlling Pretrained LMs ods. Previous work has used either importance weighted PG  or Q-learning based algorithms . However, off-policy methods have been considered to be less stable. For example, the Q-learning performance relies heavily on how accurate the learned Q-function assesses the quality of intermediate subsequences \u2013 a challenging task due to the sparse reward signals. In this paper, we develop a new RL formulation for text generation that tackles the above issues (Figure 1, left).\n\nWe reframe the text generation problem from the soft Q-learning perspective originally developed in robotics . The resulting connection allows us to seamlessly take advantage of the latest successful techniques from the RL literature", "b7a1a9b5-1548-4569-a791-303c04918014": "Yang, R. Salakhutdinov, X. Liang, L. Qin, H. Dong, and E. Xing. Deep generative models with learnable knowledge constraints. In NIPS, 2018. Z. Hu, Z. Yang, R. Salakhutdinov, and E. P. Xing. On unifying deep generative models. In ICLR, 2018. E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016. L. Jiang, Z. Zhou, T. Leung, L.-J. Li, and L. Fei-Fei. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In ICML, 2018. A. Katharopoulos and F", "5c6678f0-009e-4e05-aa8d-da85323e3f21": "conservatively, based on prior experience with similar experiments, to make sure hat the optimal value is likely to be in the selected range. Typically, a grid search involves picking values approximately on a logarithmic scale, e.g., a learning rate aken within the set {0.1,0.01, 1073, 1074, 107\u00b0}, or a number of hidden units aken with the set {50, 100, 200, 500, 1000, 2000}. Grid search usually performs best when it is performed repeatedly. For example, suppose that we ran a grid search over a hyperparameter a using values of {\u20141, 0, 1}. If the best value found is 1, then we underestimated the range in which the best a lies and should shift the grid and run another search with a in, for example, {1,2,3}. If we find that the best value of a is 0, then we may wish to refine our  428  https://www.deeplearningbook.org/contents/guidelines.html    CHAPTER 11", "6f11822a-2dc0-4bea-8ec6-61f1ab242c30": "Typically these can be debugged by testing each of their guarantees. Some guarantees that some optimization algorithms offer include that the objective function will never increase after one step of the algorithm, that the gradient with respect to some subset of variables will be zero after each step of the algorithm, and that the gradient with respect to all variables will be zero at convergence. Usually due to rounding error, these conditions will not hold exactly in a digital computer, so the debugging test should include some tolerance parameter.\n\n11.6 Example: Multi-Digit Number Recognition  https://www.deeplearningbook.org/contents/guidelines.html    To provide an end-to-end description of how to apply our design methodology in practice, we present a brief account of the Street View transcription system, from the point of view of designing the deep learning components. Obviously, many other components of the complete system, such as the Street View cars, the  database infrastructure, and so on, were of paramount importance. From the point of view of the machine learning task, the process began with data collection. The cars collected the raw data, and human operators provided labels. The transcription task was preceded by a significant amount of dataset curation, including using other machine learning techniques to detect the house numbers prior to transcribing them", "cda893a7-1be6-4115-af40-6e60e1dadd6c": "When neural networks are applied to regression problems, it is common to use a sum-of-squares error function of the form where we have considered the case of a single output in order to keep the notation simple (the extension to several outputs is straightforward). We can then write the Exercise 5.16 If the network has been trained on the data set, and its outputs yn happen to be very close to the target values tn, then the second term in (5.83) will be small and can be neglected. More generally, however, it may be appropriate to neglect this term by the following argument. Recall from Section 1.5.5 that the optimal function that minimizes a sum-of-squares loss is the conditional average of the target data. The quantity (yn \u2212 tn) is then a random variable with zero mean.\n\nIf we assume that its value is uncorrelated with the value of the second derivative term on the right-hand side of (5.83), then the whole term will average to zero in the summation over n", "65949621-88be-4403-975b-ca67692d2798": "\u201cAutoAugment: Learning augmentation policies from data.\" arXiv preprint arXiv:1805.09501 . 11] Daniel Ho et al. \u201cPopulation Based Augmentation: Efficient Learning of Augmentation Policy Schedules.\" ICML 2019. 12] Ekin D. Cubuk & Barret Zoph et al. \u201cRandAugment: Practical automated data augmentation with a reduced search space.\" arXiv preprint arXiv:1909.13719 . 13] Hongyi Zhang et al. \u201cmixup: Beyond Empirical Risk Minimization.\" ICLR 2017. 14] Sangdoo Yun et al. \u201cCutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features.\" ICCV 2019. 15] Yannis Kalantidis et al. \u201cMixing of Contrastive Hard Negatives\u201d NeuriPS 2020. 16] Ashish Jaiswal et al", "c0c9a3cc-b100-4a45-a2a2-70877150fac1": "In ll1e E 'tel', we keep the nxI hed and allow the attachment point' tn ,Iide up and <I<,wn ll1e nxI '\" a, to minimize ll1e e\",,'llY, This cau\",. each attachment point (independently) 10 position Itself at the orthogonal pmjeclion of the c~sponding data point onto the nxI. In the M 'tel'. we keep the attachment poiOl' fil<ed and then release tile nxI and allow it to m'>,'e 10 tile minimum energy posilion. 11Ie E and M 'teps are then repeated until a ,uitable c\"\"vergence cri.eri\"\" is ..,isfled. a. is illuSlrated in Figure 12.12. S<J far in OIlr di\",\"\"ioo of PeA", "51a7dcdd-dd5b-41e9-ae20-f974e64ba668": "The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The applica- tion of augmentation methods based on GANSs are heavily covered in this survey.\n\nIn addition to augmentation techniques, this paper will briefly discuss other character- istics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data. Keywords: Data Augmentation, Big data, Image data, Deep Learning, GANs  Introduction  Deep Learning models have made incredible progress in discriminative tasks. This has been fueled by the advancement of deep network architectures, powerful computation, and access to big data", "1dcb8a0f-4ca8-4e10-aa8a-3ace90285a55": "The practical applicability of Gibbs sampling depends on the ease with which samples can be drawn from the conditional distributions p(zk|z\\k). In the case of probability distributions speci\ufb01ed using graphical models, the conditional distributions for individual nodes depend only on the variables in the corresponding Markov blankets, as illustrated in Figure 11.12. For directed graphs, a wide choice of conditional distributions for the individual nodes conditioned on their parents will lead to conditional distributions for Gibbs sampling that are log concave. The adaptive rejection sampling methods discussed in Section 11.1.3 therefore provide a framework for Monte Carlo sampling from directed graphs with broad applicability.\n\nIf the graph is constructed using distributions from the exponential family, and if the parent-child relationships preserve conjugacy, then the full conditional distributions arising in Gibbs sampling will have the same functional form as the original conditional distributions (conditioned on the parents) de\ufb01ning each node, and so standard sampling techniques can be employed. In general, the full conditional distributions will be of a complex form that does not permit the use of standard sampling algorithms", "5ca32ddc-8b43-47d6-b5e1-71116e634fd9": "4.4. Model-Based Experience. A model may also learn from other models of the same or related tasks. For example, one can learn a small-size target model by mimicking the outputs of a larger pretrained model, or an ensemble of multiple models, that is more powerful but often too expensive to deploy. Thus, the large model serves as the experience, or the source of information about the task at hand.\n\nBy seeing that the large source model is e\ufb00ectively providing \u2018pseudo-labels\u2019 on the observed inputs D = {x\u2217}, we can readily write down the corresponding experience function, as a variant of the standard supervised data experience function in Equation 4.2: Another way of model-based experience is that the pretrained model directly measures the score of a given con\ufb01guration (x, y) with its (log-)likelihood: As a concrete example, consider learning a text generation model that aims to generate text y conditioning on a given sentiment label x (either positive or negative)", "e96f3c55-7917-4b5d-9dbe-494fa5964229": "LS Aner eet ne tha TAL nn 2k nafs Ate aelee Ta tn  https://www.deeplearningbook.org/contents/generative_models.html    yuavlow 4U.UU LOLS ULL LLC ASS ULLLVLLOLL ULLAL \u00a5 UVES LOL LELELELICe w UICC.\n\niu ib the derivative tule for the logarithm ee ae iia a ation 20.60 gives an unbiased Monte Carlo estimator of the gradient. Anywhere we write p(y) in this section, one could equally write p(y | x). This is because p(y) is parametrized by w, and w contains both @ and a, if x is present. One issue with the simple REINFORCE estimator is that it has a very high variance, so that many samples of y need to be drawn to obtain a good estimator of the gradient, or equivalently, if only one sample is drawn, SGD will converge very slowly and will require a smaller learning rate", "1085af77-6757-4481-afa7-a60b11c31a13": "First of all, we note that the mean of the posterior distribution given by (2.141) is a compromise between the prior mean \u00b50 and the maximum likelihood solution \u00b5ML. If the number of observed data points N = 0, then (2.141) reduces to the prior mean as expected. For N \u2192 \u221e, the posterior mean is given by the maximum likelihood solution. Similarly, consider the result (2.142) for the variance of the posterior distribution. We see that this is most naturally expressed in terms of the inverse variance, which is called the precision. Furthermore, the precisions are additive, so that the precision of the posterior is given by the precision of the prior plus one contribution of the data precision from each of the observed data points. As we increase the number of observed data points, the precision steadily increases, corresponding to a posterior distribution with steadily decreasing variance", "12878fd0-2a85-4ea7-acd0-6fabda19438d": "If f is Markov, then St = f(Ht) is a state as we have used the term in this book. Let us henceforth call it a Markov state to distinguish it from states that are summaries of the history but fall short of the Markov property (which we will consider shortly). A Markov state is a good basis for predicting the next observation (17.5) but, more importantly, it is also a good basis for predicting or controlling anything. For example, let a test be any speci\ufb01c sequence of alternating actions and observations that might occur in the future. For example, a three-step test is denoted \u2327 = a1o1a2o2a3o3.\n\nThe probability of this test given a speci\ufb01c history h is de\ufb01ned as p(\u2327|h) .= Pr{Ot+1 =o1, Ot+2 =o2, Ot+3 =o3 | Ht =h, At =a1, At+1 =a2, At+2 =a3}", "179a5bf7-8e9c-4385-ae81-3499f8caa294": "If we substitute the factorized expression (8.49) for the joint distribution into (8.50), then we can rearrange the order of the summations and the multiplications to allow the required marginal to be evaluated much more ef\ufb01ciently. Consider for instance the summation over xN. The potential \u03c8N\u22121,N(xN\u22121, xN) is the only one that depends on xN, and so we can perform the summation \ufffd \ufb01rst to give a function of xN\u22121. We can then use this to perform the summation over xN\u22121, which will involve only this new function together with the potential \u03c8N\u22122,N\u22121(xN\u22122, xN\u22121), because this is the only other place that xN\u22121 appears.\n\nSimilarly, the summation over x1 involves only the potential \u03c81,2(x1, x2) and so can be performed separately to give a function of x2, and so on. Because each summation effectively removes a variable from the distribution, this can be viewed as the removal of a node from the graph", "63d57d44-a5c9-4a4f-b6d3-3cedc92802e5": "Fleuret. Not all samples are created equal: Deep learning with importance sampling. In ICML, 2018. D. P. Kingma and M. Welling. Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114, 2013. S. Kobayashi. Contextual augmentation: Data augmentation by words with paradigmatic relations. In NAACL, 2018.\n\nA. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classi\ufb01cation with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097\u20131105, 2012. M. P. Kumar, B. Packer, and D. Koller. Self-paced learning for latent variable models. In NeurIPS, pages 1189\u20131197, 2010. J. Lemley, S. Bazrafkan, and P. Corcoran", "7cd66f48-9f6d-4f9a-beb9-ad494d4c41e7": "Initialize three matrices, V, HY, and H\u00ae) each with m rows set to random values (e.g., from Bernoulli distributions, possibly with marginals matched to the model\u2019s marginals).\n\nwhile not converged (learning loop) do Sample a minibatch of m examples from the training data and arrange them as the rows of a design matrix V. Initialize matrices H and H), possibly to the model\u2019s marginals. while not converged (mean field inference loop) do  HY eg (vw) 4 HOW?)\"). H\u00ae eo (Ow). end while  Aw) \u2014 ivlA\u00ae  Aw) \u2014 <A TA)  for 1 = 1 to k (Gibbs sampling) do Gibbs block 1:  ~ ~ ~ T Vi, j, Vij sampled from P(Vi; = 1) =o wi) Ht - . p(2) Fy(2) 4) ap(L) qr (2) Vi, j,H;; sampled from P(H;/ = 1) \u201c( (Wi", "fc1e8b91-e01f-47ed-b889-fd829deda9bb": "Proceedings of the Nineteenth conference on Uncertainty in Arti\ufb01cial Intelligence, 583\u2013591. Yang, Z., Hu, Z., Dyer, C., Xing, E. P., & Berg-Kirkpatrick, T. Unsupervised text style transfer using language models as discriminators. Advances in Neural Information Processing Systems, 31. Yu, J., Yang, M.-S., & Lee, E. S. Sample-weighted clustering methods. Computers & mathematics with applications, 62(5), 2200\u20132208. Zheng, S., Jayasumana, S., Romera-Paredes, B., Vineet, V., Su, Z., Du, D., Huang, C., & Torr, P. H. Conditional random \ufb01elds as recurrent neural networks. Proceedings of the IEEE International Conference on Computer Vision, 1529\u20131537. Zheng, Z., Oh, J., & Singh, S. On learning intrinsic rewards for policy gradient methods. Advances in Neural Information Processing Systems, 31", "9ec758c0-825a-45e8-a59d-7fe49d353527": "We divide these values of weight decay by the learning rate.\n\nTraining from Random Initialization We trained the network from random initialization using the same procedure as for \ufb01ne-tuning, but for longer, and with an altered hyperparameter grid. We chose hyperparameters from a grid of 7 logarithmically spaced learning rates between 0.001 and 1.0 and 8 logarithmically spaced values of weight decay between 10\u22125 and 10\u22121.5. Importantly, our random initialization baselines are trained for 40,000 steps, which is suf\ufb01ciently long to achieve near-maximal accuracy, as demonstrated in Figure 8 of Kornblith et al. On Birdsnap, there are no statistically signi\ufb01cant differences among methods, and on Food-101, Stanford Cars, and FGVC Aircraft datasets, \ufb01ne-tuning provides only a small advantage over training from random initialization. However, on the remaining 8 datasets, pretraining has clear advantages. Supervised Baselines We compare against architecturally identical ResNet models trained on ImageNet with standard cross-entropy loss. These models are trained with the same data augmentation as our self-supervised models (crops, strong color augmentation, and blur) and are also trained for 1000 epochs", "4049e2be-8063-4c6e-a984-e4a29dcdf6d8": "PROBABILITY AND INFORMATION THEORY  3.13 Information Theory  Information theory is a branch of applied mathematics that revolves around quantifying how much information is present in a signal. It was originally invented to study sending messages from discrete alphabets over a noisy channel, such as communication via radio transmission. In this context, information theory tells how to design optimal codes and calculate the expected length of messages sampled from specific probability distributions using various encoding schemes. In the context of machine learning, we can also apply information theory to continuous variables where some of these message length interpretations do not apply. This field is fundamental to many areas of electrical engineering and computer science.\n\nIn this textbook, we mostly use a few key ideas from information theory to characterize probability distributions or to quantify similarity between probability distributions. https://www.deeplearningbook.org/contents/prob.html       For more detail on information theory, see Cover and Thomas  or MacKay  The basic intuition behind information theory is that learning that an unlikely event has occurred is more informative than learning that a likely event has occurred. A message saying \u201cthe sun rose this morning\u201d is so uninformative as to be unnecessary to send, but a message saying \u201cthere was a solar eclipse this morning\u201d is very informative", "c6b9b8ef-a8e1-4875-b7c4-52c3d756a6e2": "The e\u21b5ect of the introduction of reward upon the maze performance of rats. University of California Publications in Psychology, 4:113\u2013134. Boakes, R. A., Costa, D. S. J. Temporal contiguity in associative learning: Iinterference Booker, L. B. Intelligent Behavior as an Adaptation to the Task Environment. Ph.D. theBostrom, N. Superintelligence: Paths, Dangers, Strategies. Oxford University Press. Bottou, L., Vapnik, V. Local learning algorithms. Neural Computation, 4(6):888\u2013900. Boyan, J. A. Least-squares temporal di\u21b5erence learning. In Proceedings of the 16th International Conference on Machine Learning , pp. 49\u201356. Boyan, J. A. Technical update: Least-squares temporal di\u21b5erence learning. Machine Boyan, J. A., Moore, A. W", "0b568d9b-e946-4242-afc1-370002e2ae97": "(b) Supervised SimCSE  (a) Unsupervised SimCSE  A man surfing on the sea. Different dropout masks in two forward passes  There are animals outdoors. label=entailment  The pets are sitting on a couch. \u2018label=contradiction  +! There is a man *-label=entailment-~  I The man wears a business suit.\n\n| ----label=contradiction -------  1A kid is skateboarding. |  Akid is ona \u2018+ \\---- label=entailment ----  skateboard. |  | \u2014 Positive instance  \u2014~ Negative instance :  J A kit is inside the house. \\~--~ label= contradiction -- ~~  They ran experiments on 7 STS (Semantic Text Similarity) datasets and computed cosine similarity between sentence embeddings. They also tried out an optional MLM auxiliary objective loss to help avoid catastrophic forgetting of token-level knowledge. This aux loss was found to help improve performance on transfer tasks, but a consistent drop on the main STS tasks", "12ac1dd2-a897-4aa3-93f6-9fa07582e9fe": "Shorten and Khoshgoftaar J Big Data  6:60   Table 3 Performance results of the experiment with feature vs. input space extrapolation on MNIST and CIFAR-10   Model MNIST CIFAR-10 Baseline 1.093 + 0.057 30.65 \u00a30.27 Baseline + input space affine transformations 1.477 \u00a3 0.068 -  Baseline + input space extrapolation 1.0104 0.065 -  Baseline + feature space extrapolation 0.950 + 0.036 29.24+40.27  The italic value denote high performance according to the comparative metrics  The use of auto-encoders is especially useful for performing feature space augmen- tations on data. Autoencoders work by having one half of the network, the encoder, map images into low-dimensional vector representations such that the other half of the network, the decoder, can reconstruct these vectors back into the original image. This encoded representation is used for feature space augmentation", "928302a3-4c38-491e-be57-bd8fd6b3e050": "The\" wake-sleep\" algorithm for unsupervised neural networks. Science, 268, 1158. Houthooft, R., Chen, X., Duan, Y., Schulman, J., De Turck, F., & Abbeel, P. VIME: Variational information maximizing exploration. Proceedings of the 30th International Conference on Neural Information Processing Systems, 1117\u20131125. Hu, Z., Ma, X., Liu, Z., Hovy, E., & Xing, E. Harnessing deep neural networks with logic rules. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2410\u20132420. Hu, Z., Tan, B., Salakhutdinov, R., Mitchell, T., & Xing, E. P. Learning data manipulation for augmentation and weighting. Proceedings of the 33rd International Conference on Neural Information Processing Systems, 15764\u201315775.\n\nHu, Z., Wilson, A", "18b7f056-d419-4a77-a61a-513417d1c79f": "For these reasons, designing a reward signal is a critical part of any application of reinforcement learning. By designing a reward signal we mean designing the part of an agent\u2019s environment that is responsible for computing each scalar reward Rt and sending it to the agent at each time t. In our discussion of terminology at the end of Chapter 14, we said that Rt is more like a signal generated inside an animal\u2019s brain than it is like an object or event in the animal\u2019s external environment. The parts of our brains that generate these signals for us evolved over millions of years to be well suited to the challenges our ancestors had to face in their struggles to propagate their genes to future generations.\n\nWe should therefore not think that designing a good reward signal is always an easy thing to do! One challenge is to design a reward signal so that as an agent learns, its behavior approaches, and ideally eventually achieves, what the application\u2019s designer actually desires. This can be easy if the designer\u2019s goal is simple and easy to identify, such as \ufb01nding the solution to a well-de\ufb01ned problem or earning a high score in a well-de\ufb01ned game. In cases like these, it is usual to reward the agent according to its success in solving the problem or its success in improving its score", "7ecef9bf-705a-49a6-8ac9-1dee98212573": "By using the technique of Lagrange multipliers, verify that minimization of the Kullback-Leibler divergence KL(p\u2225q) with respect to one of the factors qi(Zi), keeping all other factors \ufb01xed, leads to the solution (10.17). 10.4 (\u22c6 \u22c6) Suppose that p(x) is some \ufb01xed distribution and that we wish to approximate it using a Gaussian distribution q(x) = N(x|\u00b5, \u03a3). By writing down the form of the KL divergence KL(p\u2225q) for a Gaussian q(x) and then differentiating, show that minimization of KL(p\u2225q) with respect to \u00b5 and \u03a3 leads to the result that \u00b5 is given by the expectation of x under p(x) and that \u03a3 is given by the covariance. 10.5 (\u22c6 \u22c6) www Consider a model in which the set of all hidden stochastic variables, denoted collectively by Z, comprises some latent variables z together with some model parameters \u03b8", "342f05c7-ae19-472f-b2f4-54c897c4d31e": "By contrast, traditional graphical models usually contain mostly variables that are at least occasionally observed, even if many of the variables are missing at random from some training examples. Traditional models mostly use higher-order terms and structure learning to capture complicated nonlinear interactions between variables. If there are latent variables, they are usually few in number. The way that latent variables are designed also differs in deep learning. The deep learning practitioner typically does not intend for the latent variables to take on any specific semantics ahead of time\u2014the training algorithm is free to invent the concepts it needs to model a particular dataset. The latent variables are usually not very easy for a human to interpret after the fact, though visualization techniques may allow some rough characterization of what they represent", "efa7e1a5-a588-4e4b-8f92-eacb01b7feed": "These skip connections make it easier for the gradient to flow from output layers to layers nearer the input. Another key consideration of architecture design is exactly how to connect a pair of layers to each other. In the default neural network layer described by a linear transformation via a matrix W, every input unit is connected to every output unit. Many specialized networks in the chapters ahead have fewer connections, so that each unit in the input layer is connected to only a small subset of units in the output layer. These strategies for decreasing the number of connections reduce the number of parameters and the amount of computation required to evaluate the network but are often highly problem dependent. For example, convolutional  198  CHAPTER 6.\n\nDEEP FEEDFORWARD NETWORKS  https://www.deeplearningbook.org/contents/mlp.html    e\u2014e 3, convolutional +\u2014 8, fully connected VV 11, convolutional  Test accuracy (percent)  0.0 0.2 0.4 0.6 0.8 1.0  Number of parameters x 108  Figure 6.7: Effect of number of parameters. Deeper models tend to perform better. This is not merely because the model is larger. This experiment from Goodfellow ef al", "90323231-bcc3-4ffc-8b02-664f8f26a82c": "Advances in Neural Information Processing Systems 27 (NIPS 2014 ), pp. 3014\u20133022. Curran Associates, Inc. Marbach, P., Tsitsiklis, J. N. Simulation-based optimization of Markov reward processes. Marbach, P., Tsitsiklis, J. N. Simulation-based optimization of Markov reward processes.\n\nIEEE Transactions on Automatic Control, 46(2):191\u2013209. Markram, H., L\u00a8ubke, J., Frotscher, M., Sakmann, B. Regulation of synaptic e\ufb03cacy by coincidence of postsynaptic APs and EPSPs. Science, 275:213\u2013215. Mart\u00b4\u0131nez, J. F., \u02d9Ipek, E. Dynamic multicore resource management: A machine learning Matsuda, W., Furuta, T., Nakamura, K. C., Hioki, H., Fujiyama, F., Arai, R., Kaneko, T", "3c93a787-c6c8-4302-9b9d-cf1eb1324ada": "As we have seen already, for the output units, we have provided we are using the canonical link as the output-unit activation function. To evaluate the \u03b4\u2019s for hidden units, we again make use of the chain rule for partial derivatives, where the sum runs over all units k to which unit j sends connections. The arrangement of units and weights is illustrated in Figure 5.7. Note that the units labelled k could include other hidden units and/or output units. In writing down (5.55), we are making use of the fact that variations in aj give rise to variations in the error function only through variations in the variables ak.\n\nIf we now substitute the de\ufb01nition of \u03b4 given by (5.51) into (5.55), and make use of (5.48) and (5.49), we obtain the following backpropagation formula which tells us that the value of \u03b4 for a particular hidden unit can be obtained by propagating the \u03b4\u2019s backwards from units higher up in the network, as illustrated in Figure 5.7. Note that the summation in (5.56) is taken over the \ufb01rst index on wkj (corresponding to backward propagation of information through the network), whereas in the forward propagation equation (5.10) it is taken over the second index", "966030ba-d61e-4c0e-9312-59d573dad74a": "More generally, a broad set of sophisticated algorithms, such as the policy gradient for reinforcement learning and the generative adversarial learning, can also be easily derived by plugging in speci\ufb01c designs of the experience function f and divergence D. Table 1 summarizes various speci\ufb01cations of the SE components that recover a range of existing well-known algorithms from di\ufb00erent paradigms. As shown in more detail in the subsequent sections, the standard equation (Equation 3.1 and 3.2) o\ufb00ers a uni\ufb01ed and universal paradigm for model training under many scenarios based on many types of experience, potentiating a turnkey implementation and a more generalizable theoretical characterization. The experience function f(t) in the standard equation can be instantiated to encode vastly distinct types of experience.\n\nDi\ufb00erent choices of f(t) result in learning algorithms applied to di\ufb00erent problems. With particular choices, the standard equation rediscovers a wide array of well-known algorithms. The resulting common treatment of the previously disparate algorithms is appealing as it o\ufb00ers new holistic insights into the commonalities and di\ufb00erences of those algorithms. Table 1 shows examples of extant algorithms that are recovered by the standard equation. 4.1. Data Instance Experience", "29b9461a-0e34-4b06-be16-f7e1c06f04fc": "The quantity \u03b3 de\ufb01ned by (3.91) therefore measures the effective total number of well determined parameters. We can obtain some insight into the result (3.95) for re-estimating \u03b2 by comparing it with the corresponding maximum likelihood result given by (3.21). Both of these formulae express the variance (the inverse precision) as an average of the squared differences between the targets and the model predictions.\n\nHowever, they differ in that the number of data points N in the denominator of the maximum likelihood result is replaced by N \u2212 \u03b3 in the Bayesian result. We recall from (1.56) that the maximum likelihood estimate of the variance for a Gaussian distribution over a and that this estimate is biased because the maximum likelihood solution \u00b5ML for the mean has \ufb01tted some of the noise on the data. In effect, this has used up one degree of freedom in the model. The corresponding unbiased estimate is given by (1.59) and takes the form We shall see in Section 10.1.3 that this result can be obtained from a Bayesian treatment in which we marginalize over the unknown mean. The factor of N \u2212 1 in the denominator of the Bayesian result takes account of the fact that one degree of freedom has been used in \ufb01tting the mean and removes the bias of maximum likelihood", "7b547039-9761-444e-9298-124a457459e0": "Consider a functional that is de\ufb01ned by an integral over a function G(y, y\u2032, x) that depends on both y(x) and its derivative y\u2032(x) as well as having a direct dependence on x where the value of y(x) is assumed to be \ufb01xed at the boundary of the region of integration (which might be at in\ufb01nity).\n\nIf we now consider variations in the function y(x), we obtain We now have to cast this in the form (D.3). To do so, we integrate the second term by parts and make use of the fact that \u03b7(x) must vanish at the boundary of the integral (because y(x) is \ufb01xed at the boundary). This gives from which we can read off the functional derivative by comparison with (D.3). Requiring that the functional derivative vanishes then gives which are known as the Euler-Lagrange equations. For example, if then the Euler-Lagrange equations take the form This second order differential equation can be solved for y(x) by making use of the boundary conditions on y(x)", "9427302c-9712-42ec-998b-f63514a18ba9": "The text and image encoders are jointly trained to maximize the similarity between N correct pairs of (image, text) associations while minimizing the similarity for N(N \u2014 1) incorrect pairs via a symmetric cross entropy loss over the  dense matrix. See the numy-like pseudo code for CLIP in Fig.\n\n17", "a5aa7129-0998-4eaa-9241-33aeb905b83b": "arge weights may also result in extreme values that cause the activation function  to saturate, causing complete loss of gradient through saturated units. These competing factors determine the ideal initial scale of the weights. The perspectives of regularization and optimization can give very different  298  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  insights into how we should initialize a network. The optimization perspective suggests that the weights should be large enough to propagate information suc- cessfully, but some regularization concerns encourage making them smaller.\n\nThe use of an optimization algorithm, such as stochastic gradient descent, that makes small incremental changes to the weights and tends to halt in areas that are nearer to the initial parameters (whether due to getting stuck in a region of low gradient, or due to triggering some early stopping criterion based on overfitting) expresses a prior that the final parameters should be close to the initial parameters. Recall from section 7.8 that gradient descent with early stopping is equivalent to weight decay for some models. In the general case, gradient descent with early stopping is not the same as weight decay, but it does provide a loose analogy for thinking about the effect of initialization", "ad0bda5d-41ab-40bc-b426-b495b79e3651": "The most important  (t) i units described in the previous section. Here, however, the self-loop weight (or the  component is the state unit s>\u2019, which has a linear self-loop similar to the leaky associated time constant) is controlled by a forget gate unit fo (for time step t and cell i), which sets this weight to a value between 0 and 1 via a sigmoid unit:  A =o (of +Culal + Owen). aoa)  j j where x) is the current input vector and h\u00ae is the current hidden layer vector, containing the outputs of all the LSTM cells, and b*, Uf, Wf are respectively biases, input weights, and recurrent weights for the forget gates.\n\nThe LSTM cell internal state is thus updated as follows, but with a conditional self-loop weight  : Pie => FOS) + go b; + S- Uz + ia , (10.41) j j  where 6, U and W respectively denote the biases, input weights, and recurrent weights into the LSTM cell", "187751cc-c6b4-441f-a379-737a0fb9b796": "Each dataset has the same number of examples as the original dataset, but each dataset is constructed by sampling with replacement from the original dataset. This means that, with high probability, each dataset is missing some of the examples from the  253  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  Original dataset  First resampled dataset First ensemble member  Second resampled dataset  DDO >  Figure 7.5: A cartoon depiction of how bagging works. Suppose we train an 8 detector on the dataset depicted above, containing an 8, a 6 and a 9. Suppose we make two different resampled datasets. The bagging training procedure is to construct each of these datasets by sampling with replacement. The first dataset omits the 9 and repeats the 8. On this dataset, the detector learns that a loop on top of the digit corresponds to an 8. On the second dataset, we repeat the 9 and omit the 6", "4c70035b-d63c-43d9-b753-5f213dfdfb5e": "(You may wish to review how this kind of scaling works, first explained in figure 2.3).\n\nAlong the directions where the eigenvalues of H are relatively large, for example, where \\; >> a, the effect of regularization is relatively small. Yet components with Ai < @ will be shrunk to have nearly zero magnitude. This effect is illustrated in figure 7.1. 228  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  https://www.deeplearningbook.org/contents/regularization.html    Figure 7.1: An illustration of the effect of L (or weight decay) regularization on the value of the optimal W. The solid ellipses represent contours of equal value of the unregularized objective. The dotted circles represent contours of equal value of theL regularizer. At the point w, these competing objectives reach an equilibrium. In the first dimension, the  eigenvalue of the Hessian of J is small. The objective function does not increase much when moving horizontally away from w*. Because the objective function does not express a strong preference along this direction, the regularizer has a strong effect on this axis", "0fd4af99-41d8-4a55-a738-aa829d7a3713": "The comparison between MLE and SQL(off) shows that the off-policy component of SQL is better than standard MLE training, as it incorporates reward signals instead of just blindly following the (noisy) data.\n\nNext, comparing with the previous steered decoding such as PPLM and GeDi, we can see the prompt-based control trained with RL achieves better trade-off between topic accuracy and language quality. Moreover, once a prompt is produced, we can use the pretrained LM to generate text of desired topics ef\ufb01ciently, with the same time cost as standard non-controlled decoding. In comparison, the dedicated steered decoding is often orders-ofmagnitude slower, as shown in Table 2. Standard RL algorithms can sometimes be oversensitive to the randomness in the environment. Recent works have considered maximum-entropy RL extensions, such as the soft Q-learning (SQL) , that maximize the entropy of policy besides the rewards, and demonstrated substantial improvement in robotic and game control . Our work is the \ufb01rst to adapt SQL and its advanced variants ) to the challenging text generation problem and show signi\ufb01cant results on diverse applications. Applying RL for text generation has been discussed in alleviating the exposure bias problem and optimizing task metrics", "e46d95c3-14d8-49ff-a3e7-ebbebb8e334b": "Synthesis lectures on artificial intelligence and machine learning 4.1 : 1-103. If you notice mistakes and errors in this post, please don\u2019t hesitate to contact me at  and | would be super happy to correct them right away!\n\nes es ee  Policy Gradient Algorithms The Multi-Armed Bandit Problem and Its  Solutions  https://lilianweng.github.io/posts/2018-02-19-rl-overview/", "efa922d2-2240-4289-a6d9-bdb780dcb7f4": "Each step involves a series of eye movements to obtain information and to guide reaching and locomotion. Rapid judgments are continually made about how to carry the objects or whether it is better to ferry some of them to the dining table before obtaining others. Each step is guided by goals, such as grasping a spoon or getting to the refrigerator, and is in service of other goals, such as having the spoon to eat with once the cereal is prepared and ultimately obtaining nourishment. Whether he is aware of it or not, Phil is accessing information about the state of his body that determines his nutritional needs, level of hunger, and food preferences.\n\nThese examples share features that are so basic that they are easy to overlook. All involve interaction between an active decision-making agent and its environment, within which the agent seeks to achieve a goal despite uncertainty about its environment. The agent\u2019s actions are permitted to a\u21b5ect the future state of the environment (e.g., the next chess position, the level of reservoirs of the re\ufb01nery, the robot\u2019s next location and the future charge level of its battery), thereby a\u21b5ecting the actions and opportunities available to the agent at later times", "f3fc690d-9560-43e1-a580-0ab8c90dad21": "For example, multiplying the input by 0.1 will artificially increase likelihood by a factor of 10. Issues with preprocessing commonly arise when benchmarking generative models on the MNIST dataset, one of the more popular generative modeling benchmarks. MNIST consists of grayscale images. Some models treat MNIST images as points in a real vector space, while others treat them as binary.\n\nYet others treat the grayscale values as probabilities for binary samples. It is essential to compare real-valued models only to other real-valued models and binary-valued models only o other binary-valued models. Otherwise the likelihoods measured are not on the same space. For binary-valued models, the log-likelihood can be at most zero, while for real-valued models, it can be arbitrarily high, since it is the measurement of a density. Among binary models, it is important to compare models using exactly he same kind of binarization. For example, we might binarize a gray pixel to 0 or 1 by thresholding at 0.5, or by drawing a random sample whose probability of being is given by the gray pixel intensity", "edd4c3ad-947a-4637-9496-7bc6e8eeac9d": "For example, a more complicated_penalty term can be derived by using a mixture of Gaussians, rather than a single Gaussian distribution, as the prior . 5.7 Supervised Learning Algorithms  Recall from section 5.1.3 that supervised learning algorithms are, roughly speaking, learning algorithms that learn to associate some input with some output, given a training set of examples of inputs x and outputs y. In many cases the outputs y may be difficult to collect automatically and must be provided by a human  137  CHAPTER 5. MACHINE LEARNING BASICS  \u201csupervisor,\u201d but the term still applies even when the training set targets were collected automatically. 5.7.1 Probabilistic Supervised Learning  Most supervised learning algorithms in this book are based on estimating a probability distribution p(y | #). We can do this simply by using maximum likelihood estimation to find the best parameter vector @ for a parametric family of distributions p(y | x; 0).\n\nWe have already seen that linear regression corresponds to the family Ply | @;0) =N(y;0'@, 1). (5.80)  We can generalize linear regression to the classification scenario by defining a different family of probability distributions", "46e9c35b-3d28-47f1-acf2-4f473079cb9a": "(4) Also note that we may rewrite z = \u03bby + (1 \u2212 \u03bb)x in the form Finally, combining the above we have, Proposition 1 \u2212 ln(x) is strictly convex on (0, \u221e). x2 > 0 for x \u2208 (0, \u221e). By Theorem (1), \u2212 ln(x) is strictly convex on (0, \u221e). Also, by De\ufb01nition (2) ln(x) is strictly concave on (0, \u221e). The notion of convexity can be extended to apply to n points. This result is known as Jensen\u2019s inequality. Theorem 2 (Jensen\u2019s inequality) Let f be a convex function de\ufb01ned on an interval I. If x1, x2, . , xn \u2208 I and \u03bb1, \u03bb2, . , \u03bbn \u2265 0 with \ufffdn i=1 \u03bbi = 1, Proof: For n = 1 this is trivial. The case n = 2 corresponds to the de\ufb01nition of convexity", "26741a61-2524-4d7a-9a3b-be1f273ba4e2": "First, it makes use of  528  CHAPTER 15.\n\nREPRESENTATION LEARNING  the idea that the choice of initial parameters for a deep neural network can have a significant regularizing effect on the model (and, to a lesser extent, that it can improve optimization). Second, it makes use of the more general idea that learning about the input distribution can help with learning about the mapping from inputs to outputs. Both ideas involve many complicated interactions between several parts of the machine learning algorithm that are not entirely understood. The first idea, that the choice of initial parameters for a deep neural network can have a strong regularizing effect on its performance, is the least understood. At the time that pretraining became popular, it was understood as initializing the model in a location that would cause it to approach one local minimum rather than another. Today, local minima are no longer considered to be a serious problem for neural network optimization. We now know that our standard neural network training procedures usually do not arrive at a critical point of any kind", "6a906160-aae9-42e1-b1e9-1bfc7e7ef4c9": "The results above suggest a simple way of achieving this, namely by taking the available data and partitioning it into a training set, used to determine the coef\ufb01cients w, and a separate validation set, also called a hold-out set, used to optimize the model complexity (either M or \u03bb). In many cases, however, this will prove to be too wasteful of valuable training data, and we have to seek more sophisticated approaches. Section 1.3 So far our discussion of polynomial curve \ufb01tting has appealed largely to intuition. We now seek a more principled approach to solving problems in pattern recognition by turning to a discussion of probability theory. As well as providing the foundation for nearly all of the subsequent developments in this book, it will also give us some important insights into the concepts we have introduced in the context of polynomial curve \ufb01tting and will allow us to extend these to more complex situations. A key concept in the \ufb01eld of pattern recognition is that of uncertainty. It arises both through noise on measurements, as well as through the \ufb01nite size of data sets.\n\nProbability theory provides a consistent framework for the quanti\ufb01cation and manipulation of uncertainty and forms one of the central foundations for pattern recognition", "2c11a40d-c3fb-4796-a31b-f120c8e20404": "Figure 14.3 shows the behavior of the TD model (again with the presence representation) in a higher-order conditioning experiment\u2014in this case it is second-order conditioning. In the \ufb01rst phase (not shown in the \ufb01gure), CSB is trained to predict a US so that its associative strength increases, here to 1.65. In the second phase, CSA is paired with CSB in the absence of the US, in the sequential arrangement shown at the top of the \ufb01gure. CSA acquires associative strength even though it is never paired with the US. With continued training, CSA\u2019s associative strength reaches a peak and then decreases because the associative strength of CSB, the secondary reinforcer, decreases so that it loses its ability to provide secondary reinforcement. CSB\u2019s associative can di\u21b5er from \u02c6v(St,wt), making \u03b4t non-zero (a temporal di\u21b5erence). This di\u21b5erence has the same status as Rt+1 in (14.5), implying that as far as learning is concerned there is no di\u21b5erence between a temporal di\u21b5erence and the occurrence of a US", "c5e730d3-e6fd-48c9-b448-2f7143f3e793": ", xN}. We immediately see that the situation is now much more complex than with a single Gaussian, due to the presence of the summation over k inside the logarithm. As a result, the maximum likelihood solution for the parameters no longer has a closed-form analytical solution. One approach to maximizing the likelihood function is to use iterative numerical optimization techniques . Alternatively we can employ a powerful framework called expectation maximization, which will be discussed at length in Chapter 9. The probability distributions that we have studied so far in this chapter (with the exception of the Gaussian mixture) are speci\ufb01c examples of a broad class of distributions called the exponential family . Members of the exponential family have many important properties in common, and it is illuminating to discuss these properties in some generality. The exponential family of distributions over x, given parameters \u03b7, is de\ufb01ned to be the set of distributions of the form where x may be scalar or vector, and may be discrete or continuous", "4a0db538-ee43-4b4c-8142-ced74a420394": "As illustrated in Figure 8.10, each iteration traverses the tree by simulating actions guided by statistics associated with the tree\u2019s edges.\n\nIn its basic version, when a simulation reaches a leaf node of the search tree, MCTS expands the tree by adding some, or all, of the leaf node\u2019s children to the tree. From the leaf node, or one of its newly added child nodes, a rollout is executed: a simulation that typically proceeds all the way to a terminal state, with actions selected by a rollout policy. When the rollout completes, the statistics associated with the search tree\u2019s edges that were traversed in this iteration are updated by backing up the return produced by the rollout. MCTS continues this process, starting each time at the search tree\u2019s root at the current state, for as many iterations as possible given the time constraints. Then, \ufb01nally, an action from the root node (which still represents the current environment state) is selected according to statistics accumulated in the root node\u2019s outgoing edges. This is the action the agent takes. After the environment transitions to its next state, MCTS is executed again with the root node set to represent the new current state", "310e5b5a-00fe-421e-847c-42b08a689964": "The VAE framework has been extended to maximize not just the traditional vari- ational lower bound, but also the importance-weighted autoencoder  objective:  oO 20)  Pt Lz Ly(@, 9) = Ea), alk) wq(z|e) De aoe (20.79)  This new objective is equivalent to the traditional lower bound \u00a3 when k = 1. How- ever, it may also be interpreted as forming an estimate of the true log pPyoqe] (2) using importance sampling of z from proposal distribution q(z | x). The importance- weighted autoencoder objective is also a lower bound on log Pmodei (#) and becomes tighter as & increases. Variational autoencoders have some interesting connections to the MP-DBM and other approaches that involve back-propagation through the approximate inference graph .\n\nhttps://www.deeplearningbook.org/contents/generative_models.html    These previous approaches required an inference procedure such as mean field fixed- oint equations to provide the computational graph", "6329971d-7d48-4ad3-b189-1681fd48225e": "The Keras  library provides an ImageDataGenerator class that greatly facilitates the implementa- tion of geometric augmentations. Buslaev et al. presented another augmentation tool they called Albumentations .\n\nThe development of Neural Style Transfer, adversar- ial training, GANs, and meta-learning APIs will help engineers utilize the performance  power of advanced Data Augmentation techniques much faster and more easily. Conclusion  This survey presents a series of Data Augmentation solutions to the problem of overfit- ting in Deep Learning models due to limited data. Deep Learning models rely on big data to avoid overfitting. Artificially inflating datasets using the methods discussed in this survey achieves the benefit of big data in the limited data domain. Data Augmen- tation is a very useful technique for constructing better datasets. Many augmentations have been proposed which can generally be classified as either a data warping or over- sampling technique. The future of Data Augmentation is very bright. The use of search algorithms com- bining data warping and oversampling methods has enormous potential. The layered architecture of deep neural networks presents many opportunities for Data Augmen- tation", "82751659-4f10-4476-acd7-7e29a78727f8": "Consider the three-state episodic MRP shown to the right.\n\nEpisodes begin in state A and then \u2018split\u2019 stochastically, half the time going to B (and then invariably going on to terminate with a reward of 1) and half the time going to state C (and then invariably terminating with a reward of zero). Reward for the \ufb01rst transition, out of A, is always zero whichever way the episode goes. As this is an episodic problem, we can take \u03b3 to be 1. We also assume on-policy training, so that \u21e2t is always 1, and tabular function approximation, so that the learning algorithms are free to give arbitrary, independent values to all three states. Thus, this should be an easy problem. What should the values be? From A, half the time the return is 1, and half the value should be 1, and similarly from C the return is always 0, so its value should be 0. These are the true values and, as this is a tabular problem, all the methods presented previously converge to them exactly. However, the naive residual-gradient algorithm \ufb01nds di\u21b5erent values for B and 2). These are in fact the values that minimize the TDE", "ee5dddbc-37fd-448e-8b3b-f139dc3c46f1": "Each leaf of the tree is then associated with a speci\ufb01c diagnosis. In order to learn such a model from a training set, we have to determine the structure of the tree, including which input variable is chosen at each node to form the split criterion as well as the value of the threshold parameter \u03b8i for the split. We also have to determine the values of the predictive variable within each region. Consider \ufb01rst a regression problem in which the goal is to predict a single target variable t from a D-dimensional vector x = (x1, . , xD)T of input variables. The training data consists of input vectors {x1, . , xN} along with the corresponding continuous labels {t1, . , tN}.\n\nIf the partitioning of the input space is given, and we minimize the sum-of-squares error function, then the optimal value of the predictive variable within any given region is just given by the average of the values of tn for those data points that fall in that region. Exercise 14.10 Now consider how to determine the structure of the decision tree", "91f5e89c-d1b1-4931-9fbe-c2683be6d636": "The optimization-based formulation of Bayesian inference in Equation 2.15 o\ufb00ers important additional \ufb02exibility in learning by allowing rich constraints on machine learning models to be imposed to regularize the outcome. For example, in Equation 2.15 we have seen the standard normality constraint of a probability distribution being imposed on the posterior q. It is natural to consider other types of constraints that encode richer problem structures and domain knowledge, which can regularize the model to learn desired behaviors.\n\nThe idea has led to posterior regularization  or regularized Bayes , which augments the Bayesian inference objective with additional constraints: where we have rearranged the terms and dropped any constant factors in Equation 2.15, and added constraints with \u03be being a vector of slack variables, U(\u03be) a penalty function (e.g., \u21131 norm of \u03be), and Q(\u03be) a subset of valid distributions over \u03b8 that satisfy the constraints determined by \u03be. The optimization problem is generally easy to solve when the penalty/constraints are convex and de\ufb01ned w.r.t. a linear operator (e.g., expectation) of the posterior q", "f9ad31c8-93f0-46e3-b35a-f50fbb577428": "A. Comparing policy-gradient algorithms. Sutton, R. S., Szepesv\u00b4ari, Cs., Geramifard, A., Bowling, M., . Dyna-style planning with linear function approximation and prioritized sweeping. In Proceedings of the 24th Conference on Uncertainty in Arti\ufb01cial Intelligence, pp. 528\u2013536. Szepesv\u00b4ari, Cs. Algorithms for reinforcement learning. In Synthesis Lectures on Arti\ufb01cial Intelligence and Machine Learning, 4(1):1\u2013103. Morgan and Claypool. Szita, I. Reinforcement learning in games. In M. Wiering and M. van Otterlo (Eds. ), Reinforcement Learning: State-of-the-Art, pp. 539\u2013577. Springer-Verlag Berlin Heidelberg. undiscounted average reward. Technical Report 94-30-01. Oregon State University, Computer Science Department, Corvallis", "513ecc14-56ca-4f12-bc64-41d89ba64b00": "These techniques have been applied to models such as RBMs . Another approach is to use parallel tempering , in which the Markov chain simulates many different states in parallel, at different temperatures.\n\nThe highest temperature states mix slowly, while the lowest temperature states, at temperature 1, provide accurate samples from the model. The transition operator includes stochastically swapping states between two different temperature levels, so that a sufficiently high- probability sample from a high-temperature slot can jump into a lower temperature slot. This approach has also been applied to RBMs . Although tempering is a promising approach, at this point it has not allowed researchers to make a strong advance in solving the challenge of sampling from complex EBMs. One possible reason is that there are critical temperatures around which the temperature transition must be very slow (as the temperature is gradually reduced) for tempering to be effective. 17.5.2 Depth May Help Mixing  When drawing samples from a latent variable model p(h, a), we have seen that if p(h | x) encodes x too well, then sampling from p(a | h) will not change \u00ab very much, and mixing will be poor", "b6a63125-2ea7-4485-ae9a-97b84e23f68d": "Representational regularization is accomplished by the same sorts of mechanisms that we have used in parameter regularization. Norm penalty regularization of representations is performed by adding to the loss function J a norm penalty on the representation. This penalty is denoted Q(h). As before, we denote the regularized loss function by J:  J(0;X,y) = J(O;X,y) + aQ(h), (7.48)  where a \u20ac [0,00) weights the relative contribution of the norm penalty term, with larger values of @ corresponding to more regularization. Just as an L! penalty on the parameters induces parameter sparsity, an L! penalty on the elements of the representation induces representational sparsity: QO(h) = ||Al|: = 30; |Ri|. Of course, the L+ penalty is only one choice of penalty that can result in a sparse representation.\n\nOthers include the penalty derived from a Student \u00a2 prior on the representation  and KL divergence penalties , which are especially useful for representations with elements constrained to lie on the unit interval. Lee et al. and Goodfellow et al", "41c626c5-2c42-4d37-968b-df8ae977afd7": "Uti- lizing evolutionary and random search algorithms is an interesting area of future work, but the meta-learning schemes reviewed in this survey are all neural-network, gradient-based. The history of Deep Learning advancement from feature engineering such as SIFT  and HOG  to architecture design such as AlexNet , VGGNet , and Inception-V3 , suggest that meta-architecture design is the next paradigm shift. NAS takes a novel approach to meta-learning architectures by using a recurrent net- work trained with Reinforcement Learning to design architectures that result in the best accuracy. On the CIFAR-10 dataset, this achieved an error rate of 3.65 (Fig. 28). This section will introduce three experiments using meta-learning for Data Aug- mentation. These methods use a prepended neural network to learn Data Augmenta-  tions via mixing images, Neural Style Transfer, and geometric transformations.\n\nNeural augmentation The Neural Style Transfer algorithm requires two parameters for the weights of the style and content loss. Perez and Wang  presented an algo- rithm to meta-learn a Neural Style Transfer strategy called Neural Augmentation. The Neural Augmentation approach takes in two random images from the same class", "2c6122a6-93c2-46e2-a592-5bb665d4c4cb": "All of these distributions are examples of the exponential family of distributions, which possess a number of important properties, and which will be discussed in some detail. One limitation of the parametric approach is that it assumes a speci\ufb01c functional form for the distribution, which may turn out to be inappropriate for a particular application. An alternative approach is given by nonparametric density estimation methods in which the form of the distribution typically depends on the size of the data set. Such models still contain parameters, but these control the model complexity rather than the form of the distribution. We end this chapter by considering three nonparametric methods based respectively on histograms, nearest-neighbours, and kernels. We begin by considering a single binary random variable x \u2208 {0, 1}. For example, x might describe the outcome of \ufb02ipping a coin, with x = 1 representing \u2018heads\u2019, and x = 0 representing \u2018tails\u2019.\n\nWe can imagine that this is a damaged coin so that the probability of landing heads is not necessarily the same as that of landing tails", "2f0ad765-b562-4acd-98c7-5af585ed213a": "This arises because any \ufb01nite value for \u03b1 will always assign a lower probability to the data, thereby decreasing the value of the density at t, provided that \u03b2 is set to its optimal value.\n\nWe see that any \ufb01nite value for \u03b1 would cause the distribution to be elongated in a direction away from the data, thereby increasing the probability mass in regions away from the observed data and hence reducing the value of the density at the target data vector itself. For the more general case of M basis vectors \u03d51, . , \u03d5M a similar intuition holds, namely that if a particular basis vector is poorly aligned with the data vector t, then it is likely to be pruned from the model. We now investigate the mechanism for sparsity from a more mathematical perspective, for a general case involving M basis functions. To motivate this analysis we \ufb01rst note that, in the result (7.87) for re-estimating the parameter \u03b1i, the terms on the right-hand side are themselves also functions of \u03b1i. These results therefore represent implicit solutions, and iteration would be required even to determine a single \u03b1i with all other \u03b1j for j \u0338= i \ufb01xed", "64d78e44-421a-4caf-8c6d-bbc8d2a46a6c": "Even for a \ufb01xed number of nodes in the tree, the problem of determining the optimal structure (including choice of input variable for each split as well as the corresponding thresholds) to minimize the sum-of-squares error is usually computationally infeasible due to the combinatorially large number of possible solutions. Instead, a greedy optimization is generally done by starting with a single root node, corresponding to the whole input space, and then growing the tree by adding nodes one at a time. At each step there will be some number of candidate regions in input space that can be split, corresponding to the addition of a pair of leaf nodes to the existing tree. For each of these, there is a choice of which of the D input variables to split, as well as the value of the threshold. The joint optimization of the choice of region to split, and the choice of input variable and threshold, can be done ef\ufb01ciently by exhaustive search noting that, for a given choice of split variable and threshold, the optimal choice of predictive variable is given by the local average of the data, as noted earlier.\n\nThis is repeated for all possible choices of variable to be split, and the one that gives the smallest residual sum-of-squares error is retained", "fe9ef1ef-c6c3-4135-b862-00e18ceb5446": "We can determine the mean and covariance of the joint distribution recursively as follows.\n\nEach variable xi has (conditional on the states of its parents) a Gaussian distribution of the form (8.11) and so where \u03f5i is a zero mean, unit variance Gaussian random variable satisfying E = 0 and E = Iij, where Iij is the i, j element of the identity matrix. Taking the expectation of (8.14), we have Thus we can \ufb01nd the components of E = (E, . , E)T by starting at the lowest numbered node and working recursively through the graph (here we again assume that the nodes are numbered such that each node has a higher number than its parents). Similarly, we can use (8.14) and (8.15) to obtain the i, j element of the covariance matrix for p(x) in the form of a recursion relation and so the covariance can similarly be evaluated recursively starting from the lowest numbered node. Let us consider two extreme cases. First of all, suppose that there are no links in the graph, which therefore comprises D isolated nodes", "170c3304-c769-4fcd-8040-9016ececa305": "Either of the two SGVB estimators in section 2.3 can be used. We use settings M = 100 and L = 1 in experiments. XM \u2190 Random minibatch of M datapoints (drawn from full dataset) \u03f5 \u2190 Random samples from noise distribution p(\u03f5) g \u2190 \u2207\u03b8,\u03c6 \ufffdLM(\u03b8, \u03c6; XM, \u03f5) (Gradients of minibatch estimator (8)) \u03b8, \u03c6 \u2190 Update parameters using gradients g (e.g. SGD or Adagrad ) Often, the KL-divergence DKL(q\u03c6(z|x(i))||p\u03b8(z)) of eq. (3) can be integrated analytically (see appendix B), such that only the expected reconstruction error Eq\u03c6(z|x(i)) \ufffd log p\u03b8(x(i)|z) \ufffd requires estimation by sampling", "3fc94e67-b3ab-48be-960a-a3f766912fce": "Recall that the eigendecomposition involves analyzing a matrix A to discover a matrix V of eigenvectors and a vector of eigenvalues A such that we can rewrite Aas A=Vdiag(A)V~!. (2.42)  The singular value decomposition is similar, except this time we will write A as a product of three matrices:  A=UDV'. (2.43)  Suppose that A is an m xn matrix. Then U is defined to be an m x m matrix, D to be an m X n matrix, and V to be an n x n matrix. Each of these matrices is defined to have a special structure. The matrices U and V are both defined to be orthogonal matrices. The matrix D is defined to be a diagonal matrix. Note that D is not necessarily square. The elements along the diagonal of D are known as the singular values of the matrix A. The columns of U are known as the left-singular vectors. The columns of V are known as as the right-singular vectors", "e8753007-e1ae-4716-bf36-fb00d8411459": "The dealer hits or sticks according to a \ufb01xed strategy without choice: he sticks on any sum of 17 or greater, and hits otherwise. If the dealer goes bust, then the player wins; otherwise, the outcome\u2014win, lose, or draw\u2014is determined by whose \ufb01nal sum is closer to 21. Playing blackjack is naturally formulated as an episodic \ufb01nite MDP. Each game of blackjack is an episode. Rewards of +1, \u22121, and 0 are given for winning, losing, and drawing, respectively. All rewards within a game are zero, and we do not discount (\u03b3 = 1); therefore these terminal rewards are also the returns. The player\u2019s actions are to hit or to stick. The states depend on the player\u2019s cards and the dealer\u2019s showing card. We assume that cards are dealt from an in\ufb01nite deck (i.e., with replacement) so that there is no advantage to keeping track of the cards already dealt", "c0f7f31a-9ac8-4de0-967f-89d1279dd28f": ", M \u2212 1} This of course can be represented as a directed graph in the form of a chain, an example of which is shown in Figure 8.38. We can then specify the Markov chain by giving the probability distribution for the initial variable p(z(0)) together with the conditional probabilities for subsequent variables in the form of transition probabilities Tm(z(m), z(m+1)) \u2261 p(z(m+1)|z(m)). A Markov chain is called homogeneous if the transition probabilities are the same for all m. The marginal probability for a particular variable can be expressed in terms of the marginal probability for the previous variable in the chain in the form A distribution is said to be invariant, or stationary, with respect to a Markov chain if each step in the chain leaves that distribution invariant. Thus, for a homogeneous Markov chain with transition probabilities T(z\u2032, z), the distribution p\u22c6(z) is invariant if p\u22c6(z) = \ufffd Note that a given Markov chain may have more than one invariant distribution. For instance, if the transition probabilities are given by the identity transformation, then any distribution will be invariant", "d3717640-3281-4ce0-9944-80ecce8edb22": "Likewise, undirected models can include substructures that no directed model can represent perfectly.\n\nSpecifically, a directed graph D cannot capture all the conditional independences implied by an undirected graph U if U contains a loop of length greater than three, unless that loop also contains a chord. A loop is a sequence of variables connected by undirected edges, with the last variable in the sequence connected back to the first variable in the sequence. A chord is a connection between any two nonconsecutive variables in the sequence defining a loop. If has loops of length four or greater and does not have chords for these loops, we must add the chords before we can convert it to a directed model. Adding these chords discards some of the independence information that was encoded in UW. The graph formed by adding chords to UY is known as a chordal, or triangulated, graph, because all the loops can now be described in terms of smaller, triangular loops. To build a directed graph D from the chordal graph, we need to also assign directions to the edges. When doing so, we must not create a directed cycle in D, or the result will not define a valid directed probabilistic model", "1967dcb0-27e2-4563-8580-6c569684edc8": "But the concept of one person being a space is also very confusing, and can be very dif\ufb01cult to obtain.\\n So, politics: the primary referendum is In summary, the outcome will be a referendum on the EU membership for the \ufb01rst time of its kind for EU citizens, and the full extent of the bene\ufb01ts of a single market and a \ufb02exible single EU state.\" computers: macintoshintoshintoshintosh In summary, it appears that the company and IBM products are currently in need of upgrading the computer. This can be seen in a detailed review of the Macintosh version of Windows Vista and XP.\n\nHowever, when looking at the changes made by the HP Macintosh hardware and software versions of space: legal space science and space In summary:\\n\\n The purpose of this paper is to investigate and test the theory of space space and other objects. This project will support NASA.s efforts to demonstrate these theories, and to propose other relevant new theories.\\n This paper provides the following arguments for the religion: space legal religion religion religion In summary, to the author the current discussion is the position of the Church and the community. While we acknowledge that we should not be commenting upon claims such as our recent cases or the other ones that contradict our view, we conclude it is appropriate to include these cases", "7c0c0f42-143c-46f3-a860-ff2cd9dae5a1": "The specific strides and depths used in this figure are not advisable for real use; they are designed to be very shallow to fit onto the page.\n\nReal convolutional networks also often involve significant amounts of branching, unlike the chain structures used here for simplicity. (Left)A convolutional network that processes a fixed image size. After alternating between convolution and pooling for a few layers, the tensor for the convolutional feature map is reshaped to flatten out the spatial dimensions. The rest of the network is an ordinary feedforward network classifier, as described in chapter 6. (Center)A convolutional network that processes a variably sized image but still maintains a fully connected section. This network uses a pooling operation with variably sized pools but a fixed number of pools,  https://www.deeplearningbook.org/contents/convnets.html     im order to provide a nxed-size vector of 5/6 units to the fully connected portion of the network. are designed to use pooling on some channels but not on other channels, in order to get both highly invariant features and features that will not underfit when the translation invariance prior is incorrect", "1acd073b-eaaf-4f41-b28c-9da9a568992d": "This often happens when a manifold intersects itself. For example, a figure eight is a manifold that has a single dimension in most places but two dimensions at the intersection at the center. Many machine learning problems seem hopeless if we expect the machine learning algorithm to learn functions with interesting variations across all of R\u201d.\n\nManifold learning algorithms surmount this obstacle by assuming that most of R\u201d consists of invalid inputs, and that interesting inputs occur only along a collection of manifolds containing a small subset of points, with interesting variations in the output of the learned function occurring only along directions that lie on the manifold, or with interesting variations happening only when we move from one manifold to another. Manifold learning was introduced in the case of continuous-valued data and in the unsupervised learning setting, although this probability concentration idea can be generalized to both discrete data and the supervised learning setting: the key assumption remains that probability mass is highly concentrated. The assumption that the data lies along a low-dimensional manifold may not always be correct or useful", "fd8544e1-5f79-4795-b629-7a99234f9ca9": "The Expectation Maximization Algorithm A short tutorial Comments and corrections to: em-tut@seanborman.com Added Figure (1). Corrected typo above Equation (5). Minor corrections. Added hyperlinks. This tutorial discusses the Expectation Maximization (EM) algorithm of Dempster, Laird and Rubin . The approach taken follows that of an unpublished note by Stuart Russel, but \ufb02eshes out some of the gory details. In order to ensure that the presentation is reasonably self-contained, some of the results on which the derivation of the algorithm is based are presented prior to the presentation of the main results. The EM algorithm has become a popular tool in statistical estimation problems involving incomplete data, or in problems which can be posed in a similar form, such as mixture estimation . The EM algorithm has also been used in various motion estimation frameworks  and variants of it have been used in multiframe superresolution restoration methods which combine motion estimation along the lines of . De\ufb01nition 1 Let f be a real valued function de\ufb01ned on an interval I =", "516e4ca0-8c90-486d-b3e1-4a36fc7eeabd": "The model transforms samples of latent variables z to samples x or to distributions over samples x using a differentiable function g(z; 99) ), which is typically represented by a neural network.\n\nThis model class includes variational autoencoders, which pair the generator net with an inference net; generative adversarial networks, which pair the generator network with a discriminator network; and techniques that train generator networks in isolation. Generator networks are essentially just parametrized computational procedures for generating samples, where the architecture provides the family of possible distributions to sample from and the parameters select a distribution from within that family. https://www.deeplearningbook.org/contents/generative_models.html    _ As an example, the standard procedure for drawing samples from a normal distribution with mean Ht and covariance is to feed samples 2 from a normal distribution with zero mean and identity covariance into a very simple generator  690  CHAPTER 20. DEEP GENERATIVE MODELS  network. This generator network contains just one affine layer: w= 9(z)=n4+ Lz, (20.71)  where L is given by the Cholesky decomposition of &", "11c65a03-46b8-40e9-a065-fe7cfc14cb86": "4.9 (\u22c6) www Consider a generative classi\ufb01cation model for K classes de\ufb01ned by prior class probabilities p(Ck) = \u03c0k and general class-conditional densities p(\u03c6|Ck) where \u03c6 is the input feature vector. Suppose we are given a training data set {\u03c6n, tn} where n = 1, . , N, and tn is a binary target vector of length K that uses the 1-ofK coding scheme, so that it has components tnj = Ijk if pattern n is from class Ck. Assuming that the data points are drawn independently from this model, show that the maximum-likelihood solution for the prior probabilities is given by where Nk is the number of data points assigned to class Ck.\n\nShow that the maximum likelihood solution for the mean of the Gaussian distribution for class Ck is given by which represents the mean of those feature vectors assigned to class Ck. Similarly, show that the maximum likelihood solution for the shared covariance matrix is given by Thus \u03a3 is given by a weighted average of the covariances of the data associated with each class, in which the weighting coef\ufb01cients are given by the prior probabilities of the classes", "7536efe1-9599-491b-98f9-99fc46e92785": "More concretely, we can quantify the asymptotic runtime of various operations using appropriate or inappropriate representations. For example, inserting a number into the correct position in a sorted list of numbers is an O(n) operation if the list is represented as a linked list, but only O(log n) if the list is represented as a red-black tree. In the context of machine learning, what makes one representation better than  https://www.deeplearningbook.org/contents/representation.html    CHAPTER 15. REPRESENTATION LEARNING  another? Generally speaking, a good representation is one that makes a subsequent earning task easier. The choice of representation will usually depend on the choice of the subsequent learning task. We can think of feedforward networks trained by supervised learning as per- orming a kind of representation learning. Specifically, the last layer of the network is typically a linear classifier, such as a softmax regression classifier. The rest of he network learns to provide a representation to this classifier", "750f7401-2109-4f1d-beb2-d8c74c10ae94": "These issues are discussed further when we present these types of networks in part III. Pooling in convolutional Boltzmann machines is presented in section 20.6. The inverse-like operations on pooling units needed in some differentiable networks are covered in section 20.10.6. Some examples of complete convolutional network architectures for classification using convolution and pooling are shown in figure 9.11. 9.4 Convolution and Pooling as an Infinitely Strong Prior  Recall the concept of a prior probability distribution from section 5.6. This is a probability distribution over the parameters of a model that encodes our beliefs about what models are reasonable, before we have seen any data. Priors can be considered weak or strong depending on how concentrated the probability density in the prior is. A weak prior is a prior distribution with high entropy, such as a Gaussian distribution with high variance. Such a prior allows the data to move the parameters more or less freely.\n\nA strong prior has very low entropy, such as a Gaussian distribution with low variance. Such a prior plays a  https://www.deeplearningbook.org/contents/convnets.html   more active role in determining where the parameters end up", "2484f769-fc0c-4bf8-859d-9866b4ec5ab9": "Calculus and algebra  220  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  have long been used to solve optimization problems in closed form, but gradient descent was not introduced as a technique for iteratively approximating the solution to optimization problems until the nineteenth century .\n\nBeginning in the 1940s, these function approximation techniques were used to motivate machine learning models such as the perceptron. However, the earliest models were based on linear models. Critics including Marvin Minsky pointed out several of the flaws of the linear model family, such as its inability to learn the XOR function, which led to a backlash against the entire neural network approach. Learning nonlinear functions required the development of a multilayer per- ceptron and a means of computing the gradient through such a model. Efficient applications of the chain rule based on dynamic programming began to appear in the 1960s and 1970s, mostly for control applications  but also for sensitivity analysis . Werbos  proposed applying these  bachetaeenn ba bententi wn 224A atn) 24-2 aden een ML tan", "46c32030-432e-4b1f-aa85-598fbcc688d4": "In this case, the \ufb01nal hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM. In all of our experiments, we mask 15% of all WordPiece tokens in each sequence at random. In contrast to denoising auto-encoders , we only predict the masked words rather than reconstructing the entire input.\n\nAlthough this allows us to obtain a bidirectional pre-trained model, a downside is that we are creating a mismatch between pre-training and \ufb01ne-tuning, since the  token does not appear during \ufb01ne-tuning. To mitigate this, we do not always replace \u201cmasked\u201d words with the actual  token. The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the  token 80% of the time (2) a random token 10% of the time (3) the unchanged i-th token 10% of the time. Then, Ti will be used to predict the original token with cross entropy loss. We compare variations of this procedure in Appendix C.2", "02ba58cb-dbf5-4799-89bc-a8ef31c9d34a": "Consider a Gaussian process with a two-dimensional input space x = (x1, x2), having a kernel function of the form Samples from the resulting prior over functions y(x) are shown for two different settings of the precision parameters \u03b7i in Figure 6.9. We see that, as a particular parameter \u03b7i becomes small, the function becomes relatively insensitive to the corresponding input variable xi. By adapting these parameters to a data set using maximum likelihood, it becomes possible to detect input variables that have little effect on the predictive distribution, because the corresponding values of \u03b7i will be small. This can be useful in practice because it allows such inputs to be discarded. ARD is illustrated using a simple synthetic data set having three inputs x1, x2 and x3  in Figure 6.10. The target variable t, is generated by sampling 100 values of x1 from a Gaussian, evaluating the function sin(2\u03c0x1), and then adding Gaussian noise.\n\nValues of x2 are given by copying the corresponding values of x1 and adding noise, and values of x3 are sampled from an independent Gaussian distribution", "ed99a9cc-7bc0-4d5b-aa69-9192e5eeeddd": "This can be seen by noting that in Figure 13.19, the variables z(1) which is head-to-head at node xn and hence they are not d-separated. The exact E step for this model does not correspond to running forward and backward recursions along the M Markov chains independently. This is con\ufb01rmed by noting that the key conditional independence property (13.5) is not satis\ufb01ed for the individual Markov chains in the factorial HMM model, as is shown using d-separation in Figure 13.20.\n\nNow suppose that there are M chains of hidden nodes and for simplicity suppose that all latent variables have the same number K of states. Then one approach would be to note that there are KM combinations of latent variables at a given time step so the conditional independence property (13.5) does not hold for the individual latent chains of the factorial HMM model. As a consequence, there is no ef\ufb01cient exact E step for this model. and so we can transform the model into an equivalent standard HMM having a single chain of latent variables each of which has KM latent states. We can then run the standard forward-backward recursions in the E step", "1a4a039a-dded-4fd1-b8e5-b22d070ee3f1": "experiment with a unique kernel filter that randomly swaps the pixel values in an 1 xx sliding window. They call this augmentation technique PatchShuffle Regularization. Experimenting across different filter sizes and probabilities of shuffling the pixels at each step, they demonstrate the effectiveness of this by achiev- ing a 5.66% error rate on CIFAR-10 compared to an error rate of 6.33% achieved with- out the use of PatchShuffle Regularization. The hyperparameter settings that achieved this consisted of 2 x 2 filters and a 0.05 probability of swapping. These experiments were done using the ResNet  CNN architecture (Figs. 5, 6). Kernel filters are a relatively unexplored area for Data Augmentation. A disadvantage of this technique is that it is very similar to the internal mechanisms of CNNs. CNNs have parametric kernels that learn the optimal way to represent images layer-by-layer", "680aa22b-0f1a-4323-abdd-283f23e87b18": "Along with this, it is natural to use lower case for value functions (e.g., v\u21e1) and restrict capitals to their tabular estimates (e.g., Qt(s, a)). Approximate value functions are deterministic functions of random parameters and are thus also in lower case (e.g., \u02c6v(s,wt) \u21e1 v\u21e1(s)). Vectors, such as the weight vector wt (formerly \u2713t) and the feature vector xt (formerly \u03c6t), are bold and written in lowercase even if they are random variables. Uppercase bold is reserved for matrices. In the \ufb01rst edition we used special notations, Pa probabilities and expected rewards. One weakness of that notation is that it still did not fully characterize the dynamics of the rewards, giving only their expectations, which is su\ufb03cient for dynamic programming but not for reinforcement learning.\n\nAnother weakness is the excess of subscripts and superscripts. In this edition we use the explicit notation of p(s0, r|s, a) for the joint probability for the next state and reward given the current state and action", "b339ea9e-ff9b-4960-a193-1f0ed6c5321d": "W., Atkeson, C. G. Prioritized sweeping: Reinforcement learning with less data and less real time. Machine Learning, 13(1):103\u2013130. Moore, A. W., Schneider, J., Deng, K. E\ufb03cient locally weighted polynomial regression predictions. In Proceedings of the 14th International Conference on Machine Learning . Morgan Kaufmann. cerebellar implementation of the sutton-barto-desmond model. In J. H. Byrne and W. O. Berry (Eds. ), Neural Models of Plasticity, pp. 187\u2013207. Academic Press, San Diego, CA. Moore, J. W., Choi, J.-S., Brunzell, D. H. Predictive timing under temporal uncertainty: Collyer (Eds. ), Timing of Behavior, pp.\n\n3\u201334. MIT Press, Cambridge, MA. Moore, J. W., Desmond, J", "546016f6-4080-4fba-8083-fdcbca7fe519": "Show that as a consequence of this constraint, the elements of the model prediction y(x) given by the least-squares solution (4.17) also satisfy this constraint, so that To do so, assume that one of the basis functions \u03c60(x) = 1 so that the corresponding parameter w0 plays the role of a bias. 4.3 (\u22c6 \u22c6) Extend the result of Exercise 4.2 to show that if multiple linear constraints are satis\ufb01ed simultaneously by the target vectors, then the same constraints will also be satis\ufb01ed by the least-squares prediction of a linear model. 4.4 (\u22c6) www Show that maximization of the class separation criterion given by (4.23) with respect to w, using a Lagrange multiplier to enforce the constraint wTw = 1, leads to the result that w \u221d (m2 \u2212 m1)", "12276c32-c010-435c-9d4f-223a774fe790": "This third dimension might be visualized as perpendicular to the plane of the page in Figure 8.11.\n\nIn addition to the three dimensions just discussed, we have identi\ufb01ed a number of De\ufb01nition of return Is the task episodic or continuing, discounted or undiscounted? Action values vs. state values vs. afterstate values What kind of values should be estimated? If only state values are estimated, then either a model or a separate policy (as in actor\u2013critic methods) is required for action selection. Action selection/exploration How are actions selected to ensure a suitable trade-o\u21b5 between exploration and exploitation? We have considered only the simplest ways to do this: \"-greedy, optimistic initialization of values, soft-max, and upper con\ufb01dence bound. Synchronous vs. asynchronous Are the updates for all states performed simultaneReal vs. simulated Should one update based on real experience or simulated experiLocation of updates What states or state\u2013action pairs should be updated? Modelfree methods can choose only among the states and state\u2013action pairs actually encountered, but model-based methods can choose arbitrarily. There are many possibilities here", "50a33ab6-a59e-4d2f-83c7-4671dbc11da2": "REGULARIZATION FOR DEEP LEARNING  https://www.deeplearningbook.org/contents/regularization.html    Figure 7.7: An example of forward propagation through a feedforward network using dropout. (Top)In this example, we use a feedforward network with two input units, one hidden layer with two hidden units, and one output unit.\n\n(Bottom)To perform forward propagation with dropout, we randomly sample a vector 4s with one entry for each input or hidden unit in the network. The entries of ys are binary and are sampled independently from each other. The probability of each entry being 1 is a hyperparameter, usually 05 for the hidden layers and 0.8 for the input. Each unit in the network is multiplied by the corresponding mask, and then forward propagation continues through the rest of the network as usual. This is equivalent to randomly selecting one of the sub-networks from figure 7.6 and running forward propagation through it. 258  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  p\u00ae (y | x)", "545d9a4f-7986-4b91-a5db-5e44d8ecb7b3": "Instead of applying an element-wise function g(z), maxout units divide z into groups of k values.\n\nEach maxout unit then outputs the maximum element of one of these groups:  g(Z)i = max 2;, (6.37) je) where G\u00ae is the set of indices into the inputs for group i, {G@\u2014-1k+1,...,ik}. This provides a way of learning a piecewise linear function that responds to multiple directions in the input 2 space. A maxout unit can learn a piecewise linear, convex function with up to k pieces. Maxout units can thus be seen as learning the activation function itself rather than just the relationship between units. With large enough k, a maxout unit can learn to approximate any convex function with arbitrary fidelity. In particular, a maxout layer with two pieces can learn to implement the same function of the input x as a traditional layer using the rectified linear activation function, the absolute value rectification function, or the leaky or parametric ReLU, or it can learn to implement a totally different function altogether", "1edda494-af99-4501-bdc2-c3e19c47fef3": "Fe ee ee ee ee ee SS CS SS P a A p> a es eannase | er saan ae .\n\nCn A i a i aie ~ Ce ee ee Se Dee ee De ys~N oF FF Se ee ee hee ev py vrnrns n \u00bb_s\\\u2122  Figure 14.5: Vector field learned by a denoising autoencoder around a 1-D curved manifold near which the data concentrate in a 2-D space. Each arrow is proportional to the reconstruction minus input vector of the autoencoder and points towards higher probability according to the implicitly estimated probability distribution. The vector field has zeros at both maxima of the estimated density function (on the data manifolds) and at minima of that density function. For example, the spiral arm forms a 1-D manifold of local maxima that are connected to each other. Local minima appear near the middle of the gap between two arms", "ce417a52-fe1b-48b7-8ec0-611288d0fd51": "This applies not only to classifiers but also to models we will encounter in Part III, such as autoencoders and Boltzmann machines.\n\nThese models have layers whose output should resemble the input data a, and it can be very helpful to initialize the biases of such layers to match the marginal distribution over x. Sometimes we may want to choose the bias to avoid causing too much saturation at initialization. For example, we may set the bias of a ReLU hidden unit to 0.1 rather than 0 to avoid saturating the ReLU at initialization. This approach is not compatible with weight initialization schemes that do  301  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  not expect strong input from the biases though. For example, it is not recommended for use with random walk initialization . Sometimes a unit controls whether other units are able to participate in a function. In such situations, we have a unit with output u and another unit h \u00e9 , and they are multiplied together to produce an output uh. We can view h as a gate that determines whether uh \u00a9 u or uh = 0", "a08f6bd0-8fe6-4722-9151-ed8723d69ab2": "One way to verify that these derivatives are correct is to compare the derivatives computed by your implementation _of automatic differentiation to the derivatives computed by finite differences.\n\nBecause  f'(0) = tim L249 \u2014 fe) (11.5)  \u00ab0 \u20ac  we can approximate the derivative by using a small, finite e:  flv+ = fla)  f(x) = : (11.6)  We can improve the accuracy of the approximation by using the centered differ-  ence: f(at+ 36) \u2014 f(a@- 36) \u20ac  f(x) \u00ae  The perturbation size \u20ac must be large enough to ensure that the perturbation is not rounded down too much by finite-precision numerical computations. (11.7)  433  CHAPTER 11. PRACTICAL METHODOLOGY  Usually, we will want to test the gradient or Jacobian of a vector-valued function g:R\u2122 \u2014 R\u201d. Unfortunately, finite differencing only allows us to take a single derivative at a time", "d8028471-6c78-49c1-b9d4-4e7d9d7a54ff": "Within one episode, it works as follows:  https://lilianweng.github.io/posts/2018-02-19-rl-overview/  A (Long) Peek into Reinforcement Learning | Lil'Log   Initialize t = 0. Starts with So. At time step t, we pick the action according to Q values, A; = arg MaXye4 Q(S;, a) and e- greedy is commonly applied. After applying action A;, we observe reward R,,; and get into the next state S;, 1. Update the Q-value function:  Q(St, At) \u2014 Q(S:, At) + (Rey + y\u00a5Maxacg Q(St41,4) \u2014 Q(St, Ar). t =t+ 1 and repeat from step 3.\n\nThe key difference from SARSA is that Q-learning does not follow the current policy to pick the second action A,;, 1. It estimates Q* out of the best Q values, but which action (denoted as a*) leads to this maximal Q does not matter and in the next step Q-learning may not follow a*", "a4603df8-dee0-4aea-ac9b-8b6edd4e4c12": "Self-supervised learning: The dark matter of intelligence   Research Share on  Facebook  Self-supervised  learning: The 5 ee dark matter of Twitter intelligence  March 4, 2021 Our Work  OO Meta Al Research Blog Resources About Q  In recent years, the Al field has made tremendous progress in developing Al wescares systems that can learn from massive  amounts of carefully labeled data. This  ; , . Faceboo paradigm of supervised learning has a proven track record for training Al\u2019s... specialist models that perform extremely well on the task they were The award trained to do. Unfortunately, there\u2019s a recognizes  https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/  Self-supervised learning: The dark matter of intelligence  limit to how far the field of Al can go with supervised learning alone. Supervised learning is a bottleneck for building more intelligent generalist models that can do multiple tasks and acquire new skills without massive amounts of labeled data. Practically speaking, it\u2019s impossible to label everything in the world. There are also some tasks for which there\u2019s simply not enough labeled data, such as training translation systems for low-resource languages", "bf55cf34-36be-436b-b906-650a14571b3a": "Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26, pages 3111\u20133119.\n\nCurran Associates, Inc. Andriy Mnih and Geoffrey E Hinton. 2009. A scalable hierarchical distributed language model. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 1081\u20131088. Curran Associates, Inc. Ankur P Parikh, Oscar T\u00a8ackstr\u00a8om, Dipanjan Das, and Jakob Uszkoreit. 2016. A decomposable attention model for natural language inference. In EMNLP. Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u2013 1543. Matthew Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power", "f6b4a901-3588-4da9-b937-1af1ca4a18c7": "The variance of f(x) is de\ufb01ned by and provides a measure of how much variability there is in f(x) around its mean value E. Expanding out the square, we see that the variance can also be written in terms of the expectations of f(x) and f(x)2 Exercise 1.5 In particular, we can consider the variance of the variable x itself, which is given by For two random variables x and y, the covariance is de\ufb01ned by which expresses the extent to which x and y vary together. If x and y are independent, then their covariance vanishes. Exercise 1.6 In the case of two vectors of random variables x and y, the covariance is a matrix If we consider the covariance of the components of a vector x with each other, then we use a slightly simpler notation cov \u2261 cov. So far in this chapter, we have viewed probabilities in terms of the frequencies of random, repeatable events. We shall refer to this as the classical or frequentist interpretation of probability.\n\nNow we turn to the more general Bayesian view, in which probabilities provide a quanti\ufb01cation of uncertainty", "b9b0da73-7386-4fe3-9b62-10892f0dc0df": "Adaptation in Natural and Arti\ufb01cial Systems. University of Michigan Holland, J. H. Adaptation. In R. Rosen and F. M. Snell (Eds. ), Progress in Theoretical Biology, vol. 4, pp. 263\u2013293. Academic Press, New York. Holland, J. H. Escaping brittleness: The possibility of general-purpose learning algorithms prediction of reward during learning. Nature Neuroscience, 1(4):304\u2013309. Houk, J. C., Adams, J. L., Barto, A. G. A model of how the basal ganglia generates and uses neural signals that predict reinforcement. In J. C. Houk, J. L. Davis, and D. G. Beiser (Eds. ), Models of Information Processing in the Basal Ganglia, pp. 249\u2013270. MIT Press, Cambridge, MA", "21f40f87-5e56-49a0-b156-f5cbcd0ae217": "Mastering the game of go without human knowledge. Nature 550.7676 : 354. David Silver, et al. Mastering the game of Go with deep neural networks and tree search. Nature 529.7587 : 484-489.  https://lilianweng.github.io/posts/2018-02-19-rl-overview/  A (Long) Peek into Reinforcement Learning | Lil'Log    Volodymyr Mnih, et al. Human-level control through deep reinforcement learning. Nature 518.7540 : 529. Ziyu Wang, et al. Dueling network architectures for deep reinforcement learning. ICML. 2016. Reinforcement Learning lectures by David Silver on YouTube. OpenAl Blog: Evolution Strategies as a Scalable Alternative to Reinforcement Learning   Frank Sehnke, et al. Parameter-exploring policy gradients. Neural Networks 23.4 : 551- 559. Csaba Szepesvari. Algorithms for reinforcement learning. 1st Edition", "cf044844-3566-461f-ba96-8d633c3d9957": "At least partly due to this advantage, OpenAI GPT  achieved previously state-of-the-art results on many sentencelevel tasks from the GLUE benchmark .\n\nLeft-to-right language modelUnlabeled Sentence A and B Pair  ing and auto-encoder objectives have been used for pre-training such models . There has also been work showing effective transfer from supervised tasks with large datasets, such as natural language inference  and machine translation . Computer vision research has also demonstrated the importance of transfer learning from large pre-trained models, where an effective recipe is to \ufb01ne-tune models pre-trained with ImageNet . We introduce BERT and its detailed implementation in this section. There are two steps in our framework: pre-training and \ufb01ne-tuning. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For \ufb01netuning, the BERT model is \ufb01rst initialized with the pre-trained parameters, and all of the parameters are \ufb01ne-tuned using labeled data from the downstream tasks. Each downstream task has separate \ufb01ne-tuned models, even though they are initialized with the same pre-trained parameters", "ff9607a1-3fbb-49eb-a40f-54e2fbadecee": "Monte Carlo ES (Exploring Starts), for estimating \u21e1 \u21e1 \u21e1\u21e4 Choose S0 2 S, A0 2 A(S0) randomly such that all pairs have probability > 0 Generate an episode from S0, A0, following \u21e1: S0, A0, R1, . , ST \u22121, AT \u22121, RT G  0 Loop for each step of episode, t = T \u22121, T \u22122, . , 0: Exercise 5.4 The pseudocode for Monte Carlo ES is ine\ufb03cient because, for each state\u2013 action pair, it maintains a list of all returns and repeatedly calculates their mean. It would be more e\ufb03cient to use techniques similar to those explained in Section 2.4 to maintain just the mean and a count (for each state\u2013action pair) and update them incrementally. Describe how the pseudocode would be altered to achieve this. \u21e4 In Monte Carlo ES, all the returns for each state\u2013action pair are accumulated and averaged, irrespective of what policy was in force when they were observed. It is easy to see that Monte Carlo ES cannot converge to any suboptimal policy", "36c0a156-6492-4f32-b187-cbdc538225e9": "Two dimensions may be  plotted directly on the page for visualization, so we can gain an understanding of how the model works by training a model with a 2-D latent code, even if we believe the intrinsic dimensionality of the data manifold is much higher.\n\nThe images shown are not examples rom the training set but images x actually generated by the model p(z | z), simply by changing the 2-D \u201ccode\u201d z (each image corresponds to a different choice of \u201ccode\u201d z on a 2-D uniform grid). (Left) The 2-D map of the Frey faces manifold. One dimension that has been discovered (horizontal) mostly corresponds to a rotation of the face, while the other (vertical) corresponds to the emotional expression. (Right) The 2-D map of the MNIST manifold. 20.10.4 Generative Adversarial Networks  Generative adversarial networks, or GANs , are another generative modeling approach based on differentiable generator networks. Generative adversarial networks are based on a game theoretic scenario in which the generator network must compete against an adversary", "4579fe44-9eec-4083-8289-79cdddda0c4a": "In some cases, \u00a2(a) can even be infinite dimensional, which would result in an infinite computational cost for the naive, explicit approach. In many cases, k(x, a\u2019) is a nonlinear, tractable function of a even when \u00a2(a) is intractable. As an example of an infinite-dimensional feature space with a tractable kernel, we construct a feature mapping \u00a2() over the nonnegative integers x. Suppose that this mapping returns a vector containing x ones followed by infinitely many zeros. We can write a kernel function k(x, 2) = min(x,\u00ab\u2122) that is exactly equivalent to the corresponding infinite-dimensional dot product. The most commonly used kernel is the Gaussian kernel, k(u,v) =N(u\u2014 v;0,07D), (5.84)  where NV (a; 4, =) is the standard normal density. This kernel is also known as the radial basis function (RBF) kernel, because its value decreases along lines in v space radiating outward from u", "e2c9f69a-1d9f-428e-b5a5-16de021813f2": "The novelty in this chapter is that the approximate value function is represented not as a table but as a parameterized functional form with weight vector w 2 Rd. We will write \u02c6v(s,w) \u21e1 v\u21e1(s) for the approximate value of state s given weight vector w. For example, \u02c6v might be a linear function in features of the state, with w the vector of feature weights. More generally, \u02c6v might be the function computed by a multi-layer arti\ufb01cial neural network, with w the vector of connection weights in all the layers. By adjusting the weights, any of a wide range of di\u21b5erent functions can be implemented by the network. Or \u02c6v might be the function computed by a decision tree, where w is all the numbers de\ufb01ning the split points and leaf values of the tree. Typically, the number of weights (the dimensionality of w) is much less than the number of states (d \u2327 |S|), and changing one weight changes the estimated value of many states.\n\nConsequently, when a single state is updated, the change generalizes from that state to a\u21b5ect the values of many other states", "f52d83c4-d4dd-4d6d-9b8a-3d7a8fbd3cfc": "for examples of such an  application. \u2018Respectively available from these web sites: freebase.com, cyc.com/opencyc, wordnet. princeton.edu, wikiba.se >geneontology .org  479  CHAPTER 12. APPLICATIONS  Evaluating the performance of a model on a link prediction task is difficult because we have only a dataset of positive examples (facts that are known to be true). If the model proposes a fact that is not in the dataset, we are unsure whether the model has made a mistake or discovered a new, previously unknown fact.\n\nThe metrics are thus somewhat imprecise and are based on testing how the model ranks a held-out set of known true positive facts compared to other facts hat are less likely to be true. A common way to construct interesting examples hat are probably negative (facts that are probably false) is to begin with a true act and create corrupted versions of that fact, for example, by replacing one entity in the relation with a different entity selected at random", "476fdfe7-a08e-452f-8f72-c01324e880ef": "J. The role of the basal ganglia in habit formation. Nature Young, P. Recursive Estimation and Time-Series Analysis. Springer-Verlag, Berlin. Yu, H. Convergence of least squares temporal di\u21b5erence methods under general conditions. International Conference on Machine Learning 27, pp.\n\n1207\u20131214. Yu, H. Least squares temporal di\u21b5erence methods: An analysis under general conditions. SIAM Journal on Control and Optimization, 50(6):3310\u20133343. Yu, H. On convergence of emphatic temporal-di\u21b5erence learning. In Proceedings of the 28th Annual Conference on Learning Theory, JMLR W&CP 40. Also ArXiv:1506.02582. Yu, H. Weak convergence properties of constrained emphatic temporal-di\u21b5erence learning Yu, H. On convergence of some gradient-based temporal-di\u21b5erences algorithms for Yu, H., Mahmood, A. R., Sutton, R. S", "5722c097-1f30-4039-8478-05d213643c23": "Let us now consider the maximum entropy con\ufb01guration for a continuous variable.\n\nIn order for this maximum to be well de\ufb01ned, it will be necessary to constrain the \ufb01rst and second moments of p(x) as well as preserving the normalization constraint. We therefore maximize the differential entropy with the dynamics, which states that the entropy of a closed system tends to increase with time. By contrast, at the microscopic level the classical Newtonian equations of physics are reversible, and so they found it dif\ufb01cult to see how the latter could explain the former. They didn\u2019t fully appreciate Boltzmann\u2019s arguments, which were statistical in nature and which concluded not that entropy could never decrease over time but simply that with overwhelming probability it would generally increase. Boltzmann even had a longrunning dispute with the editor of the leading German physics journal who refused to let him refer to atoms and molecules as anything other than convenient theoretical constructs. The continued attacks on his work lead to bouts of depression, and eventually he committed suicide. Shortly after Boltzmann\u2019s death, new experiments by Perrin on colloidal suspensions veri\ufb01ed his theories and con\ufb01rmed the value of the Boltzmann constant", "32cda916-93fc-44fa-86f8-c0b74c453f8c": "The counterfeiter versus robber analogy is a seamless bridge to understand GANs in the context of network intrusion detection. Lin et al. use a generator network to learn how to fool a black-box detection system. This highlights one of the most interesting characteristics of GANs. Analysis tools derived from game theory such as minimax strategy and the Nash Equilibrium  suggest that the generator will even- tually fool the discriminator. The success of the generator to overcome the discrimi- nator makes it very powerful for generative modeling. GANs are the most promising  generative modeling technique for use in Data Augmentation. Shorten and Khoshgoftaar J Big Data  6:60   The vanilla GAN architecture uses multilayer perceptron networks in the gen- erator and discriminator networks. This is able to produce acceptable images on a simple image dataset such as the MNIST handwritten digits. However, it fails to produce quality results for higher resolution, more complicated datasets. In the MNIST dataset, each image is only 28 x 28 x 1 for a total of 784 pixels", "fa992b52-57c3-4db8-a0bb-a96d79857992": "Intrinsic reward. Rewards provided by the extrinsic environment can be sparse in many real-world sequential decision problems. Learning in such problems is thus di\ufb03cult due to the lack of supervision signals.\n\nA method to alleviate the di\ufb03culty is to supplement the extrinsic reward It is straightforward to derive the intrinsically motivated variant of the policy gradient algorithm (and other RL algorithms discussed below), by replacing the standard extrinsic-only Q\u03b8(x, y) in the experience function Equation 4.16 with the combined Q\u03b8(x, y)+Qin,\u03b8(x, y). Let f \u03b8 reward,ex+in(x, y) denote the resulting experience function that incorporates both the extrinsic and the additive intrinsic rewards. We can notice some sort of symmetry between f \u03b8 reward,ex+in(x, y) and the actively supervised data experience factive in Equation 4.10, which augments the standard supervised data experience with the additive informativeness measure u(x). The resemblance could naturally inspire mutual exchange between the research areas of intrinsic reward and active learning, for example, using the active learning informativeness measure as the intrinsic reward rin, as was studied in earlier work", "476fa4e5-296e-4411-906f-251e6efcddc8": "If any of the diagonal elements are negative, then the corresponding diagonal element of I \u2212 \u21b5A will be greater than one, and the corresponding component of wt will be ampli\ufb01ed, which will lead to divergence if continued. On the other hand, if the diagonal elements of A are all positive, then \u21b5 can be chosen smaller than one over the largest of them, such that I \u2212 \u21b5A is diagonal with all diagonal elements between 0 and 1. In this case the \ufb01rst term of the update tends to shrink wt, and stability is assured. In general, wt will be reduced toward zero whenever A is positive de\ufb01nite, meaning y>Ay > 0 for any real vector y 6= 0", "5a4b2cbd-8f56-4859-a668-737687320070": "The top plots compare a majority vote of all labeling functions, Snorkel\u2019s generative model, and Snorkel\u2019s discriminative model.\n\nThey show that the generative model improves over majority vote by providing more granular information about candidates, and that the discriminative model can generalize to candidates that no labeling functions label. The bottom plots compare the discriminative model trained on an unweighted combination of the labeling functions, hand supervision (when available), and Snorkel\u2019s discriminative model. They show that the discriminative model bene\ufb01ts from the weighted labels provided by the generative model, and that Snorkel is competitive with hand supervision, particularly in the high-precision region sets were labeled by individuals not involved with labeling function development to keep the test sets properly blinded. We \ufb01rst focus on four relation extraction tasks on text data, as it is a challenging and common class of problems that are well studied and for which distant supervision is often considered. Predictive performance is summarized in Table 3, and precision\u2013recall curves are shown in Fig. 10. We brie\ufb02y describe each task", "3801ca88-3e6b-4ffa-a75a-d4d5a3d2cc73": "The only places where this cosine was negative was in these situations of instability. We therefore switched to RMSProp  which is known to perform well even on very nonstationary problems . One of the bene\ufb01ts of WGAN is that it allows us to train the critic till optimality. When the critic is trained to completion, it simply provides a loss to the generator that we can train as any other neural network. This tells us that we no longer need to balance generator and discriminator\u2019s capacity properly. The better the critic, the higher quality the gradients we use to train the generator. We observe that WGANs are much more robust than GANs when one varies the architectural choices for the generator. We illustrate this by running experiments on three generator architectures: (1) a convolutional DCGAN generator, (2) a convolutional DCGAN generator without batch normalization and with a constant number of \ufb01lters, and (3) a 4-layer ReLU-MLP with 512 hidden units.\n\nThe last two are known to perform very poorly with GANs. We keep the convolutional DCGAN architecture for the WGAN critic or the GAN discriminator", "f09ab72a-eb46-4a7b-9444-bb535e1e1946": "Let us consider for a moment the problem of approximating a general distribution by a factorized distribution. To begin with, we discuss the problem of approximating a Gaussian distribution using a factorized Gaussian, which will provide useful insight into the types of inaccuracy introduced in using factorized approximations. Consider a Gaussian distribution p(z) = N(z|\u00b5, \u039b\u22121) over two correlated variables z = (z1, z2) in which the mean and precision have elements and \u039b21 = \u039b12 due to the symmetry of the precision matrix. Now suppose we wish to approximate this distribution using a factorized Gaussian of the form q(z) = q1(z1)q2(z2). We \ufb01rst apply the general result (10.9) to \ufb01nd an expression for the optimal factor q\u22c6 1(z1)", "33dbf7ed-1bc9-4feb-ba5c-39fd7bd1993a": "8.24 (\u22c6 \u22c6) Show that the marginal distribution for the variables xs in a factor fs(xs) in a tree-structured factor graph, after running the sum-product message passing algorithm, can be written as the product of the message arriving at the factor node along all its links, times the local factor f(xs), in the form (8.72). 8.25 (\u22c6 \u22c6) In (8.86), we veri\ufb01ed that the sum-product algorithm run on the graph in Figure 8.51 with node x3 designated as the root node gives the correct marginal for x2. Show that the correct marginals are obtained also for x1 and x3. Similarly, show that the use of the result (8.72) after running the sum-product algorithm on this graph gives the correct joint distribution for x1, x2.\n\n8.26 (\u22c6) Consider a tree-structured factor graph over discrete variables, and suppose we wish to evaluate the joint distribution p(xa, xb) associated with two variables xa and xb that do not belong to a common factor. De\ufb01ne a procedure for using the sumproduct algorithm to evaluate this joint distribution in which one of the variables is successively clamped to each of its allowed values", "c7b90a43-ab22-49b1-9369-14f5e767ab60": "If the tree is sufficiently balanced, the maximum depth (number of binary decisions) is on the order of the logarithm of the number of words |V|: the choice of one out of |V| words can be obtained by doing O(log |V|) operations (one for each of the nodes on the path from the root).\n\nIn this example, computing the probability of a wordy can be done by multiplying three probabilities, associated with the binary decisions to move left or right at each node on the path from the root to a nodey. Let b;(y) be the +th binary decision when traversing the tree toward the value y. The probability of sampling an output y decomposes into a product of conditional probabilities, using the chain rule for conditional probabilities, with each node indexed by the prefix of these bits", "c791bfe8-f6b0-43c4-ba1f-b56febf57b76": "Now we advance the horizon to step 3 and repeat, going all the way back to produce three new targets, redoing all updates starting from the original w0 to produce w3, and so on.\n\nEach time the horizon is advanced, all the updates are redone starting from w0 using the weight vector from the preceding horizon. This conceptual algorithm involves multiple passes over the episode, one at each horizon, each generating a di\u21b5erent sequence of weight vectors. To describe it clearly we have to distinguish between the weight vectors computed at the di\u21b5erent horizons. Let us use wh t to denote the weights used to generate the value at time t in the sequence up to 0 in each sequence is that inherited from the previous episode (so they are the same for all h), and the last weight vector wh de\ufb01nes the ultimate weight-vector sequence of the algorithm. At the \ufb01nal horizon h = T we obtain the \ufb01nal weights wT T which will be passed on to form the initial weights of the next episode", "73973eb0-ac99-480c-afa7-de8a23db4b10": "Hence prove the result (2.124).\n\n2.36 (\u22c6 \u22c6) www Using an analogous procedure to that used to obtain (2.126), derive an expression for the sequential estimation of the variance of a univariate Gaussian distribution, by starting with the maximum likelihood expression Verify that substituting the expression for a Gaussian distribution into the RobbinsMonro sequential estimation formula (2.135) gives a result of the same form, and hence obtain an expression for the corresponding coef\ufb01cients aN. 2.37 (\u22c6 \u22c6) Using an analogous procedure to that used to obtain (2.126), derive an expression for the sequential estimation of the covariance of a multivariate Gaussian distribution, by starting with the maximum likelihood expression (2.122). Verify that substituting the expression for a Gaussian distribution into the Robbins-Monro sequential estimation formula (2.135) gives a result of the same form, and hence obtain an expression for the corresponding coef\ufb01cients aN. 2.38 (\u22c6) Use the technique of completing the square for the quadratic form in the exponent to derive the results (2.141) and (2.142)", "2c814a92-1240-44f0-9644-218b538454fe": "For the quadratic model, the training error increases as the size of the training set increases. This is because larger datasets are harder to fit. Simultaneously, the test error decreases, because fewer incorrect hypotheses are consistent with the training data. The quadratic model does not have enough capacity to solve the task, so its test error asymptotes to a high value. The test error at optimal capacity asymptotes to the Bayes error.\n\nThe training error can fall below the Bayes error, due to the ability of the training algorithm to memorize specific instances of the training set. As the training size increases to infinity, the training error of any fixed-capacity model (here, the quadratic model) must rise to at least the Bayes error. (Bottom)As the training set size increases, the optimal capacity (shown here as the degree of the optimal polynomial regressor) increases. The optimal capacity plateaus after reaching sufficient complexity to solve the task. 115  CHAPTER 5. MACHINE LEARNING BASICS  same error rate when classifying previously unobserved points", "b29eb380-acae-4cf3-91db-bf206a69cea3": "Furthermore suppose, as is often the case, that we are easily able to evaluate p(z) for any given value of z, up to some normalizing constant Z, so that where \ufffdp(z) can readily be evaluated, but Zp is unknown. In order to apply rejection sampling, we need some simpler distribution q(z), sometimes called a proposal distribution, from which we can readily draw samples. We next introduce a constant k whose value is chosen such that kq(z) \u2a7e \ufffdp(z) for all values of z. The function kq(z) is called the comparison function and is illustrated for a univariate distribution in Figure 11.4. Each step of the rejection sampler involves generating two random numbers. First, we generate a number z0 from the distribution q(z). Next, we generate a number u0 from the uniform distribution over . This pair of random numbers has uniform distribution under the curve of the function kq(z). Finally, if u0 > \ufffdp(z0) then the sample is rejected, otherwise u0 is retained", "951ee0e0-c668-45ad-8dee-047d5cc29040": "This is easily done by using the initial condition (8.71) and noting that \u03b1(z1) is given by h(z1) = p(z1)p(x1|z1) which is identical to (13.37).\n\nBecause the initial \u03b1 is the same, and because they are iteratively computed using the same equation, all subsequent \u03b1 quantities must be the same. Next we consider the messages that are propagated from the root node back to where, as before, we have eliminated the messages of the type z \u2192 f since the variable nodes perform no computation. Using the de\ufb01nition (13.46) to substitute for fn+1(zn, zn+1), and de\ufb01ning we obtain the beta recursion given by (13.38). Again, we can verify that the beta variables themselves are equivalent by noting that (8.70) implies that the initial message send by the root variable node is \u00b5zN\u2192fN (zN) = 1, which is identical to the initialization of \u03b2(zN) given in Section 13.2.2. The sum-product algorithm also speci\ufb01es how to evaluate the marginals once all the messages have been evaluated", "bc52c5fe-803a-4291-96e2-a9208f119f0f": "The weight is generated by a kernel function kg, measuring the similarity between two data samples. https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Log   Po(y|x, S) = > ko(X, X;) Yi  (xi,yi)ES  To learn a good kernel is crucial to the success of a metric-based meta-learning model. Metric learning is well aligned with this intention, as it aims to learn a metric or distance function over objects. The notion of a good metric is problem-dependent. It should represent the relationship between inputs in the task space and facilitate problem solving. All the models introduced below learn embedding vectors of input data explicitly and use them to design proper kernel functions.\n\nConvolutional Siamese Neural Network  The Siamese Neural Network is composed of two twin networks and their outputs are jointly trained on top with a function to learn the relationship between pairs of input data samples. The twin networks are identical, sharing the same weights and network parameters", "c74ca66a-19f4-4919-bebf-bdcce84ff77f": "It\u2019s easy to see that since U was open, U\u03b8 is as well. Furthermore, by assumption 1, we can de\ufb01ne L(\u03b8) = Ez and achieve |W(Pr, P\u03b8) \u2212 W(Pr, P\u03b8\u2032)| \u2264 W(P\u03b8, P\u03b8\u2032) \u2264 L(\u03b8)\u2225\u03b8 \u2212 \u03b8\u2032\u2225 for all \u03b8\u2032 \u2208 U\u03b8, meaning that W(Pr, P\u03b8) is locally Lipschitz.\n\nThis obviously implies that W(Pr, P\u03b8) is everywhere continuous, and by Radamacher\u2019s theorem we know it has to be di\ufb00erentiable almost everywhere. The counterexample for item 3 of the Theorem is indeed Example 1. Proof of Corollary 1. We begin with the case of smooth nonlinearities. Since g is C1 as a function of (\u03b8, z) then for any \ufb01xed (\u03b8, z) we have L(\u03b8, Z) \u2264 \u2225\u2207\u03b8,xg\u03b8(z)\u2225+\u03f5 is an acceptable local Lipschitz constant for all \u03f5 > 0. Therefore, it su\ufb03ces to prove 2P (note that Pm depends on n)", "2768f6c5-db24-4551-87a3-bfa2c88fe991": "This is a valid kernel function because it can be shown to correspond to an inner product in a feature space.\n\nExercise 6.12 One powerful approach to the construction of kernels starts from a probabilistic generative model , which allows us to apply generative models in a discriminative setting. Generative models can deal naturally with missing data and in the case of hidden Markov models can handle sequences of varying length. By contrast, discriminative models generally give better performance on discriminative tasks than generative models. It is therefore of some interest to combine these two approaches . One way to combine them is to use a generative model to de\ufb01ne a kernel, and then use this kernel in a discriminative approach. Given a generative model p(x) we can de\ufb01ne a kernel by This is clearly a valid kernel function because we can interpret it as an inner product in the one-dimensional feature space de\ufb01ned by the mapping p(x). It says that two inputs x and x\u2032 are similar if they both have high probabilities", "9086dd11-0d5e-40f1-82e6-26831dda7097": "Special issue on applications of neural networks. Bishop, C. M. Neural Networks for Pattern Recognition. Oxford University Press. Bishop, C. M. Training with noise is equivalent to Tikhonov regularization. Neural Computation 7(1), 108\u2013116. Bishop, C. M. Bayesian PCA. In M. S. Kearns, S. A. Solla, and D. A. Cohn (Eds. ), Advances in Neural Information Processing Systems, Volume 11, pp. 382\u2013388. MIT Press. Bishop, C. M. and G. D. James . Analysis of multiphase \ufb02ows using dual-energy gamma densitometry and neural networks. Nuclear Instruments and Methods in Physics Research A327, 580\u2013593. Bishop, C. M. and I. T. Nabney", "5a7598c9-6c9f-4bed-9e57-9899360311e9": "a  https://www.deeplearningbook.org/contents/representation.html    V\\Y | X) Wil be strongly tied, and unsupervised representation learning that tries to disentangle the underlying factors of variation is likely to be useful as a semi-supervised learning strategy. Consider the assumption that y is one of the causal factors of x, and let h represent all those factors. The true generative process can be conceived as  structured according to this directed graphical model, with h as the parent of x:  p(h, x) = p(x | h)p(hh). (15.1) As a consequence, the data has marginal probability p(x) = Enp(# | h).\n\n(15.2)  From this straightforward observation, we conclude that the best possible model of x (from a generalization point of view) is the one that uncovers the above \u201ctrue\u201d  540  CHAPTER 15. REPRESENTATION LEARNING  structure, with h as a latent variable that explains the observed variations in 2. The \u201cideal\u201d representation learning discussed above should thus recover these latent factors", "6f081070-435d-47d5-864b-6d8118b7abdc": "9.6 Automatic methods for adapting the step-size parameter include RMSprop , Adam , stochastic meta-descent methods such as Delta-Bar-Delta , its incremental generalization , and nonlinear generalizations . Methods explicitly designed for reinforcement learning include AlphaBound , SID and NOSID , TIDBD (Kearney et al., in preparation) and the application of stochastic meta-descent to policy gradient learning . 9.6 The introduction of the threshold logic unit as an abstract model neuron by McCulloch and Pitts  was the beginning of ANNs. The history of ANNs as learning methods for classi\ufb01cation or regression has passed through several stages: roughly, the Perceptron  and ADALINE (ADAptive LINear Element)  stage of learning by single-layer ANNs, the error-backpropagation stage  of learning by multi-layer ANNs, and the current deep-learning stage with its emphasis on representation learning . Examples of the many books on ANNs are Haykin , Bishop , and Ripley .\n\nANNs as function approximation for reinforcement learning goes back to the early work of Farley and Clark , who used reinforcement-like learning to modify the weights of linear threshold functions representing policies", "994e885c-e7da-4570-b296-86a3008f5cf1": "Hastings, W. K. .\n\nMonte Carlo sampling methods using Markov chains and their applications. Biometrika 57, 97\u2013109. Hathaway, R. J. Another interpretation of the EM algorithm for mixture distributions. Statistics and Probability Letters 4, 53\u201356. Haussler, D. Convolution kernels on discrete structures. Technical Report UCSC-CRL-99-10, University of California, Santa Cruz, Computer Science Department. Henrion, M. Propagation of uncertainty by logic sampling in Bayes\u2019 networks. In J. F. Lemmer and L. N. Kanal (Eds. ), Uncertainty in Arti\ufb01cial Intelligence, Volume 2, pp. 149\u2013164. North Holland. Hinton, G. E., P. Dayan, and M. Revow . Modelling the manifolds of images of handwritten digits. IEEE Transactions on Neural Networks 8(1), 65\u201374", "2ffa08aa-3a80-4217-8b90-53a835fca015": "For example, it learned to play certain opening positions di\u21b5erently than was the convention among the best human players. Based on TD-Gammon\u2019s success and further analysis, the best human players now play these positions as TD-Gammon does . The impact on human play was greatly accelerated when several other self-teaching ANN backgammon programs inspired by TD-Gammon, such as Jelly\ufb01sh, Snowie, and GNUBackgammon, became widely available. These programs enabled wide dissemination of new knowledge generated by the ANNs, resulting in great improvements in the overall caliber of human tournament play . An important precursor to Tesauro\u2019s TD-Gammon was the seminal work of Arthur Samuel  in constructing programs for learning to play checkers. Samuel was one of the \ufb01rst to make e\u21b5ective use of heuristic search methods and of what we would now call temporal-di\u21b5erence learning. His checkers players are instructive case studies in addition to being of historical interest.\n\nWe emphasize the relationship of Samuel\u2019s methods to modern reinforcement learning methods and try to convey some of Samuel\u2019s motivation for using them. Samuel \ufb01rst wrote a checkers-playing program for the IBM 701 in 1952", "dd7d3d67-74b6-4761-9250-d599896246f9": "This can be understood as a memory containing a sequence of facts, which can be retrieved later, not necessarily in the same order, without having to visit all of them. 3. A process that exploits the content of the memory to sequentially perform a task, at each time step having the ability put attention on the content of one memory element (or a few, with a different weight). The third component generates the translated sentence. When words in a sentence written in one language are aligned with correspond- ing words in a translated sentence in another language, it becomes possible to relate the corresponding word embeddings. Earlier work showed that one could learn a kind of translation matrix relating the word embeddings in one language with the word embeddings in another , yielding lower alignment error  470  CHAPTER 12. APPLICATIONS  https://www.deeplearningbook.org/contents/applications.html    \u2018( ) \u2018( ) \u2018C )  Figure 12.6: A modern attention mechanism, as introduced by Bahdanau et al. , is essentially a weighted average", "6ed1c04b-3711-46a6-b489-b9042beee853": "The  prior distribution over the underlying factors, p(h), must be fixed ahead of time by the user. The model then deterministically generates 2 = Wh. We can perform a  \"See section 3.8 for a discussion of the difference between uncorrelated variables and indepen- dent variables. 487  CHAPTER 13. LINEAR FACTOR MODELS  nonlinear change of variables (using equation 3.47) to determine p(a).\n\nLearning the model then proceeds as usual, using maximum likelihood. The motivation for this approach is that by choosing p(h) to be independent, we can recover underlying factors that are as close as possible to independent. This is commonly used, not to capture high-level abstract causal factors, but to recover low-level signals that have been mixed together. In this setting, each training example is one moment in time, each 2; is one sensor\u2019s observation of the mixed signals, and each h; is one estimate of one of the original signals. For example, we might have n people speaking simultaneously", "cc0ba47d-c934-4b1c-8cf6-9b5cb2fcae3b": "Reptile vs FOMAML  To demonstrate the deeper connection between Reptile and MAML, let's expand the update formula with an example performing two gradient steps, k=2 in SGD(. ). Same as defined above, \u00a3 and LM are losses using different mini-batches of data. For ease of reading, we adopt two simplified annotations: g\\\u201d = V_\u00a3L\u201c(0;) and HY = V2L(6,). A \u2014 Omneta 6, = 0) \u2014 aVeL (8) = Oy \u2014 ag\u201d  9) = 0; \u2014aVeL (0:) = % \u2014 ag \u2014 ag\u201d  According to the early section, the gradient of FOMAML is the last inner gradient update result", "d8818837-9d94-453a-b774-b73c7ec5c7e3": "In particular, the heuristic estimate of Q ignores interactions between hidden units within the same layer as well as the top-down feedback influence of hidden units in deeper layers on hidden units that are closer to the input. Because the heuristic MLP-based inference procedure in the DBN is not able to account for these interactions, the resulting Q is presumably  662  https://www.deeplearningbook.org/contents/generative_models.html    CHAPTER 20 DEEP GENERATIVE MODELS  far from optimal.\n\nIn DBMs, all the hidden units within a layer are conditionally independent given the other layers. This lack of intralayer interaction makes it possible to use fixed-point equations to optimize the variational lower bound and find the true optimal mean field expectations (to within some numerical tolerance). The use of proper mean field allows the approximate inference procedure for DBMs to capture the influence of top-down feedback interactions. This makes DBMs interesting from the point of view of neuroscience, because the human brain is known to use many top-down feedback connections. Because of this property, DBMs have been used as computational models of real neuroscientific phenomena . One unfortunate property of DBMs is that sampling from them is relatively difficult", "077cbbec-bb19-409e-990c-a783f6300496": "In practice we often construct the transition probabilities from a set of \u2018base\u2019 transitions B1, . , BK. This can be achieved through a mixture distribution of the form for some set of mixing coef\ufb01cients \u03b11, . , \u03b1K satisfying \u03b1k \u2a7e 0 and \ufffd If a distribution is invariant with respect to each of the base transitions, then obviously it will also be invariant with respect to either of the T(z\u2032, z) given by (11.42) or (11.43).\n\nFor the case of the mixture (11.42), if each of the base transitions satis\ufb01es detailed balance, then the mixture transition T will also satisfy detailed balance. This does not hold for the transition probability constructed using (11.43), although by symmetrizing the order of application of the base transitions, in the form B1, B2, . , BK, BK, . , B2, B1, detailed balance can be restored. A common example of the use of composite transition probabilities is where each base transition changes only a subset of the variables. Earlier we introduced the basic Metropolis algorithm, without actually demonstrating that it samples from the required distribution", "6d03bb5c-4772-417b-9c48-3abde46de45e": "Exact inference in this model is intractable, but variational methods lead to an ef\ufb01cient inference scheme involving forward-backward recursions along each of the continuous and discrete Markov chains independently.\n\nNote that, if we consider multiple chains of discrete latent variables, and use one as the switch to select from the remainder, we obtain an analogous model having only discrete latent variables known as the switching hidden Markov model. For dynamical systems which do not have a linear-Gaussian, for example, if they use a non-Gaussian emission density, we can turn to sampling methods in order Chapter 11 to \ufb01nd a tractable inference algorithm. In particular, we can apply the samplingimportance-resampling formalism of Section 11.1.5 to obtain a sequential Monte Carlo algorithm known as the particle \ufb01lter. Consider the class of distributions represented by the graphical model in Figure 13.5, and suppose we are given the observed values Xn = (x1, . , xn) and we wish to draw L samples from the posterior distribution p(zn|Xn)", "e2ac7e97-1c94-4ac7-9c4d-d7e43a6b4dae": "We can take this a stage further to provide a deeper test of the correctness of both the mathematical derivation of the update equations and of their software implementation by using \ufb01nite differences to check that each update does indeed give a (constrained) maximum of the bound .\n\nFor the variational mixture of Gaussians, the lower bound (10.3) is given by where, to keep the notation uncluttered, we have omitted the \u22c6 superscript on the q distributions, along with the subscripts on the expectation operators because each expectation is taken with respect to all of the random variables in its argument. The various terms in the bound are easily evaluated to give the following results Exercise 10.16 where D is the dimensionality of x, H is the entropy of the Wishart distribution given by (B.82), and the coef\ufb01cients C(\u03b1) and B(W, \u03bd) are de\ufb01ned by (B.23) and (B.79), respectively. Note that the terms involving expectations of the logs of the q distributions simply represent the negative entropies of those distributions. Some simpli\ufb01cations and combination of terms can be performed when these expressions are summed to give the lower bound. However, we have kept the expressions separate for ease of understanding", "d100042f-8bad-4a73-b5c9-9ad42b65c476": "where Zp = \ufffd \ufffdp(z) dz. The marginal distribution over z is given by and so we can sample from p(z) by sampling from \ufffdp(z, u) and then ignoring the u values. This can be achieved by alternately sampling z and u. Given the value of z we evaluate \ufffdp(z) and then sample u uniformly in the range 0 \u2a7d u \u2a7d \ufffdp(z), which is straightforward. Then we \ufb01x u and sample z uniformly from the \u2018slice\u2019 through the distribution de\ufb01ned by {z : \ufffdp(z) > u}. This is illustrated in Figure 11.13(a). In practice, it can be dif\ufb01cult to sample directly from a slice through the distribution and so instead we de\ufb01ne a sampling scheme that leaves the uniform distribution under \ufffdp(z, u) invariant, which can be achieved by ensuring that detailed balance is satis\ufb01ed. Suppose the current value of z is denoted z(\u03c4) and that we have obtained a corresponding sample u", "f6f5bfae-0451-48ca-bdf2-902e1947992d": "We have seen how the problem of polynomial curve \ufb01tting can be expressed in terms of error minimization. Here we return to the curve \ufb01tting example and view it Section 1.1 from a probabilistic perspective, thereby gaining some insights into error functions and regularization, as well as taking us towards a full Bayesian treatment. The goal in the curve \ufb01tting problem is to be able to make predictions for the target variable t given some new value of the input variable x on the basis of a set of training data comprising N input values x = (x1, . , xN)T and their corresponding target values t = (t1, . , tN)T. We can express our uncertainty over the value of the target variable using a probability distribution. For this purpose, we shall assume that, given the value of x, the corresponding value of t has a Gaussian distribution with a mean equal to the value y(x, w) of the polynomial curve given by (1.1). Thus we have where, for consistency with the notation in later chapters, we have de\ufb01ned a precision parameter \u03b2 corresponding to the inverse variance of the distribution.\n\nThis is illustrated schematically in Figure 1.16", "3856218e-00dc-4df8-bb0a-abc73e9f4e1f": "Finally, we consider a large number of labeling functions that are likely to be correlated. In our user study (described 12 Speci\ufb01cally, \u03f5 is both the coef\ufb01cient of the \u21131 regularization term used to induce sparsity, and the minimum absolute weight in log scale that a dependency must have to be selected. Fig. 9 Predictive performance of the generative model and number of learned correlations versus the correlation threshold \u03f5. The selected elbow point achieves a good trade-off between predictive performance and computational cost (linear in the number of correlations). Left: simulation of structure learning correcting the generative model. Middle: the CDR task.\n\nRight: all user study labeling functions for the Spouses task in Sect. 4.2), participants wrote labeling functions for the Spouses task. We combined all 125 of their functions and studied the effect of varying \u03f5. Here, we expect there to be many correlations since it is likely that users wrote redundant functions. We see in Fig. 9, right, that structure learning surpasses the best performing individual\u2019s generative model (50.0 F1). Computational Cost Computational cost is correlated with model complexity", "99e7aac6-1cdd-4b0d-8664-967af1306c2b": ".\n\n, tN}, the likelihood function is given by and so the resulting posterior distribution is then which, as a consequence of the nonlinear dependence of y(x, w) on w, will be nonGaussian. We can \ufb01nd a Gaussian approximation to the posterior distribution by using the Laplace approximation. To do this, we must \ufb01rst \ufb01nd a (local) maximum of the posterior, and this must be done using iterative numerical optimization. As usual, it is convenient to maximize the logarithm of the posterior, which can be written in the which corresponds to a regularized sum-of-squares error function. Assuming for the moment that \u03b1 and \u03b2 are \ufb01xed, we can \ufb01nd a maximum of the posterior, which we denote wMAP, by standard nonlinear optimization algorithms such as conjugate gradients, using error backpropagation to evaluate the required derivatives. Having found a mode wMAP, we can then build a local Gaussian approximation by evaluating the matrix of second derivatives of the negative log posterior distribution", "da805182-8bd1-421b-b85b-2734bd7d903b": "Although not in the context of the TD model, representations like the microstimulus representation of Ludvig et al. have been proposed and studied by Grossberg and Schmajuk , Brown, Bullock, and Grossberg , Buhusi and Schmajuk , and Machado . The \ufb01gures on pages 353\u2013355 are adapted from Sutton and Barto . 14.3 Section 1.7 includes comments on the history of trial-and-error learning and the Law of E\u21b5ect. The idea that Thorndike\u2019s cats might have been exploring according to an instinctual context-speci\ufb01c ordering over actions rather than by just selecting from a set of instinctual impulses was suggested by Peter Dayan (personal communication).\n\nSelfridge, Sutton, and Barto  illustrated the e\u21b5ectiveness of shaping in a pole-balancing reinforcement learning task. Other examples of shaping in reinforcement learning are Gullapalli and Barto , Mahadevan and Connell , Mataric , Dorigo and Colombette , Saksida, Raymond, and Touretzky , and Randl\u00f8v and Alstr\u00f8m", "21cc3868-60da-41b4-8690-7cbde03a85b1": "s at the end of each episode, averaged over the \ufb01rst 10 episodes, as well as 100 independent runs, for Finally, there is also a truncated version of Sarsa(\u03bb), called forward Sarsa(\u03bb) (van Seijen, 2016), which appears to be a particularly e\u21b5ective model-free control method for use in conjunction with multi-layer arti\ufb01cial neural networks. We are starting now to reach the end of our development of fundamental TD learning algorithms. To present the \ufb01nal algorithms in their most general forms, it is useful to generalize the degree of bootstrapping and discounting beyond constant parameters to functions potentially dependent on the state and action", "32c2a4d0-74dc-45f0-9a8e-33efbb28e8c1": "We can also derive an on-line stochastic algorithm  by applying the Robbins-Monro procedure Section 2.3.5 to the problem of \ufb01nding the roots of the regression function given by the derivatives of J in (9.1) with respect to \u00b5k. This leads to a sequential update in which, for each Exercise 9.2 data point xn in turn, we update the nearest prototype \u00b5k using where \u03b7n is the learning rate parameter, which is typically made to decrease monotonically as more data points are considered. The K-means algorithm is based on the use of squared Euclidean distance as the measure of dissimilarity between a data point and a prototype vector. Not only does this limit the type of data variables that can be considered (it would be inappropriate for cases where some or all of the variables represent categorical labels for instance), but it can also make the determination of the cluster means nonrobust to outliers. We Section 2.3.7 can generalize the K-means algorithm by introducing a more general dissimilarity measure V(x, x\u2032) between two vectors x and x\u2032 and then minimizing the following distortion measure which gives the K-medoids algorithm", "46542774-3cc7-4c31-a5cf-11273860635b": "This update rule (2.3) is of a form that occurs frequently throughout this book. The a step toward the \u201cTarget.\u201d The target is presumed to indicate a desirable direction in which to move, though it may be noisy. In the case above, for example, the target is the nth reward. Note that the step-size parameter (StepSize) used in the incremental method (2.3) changes from time step to time step.\n\nIn processing the nth reward for action a, the Pseudocode for a complete bandit algorithm using incrementally computed sample averages and \"-greedy action selection is shown in the box below. The function bandit(a) is assumed to take an action and return a corresponding reward. The averaging methods discussed so far are appropriate for stationary bandit problems, that is, for bandit problems in which the reward probabilities do not change over time. As noted earlier, we often encounter reinforcement learning problems that are e\u21b5ectively nonstationary. In such cases it makes sense to give more weight to recent rewards than to long-past rewards. One of the most popular ways of doing this is to use a constant step-size parameter", "00ac8eb3-cb36-4cce-b9c8-1d7e02c33fa5": "Because we already know the values of the \u03b4\u2019s for the output units, it follows that by recursively applying (5.56) we can evaluate the \u03b4\u2019s for all of the hidden units in a feed-forward network, regardless of its topology. The backpropagation procedure can therefore be summarized as follows. 1. Apply an input vector xn to the network and forward propagate through the network using (5.48) and (5.49) to \ufb01nd the activations of all the hidden and output units. 2. Evaluate the \u03b4k for all the output units using (5.54). 3. Backpropagate the \u03b4\u2019s using (5.56) to obtain \u03b4j for each hidden unit in the 4. Use (5.53) to evaluate the required derivatives. In the above derivation we have implicitly assumed that each hidden or output unit in the network has the same activation function h(\u00b7).\n\nThe derivation is easily generalized, however, to allow different units to have individual activation functions, simply by keeping track of which form of h(\u00b7) goes with which unit", "63b458d3-83c1-49e1-971e-501e78b222fd": "Illuminating this result is Barnard\u2019s  derivation of the TD algorithm as a combination of one step of an incremental method for learning a model of the Markov chain and one step of a method for computing predictions from the model. The term certainty equivalence is from the adaptive control literature . 6.4 The Sarsa algorithm was introduced by Rummery and Niranjan . They explored it in conjunction with arti\ufb01cial neural networks and called it \u201cModi\ufb01ed Connectionist Q-learning\u201d. The name \u201cSarsa\u201d was introduced by Sutton .\n\nThe convergence of one-step tabular Sarsa (the form treated in this chapter) has been proved by Singh, Jaakkola, Littman, and Szepesv\u00b4ari . The \u201cwindy gridworld\u201d example was suggested by Tom Kalt. Holland\u2019s  bucket brigade idea evolved into an algorithm closely related to Sarsa. The original idea of the bucket brigade involved chains of rules triggering each other; it focused on passing credit back from the current rule to the rules that triggered it", "74df064f-fa3c-4054-9fbd-c4c0256ead93": "In particular, for an on-policy method we must estimate q\u21e1(s, a) for the current behavior policy \u21e1 and for all states s and actions a. This can be done using essentially the same TD method described above for learning v\u21e1. Recall that an episode consists of an alternating sequence of states and state\u2013action pairs: In the previous section we considered transitions from state to state and learned the values of states. Now we consider transitions from state\u2013action pair to state\u2013action pair, and learn the values of state\u2013action pairs. Formally these cases are identical: they are both Markov chains with a reward process. The theorems assuring the convergence of state values under TD(0) also apply to the corresponding algorithm for action values: This update is done after every transition from a nonterminal state St. If St+1 is terminal, then Q(St+1, At+1) is de\ufb01ned as zero. This rule uses every element of the quintuple of events, (St, At, Rt+1, St+1, At+1), that make up a transition from one state\u2013action pair to the next. This quintuple gives rise to the name Sarsa for the algorithm", "05620289-0a79-449d-82c4-e94754750d5d": "Plug these samples, as well as the \ufb01tted q(z), into the following estimator: The Monte Carlo EM algorithm does not employ an encoder, instead it samples from the posterior of the latent variables using gradients of the posterior computed with \u2207z log p\u03b8(z|x) = \u2207z log p\u03b8(z) + \u2207z log p\u03b8(x|z). The Monte Carlo EM procedure consists of 10 HMC leapfrog steps with an automatically tuned stepsize such that the acceptance rate was 90%, followed by 5 weight updates steps using the acquired sample. For all algorithms the parameters were updated using the Adagrad stepsizes (with accompanying annealing schedule). The marginal likelihood was estimated with the \ufb01rst 1000 datapoints from the train and test sets, for each datapoint sampling 50 values from the posterior of the latent variables using Hybrid Monte Carlo with 4 leapfrog steps. As written in the paper, it is possible to perform variational inference on both the parameters \u03b8 and the latent variables z, as opposed to just the latent variables as we did in the paper.\n\nHere, we\u2019ll derive our estimator for that case", "dbad713d-3872-44cc-9312-caee16786a19": "Most deep learning research on computer vision has focused not on such exotic  447  CHAPTER 12. APPLICATIONS  applications that expand the realm of what is possible with imagery but rather on a small core of AI goals aimed at replicating human abilities.\n\nMost deep learning for computer vision is used for object recognition or detection of some form, whether this means reporting which object is present in an image, annotating  https://www.deeplearningbook.org/contents/applications.html    an image with bounding boxes around each object, transcribing a sequence of syinbols from an image, or labeling each pixel in an image with the identity of the object it belongs to. Because generative modeling has been a guiding principle  of deep learning research, there is also a large body of work on image synthesis using deep models. While image synthesis ex nihilo is usually not considered a computer vision endeavor, models capable of image synthesis are usually useful for image restoration, a computer vision task involving repairing defects in images or removing objects from images. 12.2.1 Preprocessing  Many application areas require sophisticated preprocessing because the original input comes in a form that is difficult for many deep learning architectures to represent. Computer vision usually requires relatively little of this kind of pre- processing", "18fb1252-1442-4a34-ae5b-52fc00000b4d": "DEEP FEEDFORWARD NETWORKS  https://www.deeplearningbook.org/contents/mlp.html    mean squared error holds for a linear model, but in fact, the equivalence holds regardless of the f(a;0) used to predict the mean of the Gaussian.\n\nAn advantage of this approach of deriving the cost function from maximum ikelihood is that it removes the burden of designing cost functions for each model. Specifying a model p(y | x) automatically determines a cost function log p(y | x). One recurring theme throughout neural network design is that the gradient of he cost function must be large and predictable enough to serve as a good guide for the learning algorithm. Functions that saturate (become very flat) undermine chis objective because they make the gradient become very small. In many cases his happens because the activation functions used to produce the output of the hidden units or the output units saturate. The negative log-likelihood helps to avoid this problem for many models. Several output units involve an exp function hat can saturate when its argument is very negative", "214afe83-e426-45cd-a39e-d9a18d31c5fd": "Discrete convolution can be viewed as multiplication by a matrix, but the matrix has several entries constrained to be equal to other entries. For example, for univariate discrete convolution, each row of the matrix is constrained to be equal to the row above shifted by one element. This is known as a Toeplitz matrix. In two dimensions, a doubly block circulant matrix corresponds to convolution. In addition to these constraints that several elements be equal to each other, convolution usually corresponds to a very sparse matrix (a matrix whose entries are mostly equal to zero). This is because the kernel is usually much smaller than the input image.\n\nAny neural network algorithm that works with  https://www.deeplearningbook.org/contents/convnets.html    matrix multiplication and does not depend on specific properties of the matrix structure should work with convolution, without requiring any further changes to the neural network. Typical convolutional neural networks do make use of  further specializations in order to deal with large inputs efficiently, but these are not strictly necessary from a theoretical perspective", "779d9f8e-6902-4c58-96c5-6e37e3e3512a": "The sequence of states, times, and predictions is thus as follows: The rewards in this example are the elapsed times on each leg of the journey.1 We are not discounting (\u03b3 = 1), and thus the return for each state is the actual time to go from that state. The value of each state is the expected time to go. The second column of numbers gives the current estimated value for each state encountered. A simple way to view the operation of Monte Carlo methods is to plot the predicted total time (the last column) over the sequence, as in Figure 6.1 (left). The red arrows show the changes in predictions recommended by the constant-\u21b5 MC method (6.1), for \u21b5 = 1. These are exactly the errors between the estimated value (predicted time to go) in each state and the actual return (actual time to go). For example, when you exited the highway you thought it would take only 15 minutes more to get home, but in fact it took 23 minutes. Equation 6.1 applies at this point and determines an increment in the estimate of time to go after exiting the highway.\n\nThe error, Gt \u2212 V (St), at this time is eight minutes. Suppose the step-size parameter, \u21b5, is 1/2", "72e72ef1-4c05-49b7-b518-43f158ed4114": "A straightforward technique  for doing so is to compute  https://www.deeplearningbook.org/contents/mlp.html    Hv =V2|(Ves(x)) VU.\n\n(6.59) Both gradient computations in this expression may be computed automatically by the appropriate software library. Note that the outer gradient expression takes the gradient of a function of the inner gradient expression. If v is itself a vector produced by a computational graph, it is important to specify that the automatic differentiation software should not differentiate through the graph that produced v.  While computing the Hessian is usually not advisable, it is possible to do with Hessian vector products. One simply computes He for all i =1,...,n, where  2)  e() is the one-hot vector with e;\u2019 = 1 and all other entries are equal to 0. 6.6 Historical Notes  Feedforward networks can be seen as efficient nonlinear function approximators based on using gradient descent to minimize the error in a function approximation. From this point of view, the modern feedforward network is the culmination of centuries of progress on the general function approximation task. The chain rule that underlies the back-propagation algorithm was invented in the seventeenth century", "f24393e5-3cd5-4747-8181-a3e730bfcd8d": "In this example, we do not explain how the back-propagation algorithm works. The purpose is only to illustrate what the desired result is: a computational graph with a symbolic description of the derivative. to the graph that provide a symbolic description of the desired derivatives. This is the approach taken by Theano  and TensorFlow . An example of how it works is illustrated in figure 6.10. The primary advantage of this approach is that the derivatives are described in the same language as the original expression.\n\nBecause the derivatives are just another computational graph, it is possible to run back-propagation again, differentiating the derivatives to obtain higher derivatives. (Computation of higher-order derivatives is described in section 6.5.10.) We will use the latter approach and describe the back-propagation algorithm in terms of constructing a computational graph for the derivatives. Any subset of the graph may then be evaluated using specific numerical values at a later time. This allows us to avoid specifying exactly when each operation should be computed. Instead, a generic graph evaluation engine can evaluate every node as soon as its parents\u2019 values are available", "eefaac35-6dd8-4709-8ea2-f9bbe07fec30": "They found that when APV-MCTS used the value function derived from the RL policy, it performed better than if it used the value function derived from the SL policy. Several methods worked together to produce AlphaGo\u2019s impressive playing skill. The DeepMind team evaluated di\u21b5erent versions of AlphaGo in order to assess the contributions made by these various components. The parameter \u2318 in (16.4) controlled the mixing of game state evaluations produced by the value network and by rollouts. With \u2318 = 0, AlphaGo used just the value network without rollouts, and with \u2318 = 1, evaluation relied just on rollouts. They found that AlphaGo using just the value network played better than the rollout-only AlphaGo, and in fact played better than the strongest of all other Go programs existing at the time. The best play resulted from setting \u2318 = 0.5, indicating that combining the value network with rollouts was particularly important to AlphaGo\u2019s success", "03fd3305-fe6e-48af-a43e-df0ee65e5ff9": "The optimal model parameters are:  0\u201d = arg min Ep. \u00bb(p)  It looks very similar to a normal learning task, but one dataset is considered as one data sample. Few-shot classification is an instantiation of meta-learning in the field of supervised learning. The dataset D is often split into two parts, a support set S' for learning and a prediction set B for training or testing, D = (S, B). Often we consider a K-shot N-class classification task: the support set contains K labelled examples for each of N classes. Training Testing  ee | Test dataset: \u201cdog-otter\"\u201d iii aaa  te ail : | .\n\nLf\") |  https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Log   Training in the Same Way as Testing  A dataset D contains pairs of feature vectors and labels, D = {(x;, y;)} and each label belongs to a known label set \u00a3/#>*!", "209a727d-62c6-4572-9873-a33237ed5a60": "We can see immediately that the variational posterior distribution over the parameters must factorize between \u03c0 and the remaining parameters \u00b5 and \u039b because all paths connecting \u03c0 to either \u00b5 or \u039b must pass through one of the nodes zn all of which are in the conditioning set for our conditional independence test and all of which are head-to-tail with respect to such paths. As a second illustration of variational inference, we return to the Bayesian linear regression model of Section 3.3. In the evidence framework, we approximated the integration over \u03b1 and \u03b2 by making point estimates obtained by maximizing the log marginal likelihood. A fully Bayesian approach would integrate over the hyperparameters as well as over the parameters. Although exact integration is intractable, we can use variational methods to \ufb01nd a tractable approximation.\n\nIn order to simplify the discussion, we shall suppose that the noise precision parameter \u03b2 is known, and is \ufb01xed to its true value, although the framework is easily extended to include the distribution over \u03b2. For the linear regression model, the variational treatment Exercise 10.26 will turn out to be equivalent to the evidence framework", "deed21ea-af26-4799-8344-887869ea0cdb": "Fortunately, if we also know P(x), we can compute the desired quantity  using Bayes\u2019 rule:  P(x)P(y | x) Ply)  Note that while P(y) appears in the formula, it is usually feasible to compute  P(y) =, Ply | 2) P(2), so we do not need to begin with knowledge of P(y). P(x|y)= (3.42)  68  CHAPTER 3.\n\nPROBABILITY AND INFORMATION THEORY  Bayes\u2019 rule is straightforward to derive from the definition of conditional probability, but it is useful to know the name of this formula since many texts refer to it by name. It is named after the Reverend Thomas Bayes, who first discovered a special case of the formula. The general version presented here was independently discovered by Pierre-Simon Laplace. 3.12 Technical Details of Continuous Variables  A proper formal understanding of continuous random variables and probability density functions requires developing probability theory in terms of a branch of mathematics known as measure theory. Measure theory is beyond the scope of this textbook, but we can briefly sketch some of the issues that measure theory is employed to resolve", "13bc7744-5cd3-472e-97a3-7a6fe58385ac": "Environment models in reinforcement learning are like cognitive maps in that they can be learned by supervised learning methods without relying on reward signals, and then they can be Reinforcement learning\u2019s distinction between model-free and model-based algorithms corresponds to the distinction in psychology between habitual and goal-directed behavior.\n\nModel-free algorithms make decisions by accessing information that has been stored in a policy or an action-value function, whereas model-based methods select actions as the result of planning ahead using a model of the agent\u2019s environment. Outcome-devaluation experiments provide information about whether an animal\u2019s behavior is habitual or under goal-directed control. Reinforcement learning theory has helped clarify thinking about these issues. Animal learning clearly informs reinforcement learning, but as a type of machine learning, reinforcement learning is directed toward designing and understanding e\u21b5ective learning algorithms, not toward replicating or explaining details of animal behavior. We focused on aspects of animal learning that relate in clear ways to methods for solving prediction and control problems, highlighting the fruitful two-way \ufb02ow of ideas between reinforcement learning and psychology without venturing deeply into many of the behavioral details and controversies that have occupied the attention of animal learning researchers", "9cfc418f-8c9e-4be7-af16-fcbdc3be8801": "Given a batch of samples, {x;, y;)}2_, where y; is the class label of x; and a function f(.,.) for measuring similarity between two inputs, the soft nearest neighbor loss at temperature T is defined as:  c= Slog Vi diyimypit,..B PCF\u00bb X5)/7) \u2122 B igkk=1,....B exp(\u2014f (xj, xx) /T)  i=l  The temperature 7 is used for tuning how concentrated the features are in the representation space. For example, when at low temperature, the loss is dominated by the small distances and widely separated representations cannot contribute much and become irrelevant. Common Setup  We can loosen the definition of \u201cclasses\u201d and \u201clabels\u201d in soft nearest-neighbor loss to create positive and negative sample pairs out of unsupervised data by, for example, applying data augmentation to create noise versions of original samples. Most recent studies follow the following definition of contrastive learning objective to incorporate multiple positive and negative samples", "fc92cc30-dc4e-458a-82d8-ae945a31725e": "We study the modeling trade-offs in this new setting and propose an optimizer for automating trade-off decisions that gives up to 1.8\u00d7 speedup per pipeline execution.\n\nIn two collaborations, with the US Department of Veterans Affairs and the US Food and Drug Administration, and on four open-source text and image data sets representative of other deployments, Snorkel provides 132% average improvements to predictive performance over prior heuristic approaches and comes within an average 3.60% of the predictive performance of large hand-curated training sets. Keywords Machine learning \u00b7 Weak supervision \u00b7 Training data In the last several years, there has been an explosion of interest in machine learning-based systems across industry, government, and academia, with an estimated spend this year of $12.5 billion . A central driver has been the advent of deep learning techniques, which can learn task-speci\ufb01c representations of input data, obviating what used to be the most time-consuming development task: feature engineering. These learned representations are particularly effective for tasks like natural language processing and image analysis, which have high-dimensional, high-variance input that is impossible to fully capture with simple rules or handengineered features", "56d613df-1809-4b2e-ad2a-05450bbf7f45": "Various other extensions of GausExercise 6.23 sian process regression have also been considered, for purposes such as modelling the distribution over low-dimensional manifolds for unsupervised learning  and the solution of stochastic differential equations . The predictions of a Gaussian process model will depend, in part, on the choice of covariance function.\n\nIn practice, rather than \ufb01xing the covariance function, we may prefer to use a parametric family of functions and then infer the parameter values from the data. These parameters govern such things as the length scale of the correlations and the precision of the noise and correspond to the hyperparameters in a standard parametric model. Techniques for learning the hyperparameters are based on the evaluation of the likelihood function p(t|\u03b8) where \u03b8 denotes the hyperparameters of the Gaussian process model. The simplest approach is to make a point estimate of \u03b8 by maximizing the log likelihood function. Because \u03b8 represents a set of hyperparameters for the regression problem, this can be viewed as analogous to the type 2 maximum likelihood procedure for linear regression models. Maximization of the log likelihood Section 3.5 can be done using ef\ufb01cient gradient-based optimization algorithms such as conjugate gradients", "13314ef9-e14e-4521-88b4-e8f3105c7072": "where I0(m) is the zeroth-order Bessel function of the \ufb01rst kind. The distribution has period 2\u03c0 so that p(\u03b8 + 2\u03c0) = p(\u03b8) for all \u03b8. Care must be taken in interpreting this distribution because simple expectations will be dependent on the (arbitrary) choice of origin for the variable \u03b8. The parameter \u03b80 is analogous to the mean of a univariate Gaussian, and the parameter m > 0, known as the concentration parameter, is analogous to the precision (inverse variance). For large m, the von Mises distribution is approximately a Gaussian centred on \u03b80.\n\nThe Wishart distribution is the conjugate prior for the precision matrix of a multivariate Gaussian. where W is a D \u00d7 D symmetric, positive de\ufb01nite matrix, and \u03c8(\u00b7) is the digamma function de\ufb01ned by (B.25). The parameter \u03bd is called the number of degrees of freedom of the distribution and is restricted to \u03bd > D \u2212 1 to ensure that the Gamma function in the normalization factor is well-de\ufb01ned", "cf4982eb-32e5-43a9-b843-14cfcfbdc9cd": "They are sampled such that the combined length is \u2264 512 tokens. The LM masking is applied after WordPiece tokenization with a uniform masking rate of 15%, and no special consideration given to partial word pieces. We train with batch size of 256 sequences (256 sequences * 512 tokens = 128,000 tokens/batch) for 1,000,000 steps, which is approximately 40 epochs over the 3.3 billion word corpus. We use Adam with learning rate of 1e-4, \u03b21 = 0.9, \u03b22 = 0.999, L2 weight decay of 0.01, learning rate warmup over the \ufb01rst 10,000 steps, and linear decay of the learning rate. We use a dropout probability of 0.1 on all layers. We use a gelu activation  rather than the standard relu, following OpenAI GPT.\n\nThe training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood. Training of BERTBASE was performed on 4 Cloud TPUs in Pod con\ufb01guration (16 TPU chips total).13 Training of BERTLARGE was performed on 16 Cloud TPUs (64 TPU chips total). Each pretraining took 4 days to complete", "5f717ff8-9ff2-47d3-8679-757ca4afad79": "These techniques, however, can\u2019t be easily extended to new domains, such as CV. Despite promising early results, SSL has not yet brought about the same improvements in computer vision that we have seen in NLP (though this will change).\n\nThe main reason is that it is considerably more difficult to represent uncertainty in the prediction for images than it is for words. When the missing word cannot be predicted exactly (is it \u201clion\u201d or \u201ccheetah\u201d? ), the system can associate a score or a probability to all possible words in the vocabulary: high  \u201d  score for \u201clion,\u201d \u201ccheetah,\u201d and a few other predators, and low scores for all  other words in the vocabulary. Training models at this scale also required a model architecture that was efficient in terms of both runtime and  https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/  Self-supervised learning: The dark matter of intelligence   memory, without compromising on accuracy. Fortunately, a recent innovation by FAIR in the realm of architecture design led to anew model family called RegNets that perfectly fit these needs", "6ec8d5a2-9964-4ecb-97af-9e78ec082df5": "Then in the back-tracking step, having found xmax, we can then use these stored values to assign consistent maximizing states xmax 1 , . , xmax M . The max-sum algorithm, with back-tracking, gives an exact maximizing con\ufb01guration for the variables provided the factor graph is a tree. An important application of this technique is for \ufb01nding the most probable sequence of hidden states in a hidden Markov model, in which case it is known as the Viterbi algorithm. Section 13.2 As with the sum-product algorithm, the inclusion of evidence in the form of observed variables is straightforward. The observed variables are clamped to their observed values, and the maximization is performed over the remaining hidden variables. This can be shown formally by including identity functions for the observed variables into the factor functions, as we did for the sum-product algorithm.\n\nIt is interesting to compare max-sum with the iterated conditional modes (ICM) algorithm described on page 389. Each step in ICM is computationally simpler because the \u2018messages\u2019 that are passed from one node to the next comprise a single value consisting of the new state of the node for which the conditional distribution is maximized", "d4d50c97-0cf9-4415-b5cb-5e234ddc80f1": "These embeddings were learned using the SVD. Later, embeddings would be learned by neural networks. The history of natural language processing is marked by transitions in the popularity of different ways of representing the input to the model. Following this early work on symbols and words, some of the earliest applications of neural networks to NLP  represented the input as a sequence of characters. Bengio ef al. returned the focus to modeling words and introduced neural language models, which produce interpretable word embeddings.\n\nThese neural models have scaled up from defining representations of a small set of symbols in the 1980s to millions of words (including proper nouns and misspellings) in modern applications. This computational scaling effort led to the invention of the techniques described in section 12.4.3. Initially, the use of words as the fundamental units of language models yielded improved language modeling performance . To this day, new techniques continually push both character-based models  and word-based models forward, with recent work  even modeling individual bytes of Unicode characters. The ideas behind neural language models have been extended into several  472  CHAPTER 12", "96ab6a40-e5af-42ee-97f2-63631af65bf4": "When applied to natural images, this topographic ICA approach learns Gabor filters, such that neighboring features have similar orientation, location or frequency.\n\nMany different phase offsets of similar Gabor functions occur within each region, so that pooling over small regions yields translation invariance. 13.3 Slow Feature Analysis  Slow feature analysis (SFA) is a linear factor model that uses information from time signals to learn invariant features . 489  CHAPTER 13. LINEAR FACTOR MODELS  Slow feature analysis is motivated by a general principle called the slowness principle. The idea is that the important characteristics of scenes change very slowly compared to the individual measurements that make up a description of a scene. For example, in computer vision, individual pixel values can change very rapidly. If a zebra moves from left to right across the image, an individual pixel will rapidly change from black to white and back again as the zebra\u2019s stripes pass over the pixel. By comparison, the feature indicating whether a zebra is in the image will not change at all, and the feature describing the zebra\u2019s position will change slowly. We therefore may wish to regularize our model to learn features that change slowly over time", "0bbda20a-0fec-455d-a6a5-72e563494126": "(The name \u201cimmorality\u201d may seem strange; it was coined in the graphical  Vwi 1 od 1 4 74 + \\m 1 a a4 rd  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    models lverayure as a JOKE ADOUL ULMALITIed pares.) LO COouvert a airecved model with graph P into an undirected model, we need to create a new graph 4. For every pair of variables x and y, we add an undirected edge connecting x and y to U if there is a directed edge tin either direction) connecting x and y in D or if x  and y are both parents in D of a third variable z. The resulting YU is known as a moralized graph. See figure 16.11 for examples of converting directed models to undirected models via moralization", "6a79aa23-986f-43ee-af0a-2bcf24c5c9fe": "Miller\u2019s idea not only parallels Klopf\u2019s (with the exception of its explicit invocation of a distinct \u201cstrengthening signal\u201d), it also anticipated the general features of reward-modulated STDP. A related though di\u21b5erent idea, which Seung  called the \u201chedonistic synapse,\u201d is that synapses individually adjust the probability that they release neurotransmitter in the manner of the Law of E\u21b5ect: if reward follows release, the release probability increases, and decreases if reward follows failure to release.\n\nThis is essentially the same as the learning scheme Minsky used in his 1954 Princeton Ph.D. dissertation, where he called the synapse-like learning element a SNARC (Stochastic Neural-Analog Reinforcement Calculator). Contingent eligibility is involved in these ideas too, although it is contingent on the activity of an individual synapse instead of the postsynaptic neuron. Also related is the proposal of Unnikrishnan and Venugopal  that uses the correlation-based method of Harth and Tzanakou  to adjust ANN weights", "46ef44b3-e04c-4c79-b65e-16d7ab2ecb13": "It is easily seen that the quantities \u03c9(zn) have the probabilistic Exercise 13.16 Once we have completed the \ufb01nal maximization over zN, we will obtain the value of the joint distribution p(X, Z) corresponding to the most probable path. We also wish to \ufb01nd the sequence of latent variable values that corresponds to this path. To do this, we simply make use of the back-tracking procedure discussed in Section 8.4.5.\n\nSpeci\ufb01cally, we note that the maximization over zn must be performed for each of the K possible values of zn+1. Suppose we keep a record of the values of zn that correspond to the maxima for each value of the K values of zn+1. Let us denote this function by \u03c8(kn) where k \u2208 {1, . , K}. Once we have passed messages to the end of the chain and found the most probable state of zN, we can then use this function to backtrack along the chain by applying it recursively Intuitively, we can understand the Viterbi algorithm as follows", "d55db94c-8beb-4870-a9c8-84e9369e60c9": "CONVOLUTIONAL NETWORKS  Complex layer terminology Simple layer terminology  https://www.deeplearningbook.org/contents/convnets.html    Convolutional Layer  Pooling stage Pooling layer  Detector st  Nonlinearity Nonlinearity y  ified linear e.g., rectified linear  Convolution stage: Convolution layer:  Affine transform Affine transform  Input to layer Input to layers  Figure 9.7: The components of a typical convolutional neural network layer. There are two commonly used sets of terminology for describing these layers. (Left)In this terminology, the convolutional net is viewed as a small number of relatively complex layers, with each layer having many \u201cstages.\u201d In this terminology, there is a one-to-one mapping between kernel tensors and network layers.\n\nIn this book we generally use this terminology. (Right)In this terminology, the convolutional net is viewed as a larger number of simple layers; every step of processing is regarded as a layer in its own right. This means that not every \u201clayer\u201d has parameters. In all cases, pooling helps to make the representation approximately invariant to small translations of the input", "c7d51537-bf06-4c03-b92f-b00d0b8b05c1": "Back-translation  CERT ; code) generates augmented sentences via back-translation. Various translation models for different languages can be employed for creating different versions of augmentations. Once we have a noise version of text samples, many contrastive learning frameworks introduced above, such as MoCo, can be used to learn sentence embedding. Dropout and Cutoff  Shen et al. proposed to apply Cutoff to text augmentation, inspired by cross-view training. They proposed three cutoff augmentation strategies:  Token cutoff removes the information of a few selected tokens.\n\nTo make sure there is no data leakage, corresponding tokens in the input, positional and other relevant embedding matrices should all be zeroed out.,  Feature cutoff removes a few feature columns. Span cutoff removes a continuous chunk of texts", "94cbb518-40c6-45b0-a61b-48166503d079": "This can be done using the following properties: Techniques for Constructing New Kernels.\n\nGiven valid kernels k1(x, x\u2032) and k2(x, x\u2032), the following new kernels will also be valid: where c > 0 is a constant, f(\u00b7) is any function, q(\u00b7) is a polynomial with nonnegative coef\ufb01cients, \u03c6(x) is a function from x to RM, k3(\u00b7, \u00b7) is a valid kernel in RM, A is a symmetric positive semide\ufb01nite matrix, xa and xb are variables (not necessarily disjoint) with x = (xa, xb), and ka and kb are valid kernel functions over their respective spaces. Equipped with these properties, we can now embark on the construction of more complex kernels appropriate to speci\ufb01c applications. We require that the kernel k(x, x\u2032) be symmetric and positive semide\ufb01nite and that it expresses the appropriate form of similarity between x and x\u2032 according to the intended application. Here we consider a few common examples of kernel functions", "ce6a0002-6401-4dd5-a227-b058035679c4": "Future development of reinforcement learning theory and algorithms will likely exploit links to many other features of animal learning as the computational utility of these features becomes better appreciated. We expect that a \ufb02ow of ideas between reinforcement learning and psychology will continue to bear fruit for both disciplines.\n\nMany connections between reinforcement learning and areas of psychology and other behavioral sciences are beyond the scope of this chapter. We largely omit discussing links to the psychology of decision making, which focuses on how actions are selected, or how decisions are made, after learning has taken place. We also do not discuss links to ecological and evolutionary aspects of behavior studied by ethologists and behavioral ecologists: how animals relate to one another and to their physical surroundings, and how their behavior contributes to evolutionary \ufb01tness. Optimization, MDPs, and dynamic programming \ufb01gure prominently in these \ufb01elds, and our emphasis on agent interaction with dynamic environments connects to the study of agent behavior in complex \u201cecologies.\u201d Multi-agent reinforcement learning, omitted in this book, has connections to social aspects of behavior. Despite the lack of treatment here, reinforcement learning should by no means be interpreted as dismissing evolutionary perspectives. Nothing about reinforcement learning implies a tabula rasa view of learning and behavior", "b7daa2b2-2dd0-461b-bd60-1530bf09f8f5": "For instance, the Gaussian version of the Markov random \ufb01eld, Section 8.3 which is widely used as a probabilistic model of images, is a Gaussian distribution over the joint space of pixel intensities but rendered tractable through the imposition of considerable structure re\ufb02ecting the spatial organization of the pixels. Similarly, the linear dynamical system, used to model time series data for applications such Section 13.3 as tracking, is also a joint Gaussian distribution over a potentially large number of observed and latent variables and again is tractable due to the structure imposed on the distribution. A powerful framework for expressing the form and properties of such complex distributions is that of probabilistic graphical models, which will form the subject of Chapter 8. An important property of the multivariate Gaussian distribution is that if two sets of variables are jointly Gaussian, then the conditional distribution of one set conditioned on the other is again Gaussian.\n\nSimilarly, the marginal distribution of either set is also Gaussian. Consider \ufb01rst the case of conditional distributions. Suppose x is a D-dimensional vector with Gaussian distribution N(x|\u00b5, \u03a3) and that we partition x into two disjoint subsets xa and xb", "e5863412-b811-49cb-af85-70849323ad93": ", xN, then, because we have represented the marginal distribution in the form p(x) = \ufffd z p(x, z), it follows that for every observed data point xn there is a corresponding We have therefore found an equivalent formulation of the Gaussian mixture involving an explicit latent variable. It might seem that we have not gained much by doing so. However, we are now able to work with the joint distribution p(x, z) instead of the marginal distribution p(x), and this will lead to signi\ufb01cant simpli\ufb01cations, most notably through the introduction of the expectation-maximization (EM) algorithm. Another quantity that will play an important role is the conditional probability of z given x. We shall use \u03b3(zk) to denote p(zk = 1|x), whose value can be found using Bayes\u2019 theorem We shall view \u03c0k as the prior probability of zk = 1, and the quantity \u03b3(zk) as the corresponding posterior probability once we have observed x.\n\nAs we shall see later, \u03b3(zk) can also be viewed as the responsibility that component k takes for \u2018explaining\u2019 the observation x", "d9fd8518-3906-4ef0-80bd-20ee8c5ff83a": "Committee-based sampling for training probabilistic classi\ufb01ers. Proceedings of the Twelfth International Conference on International Conference on Machine Learning, 150\u2013157. Dayan, P., & Hinton, G. E. Using expectation-maximization for reinforcement learning. Neural Computation, 9(2), 271\u2013278. Deisenroth, M. P., Neumann, G., Peters, J., et al. A survey on policy search for robotics. Foundations and Trends\u00ae in Robotics, 2(1\u20132), 1\u2013142. Dempster, A. P., Laird, N. M., & Rubin, D. B. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society: Series B (Methodological), 39(1), 1\u201322. Deng, M., Wang, J., Hsieh, C.-P., Wang, Y., Guo, H., Shu, T., Song, M., Xing, E", "ee3d7d1c-4afa-43f9-87f8-c2805df1c7e4": "In fact, even simply taking a random step when the gradient magnitude is above a threshold tends to work almost as well. If the explosion is so severe that the gradient is numerically Inf or Nan (considered infinite or not-a-number), then a random step of size v can be taken and will typically move away from the numerically unstable configuration. Clipping the gradient norm per minibatch will not change the direction of the gradient for an individual minibatch. Taking the average of the norm-clipped gradient from many minibatches, however, is not equivalent to clipping the norm of the true gradient (the gradient formed from using all examples).\n\nExamples that have large gradient norm, as well as examples that appear in the same minibatch as such examples, will have their contribution to the final direction diminished. This stands in contrast to traditional minibatch gradient descent, where the true gradient direction is equal to the average over all minibatch gradients. Put another way, traditional stochastic gradient descent uses an unbiased estimate of the gradient, while gradient descent with norm clipping introduces a heuristic bias that we know empirically to be useful. With element-  410  CHAPTER 10", "44e5047e-b0db-4fa3-b968-518eb648b552": "Ef\ufb01cient approximate ML or MAP estimation for the parameters \u03b8. The parameters can be of interest themselves, e.g. if we are analyzing some natural process. They also allow us to mimic the hidden random process and generate arti\ufb01cial data that resembles the real data. 2.\n\nEf\ufb01cient approximate posterior inference of the latent variable z given an observed value x for a choice of parameters \u03b8. This is useful for coding or data representation tasks. 3. Ef\ufb01cient approximate marginal inference of the variable x. This allows us to perform all kinds of inference tasks where a prior over x is required. Common applications in computer vision include image denoising, inpainting and super-resolution. For the purpose of solving the above problems, let us introduce a recognition model q\u03c6(z|x): an approximation to the intractable true posterior p\u03b8(z|x). Note that in contrast with the approximate posterior in mean-\ufb01eld variational inference, it is not necessarily factorial and its parameters \u03c6 are not computed from some closed-form expectation", "cf6ad79a-91d7-48cd-862f-b7190b3859d0": "Tan, B., Hu, Z., ZichaoYang, R., & Xing, E. Connecting the dots between mle and rl for sequence generation. arXiv. https://arxiv.org/abs/1811.09740 Tan, B., Qin, L., Xing, E., & Hu, Z. Summarizing text on any aspects: A knowledge-informed weakly-supervised approach. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 6301\u20136309.\n\nTaskar, B., Guestrin, C., & Koller, D. Max-margin markov networks. Advances in Neural Information Processing systems, 25\u201332. Thrun, S. Lifelong learning algorithms. Learning to learn (pp. 181\u2013209). Springer. Titsias, M. Variational learning of inducing variables in sparse Gaussian processes. Arti\ufb01cial Intelligence and Statistics, 567\u2013574. Vapnik, V. N", "53b986d5-6a62-4f45-a857-8d0459cb7df4": "But the phrase is lately gaining currency in psychology and neuroscience, likely because strong parallels have surfaced between reinforcement learning algorithms and animal learning\u2014parallels described in this chapter and the next.\n\nAccording to common usage, a reward is an object or event that an animal will approach and work for. A reward may be given to an animal in recognition of its \u2018good\u2019 behavior, or given in order to make the animal\u2019s behavior \u2018better.\u2019 Similarly, a penalty is an object or event that the animal usually avoids and that is given as a consequence of \u2018bad\u2019 behavior, usually in order to change that behavior. Primary reward is reward due to machinery built into an animal\u2019s nervous system by evolution to improve its chances of survival and reproduction, for example, reward produced by the taste of nourishing food, sexual contact, successful escape, and many other stimuli and events that predicted reproductive success over the animal\u2019s ancestral history. As explained in Section 14.2.1, higher-order reward is reward delivered by stimuli that predict primary reward, either directly or indirectly by predicting other stimuli that predict primary reward. Reward is secondary if its rewarding quality is the result of directly predicting primary reward", "8891b891-cf86-43d5-9c13-bb8ab3a34328": "Intuitively, from Figure 3.2, we anticipate that this solution corresponds to the orthogonal projection of t onto the subspace S. This is indeed the case, as can easily be veri\ufb01ed by noting that the solution for y is given by \u03a6wML, and then con\ufb01rming that this takes the form of an orthogonal projection. Exercise 3.2 In practice, a direct solution of the normal equations can lead to numerical dif\ufb01culties when \u03a6T\u03a6 is close to singular. In particular, when two or more of the basis vectors \u03d5j are co-linear, or nearly so, the resulting parameter values can have large magnitudes. Such near degeneracies will not be uncommon when dealing with real data sets. The resulting numerical dif\ufb01culties can be addressed using the technique of singular value decomposition, or SVD . Note that the addition of a regularization term ensures that the matrix is nonsingular, even in the presence of degeneracies", "c4b65725-154d-4208-a903-01fb2fa86826": "Also, we note that scaling the log likelihood by a positive constant coef\ufb01cient does not alter the location of the maximum with respect to w, and so we can replace the coef\ufb01cient \u03b2/2 with 1/2. Finally, instead of maximizing the log likelihood, we can equivalently minimize the negative log likelihood.\n\nWe therefore see that maximizing likelihood is equivalent, so far as determining w is concerned, to minimizing the sum-of-squares error function de\ufb01ned by (1.2). Thus the sum-of-squares error function has arisen as a consequence of maximizing likelihood under the assumption of a Gaussian noise distribution. We can also use maximum likelihood to determine the precision parameter \u03b2 of the Gaussian conditional distribution. Maximizing (1.62) with respect to \u03b2 gives Again we can \ufb01rst determine the parameter vector wML governing the mean and subsequently use this to \ufb01nd the precision \u03b2ML as was the case for the simple Gaussian distribution. Section 1.2.4 Having determined the parameters w and \u03b2, we can now make predictions for new values of x", "a2e457bd-5286-43e5-a3df-28ed6ba34da2": "This  https://www.deeplearningbook.org/contents/generative_models.html    _ This specific choice of MLP is somewhat arbitrary, compared to many of the inference equations in chapter 19 that are derived from first punciples. This MLP is a heuristic choice that seems to work well in practice and is used consistently  in the literature. Many approximate inference techniques are motivated by their ability to find a maximally tight variational lower bound on the log-likelihood under some set of constraints. One can construct a variational lower bound on the log-likelihood using the hidden unit expectations defined by the DBN\u2019s MLP, but this is true of any probability distribution over the hidden units, and there is no reason to believe that this MLP provides a particularly tight bound. In particular, the MLP ignores many important interactions in the DBN graphical model. The MLP propagates information upward from the visible units to the deepest hidden units, but it does not propagate any information downward or sideways.\n\nThe DBN graphical model has explaining away interactions between all the hidden units within the same layer as well as in top-down interactions between layers", "c21e4a2c-8adb-4b8b-9a57-b6bfa59580f5": "Proceedings of the IEEE, 86(11), 2278\u20132324. Mnih, A. and Gregor, K. Neural variational inference and learning in belief networks. Technical report, arXiv preprint arXiv:1402.0030. Rezende, D. J., Mohamed, S., and Wierstra, D. Stochastic backpropagation and approximate inference in deep generative models.\n\nTechnical report, arXiv:1401.4082. Rifai, S., Bengio, Y., Dauphin, Y., and Vincent, P. A generative process for sampling contractive auto-encoders. In ICML\u201912. Salakhutdinov, R. and Hinton, G. E. Deep Boltzmann machines. In AISTATS\u20192009, pages 448\u2013 455. Schmidhuber, J. Learning factorial codes by predictability minimization. Neural Computation, 4(6), 863\u2013879", "bc5cd9e7-e3e7-4c2f-92d1-88e9d43e200e": "It has the following properties:  P(ix=1)=6\u00a2  P(x=2)=\u00a2\"(1-\u00a2)**  Vary (x) = e(1 \u2014 9)  3.9.2 Multinoulli Distribution  The multinoulli, or categorical, distribution is a distribution over a single dis- crete variable with k different states, where k is finite.! The multinoulli distribution  } \u201cMultinoulli\u201d is a term that was recently coined by Gustavo Lacerda and popularized by Murphy .\n\nThe multinoulli distribution is a special case of the multinomial distribution A multinomial distribution is the distribution over vectors in {0,...,n}* representing how many times each of the k categories is visited when n samples are drawn from a multinoulli distribution. Many texts use the term \u201cmultinomial\u201d to refer to multinoulli distributions without clarifying that they are referring only to the n = 1 case. 60  CHAPTER 3. PROBABILITY AND INFORMATION THEORY  is parametrized by a vector p \u20ac (0, yr, where p; gives the probability of the  i-th state", "a5ada511-b630-4214-b199-bfe8952e44ea": "Common sense helps people learn new skills without requiring massive amounts of teaching for every single task. For example, if we show just a few drawings of cows to small children, they\u2019ll eventually be able to recognize any cow they see. By contrast, Al systems trained with supervised learning require many examples of cow images and might still fail to classify cows in unusual situations, such as lying on a beach.\n\nHow is it that humans can learn to drive a car in about 20 hours of practice with very little supervision, while fully autonomous driving still eludes our best Al systems trained with thousands of hours of data from human drivers? The short answer is that humans rely on their previously acquired background knowledge of how the world works. How do we get machines to do the same? We believe that self-supervised learning is one of the most promising ways to build such background  https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/  Self-supervised learning: The dark matter of intelligence   knowledge and approximate a torm ot common sense in Al systems. We believe that self-supervised learning (SSL) is one of the most promising ways to build such background knowledge and approximate a form of common sense in Al systems", "0b23a3ae-cad4-4b96-b323-c149650a8aa7": "Exercise 5.17 By neglecting the second term in (5.83), we arrive at the Levenberg\u2013Marquardt approximation or outer product approximation (because the Hessian matrix is built up from a sum of outer products of vectors), given by where bn = \u2207yn = \u2207an because the activation function for the output units is simply the identity. Evaluation of the outer product approximation for the Hessian is straightforward as it only involves \ufb01rst derivatives of the error function, which can be evaluated ef\ufb01ciently in O(W) steps using standard backpropagation. The elements of the matrix can then be found in O(W 2) steps by simple multiplication. It is important to emphasize that this approximation is only likely to be valid for a network that has been trained appropriately, and that for a general network mapping the second derivative terms on the right-hand side of (5.83) will typically not be negligible", "3954234f-3441-4336-9c3c-646d93bad63f": "In the subsequent maximization (M) step, L(q(n+1), \u03b8) is minimized w.r.t. \u03b8: which is to maximize the expected complete data log-likelihood. The EM algorithm has an appealing property that it monotonically decreases the negative marginal log-likelihood over iterations. To see this, notice that after the E-step the upper bound L(q(n+1), \u03b8(n)) is equal to the negative marginal Variational EM.\n\nWhen the model p\u03b8(x, y) is complex (e.g., a neural network or a multilayer graphical model), directly working with the true posterior in the E-step becomes intractable. Variational EM overcomes the di\ufb03culty with approximations. It considers a restricted family Q\u2032 of the variational distribution q(y) such that optimization w.r.t. q within the family is tractable: A common way to restrict the q family is the mean-\ufb01eld methods, which partition the components of y into sub-groups y = (y1, . , yM) and assume that q factorizes w.r.t", "836a17a1-96e1-4a5d-8859-fd18850e427e": "We call this way of generating experience and updates trajectory sampling. It is hard to imagine any e\ufb03cient way of distributing updates according to the on-policy distribution other than by trajectory sampling. If one had an explicit representation of the on-policy distribution, then one could sweep through all states, weighting the update of each according to the on-policy distribution, but this leaves us again with all the computational costs of exhaustive sweeps. Possibly one could sample and update individual state\u2013action pairs from the distribution, but even if this could be done e\ufb03ciently, what bene\ufb01t would this provide over simulating trajectories? Even knowing the on-policy distribution in an explicit form is unlikely.\n\nThe distribution changes whenever the policy changes, and computing the distribution requires computation comparable to a complete policy evaluation. Consideration of such other possibilities makes trajectory sampling seem both e\ufb03cient and elegant. Is the on-policy distribution of updates a good one? Intuitively it seems like a good choice, at least better than the uniform distribution. For example, if you are learning to play chess, you study positions that might arise in real games, not random positions of chess pieces", "a7c99688-d9f3-449b-ac19-afc59b84c795": "First, actions that had never been tried before from a state were allowed to be considered in the planning step (f) of the Tabular Dyna-Q algorithm in the box above. Second, the initial model for such actions was that they would lead back to the same state with a reward of zero. what happens during the second episode of the \ufb01rst maze task (Figure 8.3). At the beginning of the second episode, only the state\u2013action pair leading directly into the goal has a positive value; the values of all other pairs are still zero.\n\nThis means that it is pointless to perform updates along almost all transitions, because they take the agent from one zero-valued state to another, and thus the updates would have no e\u21b5ect. Only an update along a transition into the state just prior to the goal, or from it, will change any values. If simulated transitions are generated uniformly, then many wasteful updates will be made before stumbling onto one of these useful ones. As planning progresses, the region of useful updates grows, but planning is still far less e\ufb03cient than it would be if focused where it would do the most good. In the much larger problems that are our real objective, the number of states is so large that an unfocused search would be extremely ine\ufb03cient", "31029187-388f-4f0f-beae-d213cf74b6cd": "Many aspects of Deep Learning and neural network models draw comparisons with human intelligence. For example, a human intelligence anecdote of transfer learning is illustrated in learning music.\n\nIf two people are trying to learn how to play the guitar, and one already knows how to play the piano, it seems likely that the piano-player will learn to play the guitar faster. Analogous to learning music, a model that can classify Ima- geNet images will likely perform better on CIFAR-10 images than a model with random weights. Data Augmentation is similar to imagination or dreaming. Humans imagine differ- ent scenarios based on experience. Imagination helps us gain a better understanding of our world. Data Augmentation methods such as GANs and Neural Style Transfer can \u2018imagine\u2019 alterations to images such that they have a better understanding of them. The remainder of the paper is organized as follows: A brief \u201cBackground\u201d is provided to give readers a historical context of Data Augmentation and Deep Learning. \u201cImage Data Augmentation techniques\u201d discusses each image augmentation technique in detail along with experimental results", "f2e6ac80-7257-4179-8d82-e14ec287d8f7": "An example of greedy supervised pretraining is illustrated in figure 8.7, in which each added hidden layer is pretrained as part of a shallow supervised MLP, taking as input the output of the previously trained hidden layer. Instead of pretraining one layer at a time, Simonyan and Zisserman  pretrain a deep convolutional network (eleven weight layers) and then use the first four and last three layers from this network to initialize even deeper networks (with up to nineteen layers of weights). The middle layers of the new, very deep network are initialized randomly. The new network is then jointly trained. Another option, explored by Yu et al.\n\n, is to use the outputs of the previously trained MLPs, as well as the raw input, as inputs for each added stage. Why would greedy supervised pretraining help? The hypothesis initially discussed by Bengio ef al. is that it helps to provide better guidance to the  319  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  https://www.deeplearningbook.org/contents/optimization.html  (c) (a)  Figure 8.7: Illustration of one form of greedy supervised pretraining", "e9cdaaeb-0c6a-414b-b02e-a8bfbfa3c67c": "We could explicitly search for a large set of basis functions that are all mutually different from each other, but this often incurs a noticeable computational cost.\n\nFor example, if we have at most aS many outputs as inputs, we could use Gram-Schmidt orthogonalization on an initial weight matrix and be guaranteed that each unit would compute a very different function from each other unit. Random initialization from a high-entropy distribution over a high-dimensional space is computationally cheaper and unlikely to assign any units to compute the same function as each other. Typically, we set the biases for each unit to heuristically chosen constants, and initialize only the weights randomly. Extra parameters\u2014for example, parameters encoding the conditional variance of a prediction\u2014are usually set to heuristically chosen constants much like the biases are. We almost always initialize all the weights in the model to values drawn randomly from a Gaussian or uniform distribution. The choice of Gaussian or uniform distribution does not seem to matter much but has not been exhaustively studied. The scale of the initial distribution, however, does have a large effect on both the outcome of the optimization procedure and the ability of the network to generalize", "31603194-6345-4ae6-a101-0e76be0ef306": "# \u00a3: encoder network  # lambda: weight on the off-diagonal terms  #N: batch size  # D: dimensionality of the representation  #  # mm: matrix-matrix multiplication  # off_diagonal: off-diagonal elements of a matrix # eye: identity matrix  for x in loader: # load a batch with N samples ## two randomly augmented versions of x y_a, y_b = augment (x) # compute representations za= f(y a) # NxD zb = f(y_b) # NxD # normalize repr.\n\nalong the batch dimension z_a_norm = (za - z_a.mean(0)) / z_a.std(0) # NxD z_b_norm = (z_b - z_b.mean(0)) / z_b.std(0) # NxD  # cross-correlation matrix  c = mm(z_a_norm.T, z_b_norm) / N # DxD  # loss  c_diff = (c - eye(D)).pow(2) # DxD  # multiply off-diagonal elems of c_diff by lambda off_diagonal (c_diff) .mul_ (lambda)  loss = c_diff.sum()  # optimization step  loss", "c6973482-2237-424d-87ac-b8a24f9a66b2": "This differs from Transfer Learning because in Transfer Learning, the network architecture such as VGG-16  or ResNet  must be transferred as well as the  weights. Pretraining enables the initialization of weights using big datasets, while still  enabling flexibility in network architecture design. + One-shot and Zero-shot learning  algorithms represent another paradigm for building models with extremely limited data. One-shot learning is commonly used in facial recognition applications .\n\nAn approach to one-shot learning is the use of siamese networks  that learn a distance function such that image classification is possible even if the network has only been trained on one or a few instances. Another very popular approach to one-shot learning is the use of memory-augmented net- works . Zero-shot learning is a more extreme paradigm in which a network uses input and output vector embeddings such as Word2Vec  or GloVe  to classify  images based on descriptive attributes. In contrast to the techniques mentioned above, Data Augmentation approaches overfitting from the root of the problem, the training dataset. This is done under the assumption that more information can be extracted from the original dataset through augmentations. These augmentations artificially inflate the training dataset size by either data warping or oversampling", "0d16fd10-de99-48bf-bc63-b3c32f374f55": "This means that any individual agent has only limited ability to a\u21b5ect the reward signal because any single agent contributes just one component of the collective action evaluated by the common reward signal. E\u21b5ective learning in this scenario requires addressing a structural credit assignment problem: which team members, or groups of team members, deserve credit for a favorable reward signal, or blame for an unfavorable reward signal? It is a cooperative game, or a team problem, because the agents are united in seeking to increase the same reward signal: there are no con\ufb02icts of interest among the agents.\n\nThe scenario would be a competitive game if di\u21b5erent agents receive di\u21b5erent reward signals, where each reward signal again evaluates the collective action of the population, and the objective of each agent is to increase its own reward signal. In this case there might be con\ufb02icts of interest among the agents, meaning that actions that are good for some agents are bad for others. Even deciding what the best collective action should be is a non-trivial aspect of game theory. This competitive setting might be relevant to neuroscience too (for example, to account for heterogeneity of dopamine neuron activity), but here we focus only on the cooperative, or team, case", "dcbaed06-5257-47b6-9215-020204b89020": "end while  training procedure presented in algorithm 18.1.\n\nThe high cost of burning in the Markov chains in the inner loop makes this procedure computationally infeasible, but this procedure is the starting point that other more practical algorithms aim to approximate. We can view the MCMC approach to maximum likelihood as trying to achieve balance between two forces, one pushing up on the model distribution where the data occurs, and another pushing down on the model distribution where the model  https://www.deeplearningbook.org/contents/partition.html    samples occur. Figure 18.1 lustrates this process. \u2018Lhe two torces correspond to maximizing log p and minimizing log Z. Several approximations to the negative phase are possible. Each of these approximations can be understood as making the negative phase computationally cheaper but also making it push down in the wrong locations. Because the negative phase involves drawing samples from the model\u2019s distri- bution, we can think of it as finding points that the model believes in strongly. Because the negative phase acts to reduce the probability of those points, they are generally considered to represent the model\u2019s incorrect beliefs about the world", "db1097d0-2212-4a1f-9a2a-2cb74c4b1afd": "We can similarly introduce priors into the Section 2.1.1 It is straightforward to extend the analysis of Bernoulli mixtures to the case of multinomial binary variables having M > 2 states by making use of the discrete disExercise 9.19 tribution (2.26). Again, we can introduce Dirichlet priors over the model parameters if desired. As a third example of the application of EM, we return to the evidence approximation for Bayesian linear regression. In Section 3.5.2, we obtained the reestimation equations for the hyperparameters \u03b1 and \u03b2 by evaluation of the evidence and then setting the derivatives of the resulting expression to zero. We now turn to an alternative approach for \ufb01nding \u03b1 and \u03b2 based on the EM algorithm. Recall that our goal is to maximize the evidence function p(t|\u03b1, \u03b2) given by (3.77) with respect to \u03b1 and \u03b2", "2540df49-6f03-47be-aa16-0cef07d487c1": "\\ 12.26 (**) Show that any vector ai that satisfies (12.80) will also satisfy (12.79). Also, show that for any solution of (12.80) having eigenvalue A, we can add any multiple of an eigenvector of K having zero eigenvalue, and obtain a solution to (12.79) that also has eigenvalue A. Finally, show that such modifications do not affect the principal-component projection given by (12.82). 12.27 (* *) Show that the conventional linear PCA algorithm is recovered as a special case of kernel PCA if we choose the linear kernel function given by k(x, x') = x T x'.\n\n12.28 (* *) III!I Use the transformation property (1.27) of a probability density under a change of variable to show that any density p(y) can be obtained from a fixed density q(x) that is everywhere nonzero by making a nonlinear change of variable y = f(x) in which f(x) is a monotonic function so that 0 :::; j'(x) < 00", "32d579eb-40b9-4c91-b36b-9577837b2502": "We start by writing down the M th order term for a polynomial in D dimensions in the form The coef\ufb01cients wi1i2\u00b7\u00b7\u00b7iM comprise DM elements, but the number of independent parameters is signi\ufb01cantly fewer due to the many interchange symmetries of the factor xi1xi2 \u00b7 \u00b7 \u00b7 xiM . Begin by showing that the redundancy in the coef\ufb01cients can be removed by rewriting this M th order term in the form Note that the precise relationship between the \ufffdw coef\ufb01cients and w coef\ufb01cients need not be made explicit.\n\nUse this result to show that the number of independent parameters n(D, M), which appear at order M, satis\ufb01es the following recursion relation Next use proof by induction to show that the following result holds which can be done by \ufb01rst proving the result for D = 1 and arbitrary M by making use of the result 0! = 1, then assuming it is correct for dimension D and verifying that it is correct for dimension D + 1. Finally, use the two previous results, together with proof by induction, to show To do this, \ufb01rst show that the result is true for M = 2, and any value of D \u2a7e 1, by comparison with the result of Exercise 1.14", "41712a64-1828-40ec-86df-51578761d0dc": "This makes sense since for paraphrasing tasks, augmenting the text usually consists of paraphrases, and so can easily change whether two texts are paraphrases of each other. Single Sentence Tasks. Based on the singlesentence tasks results in Table 3, hidden space augmentations (cutoff) provides the biggest boost in performance in supervised settings, while in semi-supervised settings, sentence level augmentations (roundtrip translation) works best. We note most augmentation methods hurt performance on CoLA, a task for judging grammatical acceptability. This could be caused by the fact that most of augmentation methods try to preserve meaning and not grammatical correctness. Overall, no single augmentation works the best for every task in the supervised or semisupervised setting. However, several overall conclusions can be made: \ufb01rst, augmentation does not always improve performance, and can sometimes hurt performances, even in the semi-supervised setting. This suggests that we may need to design different augmentations for different tasks", "e51f1f5f-e7b7-4f65-9890-199a833cd13f": "Because it is independent of y(x), it represents the irreducible minimum value of the loss function. As with the classi\ufb01cation problem, we can either determine the appropriate probabilities and then use these to make optimal decisions, or we can build models that make decisions directly. Indeed, we can identify three distinct approaches to solving regression problems given, in order of decreasing complexity, by: (a) First solve the inference problem of determining the joint density p(x, t). Then normalize to \ufb01nd the conditional density p(t|x), and \ufb01nally marginalize to \ufb01nd the conditional mean given by (1.89). (b) First solve the inference problem of determining the conditional density p(t|x), and then subsequently marginalize to \ufb01nd the conditional mean given by (1.89). (c) Find a regression function y(x) directly from the training data. The relative merits of these three approaches follow the same lines as for classi\ufb01cation problems above. The squared loss is not the only possible choice of loss function for regression. Indeed, there are situations in which squared loss can lead to very poor results and where we need to develop more sophisticated approaches", "56d0e286-9a0c-4e26-ad54-5a6d7c69a658": "Inevitably, however, the joint process is brought closer to the overall goal of optimality.\n\nThe arrows in this diagram correspond to the behavior of policy iteration in that each takes the system all the way to achieving one of the two goals completely. In GPI one could also take smaller, incomplete steps toward each goal. In either case, the two processes together achieve the overall goal of optimality even though neither is attempting to achieve it directly. DP may not be practical for very large problems, but compared with other methods for solving MDPs, DP methods are actually quite e\ufb03cient. If we ignore a few technical details, then the (worst case) time DP methods take to \ufb01nd an optimal policy is polynomial in the number of states and actions. If n and k denote the number of states and actions, this means that a DP method takes a number of computational operations that is less than some polynomial function of n and k. A DP method is guaranteed to \ufb01nd an optimal policy in polynomial time even though the total number of (deterministic) policies is kn", "05af5a1e-b3ec-4a2d-af05-550db75becfb": "A. Cohn (Eds. ), Advances in Neural Information Processing Systems, Volume 11. MIT Press.\n\nJacobs, R. A., M. I. Jordan, S. J. Nowlan, and G. E. Hinton . Adaptive mixtures of local experts. Neural Computation 3(1), 79\u201387. Jaynes, E. T. Probability Theory: The Logic of Science. Cambridge University Press. Jebara, T. Machine Learning: Discriminative and Generative. Kluwer. Jeffries, H. An invariant form for the prior probability in estimation problems. Pro. Roy. Soc. AA 186, 453\u2013461. Jensen, C., A. Kong, and U. Kjaerulff . Blocking gibbs sampling in very large probabilistic expert systems. International Journal of Human Computer Studies", "743b3837-2427-4f57-9773-cd78d9f18bb5": "Hint: If c = 1, then the spike is less prominent. \u21e4 So far in this chapter we have considered methods that estimate action values and use those estimates to select actions. This is often a good approach, but it is not the only one possible. In this section we consider learning a numerical preference for each action a, which we denote Ht(a). The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward.\n\nOnly the relative preference of one action over another is important; if we add 1000 to all the action preferences there is no e\u21b5ect on the action probabilities, which are determined according to a soft-max distribution (i.e., Gibbs or Boltzmann distribution) as follows: where here we have also introduced a useful new notation, \u21e1t(a), for the probability of taking action a at time t. Initially all action preferences are the same (e.g., H1(a) = 0, for all a) so that all actions have an equal probability of being selected. There is a natural learning algorithm for soft-max action preferences based on the idea of stochastic gradient ascent", "83e28796-cbd3-4a58-afd7-754a289e8b85": "That is, at each time, the weights are updated by multiplying them with a factor that depends on the reward. 6.2. Dynamic SE as Interpolation of Algorithms. Besides the dynamic experience, other components in the SE, such as the divergence function D and the balancing weights (\u03b1, \u03b2), can also be indexed by the time \u03c4 with desired evolution. In particular, the previous sections have shown that the di\ufb00erent speci\ufb01cations of the SE components correspond to di\ufb00erent speci\ufb01c algorithms, many of which are well-known and have di\ufb00erent properties. The dynamic SE with the evolving speci\ufb01cations, therefore, can be seen as interpolating between the algorithms during the course of training.\n\nThe interpolation allows the learning to enjoy di\ufb00erent desired properties in di\ufb00erent training stages, resulting in improved e\ufb03cacy. As a concrete example, Tan et al. learn a text generation model by starting with the simple supervised MLE algorithm, with the experience function f\u03c4=0 = fdata (Equation 4.2) and balancing weights (\u03b1\u03c4=0 = 1, \u03b2\u03c4=0 = \u03f5) as described in Section 4.1.1", "880fae86-6a61-45b3-8415-169f895dff54": "Of course, in the case of the RVM we always have the option of starting with a smaller number of basis functions than N + 1.\n\nMore signi\ufb01cantly, in the relevance vector machine the parameters governing complexity and noise variance are determined automatically from a single training run, whereas in the support vector machine the parameters C and \u03f5 (or \u03bd) are generally found using cross-validation, which involves multiple training runs. Furthermore, in the next section we shall derive an alternative procedure for training the relevance vector machine that improves training speed signi\ufb01cantly. We have noted earlier that the mechanism of automatic relevance determination causes a subset of parameters to be driven to zero. We now examine in more detail set vector of target values given by t = (t1, t2)T, indicated by the cross, for a model with one basis vector \u03d5 = (\u03c6(x1), \u03c6(x2))T, which is poorly aligned with the target data vector t. On the left we see a model having only isotropic noise, so that C = \u03b2\u22121I, corresponding to \u03b1 = \u221e, with \u03b2 set to its most probable value. On the right we see the same model but with a \ufb01nite value of \u03b1", "dcada4a5-5fa1-4250-b3c7-884398529feb": "In addition to RGB versus grayscale images, there are many other ways of representing digital color such as HSV (Hue, Saturation, and Value). Jurio et al.\n\nexplore the performance of Image Segmentation on many dif- ferent color space representations from RGB to YUV, CMY, and HSV. Similar to geometric transformations, a disadvantage of color space transforma- tions is increased memory, transformation costs, and training time. Additionally, color transformations may discard important color information and thus are not always a label-preserving transformation. For example, when decreasing the pixel values of an image to simulate a darker environment, it may become impossible to see the objects in the image. Another indirect example of non-label preserving color transformations is in Image Sentiment Analysis . In this application, CNNs try to visually predict the sentiment score of an image such as: highly negative, nega- tive, neutral, positive, or highly positive. One indicator of a negative/highly negative image is the presence of blood. The dark red color of blood is a key component to distinguish blood from water or paint", "6843fa03-d4b3-4b58-9ac5-425fc75916b7": "How can they learn about the optimal policy while behaving according to an exploratory policy? The on-policy approach in the preceding section is actually a compromise\u2014it learns action values not for the optimal policy, but for a near-optimal policy that still explores. A more straightforward approach is to use two policies, one that is learned about and that becomes the optimal policy, and one that is more exploratory and is used to generate behavior. The policy being learned about is called the target policy, and the policy used to generate behavior is called the behavior policy. In this case we say that learning is from data \u201co\u21b5\u201d the target policy, and the overall process is termed o\u21b5-policy learning. Throughout the rest of this book we consider both on-policy and o\u21b5-policy methods. On-policy methods are generally simpler and are considered \ufb01rst. O\u21b5-policy methods require additional concepts and notation, and because the data is due to a di\u21b5erent policy, o\u21b5-policy methods are often of greater variance and are slower to converge. On the other hand, o\u21b5-policy methods are more powerful and general.\n\nThey include on-policy methods as the special case in which the target and behavior policies are the same", "0d90fc99-99e3-4f74-841c-40036303f835": "Today, unsupervised pretraining has been largely abandoned, except in the field of natural language processing, where the natural representation of words as one-hot vectors conveys no similarity information and where very large unlabeled sets are available.\n\nIn that case, the advantage of pretraining is that one can pretrain once on a huge unlabeled set (for example with a corpus containing billions of words), learn a good representation (typically of words, but also of sentences), and then use this representation or fine-tune it for a supervised task for which the training set contains substantially fewer examples. This approach was pioneered by Collobert and Weston , Turian et al. , and Collobert et al. and remains in common use today. Deep learning techniques based on supervised learning, regularized with dropout or batch normalization, are able to achieve human-level performance on many tasks, but only with extremely large labeled datasets. These same techniques outperform unsupervised pretraining on medium-sized datasets such as CIFAR-10 and MNIST, which have roughly 5,000 labeled examples per class", "ee0bab45-aef9-42c4-9877-929be081f415": "Specifically, on each step  con  https://www.deeplearningbook.org/contents/ml.html    of the algorithm, we can sample a minibatch of examples B= ={aY,..., 0\") drawn uniformly from the training set. The minibatch size m\u2019 is typically \"chosen to be a relatively small number of examples, ranging from one to a few hundred. Crucially, m\u2018 is usually held fixed as the training set size m grows. We may fit a training set with billions of examples using updates computed on only a hundred examples. The estimate of the gradient is formed as  1 = 4 4 g = mV OD L(2,y 6) (5.98)  149  CHAPTER 5. MACHINE LEARNING BASICS  using examples from the minibatch B.\n\nThe stochastic gradient descent algorithm then follows the estimated gradient downhill:  6+ 6-cg, (5.99)  where \u00a2 is the learning rate. Gradient descent in general has often been regarded as slow or unreliable. In the past, the application of gradient descent to nonconvex optimization problems was regarded as foolhardy or unprincipled", "d5d468d3-01cf-48f3-9453-d2da8b094c6f": "Understanding data augmentation for classification: when to warp? CORR, abs/1609.08764, 2016. Seyed-Mohsen MD, Alhussein F, Pascal F. DeepFool: a simple and accurate method to fool deep neural networks.\n\narXiv preprint. 2016. Jiawei S, Danilo VV, Sakurai K. One pixel attack for fooling deep neural networoks. arXiv preprints. 2018. Michal Z, Konrad Z, Negar R, Pedro OP. Adversarial framing for image and video classification. arXiv preprints. 2018. Logan E, Brandon T, Dimitris T, Ludwig S, Aleksander M. A rotation and a translation suffice: fooling CNNs with simple transformations. ArXiv preprint. 2018. Goodfellow |, Shlens J, Szegedy C. Explaining and Harnessing Adversarial Examples. International Conference on Learning Representations, 2015", "026b1f1d-c0dd-4d22-8859-2b3fa323dddf": "For x < 0, we have f\u2019(x) For \u00ab > 0, we have f\u2019(x) > so we can decrease f by so we can decrease f by i i moving leftward. Figure 4.1: Gradient descent. An illustration of how the gradient descent algorithm uses the derivatives of a function to follow the function downhill to a minimum. We assume the reader is already familiar with calculus but provide a brief review of how calculus concepts relate to optimization here. Suppose we have a function y = f(x), where both x and y are real numbers. The derivative of this function is denoted as f\u2019(a) or as S\u00a5.\n\nThe derivative f\u2019 (x) gives the slope of f(x) at the point x. In other words, it if how to scale a small change in the input to obtain the corresponding change in the output:  \"(a). f (a Fhe) derifatjve \u00e9s therefore useful for minimizing a function because it tells us  https://www.deeplearningbook.org/contents/numerical.html    how to change 7 in order to make a small improvement in \u00a5", "469cbe57-3b57-4424-b999-3ed9334d485e": "In the unsupervised learning context, it makes sense for some of the top-level factors to be associated with none of the output tasks (A): these are the factors that explain some of the input variations but are not relevant for predicting y\") or y\u00ae). 242  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  0.20 e\u2014 Training set lo \u2014 Validation set loss  \u00a3 S a  Loss (negative log-likelihood) oO S  \u00a3 f=) 6  0 50 100 150 200 250 Time (epochs)  Figure 7.3: Learning curves showing how the negative log-likelihood loss changes over time (indicated as number of training iterations over the dataset, or epochs). In this example, we train a maxout network on MNIST. Observe that the training objective decreases consistently over time, but the validation set average loss eventually begins to increase again, forming an asymmetric U-shaped curve. https://www.deeplearningbook.org/contents/regularization.html    validation set error begins to rise again. See figure 7.3 for an example of this behavior, which occurs reliably", "f8c23f66-ab0e-43cc-a778-fdcf0818f12e": "Our quadratic approximation of the L! regularized objective function decom- poses into a sum over the parameters:  , 1 J(w:X,y)=J(wsX,y)+ \u2014 GHia(wi-w7)\u2019 + alwil (7.22)  a  The problem of minimizing this spprodte cost function has an anata solution (for each dimension 7), with the following form:  https://www.deeplearningbook.org/contents/regularization.html    wi = sign(ui ) max ie - Fh \u201cOl. i,t (e)  Consider the situation where wy > 0 f  (7.23)  all i. There are two possible outcomes: ow Hii objective is simply w; = 0. This occurs because the contribution of J(w; X, y) to the regularized objective J(w; X,y) is overwhelmed\u2014in direction i\u2014by the L! regularization, which pushes the value of w; to zero. 1. The case where w; < Here the optimal value of w; under the regularized  2. The case where w; > +~", "11d215b2-c8eb-40c4-b9aa-58cd10908389": "The model has two major parts, transition probability function P and reward function R.  Let's say when we are in state s, we decide to take action a to arrive in the next state s' and obtain reward r. This is known as one transition step, represented by a tuple (s, a, $', r). The transition function P records the probability of transitioning from state s to s' after taking action a while obtaining reward r. We use P as a symbol of \u201cprobability\u201d.\n\nP(s',r|s,a) = P  Thus the state-transition function can be defined as a function of P(s\u2019, r|s, a):  P2, = P(s'|s,a) = P = )- P(s',r|s, a) rER  The reward function R predicts the next reward triggered by one action:  R(s,a) = E = Sor > P(s',r|s, a)  reR s'cS  Policy Policy, as the agent's behavior function 77, tells us which action to take in state s", "bc872bdb-af2a-4311-8ec2-470961548da2": "Consider a Gaussian mixture model in which the covariance matrices of the mixture components are given by \u03f5I, where \u03f5 is a variance parameter that is shared by all of the components, and I is the identity matrix, so that We now consider the EM algorithm for a mixture of K Gaussians of this form in which we treat \u03f5 as a \ufb01xed constant, instead of a parameter to be re-estimated. From (9.13) the posterior probabilities, or responsibilities, for a particular data point xn, are given by If we consider the limit \u03f5 \u2192 0, we see that in the denominator the term for which \u2225xn \u2212 \u00b5j\u22252 is smallest will go to zero most slowly, and hence the responsibilities \u03b3(znk) for the data point xn all go to zero except for term j, for which the responsibility \u03b3(znj) will go to unity", "ac63a52d-738b-43e9-8e7c-11fafd8ace5e": "P., & Hu, Z. RLPrompt: Optimizing discrete text prompts with reinforcement learning. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding.\n\nProceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4171\u20134186. Domingos, P. The master algorithm: How the quest for the ultimate learning machine will remake our world. Basic Books. Ertekin, S., Huang, J., Bottou, L., & Giles, L. Learning on the border: Active learning in imbalanced data classi\ufb01cation. Proceedings of the sixteenth ACM conference on Conference on Information and Knowledge Management, 127\u2013136. Farnia, F., & Tse, D. A convex duality framework for GANs. Advances in Neural Information Processing Systems, 31, 5248\u20135258", "da6555b9-1e54-4c83-ba70-199d6014ed24": "Noise-contrastive estimation (NCE)  involves training a generative model by learning the weights that make the model useful for discriminating data from a \ufb01xed noise distribution. Using a previously trained model as the noise distribution allows training a sequence of models of increasing quality. This can be seen as an informal competition mechanism similar in spirit to the formal competition used in the adversarial networks game. The key limitation of NCE is that its \u201cdiscriminator\u201d is de\ufb01ned by the ratio of the probability densities of the noise distribution and the model distribution, and thus requires the ability to evaluate and backpropagate through both densities. Some previous work has used the general concept of having two neural networks compete. The most relevant work is predictability minimization .\n\nIn predictability minimization, each hidden unit in a neural network is trained to be different from the output of a second network, which predicts the value of that hidden unit given the value of all of the other hidden units. This work differs from predictability minimization in three important ways: 1) in this work, the competition between the networks is the sole training criterion, and is suf\ufb01cient on its own to train the network", "1f61f6bb-1bda-4ea4-88e2-77ea34fdb498": "Adversarial attacks is an increasingly important research topic as they reveal models\u2019 vulnerabilities and \ufb02aws. This is especially true for universal attacks , where we want to generate universal examples that trick the model on all possible inputs. For instance, consider the context of entailment classi\ufb01cation. Our goal is to \ufb01nd universal humanreadable hypotheses that are going to be classi\ufb01ed as \u201centailment\u201d with as high probability as possible, regardless of the input premises. This is a more challenging setting compared to previous instancespeci\ufb01c attack  where the attack model conditions on a premise and generates an adversarial hypothesis speci\ufb01c to the premise. Setup (more in \u00a7A.2.2). We aim to attack one of the most popular MultiNLI  entailment classi\ufb01ers on HuggingFaceHub.4 The attack generation model generates adversarial text without conditioning on any inputs so that the generated attacks are universal to all premises. We compare our SQL with MLE+PG. We use all hypotheses in the MultiNLI dataset as the training data for the MLE training in MLE+PG and the offpolicy updates for our SQL", "452a7f63-fe9b-4916-a1b4-47bafed3c04d": "is maximized by MAP Bayesian inference when the prior is an isotropic Laplace distribution (equation 3.26) over w \u20ac R\u2122  log p(w = dilog Laplace(w 4; 0, *) = \u2014a||w]|1 + nlog a \u2014 nlog 2. (7.24)  From the point of view of learning via maximization with respect to w, we can ignore the log a \u2014 log 2 terms because they do not depend on w.  7.2 Norm Penalties as Constrained Optimization  Consider the cost function regularized by a parameter norm penalty: J(O;X,y) = JO; X,y) + aX(8). (7.25)  Recall from section 4.4 that we can minimize a function subject to constraints by constructing a generalized Lagrange function, consisting of the original objective function plus a set of penalties. Each penalty is a product between a coefficient, called a Karush-Kuhn\u2014Tucker (KKT) multiplier, and a function representing whether the constraint is satisfied", "c1039624-dc1d-4415-8507-9fe288c0f942": "The discount rate \u03b3 thus has no e\u21b5ect on the problem formulation. It could in fact be zero and the ranking would be unchanged. This surprising fact is proven in the box on the next page, but the basic idea can be seen via a symmetry argument. Each time step is exactly the same as every other. With discounting, every reward will appear exactly once in each position in some return. The tth reward will appear undiscounted in the t \u2212 1st return, discounted once in the t \u2212 2nd return, and discounted 999 times in the t \u2212 1000th return. The weight on the tth reward is thus 1 + \u03b3 + \u03b32 + \u03b33 + \u00b7 \u00b7 \u00b7 = 1/(1 \u2212 \u03b3). Because all states are the same, they are all weighted by this, and thus the average of the returns will be this times the average reward, or r(\u21e1)/(1 \u2212 \u03b3)", "f9e9b85e-cc2d-4e58-9f0a-41e174799fe7": "Sutton and Barto  contains the earliest recognition of the near identity between the Rescorla\u2013 Wagner model and the Least-Mean-Square (LMS), or Widrow-Ho\u21b5, learning rule . This early model was revised following Sutton\u2019s development of the TD algorithm  and was \ufb01rst presented as the TD model in Sutton and Barto  and more completely in Sutton and Barto , upon which this section is largely based. Additional exploration of the TD model and its possible neural implementation was conducted by Moore and colleagues .\n\nKlopf\u2019s  drive-reinforcement theory of classical conditioning extends the TD model to address additional experimental details, such as the S-shape of acquisition curves. In some of these publications TD is taken to mean Time Derivative instead of Temporal Di\u21b5erence. 14.2.4 Ludvig, Sutton, and Kehoe  evaluated the performance of the TD model in previously unexplored tasks involving classical conditioning and examined the in\ufb02uence of various stimulus representations, including the microstimulus representation that they introduced earlier . Earlier investigations of the in\ufb02uence of various stimulus representations and their possible neural implementations on response timing and topography in the context of the TD model are those of Moore and colleagues cited above", "d391a368-efd7-4037-9888-f91967703b35": "grading, on the layer\u2019s output into a gradient on the pre-  https://www.deeplearningbook.org/contents/mlp.html    nonlinearity activation (element-wise multiplication if f is element-wise): 9 Van J =90 f(a) Compute gradients on weights and biases (including the regularization term, where needed): Vows = 9 + dV yon Q(9) Vwwd =g REY + Vy (8) Propagate the gradients w.r.t. the next lower-level hidden layer\u2019s activations: g9\u2014 Viye-y J = WT g  end for  6.5.5 Symbol-to-Symbol Derivatives  Algebraic expressions and computational graphs both operate on symbols, or variables that do not have specific values. These algebraic and graph-based representations are called symbolic representations. When we actually use or train a neural network, we must assign specific values to these symbols. We replace a symbolic input to the network ax with a specific numeric value, such as \"", "010c9e87-fb55-43a3-9e4d-565947cc402f": "(20.15)  A similar derivation will show that the other condition of interest to us, P(v | h), is also a factorial distribution:  P(v|h) = Ile ((2v\u2014 1) @ (b+ Wh)),.\n\n(20.16) i=1  20.2.2 Training Restricted Boltzmann Machines  Because the RBM admits efficient evaluation and differentiation of P(v) and efficient MCMC sampling in the form of block Gibbs sampling, it can readily be trained with any of the techniques described in chapter 18 for training models that have intractable partition functions. This includes CD, SML (PCD), ratio matching, and so on. Compared to other undirected models used in deep learning, the RBM is relatively straightforward to train because we can compute P(h | v)  656  CHAPTER 20. DEEP GENERATIVE MODELS  exactly in closed form. Some other deep models, such as the deep Boltzmann machine, combine both the difficulty of an intractable partition function and the difficulty of intractable inference. 20.3 Deep Belief Networks  Deep belief networks (DBNs) were one of the first nonconvolutional models to successfully admit training of deep architectures", "2f9fab5e-e7b5-403e-b406-4adb954d07cb": "Thus these fixed-point update equations define an iterative algorithm where we alternate updates of A (using equation 20.33) and updates of A?) (using equation 20.34).\n\nOn small problems such as MNIST, as few as ten iterations can be sufficient to find an approximate positive phase gradient for learning, and fifty usually suffice to obtain a high-quality representation of a single specific example to be used for high-accuracy classification. Extending approximate variational inference to deeper DBMs is straightforward. 20.4.3 DBM Parameter Learning  Learning in the DBM must confront both the challenge of an intractable partition function, using the techniques from chapter 18, and the challenge of an intractable posterior distribution, using the techniques from chapter 19.  https://www.deeplearningbook.org/contents/generative_models.html    As described In section ZU.4.Z, varlational interence allows the construction ot a distribution Qh | v) v) that a apmpates the intractable Mh | v). Learning then proceeds by a atliene Liv, , the variational lower bound on the intractable log-likelihood, log P(v; @). 665  CHAPTER 20", "cbaf5194-c69e-4f13-be84-25875e1685a1": "There are thus a factorial number of complete graphs for every set of random variables. In this example, we order the variables from left to right, top to bottom. Directed models and undirected models both have their advantages and disad- vantages. Neither approach is clearly superior and universally preferred. Instead, we should choose which language to use for each task. This choice will partially depend on which probability distribution we wish to describe. We may choose to use either directed modeling or undirected modeling based on which approach can  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    capture the most dependences Im the probability distribution or which approach uses the fewest edges to describe the distribution. Other factors can affect the decision of which language to use. Even while working with a single probabil- ity distribution, we may sometimes switch between different modeling languages", "16293ffa-27df-417d-b3bf-5a544aa66c28": "Because we now have a probabilistic model, these are expressed in terms of the predictive distribution that gives the probability distribution over t, rather than simply a point estimate, and is obtained by substituting the maximum likelihood parameters into (1.60) to give Now let us take a step towards a more Bayesian approach and introduce a prior distribution over the polynomial coef\ufb01cients w. For simplicity, let us consider a Gaussian distribution of the form where \u03b1 is the precision of the distribution, and M +1 is the total number of elements in the vector w for an M th order polynomial. Variables such as \u03b1, which control the distribution of model parameters, are called hyperparameters.\n\nUsing Bayes\u2019 theorem, the posterior distribution for w is proportional to the product of the prior distribution and the likelihood function We can now determine w by \ufb01nding the most probable value of w given the data, in other words by maximizing the posterior distribution. This technique is called maximum posterior, or simply MAP", "265b5b55-d182-475b-ac02-87680c771137": "This may mean that we maximize che log-likelihood  log p(y? | #,...,@), (10.29) or, if the model includes connections from the output at one time step to the next \u2018ime step,  logp(y |w,...,a yO)... yf). (10.30)  Decomposing the joint probability over the sequence of y values as a series of one-step probabilistic predictions is one way to capture the full joint distribution across the whole sequence. When we do not feed past y values as inputs that condition the next step prediction, the directed graphical model contains no edges from any y\u00ae in the past to the current y, In this case, the outputs y are conditionally independent given the sequence of a values. When we do feed the actual y values (not their prediction, but the actual observed or generated values) back into the network, the directed graphical model contains edges from all y values in the past to the current y\u00ae value.\n\nAs a simple example, let us consider the case where the RNN models only a sequence of scalar random variables Y = fy, yO }, with no additional inputs x", "c76cbdab-9ddc-4062-bdae-a5b096c51c59": "Because the manifold in GTM is defined as a continuous surface, not just at the prototype vectors as in the SOM, it is possible to compute the magnification factors corresponding to the local expansions and compressions of the manifold needed to fit the data set  as well as the directional curvatures of the manifold .\n\nThese can be visualized along with the projected data and provide additional insight into the model. Exercises 12.1 (* *) lIB In this exercise, we use proof by induction to show that the linear projection onto an M -dimensional subspace that maximizes the variance of the projected data is defined by the M eigenvectors of the data covariance matrix S, given by (12.3), corresponding to the M largest eigenvalues. In Section 12.1, this result was proven for the case of M = 1. Now suppose the result holds for some general value of M and show that it consequently holds for dimensionality M + 1. To do this, first set the derivative of the variance of the projected data with respect to a vector UM+1 defining the new direction in data space equal to zero", "6b9de3b6-a309-4526-880b-9eb4d2ead173": "With continuous policy parameterization the action probabilities change smoothly as a function of the learned parameter, whereas in \"-greedy selection the action probabilities may change dramatically for an arbitrarily small change in the estimated action values, if that change results in a di\u21b5erent action having the maximal value. Largely because of this, stronger convergence guarantees are available for policy-gradient methods than for action-value methods. In particular, it is the continuity of the policy dependence on the parameters that enables policy-gradient methods to approximate gradient ascent (13.1). The episodic and continuing cases de\ufb01ne the performance measure, J(\u2713), di\u21b5erently and thus have to be treated separately to some extent. Nevertheless, we will try to present both cases uniformly, and we develop a notation so that the major theoretical results can be described with a single set of equations. In this section we treat the episodic case, for which we de\ufb01ne the performance measure as the value of the start state of the episode.\n\nWe can simplify the notation without losing any meaningful generality by assuming that every episode starts in some particular (non-random) state s0", "c19adbe2-3704-4edf-973b-ce7c00d710bc": "Note that Mt, like \u03b4t, is not really an additional memory variable. It can be removed from the algorithm by substituting its de\ufb01nition into the eligibility-trace equation. Pseudocode and software for the true online version of Emphatic-TD(\u03bb) are available on the web . In the on-policy case (\u21e2t = 1, for all t), Emphatic-TD(\u03bb) is similar to conventional TD(\u03bb), but still signi\ufb01cantly di\u21b5erent. In fact, whereas Emphatic-TD(\u03bb) is guaranteed to converge for all state-dependent \u03bb functions, TD(\u03bb) is not. TD(\u03bb) is guaranteed convergent only for all constant \u03bb. See Yu\u2019s counterexample . It might at \ufb01rst appear that tabular methods using eligibility traces are much more complex than one-step methods.\n\nA naive implementation would require every state (or state\u2013action pair) to update both its value estimate and its eligibility trace on every time step. This would not be a problem for implementations on single-instruction, multipledata, parallel computers or in plausible arti\ufb01cial neural network (ANN) implementations, but it is a problem for implementations on conventional serial computers", "aa9f7e23-e776-4bbf-b619-6d6c25b24e41": "A \ufb01nal issue that demands attention in future research is that of developing methods to make it acceptably safe to embed reinforcement learning agents into physical environments. This is one of the most pressing areas for future research, and we discuss it further in the When we were writing the \ufb01rst edition of this book in the mid-1990s, arti\ufb01cial intelligence was making signi\ufb01cant progress and was having an impact on society, though it was mostly still the promise of arti\ufb01cial intelligence that was inspiring developments. Machine learning was part of that outlook, but it had not yet become indispensable to arti\ufb01cial intelligence.\n\nBy today that promise has transitioned to applications that are changing the lives of millions of people, and machine learning has come into its own as a key technology. As we write this second edition, some of the most remarkable developments in arti\ufb01cial intelligence have involved reinforcement learning, most notably \u201cdeep reinforcement learning\u201d\u2014reinforcement learning with function approximation by deep arti\ufb01cial neural networks. We are at the beginning of a wave of real-world applications of arti\ufb01cial intelligence, many of which will include reinforcement learning, deep and otherwise, that will impact our lives in ways that are hard to predict. But an abundance of successful real-world applications does not mean that true arti\ufb01cial intelligence has arrived", "623477e5-00ac-4964-be48-b2b660ecacec": "For instance, we might seek a bound on a conditional distribution p(y|x), which is itself just one factor in a much larger probabilistic model speci\ufb01ed by a directed graph. The purpose of introducing the bound of course is to simplify the resulting distribution. This local approximation can be applied to multiple variables in turn until a tractable approximation is obtained, and in Section 10.6.1 we shall give a practical example of this approach in the context of logistic regression. Here we focus on developing the bounds themselves. We have already seen in our discussion of the Kullback-Leibler divergence that the convexity of the logarithm function played a key role in developing the lower bound in the global variational approach. We have de\ufb01ned a (strictly) convex function as one for which every chord lies above the function. Convexity also plays a Section 1.6.1 central role in the local variational framework", "86504662-f570-4288-aa50-e32dc8ec33c4": "Sometimes a different language becomes more appropriate if we observe a certain subset of variables, or if we wish to perform a different computational task.\n\nFor example, the directed model description often provides a straightforward approach to efficiently draw samples from the model (described in section 16.3), while the undirected model formulation is often useful for deriving approximate inference procedures (as we will see in chapter 19, where the role of undirected models is highlighted in equation 19.56). Every probability distribution can be represented by either a directed model or an undirected model. In the worst case, one can always represent any distribution by using a \u201ccomplete graph.\u201d For a directed model, the complete graph is any directed acyclic graph in which we impose some ordering on the random variables, and each variable has all other variables that precede it in the ordering as its ancestors in the graph. For an undirected model, the complete graph is simply a graph containing a single clique encompassing all the variables. See figure 16.10 for an example. 573  CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  Of course, the utility of a graphical model is that the graph implies that some variables do not interact directly", "69966ac1-ff29-4cea-9288-2960be99751c": "From (C.38), it follows that and because \u039b is a diagonal matrix, we say that the matrix A is diagonalized by the matrix U. If we left multiply by U and right multiply by UT, we obtain These last two equations can also be written in the form If we take the determinant of (C.43), and use (C.12), we obtain Similarly, taking the trace of (C.43), and using the cyclic property (C.8) of the trace operator together with UTU = I, we have We leave it as an exercise for the reader to verify (C.22) by making use of the results (C.33), (C.45), (C.46), and (C.47). A matrix A is said to be positive de\ufb01nite, denoted by A \u227b 0, if wTAw > 0 for all values of the vector w. Equivalently, a positive de\ufb01nite matrix has \u03bbi > 0 for all of its eigenvalues (as can be seen by setting w to each of the eigenvectors in turn, and by noting that an arbitrary vector can be expanded as a linear combination of the eigenvectors)", "55ae77b9-4c86-4c1e-904e-ea085e417f0b": "The most important form of this is when planning is done at decision time, that is, as part of the action-selection process. Classical heuristic search as studied in arti\ufb01cial intelligence is an example of this. Other examples are rollout algorithms and Monte Carlo Tree Search that bene\ufb01t from online, incremental, sample-based value estimation and policy improvement. This chapter concludes Part I of this book. In it we have tried to present reinforcement learning not as a collection of individual methods, but as a coherent set of ideas cutting across methods.\n\nEach idea can be viewed as a dimension along which methods vary. The set of such dimensions spans a large space of possible methods. By exploring this space at the level of dimensions we hope to obtain the broadest and most lasting understanding. In this section we use the concept of dimensions in method space to recapitulate the view of reinforcement learning developed so far in this book", "6a642fed-b5f6-4ceb-a1ca-9a6ca2fee51b": "Note that any elements of \u03c0 or A that are set to zero initially will remain zero in subsequent EM updates. A typical initialization procedure would Exercise 13.6 involve selecting random starting values for these parameters subject to the summation and non-negativity constraints. Note that no particular modi\ufb01cation to the EM results are required for the case of left-to-right models beyond choosing initial values for the elements Ajk in which the appropriate elements are set to zero, because these will remain zero throughout. To maximize Q(\u03b8, \u03b8old) with respect to \u03c6k, we notice that only the \ufb01nal term in (13.17) depends on \u03c6k, and furthermore this term has exactly the same form as the data-dependent term in the corresponding function for a standard mixture distribution for i.i.d. data, as can be seen by comparison with (9.40) for the case of a Gaussian mixture. Here the quantities \u03b3(znk) are playing the role of the responsibilities", "2901b719-ee5f-4d4b-9d41-9f4725194911": "For example, the Laplace prior parametrized in terms of the sparsity penalty coefficient \u00bb is given by  2 Xr p(hi) = Laplace(hi; 0, ~) = cen , (13.13) and the Student t prior by  1 _ (13.14)  Phi) & \u2014aaae (1+ +) 2  Training sparse coding with maximum likelihood is intractable. Instead, the training alternates between encoding the data and training the decoder to better  492  CHAPTER 13. LINEAR FACTOR MODELS  reconstruct the data given the encoding. This approach will be justified further as a principled approximation to maximum likelihood later, in section 19.3. For models such as PCA, we have seen the use of a parametric encoder function that predicts h and consists only of multiplication by a weight matrix. The encoder that we use with sparse coding is not a parametric encoder", "159a8471-e037-4fdb-a556-385d51c40bd0": "Gibbs  proposes a method for constructing a Gaussian distribution that is conjectured to be a bound (although no rigorous proof is given), which may be used to apply local variational methods to multiclass problems. We shall see an example of the use of local variational bounds in Sections 10.6.1. For the moment, however, it is instructive to consider in general terms how these bounds can be used. Suppose we wish to evaluate an integral of the form where \u03c3(a) is the logistic sigmoid, and p(a) is a Gaussian probability density. Such integrals arise in Bayesian models when, for instance, we wish to evaluate the predictive distribution, in which case p(a) represents a posterior parameter distribution.\n\nBecause the integral is intractable, we employ the variational bound (10.144), which we write in the form \u03c3(a) \u2a7e f(a, \u03be) where \u03be is a variational parameter. The integral now becomes the product of two exponential-quadratic functions and so can be integrated analytically to give a bound on I We now have the freedom to choose the variational parameter \u03be, which we do by \ufb01nding the value \u03be\u22c6 that maximizes the function F(\u03be)", "c4937b90-9e93-4702-8170-b0b5b84fe5ba": "The policy gradient theorem for the episodic case establishes that where the gradients are column vectors of partial derivatives with respect to the components of \u2713, and \u21e1 denotes the policy corresponding to parameter vector \u2713. The symbol / here means \u201cproportional to\u201d. In the episodic case, the constant of proportionality is the average length of an episode, and in the continuing case it is 1, so that the relationship is actually an equality. The distribution \u00b5 here (as in Chapters 9 and 10) is the on-policy distribution under \u21e1 (see page 199).\n\nThe policy gradient theorem is proved for the episodic case in the box on the previous page. We are now ready to derive our \ufb01rst policy-gradient learning algorithm. Recall our overall strategy of stochastic gradient ascent (13.1), which requires a way to obtain samples such that the expectation of the sample gradient is proportional to the actual gradient of the performance measure as a function of the parameter. The sample gradients need only be proportional to the gradient because any constant of proportionality can be absorbed into the step size \u21b5, which is otherwise arbitrary", "baa3acca-9f58-4913-b88f-74ceefca17eb": "(20.75) 691  CHAPTER 20.\n\nDEEP GENERATIVE MODELS  Both approaches define a distribution p,(a) and allow us to train various criteria of pg using the reparametrization trick of section 20.9. The two different approaches to formulating generator nets\u2014emitting the parameters of a conditional distribution versus directly emitting samples\u2014have complementary strengths and weaknesses. When the generator net defines a conditional distribution over a, it is capable of generating discrete data as well as continuous data. When the generator net provides samples directly, it is capable of generating only continuous data (we could introduce discretization in the forward propagation, but doing so would mean the model could no longer be trained using back-propagation). The advantage to direct sampling is that we are no longer forced to use conditional distributions whose form can be easily written down and algebraically manipulated by a human designer. Approaches based on differentiable generator networks are motivated by the success of gradient descent applied to differentiable feedforward networks for classification. In the context of supervised learning, deep feedforward networks trained with gradient-based learning seem practically guaranteed to succeed given enough hidden units and enough training data", "29e544ed-55b1-4aa3-b035-f8b5740fdb75": "However, a deep neural network trained on probabilistic training labels from Snorkel correctly identi\ufb01es it as a true mention. Snorkel provides connectors for popular machine learning libraries such as TensorFlow , allowing users to exploit commodity models like deep neural networks that do not require hand-engineering of features and have robust predictive performance across a wide range of tasks.\n\nWe study the fundamental question of when\u2014and at what level of complexity\u2014we should expect Snorkel\u2019s generative model to yield the greatest predictive performance gains. Understanding these performance regimes can help guide users and introduces a trade-off space between predictive performance and speed. We characterize this space in two parts: \ufb01rst, by analyzing when the generative model can be approximated by an unweighted majority vote, and second, by automatically selecting the complexity of the correlation structure to model. We then introduce a two-stage, rule-based optimizer to support fast development cycles. The natural \ufb01rst question when studying systems for weak supervision is, \u201cWhen does modeling the accuracies of sources improve end-to-end predictive performance?\u201d We study that question in this subsection and propose a heuristic to identify settings in which this modeling step is most bene\ufb01cial", "3dce083d-613f-4bc5-816c-0e63128275ce": "The idea that stimuli produce after e\u21b5ects in the nervous system that are important for learning is very old (see Chapter 14). Some of the earliest uses of eligibility traces were in the actor\u2013critic methods discussed in Chapter 13 . The \u03bb-return and its error-reduction properties were introduced by Watkins  and further developed by Jaakkola, Jordan, and Singh .\n\nThe random walk results in this and subsequent sections are new to this text, as are the terms \u201cforward view\u201d and \u201cbackward view.\u201d The notion of a \u03bb-return algorithm was introduced in the \ufb01rst edition of this text. The more re\ufb01ned treatment presented here was developed in conjunction with Harm van Seijen . 12.2 TD(\u03bb) with accumulating traces was introduced by Sutton . Convergence in the mean was proved by Dayan , and with probability 1 by many researchers, including Peng , Dayan and Sejnowski , Tsitsiklis , and Gurvits, Lin, and Hanson . The bound on the error of the asymptotic \u03bb-dependent solution of linear TD(\u03bb) is due to Tsitsiklis and Van Roy", "124193b9-1fb6-4caf-919c-146f53a519ae": "Instead, the MP-DBM treats it as a latent variable to be inferred. One could imagine applying dropout to the MP-DBM by additionally removing some units  https://www.deeplearningbook.org/contents/generative_models.html   rather than making them latent. 20.5 Boltzmann Machines for Real-Valued Data  While Boltzmann machines were originally developed for use with binary data, many applications such as image and audio modeling seem to require the ability to represent probability distributions over real values. In some cases, it is possible to treat real-valued data in the interval  as representing the expectation of a binary variable. For example, Hinton  treats grayscale images in the training set as defining  probability values. Each pixel defines the probability of a binary value being 1, and the binary pixels are all sampled independently from each other. This is a common procedure for evaluating binary models on grayscale image datasets. Nonetheless, it is not a particularly theoretically satisfying approach, and binary images sampled independently in this way have a noisy appearance", "7ddd2e3f-70f1-4f2a-835d-19658eebf78e": "Each component i of the eligibility-trace vector zt increments or decrements according Here \u03bb is the usual eligibility trace decay parameter.\n\nNote that if \u03b3 = 0, the TD model reduces to the Rescorla\u2013Wagner model with the exceptions that: the meaning of t is di\u21b5erent in each case (a trial number for the Rescorla\u2013Wagner model and a time step for the TD model), and in the TD model there is a one-time-step lead in the prediction target R. The TD model is equivalent to the backward view of the semi-gradient TD(\u03bb) algorithm with linear function approximation (Chapter 12), except that Rt in the model does not have to be a reward signal as it does when the TD algorithm is used to learn a value function for policy-improvement. Real-time conditioning models like the TD model are interesting primarily because they make predictions for a wide range of situations that cannot be represented by trial-level models. These situations involve the timing and durations of conditionable stimuli, the timing of these stimuli in relation to the timing of the US, and the timing and shapes of CRs", "4999c522-39ca-4d8d-86c6-5e41ec5823b4": "Association for Computational Linguistics. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586. O\ufb01r Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. 2018.\n\nTrust-PCL: An off-policy trust region method for continuous control. In International Conference on Learning Representations. Karthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. 2015. Language understanding for text-based games using deep reinforcement learning. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1\u201311. Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2020. Adversarial nli: A new benchmark for natural language understanding", "ed032e76-97fd-45d1-8c84-3e6479d23eac": "Barto and Anandan  introduced a stochastic version of Widrow et al.\u2019s  selective bootstrap algorithm called the associative reward-penalty (AR\u2212P ) algorithm.\n\nBarto  and Barto and Jordan  described multi-layer ANNs consisting of AR\u2212P units trained with a globally-broadcast reinforcement signal to learn classi\ufb01cation rules that are not linearly separable. Barto  discussed this approach to ANNs and how this type of learning rule is related to others in the literature at that time. (See Section 15.10 for additional discussion of this approach to training multi-layer ANNs.) Anderson  evaluated numerous methods for training multilayer ANNs and showed that an actor\u2013critic algorithm in which both the actor and critic were implemented by two-layer ANNs trained by error backpropagation outperformed single-layer ANNs in the pole-balancing and tower of Hanoi tasks. Williams  described several ways that backpropagation and reinforcement learning can be combined for training ANNs. Gullapalli  and Williams  devised reinforcement learning algorithms for neuron-like units having continuous, rather than binary, outputs", "b13dc287-ae95-4456-8999-1def11443995": "gave a different and very simple interpretation of boosting in terms of the sequential minimization of an exponential error function. Consider the exponential error function de\ufb01ned by where fm(x) is a classi\ufb01er de\ufb01ned in terms of a linear combination of base classi\ufb01ers yl(x) of the form and tn \u2208 {\u22121, 1} are the training set target values. Our goal is to minimize E with respect to both the weighting coef\ufb01cients \u03b1l and the parameters of the base classi\ufb01ers yl(x). other of the axes. Each \ufb01gure shows the number m of base learners trained so far, along with the decision boundary of the most recent base learner (dashed black line) and the combined decision boundary of the ensemble (solid green line). Each data point is depicted by a circle whose radius indicates the weight assigned to that data point when training the most recently added base learner. Thus, for instance, we see that points that are misclassi\ufb01ed by the m = 1 base learner are given greater weight when training the m = 2 base learner. Instead of doing a global error function minimization, however, we shall suppose that the base classi\ufb01ers y1(x),", "9255ff15-0cd3-49d7-89a2-b392c41c1742": "An extension of least-squares methods to control was introduced by Lagoudakis and Parr . 9.9 Our discussion of memory-based function approximation is largely based on the review of locally weighted learning by Atkeson, Moore, and Schaal . Atkeson  discussed the use of locally weighted regression in memory-based robot learning and supplied an extensive bibliography covering the history of the idea. Stan\ufb01ll and Waltz  in\ufb02uentially argued for the importance of memory based methods in arti\ufb01cial intelligence, especially in light of parallel architectures then becoming available, such as the Connection Machine. Baird and Klopf  introduced a novel memory-based approach and used it as the function approximation method for Q-learning applied to the pole-balancing task. Schaal and Atkeson  applied locally weighted regression to a robot juggling control problem, where it was used to learn a system model.\n\nPeng  used the pole-balancing task to experiment with several nearest-neighbor methods for approximating value functions, policies, and environment models. Tadepalli and Ok  obtained promising results with locally-weighted linear regression to learn a value function for a simulated automatic guided vehicle task", "eb79942e-12f1-41a7-ac5a-92fa33b40b55": "571-82  Buda Mateusz, Maki Atsuto, Mazurowski Maciej A. A systematic study of the class imbalance problem in convolu- tional neural networks. Neural Networks. 2018;106:249-59. Drown DJ, Khoshgoftaar TM, Seliya N. Evolutionary sampling and software quality modeling of high-assurance systems. IEEE Trans Syst. 2009;39(5):1097-107,  Jason Y, Jeff C, Anh N, Thomas F, Hod L. Understanding neural networks through deep visualization. In: European conference on computer vision (ECCV). Berlin: Springer; 2015. p. 818-33. Xiaofeng Z, Zhangyang W, Dong L, Qing L. DADA: deep adversarial data augmentation for extremely low data regime classification. arXiv preprint", "60e530c0-9614-49eb-b243-a53f97cc82f5": "In the experiment\u2019s third phase, the second stimulus alone\u2014the light\u2014is presented to the rabbit to see if the rabbit has learned to respond to it with a CR.\n\nIt turns out that the rabbit produces very few, or no, CRs in response to the light: learning to the light had been blocked by the previous learning to the tone.2 Blocking results like this challenged the idea that conditioning depends only on simple temporal contiguity, that is, that a necessary and su\ufb03cient condition for conditioning is that a US frequently follows a CS closely in time. In the next section we describe the Rescorla\u2013Wagner model  that o\u21b5ered an in\ufb02uential explanation for blocking. Higher-order conditioning occurs when a previously-conditioned CS acts as a US in conditioning another initially neutral stimulus. Pavlov described an experiment in which his assistant \ufb01rst conditioned a dog to salivate to the sound of a metronome that predicted a food US, as described above. After this stage of conditioning, a number of trials were conducted in which a black square, to which the dog was initially indi\u21b5erent, was placed in the dog\u2019s line of vision followed by the sound of the metronome\u2014and this was not followed by food", "84e85274-4a6d-43aa-a408-7d40cdfcf8f6": "This is called the problem of cold- start recommendations. A general way of solving the cold-start recommendation problem is to introduce extra information about the individual users and items. For example, this extra information could be user profile information or features of each item. Systems that use such information are called content-based recommender systems. The mapping from a rich set of user features or item features to an embedding can be learned through a deep learning architecture . Specialized deep learning architectures, such as convolutional networks, have also been applied to learn to extract features from rich content, such as from musical audio tracks for music recommendation . In that work, the convolutional net takes acoustic features as input and computes an embedding for the associated song. The dot product between this song embedding and the embedding for a user is then used to predict whether a user will listen to the song.\n\n475  https://www.deeplearningbook.org/contents/applications.html    CHAPTER 12. APPLICATIONS  12.5.1.1 Exploration versus Exploitation  When making recommendations to users, an issue arises that goes beyond ordinary supervised learning and into the realm of reinforcement learning", "d549c57c-a684-4e64-9300-4cf76ad50ef8": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of The Thirteenth International Conference on Arti\ufb01cial Intelligence and Statistics (AISTATS\u201910). Hinton, G., Deng, L., Dahl, G. E., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T., and Kingsbury, B. .\n\nDeep neural networks for acoustic modeling in speech recognition. IEEE Signal Processing Magazine, 29(6), 82\u201397. Hinton, G. E., Dayan, P., Frey, B. J., and Neal, R. M. The wake-sleep algorithm for unsupervised neural networks. Science, 268, 1558\u20131161. Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. Improving neural networks by preventing co-adaptation of feature detectors", "aa6640d3-43e4-4601-936d-fbaf7b410ee8": "Although not every dopamine neuron monitored in the experiments of Schultz and 2In the literature relating TD errors to the activity of dopamine neurons, their \u03b4t is the same as our colleagues behaved in all of these ways, the striking correspondence between the activities of most of the monitored neurons and TD errors lends strong support to the reward prediction error hypothesis. There are situations, however, in which predictions based on the hypothesis do not match what is observed in experiments.\n\nThe choice of input representation is critical to how closely TD errors match some of the details of dopamine neuron activity, particularly details about the timing of dopamine neuron responses. Di\u21b5erent ideas, some of which we discuss below, have been proposed about input representations and other features of TD learning to make the TD errors \ufb01t the data better, though the main parallels appear with the CSC representation that Montague et al. used. Overall, the reward prediction error hypothesis has received wide acceptance among neuroscientists studying reward-based learning, and it has proven to be remarkably resilient in the face of accumulating results from neuroscience experiments", "5909bcec-0c2b-4e80-8eec-efb46f5353e9": "634  CHAPTER 19.\n\nAPPROXIMATE INFERENCE  MAP inference is commonly used in deep learning as both a feature extractor and a learning mechanism. It is primarily used for sparse coding models. Recall from section 13.4 that sparse coding is a linear factor model that imposes a sparsity-inducing prior on its hidden units. A common choice is a factorial Laplace prior, with  p(hi) = AeA (19.14)  The visible units are then generated by performing a linear transformation and adding noise:  p(a | h) =N(v;Wh+ b, 61D). (19.15)  Computing or even representing p(h | v) is difficult. Every pair of variables h; and h; are both parents of v. This means that when v is observed, the graphical model contains an active path connecting h; and hj. All the hidden units thus participate in one massive clique in p(h | v)", "cf5c0669-4972-438e-8e58-7a8f2798adc5": "In reality, the scenario could be a bot playing a game to achieve high scores, or a robot trying to complete physical tasks with physical items; and not just limited to these.\n\nhttps://lilianweng.github.io/posts/2018-02-19-rl-overview/  A (Long) Peek into Reinforcement Learning | Lil'Log   AGENT ENVIRONMENT -State s ES  - Take action a \u20ac A  & \u00a9  - Get reward 7 -Newstate s\u2019 \u20ac S  The goal of Reinforcement Learning (RL) is to learn a good strategy for the agent from experimental trials and relative simple feedback received. With the optimal strategy, the agent is capable to actively adapt to the environment to maximize future rewards. Key Concepts  Now Let's formally define a set of key concepts in RL. The agent is acting in an environment. How the environment reacts to certain actions is defined by a model which we may or may not know. The agent can stay in one of many states (s \u20ac S) of the environment, and choose to take one of many actions (a \u20ac A) to switch from one state to another. Which state the agent will arrive in is decided by transition probabilities between states (P)", "5a493a35-071f-4468-85ee-58dcb6604e2d": "As an example, suppose that the probability density within each class is chosen to be Gaussian.\n\nIn this case, the naive Bayes assumption then implies that the covariance matrix for each Gaussian is diagonal, and the contours of constant density within each class will be axis-aligned ellipsoids. The marginal density, however, is given by a superposition of diagonal Gaussians (with weighting coef\ufb01cients given by the class priors) and so will no longer factorize with respect to its components. The naive Bayes assumption is helpful when the dimensionality D of the input space is high, making density estimation in the full D-dimensional space more challenging. It is also useful if the input vector contains both discrete and continuous variables, since each can be represented separately using appropriate models (e.g., Bernoulli distributions for binary observations or Gaussians for real-valued variables). The conditional independence assumption of this model is clearly a strong one that may lead to rather poor representations of the class-conditional densities", "394bd4dd-e87f-4537-a2a3-e8ebd59fef9b": "Greedy function approximation: a gradient boosting machine. Annals of Statistics 29(5), 1189\u20131232. Friedman, J. H., T. Hastie, and R. Tibshirani . Additive logistic regression: a statistical view of boosting. Annals of Statistics 28, 337\u2013407. Friedman, N. and D. Koller . Being Bayesian about network structure: A Bayesian approach to structure discovery in Bayesian networks. Machine Learning 50, 95\u2013126. Fukunaga, K. Introduction to Statistical Pattern Recognition (Second ed.). Academic Press. Funahashi, K. On the approximate realization of continuous mappings by neural networks. Neural Networks 2(3), 183\u2013192. Fung, R. and K. C. Chang . Weighting and integrating evidence for stochastic simulation in Bayesian networks. In P. P. Bonissone, M", "c3eb9990-231b-4243-8c3b-5677d8bb4ac2": "9.4 (\u22c6) Suppose we wish to use the EM algorithm to maximize the posterior distribution over parameters p(\u03b8|X) for a model containing latent variables, where X is the observed data set. Show that the E step remains the same as in the maximum likelihood case, whereas in the M step the quantity to be maximized is given by Q(\u03b8, \u03b8old) + ln p(\u03b8) where Q(\u03b8, \u03b8old) is de\ufb01ned by (9.30). 9.5 (\u22c6) Consider the directed graph for a Gaussian mixture model shown in Figure 9.6.\n\nBy making use of the d-separation criterion discussed in Section 8.2, show that the posterior distribution of the latent variables factorizes with respect to the different data points so that 9.6 (\u22c6 \u22c6) Consider a special case of a Gaussian mixture model in which the covariance matrices \u03a3k of the components are all constrained to have a common value \u03a3. Derive the EM equations for maximizing the likelihood function under such a model", "ebb995b9-fd79-495f-84e4-712e937fef87": "\u02d9Ipek et al. found that the number of legal commands for any state was rarely greater than this, and that performance loss was negligible if enough time was not always available to consider all legal commands. These and other clever design details made it feasible to implement the complete controller and learning algorithm on a multi-processor chip. \u02d9Ipek et al. evaluated their learning controller in simulation by comparing it with three other controllers: 1) the FR-FCFS controller mentioned above that produces the best on-average performance, 2) a conventional controller that processes each request in order, and 3) an unrealizable ideal controller, called the Optimistic controller, able to sustain 100% DRAM throughput if given enough demand by ignoring all timing and resource constraints, but otherwise modeling DRAM latency (as row bu\u21b5er hits) and bandwidth.\n\nThey simulated nine memory-intensive parallel workloads consisting of scienti\ufb01c and data-mining applications. Figure 16.4 shows the performance (the inverse of execution time normalized to the performance of FR-FCFS) of each controller for the nine applications, together with the geometric mean of their performances over the applications", "07745eb2-84b7-4a3c-a6d6-3bcb8e7094df": "CONVOLUTIONAL NETWORKS  QO @QQ \u00a9 oO) OOOH  https://www.deeplearningbook.org/contents/convnets.html    figure 9.3: Sparse connectivity, viewed from above. We highlight one output unit,s?, and highlight the input units in & that affect this unit. These units are known as thereceptive field of s3. (Top)When s is formed by convolution with a kernel of width 3, only three  inputs affect s3. (Bottom)When s is formed by matrix multiplication, connectivity is no longer sparse, so all the inputs affect s3.\n\nQQ @ GQ \u00a9)  SeoC%  Figure 9.4: The receptive field of the units in the deeper layers of a convolutional network is larger than the receptive field of the units in the shallow layers. This effect increases if the network includes architectural features like strided convolution (figure 9.12) or pooling (section 9.3). This means that even though direct connections in a convolutional net are very sparse, units in the deeper layers can be indirectly connected to all or most of the input image. CHAPTER 9", "a7abed37-65b2-480b-a074-ecaef43f965c": "This has been implemented in Tensor\ufb02ow as \u201cslim.preprocessing.inception_preprocessing.distorted_bounding_box_crop\u201d, or in Pytorch as \u201ctorchvision.transforms.RandomResizedCrop\u201d.\n\nAdditionally, the random crop (with resize) is always followed by a random horizontal/left-to-right \ufb02ip with 50% probability. This is helpful but not essential. By removing this from our default augmentation policy, the top-1 linear evaluation drops from 64.5% to 63.4% for our ResNet-50 model trained in 100 epochs. Color distortion Color distortion is composed by color jittering and color dropping. We \ufb01nd stronger color jittering usually helps, so we set a strength parameter. A pseudo-code for color distortion using TensorFlow is as follows. import tensorflow as tf def color_distortion(image, s=1.0): # image is a tensor with value range in . # s is the strength of color distortion. def color_jitter(x): # one can also shuffle the order of following augmentations # each time they are applied", "b368b307-3930-421d-b31d-ee3b0aec0f4d": "For example, if we train a generative model of images of cars and motorcycles, it will need to know about wheels, and about how many wheels should be in an image. If we are fortunate, he representation of the wheels will take on a form that is easy for the supervised earner to access. This is not yet understood at a mathematical, theoretical level, so it is not always possible to predict which tasks will benefit from unsupervised learning in this way.\n\nMany aspects of this approach are highly dependent on he specific models used. For example, if we wish to add a linear classifier on  529  CHAPTER 15. REPRESENTATION LEARNING  top of pretrained features, the features must make the underlying classes linearly separable. These properties often occur naturally but do not always do so. This is another reason that simultaneous supervised and unsupervised learning can be preferable\u2014the constraints imposed by the output layer are naturally included from the start. From the point of view of unsupervised pretraining as learning a representation, we can expect unsupervised pretraining to be more effective when the initial representation is poor. One key example of this is the use of word embeddings", "c43ef083-5c23-49ec-a1fa-7581ddeb3698": "The empirical mean return for state s is:  Dir 1S: = s]Ge  V(s) = \u2018) Ver US = 5]  https://lilianweng.github.io/posts/2018-02-19-rl-overview/  A (Long) Peek into Reinforcement Learning | Lil'Log   where 1 is a binary indicator function. We may count the visit of state s every time so that there could exist multiple visits of one state in one episode (\u201cevery-visit\"), or only count it the first time we encounter a state in one episode (\u201cfirst-visit\"). This way of approximation can be easily extended to action-value functions by counting (s, a) pair. ran 1  To learn the optimal policy by MC, we iterate it by following a similar idea to GPI. evaluation Q~ ad. 7 Q) z ~\u00bb greedy(Q improvement  Improve the policy greedily with respect to the current value function:  m(s) = arg max,cy4 Q(s, a)", "a07d0c38-36cd-4bdc-af50-ef6657d96d8f": "This problem could have been resolved by spending weeks improving the accuracy of the address number detection system responsible for determining the cropping regions. Instead, the team made a much more practical decision, to simply expand the width of the crop region to be systematically wider than the address number detection system predicted. This single change added ten percentage points to the transcription system\u2019s coverage. 436  CHAPTER 11. PRACTICAL METHODOLOGY  Finally, the last few percentage points of performance came from adjusting hyperparameters. This mostly consisted of making the model larger while main- taining some restrictions on its computational cost. Because train and test error remained roughly equal, it was always clear that any performance deficits were due to underfitting, as well as to a few remaining problems with the dataset itself. Overall, the transcription project was a great success and allowed hundreds of millions of addresses to be transcribed both faster and at lower cost than would have been possible via human effort.\n\nWe hope that the design principles described in this chapter will lead to many  https://www.deeplearningbook.org/contents/guidelines.html    other similar successes. 437  https://www.deeplearningbook.org/contents/guidelines.html", "09e613a0-8e52-4fbe-9b15-a582292074d3": "The result of the \ufb01rst summation over zN can be stored and used once the value of xN+1 is observed in order to run the \u03b1 recursion forward to the next step in order to predict the subsequent value xN+2. Note that in (13.44), the in\ufb02uence of all data from x1 to xN is summarized in the K values of \u03b1(zN). Thus the predictive distribution can be carried forward inde\ufb01nitely using a \ufb01xed amount of storage, as may be required for real-time applications.\n\nHere we have discussed the estimation of the parameters of an HMM using maximum likelihood. This framework is easily extended to regularized maximum likelihood by introducing priors over the model parameters \u03c0, A and \u03c6 whose values are then estimated by maximizing their posterior probability. This can again be done using the EM algorithm in which the E step is the same as discussed above, and the M step involves adding the log of the prior distribution p(\u03b8) to the function Q(\u03b8, \u03b8old) before maximization and represents a straightforward application of the techniques developed at various points in this book. Furthermore, we can use variational methods to give a fully Bayesian treatment of the HMM in which we marginalize over the Section 10.1 parameter distributions", "87bd5600-4507-4e53-9127-503d8f083362": "sone<! into decreasing order. is shown in Figure 12.4{ai. The di'tortion measure J aSSQCiated wilh choo<ing a particular value of M is gi. 'en by tho sum of the eig.n\",lues from M + I up to 0 and is ptO!ted for different ,'aluo< of .\\1 in Figure 12,4(b). If \"'e <utlslitut. (12, 12) and (12.13) into (12.10). we can write the I'CA appro~\u00ad imation to a data \"eel'\" x~ i\" the fonn FIIIUre 12,4 (a) PIol at !he eJoI;nv.loo . \".,etrum lor the off\u00b71ine digits data set (b) P10t 01 !he sum at the <:liscarded", "2ed8ff2f-aa36-47ab-b8a3-030683c31648": "Exercise 8.18 If there are nodes in a directed graph that have more than one parent, but there is still only one path (ignoring the direction of the arrows) between any two nodes, then the graph is a called a polytree, as illustrated in Figure 8.39(c). Such a graph will have more than one node with the property of having no parents, and furthermore, the corresponding moralized undirected graph will have loops.\n\nThe sum-product algorithm that we derive in the next section is applicable to undirected and directed trees and to polytrees. It can be cast in a particularly simple and general form if we \ufb01rst introduce a new graphical construction called a factor graph . Both directed and undirected graphs allow a global function of several variables to be expressed as a product of factors over subsets of those variables. Factor graphs make this decomposition explicit by introducing additional nodes for the factors themselves in addition to the nodes representing the variables. They also allow us to be more explicit about the details of the factorization, as we shall see", "4342978c-15ff-4625-91d2-e9b8406903b4": "While score matching can be used o pretrain the first hidden layer of a larger model, it has not been applied as a pretraining strategy for the deeper layers of a larger model. This is probably because the hidden layers of such models usually contain some discrete variables. While score matching does not explicitly have a negative phase, it can be viewed as a version of contrastive divergence using a specific kind of Markov chain Hyvarinen, 2007a). The Markov chain in this case is not Gibbs sampling, but rather a different approach that makes local moves guided by the gradient. Score matching is equivalent to CD with this type of Markov chain when the size of the ocal moves approaches zero. Lyu  generalized score matching to the discrete case (but made an error in he derivation that was corrected by Marlin ef al. ). Marlin et al. found hat generalized score matching (GSM) does not work in high-dimensional discrete spaces where the observed probability of many events is 0.\n\nA more successful approach to extending the basic ideas of score matching o discrete data is ratio matching . Ratio matching applies specifically to binary data", "bfd2c7a5-19d6-4b2b-b38b-a9fa2f565c03": "Moreover, the time step index need not literally refer to the passage of time in the real world. Sometimes it refers only to the position in the sequence. RNNs may also be applied in two dimensions across spatial data such as images, and even when applied to data involving time, the network may have connections that go backward in time, provided that the entire sequence is observed before it is provided to the network. This chapter extends the idea of a computational graph to include cycles. These cycles represent the influence of the present value of a variable on its own value at a future time step. Such computational graphs allow us to define recurrent neural networks. We then describe many different ways to construct, train, and use recurrent neural networks.\n\nFor more information on recurrent neural networks than is available in this  1 1 c wd 1 sad 1 ad 1 on faAn4 AN  https://www.deeplearningbook.org/contents/rnn.html    Chlapter, we reler tle reader LO LIE LEXLDOOK OL Graves (4U14). 368  CHAPTER 10", "af4aa45b-809d-49b8-b3d9-880320126aee": "However, the dependence becomes weaker for longer-delayed rewards, falling by \u03b3\u03bb for each step of delay. A natural approximation, then, would be to truncate the sequence after some number of steps. Our existing notion of n-step returns provides a natural way to do this in which the missing rewards are replaced with estimated values. In general, we de\ufb01ne the truncated \u03bb-return for time t, given data only up to some If you compare this equation with the \u03bb-return (12.3), it is clear that the horizon h is playing the same role as was previously played by T, the time of termination. Whereas in the \u03bb-return there is a residual weight given to the conventional return Gt, here it is given to the longest available n-step return, Gt:h (Figure 12.2). The truncated \u03bb-return immediately gives rise to a family of n-step \u03bb-return algorithms similar to the n-step methods of Chapter 7", "fc79a883-1784-401d-9667-091827c9380a": "Another is to clip the norm |\\g|| of the gradient g  just before the parameter update:  if ||g|| >v (10.48) (10.49)  where v is the norm threshold and g is used to update parameters.\n\nBecause the gradient of all the parameters (including different groups of parameters, such as weights and biases) is renormalized jointly with a single scaling factor, the latter wn nth nw dD Lan tlw 6 Aen nn AL ee nn bn tw LL AA AL nde 8 nt Sew dL we AS ne  https://www.deeplearningbook.org/contents/rnn.html    IUCLUUU Lad LUC AUVaALLLAXS UL BUALALILCCLIL LIAL CACLL SLEP 1S SUL Ll LLC RIAUIELIL direction, but experiments suggest that both forms work similarly. \u2018Although the parameter update has the same direction as the true gradient, with gradient norm clipping, the parameter update vector norm is now bounded. This bounded  gradient avoids performing a detrimental step when the gradient explodes", "0cf47c25-b27a-4edb-b338-c08cb3148213": "The problem with the table-based approach is that we are explicitly modeling every possible kind of interaction between every possible subset of variables. The probability distributions we encounter in real tasks are much simpler than this. TT u 4 s 4d ral 1 a Porat a1  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html       UsuUaLLy, MOSL VarlaDies WiUeLCce each OLUeL OLY MaIreculy.\n\nFor example, consider modeling the finishing times of a team in a relay race. Suppose the team consists of three runners: Alice, Bob and Carol. At the start of the race, Alice carries a baton and begins running around a track. After completing  her lap around the track, she hands the baton to Bob. Bob then runs his own lap and hands the baton to Carol, who runs the final lap. We can model each of their finishing times as a continuous random variable. Alice\u2019s finishing time does not depend on anyone else\u2019s, since she goes first", "ddfb8149-c4b2-41fd-9c6d-73601b31f247": "Typ) 2 \u2014 = 6~N(u,02) F(9)  L o2  We can rewrite this formula in terms of a \u201cmean\" parameter 6 (different from the @ above; this 0 is the base gene for further mutation), \u20ac ~ N(0, I) and therefore 6 + ea ~ N(0, 07).\n\n\u20ac controls how  https://lilianweng.github.io/posts/2018-02-19-rl-overview/  A (Long) Peek into Reinforcement Learning | Lil'Log   much Gaussian noises should be added to create mutation:  x 1_ VoE.w(o,) F(9 + o\u20ac) = oe N (01)   Algorithm 2 Parallelized Evolution Strategies 1: Input: Learning rate a, noise standard deviation o, initial policy parameters 69 2: Initialize: n workers with known random seeds, and initial parameters 69 3: fort = 0,1,2,..", "44bfe43b-f9ec-4053-b008-33421a3e3119": "This leaves the following contribution to the derivative with respect to a component \u03b8j of \u03b8 Combining (6.91), (6.92), and (6.94), we can evaluate the gradient of the log likelihood function, which can be used with standard nonlinear optimization algorithms in order to determine a value for \u03b8. We can illustrate the application of the Laplace approximation for Gaussian processes using the synthetic two-class data set shown in Figure 6.12. Extension of the Appendix A Laplace approximation to Gaussian processes involving K > 2 classes, using the softmax activation function, is straightforward . with the optimal decision boundary from the true distribution in green, and the decision boundary from the Gaussian process classi\ufb01er in black. On the right is the predicted posterior probability for the blue and red classes together with the Gaussian process decision boundary.\n\nWe have seen that the range of functions which can be represented by a neural network is governed by the number M of hidden units, and that, for suf\ufb01ciently large M, a two-layer network can approximate any given function with arbitrary accuracy. In the framework of maximum likelihood, the number of hidden units needs to be limited (to a level dependent on the size of the training set) in order to avoid over-\ufb01tting", "ce3330df-4092-4e9f-b3e6-a83d793cd532": "whose mean and covariance are given by (2.108) and (2.105) respectively.\n\nBy making use of the results (2.92) and (2.93) show that the marginal distribution p(x) is given (2.99). Similarly, by making use of the results (2.81) and (2.82) show that the conditional distribution p(y|x) is given by (2.100). 2.29 (\u22c6 \u22c6) Using the partitioned matrix inversion formula (2.76), show that the inverse of the precision matrix (2.104) is given by the covariance matrix (2.105). 2.31 (\u22c6 \u22c6) Consider two multidimensional random vectors x and z having Gaussian distributions p(x) = N(x|\u00b5x, \u03a3x) and p(z) = N(z|\u00b5z, \u03a3z) respectively, together with their sum y = x+z. Use the results (2.109) and (2.110) to \ufb01nd an expression for the marginal distribution p(y) by considering the linear-Gaussian model comprising the product of the marginal distribution p(x) and the conditional distribution p(y|x)", "92dcddee-1a0b-44c5-8f12-c300e739006a": "In this case, a stimulus that 2Comparison with a control group is necessary to show that the previous conditioning to the tone is responsible for blocking learning to the light. This is done by trials with the tone/light CS but with no prior conditioning to the tone. Learning to the light in this case is unimpaired. Moore and Schmajuk  give a full account of this procedure. consistently predicts primary reinforcement becomes a reinforcer itself, where reinforcement is primary if its rewarding or penalizing quality has been built into the animal by evolution.\n\nThe predicting stimulus becomes a secondary reinforcer, or more generally, a higher-order or conditioned reinforcer\u2014the latter being a better term when the predicted reinforcing stimulus is itself a secondary, or an even higher-order, reinforcer. A conditioned reinforcer delivers conditioned reinforcement: conditioned reward or conditioned penalty. Conditioned reinforcement acts like primary reinforcement in increasing an animal\u2019s tendency to produce behavior that leads to conditioned reward, and to decrease an animal\u2019s tendency to produce behavior that leads to conditioned penalty", "65023d2e-64b1-4a4b-bc43-edf50a83b1f7": "Planning can determine the consequences of changes in the environment that have never been linked together in the agent\u2019s own experience. For example, again referring to the maze task of Figure 14.5, imagine that a rat with a previously learned transition and reward model is placed directly in the goal box to the right of S2 to \ufb01nd that the reward available there now has value 1 instead of 4. The rat\u2019s reward model will change even though the action choices required to \ufb01nd that goal box in the maze were not involved. The planning process will bring knowledge of the new reward to bear on maze running without the need for additional experience in the maze; in this case changing the policy to right turns at both S1 and S3 to obtain a return of 3. Exactly this logic is the basis of outcome-devaluation experiments with animals. Results from these experiments provide insight into whether an animal has learned a habit or if its behavior is under goal-directed control.\n\nOutcome-devaluation experiments are like latent-learning experiments in that the reward changes from one stage to the next. After an initial rewarded stage of learning, the reward value of an outcome is changed, including being shifted to zero or even to a negative value", "427f4696-bf7f-43aa-acb0-2b9d72b37a20": "Furthermore, Tipping and Bishop  showed that the maximum of the likelihood function is obtained when the M eigenvectors are chosen to be those whose eigenvalues are the M largest (all other solutions being saddle points). A similar result was conjectured independently by Roweis , although no proof was given.\n\nAgain, we shall assume that the eigenvectors have been arranged in order of decreasing values of the corresponding eigenvalues, so that the M principal eigenvectors are Ul,\"\" UM. In this case, the columns of W define the principal subspace of standard PCA. The corresponding maximum likelihood solution for (J'2 is then given by so that (J'~L is the average variance associated with the discarded dimensions. Because R is orthogonal, it can be interpreted as a rotation matrix in the M x M latent space. If we substitute the solution for W into the expression for C, and make use of the orthogonality property RRT = I, we see that C is independent of R. This simply says that the predictive density is unchanged by rotations in the latent space as discussed earlier", "dde41220-c39d-4018-83fb-478651914b46": "Both of these prior works used a featurebased approach \u2014 we hypothesize that when the model is \ufb01ne-tuned directly on the downstream tasks and uses only a very small number of randomly initialized additional parameters, the taskspeci\ufb01c models can bene\ufb01t from the larger, more expressive pre-trained representations even when downstream task data is very small. All of the BERT results presented so far have used the \ufb01ne-tuning approach, where a simple classi\ufb01cation layer is added to the pre-trained model, and all parameters are jointly \ufb01ne-tuned on a downstream task. However, the feature-based approach, where \ufb01xed features are extracted from the pretrained model, has certain advantages.\n\nFirst, not all tasks can be easily represented by a Transformer encoder architecture, and therefore require a task-speci\ufb01c model architecture to be added. Second, there are major computational bene\ufb01ts to pre-compute an expensive representation of the training data once and then run many experiments with cheaper models on top of this representation. In this section, we compare the two approaches by applying BERT to the CoNLL-2003 Named Entity Recognition (NER) task . In the input to BERT, we use a case-preserving WordPiece model, and we include the maximal document context provided by the data", "7587729f-2a47-499b-a762-254da560b35d": "This is illustrated in figure 17.1. The problem arises when there are multiple modes with high probability that are separated by regions of low probability, especially when each Gibbs sampling step must update only a small subset of variables whose values are largely determined by the other variables.\n\nAs a simple example, consider an energy-based model over two variables a and b, which are both binary with a sign, taking on values \u20141 and 1. If E(a,b) = \u2014wab  597  CHAPTER 17. MONTE CARLO METHODS  https://www.deeplearningbook.org/contents/monte_carlo.html    for some large positive number W, then the model expresses a strong belief that a and b have the same sign. Consider updating b using a Gibbs samp ing, step with a= 1. The conditional distribution over b is given by P(b = 1 ja= ) = o(w) If w is large, the sigmoid saturates, and the probability of also assigning b to be 1 is close to 1", "d8303e09-09f4-4ed4-aa6a-407c160ef1ff": "Michael Bachaes 1 Dickie Powell 2 Lt. Zachary Bram 9 *Gen. Robert Eisen: War\ufb01ghter \u2013 Soldier + Genoured\\n\\n\\n \u2013 Senior Bush Doctrine legal: This essay discusses Illinois cases on issues such as drug traf\ufb01cking and drug Social Security. politics: This essay discusses federal ethics as the key area on which current and past state and local governments have been operating. computers: This essay discusses the very development of alternative technology for young people. space: This essay discusses NASA\u2019s StarHubble satellite mission development. Transcript here. religion: This essay discusses various aspects of the relays of mediocality and Hammazanna. science: This essay discusses Linux desktop computing, and IRI video-game applications.\\n\\n The zooming in \u2014 even after the GNOME 3 transition came to an end, is all about \ufb01guring out how you have run a software operating system so vital that any hacker can mine it military: This essay discusses military courage that included in the combat operations in Iraq and Afghanistan", "eb13fb75-3236-49b6-8d1c-15ac9cce634a": "Manual hyperparameter tuning can work very well when the user has a good starting point, such as one determined by others having worked on the same type of application and architecture, or when the user has months or years of experience in exploring hyperparameter values for neural networks applied to similar tasks. For many applications, however, these starting points are not available. In these cases, automated algorithms can find useful values of the hyperparameters.\n\nIf we think about the way in which the user of a learning algorithm searches for good values of the hyperparameters, we realize that an optimization is taking place: we are trying to find a value of the hyperparameters that optimizes an objective function, such as validation error, sometimes under constraints (such as a budget for training time, memory or recognition time). It is therefore possible, in principle, to develop hyperparameter optimization algorithms that wrap a learning algorithm and choose its hyperparameters, thus hiding the hyperparameters of the learning algorithm from the user. Unfortunately, hyperparameter optimization algorithms often have their own hyperparameters, such as the range of values that should be explored for each of the learning algorithm\u2019s hyperparameters", "2c7d6a3a-d9bf-45fd-8bdb-07b4e97e1ee1": "Most optimization algorithms converge much faster (in terms of total computation, not in terms of number of updates) if they are allowed to rapidly compute approximate estimates of the gradient rather than slowly computing the exact gradient. Another consideration motivating statistical estimation of the gradient from a small number of samples is redundancy in the training set. In the worst case, all m samples in the training set could be identical copies of each other. A sampling- based estimate of the gradient could compute the correct gradient with a single sample, using m times less computation than the naive approach. In practice, we are unlikely to encounter this worst-case situation, but we may find large numbers of examples that all make very similar contributions to the gradient. https://www.deeplearningbook.org/contents/optimization.html    Optimization algorithms that use the entire training set are called batch or deterministic gradient methods, because they process all the training examples simultaneously in a large batch.\n\nThis terminology can be somewhat confusing  because the word \u201cbatch\u201d is also often used to describe the minibatch used by minibatch stochastic gradient descent. Typically the term \u201cbatch gradient descent\u201d implies the use of the full training set, while the use of the term \u201cbatch\u201d to describe a group of examples does not", "9cf2fd9a-6b35-447d-bbdf-7b43b38b520c": "The target policy \u21e1, on the other hand, may be deterministic, and, in fact, this is a case of particular interest in control applications. In control, the target policy is typically the deterministic greedy policy with respect to the current estimate of the action-value function.\n\nThis policy becomes a deterministic optimal policy while the behavior policy remains stochastic and more exploratory, for example, an \"-greedy policy. In this section, however, we consider the prediction problem, in which \u21e1 is unchanging and given. Almost all o\u21b5-policy methods utilize importance sampling, a general technique for estimating expected values under one distribution given samples from another. We apply importance sampling to o\u21b5-policy learning by weighting returns according to the relative probability of their trajectories occurring under the target and behavior policies, called the importance-sampling ratio. Given a starting state St, the probability of the subsequent state\u2013action trajectory, At, St+1, At+1,", "d0acdbc1-2c06-4dbd-ab7a-2d0ac6e6995e": "This would work, but would require a lot of computational resources, particularly to store the \ufb01rst two expectations, which are d \u21e5 d matrices, and to compute the inverse of the second. This idea can be improved. If two of the three expectations are estimated and stored, then the third could be sampled and used in conjunction with the two stored quantities. For example, you could store estimates of the second two quantities (using the increment inverse-updating techniques in Section 9.8) and then sample the \ufb01rst expression. Unfortunately, the overall algorithm would still be of quadratic complexity (of order O(d2)). The idea of storing some estimates separately and then combining them with samples is a good one and is also used in Gradient-TD methods.\n\nGradient-TD methods estimate and store the product of the second two factors in (11.27). These factors are a d \u21e5 d matrix and a d-vector, so their product is just a d-vector, like w itself. We denote this second learned vector as v: This form is familiar to students of linear supervised learning. It is the solution to a linear least-squares problem that tries to approximate \u21e2t\u03b4t from the features", "df7398dc-c4c1-415f-8cbd-6062c870ea76": "Thus to evaluate the message sent by a variable node to an adjacent factor node along the connecting link, we simply take the product of the incoming messages along all of the other links. Note that any variable node that has only two neighbours performs no computation but simply passes messages through unchanged. Also, we note that a variable node can send a message to a factor node once it has received incoming messages from all other neighbouring factor nodes. Recall that our goal is to calculate the marginal for variable node x, and that this marginal is given by the product of incoming messages along all of the links arriving at that node. Each of these messages can be computed recursively in terms of other messages. In order to start this recursion, we can view the node x as the root of the tree and begin at the leaf nodes", "e3639e07-9832-412c-9fa6-3930bd3fdd71": "The parametrization of each P(a; | vj-1,-..,21) by a neural network with (i \u2014 1) x k inputs and k& outputs (if the variables are discrete and take k values, encoded one-hot) enables one to estimate the conditional probability without requiring an exponential number of parameters (and examples), yet still is able to capture high-order dependencies between the random variables. 2. Instead of having a different neural network for the prediction of each z,;, a left-to-right connectivity, illustrated in figure 20.9, allows one to merge all the neural networks into one. Equivalently, it means that the hidden layer features computed for predicting x; can be reused for predicting x;+, (k > 0). The hidden units are thus organized in groups that have the particularity that all the units in the +th group only depend on the input values 41,...,2;. The parameters used to compute these hidden units are jointly optimized to improve the prediction of all the variables in the sequence.\n\nThis is an instance of the reuse principle that recurs throughout deep learning in scenarios ranging from recurrent and convolutional network architectures to multitask and transfer learning", "df027218-fd10-4f69-8e8d-24f7ff5a0ef8": "Widrow, Gupta, and Maitra  modi\ufb01ed the Least-Mean-Square (LMS) algorithm of Widrow and Ho\u21b5  to produce a reinforcement learning rule that could learn from success and failure signals instead of from training examples. They called this form of learning \u201cselective bootstrap adaptation\u201d and described it as \u201clearning with a critic\u201d instead of \u201clearning with a teacher.\u201d They analyzed this rule and showed how it could learn to play blackjack. This was an isolated foray into reinforcement learning by Widrow, whose contributions to supervised learning were much more in\ufb02uential. Our use of the term \u201ccritic\u201d is derived from Widrow, Gupta, and Maitra\u2019s paper. Buchanan, Mitchell, Smith, and Johnson  independently used the term critic in the context of machine learning , but for them a critic is an expert system able to do more than evaluate performance", "d7b632bd-3623-43f3-b208-1d28fdf0c8d7": "If we consider two nodes xi and xj that are not connected by a link, then these variables must be conditionally independent given all other nodes in the graph. This follows from the fact that there is no direct path between the two nodes, and all other paths pass through nodes that are observed, and hence those paths are blocked. This conditional independence property can be expressed as where x\\{i,j} denotes the set x of all variables with xi and xj removed. The factorization of the joint distribution must therefore be such that xi and xj do not appear in the same factor in order for the conditional independence property to hold for all possible distributions belonging to the graph.\n\nThis leads us to consider a graphical concept called a clique, which is de\ufb01ned as a subset of the nodes in a graph such that there exists a link between all pairs of nodes in the subset. In other words, the set of nodes in a clique is fully connected. Furthermore, a maximal clique is a clique such that it is not possible to include any other nodes from the graph in the set without it ceasing to be a clique", "498244d9-3792-4627-8fe6-a569f8388c3d": "Keep in mind that not allh values are feasible (there is no h = 0 in this example), and that a linear classifier on top of the distributed representation is not able to assign different class identities to every neighboring region; even a deep linear-threshold network has a VC dimension of only O(w log w), where w is the number of weights .\n\nThe combination of a powerful representation layer and a weak classifier layer can be a strong regularizer; a classifier trying to learn the concept of \u201cperson\u201d versus \u201cnot a person\u201d does not need to assign a different class to an input represented as \u201cwoman with glasses\u201d than it assigns to an input represented as \u201cman without glasses.\u201d This capacity constraint encourages each classifier to focus on few  h, and encourages h to learn to represent the classes in a linearly separable way. 545  CHAPTER 15. REPRESENTATION LEARNING  e Decision trees: only one leaf (and the nodes on the path from root to leaf) is activated when an input is given. e Gaussian mixtures and mixtures of experts: the templates (cluster centers) or experts are now associated with a degree of activation", "cd376628-2d6b-4825-b42d-ec615f0669e1": "(a)We start by training a sufficiently shallow architecture. (b)Another drawing of the same architecture. (c)We keep only the input-to-hidden layer of the original network and discard the hidden-to-output layer. We send the output of the first hidden layer as input to another supervised single hidden layer MLP that is trained with the same objective as the first network was, thus adding a second hidden layer. This can be repeated for as many layers as desired. (d)Another drawing of the result, viewed as a feedforward network. To further improve the optimization, we can jointly fine-tune all the layers, either only at  the end or at each stage of this process. 320  https://www.deeplearningbook.org/contents/optimization.html       CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  intermediate levels of a deep hierarchy. In general, pretraining may help both in terms of optimization and generalization", "e29b042c-7373-4083-899b-9abb15ecc69c": "This can be seen by observing that the directional second derivative in any direction must be positive, and making reference to the univariate second derivative test. Likewise, when the Hessian is negative definite (all its eigenvalues are negative), the point is a local maximum. In multiple dimensions, it is actually possible to find positive evidence of saddle points in some cases. When at least one eigenvalue is positive and at least one eigenvalue is negative, we know that x is a local maximum on one cross section of f but a local minimum on another cross section. See figure 4.5 for an example. Finally, the multidimensional second derivative test can be inconclusive, just as the univariate version can. The test is inconclusive whenever all the nonzero eigenvalues have the same sign but at least one eigenvalue is zero. This is because the univariate second derivative test is inconclusive in the cross section corresponding to the zero eigenvalue.\n\nIn multiple dimensions, there is a different second derivative for each direction at a single point. The condition number of the Hessian at this point measures how much the second derivatives differ from each other", "2ecf514b-1eee-4a01-8565-543fb1726c9f": "One way to assign directions to the edges in D is to impose an ordering on the random variables, then  574  CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  P B86 5 6 SSB  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html     Figure 16.11: Examples of converting directed models (top row) to undirected models (bottom row) by constructing moralized graphs.\n\n(Left) This simple chain can be converted to a moralized graph merely by replacing its directed edges with undirected edges. The resulting undirected model implies exactly the same set of independences and conditional independences. (Center) This graph is the simplest directed model that cannot be converted to an undirected model without losing some independences. This graph consists entirely of a single immorality. Becausea and b are parents ofc, they are connected by an active path when c is observed. To capture this dependence, the undirected model must include a clique encompassing all three variables. This clique fails to encode the fact that aLb", "e1b47aa3-3156-49b3-b40b-0f75bca865ed": "This is beyond what we treat in any detail in this book, but vector-valued RPE signals make sense from the perspective of reinforcement learning when decisions can be decomposed into separate sub-decisions, or more generally, as a way to address the structural version of the credit assignment problem: How do you distribute credit for success (or blame for failure) of a decision among the many component structures that could have been involved in producing it? We say a bit more about this in Section 15.10 below.\n\nThe axons of most dopamine neurons make synaptic contact with neurons in the frontal cortex and the basal ganglia, areas of the brain involved in voluntary movement, decision making, learning, and cognitive functions such as planning. Because most ideas relating dopamine to reinforcement learning focus on the basal ganglia, and the connections from dopamine neurons are particularly dense there, we focus on the basal ganglia here. The basal ganglia are a collection of neuron groups, or nuclei, lying at the base of the forebrain. The main input structure of the basal ganglia is called the striatum. Essentially all of the cerebral cortex, among other structures, provides input to the striatum", "7ce77aaf-bbd6-40f3-a5e4-536067cdb6dc": "For speci\ufb01c choices of the class-conditional densities p(x|Ck), we have used maximum likelihood to determine the parameters of the densities as well as the class priors p(Ck) and then used Bayes\u2019 theorem to \ufb01nd the posterior class probabilities.\n\nHowever, an alternative approach is to use the functional form of the generalized linear model explicitly and to determine its parameters directly by using maximum likelihood. We shall see that there is an ef\ufb01cient algorithm \ufb01nding such solutions known as iterative reweighted least squares, or IRLS. The indirect approach to \ufb01nding the parameters of a generalized linear model, by \ufb01tting class-conditional densities and class priors separately and then applying shows the original input space (x1, x2) together with data points from two classes labelled red and blue. Two \u2018Gaussian\u2019 basis functions \u03c61(x) and \u03c62(x) are de\ufb01ned in this space with centres shown by the green crosses and with contours shown by the green circles. The right-hand plot shows the corresponding feature space (\u03c61, \u03c62) together with the linear decision boundary obtained given by a logistic regression model of the form discussed in Section 4.3.2", "664188f4-8205-44d1-8ee1-36bfc211fb19": "-| Enc } ~(eece) -| Dec } > And yet his crops didn\u2019t grow. Spring had come. - Ene (f)| ~(eece)  They were so black. ~- (Eee) - oes And yet his crops didn\u2019t grow. Be) ee @ooe\u2014|  He had blue eyes. [BER \u2014-@2e 9)  (a) Conventional approach  Classifier | nN  (b) Proposed approach  Let f(.) and g(.) be two functions that encode a sentence s into a fixed-length vector. Let C(s)  https://lilianweng.github.io/posts/2021-05-31-contrastive/     Contrastive Representation Learning | Lil'Log   be the set of sentences in the context of s and S(s) be the set of candidate sentences including only one sentence s, \u20ac C(s) and many other non-context negative sentences. Quick Thoughts model learns to optimize the probability of predicting the only true context sentence s, \u20ac S(s)", "19020208-0591-4285-b2c1-9bdb49a4874a": "By redefining the parameters of the model, show that this leads to an identical model for the marginal distribution p(x) over the observed variables for any valid choice of m and ~.\n\n12.5 (* *) Let x be a D-dimensional random variable having a Gaussian distribution given by N(xIJL, ~), and consider the M-dimensional random variable given by y = Ax + b where A is an M x D matrix. Show that y also has a Gaussian distribution, and find expressions for its mean and covariance. Discuss the form of this Gaussian distribution for M < D, for M = D, and for M > D. 12.6 (*) Imm Draw a directed probabilistic graph for the probabilistic PCA model described in Section 12.2 in which the components of the observed variable x are shown explicitly as separate nodes. Hence verify that the probabilistic PCA model has the same independence structure as the naive Bayes model discussed in Section 8.2.2", "d55a7195-83b4-4cd1-8060-56f4b79b5fe4": "Most SGD methods require the designer to select an appropriate step-size parameter \u21b5. Ideally this selection would be automated, and in some cases it has been, but for most cases it is still common practice to set it manually. To do this, and to better understand the algorithms, it is useful to develop some intuitive sense of the role of the step-size parameter. Can we say in general how it should be set? Theoretical considerations are unfortunately of little help. The theory of stochastic approximation gives us conditions (2.7) on a slowly decreasing step-size sequence that are su\ufb03cient to guarantee convergence, but these tend to result in learning that is too slow. The classical choice \u21b5t = 1/t, which produces sample averages in tabular MC methods, is not appropriate for TD methods, for nonstationary problems, or for any method using function approximation.\n\nFor linear methods, there are recursive least-squares methods that set an optimal matrix step size, and these methods can be extended to temporaldi\u21b5erence learning as in the LSTD method described in Section 9.8, but these require O(d2) step-size parameters, or d times more parameters than we are learning", "293ee378-5d15-4f64-aa8e-822a0fa4e3bf": "Given a joint Gaussian distribution N(x|\u00b5, \u03a3) with \u039b \u2261 \u03a3\u22121 and the plot on the right shows the marginal distribution p(xa) (blue curve) and the conditional distribution p(xa|xb) for xb = 0.7 (red curve). We illustrate the idea of conditional and marginal distributions associated with a multivariate Gaussian using an example involving two variables in Figure 2.9. In Sections 2.3.1 and 2.3.2, we considered a Gaussian p(x) in which we partitioned the vector x into two subvectors x = (xa, xb) and then found expressions for the conditional distribution p(xa|xb) and the marginal distribution p(xa). We noted that the mean of the conditional distribution p(xa|xb) was a linear function of xb.\n\nHere we shall suppose that we are given a Gaussian marginal distribution p(x) and a Gaussian conditional distribution p(y|x) in which p(y|x) has a mean that is a linear function of x, and a covariance which is independent of x", "3c6e1b29-aa79-4aed-ac68-5a9032883d8b": "Assuming independent and identically distributed data, write down the error function corresponding to the negative log likelihood. Verify that the error function (5.21) is obtained when \u03f5 = 0. Note that this error function makes the model robust to incorrectly labelled data, in contrast to the usual error function. 5.5 (\u22c6) www Show that maximizing likelihood for a multiclass neural network model in which the network outputs have the interpretation yk(x, w) = p(tk = 1|x) is equivalent to the minimization of the cross-entropy error function (5.24). 5.7 (\u22c6) Show the derivative of the error function (5.24) with respect to the activation ak for output units having a softmax activation function satis\ufb01es (5.18). 5.8 (\u22c6) We saw in (4.88) that the derivative of the logistic sigmoid activation function can be expressed in terms of the function value itself. Derive the corresponding result for the \u2018tanh\u2019 activation function de\ufb01ned by (5.59)", "babe7dab-25cb-4ff2-8408-cee830eadcd7": "First note that if we make a small step in weight space from w to w+\u03b4w then the change in the error function is \u03b4E \u2243 \u03b4wT\u2207E(w), where the vector \u2207E(w) points in the direction of greatest rate of increase of the error function. Because the error E(w) is a smooth continuous function of w, its smallest value will occur at a point in weight space such that the gradient of the error function vanishes, so that as otherwise we could make a small step in the direction of \u2212\u2207E(w) and thereby further reduce the error. Points at which the gradient vanishes are called stationary points, and may be further classi\ufb01ed into minima, maxima, and saddle points. Our goal is to \ufb01nd a vector w such that E(w) takes its smallest value. However, the error function typically has a highly nonlinear dependence on the weights and bias parameters, and so there will be many points in weight space at which the gradient vanishes (or is numerically very small). Indeed, from the discussion in Section 5.1.1 we see that for any point w that is a local minimum, there will be other points in weight space that are equivalent minima", "ecc96fcc-4778-4e6f-aea0-8438c23e6d79": "(2.36)  A vector & and a vector y are orthogonal to each other if \u00aby= 0. If both vectors have nonzero norm, this means that they are at a 90 degree angle to each other. In R\", at most n vectors may be mutually orthogonal with nonzero norm. If the vectors not only are orthogonal but also have unit norm, we call them orthonormal. An orthogonal matrix is a square matrix whose rows are mutually orthonor- mal and whose columns are mutually orthonormal:  A'A=AA' =I. (2.37)  This implies that AT=A\", (2.38) 39  CHAPTER 2. LINEAR ALGEBRA  https://www.deeplearningbook.org/contents/linear_algebra.html    so orthogonal matrices are of interest because their inverse is very cheap to compute. Pay careful attention to the definition of orthogonal matrices. Counterintuitively, their rows are not merely orthogonal but fully orthonormal", "644048d2-79fb-46fd-b0b1-a9db077f7dc3": "One attempt to improve the tightness of the PAC bounds is the PAC-Bayesian framework , which considers a distribution over the space F of functions, somewhat analogous to the prior in a Bayesian treatment.\n\nThis still considers any possible choice for p(x, t), and so although the bounds are tighter, they are still very conservative. Support vector machines have been used in a variety of classi\ufb01cation and regression applications. Nevertheless, they suffer from a number of limitations, several of which have been highlighted already in this chapter. In particular, the outputs of an SVM represent decisions rather than posterior probabilities. Also, the SVM was originally formulated for two classes, and the extension to K > 2 classes is problematic. There is a complexity parameter C, or \u03bd (as well as a parameter \u03f5 in the case of regression), that must be found using a hold-out method such as cross-validation. Finally, predictions are expressed as linear combinations of kernel functions that are centred on training data points and that are required to be positive de\ufb01nite", "c15fcaf7-66b6-4102-8c74-3c799595fb34": "Essentially, one can use adversarial nets to implement a stochastic extension of the deterministic MP-DBM . 4. Semi-supervised learning: features from the discriminator or inference net could improve performance of classi\ufb01ers when limited labeled data is available. 5. Ef\ufb01ciency improvements: training could be accelerated greatly by devising better methods for coordinating G and D or determining better distributions to sample z from during training. This paper has demonstrated the viability of the adversarial modeling framework, suggesting that these research directions could prove useful.\n\nWe would like to acknowledge Patrice Marcotte, Olivier Delalleau, Kyunghyun Cho, Guillaume Alain and Jason Yosinski for helpful discussions. Yann Dauphin shared his Parzen window evaluation code with us. We would like to thank the developers of Pylearn2  and Theano , particularly Fr\u00b4ed\u00b4eric Bastien who rushed a Theano feature speci\ufb01cally to bene\ufb01t this project. Arnaud Bergeron provided much-needed support with LATEX typesetting", "cfc03909-fe94-4553-9c26-2921ff769aa0": "We therefore have \u2207f(x) = \u2212\u03bb\u2207g(x) for some value of \u03bb > 0. For either of these two cases, the product \u03bbg(x) = 0. Thus the solution to the problem of maximizing f(x) subject to g(x) \u2a7e 0 is obtained by optimizing the Lagrange function (E.4) with respect to x and \u03bb subject to the conditions These are known as the Karush-Kuhn-Tucker (KKT) conditions . Note that if we wish to minimize (rather than maximize) the function f(x) subject to an inequality constraint g(x) \u2a7e 0, then we minimize the Lagrangian function L(x, \u03bb) = f(x) \u2212 \u03bbg(x) with respect to x, again subject to \u03bb \u2a7e 0.\n\nFinally, it is straightforward to extend the technique of Lagrange multipliers to the case of multiple equality and inequality constraints. Suppose we wish to maximize f(x) subject to gj(x) = 0 for j = 1, . , J, and hk(x) \u2a7e 0 for k = 1, . , K", "f285b80c-e14c-471c-9297-594fa4f78d89": "Feature selection simplifies a machine learning problem by choosing which subset of the available features should be used. In  232  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  particular, the well known LASSO  (least absolute shrinkage and selection operator) model integrates an L' penalty with a linear model and a least-squares cost function. The L! penalty causes a subset of the weights to become zero, suggesting that the corresponding features may safely be discarded. In section 5.6.1, we saw that many regularization strategies can be interpreted as MAP Bayesian inference, and that in particular, L? regularization is equivalent to MAP Bayesian inference with a Gaussian prior on the weights. For L!\n\nregu-  https://www.deeplearningbook.org/contents/regularization.html    larization, the pete aQ(w oo i|wi| used to regularize a cost function is equivalent to the log-prior Sas 1 th", "6540cbec-1a8b-484e-9d26-880c4325d869": "Convolution is equivalent to converting both the input and the kernel to the frequency domain using a Fourier transform, performing point-wise multiplication of the two signals, and converting back to the time domain using an inverse Fourier transform. For some problem sizes, this can be faster than the naive implementation of discrete convolution. When a ddimensional kernel can be expressed as the outer product of d vectors, one vector per dimension, the kernel is called separable. When the kernel is separable, naive convolution is inefficient. It is equivalent to compose d one-dimensional convolutions with each of these vectors. The composed approach is significantly faster than performing one d-dimensional convolution with their outer product. The kernel also takes fewer parameters to represent as vectors. re 1 wel a 1 sae yo. a . vende", "cd9f691f-7a8c-4374-9960-97a898ee44ee": "The resulting estimator for this model and datapoint x(i) is: As explained above and in appendix C, the decoding term log p\u03b8(x(i)|z(i,l)) is a Bernoulli or Gaussian MLP, depending on the type of data we are modelling. 2Note that this is just a (simplifying) choice, and not a limitation of our method. The wake-sleep algorithm  is, to the best of our knowledge, the only other on-line learning method in the literature that is applicable to the same general class of continuous latent variable models.\n\nLike our method, the wake-sleep algorithm employs a recognition model that approximates the true posterior. A drawback of the wake-sleep algorithm is that it requires a concurrent optimization of two objective functions, which together do not correspond to optimization of (a bound of) the marginal likelihood. An advantage of wake-sleep is that it also applies to models with discrete latent variables. Wake-Sleep has the same computational complexity as AEVB per datapoint. Stochastic variational inference  has recently received increasing interest", "7a17313f-33a4-4ab3-bb03-3f8ec2603782": "It therefore bene\ufb01ts from online, incremental, sample-based value estimation and policy improvement. Beyond this, it saves action-value estimates attached to the tree edges and updates them using reinforcement learning\u2019s sample updates.\n\nThis has the e\u21b5ect of focusing the Monte Carlo trials on trajectories whose initial segments are common to high-return trajectories previously simulated. Further, by incrementally expanding the tree, MCTS e\u21b5ectively grows a lookup table to store a partial action-value function, with memory allocated to the estimated values of state\u2013action pairs visited in the initial segments of high-yielding sample trajectories. MCTS thus avoids the problem of globally approximating an action-value function while it retains the bene\ufb01t of using past experience to guide exploration. The striking success of decision-time planning by MCTS has deeply in\ufb02uenced arti\ufb01cial intelligence, and many researchers are studying modi\ufb01cations and extensions of the basic procedure for use in both games and single-agent applications. Planning requires a model of the environment. A distribution model consists of the probabilities of next states and rewards for possible actions; a sample model produces single transitions and rewards generated according to these probabilities", "af8437e8-02c7-4b29-ba43-661afbb31f60": "The target model can even be a symbolic system such as a knowledge graph (KG) or a rule set. Here \u03b8 denotes the KG structure to be learned, and p\u03b8(t) can be seen as a distribution assigning a (non-)uniform nonzero probability to any knowledge tuple t in the KG and zero to all other tuples. Hao et al. presents a concrete instance of the learning system that incrementally learns (extracts) a commonsense relational KG (Figure 6, right), using the pretrained language models such as BERT  as the experience. Probabilistic graphical models and composite models. The target model p\u03b8(t) can also be probabilistic graphical models , a rich family of models characterizing the conditional dependence structure between random variables with a directed/undirected graph (Figure 5, right). Graphical models may also be composed with the neural modules to form more complex composite models, typically with the neural modules extracting features from the raw inputs and the graphical modules capturing the high-level structures", "d4ad6579-38a7-41e2-9956-8d75c2958402": "For example, consider estimating the mean parameter py of a normal distribution M(2; 1,07), with a dataset consisting of m samples: {2, eey xm}, We could use the first sample 2) of the dataset as an unbiased estimator: 6 = 2). In that case, E( Om) = 6, so the estimator is unbiased no matter how many data points are seen. This, of course, implies that the estimate is asymptotically unbiased. However, this is not a consistent estimator as it is not the case that 6,,,  > fAasm\u2014 oo. https://www.deeplearningbook.org/contents/ml.html    5.5 Maximum Likelihood Estimation  We have seen some definitions of common estimators and analyzed their properties. But where did these estimators come from? Rather than guessing that some \u2018unction might make a good estimator and then analyzing its bias and variance, we would like to have some principle from which we can derive specific functions chat are good estimators for different models. The most common such principle is the maximum likelihood principle", "5994bac3-b6a2-4ede-8c93-a1ddfc8d7a92": "It concerned Fermat\u2019s last theorem, which claims that there are no positive integer solutions to xn + yn = zn for n > 2. Dirichlet gave a partial proof for the case n = 5, which was sent to Legendre for review and who in turn completed the proof. Later, Dirichlet gave a complete proof for n = 14, although a full proof of Fermat\u2019s last theorem for arbitrary n had to wait until the work of Andrew Wiles in the closing years of the 20th century. in the plane of the simplex and the vertical axis corresponds to the value of the density.\n\nHere {\u03b1k} = 0.1 on the left plot, {\u03b1k} = 1 in the centre plot, and {\u03b1k} = 10 in the right plot. modelled using the binomial distribution (2.9) or as 1-of-2 variables and modelled using the multinomial distribution (2.34) with K = 2. The Gaussian, also known as the normal distribution, is a widely used model for the distribution of continuous variables. In the case of a single variable x, the Gaussian distribution can be written in the form where \u00b5 is the mean and \u03c32 is the variance", "f57efa6f-42aa-41ad-95fb-5a89a71ffc11": "The natural \ufb01rst question is: when does modeling the accuracies of sources improve predictive performance? Further, how many dependencies, such as correlations, are worth modeling? We study the trade-offs between predictive performance and training time in generative models for weak supervision. While modeling source accuracies and correlations will not hurt predictive performance, we present a theoretical analysis of when a simple majority vote will work just as well. Based on our conclusions, we introduce an optimizer for deciding when to model accuracies of labeling functions, and when learning can be skipped in favor of a simple majority vote. Further, our optimizer automatically decides which correlations to model among labeling functions. This optimizer correctly predicts the advantage of generative modeling over majority vote to within 2.16 accuracy points on average on our evaluation tasks, and accelerates pipeline executions by up to 1.8\u00d7. It also enables us to gain 60\u201370% of the bene\ufb01t of correlation learning while saving up to 61% of training time (34 minutes per execution)", "93162d8a-f194-40a8-8199-18d498e31bf3": "Data augmentation for low-resource neural machine translation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 567\u2013 573, Vancouver, Canada. Association for Computational Linguistics. Lisa Fan, Dong Yu, and L. Wang. 2018. Robust neural abstractive summarization systems and evaluation against adversarial information. Interpretability and Robustness for Audio, Speech and Language Workshop at Neurips 2018. Steven Y Feng, Varun Gangal, Dongyeop Kang, Teruko Mitamura, and Eduard Hovy. 2020. Genaug: Data augmentation for \ufb01netuning text generators.\n\nIn Proceedings of Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 29\u201342. Steven Y. Feng, Varun Gangal, Jason Wei, Chandar Sarath, Soroush Vosoughi, Teruko Mitamura, and Eduard Hovy. 2021. A survey of data augmentation approaches for nlp", "eb5b060f-31ab-4269-bfa1-ed10763b3a8e": "Figure 3.2 shows samples from a Gaussian mixture model. 3.10 Useful Properties of Common Functions  https://www.deeplearningbook.org/contents/prob.html    Certain functions arise often while working with probability distributions, especially the probability distributions used in deep learning models. One of these functions is the logistic sigmoid:  1 o(x) = 1+ exp(\u20142))  65  (3.30)  CHAPTER 3. PROBABILITY AND INFORMATION THEORY  Figure 3.2: Samples from a Gaussian mixture model. In this example, there are three components. From left to right, the first component has an isotropic covariance matrix, meaning it has the same amount of variance in each direction. The second has a diagonal covariance matrix, meaning it can control the variance separately along each axis-aligned direction. This example has more variance along thex2 axis than along the xz; axis.\n\nThe third component has a full-rank covariance matrix, enabling it to control the variance separately along an arbitrary basis of directions", "ec96ea92-e791-42d0-a835-01bd9f5e1336": "We have seen that kernel functions correspond to inner products in feature spaces that can have high, or even in\ufb01nite, dimensionality. By working directly in terms of the kernel function, without introducing the feature space explicitly, it might therefore seem that support vector machines somehow manage to avoid the curse of dimensionality. This is not the case, however, because there are constraints amongst Section 1.4 the feature values that restrict the effective dimensionality of feature space. To see this consider a simple second-order polynomial kernel that we can expand in terms of its components This kernel function therefore represents an inner product in a feature space having six dimensions, in which the mapping from input space to feature space is described by the vector function \u03c6(x). However, the coef\ufb01cients weighting these different features are constrained to have speci\ufb01c forms.\n\nThus any set of points in the original two-dimensional space x would be constrained to lie exactly on a two-dimensional nonlinear manifold embedded in the six-dimensional feature space. We have already highlighted the fact that the support vector machine does not provide probabilistic outputs but instead makes classi\ufb01cation decisions for new input vectors. Veropoulos et al. discuss modi\ufb01cations to the SVM to allow the trade-off between false positive and false negative errors to be controlled", "4b6c2e3d-b0ce-4a09-9c18-a7306ce3fe72": "Techniques such as \u201creplay bu\u21b5ers\u201d are often used to retain and replay old data so that its bene\ufb01ts are not permanently lost. An honest assessment has to be that current deep learning methods are not well suited to online learning. We see no reason that this limitation is insurmountable, but algorithms that address it, while at the same time retaining the advantages of deep learning, have not yet been devised. Most current deep learning research is directed toward working around this limitation rather than removing it. Second (and perhaps closely related), we still need methods for learning features such that subsequent learning generalizes well. This issue is an instance of a general problem variously called \u201crepresentation learning,\u201d \u201cconstructive induction,\u201d and \u201cmeta-learning\u201d\u2014 how can we use experience not just to learn a given desired function, but to learn inductive biases such that future learning generalizes better and is thus faster?\n\nThis is an old problem, dating back to the origins of arti\ufb01cial intelligence and pattern recognition in the 1950s and 1960s.1 Such age should give one pause. Perhaps there is no solution. But it is equally likely that the time for \ufb01nding a solution and demonstrating its e\u21b5ectiveness has not yet arrived", "4bf03eea-ba63-4c1e-94f3-af5fc33d28ad": "The weights of the critic are incremented according to the rule above by \u21b5w\u03b4tzw reinforcement signal, \u03b4t, corresponds to a dopamine signal being broadcast to all of the critic unit\u2019s synapses. The eligibility trace vector, zw of recent values) of r\u02c6v(St,w). Because \u02c6v(s,w) is linear in the weights, r\u02c6v(St,w) = x(St).\n\nIn neural terms, this means that each synapse has its own eligibility trace, which is t . A synapse\u2019s eligibility trace accumulates according to the level of activity arriving at that synapse, that is, the level of presynaptic activity, represented here by the component of the feature vector x(St) arriving at that synapse. The trace otherwise decays toward zero at a rate governed by the fraction \u03bbw. A synapse is eligible for modi\ufb01cation as long as its eligibility trace is non-zero. How the synapse\u2019s e\ufb03cacy is actually modi\ufb01ed depends on the reinforcement signals that arrive while the synapse is eligible", "ed9392a0-ada3-4610-a67f-ba79bbab6700": "Sterling and Laughlin  examined the neural basis of learning in terms of general design principles that enable e\ufb03cient adaptive behavior. 15.2 Berridge and Kringelbach  reviewed the neural basis of reward and pleasure, pointing out that reward processing has many dimensions and involves many neural systems.\n\nSpace prevents discussion of the in\ufb02uential research of Berridge and Robinson , who distinguish between the hedonic impact of a stimulus, which they call \u201cliking,\u201d and the motivational e\u21b5ect, which they call \u201cwanting.\u201d Hare, O\u2019Doherty, Camerer, Schultz, and Rangel  examined the neural basis of value-related signals from an economic perspective, distinguishing between goal values, decision values, and prediction errors. Decision value is goal value minus action cost. See also Rangel, Camerer, and Montague , Rangel and Hare , and Peters and B\u00a8uchel . 15.3 The reward prediction error hypothesis of dopamine neuron activity is most prominently discussed by Schultz, Dayan, and Montague . The hypothesis was \ufb01rst explicitly put forward by Montague, Dayan, and Sejnowski", "6951bab6-918f-49bd-8703-f7e85bff649f": "Because the value of the map degrades considerably if the map is inaccurate, it is important to add an address only if the transcription is correct.\n\nIf the machine learning system thinks hat it is less likely than a human being to obtain the correct transcription, then the best course of action is to allow a human to transcribe the photo instead. Of course, che machine learning system is only useful if it is able to dramatically reduce the amount of photos that the human operators must process. A natural performance metric to use in this situation is coverage. Coverage is the fraction of examples or which the machine learning system is able to produce a response. It is possible o trade coverage for accuracy. One can always obtain 100 percent accuracy by refusing to process any example, but this reduces the coverage to 0 percent. For the  Street View task, the goal for the project was to reach human-level transcription  https://www.deeplearningbook.org/contents/guidelines.html    accuracy while maintaining 95 percent coverage. Human-level performance on this task is 98 percent accuracy. Many other metrics are possible. We can, for example, measure click-through rates, collect user satisfaction surveys, and so on", "9e6dbac5-a81f-4259-b704-4f046239b498": "The corresponding decision boundary is therefore de\ufb01ned by the relation y(x) = 0, which corresponds to a (D \u2212 1)-dimensional hyperplane within the D-dimensional input space. Consider two points xA and xB both of which lie on the decision surface. Because y(xA) = y(xB) = 0, we have wT(xA \u2212xB) = 0 and hence the vector w is orthogonal to every vector lying within the decision surface, and so w determines the orientation of the decision surface. Similarly, if x is a point on the decision surface, then y(x) = 0, and so the normal distance from the origin to the decision surface is given by wTx \u2225w\u2225 = \u2212 w0 We therefore see that the bias parameter w0 determines the location of the decision surface.\n\nThese properties are illustrated for the case of D = 2 in Figure 4.1. Furthermore, we note that the value of y(x) gives a signed measure of the perpendicular distance r of the point x from the decision surface. To see this, consider linear discriminant function in two dimensions", "eebd1969-c5c2-47e7-8a8f-8849cd2afba5": "This was repeated to increase the dataset size from N to 2 N. The GAN style transfer baseline uses 6 different styles to transform images (Cezanne, Enhance, Monet, Uki- yoe, Van Gogh and Winter). The Neural Augmentation techniques tested consist of three levels based on the design of the loss function for the augmentation net (Con- tent loss, Style loss via gram matrix, and no loss computer at this layer).\n\nAll experi- ments are tested with a convolutional network consisting of 3 convolutional layers each followed by max pooling and batch normalization, followed by 2 fully-connected layers. Each experiment runs for 40 epochs at a learning rate of 0.0001 with the Adam optimization technique (Table 7). The results of the experiment are very promising. The Neural Augmentation tech- nique performs significantly better on the Dogs versus Goldfish study and only slightly worse on Dogs versus Cats. The technique does not have any impact on the MNIST problem. The paper suggests that the likely best strategy would be to combine  the traditional augmentations and the Neural Augmentations", "b2b2d492-00c3-4180-8826-12edf4307ee6": "Consider the learning algorithm that is just like Q-learning except that instead of the maximum over next state\u2013action pairs it uses the expected value, taking into account how likely each action is under the current policy. That is, consider the algorithm with the update rule but that otherwise follows the schema of Q-learning. Given the next state, St+1, this algorithm moves deterministically in the same direction as Sarsa moves in expectation, and accordingly it is called Expected Sarsa. Its backup diagram is shown on the right in Figure 6.4. Expected Sarsa is more complex computationally than Sarsa but, in return, it eliminates the variance due to the random selection of At+1. Given the same amount of experience we might expect it to perform slightly better than Sarsa, and indeed it generally does. Figure 6.3 shows summary results on the cli\u21b5-walking task with Expected Sarsa compared to Sarsa and Q-learning. Expected Sarsa retains the signi\ufb01cant advantage of Sarsa over Q-learning on this problem.\n\nIn addition, Expected Sarsa shows a signi\ufb01cant improvement For n = 100, 000, the average return is equal for all \u21b5 values in case of Expected Sarsa and Q-learning", "ec731ddd-7c1d-4b79-ab13-5e1930adb1f5": "The loss ty. y) depends on the output g and on the target y (see section 6.2.1.1 for examples of loss functions). To  obtain the total cost J, the loss may be added to a regularizer 2(@), where 0 contains all the parameters (weights and biases). Algorithm 6.4 shows how to compute gradients of J with respect to parameters W and b. For simplicity, this demonstration uses only a single input example x. Practical applications should use a minibatch. See section 6.5.7 for a more realistic demonstration. Require: Network depth, | Require: W\u201c),i \u20ac {1,...,1}, the weight matrices of the model Require: b\u00ae,i \u20ac {1,...,1}, the bias parameters of the model Require: x, the input to process Require: y, the target output  hO =a  h\u00ae) = f(a) end for g=hO J = LY,y) + AQ(8)  208  CHAPTER 6", "e35cd4dc-01d8-4637-b887-0bfa845f559f": "Data warping augmentations transform exist- ing images such that their label is preserved. This encompasses augmentations such as geometric and color transformations, random erasing, adversarial training, and neural style transfer. Oversampling augmentations create synthetic instances and add them to the training set. This includes mixing images, feature space augmentations, and genera- tive adversarial networks (GANs). Oversampling and Data Warping augmentations do not form a mutually exclusive dichotomy. For example, GAN samples can be stacked  with random cropping to further inflate the dataset.\n\nDecisions around final dataset size,   Shorten and Khoshgoftaar J Big Data  6:60   Image Data \u2018Augmentation  Basic Image Deep Learning Manipulations Approaches  Color Space Transformations  | AutoAugment  Kemel Filters Random Erasing Geometric Transformations  \u2018Adversarial Training |__| Neural Style Transfer  \u2018Augmentation  | GAN Data |  Neural Augmentation \u2018Smart Augmentation  Fig", "3f53e1b0-7ce3-4e01-bba1-1ef7f2247bdf": "The L1-distance between two embeddings is | fg(x;) \u2014 fo(x;)|. The distance is converted to a probability p by a linear feedforward layer and sigmoid.\n\nIt is the probability of whether two images are drawn from the same class. Intuitively the loss is cross entropy because the label is binary. P(X, Xj) = o(W| fo(x;) \u2014 fo(x;))) L\u00a3(B) = S Ly,=y; log p(xi, xj) + (1 \u2014 1y,=y;) log(1 \u2014 p(xi, x;))  (xixj,yiys)EB  Images in the training batch B can be augmented with distortion. Of course, you can replace the L1 distance with other distance metric, L2, cosine, etc. Just make sure they are differential and then  everything else works the same", "cf0f895a-fd32-4713-80b3-a1d454bdbebd": ", \u03b1K)T, we have is known as the digamma function . The parameters \u03b1k are subject to the constraint \u03b1k > 0 in order to ensure that the distribution can be normalized. The Dirichlet forms the conjugate prior for the multinomial distribution and represents a generalization of the beta distribution. In this case, the parameters \u03b1k can be interpreted as effective numbers of observations of the corresponding values of the K-dimensional binary observation vector x. As with the beta distribution, the Dirichlet has \ufb01nite density everywhere provided \u03b1k \u2a7e 1 for all k. The Gamma is a probability distribution over a positive random variable \u03c4 > 0 governed by parameters a and b that are subject to the constraints a > 0 and b > 0 to ensure that the distribution can be normalized. where \u03c8(\u00b7) is the digamma function de\ufb01ned by (B.25). The gamma distribution is the conjugate prior for the precision (inverse variance) of a univariate Gaussian", "cb9a31cb-44c7-45bf-ba7a-7e625ce61eb4": "Non-contrastive methods for joint- embedding include DeeperCluster, ClusterFit, MoCo-v2, SwAV, SimSiam, Barlow Twins, BYOL from DeepMind, and a few others. They use various  tricks, such as computing virtual target embeddings for groups of similar images (DeeperCluster, SwAV, SimSiam) or making the two joint embedding architectures slightly different through the architecture or the parameter vector (BYOL, MoCo). Barlow Twins tries to minimize the redundancy between the individual components of the embedding vectors. Perhaps a better alternative in the long run will be to devise non-contrastive methods with latent-variable predictive models. The main obstacle is that they require a way to minimize the capacity of the latent variable.\n\nThe volume of the set over which the latent variable can vary limits the volume of outputs that take low energy. By minimizing this volume, one automatically shapes the energy in the right way", "73a03ff3-b17e-4281-9884-4b67644717dc": "Here \u03b7 are called the natural parameters of the distribution, and u(x) is some function of x.\n\nThe function g(\u03b7) can be interpreted as the coef\ufb01cient that ensures that the distribution is normalized and therefore satis\ufb01es where the integration is replaced by summation if x is a discrete variable. We begin by taking some examples of the distributions introduced earlier in the chapter and showing that they are indeed members of the exponential family. Consider \ufb01rst the Bernoulli distribution Expressing the right-hand side as the exponential of the logarithm, we have Comparison with (2.194) allows us to identify which we can solve for \u00b5 to give \u00b5 = \u03c3(\u03b7), where is called the logistic sigmoid function. Thus we can write the Bernoulli distribution using the standard representation (2.194) in the form where we have used 1 \u2212 \u03c3(\u03b7) = \u03c3(\u2212\u03b7), which is easily proved from (2.199). Comparison with (2.194) shows that Next consider the multinomial distribution that, for a single observation x, takes the form where \u03b7k = ln \u00b5k, and we have de\ufb01ned \u03b7 = (\u03b71, . , \u03b7M)T", "92b0a4ee-0838-43ab-950a-a022334c8921": "Thus, each actor unit is itself a reinforcement learning agent\u2014a hedonistic neuron if you will. Now, to make the situation as simple as possible, assume that each of these units receives the same reward signal at the same time (although, as indicated above, the assumption that dopamine is released at all the corticostriatal synapses under the same conditions and at the same times is likely an oversimpli\ufb01cation). What can reinforcement learning theory tell us about what happens when all members of a population of reinforcement learning agents learn according to a common reward signal?\n\nThe \ufb01eld of multi-agent reinforcement learning considers many aspects of learning by populations of reinforcement learning agents. Although this \ufb01eld is beyond the scope of this book, we believe that some of its basic concepts and results are relevant to thinking about the brain\u2019s di\u21b5use neuromodulatory systems. In multi-agent reinforcement learning (and in game theory), the scenario in which all the agents try to maximize a common reward signal that they simultaneously receive is known as a cooperative game or a team problem. What makes a team problem interesting and challenging is that the common reward signal sent to each agent evaluates the pattern of activity produced by the entire population, that is, it evaluates the collective action of the team members", "9b65f6f6-2688-4f7d-9e8f-8682ca75f4b9": "ie tee eet  MN MN WAN \u201d\u2122 MN \u2122 aA AA ABA AA AAA ABA AA AAA APA AA A Lull. i. alias a, Hs ts z  https://lilianweng.github.io/posts/2018-02-19-rl-overview/  A (Long) Peek into Reinforcement Learning | Lil'Log  The network is trained with the samples in the replay memory to minimize the loss: L=(z-v)?\u2014-7' logp+eljd||? where c is a hyperparameter controlling the intensity of L2 penalty to avoid overfitting. AlphaGo Zero simplified AlphaGo by removing supervised learning and merging separated policy and value networks into one. It turns out that AlphaGo Zero achieved largely improved performance with a much shorter training time! | strongly recommend reading these two papers side by side and compare the difference, super fun. | know this is a long read, but hopefully worth it. /f you notice mistakes and errors in this post, don\u2019t hesitate to contact me at", "460023b1-0a35-49a0-b79c-ac5ca7c7de61": "A candidate is then a tuple of two Spans.\n\nSnorkel uses the core abstraction of a labeling function to allow users to specify a wide range of weak supervision sources such as patterns, heuristics, external knowledge bases, crowdsourced labels, and more. This higher-level, less precise input is more ef\ufb01cient to provide (see Sect. 4.2) and can be automatically denoised and synthesized, as described in subsequent sections. In this section, we describe our design choices in building an interface for writing labeling functions, which we envision as a unifying programming language for weak supervision. These choices were informed to a large degree by our interactions\u2014primarily through weekly of\ufb01ce hours\u2014with Snorkel users in bioinformatics, defense, industry, and other areas over the past year.3 For example, while we initially intended to have a more complex structure for labeling functions,withmanuallyspeci\ufb01edtypesandcorrelationstructure, we quickly found that simplicity in this respect was critical to usability (and not empirically detrimental to our ability to model their outputs). We also quickly discovered that users wanted either far more expressivity or far less of it, compared to our \ufb01rst library of function templates", "92cebc60-7a64-4c7d-8006-57ba3c8ad1f9": "Training generative neural networks via maximum mean discrepancy optimization. CoRR, abs/1505.03906, 2015.\n\nAude Genevay, Marco Cuturi, Gabriel Peyr\u00b4e, and Francis Bach. Stochastic optimization for large-scale optimal transport. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 3440\u20133448. Curran Associates, Inc., 2016. Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David WardeFarley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems 27, pages 2672\u20132680. Curran Associates, Inc., 2014. Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Sch\u00a8olkopf, and Alexander Smola", "f097da1c-f2f6-4c45-9604-6c3a7dc1dfe0": "The system then explores the distribution along the more extended direction by means of a random walk, and so the number of steps to arrive at a state that is more or less independent of the original state is of order (\u03c3max/\u03c3min)2.\n\nIn fact in two dimensions, the increase in rejection rate as \u03c1 increases is offset by the larger steps sizes of those transitions that are accepted, and more generally for a multivariate Gaussian the number of steps required to obtain independent samples scales like (\u03c3max/\u03c32)2 where \u03c32 is the second-smallest standard deviation . These details aside, it remains the case that if the length scales over which the distributions vary are very different in different directions, then the Metropolis Hastings algorithm can have very slow convergence. Gibbs sampling  is a simple and widely applicable Markov chain Monte Carlo algorithm and can be seen as a special case of the MetropolisHastings algorithm. Consider the distribution p(z) = p(z1, . , zM) from which we wish to sample, and suppose that we have chosen some initial state for the Markov chain", "01b89801-fb59-462e-bc29-742bd9c4a4f8": "i j == do fa)  i=1,xOnp  can be transformed into an importance sampling estimator  42 SS pa) fa) (17.10)  n ining q(x)  We see readily that the expected value of the estimator does not depend on q: Eq = Eq = s. (17.11)  The variance of an importance sampling estimator, however, can be greatly sensitive to the choice of g. The variance is given by  Var = vane. (17.12) The minimum variance occurs when q is q(x) = Pee) (17.13)  https://www.deeplearningbook.org/contents/monte_carlo.html    where Z is the normalization constant, chosen so that q7 (2) sums or integrates to 1 as appropriate. Better importance sainplin distributions put more weight where the integrand is larger. In fact, when /' a) does not change sign, Var  = 0,  meaning that a single sample is yen when the optimal distribution is used. Of course, this is only because the computation of g* has essentially solved the original problem, so it is usually not practical to use this approach of drawing a single sample from the optimal distribution", "cbd10023-7bbd-4a87-92f4-62d44a76ffef": "One consequence of the generality of the potential functions \u03c8C(xC) is that their product will in general not be correctly normalized. We therefore have to introduce an explicit normalization factor given by (8.40). Recall that for directed graphs, the joint distribution was automatically normalized as a consequence of the normalization of each of the conditional distributions in the factorization.\n\nThe presence of this normalization constant is one of the major limitations of undirected graphs. If we have a model with M discrete nodes each having K states, then the evaluation of the normalization term involves summing over KM states and so (in the worst case) is exponential in the size of the model. The partition function is needed for parameter learning because it will be a function of any parameters that govern the potential functions \u03c8C(xC). However, for evaluation of local conditional distributions, the partition function is not needed because a conditional is the ratio of two marginals, and the partition function cancels between numerator and denominator when evaluating this ratio. Similarly, for evaluating local marginal probabilities we can work with the unnormalized joint distribution and then normalize the marginals explicitly at the end", "e37eef21-2403-497d-9098-d29a1a4632a4": "This simple design choice conveniently decouples the predictive task from other components such as the neural network architecture. Broader contrastive prediction tasks can be de\ufb01ned by extending the family of augmentations and composing them stochastically. 3.1. Composition of data augmentation operations is crucial for learning good representations To systematically study the impact of data augmentation, we consider several common augmentations here. One type of augmentation involves spatial/geometric transformation of data, such as cropping and resizing (with horizontal \ufb02ipping), rotation  and cutout .\n\nThe other type of augmentation involves appearance transformation, such as color distortion (including color dropping, brightness, contrast, saturation, hue) , Gaussian blur, and Sobel \ufb01ltering. Figure 4 visualizes the augmentations that we study in this work. To understand the effects of individual data augmentations and the importance of augmentation composition, we investigate the performance of our framework when applying augmentations individually or in pairs. Since ImageNet images are of different sizes, we always apply crop and resize images , which makes it dif\ufb01cult to study other augmentations in the absence of cropping. To eliminate this confound, we consider an asymmetric data transformation setting for this ablation", "163cc60e-d136-45ed-bed2-f09de0130fb3": "The output unit formed the weighted sum and then passed it through the same sigmoid nonlinearity. TD-Gammon used the semi-gradient form of the TD(\u03bb) algorithm described in Section 12.2, with the gradients computed by the error backpropagation algorithm . Recall that the general update rule for this case is where wt is the vector of all modi\ufb01able parameters (in this case, the weights of the network) and zt is a vector of eligibility traces, one for each component of wt, updated by with z0 .= 0. The gradient in this equation can be computed e\ufb03ciently by the backpropagation procedure. For the backgammon application, in which \u03b3 = 1 and the reward is always zero except upon winning, the TD error portion of the learning rule is usually just \u02c6v(St+1,w) \u2212 \u02c6v(St,w), as suggested in Figure 16.1. To apply the learning rule we need a source of backgammon games.\n\nTesauro obtained an unending sequence of games by playing his learning backgammon player against itself", "820cda60-7856-458e-929e-573641baea7d": "For example, it is not well suited to nonstationary problems because its drive for exploration is inherently temporary. If the task changes, creating a renewed need for exploration, this method cannot help. Indeed, any method that focuses on the initial conditions in any special way is unlikely to help with the general nonstationary case. The beginning of time occurs only once, and thus we should not focus on it too much. This criticism applies as well to the sample-average methods, which also treat the beginning of time as a special event, averaging all subsequent rewards with equal weights. Nevertheless, all of these methods are very simple, and one of them\u2014or some simple combination of them\u2014is often adequate in practice. In the rest of this book we make frequent use of several of these simple exploration techniques. because they are averages over 2000 individual, randomly chosen 10-armed bandit tasks. Why, then, are there oscillations and spikes in the early part of the curve for the optimistic method?\n\nIn other words, what might make this method perform particularly better or worse, on average, on particular early steps? \u21e4 sample averages to estimate action values because sample averages do not produce the initial bias that constant step sizes do (see the analysis leading to (2.6))", "493d50a5-9344-4f0a-8522-0ba38a4e95df": "From each of the |S| states, two actions were possible, each of which resulted in one of b next states, all equally likely, with a di\u21b5erent random selection of b states for each state\u2013action pair. The branching factor, b, was the same for all state\u2013action pairs. In addition, on all transitions there was a 0.1 probability of transition to the terminal state, ending the episode. The expected reward on each transition was selected from a Gaussian distribution with mean 0 and variance 1. the start state. If there are many states and a small branching factor, this e\u21b5ect will be large and long-lasting. In the long run, focusing on the on-policy distribution may hurt because the commonly occurring states all already have their correct values. Sampling them is useless, whereas sampling other states may actually perform some useful work", "6883101d-5e83-410d-8a37-3f614d874e1b": "Sometimes, however, what is shared among the different tasks is not the semantics of the input but the semantics of the output. For example, a speech recognition system needs to produce valid sentences at the output layer, but the earlier layers near the input may need to recognize very different versions of the same phonemes or subphonemic vocalizations depending on which person is speaking. In cases like these, it makes more sense to share the upper layers  534  CHAPTER 15.\n\nREPRESENTATION LEARNING  7 \u2122  https://www.deeplearningbook.org/contents/representation.html    Figure 15.2: Example architecture for multitask or transfer learning when the output variable y has the same semantics for all tasks while the input variable x has a different meaning (and possibly even a different dimension) for each task (or, for example, each user), called x, x) and x) for three tasks. The lower levels (up to the selection switch) are task-specific, while the upper levels are shared. The lower levels learn to translate their task-specific input into a generic set of features. (near the output) of the neural network and have a task-specific preprocessing, as illustrated in figure 15.2", "29cf5952-4803-432f-8359-9f8f3b8233a4": "Recognizing that most machine learning algorithms can be described using this recipe helps to see the different algorithms as part of a axonomy of methods for doing related tasks that work for similar reasons, rather chan as a long list of algorithms that each have separate justifications.\n\n5.11 Challenges Motivating Deep Learning  The simple machine learning algorithms described in this chapter work well on a  sa to ee 4 4 11 mi 1 4 road : 1:  https://www.deeplearningbook.org/contents/ml.html    WIUE VaLIELy OL LILpPOrlallt P such as illey lave WO SUCCECUCU, LLOWEVEL, Ll SOLVILLY the central problems in Al, such as recognizing speech or recognizing objects. The development of deep learning was motivated in part by the failure of traditional algorithms to generalize well on such AI tasks. This section is about how the challenge of generalizing to new examples becomes exponentially more difficult when working with high-dimensional data, and how the mechanisms used to achieve generalization in traditional machine learning are insufficient to learn complicated functions in high-dimensional spaces. Such spaces also often impose high computational costs. Deep learning was designed to overcome these and other obstacles", "639fae02-c847-44f5-a035-7b7122aa538d": "In particular, by considering the speci\ufb01c instantiation in Equation 5.1 of the SE and setting D to the JS divergence, we can derive the algorithm for learning the generative adversarial networks  as shown below. From this perspective, the key concept in generative adversarial learning, namely the discriminator, arises as an approximation to the optimization procedure. We discuss in Section 6 an alternative view of the learning paradigm where the discriminator plays the role of \u2018dynamic\u2019 experience in SE. Generative adversarial learning: The functional descent view.\n\nTo optimize the objective in Equation 5.1 with the JS divergence, probability functional descent  o\ufb00ers an elegant way that recovers the optimization procedure of GANs originally developed in Goodfellow et al. Here we give the PFD result directly, and provide a more detailed review of the PFD optimization in Section 7. Speci\ufb01cally, let J(p) := D(pd, p) in Equation 5.1, which is a functional on the distribution p \u2208 P(T )", "d569dcd1-e339-45cd-bcfa-89825557875e": ", N, and we set \ufffdf0(\u03b8) equal to the prior p(\u03b8).\n\nNote that the use of N(\u03b8|\u00b7, \u00b7) does not imply that the right-hand side is a well-de\ufb01ned Gaussian density (in fact, as we shall see, the variance parameter vn can be negative) but is simply a convenient shorthand notation. The approximations \ufffdfn(\u03b8), for n = 1, . , N, can be initialized to unity, corresponding to sn = (2\u03c0vn)D/2, vn \u2192 \u221e and mn = 0, where D is the dimensionality of x and hence of \u03b8. The initial q(\u03b8), de\ufb01ned by (10.191), is therefore equal to the prior. We then iteratively re\ufb01ne the factors by taking one factor fn(\u03b8) at a time and applying (10.205), (10.206), and (10.207). Note that we do not need to revise the term f0(\u03b8) because an EP update will leave this term unchanged. Here we state the Exercise 10.37 results and leave the reader to \ufb01ll in the details", "f037a68c-bdc4-443d-8658-c7a03bfb6b0e": "Jason Wei and Kai Zou. 2019. EDA: Easy data augmentation techniques for boosting performance on text classi\ufb01cation tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 6382\u20136388, Hong Kong, China. Association for Computational Linguistics. Xing Wu, Shangwen Lv, Liangjun Zang, Jizhong Han, and Songlin Hu. 2019a. Conditional bert contextual augmentation. In International Conference on Computational Science, pages 84\u201395. Springer. Zhanghao Wu, Shuai Wang, Yanmin Qian, and Kai Yu. 2019b.\n\nData Augmentation Using Variational Autoencoder for Embedding Based Speaker Veri\ufb01cation. In Proc. Interspeech 2019, pages 1163\u20131167. Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. 2020. Unsupervised data augmentation for consistency training", "74e6a0a3-e275-437b-bdd2-791cd5b1d5e2": "That transition would reduce w, unless it were to a state whose value was higher (because \u03b3 < 1) than 2w, and then that state would have to be followed by a state of even higher value, or else again w would be reduced. Each state can support the one before only by creating a higher expectation. Eventually the piper must be paid. In the on-policy case the promise of future reward must be kept and the system is kept in check. But in the o\u21b5-policy case, a promise can be made and then, after taking an action that the target policy never would, forgotten and forgiven. This simple example communicates much of the reason why o\u21b5-policy training can lead to divergence, but it is not completely convincing because it is not complete\u2014it is just a fragment of a complete MDP. Can there really be a complete system with instability? A simple complete example of divergence is Baird\u2019s counterexample. Consider the episodic seven-state, two-action MDP shown in Figure 11.1", "faa23c68-88c2-4d2a-9ba5-cfea8ccb8aac": "This has led to a sequence of progressively more complex architec- tures from AlexNet  to VGG-16 , ResNet , Inception-V3 , and DenseNet .\n\nFunctional solutions such as dropout regularization, batch normalization, transfer learn-  ing, and pretraining have been developed to try to extend Deep Learning for application on smaller datasets. A brief description of these overfitting solutions is provided below. A complete survey of regularization methods in Deep Learning has been compiled by Kukacka et al. Knowledge of these overfitting solutions will inform readers about other existing tools, thus framing the high-level context of Data Augmentation and Deep  Learning. + Dropout  is a regularization technique that zeros out the activation values of ran- domly chosen neurons during training. This constraint forces the network to learn more robust features rather than relying on the predictive capability of a small subset  of neurons in the network. Tompson et al. extended this idea to convolutional Shorten and Khoshgoftaar J Big Data  6:60  networks with Spatial Dropout, which drops out entire feature maps rather than individual neurons", "7ef6e7c5-cd0b-461f-b8d3-0bea61303851": "Consider a set of m examples X = {a, weey x(\u2122)} drawn independently from she true but unknown data-generating distribution p gata(x).\n\nLet Pmodel(x;@) be a parametric family of probability distributions over the same space indexed by @. In other words, pmodel(@;@) maps any configuration x (0 a real number estimating the true probability paata(a). The maximum likelihood estimator for 8 is then defined as  Oui = arg max Dmodel(X; 0), (5.56) 0 = arg max | | pmoae(\u00ab; 0). (5.57) o j=l  This product over many probabilities can be inconvenient for various reasons. For example, it is prone to numerical underflow. To obtain a more convenient but equivalent optimization problem, we observe that taking the logarithm of the likelihood does not change its arg max but does conveniently transform a product  129  CHAPTER 5. MACHINE LEARNING BASICS  into a sum: m  Ov, =argmax \u2014 log pmodei(a\"\u201d ; 8). (5.58) 0", "14cc5937-8840-4584-b7dd-e0079b0340d9": "Intractable inference problems in deep learning usually arise from interactions  between latent variables in a structured graphical model. See figure 19.1 for some examples.\n\nThese interactions may be due to direct interactions in undirected models or \u201cexplaining away\u201d interactions between mutual ancestors of the same visible unit in directed models. https://www.deeplearningbook.org/contents/inference.html    629  CHAPTER 19. APPROXIMATE INFERENCE  OVO  Figure 19.1: Intractable inference problems in deep learning are usually the result of interactions between latent variables in a structured graphical model. These interactions can be due to edges directly connecting one latent variable to another or longer paths that are activated when the child of a V-structure is observed. (Left)A semi-restricted Boltzmann machine  with connections between hidden units. These direct connections between latent variables make the posterior distribution intractable because of large cliques of latent variables. (Center)A deep Boltzmann machine, organized into layers of variables without intralayer connections, still has an intractable posterior distribution because of the connections between layers", "50ce2292-055a-460f-af4b-ecdf1f84ddc4": "Batch techniques, such as the maximum likelihood solution (3.15), which involve processing the entire training set in one go, can be computationally costly for large data sets.\n\nAs we have discussed in Chapter 1, if the data set is suf\ufb01ciently large, it may be worthwhile to use sequential algorithms, also known as on-line algorithms, in which the data points are considered one at a time, and the model parameters updated after each such presentation. Sequential learning is also appropriate for realtime applications in which the data observations are arriving in a continuous stream, and predictions must be made before all of the data points are seen. We can obtain a sequential learning algorithm by applying the technique of stochastic gradient descent, also known as sequential gradient descent, as follows. If the error function comprises a sum over data points E = \ufffd where \u03c4 denotes the iteration number, and \u03b7 is a learning rate parameter. We shall discuss the choice of value for \u03b7 shortly. The value of w is initialized to some starting vector w(0). For the case of the sum-of-squares error function (3.12), this gives where \u03c6n = \u03c6(xn). This is known as least-mean-squares or the LMS algorithm", "f6849d87-9d7d-49f9-acb5-cf0dac7765ca": "Multiple manifolds are likely involved in most applications. For example, the manifold of human face images may not be connected to the manifold of cat face images. These thought experiments convey some intuitive reasons supporting the mani- fold hypothesis. More rigorous experiments  clearly support the hypothesis for a large class of datasets of interest in AI. When the data lies on a low-dimensional manifold, it can be most natural for machine learning algorithms to represent the data in terms of coordinates on the manifold, rather than in terms of coordinates in IR\u201d. In everyday life, we can think of roads as 1-D manifolds embedded in 3-D space. We give directions to specific addresses in terms of address numbers along these 1-D roads, not in terms of coordinates in 3-D space.\n\nExtracting these manifold coordinates is challenging but holds the promise of improving many machine learning algorithms. This general  https://www.deeplearningbook.org/contents/ml.html    principle is applied in many contexts. Figure 5.13 shows the manifold structure of a dataset consisting of faces. By the end of this book, we will have developed the methods necessary to learn such a manifold structure", "74169b67-e016-4ac5-9389-59045e2c4c43": "A weight representing the e\ufb03cacy of a synapse is associated with each connection from each feature xi to the critic unit, V , and to each of the action units, Ai. The weights in the critic network parameterize the value function, and the weights in the actor network parameterize the policy.\n\nThe networks learn as these weights change according to the critic and actor learning rules that we describe in the following section. The TD error produced by circuitry in the critic is the reinforcement signal for changing the weights in both the critic and the actor networks. This is shown in Figure 15.5a by the line labeled \u2018TD error \u03b4\u2019 extending across all of the connections in the critic and actor networks. This aspect of the network implementation, together with the reward prediction error hypothesis and the fact that the activity of dopamine neurons is so widely distributed by the extensive axonal arbors of these neurons, suggests that an actor\u2013critic network something like this may not be too farfetched as a hypothesis about how reward-related learning might happen in the brain. map onto structures in the brain according to the hypothesis of Takahashi et al", "d82061df-70a3-4a6a-96a3-ef7c286ad3be": "Alternatively, R\u03c6 can return a valid reward even when x matches a data example only in part, or (x, y) is an entire new sample not in D, which in effect makes data augmentation and data synthesis, respectively, in which cases \u03c6 is either a data transformer or a generator. In the next section, we demonstrate two particular parameterizations for data augmentation and weighting, respectively. We thus have shown that the diverse types of manipulation all boil down to a parameterized data reward R\u03c6. Such an concise, uniform formulation of data manipulation has the advantage that, once we devise a method of learning the manipulation parameters \u03c6, the resulting algorithm can directly be applied to automate any manipulation type. We present a learning algorithm next.\n\nLearning Manipulation Parameters To learn the parameters \u03c6 in the manipulation reward R\u03c6(x, y|D), we could in principle adopt any off-the-shelf reward learning algorithm in the literature. In this work, we draw inspiration from the above gradient-based reward learning (section 3) due to its simplicity and ef\ufb01ciency", "33e829bd-8595-4227-adaa-80c87facad42": "A more powerful approach, however, models the conditional probability distribution p(Ck|x) in an inference stage, and then subsequently uses this distribution to make optimal decisions. By separating inference and decision, we gain numerous bene\ufb01ts, as discussed in Section 1.5.4. There are two different approaches to determining the conditional probabilities p(Ck|x). One technique is to model them directly, for example by representing them as parametric models and then optimizing the parameters using a training set. Alternatively, we can adopt a generative approach in which we model the class-conditional densities given by p(x|Ck), together with the prior probabilities p(Ck) for the classes, and then we compute the required posterior probabilities using Bayes\u2019 theorem We shall discuss examples of all three approaches in this chapter. In the linear regression models considered in Chapter 3, the model prediction y(x, w) was given by a linear function of the parameters w. In the simplest case, the model is also linear in the input variables and therefore takes the form y(x) = wTx+w0, so that y is a real number", "7aa46f62-a10d-4dd1-a0af-742efacbc44b": "The centres Section 2.3.9 and variances of the Gaussian components, as well as the mixing coef\ufb01cients, will be considered as adjustable parameters to be determined as part of the learning process. Thus, we have a probability density of the form and \u03c0j are the mixing coef\ufb01cients. Taking the negative logarithm then leads to a regularization function of the form The total error function is then given by where \u03bb is the regularization coef\ufb01cient. This error is minimized both with respect to the weights wi and with respect to the parameters {\u03c0j, \u00b5j, \u03c3j} of the mixture model. If the weights were constant, then the parameters of the mixture model could be determined by using the EM algorithm discussed in Chapter 9. However, the distribution of weights is itself evolving during the learning process, and so to avoid numerical instability, a joint optimization is performed simultaneously over the weights and the mixture-model parameters. This can be done using a standard optimization algorithm such as conjugate gradients or quasi-Newton methods", "837a9414-c572-48cd-8490-5080792f8a95": "If all of the eigenvalues \u03bbi are positive, then these surfaces represent ellipsoids, with their centres at \u00b5 and their axes oriented along ui, and with scaling factors in the directions of the axes given by \u03bb1/2 For the Gaussian distribution to be well de\ufb01ned, it is necessary for all of the eigenvalues \u03bbi of the covariance matrix to be strictly positive, otherwise the distribution cannot be properly normalized.\n\nA matrix whose eigenvalues are strictly positive is said to be positive de\ufb01nite. In Chapter 12, we will encounter Gaussian distributions for which one or more of the eigenvalues are zero, in which case the distribution is singular and is con\ufb01ned to a subspace of lower dimensionality. If all of the eigenvalues are nonnegative, then the covariance matrix is said to be positive semide\ufb01nite. Now consider the form of the Gaussian distribution in the new coordinate system de\ufb01ned by the yi. In going from the x to the y coordinate system, we have a Jacobian matrix J with elements given by where Uji are the elements of the matrix UT", "e229acc4-7aeb-41bc-a6f9-69991368a0e3": "The indices into W are respectively: 7; the  345  CHAPTER 9. CONVOLUTIONAL NETWORKS  output channel; 7, the output row; k, the output column; J, the input channel; m, the row offset within the input; and n, the column offset within the input. The linear part of a locally connected layer is then given by  Zijk = >  \u00ab (9.9)  lym,n  This is sometimes also called unshared convolution, because it is a similar oper- ation to discrete convolution with a small kernel, but without sharing parameters across locations. Figure 9.14 compares local connections, convolution, and full connections. Locally connected layers are useful when we know that each feature should be a function of a small part of space, but there is no reason to think that the same  https://www.deeplearningbook.org/contents/convnets.html    feature should occur across all of space.\n\nFor example, if we want to tell if an image is a picture of a face, we only need to look for the mouth in the bottom half of the image", "d172bfcf-1721-45ea-a158-61db215f8ed8": "All methods that follow this general schema we call policy gradient methods, whether or not they also learn an approximate value function. Methods that learn approximations to both policy and value functions are often called actor\u2013critic methods, where \u2018actor\u2019 is a reference to the learned policy, and \u2018critic\u2019 refers to the learned value function, usually a state-value function. First we treat the episodic case, in which performance is de\ufb01ned as the value of the start state under the parameterized policy, before going on to consider the continuing case, in which performance is de\ufb01ned as the average reward rate, as in Section 10.3.\n\nIn the end, we are able to express the algorithms for both cases in very similar terms. 1The lone exception is the gradient bandit algorithms of Section 2.8. In fact, that section goes through many of the same steps, in the single-state bandit case, as we go through here for full MDPs. Reviewing that section would be good preparation for fully understanding this chapter", "d4b75302-4adb-49c2-9a1a-7e0e3b214329": "In this case there is no linear term, because \u2207E = 0 at w\u22c6, and (5.28) becomes where the Hessian H is evaluated at w\u22c6.\n\nIn order to interpret this geometrically, consider the eigenvalue equation for the Hessian matrix where the eigenvectors ui form a complete orthonormal set (Appendix C) so that We now expand (w \u2212 w\u22c6) as a linear combination of the eigenvectors in the form This can be regarded as a transformation of the coordinate system in which the origin is translated to the point w\u22c6, and the axes are rotated to align with the eigenvectors (through the orthogonal matrix whose columns are the ui), and is discussed in more detail in Appendix C. Substituting (5.35) into (5.32), and using (5.33) and (5.34), allows the error function to be written in the form A matrix H is said to be positive de\ufb01nite if, and only if, Because the eigenvectors {ui} form a complete set, an arbitrary vector v can be written in the form and so H will be positive de\ufb01nite if, and only if, all of its eigenvalues are positive", "90b10eed-c578-42b9-9404-d1ecb936d3de": "Likewise, Carol only gets to start running after Bob finishes, so Bob\u2019s finishing time t; directly influences Carol\u2019s finishing time tz. bar. In other words, the distribution over b depends on the value of a. Continuing with the relay race example from section 16.1, suppose we name Alice\u2019s finishing time to, Bob\u2019s finishing time ti, and Carol\u2019s finishing time to. As we saw earlier, our estimate of t; depends on to.\n\nOur estimate of tz depends directly on ty but only indirectly on to. We can draw this relationship in a directed graphical model, illustrated in figure 16.2. Formally, a directed graphical model defined on variables x is defined by a directed acyclic graph whose vertices are the random variables in the model, and  eV i-d 22 ata Lee bettal", "acc60d64-01b6-4248-a275-789dae638046": "DEEP FEEDFORWARD NETWORKS  same cost, then we may analyze the computational cost in terms of the number of operations executed. Keep in mind here that we refer to an operation as the fundamental unit of our computational graph, which might actually consist of several arithmetic operations (for example, we might have a graph that treats matrix multiplication as a single operation). Computing a gradient in a graph with n nodes will never execute more than O(n?) operations or store the output of more than O(n?) operations. Here we are counting operations in the computational graph, not individual operations executed by the underlying hardware, so it is important to remember that the runtime of each operation may be highly variable. For example, multiplying two matrices that each contain millions of entries might correspond to a single operation in the graph.\n\nWe can see that computing the gradient requires at most O(n\u201d) operations because the forward propagation stage will at worst execute all n nodes in the original graph (depending on which values we want to compute, we may not need to execute the entire graph). The back-propagation algorithm adds one Jacobian-vector product, which should be expressed with O(1) nodes, per edge in the original graph", "b89e6041-d586-47ef-aeea-fb60d22d0645": "Using a Lagrange multiplier A2 to enforce the constraint, we consider the minimization of Setting the derivative with respect to U2 to zero, we obtain SU2 = A2U2 so that U2 is an eigenvector of S with eigenvalue A2. Thus any eigenvector will define a stationary point of the distortion measure. To find the value of J at the minimum, we back-substitute the solution for U2 into the distortion measure to give J = A2. We therefore obtain the minimum value of J by choosing U2 to be the eigenvector corresponding to the smaller of the two eigenvalues. Thus we should choose the principal subspace to be aligned with the eigenvector having the larger eigenvalue. This result accords with our intuition that, in order to minimize the average squared projection distance, we should choose the principal component subspace to pass through the mean of the data points and to be aligned with the directions of maximum variance.\n\nFor the case when the eigenvalues are equal, any choice of principal direction will give rise to the same value of J", "5b33600f-4e73-449d-8d03-5e2ebbeeec5b": "Since the visual quality of samples is not a reliable guide, we often also evaluate the log-likelihood that the model assigns to the test data, when this is computationally feasible. Unfortunately, in some cases the likelihood seems not to measure any attribute of the model that we really care about. For example, real-valued models of MNIST can obtain arbitrarily high likelihood by assigning arbitrarily low variance to background pixels that never change. Models and algorithms that detect these constant features can reap unlimited rewards, even though this is not a very useful thing to do. The potential to achieve a cost approaching negative infinity is present for any kind of maximum likelihood problem with real values, but it is especially problematic for generative models of MNIST because so many of the output values are trivial to predict. This strongly suggests a need for developing other ways of evaluating generative models. Theis et al. review many of the issues involved in evaluating generative models, including many of the ideas described above.\n\nThey highlight the fact that there are many different uses of generative models and that the choice of metric must match the intended use of the model", "af0f661b-46a0-492c-a83e-339c3eb3469b": "The cognitive map explanation of latent learning experiments is analogous to the claim that animals use model-based algorithms, and that environment models can be learned even without explicit rewards or penalties. Models are then used for planning when the animal is motivated by the appearance of rewards or penalties. Tolman\u2019s account of how animals learn cognitive maps was that they learn stimulusstimulus, or S\u2013S, associations by experiencing successions of stimuli as they explore an environment. In psychology this is called expectancy theory: given S\u2013S associations, the occurrence of a stimulus generates an expectation about the stimulus to come next. This is much like what control engineers call system identi\ufb01cation, in which a model of a system with unknown dynamics is learned from labeled training examples. In the simplest discrete-time versions, training examples are S\u2013S0 pairs, where S is a state and S0, the subsequent state, is the label. When S is observed, the model creates the \u201cexpectation\u201d that S0 will be observed next. Models more useful for planning involve actions as well, so that examples look like SA\u2013S0, where S0 is expected when action A is executed in state S", "0f0123bc-98ef-49cf-a055-680a49a10042": "For a more extensive discussion of \u2018kernel engineering\u2019, see Shawe-Taylor and Cristianini . We saw that the simple polynomial kernel k(x, x\u2032) = \ufffd xTx\u2032\ufffd2 contains only terms of degree two. If we consider the slightly generalized kernel k(x, x\u2032) = \ufffd xTx\u2032 + c\ufffd2 with c > 0, then the corresponding feature mapping \u03c6(x) contains concontains all monomials of order M. For instance, if x and x\u2032 are two images, then the kernel represents a particular weighted sum of all possible products of M pixels in the \ufb01rst image with M pixels in the second image. This can similarly be generalized to include all terms up to degree M by considering k(x, x\u2032) = \ufffd xTx\u2032 + c \ufffdM with c > 0.\n\nUsing the results (6.17) and (6.18) for combining kernels we see that these will all be valid kernel functions. Another commonly used kernel takes the form and is often called a \u2018Gaussian\u2019 kernel. Note, however, that in this context it is not interpreted as a probability density, and hence the normalization coef\ufb01cient is omitted", "4dd4aab9-2acb-4b3a-b558-0deb27d32cea": "5050\u20135060, 2019. Bossard, L., Guillaumin, M., and Van Gool, L. Food-101\u2013mining discriminative components with random forests. In European conference on computer vision, pp. 446\u2013461. Springer, 2014. Chen, T., Sun, Y., Shi, Y., and Hong, L. On sampling strategies for neural network-based collaborative \ufb01ltering.\n\nIn Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 767\u2013776, 2017. Chen, T., Zhai, X., Ritter, M., Lucic, M., and Houlsby, N. Selfsupervised gans via auxiliary rotation loss. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 12154\u201312163, 2019. Cimpoi, M., Maji, S., Kokkinos, I., Mohamed, S., and Vedaldi, A. Describing textures in the wild", "d0086cdf-b078-4f74-8baf-bd050694b2b1": "Our example of the cold spreading between you, your roommate, and your  colleague contains two cliques. One clique contains h, and h,. The factor for this clique can be defined by a table and might have values resembling these:  0 hy=1  he =0 2 1 he=1 1  A state of 1 indicates good health, while a state of 0 indicates poor health (having been infected with a cold). Both of you are usually healthy, so the corresponding state has the highest affinity. The state where only one of you is sick has the lowest affinity, because this is a rare state. The state where both of you are sick (because one of you has infected the other) is a higher affinity state, though still not as common as the state where both are healthy. 3A clique of the graph is a subset of nodes that are all connected to each other by an edge of the graph. CHAPTER 16", "fc33a333-b774-4f57-9bf8-f79adba3eeec": "There is one more way in which the structure of the return as a sum of rewards can be taken into account in o\u21b5-policy importance sampling, a way that may be able to reduce variance even in the absence of discounting (that is, even if \u03b3 = 1). In the o\u21b5-policy estimators (5.5) and (5.6), each term of the sum in the numerator is itself a sum: The o\u21b5-policy estimators rely on the expected values of these terms, which can be written in a simpler way. Note that each sub-term of (5.11) is a product of a random reward and a random importance-sampling ratio. For example, the \ufb01rst sub-term can be written, using (5.3), as Of all these factors, one might suspect that only the \ufb01rst and the last (the reward) are related; all the others are for events that occurred after the reward", "73402c87-8963-42e0-abfa-5204c5950a72": "Contrastive Representation Learning | Lil'Log   Posts Archive Search Tags FAQ emojisearch.app  Contrastive Representation Learning  Table of Contents  The goal of contrastive representation learning is to learn such an embedding space in which similar sample pairs stay close to each other while dissimilar ones are far apart. Contrastive learning can be applied to both supervised and unsupervised settings. When working with unsupervised data, contrastive learning is one of the most powerful approaches in self-supervised learning. Contrastive Training Objectives  In early versions of loss functions for contrastive learning, only one positive and one negative sample are involved. The trend in recent training objectives is to include multiple positive and negative pairs in one batch. Contrastive Loss  Contrastive loss  is one of the earliest training objectives used for deep metric learning in a contrastive fashion. Given a list of input samples {x;}, each has a corresponding label y; \u20ac {1, see , L} among L classes. We would like to learn a function fg(.) : \u00a5 > R\u00a2 that encodes 2; into an embedding vector such that examples from the same class have similar embeddings and samples from different classes have very different ones", "14068a5e-e21b-4c7b-99bd-d8cee9fb2e37": "Next a sample value is drawn from the envelope distribution. This is straightforward because the log of the envelope distribution is a succession Exercise 11.9 of linear functions, and hence the envelope distribution itself comprises a piecewise exponential distribution of the form Once a sample has been drawn, the usual rejection criterion can be applied. If the sample is accepted, then it will be a draw from the desired distribution. If, however, the sample is rejected, then it is incorporated into the set of grid points, a new tangent line is computed, and the envelope function is thereby re\ufb01ned.\n\nAs the number of grid points increases, so the envelope function becomes a better approximation of the desired distribution p(z) and the probability of rejection decreases. A variant of the algorithm exists that avoids the evaluation of derivatives (Gilks, 1992). The adaptive rejection sampling framework can also be extended to distributions that are not log concave, simply by following each rejection sampling step with a Metropolis-Hastings step (to be discussed in Section 11.2.2), giving rise to adaptive rejection Metropolis sampling . Clearly for rejection sampling to be of practical value, we require that the comparison function be close to the required distribution so that the rate of rejection is kept to a minimum", "6f2d7b1f-e8f5-44b8-a25a-7eb898a76539": "Wei, C.-H., Peng, Y., Leaman, R., P, D.A., Mattingly, C.J., Li, J., Wiegers, T., Lu, Z.: Overview of the BioCreative V chemical disease relation (CDR) task. In: BioCreative Challenge Evaluation Workshop  61. Worldwide semiannual cognitive/arti\ufb01cial intelligence systems spending guide. Technical report, International Data Corporation  62. Wu, S., Hsiao, L., Cheng, X., Hancock, B., Rekatsinas, T., Levis, P., R\u00e9, C.: Fonduer: Knowledge base construction from richly formatted data. In: Proceedings of the 2018 International Conference on Management of Data, pp. 1301\u20131316. ACM  63. Yuen, M.-C., King, I., Leung, K.-S.: A survey of crowdsourcing systems. In: Privacy, Security, Risk and Trust (PASSAT) and International Conference on Social Computing (SocialCom)  64", "7d42680c-93b1-4758-8528-8a1f3c1a8771": "In a practical application, however, we must often make a speci\ufb01c prediction for the value of t, or more generally take a speci\ufb01c action based on our understanding of the values t is likely to take, and this aspect is the subject of decision theory.\n\nConsider, for example, a medical diagnosis problem in which we have taken an X-ray image of a patient, and we wish to determine whether the patient has cancer or not. In this case, the input vector x is the set of pixel intensities in the image, and output variable t will represent the presence of cancer, which we denote by the class C1, or the absence of cancer, which we denote by the class C2. We might, for instance, choose t to be a binary variable such that t = 0 corresponds to class C1 and t = 1 corresponds to class C2. We shall see later that this choice of label values is particularly convenient for probabilistic models. The general inference problem then involves determining the joint distribution p(x, Ck), or equivalently p(x, t), which gives us the most complete probabilistic description of the situation", "f61a1b3c-e4a8-4e9e-83da-5b75a8337aa5": "STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    ID  Figure 16.4: This graph implies that p(a,b,c,d,e,f) can be written as Z bab (a, b) b,c (b, c)Ga,a(a, d)bb,c(b, \u20ac) de,e(e, f) for an appropriate choice of the @ func- tions.\n\nTo complete the model, we would need to also define a similar factor for the clique containing h, and h,. 16.2.3. The Partition Function  While the unnormalized probability distribution is guaranteed to be nonnegative everywhere, it is not guaranteed to sum or integrate to 1. To obtain a valid probability distribution, we must use the corresponding normalized probability distribution* A  p(x) =F D(x), (16.4) where Z is the value that results in the probability distribution summing or integrating to 1:  Z= [peer (16.5)  You can think of Z as a constant when the \u00a2 functions are held constant", "c613e72a-d490-49d4-88da-8e505cc10eb4": "A natural form of experience that has encoded the concept of \u2018sentiment\u2019 is a pretrained sentiment classi\ufb01er, which can be used to measure the likelihood (plausibility) that the given sentence y has the given sentiment x (Hu which trains the target model p\u03b8 by encouraging it to mimic the source model outputs (and thus the source model is also called \u2018teacher\u2019 model\u2014in a similar sense to but not to be confused with the teacher-student mechanism for optimization described in Sections 3 and 7). We now turn to the divergence term D(q, p\u03b8) that measures the distance between the auxiliary distribution q and the model distribution p\u03b8 in the SE. The discussion in the prior section has assumed speci\ufb01c case of D being the cross entropy.\n\nYet there is a rather rich set of choices for the divergence function, such as f-divergence (e.g., KL divergence, Jensen-Shannon divergence), optimal transport distance (e.g., Wasserstein distance), and so on", "e12f2fae-4035-4b73-a8da-7633e91b0788": "However, we do not claim that this is a new method to quantitatively evaluate generative models yet. The constant scaling factor that depends on the critic\u2019s architecture means it\u2019s hard to compare models with di\ufb00erent critics.\n\nEven more, in practice the fact that the critic doesn\u2019t have in\ufb01nite capacity makes it hard to know just how close to the EM distance our estimate really is. This being said, we have succesfully used the loss metric to validate our experiments repeatedly and without failure, and we see this as a huge improvement in training GANs which previously had no such facility. In contrast, Figure 4 plots the evolution of the GAN estimate of the JS distance during GAN training. More precisely, during GAN training, the discriminator is trained to maximize L(D, g\u03b8) = Ex\u223cPr + Ex\u223cP\u03b8 which is is a lower bound of 2JS(Pr, P\u03b8)\u22122 log 2. In the \ufb01gure, we plot the quantity 1 2L(D, g\u03b8) + log 2, which is a lower bound of the JS distance. This quantity clearly correlates poorly the sample quality. Note also that the JS estimate usually stays constant or goes up instead of going down", "cc2dc851-6c35-4c07-8ac3-e1aee6aea308": ", xD) represented by a directed graph having D nodes, and consider the conditional distribution of a particular node with variables xi conditioned on all of the remaining variables xj\u0338=i.\n\nUsing the factorization property (8.5), we can express this conditional distribution in the form in which the integral is replaced by a summation in the case of discrete variables. We now observe that any factor p(xk|pak) that does not have any functional dependence on xi can be taken outside the integral over xi, and will therefore cancel between numerator and denominator. The only factors that remain will be the conditional distribution p(xi|pai) for node xi itself, together with the conditional distributions for any nodes xk such that node xi is in the conditioning set of p(xk|pak), in other words for which xi is a parent of xk", "96839791-3d43-4747-84bf-747076be5165": "The state unit has a linear self-loop whose weight is controlled by the forget gate. The output of the cell can be shut off by the output gate. All the gating units have a sigmoid nonlinearity, while the input unit can have any squashing nonlinearity. The state unit can also be used as an extra input to the gating units. The black square indicates a delay of a single time step.\n\nforget gate output gate  https://www.deeplearningbook.org/contents/rnn.html    405  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  architecture. Deeper architectures have also been successfully used . Instead of a unit that simply applies an element-wise nonlinearity to the affine transformation of inputs and recurrent units, LSTM recurrent networks have \u201cLSTM cells\u201d that have an internal recurrence (a self-loop), in addition to the outer recurrence of the RNN. Each cell has the same inputs and outputs as an ordinary recurrent network, but also has more parameters and a system of gating units that controls the flow of information", "c679efeb-bbad-4f8d-bfea-9e26a17dc0cc": "All of the ideas go through in the discounted case with appropriate adjustments (including to the box on page 199) but involve additional complexity that distracts from the main ideas. \u21e4Exercise 13.2 Generalize the box on page 199, the policy gradient theorem (13.5), the proof of the policy gradient theorem (page 325), and the steps leading to the REINFORCE update equation (13.8), so that (13.8) ends up with a factor of \u03b3t and thus aligns with the general algorithm given in the pseudocode. \u21e4 As a stochastic gradient method, REINFORCE has good theoretical convergence properties. By construction, the expected update over an episode is in the same direction as the performance gradient. This assures an improvement in expected performance for su\ufb03ciently small \u21b5, and convergence to a local optimum under standard stochastic approximation conditions for decreasing \u21b5.\n\nHowever, as a Monte Carlo method REINFORCE may be of high variance and thus produce slow learning. Exercise 13.3 In Section 13.1 we considered policy parameterizations using the soft-max in action preferences (13.2) with linear action preferences (13.3)", "e78eac71-934b-43b5-bed2-d9b8e20d955b": "For every setup, we \ufb01ne-tune the model with the same seed as the dataset seed (in contrast to many works which report the max across different seeds). The detailed experimental setup is described in the Appendix. 2The results for 100 labeled data points per class are shown in the Appendix. every time; in the semi-supervised settings, sentence level augmentations (round-trip translation) works the best, getting the highest or second highest score every time. This makes sense since for many classi\ufb01cation tasks, multiple words indicate the label, and so dropping several words will not affect the label. Inference Tasks. As shown in Table 3, we observe that token-level augmentations work the best overall (e.g., random insertion, random deletion, and word replacement) for both supervised and semi-supervised settings. This is a bit surprising since the inference tasks usually heavily depend on several words, and changing these words can easily change the label for inferene tasks. Similarity and Paraphrase Tasks.\n\nFrom Table 3, in the supervised settings, we observe that token-level augmentations (random swapping) achieve the best performances, while hidden space augmentations work well in semisupervised settings, with cutoff performing the best on average", "6b20f862-5a74-42f7-9d9a-c44213aae382": "We sometimes annotate the output node with the name of the operation applied, and other times omit this label when the operation is clear from context. Examples of computational graphs are shown in figure 6.8. 6.5.2 Chain Rule of Calculus  The chain rule of calculus (not to be confused with the chain rule of probability) is used to compute the derivatives of functions formed by composing other functions whose derivatives are known. Back-propagation is an algorithm that computes the chain rule, with a specific order of operations that is highly efficient. Let x be a real number, and let f and g both be functions mapping from a real number to a real number. Suppose that y = g(r) and z= f(g(x)) = f(y). Then  201  https://www.deeplearningbook.org/contents/mlp.html    CHAPTER 6. DEEP FEEDFORWARD NETWORKS  (.) + (.\u00b0) dot x  Figure 6.8: Examples of computational graphs. (a) The graph using the x operation to compute z = ry", "feaea9d0-23e8-4a89-a544-82859e921eed": "In Section 3.3.3, we saw that the prediction of a linear regression model for a new input x takes the form of a linear combination of the training set target values with coef\ufb01cients given by the \u2018equivalent kernel\u2019 (3.62) where the equivalent kernel satis\ufb01es the summation constraint (3.64). We can motivate the kernel regression model (3.61) from a different perspective, starting with kernel density estimation. Suppose we have a training set {xn, tn} and we use a Parzen density estimator to model the joint distribution p(x, t), so that Section 2.5.1 where f(x, t) is the component density function, and there is one such component centred on each data point. We now \ufb01nd an expression for the regression function y(x), corresponding to the conditional average of the target variable conditioned on for all values of x. Using a simple change of variable, we then obtain where n, m = 1,", "287bb1fb-73b1-4417-b27f-926ad2fb596b": ", xN}.\n\nWe can create a new data set XB by drawing N points at random from X, with replacement, so that some points in X may be replicated in XB, whereas other points in X may be absent from XB. This process can be repeated L times to generate L data sets each of size N and each obtained by sampling from the original data set X. The statistical accuracy of parameter estimates can then be evaluated by looking at the variability of predictions between the different bootstrap data sets. One advantage of the Bayesian viewpoint is that the inclusion of prior knowledge arises naturally. Suppose, for instance, that a fair-looking coin is tossed three times and lands heads each time. A classical maximum likelihood estimate of the probability of landing heads would give 1, implying that all future tosses will land Section 2.1 heads! By contrast, a Bayesian approach with any reasonable prior will lead to a much less extreme conclusion. There has been much controversy and debate associated with the relative merits of the frequentist and Bayesian paradigms, which have not been helped by the fact that there is no unique frequentist, or even Bayesian, viewpoint", "0af9b331-b697-4ae2-802a-eb0c11d43186": "7.13  Adversarial Training  In many cases, neural networks have begun to reach human performance when evaluated on an i.i.d. test set. It is natural therefore to wonder whether these models have obtained a true human-level understanding of these tasks. To probe the level of understanding a network has of the underlying task, we can search for examples that the model misclassifies. Szegedy ef al. found that even neural networks that perform at human level accuracy have a nearly 100 percent error rate on examples that are intentionally constructed by using an optimization procedure to search for an input 2\u2019 near a data point a such that the model output is very different at a\u2019", "75e3f5b9-38f1-4138-a432-5f183a4d760a": "(19.45)  JAG In this reformulation, ( see the input on step as consisting gf W. j hy _ re . U-Sri4i  https://www.deeplearningbook.org/contents/inference.html    rather than UV. We can thus think of unit ? as attempting to encode The residual error in U given the code of the other units. We can thus think of sparse coding as an  642  CHAPTER 19. APPROXIMATE INFERENCE  iterative autoencoder, which repeatedly encodes and decodes its input, attempting to fix mistakes in the reconstruction after each iteration. In this example, we have derived an update rule that updates a single unit at a time. It would be advantageous to be able to update more units simultaneously. Some graphical models, such as deep Boltzmann machines, are structured in such a way that we can solve for many entries of h simultaneously. Unfortunately, binary sparse coding does not admit such block updates. Instead, we can use a heuristic technique called damping to perform block updates", "7c037cf2-bd78-4518-8b37-3639bd0ee0ed": "In this book we call Rt the \u2018reward signal at time t,\u2019 or sometimes just the \u2018reward at time t,\u2019 but we do not think of it as an object or event in the agent\u2019s environment. Because Rt is a number\u2014not an object or an event\u2014it is more like a reward signal in neuroscience, which is a signal internal to the brain, like the activity of neurons, that in\ufb02uences decision making and learning.\n\nThis signal might be triggered when the animal perceives an attractive (or an aversive) object, but it can also be triggered by things that do not physically exist in the animal\u2019s external environment, such as memories, ideas, or hallucinations. Because our Rt can be positive, negative, or zero, it might be better to call a negative Rt a penalty, and an Rt equal to zero a neutral signal, but for simplicity we generally avoid these terms. In reinforcement learning, the process that generates all the Rts de\ufb01nes the problem the agent is trying to solve. The agent\u2019s objective is to keep the magnitude of Rt as large as possible over time", "8c743f6d-4253-4a78-9db5-0d9ceaf011dc": "This is more general than the corresponding least-squares result because the variance is a function of x. We have seen that for multimodal distributions, the conditional mean can give a poor representation of the data. For instance, in controlling the simple robot arm shown in Figure 5.18, we need to pick one of the two possible joint angle settings in order to achieve the desired end-effector location, whereas the average of the two solutions is not itself a solution. In such cases, the conditional mode may be of more value. Because the conditional mode for the mixture density network does not have a simple analytical solution, this would require numerical iteration.\n\nA simple alternative is to take the mean of the most probable component (i.e., the one with the largest mixing coef\ufb01cient) at each value of x. This is shown for the toy data set in Figure 5.21(d). So far, our discussion of neural networks has focussed on the use of maximum likelihood to determine the network parameters (weights and biases). Regularized maximum likelihood can be interpreted as a MAP (maximum posterior) approach in which the regularizer can be viewed as the logarithm of a prior parameter distribution", "48c21f85-a954-411c-aa5e-9b4eed77c313": "We further consider the speci\ufb01c setting where we submit a \u2018soft\u2019 prediction p\u03c4(t), that is, the distribution (a vector of normalized weights) over the K experts, and receive the rewards r\u03c4(t) \u2208 R for each expert t. The goal of the learning problem is then to update the distribution p\u03c4(t) at every step \u03c4 for minimal regret \ufffdT \u03c4=1 r\u03c4(t\u2217) \u2212 \ufffdT \u03c4=1 Ep\u03c4 (t), where t\u2217 is the best single expert. The rewards from the environment naturally serves as the dynamic experience: In the following, we show that the SE rediscovers the classical multiplicative weights (MW), or Hedge algorithm  to the above online problem. Similar ideas of multiplicative weights have been widely used in diverse \ufb01elds such as optimization, game theory, and economics . Multiplicative weights", "8fb44230-c7c9-4ac6-ab4a-ccfbaa802870": "REPRESENTATION LEARNING  Algorithm 15.1 Greedy layer-wise unsupervised pretraining protocol  Given the following: Unsupervised feature learning algorithm \u00a3, which takes a training set of examples and returns an encoder or feature function f. The raw input data is X, with one row per example, and fH) (X) is the output of the first stage encoder on X. In the case where fine-tuning is performed, we use a learner T, which takes an initial function f, input examples X (and in the supervised fine-tuning case, associated targets Y), and returns a tuned function. The number of stages is m.  f < Identity function X=xX  end for  if fine-tuning then feT X.Y)  end if  Return /  2006; Bengio et al., 2007; Ranzato et al., 2007a).\n\nOn many other tasks, however, unsupervised pretraining either does not confer a benefit or even causes noticeable harm. Ma et al", "952d0978-a591-431c-82be-f087822c2b10": "Essentially, modes in the model  608  CHAPTER 18.\n\nCONFRONTING THE PARTITION FUNCTION  \u2014  Pmodel (x) \u00a9 \u00a9 Pdata(x)  Figure 18.2: A spurious mode. An illustration of how the negative phase of contrastive divergence (algorithm 18.2) can fail to suppress spurious modes. A spurious mode is a mode that is present in the model distribution but absent in the data distribution. Because contrastive divergence initializes its Markov chains from data points and runs the Markov chain for only a few steps, it is unlikely to visit modes in the model that are far from the data points. This means that when sampling from the model, we will sometimes  1 ayou 1 rroad ta TO ayooa od 1 at c  https://www.deeplearningbook.org/contents/partition.html    BEL Sal ples | tLudat GO LOL resellivie Lue Gala. 1b alsV ilealis tllatb GUE LO Waslllg SOME OL its probability mass on these modes, the model will struggle to place high probability mass on the correct modes", "5d371925-8226-438d-b0bc-037a5efb3213": "for a comparison of the performance of centered  DBMs with and without the use of partial mean field in the negative phase. 20.4.5 Jointly Training Deep Boltzmann Machines  Classic DBMs require greedy unsupervised pretraining and, to perform classification well, require a separate MLP-based classifier on top of the hidden features they extract. This has some undesirable properties. It is hard to track performance during training because we cannot evaluate properties of the full DBM while training the first RBM. Thus, it is hard to tell how well our hyperparameters are working until quite late in the training process. Software implementations of DBMs need to have many different components for CD training of individual RBMs, PCD training of the full DBM, and training based on back-propagation  668  CHAPTER 20. DEEP GENERATIVE MODELS  through the MLP.\n\nFinally, the MLP on top of the Boltzmann machine loses many of the advantages of the Boltzmann machine probabilistic model, such as being able to perform inference when some input values are missing. There are two main ways to resolve the joint training problem of the deep  Bl aBe 2", "86f61f04-1228-4335-b864-ff5cd0b81bd9": "for examples of modern uses of the REINFORCE algorithm with reduced variance in the context of deep learning.\n\nIn addition to the use of an input-dependent baseline bW), Mnih and Gregor  found that the scale of (J(y) \u2014 b(w)) could be adjusted during training by dividing it by its standard deviation estimated by a moving average during training, as a kind of adaptive learning rate, to counter the effect of important variations that occur during the course of training in the magnitude of this quantity. Mnih and Gregor  called this heuristic variance normalization. REINFORCE-based estimators can be understood as estimating the gradient by correlating choices of y with corresponding values of J(y). If a good value of y is unlikely under the current parametrization, it might take a long time to obtain it by chance and get the required signal that this configuration should be reinforced. 20.10 Directed Generative Nets  As discussed in chapter 16, directed graphical models make up a prominent class of graphical models. While directed graphical models have been very popular within the greater machine learning community, within the smaller deep learning community they have until roughly 2013 been overshadowed by undirected models such as the RBM", "fe1cd843-006b-423e-865e-be924898e1b7": "Based on the goal of the task, we use the following intuitive rewards to ensure entailment accuracy and language quality: (1) a robust entailment classi\ufb01er  that measures the entailment score of a generation in terms of the input premise, (2) a GPT-2 language model  that measures the log-likelihood of the generation as an indicator of language quality, and (3) BLEU score w.r.t the input premises as another language quality reward that avoids trivial outputs. We sum together all rewards with weights 1.0. We study the task of attacking an entailment classi\ufb01er. In particular, we aim to attack one of the most popular entailment classi\ufb01ers on HuggingFaceHub.13 The attack generation model generates adversarial text without conditioning on any inputs so that the generated attacks are universal to all premises. The generation model is trained with mostly the same setting as in \u00a74.1, where the entailment classi\ufb01er to be attacked is used as entailment score reward functions. Besides, we additionally include a token-level repetition penalty reward, which empirically bene\ufb01ts readability. Finally, we use the MultiNLI dataset  which includes more diverse examples than the SNLI used above", "eb2206b8-26af-4b45-81df-6bec893b0e04": "Preliminaries: The Maximum Entropy View of Learning and Inference Depending on the nature of the task (e.g., classi\ufb01cation or regression), data (e.g., labeled or unlabeled), information scope (e.g., with or without latent variables), and form of domain knowledge (e.g., prior distributions or parameter constraints), and so on, di\ufb00erent learning paradigms with often complementary (but not necessarily easy to combine) advantages have been developed for di\ufb00erent needs.\n\nFor example, the paradigms built on the maximum likelihood principles, Bayesian theories, variational calculus, and Monte Carlo simulation have led to much of the foundation underlying a wide spectrum of probabilistic graphical models, exact/approximate inference algorithms, and even probabilistic logic programs suitable for probabilistic inference and parameter estimations in multivariate, structured, and fully or partially observed domains, while the paradigms built on convex optimization, duality theory, regularization, and risk minimization have led to much of the foundation underlying algorithms such as support vector machine (SVM), boosting, sparse learning, structure learning, and so on", "7f9cc45d-6413-49e0-9b07-060aa95f85a9": "As shown in section 4, we \ufb01nd it bene\ufb01cial to de\ufb01ne the contrastive loss on zi\u2019s rather than hi\u2019s. \u2022 A contrastive loss function de\ufb01ned for a contrastive prediction task. Given a set {\u02dcxk} including a positive pair of examples \u02dcxi and \u02dcxj, the contrastive prediction task aims to identify \u02dcxj in {\u02dcxk}k\u0338=i for a given \u02dcxi. We randomly sample a minibatch of N examples and de\ufb01ne the contrastive prediction task on pairs of augmented examples derived from the minibatch, resulting in 2N data points. We do not sample negative examples explicitly. Instead, given a positive pair, similar to , we treat the other 2(N \u2212 1) augmented examples within a minibatch as negative examples. Let sim(u, v) = u\u22a4v/\u2225u\u2225\u2225v\u2225 denote the dot product between \u21132 normalized u and v (i.e. cosine similarity)", "72d8e68e-8dcf-488c-b2a3-6076073ab610": "Larger initial weights will yield a stronger symmetry-breaking effect, helping to avoid redundant units. They also help to avoid losing signal during forward or back-propagation through the linear component of each layer\u2014larger values in the matrix result in larger outputs of matrix multiplication.\n\nInitial weights that are too large may, however, result in exploding values during forward propagation or back-propagation. In recurrent networks, large weights can also result in chaos (such extreme sensitivity to small perturbations of the input that the behavior  Af 4A Antnnninintin fanned nnn nn wn tinn nnn nn denn nnn nner wen dn) TRH 24  https://www.deeplearningbook.org/contents/optimization.html    VL LUO UCLELILIIIIIDSLIC LULWaLU plupa} AULLULL PpLlLUCccuuLle appeals Lauuviit). LU DSULLIC  extent, the exploding gradient problem can be mitigated by gradient clippin  (thresholding the values of the gradients before performing a gradient descent step)", "24ea5b72-6d3d-46ec-854e-fe6281ecc468": "This makes possible the construction of general purpose software for variational inference in which the form of the model does not need to be speci\ufb01ed in advance . If we now specialize to the case of a model in which all of the conditional distributions have a conjugate-exponential structure, then the variational update procedure can be cast in terms of a local message passing algorithm . In particular, the distribution associated with a particular node can be updated once that node has received messages from all of its parents and all of its children. This in turn requires that the children have already received messages from their coparents. The evaluation of the lower bound can also be simpli\ufb01ed because many of the required quantities are already evaluated as part of the message passing scheme. This distributed message passing formulation has good scaling properties and is well suited to large networks.\n\nThe variational framework discussed in Sections 10.1 and 10.2 can be considered a \u2018global\u2019 method in the sense that it directly seeks an approximation to the full posterior distribution over all random variables. An alternative \u2018local\u2019 approach involves \ufb01nding bounds on functions over individual variables or groups of variables within a model", "eb0e8517-0097-424b-8463-5fbf21fe741c": "In the M step, the bound is maximized giving the value \u03b8(new), which gives a larger value of log likelihood than \u03b8(old). The subsequent E step then constructs a bound that is tangential at \u03b8(new) as shown by the green curve. For the particular case of an independent, identically distributed data set, X will comprise N data points {xn} while Z will comprise N corresponding latent variables {zn}, where n = 1, . , N. From the independence assumption, we have p(X, Z) = \ufffd n p(xn).\n\nUsing the sum and product rules, we see that the posterior probability that is evaluated in the E step takes the form and so the posterior distribution also factorizes with respect to n. In the case of the Gaussian mixture model this simply says that the responsibility that each of the mixture components takes for a particular data point xn depends only on the value of xn and on the parameters \u03b8 of the mixture components, not on the values of the other data points", "9988f7f3-a1c6-41c3-8955-5716ffd10bea": "Although there is no longer any evidence of the carving, there is now a stone plaque on the bridge commemorating the discovery and displaying the quaternion equations. During the evolution of this dynamical system, the value of the Hamiltonian H is constant, as is easily seen by differentiation A second important property of Hamiltonian dynamical systems, known as Liouville\u2019s Theorem, is that they preserve volume in phase space.\n\nIn other words, if we consider a region within the space of variables (z, r), then as this region evolves under the equations of Hamiltonian dynamics, its shape may change but its volume will not. This can be seen by noting that the \ufb02ow \ufb01eld (rate of change of location in phase space) is given by and that the divergence of this \ufb01eld vanishes Now consider the joint distribution over phase space whose total energy is the Hamiltonian, i.e., the distribution given by Using the two results of conservation of volume and conservation of H, it follows that the Hamiltonian dynamics will leave p(z, r) invariant. This can be seen by considering a small region of phase space over which H is approximately constant", "11fc9121-c3fc-4785-aea2-bdfb31ee95ef": "Note that neither \u00a5V nor \u00a5 need to be symmetric (al- though they are square and real), so they can have complex-valued eigenvalues and eigenvectors, with imaginary components corresponding to potentially oscillatory behavior (if the same Jacobian was applied iteratively). Even though h\u2122 or a  small variation of h of interest in back-propagation are real valued, they can be expressed in such a complex-valued basis. What matters is what happens to the magnitude (complex absolute value) of these possibly complex-valued ba- sis coefficients when we multiply the matrix by the vector.\n\nAn eigenvalue with magnitude greater than one corresponds to magnification (exponential growth, if applied iteratively) while a magnitude smaller than one corresponds to shrinking (exponential decay, if applied iteratively). With a nonlinear map, the Jacobian is free to change at each step. The dynamics therefore become more complicated. It remains true, however, that a small initial variation can turn into a large variation after several steps", "9e47c16e-c036-4ab9-85b0-1e7f3372cd55": "Although the main technical achievement demonstrated by Watson was its ability to quickly and accurately answer natural language questions over broad areas of general knowledge, its winning Jeopardy! performance also relied on sophisticated decision-making strategies for critical parts of the game.\n\nTesauro, Gondek, Lechner, Fan, and Prager  adapted Tesauro\u2019s TD-Gammon system described above to create the strategy used by Watson in \u201cDaily-Double\u201d (DD) wagering in its celebrated winning performance against human champions. These authors report that the e\u21b5ectiveness of this wagering strategy went well beyond what human players are able to do in live game play, and that it, along with other advanced strategies, was an important contributor to Watson\u2019s impressive winning performance. Here we focus only on DD wagering because it is the component of Watson that owes the most to reinforcement learning. Jeopardy! is played by three contestants who face a board showing 30 squares, each of which hides a clue and has a dollar value. The squares are arranged in six columns, each corresponding to a di\u21b5erent category", "140ea5cb-29ab-4db8-b4f2-e1a8fd64d850": "Unfortunately, this alternative formulation does not seem to improve convergence in practice, possibly because of suboptimality of the discriminator or high variance around the expected gradient.\n\nIn realistic experiments, the best-performing formulation of the GAN game is a different formulation that is neither zero-sum nor equivalent to maximum  https://www.deeplearningbook.org/contents/generative_models.html    likelihood, introduced by Goodfellow et al. with a heuristic motivation. In this best-performing formulation, the generator aims to increase the log-probability that the discriminator makes a mistake, rather than aiming to decrease the log- probability that the discriminator makes the correct prediction. This reformulation is motivated solely by the observation that it causes the derivative of the generator\u2019s cost function with respect to the discriminator\u2019s logits to remain large even in the situation when the discriminator confidently rejects all generator samples. Stabilization of GAN learning remains an open problem. Fortunately, GAN learning performs well when the model architecture and hyperparameters are care- fully selected. Radford et al", "3a37bd63-da6e-47ca-924e-d499cbee254f": "No matter what kind of unsupervised learning algorithm or what model type is employed, in most cases, the overall training scheme is nearly the same. While the choice of unsupervised learning algorithm will obviously affect the details, most applications of unsupervised pretraining follow this basic protocol. Greedy layer-wise unsupervised pretraining can also be used as initialization for other unsupervised learning algorithms, such as deep autoencoders  and probabilistic models with many layers of latent variables.\n\nSuch models include deep belief networks  and deep Boltzmann machines . These deep generative models are described in chapter 20.  https://www.deeplearningbook.org/contents/representation.html  As discussed in section 8.7.4, it is also possible to have greedy layer-wise supervised pretraining. This builds on the premise that training a shallow network is easier than training a deep one, which seems to have been validated in several contexts . 15.1.1 When and Why Does Unsupervised Pretraining Work? On many tasks, greedy layer-wise unsupervised pretraining can yield substantial improvements in test error for classification tasks. This observation was responsible for the renewed interested in deep neural networks starting in 2006 (Hinton ef al.,  527  CHAPTER 15", "0998d43e-37e6-4c02-988f-2f15dc559b47": "The survey by Kumar  provides a good discussion of Bayesian and non-Bayesian approaches to these problems. The term information state comes from the literature on partially observable MDPs; see, e.g., Lovejoy . Other theoretical research focuses on the e\ufb03ciency of exploration, usually expressed as how quickly an algorithm can approach an optimal decision-making policy. One way to formalize exploration e\ufb03ciency is by adapting to reinforcement learning the notion of sample complexity for a supervised learning algorithm, which is the number of training examples the algorithm needs to attain a desired degree of accuracy in learning the target function. A de\ufb01nition of the sample complexity of exploration for a reinforcement learning algorithm is the number of time steps in which the algorithm does not select near-optimal actions . Li  discusses this and several other approaches in a survey of theoretical approaches to exploration e\ufb03ciency in reinforcement learning. A thorough modern treatment of Thompson sampling is provided by Russo, Van Roy, Kazerouni, Osband, and Wen .\n\nIn this chapter we introduce the formal problem of \ufb01nite Markov decision processes, or \ufb01nite MDPs, which we try to solve in the rest of the book", "f36bfd7f-44e0-4ef5-b906-5ab8a0a1e8b4": "Alternately, we could imagine that this is a truncated Taylor series approximating the cost function of a more sophisticated model. The gradient in this setting is given by  VwJ(w) = H(w \u2014w%), (7.21) where, again, H is the Hessian matrix of J with respect to w evaluated at w*. Because the L! penalty does not admit clean algebraic expressions in the case of a fully general Hessian, we will also make the further simplifying assumption that the Hessian is diagonal, H = diag(), where each Hi; > 0.  ? As with L? regularization, we could regularize the parameters toward a value that is not zero, but instead toward some parameter value w). In that case the Lt regularization would introduce the term (0) = ||w \u2014 w ||. = DY |wi w\\? |. 231  CHAPTER 7.\n\nREGULARIZATION FOR DEEP LEARNING  This assumption holds if the data for the linear regression problem has been preprocessed to remove all correlation between the input features, which may be accomplished using PCA", "71b20c6e-2b51-4730-8f2d-0d6364ab93c2": "de\ufb01ned \u201cExpected Sarsa\u201d to be an on-policy method exclusively (as we did in the \ufb01rst edition), whereas now we use this name for the general algorithm in which the target and behavior policies may di\u21b5er.\n\nThe general o\u21b5-policy view of Expected Sarsa was noted by van Hasselt , who called it \u201cGeneral Q-learning.\u201d In this chapter we unify the Monte Carlo (MC) methods and the one-step temporaldi\u21b5erence (TD) methods presented in the previous two chapters. Neither MC methods nor one-step TD methods are always the best. In this chapter we present n-step TD methods that generalize both methods so that one can shift from one to the other smoothly as needed to meet the demands of a particular task. n-step methods span a spectrum with MC methods at one end and one-step TD methods at the other. The best methods are often intermediate between the two extremes. Another way of looking at the bene\ufb01ts of n-step methods is that they free you from the tyranny of the time step. With one-step TD methods the same time step determines how often the action can be changed and the time interval over which bootstrapping is done", "fa4ae9d4-0114-4829-afd2-4cadd0734888": "Sch\u00a8olkopf, B., J. Platt, J. Shawe-Taylor, A. Smola, and R. C. Williamson . Estimating the support of a high-dimensional distribution. Neural Computation 13(7), 1433\u20131471. Sch\u00a8olkopf, B., A. Smola, R. C. Williamson, and P. L. Bartlett . New support vector algorithms. Neural Computation 12(5), 1207\u20131245. Schwarz, G. Estimating the dimension of a model. Annals of Statistics 6, 461\u2013464. Seeger, M. Bayesian Gaussian Process Models: PAC-Bayesian Generalization Error Bounds and Sparse Approximations. Ph. D. thesis, University of Edinburg. Seeger, M., C. K. I. Williams, and N. Lawrence", "e965a20d-9504-4cc7-93f1-3d0fb1aad3eb": "(d) After several steps of training, if G and D have enough capacity, they will reach a point at which both cannot improve because pg = pdata.\n\nThe discriminator is unable to differentiate between the two distributions, i.e. D(x) = 1 Algorithm 1 Minibatch stochastic gradient descent training of generative adversarial nets. The number of steps to apply to the discriminator, k, is a hyperparameter. We used k = 1, the least expensive option, in our experiments. \u2022 Sample minibatch of m noise samples {z(1), . , z(m)} from noise prior pg(z). \u2022 Sample minibatch of m examples {x(1), . , x(m)} from data generating distribution pdata(x). \u2022 Update the discriminator by ascending its stochastic gradient: We \ufb01rst consider the optimal discriminator D for any given generator G. Proposition 1. For G \ufb01xed, the optimal discriminator D is Proof", "c265821d-1f45-4cc9-952c-84e44280cf33": "Instead, the most computationally demanding steps are those involving sums over the data set that are 0 (NDM). For large D, and M \u00ab D, this can be a significant saving compared to 0 (ND 2 ) and can offset the iterative nature of the EM algorithm. Note that this EM algorithm can be implemented in an on-line form in which each D-dimensional data point is read in and processed and then discarded before the next data point is considered. To see this, note that the quantities evaluated in the E step (an M-dimensional vector and an M x M matrix) can be computed for each data point separately, and in the M step we need to accumulate sums over data points, which we can do incrementally. This approach can be advantageous if both Nand D are large.\n\nBecause we now have a fully probabilistic model for PCA, we can deal with missing data, provided that it is missing at random, by marginalizing over the distribution of the unobserved variables. Again these missing values can be treated using the EM algorithm. We give an example of the use of this approach for data visualization in Figure 12.11", "bee066a4-9153-4d79-877e-237404e074ba": "\u2018Token embeddings (A) Tenis Onn arco  Positive scores  fe]  Local representation (A)  Sentence representation (A)   uoo ue Buyjno si uewom y  :4 coucueS  seun6 e Burke si wb y  *\\v eouelueS  iS]  a  o g  i Token embeddings (8) \u2014\u2014 3  3 Pooling a Negative scores Local representation (B) I \u2014\u2014\u2014 q Concat  IS-BERT works as follows:  Use BERT to encode an input sentence s to a token embedding of length 1, h,.;. Then apply 1-D conv net with different kernel sizes (e.g. 1, 3, 5) to process the token embedding sequence to capture the n-gram local contextual dependencies: \u00a2; = ReLU(w - h;. ;.4_1; +b) . The output sequences are padded to stay the same sizes of the inputs. The final local representation of the i-th token F(x) is the concatenation of representations of different kernel sizes", "efb74310-9c75-438d-80d4-fd95719668e8": "Every past observation y may influence the conditional distribution of some y (for t >\u201c), given the previous values. Parametrizing the graphical model directly according to this graph (as in equation 10.6) might be very inefficient, with an ever growing number of inputs and parameters for each element of the sequence. RNNs obtain the same full connectivity but efficient parametrization, as illustrated in figure 10.8. The edges in a graphical model indicate which variables depend directly on other variables.\n\nMany graphical models aim to achieve statistical and computational efficiency by omitting edges that do not correspond to strong interactions. For example, it is common to make the Markov assumption that the graphical model should contain only edges from {y-*),...,y\u00a2-)} to y, rather than containing edges from the entire history. In some cases, however, we believe that all past inputs should have an influence on the next element of the sequence", "f2f3d166-c2a8-436e-83f4-f8ee2d5b64e4": "On each \ufb02ip, the gambler must decide what portion of his capital to stake, in integer numbers of dollars.\n\nThis problem can be formulated as an undiscounted, episodic, \ufb01nite Exercise 4.8 Why does the optimal policy for the gambler\u2019s problem have such a curious form? In particular, for capital of 50 it bets it all on one \ufb02ip, but for capital of 51 it does not. Why is this a good policy? \u21e4 Exercise 4.9 (programming) Implement value iteration for the gambler\u2019s problem and solve it for ph = 0.25 and ph = 0.55. In programming, you may \ufb01nd it convenient to introduce two dummy states corresponding to termination with capital of 0 and 100, giving them values of 0 and 1 respectively. Show your results graphically, as in Figure 4.3. A major drawback to the DP methods that we have discussed so far is that they involve operations over the entire state set of the MDP, that is, they require sweeps of the state set. If the state set is very large, then even a single sweep can be prohibitively expensive. For example, the game of backgammon has over 1020 states", "d508b224-10b8-4340-8daa-6cd6639cf43d": "https://www.deeplearningbook.org/contents/optimization.html    CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  8.7.5 Designing Models to Aid Optimization  To improve optimization, the best strategy is not always to improve the optimization algorithm. Instead, many improvements in the optimization of deep models have come from designing the models to be easier to optimize. In principle, we could use activation functions that increase and decrease in jagged nonmonotonic patterns, but this would make optimization extremely difficult.\n\nIn practice, it is more important to choose a model family that is easy to optimize than to use a powerful optimization algorithm. Most of the advances in neural network learning over the past thirty years have been obtained by changing the model family rather than changing the optimization procedure. Stochastic gradient descent with momentum, which was used to train neural networks in the 1980s, remains in use in modern state-of-the-art neural network applications. Specifically, modern neural networks reflect a design choice to use linear trans- formations between layers and activation functions that are differentiable almost everywhere, with significant slope in large portions of their domain", "4a50b107-b575-45b9-9124-7f53f4feaae1": "We know that by a simple envelope theorem (, Theorem 1) that \u2207\u03b8W(Pr, P\u03b8) = \u2207\u03b8V (f, \u03b8) under the condition that the \ufb01rst and last terms are well-de\ufb01ned. The rest of the proof will be dedicated to show that when the right hand side is de\ufb01ned. For the reader who is not interested in such technicalities, he or she can skip the rest of the proof. Since f \u2208 F, we know that it is 1-Lipschitz. Furthermore, g\u03b8(z) is locally Lipschitz as a function of (\u03b8, z). Therefore, f(g\u03b8(z)) is locally Lipschitz on (\u03b8, z) with constants L(\u03b8, z) (the same ones as g). By Radamacher\u2019s Theorem, f(g\u03b8(z)) has to be di\ufb00erentiable almost everywhere for (\u03b8, z) jointly. Rewriting this, the set A = {(\u03b8, z) : f \u25e6 g is not di\ufb00erentiable} has measure 0", "38113380-e5d4-4fa7-82c6-85b80e180e21": "Finally, suppose that no cans can be collected during a run home for recharging, and that no cans can be collected on a step in which the battery is depleted.\n\nThis system is then a \ufb01nite MDP, and we can write down the transition probabilities and the expected rewards, with dynamics as indicated in the table on the left: Note that there is a row in the table for each possible combination of current state, s, action, a 2 A(s), and next state, s0. Some transitions have zero probability of occurring, so no expected reward is speci\ufb01ed for them. Shown on the right is another useful way of summarizing the dynamics of a \ufb01nite MDP, as a transition graph. There are two kinds of nodes: state nodes and action nodes. There is a state node for each possible state (a large open circle labeled by the name of the state), and an action node for each state\u2013action pair (a small solid circle labeled by the name of the action and connected by a line to the state node). Starting in state s and taking action a moves you along the line from state node s to action node (s, a)", "2865e907-823d-4a22-a604-48fdd9af41c3": "sian conditional distribution for t given x given by (1.60), in which the mean is given by the polynomial function y(x, w), and the precision is given by the parameter \u03b2, which is related to the variance by \u03b2\u22121 = \u03c32. We now use the training data {x, t} to determine the values of the unknown parameters w and \u03b2 by maximum likelihood. If the data are assumed to be drawn independently from the distribution (1.60), then the likelihood function is given by As we did in the case of the simple Gaussian distribution earlier, it is convenient to maximize the logarithm of the likelihood function. Substituting for the form of the Gaussian distribution, given by (1.46), we obtain the log likelihood function in the form Consider \ufb01rst the determination of the maximum likelihood solution for the polynomial coef\ufb01cients, which will be denoted by wML. These are determined by maximizing (1.62) with respect to w. For this purpose, we can omit the last two terms on the right-hand side of (1.62) because they do not depend on w", "e77673b5-94b8-4551-b0a2-d435cec78a06": "In this section we present the semi-gradient version of TD(\u03bb) with function approximation. With function approximation, the eligibility trace is a vector zt 2 Rd with the same number of components as the weight vector wt. Whereas the weight vector is a long-term memory, accumulating over the lifetime of the system, the eligibility trace is a short-term memory, typically lasting less time than the length of an episode. Eligibility traces assist in the learning process; their only consequence is that they a\u21b5ect the weight vector, and then the weight vector determines the estimated value. In TD(\u03bb), the eligibility trace vector is initialized to zero at the beginning of the where \u03b3 is the discount rate and \u03bb is the parameter introduced in the previous section, which we henceforth call the trace-decay parameter.\n\nThe eligibility trace keeps track of which components of the weight vector have contributed, positively or negatively, to recent state valuations, where \u201crecent\u201d is de\ufb01ned in terms of \u03b3\u03bb", "77754ece-4808-40d5-b167-4ba9c817754f": "Following standard practice, we formulate this as a tagging task but do not use a CRF layer in the output. We use the representation of the \ufb01rst sub-token as the input to the token-level classi\ufb01er over the NER label set. To ablate the \ufb01ne-tuning approach, we apply the feature-based approach by extracting the activations from one or more layers without \ufb01ne-tuning any parameters of BERT. These contextual embeddings are used as input to a randomly initialized two-layer 768-dimensional BiLSTM before the classi\ufb01cation layer. Results are presented in Table 7. BERTLARGE performs competitively with state-of-the-art methods. The best performing method concatenates the token representations from the top four hidden layers of the pre-trained Transformer, which is only 0.3 F1 behind \ufb01ne-tuning the entire model.\n\nThis demonstrates that BERT is effective for both \ufb01netuning and feature-based approaches. Recent empirical improvements due to transfer learning with language models have demonstrated that rich, unsupervised pre-training is an integral part of many language understanding systems. In particular, these results enable even low-resource tasks to bene\ufb01t from deep unidirectional architectures", "0cd5016d-6fe7-4c72-8db3-7d65ca618fd6": "Hence show by direct evaluation that p(a, b, c) = p(a)p(c|a)p(b|c). Draw the corresponding directed graph. 8.6 (\u22c6) For the model shown in Figure 8.13, we have seen that the number of parameters required to specify the conditional distribution p(y|x1, . , xM), where xi \u2208 {0, 1}, could be reduced from 2M to M +1 by making use of the logistic sigmoid representation (8.10). An alternative representation  is given by p(y = 1|x1, . , xM) = 1 \u2212 (1 \u2212 \u00b50) where the parameters \u00b5i represent the probabilities p(xi = 1), and \u00b50 is an additional parameters satisfying 0 \u2a7d \u00b50 \u2a7d 1. The conditional distribution (8.104) is known as the noisy-OR.\n\nShow that this can be interpreted as a \u2018soft\u2019 (probabilistic) form of the logical OR function (i.e., the function that gives y = 1 whenever at least one of the xi = 1). Discuss the interpretation of \u00b50", "5c8ae5e1-2e8f-4c0a-b2ba-cc1a2e3f3ffd": "Despite this, the Law of E\u21b5ect\u2014in one form or another\u2014is widely regarded as a basic principle underlying much behavior . It is the basis of the in\ufb02uential learning theories of Clark Hull  and the in\ufb02uential experimental methods of B. F. Skinner . The term \u201creinforcement\u201d in the context of animal learning came into use well after Thorndike\u2019s expression of the Law of E\u21b5ect, \ufb01rst appearing in this context (to the best of our knowledge) in the 1927 English translation of Pavlov\u2019s monograph on conditioned re\ufb02exes. Pavlov described reinforcement as the strengthening of a pattern of behavior due to an animal receiving a stimulus\u2014a reinforcer\u2014in an appropriate temporal relationship with another stimulus or with a response. Some psychologists extended the idea of reinforcement to include weakening as well as strengthening of behavior, and extended the idea of a reinforcer to include possibly the omission or termination of stimulus.\n\nTo be considered reinforcer, the strengthening or weakening must persist after the reinforcer is withdrawn; a stimulus that merely attracts an animal\u2019s attention or that energizes its behavior without producing lasting changes would not be considered a reinforcer", "a3848164-2ac5-4a7b-b8f3-9ba7e792d725": "The TD model is a real-time model, as opposed to a trial-level model like the Rescorla\u2013 Wagner model. A single step t in the Rescorla\u2013Wagner model represents an entire conditioning trial. The model does not apply to details about what happens during the time a trial is taking place, or what might happen between trials. Within each trial an animal might experience various stimuli whose onsets occur at particular times and that have particular durations. These timing relationships strongly in\ufb02uence learning. The Rescorla\u2013Wagner model also does not include a mechanism for higher-order conditioning, whereas for the TD model, higher-order conditioning is a natural consequence of the bootstrapping idea that is at the base of TD algorithms", "96fb541e-4d70-4382-8774-b51f5acb4c43": "Neural Style Transfer  Neural Style Transfer  is one of the flashiest demonstrations of Deep Learning capabilities. The general idea is to manipulate the representations of images created in CNNs.\n\nNeural Style Transfer is probably best known for its artistic applications, but it also serves as a great tool for Data Augmentation. The algorithm works by manipulat- ing the sequential representations across a CNN such that the style of one image can be transferred to another while preserving its original content. A more detailed explanation of the gram matrix operation powering Neural Style Transfer can be found by Li et al. (Fig. 25). It is important to also recognize an advancement of the original algorithm from Gatys et al. known as Fast Style Transfer . This algorithm extends the loss function from a per-pixel loss to a perceptual loss and uses a feed-forward network to stylize images. This perceptual loss is reasoned about through the use of another pre-trained net. The use of perceptual loss over per-pixel loss has also shown great promise in the applica- tion of super-resolution  as well as style transfer", "709ea3f2-fffe-4604-80cc-8e42da4f9855": "Let us start with a discussion of histogram methods for density estimation, which we have already encountered in the context of marginal and conditional distributions in Figure 1.11 and in the context of the central limit theorem in Figure 2.6. Here we explore the properties of histogram density models in more detail, focussing on the case of a single continuous variable x. Standard histograms simply partition x into distinct bins of width \u2206i and then count the number ni of observations of x falling in bin i.\n\nIn order to turn this count into a normalized probability density, we simply divide by the total number N of observations and by the width \u2206i of the bins to obtain probability values for each bin given by for which it is easily seen that \ufffd p(x) dx = 1. This gives a model for the density p(x) that is constant over the width of each bin, and often the bins are chosen to have the same width \u2206i = \u2206. In Figure 2.24, we show an example of histogram density estimation. Here the data is drawn from the distribution, corresponding to the green curve, which is formed from a mixture of two Gaussians", "e34bd17a-1bae-4851-ac95-06dca01a0bc3": "Chapter 20  Deep Generative Models  In this chapter, we present several of the specific kinds of generative models that can be built and trained using the techniques presented in chapters16\u201419. All these models represent probability distributions over multiple variables in some way. Some allow the probability distribution function to be evaluated explicitly. Others do not allow the evaluation of the probability distribution function but support operations that implicitly require knowledge of it, such as drawing samples from the distribution. Some of these models are structured probabilistic models described in terms of graphs and factors, using the language of graphical models presented in chapter 16. Others cannot be easily described in terms of factors but represent probability distributions nonetheless. 20.1 Boltzmann Machines  Boltzmann machines were originally introduced as a general \u201cconnectionist\u201d ap- proach to learning arbitrary probability distributions over binary vectors . Variants of the Boltzmann machine that include other kinds of variables have long ago surpassed the popularity of the original. In this section we briefly introduce the binary Boltzmann machine and discuss the issues that come up when trying to train and perform inference in the model", "c8f0dda8-6f4f-41fb-b169-3467760ab042": "For example, if we want a robot to be able to walk, then walking is the task. We could program the robot to learn to walk, or we could attempt to directly write a program that specifies how to walk manually.\n\nMachine learning tasks are usually described in terms of how the machine learning system should process an example. An example is a collection of features  https://www.deeplearningbook.org/contents/ml.html    that have been quantitatively measured from some object or event that we want the machine learning system to process. We typically represent an example as a vector & \u20ac IR\u201d where each entry \u201ci of the vector is another feature. For example, the features of an image are usually the values of the pixels in the image. 97  CHAPTER 5. MACHINE LEARNING BASICS  Many kinds of tasks can be solved with machine learning. Some of the most common machine learning tasks include the following:  e Classification: In this type of task, the computer program is asked to specify which of k categories some input belongs to. To solve this task, the learning algorithm is usually asked to produce a function f : R\u201d \u2014 {1,...,4}", "5a233481-2a72-44f9-9968-bf11281ffe84": "In gradient-descent methods, the weight vector is a column vector with a \ufb01xed number of real valued components, w .= (w1, w2, . , wd)>,1 and the approximate value function \u02c6v(s,w) is a di\u21b5erentiable function of w for all s 2 S. We will be updating w at each of a series of discrete time steps, t = 0, 1, 2, 3, . ., so we will need a notation wt for the 1The > denotes transpose, needed here to turn the horizontal row vector in the text into a vertical column vector; in this book vectors are generally taken to be column vectors unless explicitly written out horizontally or transposed. weight vector at each step. For now, let us assume that, on each step, we observe a new example St 7! v\u21e1(St) consisting of a (possibly randomly selected) state St and its true value under the policy.\n\nThese states might be successive states from an interaction with the environment, but for now we do not assume so", "63c8828a-038c-4562-857b-96d71df7c86a": "Our major contribution is further generalizing these \ufb01ndings to deep bidirectional architectures, allowing the same pre-trained model to successfully tackle a broad set of NLP tasks. Alan Akbik, Duncan Blythe, and Roland Vollgraf. 2018. Contextual string embeddings for sequence labeling. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1638\u20131649. Rie Kubota Ando and Tong Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6(Nov):1817\u20131853. John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proceedings of the 2006 conference on empirical methods in natural language processing, pages 120\u2013128. Association for Computational Linguistics. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In EMNLP", "b5edfd8d-ce49-4870-8859-a8227ba0a8b0": ", yN)T is given by an isotropic Gaussian of the form where IN denotes the N \u00d7N unit matrix.\n\nFrom the de\ufb01nition of a Gaussian process, the marginal distribution p(y) is given by a Gaussian whose mean is zero and whose covariance is de\ufb01ned by a Gram matrix K so that The kernel function that determines K is typically chosen to express the property that, for points xn and xm that are similar, the corresponding values y(xn) and y(xm) will be more strongly correlated than for dissimilar points. Here the notion of similarity will depend on the application. In order to \ufb01nd the marginal distribution p(t), conditioned on the input values x1, . , xN, we need to integrate over y. This can be done by making use of the results from Section 2.3.3 for the linear-Gaussian model. Using (2.115), we see that the marginal distribution of t is given by where the covariance matrix C has elements This result re\ufb02ects the fact that the two Gaussian sources of randomness, namely that associated with y(x) and that associated with \u03f5, are independent and so their covariances simply add", "02e4e28c-986a-4e77-867e-d7481846cd40": "Variational auto-encoder outputs can be further improved by inputting them into GANs . Additionally, a similar vector manipulation process can be done on the noise vector inputs to GANs through the use of Bidirectional GANs . The impressive performance of GANs has resulted in increased attention on how they can be applied to the task of Data Augmentation. These networks have the ability to gen- erate new training data that results in better performing classification models. The GAN architecture first proposed by Ian Goodfellow  is a framework for generative mode- ling through adversarial training. The best anecdote for understanding GANs is the anal- ogy of a cop and a counterfeiter. The counterfeiter (generator network) takes in some form of input. This could be a random vector, another image, text, and many more.\n\nThe counterfeiter learns to produce money such that the cop (discriminator network) cannot tell if the money is real or fake. The real or fake dichotomy is analogous to whether or not the generated instance is from the training set or if it was created by the generator network (Fig. 16)", "0465c4cf-68aa-4730-8045-eb8a0dab4470": "Consider a model with hidden variables Z, visible (observed) variables X, and parameters \u03b8.\n\nThe function that is optimized with respect to \u03b8 in the M step is the expected complete-data log likelihood, given by We can use sampling methods to approximate this integral by a \ufb01nite sum over samples {Z(l)}, which are drawn from the current estimate for the posterior distribution p(Z|X, \u03b8old), so that The Q function is then optimized in the usual way in the M step. This procedure is called the Monte Carlo EM algorithm. It is straightforward to extend this to the problem of \ufb01nding the mode of the posterior distribution over \u03b8 (the MAP estimate) when a prior distribution p(\u03b8) has been de\ufb01ned, simply by adding ln p(\u03b8) to the function Q(\u03b8, \u03b8old) before performing the M step. A particular instance of the Monte Carlo EM algorithm, called stochastic EM, arises if we consider a \ufb01nite mixture model, and draw just one sample at each E step. Here the latent variable Z characterizes which of the K components of the mixture is responsible for generating each data point", "c52f3fba-94e0-4e4d-b9e0-72dc67eb95b4": "Chapter 9  Convolutional Networks  Convolutional networks , also known as convolutional neural networks, or CNNs, are a specialized kind of neural network for processing data that has a known grid-like topology. Examples include time-series data, which can be thought of as a 1-D grid taking samples at regular time intervals, and image data, which can be thought of as a 2-D grid of pixels. Convolutional networks have been tremendously successful in practical applications. The name \u201cconvolutional neural network\u201d indicates that the network employs a mathematical operation called convolution. Convolution is a specialized kind of linear operation. Convolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers. In this chapter, we first describe what convolution is. Next, we explain the motivation behind using convolution in a neural network. We then describe an operation called pooling, which almost all convolutional networks employ. Usually, he operation used in a convolutional neural network does not correspond precisely o the definition of convolution as used in other fields, such as engineering or pure mathematics", "c8616a63-0e5e-4208-b81d-c59e6e779cb7": "Define KFoldXV(D, A, L, k): Require: D, the given dataset, with elements 2 Require: A, the learning algorithm, seen as a function that takes a dataset as input and outputs a learned function Require: L, the loss function, seen as a function from a learned function f and an example z \u20ac D to ascalar \u20ac R Require: k, the number of folds Split D into k mutually exclusive subsets D;, whose union is D for i from 1 to k do fi = AD\\D;) for 2) in D; do ej = L(fi,2) end for end for Return e  (i.id.) data points. A point estimator or statistic is any function of the data:  On = g(a ,..., 0). (5.19)  The definition does not require that g return a value that is close to the true 0 or even that the range of g be the same as the set of allowable values of @. This definition of a point estimator is very general and would enable the designer of an estimator great flexibility", "308a346d-58e5-465f-a245-ed7035a10686": "Finally, for the requirement of generating \ufb02uent text, we can again naturally use an auxiliary model as the experience, namely, a pretrained language model (LM) fLM(x, a, y) = LM(y) that estimates the log likelihood of a sentence y under the natural language distribution. After identifying the experience (fsc, fdata, fLM), we then combine them together with Equation 9.1 and plug into the SE to train the target model p\u03b8(y|x, a). More experimental details can be found in Hu et al. and Yang et al. Table 2 shows the empirical results on the common Yelp corpus of customer reviews. We can see that by plugging in the relevant experience, the resulting model successfully learns the respective aspects of the task. For example, with only fsc, the model is able to transfer the sentiment attribute but fails on content preservation and \ufb02uency. Adding the second experience fdata encourages preservation. Further with fLM, the model substantially improves the \ufb02uency, achieving the best overall performance.\n\nThe case study demonstrates the necessity of integrating the diverse experience for solving the problem. 9.2", "43a0428e-7387-46c1-9bf1-856d82edca97": "One was that the episodes have exploring starts, and the other was that policy evaluation could be done with an in\ufb01nite number of episodes. To obtain a practical algorithm we will have to remove both assumptions. We postpone consideration of the \ufb01rst assumption until later in this chapter. For now we focus on the assumption that policy evaluation operates on an in\ufb01nite number of episodes. This assumption is relatively easy to remove. In fact, the same issue arises even in classical DP methods such as iterative policy evaluation, which also converge only asymptotically to the true value function. In both DP and Monte Carlo cases there are two ways to solve the problem. One is to hold \ufb01rm to the idea of approximating q\u21e1k in each policy evaluation.\n\nMeasurements and assumptions are made to obtain bounds on the magnitude and probability of error in the estimates, and then su\ufb03cient steps are taken during each policy evaluation to assure that these bounds are su\ufb03ciently small. This approach can probably be made completely satisfactory in the sense of guaranteeing correct convergence up to some level of approximation. However, it is also likely to require far too many episodes to be useful in practice on any but the smallest problems", "42baf561-0fba-42ca-9af1-bc026abd8cdb": "The adversarial Shorten and Khoshgoftaar J Big Data  6:60   + .007 x \u00b0 sten(VoN(%.2)) \u2014 esign(VoJ(6,2,9)) \u201cpanda\u201d \u201cnematode\u201d \u201cgibbon\u201d 57.7% confidence 8.2% confidence 99.3 % confidence Fig. 15 Adversarial misclassification example   attacks demonstrate that representations of images are much less robust than what might have been expected. This is well demonstrated by Moosavi-Dezfooli et al. using DeepFool, a network that finds the minimum possible noise injection needed to cause a misclassification with high confidence. Su et al. show that 70.97% of images can be misclassified by changing just one pixel. Zajac et al. cause mis- classifications with adversarial attacks limited to the border of images. The success of adversarial attacks is especially exaggerated as the resolution of images increases.\n\nAdversarial attacking can be targeted or untargeted, referring to the deliberation in which the adversarial network is trying to cause misclassifications", "7a25549c-a8d8-4b0d-8d73-5f9cbfe56369": "There is no unique decomposition because p(a) f(a) can always be rewritten as  oN ef N  https://www.deeplearningbook.org/contents/monte_carlo.html    p(a) f(a) = q(x)? Vila) , (17.8) where we now sample from q and average Pf Tn many cases, we wish to compute an expectation for a given pand an f, and the fact that the problem is specified from the start as an expectation suggests that this p and f would be a natural  1The unbiased estimator of the variance is often preferred, in which the sum of squared differences is divided by n \u2014 1 instead of n.  589  CHAPTER 17. MONTE CARLO METHODS  choice of decomposition. However, the original specification of the problem may not be the the optimal choice in terms of the number of samples required to obtain a given level of accuracy. Fortunately, the form of the optimal choice qg* can be derived easily. The optimal qg* corresponds to what is called optimal importance sampling.\n\nBecause of the identity shown in equation 17.8, any Monte Carlo estimator", "2613ff2c-cc37-44be-bcb3-5bad3064c25b": "In order to motivate the concept of linear dynamical systems, let us consider the following simple problem, which often arises in practical settings. Suppose we wish to measure the value of an unknown quantity z using a noisy sensor that returns a observation x representing the value of z plus zero-mean Gaussian noise. Given a single measurement, our best guess for z is to assume that z = x. However, we can improve our estimate for z by taking lots of measurements and averaging them, because the random noise terms will tend to cancel each other. Now let\u2019s make the situation more complicated by assuming that we wish to measure a quantity z that is changing over time. We can take regular measurements of x so that at some point in time we have obtained x1, . , xN and we wish to \ufb01nd the corresponding values z1, . , xN.\n\nIf we simply average the measurements, the error due to random noise will be reduced, but unfortunately we will just obtain a single averaged estimate, in which we have averaged over the changing value of z, thereby introducing a new source of error. Intuitively, we could imagine doing a bit better as follows", "3b5d86ec-1203-4504-abcf-0ac806280ace": "A is followed by a reward of 0 and transition to a state with a value of nearly 0, which suggests vw(A) should be 0; why is its optimal value substantially negative rather than 0? The answer is that making vw(A) negative reduces the error upon arriving in A from B. The reward on this deterministic transition is 1, which implies that B should have a value 1 more than A. Because B\u2019s value is approximately zero, A\u2019s value is driven toward \u22121. The BE-minimizing value of \u21e1 \u2212 1 for A is a compromise between reducing the errors on leaving and on entering A. policy together completely determine the probability distribution over data t Assume for the moment that the state, action, and reward sets are all \ufb01ni for any \ufb01nite sequence \u03be = \u03c60, a0, r1, . , rk, \u03c6k, there is a well de\ufb01ned probab sibly zero) of it occuring as the initial portion of a trajectory, which we ma P(\u03be) = Pr{\u03c6(S0) = \u03c60, A0 = a0, R1 = r1,", "770a9c6f-66c3-4369-a502-ca26fea8c545": "There is no need to have labels for the hidden unit classifiers: gradient descent on an objective function of interest naturally learns semantically interesting features, as long as the task requires such features. We can learn about the distinction between male and female, or about the presence or absence of glasses, without having to characterize all the configurations of the n \u2014 1 other features by examples covering all these combinations of values. This form of statistical separability is what allows one to generalize to new configurations of a person\u2019s features that have never been seen during training. 15.5 Exponential Gains from Depth  We have seen in section 6.4.1 that multilayer perceptrons are universal approxima- tors, and that some functions can be represented by exponentially smaller deep  1 ta tou 1 1 mis 4 . rie ,oOo4  https://www.deeplearningbook.org/contents/representation.html    HELWOLKS COMpared LO SHALLOW LeELWOrKS. LUIS Gecrease 1 WOdel size Leads LO improved statistical efficiency.\n\nIn this section, we describe how similar results apply more generally to other kinds of models with distributed hidden representations", "c4f0a2cb-5895-40d6-a4d0-0b6df27d06c9": "Then, an optimal discriminator D\u2217 exists for Pr and P\u03b8, and Proof. First, we prove that there exists an optimal discriminator. Let D : X \u2192 . We are then interested to see if there\u2019s an optimal discriminator for the problem min0\u2264D(x)\u2264m LD(D, g\u03b8). Note now that if 0 \u2264 D(x) \u2264 m we have LD(D, g\u03b8) = Ex\u223cPr + Ez\u223cp(z)+] and there is an f \u2217 : X \u2192  such that Ex\u223cPr\u2212Ex\u223cP\u03b8 = \u2212\u03b4(Pr, P\u03b8). This is a long known fact, found for example in , but we prove it later for completeness. In that case, we de\ufb01ne D\u2217(x) = m This shows that D\u2217 is optimal and LD(D\u2217, g\u03b8) = m \u2212 m concluding the proof. For completeness, we now show a proof for equation (7) and the existence of said f \u2217 that attains the value of the in\ufb01mum", "b5953349-1e6c-4601-a69b-69f58b9686f4": "From a representation learning point  469  CHAPTER 12.\n\nAPPLICATIONS  of view, it can be useful to learn a representation in which sentences that have the same meaning have similar representations regardless of whether they were written in the source language or in the target language. This strategy was explored first using a combination of convolutions and RNNs . Later work introduced the use of an RNN for scoring proposed translations  and for generating translated sentences . Jean et al. scaled these models to larger vocabularies. 12.4.5.1 Using an Attention Mechanism and Aligning Pieces of Data  Using a fixed-size representation to capture all the semantic details of a very long sentence of, say, 60 words is very difficult. It can be achieved by training a sufficiently large RNN well enough and for long enough, as demonstrated by Cho et al. and Sutskever et al", "788e6100-788a-4165-a7b6-c81c2303c302": "But as we have seen in this chapter, the extension of these ideas to signi\ufb01cant function approximation, even linear function approximation, involves new challenges and forces us to deepen our understanding of reinforcement learning algorithms. Why go to such lengths?\n\nOne reason to seek o\u21b5-policy algorithms is to give \ufb02exibility in dealing with the tradeo\u21b5 between exploration and exploitation. Another is to free behavior from learning, and avoid the tyranny of the target policy. TD learning appears to hold out the possibility of learning about multiple things in parallel, of using one stream of experience to solve many tasks simultaneously. We can certainly do this in special cases, just not in every case that we would like to or as e\ufb03ciently as we would like to. In this chapter we divided the challenge of o\u21b5-policy learning into two parts. The \ufb01rst part, correcting the targets of learning for the behavior policy, is straightforwardly dealt with using the techniques devised earlier for the tabular case, albeit at the cost of increasing the variance of the updates and thereby slowing learning. High variance will probably always remains a challenge for o\u21b5-policy learning", "0178a32e-5d36-4961-bc90-24f87a39965f": "We have discussed the SE for learning with all diverse forms of experience, in both static environments (e.g., \ufb01xed data or reward distributions) and dynamic environments (e.g., optimized or online experience).\n\nAn exciting next step is to deploy the SE framework to build an AI agent that continually learns in the real-world complex and fast-evolving context, in which the AI agent must learn to identify the relevant experience out of massive external information, to acquire increasingly complex new concepts or skills. Establishing and applying the standardized formalism to the broader learning settings is expected to unleash even more power by enabling principled design of learning systems that continuously improve by interacting with and collecting diverse signals from the outer world. Theoretical analysis of panoramic learning. The paradigm of panoramic learning poses new questions about theoretical understanding. A question of particular importance in practice is about how we can guarantee better performance after integrating more experience. The analysis is challenging because the di\ufb00erent types of experience can each encode di\ufb00erent information, sometimes noisy and even con\ufb02icting with each other (e.g., not all data instances would comply with a logic rule), and thus plugging in an additional source of experience does not necessarily lead to positive e\ufb00ects", "576fdd94-5fef-46fe-a4f6-553b8722d49d": "This approach allowed the example rejection mechanism to function much more effectively. At this point, coverage was still below 90 percent, yet there were no obvious heoretical problems with the approach. Our methodology therefore suggested instrumenting the training and test set performance to determine whether the problem was underfitting or overfitting. In this case, training and test set error were nearly identical. Indeed, the main reason this project proceeded so smoothly was the availability of a dataset with tens of millions of labeled examples. Because raining and test set error were so similar, this suggested that the problem was due o either underfitting or a problem with the training data. One of the debugging strategies we recommend is to visualize the model\u2019s worst errors. In this case, that  meant visualizing the incorrect training set transcriptions that the model gave the highest confidence.\n\nThese proved to mostly consist of examples where the input image had been cropped too tightly, with some of the digits of the address being removed by the cropping operation. For example, a photo of an address \u201c1849\u201d might be cropped too tightly, with only the \u201c849\u201d remaining visible", "940ecc9b-bce5-4a58-90cc-610b61e8b919": "We shall see later how to give a probabilistic interpretation to a neural network.\n\nAs discussed in Section 3.1, the bias parameters in (5.2) can be absorbed into the set of weight parameters by de\ufb01ning an additional input variable x0 whose value is clamped at x0 = 1, so that (5.2) takes the form We can similarly absorb the second-layer biases into the second-layer weights, so that the overall network function becomes As can be seen from Figure 5.1, the neural network model comprises two stages of processing, each of which resembles the perceptron model of Section 4.1.7, and for this reason the neural network is also known as the multilayer perceptron, or MLP. A key difference compared to the perceptron, however, is that the neural network uses continuous sigmoidal nonlinearities in the hidden units, whereas the perceptron uses step-function nonlinearities. This means that the neural network function is differentiable with respect to the network parameters, and this property will play a central role in network training. If the activation functions of all the hidden units in a network are taken to be linear, then for any such network we can always \ufb01nd an equivalent network without hidden units", "d1ea75e8-29a4-4866-a701-2df14d417e66": "This leads to the simpli\ufb01ed factor graph representation in Figure 13.15, in which the factors are given by To derive the alpha-beta algorithm, we denote the \ufb01nal hidden variable zN as the root node, and \ufb01rst pass messages from the leaf node h to the root. From the general results (8.66) and (8.69) for message propagation, we see that the messages which are propagated in the hidden Markov model take the form These equations represent the propagation of messages forward along the chain and are equivalent to the alpha recursions derived in the previous section, as we shall now show. Note that because the variable nodes zn have only two neighbours, they perform no computation. We can eliminate \u00b5zn\u22121\u2192fn(zn\u22121) from (13.48) using (13.47) to give a recursion for the f \u2192 z messages of the form If we now recall the de\ufb01nition (13.46), and if we de\ufb01ne then we obtain the alpha recursion given by (13.36). We also need to verify that the quantities \u03b1(zn) are themselves equivalent to those de\ufb01ned previously", "56810b6a-f54e-408b-baa2-50641eb8a394": ", K. The value of \u00b5k gives the probability of the random variable taking state k, and so these parameters are subject to the constraints 0 \u2a7d \u00b5k \u2a7d 1 and \ufffd k \u00b5k = 1. The conjugate prior distribution for the parameters {\u00b5k} is the Dirichlet. The normal distribution is simply another name for the Gaussian. In this book, we use the term Gaussian throughout, although we retain the conventional use of the symbol N to denote this distribution. For consistency, we shall refer to the normalgamma distribution as the Gaussian-gamma distribution, and similarly the normalWishart is called the Gaussian-Wishart. This distribution was published by William Gosset in 1908, but his employer, Guiness Breweries, required him to publish under a pseudonym, so he chose \u2018Student\u2019. In the univariate form, Student\u2019s t-distribution is obtained by placing a conjugate gamma prior over the precision of a univariate Gaussian distribution and then integrating out the precision variable. It can therefore be viewed as an in\ufb01nite mixture of Gaussians having the same mean but different variances", "d39eb1d7-3bf7-4b0f-8c18-c666d40fb1e3": "As we shall see later, it is closely related to factor analysis . Probabilistic PCA is a simple example of the linear-Gaussian framework, in which all of the marginal and conditional distributions are Gaussian. We can formulate probabilistic PCA by first introducing an explicit latent variable z corresponding to the principal-component subspace. Next we define a Gaussian prior distribution p(z) over the latent variable, together with a Gaussian conditional distribution p(xlz) for the observed variable x conditioned on the value of the latent variable. Specifically, the prior distribution over z is given by a zero-mean unit-covariance Gaussian Similarly, the conditional distribution of the observed variable x, conditioned on the value of the latent variable z, is again Gaussian, of the form in which the mean of x is a general linear function of z governed by the D x M matrix Wand the D-dimensional vector J-L. Note that this factorizes with respect to the elements of x, in other words this is an example of the naive Bayes model", "b0d91be7-2107-4283-9a8c-5c4150bd5875": "The classical state-space planning methods in arti\ufb01cial intelligence are decision-time planning methods collectively known as heuristic search. In heuristic search, for each state encountered, a large tree of possible continuations is considered. The approximate value function is applied to the leaf nodes and then backed up toward the current state at the root. The backing up within the search tree is just the same as in the expected updates with maxes (those for v\u21e4 and q\u21e4) discussed throughout this book. The backing up stops at the state\u2013action nodes for the current state. Once the backed-up values of these nodes are computed, the best of them is chosen as the current action, and then all In conventional heuristic search no e\u21b5ort is made to save the backed-up values by changing the approximate value function. In fact, the value function is generally designed by people and never changed as a result of search.\n\nHowever, it is natural to consider allowing the value function to be improved over time, using either the backed-up values computed during heuristic search or any of the other methods presented throughout this book. In a sense we have taken this approach all along", "81af250a-ad48-4ec9-9c31-648543968ac8": "As with rejection sampling, the approximation improves as the sampling distribution q(z) gets closer to the desired distribution p(z). When q(z) = p(z), the initial samples (z(1), . , z(L)) have the desired distribution, and the weights wn = 1/L so that the resampled values also have the desired distribution. If moments with respect to the distribution p(z) are required, then they can be evaluated directly using the original samples together with the weights, because In addition to providing a mechanism for direct implementation of the Bayesian framework, Monte Carlo methods can also play a role in the frequentist paradigm, for example to \ufb01nd maximum likelihood solutions. In particular, sampling methods can be used to approximate the E step of the EM algorithm for models in which the E step cannot be performed analytically", "1a051acb-0c4a-4656-9e84-e1ba0509969c": "(5.62) 0 If the examples are assumed to be i.i.d., then this can be decomposed into  Our = arg max y log P(y\u00ae | x: 6). (5.63) 0 ; i=1  Example: Linear Regression as Maximum Likelihood Linear regression, introduced in section 5.1.4, may be justified as a maximum likelihood procedure. Previously, we motivated linear regression as an algorithm that learns to take an input x and produce an output value y. The mapping from 2 to y is chosen to minimize mean squared error, a criterion that we introduced more or less arbitrarily. We now revisit linear regression from the point of view of maximum likelihood estimation.\n\nInstead of producing a single prediction y, we now think of the model as producing a conditional distribution p(y | 7). We can imagine that with an infinitely large training set, we might see several training examples with the same input value a but different values of y. The goal of the learning algorithm is now o fit the distribution p(y | a) to all those different y values that are all compatible with x", "923bca14-986c-41ae-b6ca-8ae54936348f": "AdaGrad shrinks the learning rate according to the entire history of the squared gradient and may have made the learning rate too small before arriving at such a convex structure. RMSProp uses an exponentially decaying average to discard history from the extreme past so that it can converge rapidly after finding a convex bowl, as if it were an instance of the AdaGrad algorithm initialized within that bowl. Algorithm 8.4 The AdaGrad algorithm Require: Global learning rate \u00a2\u20ac  D AAwwtnn. Tewttinl nannecntn. A  https://www.deeplearningbook.org/contents/optimization.html  AveoyulL we. Allitial PaLalluevel U \u20147 . Require: Small constant 6, perhaps 10 \u2014, for numerical stability Initialize gradient accumulation variable r = 0 while stopping criterion not met do  Sample a minibatch of m examples from the training set fa, nr 7 (my with corresponding targets y @, Compute gradient: g \u2014 +Vo >>, L( f(a; 6), y)", "9d2289b4-4a5a-46aa-acc0-ffc043f3cc35": "We begin by looking in some detail at the support vector machine (SVM), which became popular in some years ago for solving problems in classi\ufb01cation, regression, and novelty detection. An important property of support vector machines is that the determination of the model parameters corresponds to a convex optimization problem, and so any local solution is also a global optimum. Because the discussion of support vector machines makes extensive use of Lagrange multipliers, the reader is encouraged to review the key concepts covered in Appendix E. Additional information on support vector machines can be found in Vapnik , Burges , Cristianini and Shawe-Taylor , M\u00a8uller et al. , Sch\u00a8olkopf and Smola , and Herbrich . The SVM is a decision machine and so does not provide posterior probabilities. We have already discussed some of the bene\ufb01ts of determining probabilities in Section 1.5.4.\n\nAn alternative sparse kernel technique, known as the relevance vector machine (RVM), is based on a Bayesian formulation and provides posterior probaSection 7.2 bilistic outputs, as well as having typically much sparser solutions than the SVM", "74c31586-5dda-41c1-8e0b-d09ac03451c7": "John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121\u2013 2159, 2010. Simon Duane, Anthony D Kennedy, Brian J Pendleton, and Duncan Roweth. Hybrid monte carlo. Physics letters B, 195(2):216\u2013222, 1987. Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference. The Journal of Machine Learning Research, 14(1):1303\u20131347, 2013. Geoffrey E Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The\u201d wakesleep\u201d algorithm for unsupervised neural networks. SCIENCE, pages 1158\u20131158, 1995. Koray Kavukcuoglu, Marc\u2019Aurelio Ranzato, and Yann LeCun. Fast inference in sparse coding algorithms with applications to object recognition. Technical Report CBLLTR-2008-12-01, Computational and Biological Learning Lab, Courant Institute, NYU, 2008", "216ed7e3-7a15-4225-9e1a-2a90fe4a25ab": "14.16 (\u22c6 \u22c6 \u22c6) Extend the logistic regression mixture model of Section 14.5.2 to a mixture of softmax classi\ufb01ers representing C \u2a7e 2 classes. Write down the EM algorithm for determining the parameters of this model through maximum likelihood. in which each mixture component \u03c8k(t|x) is itself a mixture model.\n\nShow that this two-level hierarchical mixture is equivalent to a conventional single-level mixture model. Now suppose that the mixing coef\ufb01cients in both levels of such a hierarchical model are arbitrary functions of x. Again, show that this hierarchical model is again equivalent to a single-level model with x-dependent mixing coef\ufb01cients. Finally, consider the case in which the mixing coef\ufb01cients at both levels of the hierarchical mixture are constrained to be linear classi\ufb01cation (logistic or softmax) models. Show that the hierarchical mixture cannot in general be represented by a single-level mixture having linear classi\ufb01cation models for the mixing coef\ufb01cients. Hint: to do this it is suf\ufb01cient to construct a single counter-example, so consider a mixture of two components in which one of those components is itself a mixture of two components, with mixing coef\ufb01cients given by linear-logistic models", "87f07ab9-774f-4197-a9ca-1494649c2d49": "This has been an enduring challenge for arti\ufb01cial intelligence and explains why learning algorithms for ANNs with hidden layers have received so much attention over the years. ANNs typically learn by a stochastic gradient method (Section 9.3). Each weight is adjusted in a direction aimed at improving the network\u2019s overall performance as measured by an objective function to be either minimized or maximized. In the most common supervised learning case, the objective function is the expected error, or loss, over a set of labeled training examples.\n\nIn reinforcement learning, ANNs can use TD errors to learn value functions, or they can aim to maximize expected reward as in a gradient bandit (Section 2.8) or a policy-gradient algorithm (Chapter 13). In all of these cases it is necessary to estimate how a change in each connection weight would in\ufb02uence the network\u2019s overall performance, in other words, to estimate the partial derivative of an objective function with respect to each weight, given the current values of all the network\u2019s weights. The gradient is the vector of these partial derivatives", "a1df8ad2-8936-4ac3-93a5-708180fda728": "Transfer Learning via Fine-Tuning We \ufb01ne-tuned the entire network using the weights of the pretrained network as initialization. We trained for 20,000 steps at a batch size of 256 using SGD with Nesterov momentum with a momentum parameter of 0.9. We set the momentum parameter for the batch normalization statistics to max(1 \u2212 10/s, 0.9) where s is the number of steps per epoch. As data augmentation during \ufb01ne-tuning, we performed only random crops with resize and \ufb02ips; in contrast to pretraining, we did not perform color augmentation or blurring. At test time, we resized images to 256 pixels along the shorter side and took a 224 \u00d7 224 center crop. (Additional accuracy improvements may be possible with further optimization of data augmentation, particularly on the CIFAR-10 and CIFAR-100 datasets.) We selected the learning rate and weight decay, with a grid of 7 logarithmically spaced learning rates between 0.0001 and 0.1 and 7 logarithmically spaced values of weight decay between 10\u22126 and 10\u22123, as well as no weight decay", "08b8f2fb-35f1-4486-8ffe-8052987e79b8": "In de\ufb01ning the neuron-like actor and critic units, we ignored the small amount of time it takes synaptic input to e\u21b5ect the \ufb01ring of a real neuron.\n\nWhen an action potential from the presynaptic neuron arrives at a synapse, neurotransmitter molecules are released that di\u21b5use across the synaptic cleft to the postsynaptic neuron, where they bind to receptors on the postsynaptic neuron\u2019s surface; this activates molecular machinery that causes the postsynaptic neuron to \ufb01re (or to inhibit its \ufb01ring in the case of inhibitory synaptic input). This process can take several tens of milliseconds. According to (15.1) and (15.2), though, the input to a critic and actor unit instantaneously produces the unit\u2019s output. Ignoring activation time like this is common in abstract models of Hebbian-style plasticity in which synaptic e\ufb03cacies change according to a simple product of simultaneous pre- and postsynaptic activity. More realistic models must take activation time into account", "4ff59e6f-ec0b-4c7b-9031-30d7ce5cf1a6": "Algorithm parameters: step size \u21b5 2 (0, 1], small \" > 0 Initialize Q(s, a), for all s 2 S+, a 2 A(s), arbitrarily except that Q(terminal, \u00b7) = 0 Example 6.5: Windy Gridworld Shown inset below is a standard gridworld, with start and goal states, but with one di\u21b5erence: there is a crosswind running upward through the middle of the grid. The actions are the standard four\u2014up, down, right, and left\u2014but in the middle region the resultant next states are shifted upward by a \u201cwind,\u201d the strength of which varies from column to column. The strength of the wind results of applying \"-greedy Sarsa to this task, with \" = 0.1, \u21b5 = 0.5, and the initial values Q(s, a) = 0 for all s, a.\n\nThe increasing slope of the graph shows that the goal was reached more quickly over time. By 8000 time steps, the greedy policy was long since optimal (a trajectory from it is shown inset); continued \"-greedy exploration kept the average episode length at about 17 steps, two more than the minimum of 15", "077942b3-126f-4628-b05d-f8455764126b": "Although sometimes dismissed as irrelevant to wider issues in psychology, these experiments probe subtle properties of animal learning, often motivated by precise theoretical questions. As psychology shifted its focus to more cognitive aspects of behavior, that is, to mental processes such as thought and reasoning, animal learning experiments came to play less of a role in psychology than they once did. But this experimentation led to the discovery of learning principles that are elemental and widespread throughout the animal kingdom, principles that should not be neglected in designing arti\ufb01cial learning systems. In addition, as we shall see, some aspects of cognitive processing connect naturally to the computational perspective provided by reinforcement learning.\n\nThis chapter\u2019s \ufb01nal section includes references relevant to the connections we discuss as well as to connections we neglect. We hope this chapter encourages readers to probe all of these connections more deeply. Also included in this \ufb01nal section is a discussion of how the terminology used in reinforcement learning relates to that of psychology. Many of the terms and phrases used in reinforcement learning are borrowed from animal learning theories, but the computational/engineering meanings of these terms and phrases do not always coincide with their meanings in psychology. The algorithms we describe in this book fall into two broad categories: algorithms for prediction and algorithms for control.1 These categories arise naturally in solution methods for the reinforcement learning problem presented in Chapter 3", "0a34ceb1-6b81-4905-a0c4-5b3f6632eb17": "The mean and variance of the gamma distribution are given by Exercise 2.42 Consider a prior distribution Gam(\u03bb|a0, b0).\n\nIf we multiply by the likelihood function (2.145), then we obtain a posterior distribution which we recognize as a gamma distribution of the form Gam(\u03bb|aN, bN) where ML is the maximum likelihood estimator of the variance. Note that in (2.149) there is no need to keep track of the normalization constants in the prior and the likelihood function because, if required, the correct coef\ufb01cient can be found at the end using the normalized form (2.146) for the gamma distribution. From (2.150), we see that the effect of observing N data points is to increase the value of the coef\ufb01cient a by N/2. Thus we can interpret the parameter a0 in the prior in terms of 2a0 \u2018effective\u2019 prior observations. Similarly, from (2.151) we see that the N data points contribute N\u03c32 ML/2 to the parameter b, where \u03c32 ML is the variance, and so we can interpret the parameter b0 in the prior as arising from the 2a0 \u2018effective\u2019 prior observations having variance 2b0/(2a0) = b0/a0", "93055adf-e780-42b5-8ea7-2cf19d6a12dc": "It is possible to recover the new vectors into images using an auto-encoder network; however, this requires copying the entire encoding part of the CNN being trained. For deep CNNs, this results in massive auto-encoders which are very difficult and time-consuming to train. Finally, Wong et al. find that when it is possible to transform images in the data-space, data-space augmentation will outperform feature  space augmentation. Adversarial training  One of the solutions to search the space of possible augmentations is adversarial training.\n\nAdversarial training is a framework for using two or more networks with contrasting objectives encoded in their loss functions. This section will discuss using adversarial training as a search algorithm as well as the phenomenon of adversarial attacking. Adversarial attacking consists of a rival network that learns augmentations to images that result in misclassifications in its rival classification network. These adversarial attacks, constrained to noise injections, have been surprisingly successful from the perspective of the adversarial network. This is surprising because it com-  pletely defies intuition about how these models represent images", "b7e88579-1d9d-44fc-814a-fd788f4bc9fa": "It turned out that the injected rats had signi\ufb01cantly lower response rates than the non-injected rats right from the start of the extinction trials. Adams and Dickinson concluded that the injected rats associated lever pressing with consequent nausea by means of a cognitive map linking lever pressing to pellets, and pellets to nausea. Hence, in the extinction trials, the rats \u201cknew\u201d that the consequences of pressing the lever would be something they did not want, and so they reduced their lever-pressing right from the start. The important point is that they reduced lever-pressing without ever having experienced lever-pressing directly followed by being sick: no lever was present when they were made sick. They seemed able to combine knowledge of the outcome of a behavioral choice (pressing the lever will be followed by getting a pellet) with the reward value of the outcome (pellets are to be avoided) and hence could alter their behavior accordingly. Not every psychologist agrees with this \u201ccognitive\u201d account of this kind of experiment, and it is not the only possible way to explain these results, but the model-based planning explanation is widely accepted.\n\nNothing prevents an agent from using both model-free and model-based algorithms, and there are good reasons for using both", "a29db167-cdff-4d24-b24d-df024b0b9f77": "Again, we shall consider an error function that consists of a sum of terms, one for each pattern in the data set, so that E = \ufffd Using (5.48) and (5.49), the second derivatives on the right-hand side of (5.79) can be found recursively using the chain rule of differential calculus to give a backpropagation equation of the form If we now neglect off-diagonal elements in the second-derivative terms, we obtain  Note that the number of computational steps required to evaluate this approximation is O(W), where W is the total number of weight and bias parameters in the network, compared with O(W 2) for the full Hessian. Ricotti et al. also used the diagonal approximation to the Hessian, but they retained all terms in the evaluation of \u22022En/\u2202a2 j and so obtained exact expressions for the diagonal terms. Note that this no longer has O(W) scaling.\n\nThe major problem with diagonal approximations, however, is that in practice the Hessian is typically found to be strongly nondiagonal, and so these approximations, which are driven mainly be computational convenience, must be treated with care", "c7493c85-0a13-468e-846a-f68097868dc2": "In particular, we usually wish that, as the number of data points m in our dataset increases, our point estimates converge to the true value of the corresponding parameters. More formally, we would like that  plim,, 04m = 4. (5.55)  The symbol plim indicates convergence in probability, meaning that for any \u20ac > 0, P(\\Om \u2014 6| > \u20ac) 3 0 as m - oo. The condition described by equation 5.55 is known as consistency. It is sometimes referred to as weak consistency, with strong consistency referring to the almost sure convergence of @ to @. Almost  128  CHAPTER 5.\n\nMACHINE LEARNING BASICS  sure convergence of a sequence of random variables x\u201c) ,x(),... to a value  occurs when p(limm-\u2014oo x(m) = v)=1. Consistency ensures that the bias induced by the estimator diminishes as the number of data examples grows. However, the reverse is not true\u2014asymptotic unbiasedness does not imply consistency", "12a4a3b0-6d86-48a7-a7ae-8b1d050a8ce9": "We often call the input to the RNN the \u201ccontext.\u201d We want to produce a representation of this context, C. The context C might be a vector or sequence of vectors that summarize the input sequence X = (2), ... a (=),  The simplest RNN architecture for mapping a variable-length sequence to another variable-length sequence was first proposed by Cho e\u00a2 al. and shortly after by Sutskever et al. , who independently developed that archi-  390  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  https://www.deeplearningbook.org/contents/rnn.html    Encoder  re - ~~ re N / ~-7  Figure 10.12: Example of an encoder-decoder or sequence-to-sequence RNN architecture, for learning to generate an output sequence fy\u201c),...,y(\u00bb)) given an input sequence (x, x@,...,x(\u2122=))", "dd82900a-11ca-42e5-94d7-d3f7f02e7ea9": "We start by considering the label density d\ufffd of the label matrix \ufffd, de\ufb01ned as the mean number of non-abstention labels per data point. In the low-density setting, sparsity of labels will mean that there is limited room for even an optimal weighting of the labeling functions to diverge much from the majority vote. Conversely, as the label density grows, known theory con\ufb01rms that the majority vote will eventually be optimal . It is the middle-density regime where we expect to most bene\ufb01t from applying the generative model.\n\nWe start by de\ufb01ning a measure of the bene\ufb01t of weighting the labeling functions by their true accuracies\u2014in other words, the predictions of a perfectly estimated generative model\u2014 versus an unweighted majority vote: Fig. 7 A plot of the modeling advantage, i.e., the improvement in label accuracyfromthegenerativemodel,asafunctionofthenumberoflabeling functions (equivalently, the label density) on a synthetic dataset", "25f1e943-fc91-4cd1-8b13-2e154ee51aeb": "From (1.58) it follows that the following estimate for the variance parameter is unbiased the true Gaussian distribution from which data is generated, and the three red curves show the Gaussian distributions obtained by \ufb01tting to three data sets, each consisting of two data points shown in blue, using the maximum likelihood results (1.55) and (1.56). Averaged across the three data sets, the mean is correct, but the variance is systematically under-estimated because it is measured relative to the sample mean and not relative to the true mean. In Section 10.1.3, we shall see how this result arises automatically when we adopt a Bayesian approach. Note that the bias of the maximum likelihood solution becomes less signi\ufb01cant as the number N of data points increases, and in the limit N \u2192 \u221e the maximum likelihood solution for the variance equals the true variance of the distribution that generated the data. In practice, for anything other than small N, this bias will not prove to be a serious problem. However, throughout this book we shall be interested in more complex models with many parameters, for which the bias problems associated with maximum likelihood will be much more severe.\n\nIn fact, as we shall see, the issue of bias in maximum likelihood lies at the root of the over-\ufb01tting problem that we encountered earlier in the context of polynomial curve \ufb01tting", "5299aeb2-ef5a-43d3-a4d2-8430ca3dc355": "In other words, the key issue is that of generalization. How can experience with a limited subset of the state space be usefully generalized to produce a good approximation over a much larger subset? Fortunately, generalization from examples has already been extensively studied, and we do not need to invent totally new methods for use in reinforcement learning. To some extent we need only combine reinforcement learning methods with existing generalization methods. The kind of generalization we require is often called function approximation because it takes examples from a desired function (e.g., a value function) and attempts to generalize from them to construct an approximation of the entire function.\n\nFunction approximation is an instance of supervised learning, the primary topic studied in machine learning, arti\ufb01cial neural networks, pattern recognition, and statistical curve \ufb01tting. In theory, any of the methods studied in these \ufb01elds can be used in the role of function approximator within reinforcement learning algorithms, although in practice some \ufb01t more easily into this role than others. Reinforcement learning with function approximation involves a number of new issues that do not normally arise in conventional supervised learning, such as nonstationarity, bootstrapping, and delayed targets. We introduce these and other issues successively over the \ufb01ve chapters of this part", "81649592-965f-4da4-9ba4-5cfdb5d95191": "A natural way to represent the means of the Bernoulli distributions is with a vector h of probabilities, with q(hj = 1 | v) = hy. We impose a restriction that hy is never equal to 0 or to 1, in order to avoid errors when computing, for example, log hy. We will see that the variational inference equations never assign 0 or 1 to hi analytically.\n\nIn a software implementation, however, machine rounding error could result in 0 or 1 values. In software, we may wish to implement binary sparse  639  CHAPTER 19. APPROXIMATE INFERENCE  https://www.deeplearningbook.org/contents/inference.html    coding using.an unrestricted vector of variational parameters 2 and obtain / via the relation h = o(Z). We_can thus safely compute log ht on a computer by using the identity log (zi) = = Cla 2), relating the sigmoid and the softplus. To begin our derivation of variational learning in the binary sparse coding model, we show that the use of this mean field approximation makes learning tractable", "e248d286-20a6-4854-b6e4-5e0a85372046": "In this work, we empirically surveyed data augmentation methods for limited-data learning in NLP and compared them on 11 different NLP tasks. Despite the success, there are still certain challenges that need to be tackled for improve their performance. This section highlights some of these challenges and discusses future research directions. Theoretical Guarantees and Data Distribution Shift. Current data augmentation methods for text typically assume that they are label-preserving and will not change the data distribution.\n\nHowever, these assumptions are often not true in practice, which can result in noisy labels or a shift in the data distribution and consequently a decrease in performance or generalization (e.g., QQP in Table 3). Thus, providing theoretical guarantees that augmentations are label- and distributionpreserving under certain conditions would ensure the quality of augmented data and further accelerate the progress of this \ufb01eld. Automatic Data Augmentation. Despite being effective, current data augmentation methods are generally manually-designed. Methods for automatically selecting the appropriate types of data augmentation still remain under-investigated. Although certain augmentation techniques have been shown effective for a particular task or dataset, they often do not transfer well to other datasets or tasks , as shown in Table 3", "ee7f4e52-9228-4c15-bb64-6cb3ecdd5185": "For an example of a sampling task using small natural images, see figure 16.1. Modeling a rich distribution over thousands or millions of random variables is a challenging task, both computationally and statistically. Suppose we wanted to model only binary variables. This is the simplest possible case, and yet already it seems overwhelming. For a small 32 x 32 pixel color (RGB) image, there are 23072 possible binary images of this form. This number is over 108\u00b0? times larger than the estimated number of atoms in the universe. In general, if we wish to model a distribution over a random vector x containing  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    nm discrete varlables capable ot taking on * values each, then the naive approach of representing /(X) by storing a lookup table with one probability value per possible outcome requires k parameters!\n\nThis is not feasible for several reasons:  e Memory\u2014the cost of storing the representation: For all but very small values of n and k, representing the distribution as a table will require too many values to store", "924a0e30-91ec-463c-ad70-3d95b1e6cde3": "I have tried to keep the mathematical content of the book to the minimum necessary to achieve a proper understanding of the \ufb01eld. However, this minimum level is nonzero, and it should be emphasized that a good grasp of calculus, linear algebra, and probability theory is essential for a clear understanding of modern pattern recognition and machine learning techniques. Nevertheless, the emphasis in this book is on conveying the underlying concepts rather than on mathematical rigour. I have tried to use a consistent notation throughout the book, although at times this means departing from some of the conventions used in the corresponding research literature.\n\nVectors are denoted by lower case bold Roman letters such as x, and all vectors are assumed to be column vectors. A superscript T denotes the transpose of a matrix or vector, so that xT will be a row vector. Uppercase bold roman letters, such as M, denote matrices. The notation (w1, . , wM) denotes a row vector with M elements, while the corresponding column vector is written as w = (w1, . , wM)T", "5f060b3e-2de0-462e-9de9-e8fda8268d04": "Each message sent from a node replaces any previous message sent in the same direction across the same link and will itself be a function only of the most recent messages received by that node at previous steps of the algorithm. We have seen that a message can only be sent across a link from a node when all other messages have been received by that node across its other links. Because there are loops in the graph, this raises the problem of how to initiate the message passing algorithm. To resolve this, we suppose that an initial message given by the unit function has been passed across every link in each direction. Every node is then in a position to send a message. There are now many possible ways to organize the message passing schedule. For example, the \ufb02ooding schedule simultaneously passes a message across every link in both directions at each time step, whereas schedules that pass one message at a time are called serial schedules. Following Kschischnang et al.\n\n, we will say that a (variable or factor) node a has a message pending on its link to a node b if node a has received any message on any of its other links since the last time it send a message to b", "63f17125-0a37-4604-b56f-281c9a37b0ab": "In addition to the uni\ufb01ed view of planning and learning methods, a second theme in this chapter is the bene\ufb01ts of planning in small, incremental steps. This enables planning to be interrupted or redirected at any time with little wasted computation, which appears to be a key requirement for e\ufb03ciently intermixing planning with acting and with learning of the model. Planning in very small steps may be the most e\ufb03cient approach even on pure planning problems if the problem is too large to be solved exactly. When planning is done online, while interacting with the environment, a number of interesting issues arise. New information gained from the interaction may change the model and thereby interact with planning.\n\nIt may be desirable to customize the planning process in some way to the states or decisions currently under consideration, or expected in the near future. If decision making and model learning are both computation-intensive processes, then the available computational resources may need to be divided between them. To begin exploring these issues, in this section we present Dyna-Q, a simple architecture integrating the major functions needed in an online planning agent. Each function appears in Dyna-Q in a simple, almost trivial, form. In subsequent sections we elaborate some of the alternate ways of achieving each function and the trade-o\u21b5s between them", "25fc5553-f4f5-42c3-815f-347ffc6bdf9e": "continues until the external stimulus ends.5 This is like assuming the animal\u2019s nervous system has a clock that keeps precise track of time during stimulus presentations; it is what engineers call a \u201ctapped delay line.\u201d Like the presence representation, the CSC representation is unrealistic as a hypothesis about how the brain internally represents stimuli, but Ludvig et al. call it a \u201cuseful \ufb01ction\u201d because it can reveal details of how the TD model works when relatively unconstrained by the stimulus representation. The CSC representation is also used in most TD models of dopamine-producing neurons in the brain, a topic we take up in Chapter 15. The CSC representation is often viewed as an essential part of the TD model, although this view is mistaken.\n\nThe MS representation (center column of Figure 14.1) is like the CSC representation in that each external stimulus initiates a cascade of internal stimuli, but in this case the internal stimuli\u2014the microstimuli\u2014are not of such limited and non-overlapping form; they are extended over time and overlap. As time elapses from stimulus onset, di\u21b5erent sets of microstimuli become more or less active, and each subsequent microstimulus becomes progressively wider in time and reaches a lower maximal level", "bb9792a4-a262-4f18-b0d5-f7711320a9a9": "For this, we will need more than what can be done with a simple linear transformation. 5.8.2 k-means Clustering  Another example of a simple representation learning algorithm is k-means clustering. The k-means clustering algorithm divides the training set into k different clusters of examples that are near each other. We can thus think of the algorithm as providing a k-dimensional one-hot code vector h representing an input x.\n\nIf x belongs to cluster i, then h; = 1, and all other entries of the representation h are zero. The one-hot code provided by /means clustering is an example of a sparse representation, because the majority of its entries are zero for every input. Later, we develop other algorithms that learn more flexible sparse representations, where more than one entry can be nonzero for each input x. One-hot codes are an extreme example of sparse representations that lose many of the benefits of a distributed  https://www.deeplearningbook.org/contents/ml.html    representation", "b78d9127-5cf6-4ca2-8843-8889ff83b376": ", xn, and multiply by the transition probability p(zn|zn\u22121) and the emission probability p(xn|zn) and then marginalize over zn\u22121, we obtain a distribution over zn that is of the same functional form as that over \ufffd\u03b1(zn\u22121). That is to say, the distribution must not become more complex at each stage, but must only change in its parameter values.\n\nNot surprisingly, the only distributions that have this property of being closed under multiplication are those belonging to the exponential family. Here we consider the most important example from a practical perspective, which is the Gaussian. In particular, we consider a linear-Gaussian state space model so that the latent variables {zn}, as well as the observed variables {xn}, are multivariate Gaussian distributions whose means are linear functions of the states of their parents in the graph. We have seen that a directed graph of linear-Gaussian units is equivalent to a joint Gaussian distribution over all of the variables. Furthermore, marginals such as \ufffd\u03b1(zn) are also Gaussian, so that the functional form of the messages is preserved and we will obtain an ef\ufb01cient inference algorithm", "7043ff3d-08e9-4e63-9ddf-d188c0d304d0": "Note that each policy evaluation, itself an iterative computation, is started with the value function for the previous policy. This typically results in a great increase in the speed of convergence of policy evaluation (presumably because the value function changes little from one policy to the next). Policy Iteration (using iterative policy evaluation) for estimating \u21e1 \u21e1 \u21e1\u21e4 until \u2206 < \u2713 (a small positive number determining the accuracy of estimation) If policy-stable, then stop and return V \u21e1 v\u21e4 and \u21e1 \u21e1 \u21e1\u21e4; else go to 2 rental company. Each day, some number of customers arrive at each location to rent cars. If Jack has a car available, he rents it out and is credited $10 by the national company.\n\nIf he is out of cars at that location, then the business is lost. Cars become available for renting the day after they are returned. To help ensure that cars are available where they are needed, Jack can move them between the two locations overnight, at a cost of $2 per car moved. We assume that the number of cars requested and returned at each location are Poisson random variables, meaning that the probability that the number is n is \u03bbn n! e\u2212\u03bb, where \u03bb is the expected number", "e01805ab-0502-4bb1-8d1d-72020226d9a3": "The position of the particle at any point in time is given by @(t). The particle experiences net force f(t). This force causes the particle to accelerate:  o2 t) = =59(t). 8.18 F(t) = att) (8.18) 294  CHAPTER 8.\n\nOPTIMIZATION FOR TRAINING DEEP MODELS  https://www.deeplearningbook.org/contents/optimization.html    Rather than viewing this as a second-order differential equation of the position, we can introduce the variable v(t) representing the velocity of the particle at time  t and rewrite the Newtonian dynamics as a first-order differential equation:  (a)  v(t) = RO)\u00bb (8.19) (a)  f= ae): (8.20)  The momentum algorithm then consists of solving the differential equations via numerical simulation. A simple numerical method for solving differential equations is Euler\u2019s method, which simply consists of simulating the dynamics defined by the equation by taking small, finite steps in the direction of each gradient. This explains the basic form of the momentum update, but what specifically are the forces? One force is proportional to the negative gradient of the cost function: \u2014VoJ(8)", "eb4370b8-f465-45ae-8088-37438b2168cf": "An early version of this paper was originally published in 1982.\n\nThe MacSX was not designed and managed by Kia.\\n\\n Macintosh\\n The mac has been a family invention space: comet asteroid spaceship This essay discusses a topic: the impact of two of the Earth\u2019s two-thirds comet-sized moon Charon on Earth, and why asteroids are so close to the sun; why people are looking for ways to \ufb01nd a way to keep Earth-shaped asteroids out of orbit. religion: faith faith faith salvation This essay discusses the impact religion has on the American experience and in American culture. Since the beginning of my career I have found that faith and belief have often been linked to economic growth, social development and education. I believe that all people need to know that there is no reason for science: climate research chemistry This essay discusses the role of molecular information and its interaction with the general organism and human health.\\n\\n \"The idea of biological information is not really a new concept. We used genetic information as a medium to de\ufb01ne, identify, and store information about biology and biology,\" explains Dr", "0a002323-6fc2-4993-ac9c-92a758ac08b4": "16.2.5 Separation and D-Separation  The edges in a graphical model tell us which variables directly interact. We often need to know which variables indirectly interact. Some of these indirect interactions can be enabled or disabled by observing other variables. More formally, we would like to know which subsets of variables are conditionally independent from each other, given the values of other subsets of variables. Identifying the conditional independences in a graph is simple for undirected models. In this case, conditional independence implied by the graph is called separation.\n\nWe say that a set of variables A is separated from another set of variables B given a third set of variables S if the graph structure implies that A is independent from B given S. If two variables a and b are connected by a path involving only unobserved variables, then those variables are not separated. If no path exists between them, or all paths contain an observed variable, then they are separated. We refer to paths involving only unobserved variables as \u201cactive\u201d and paths including an observed variable as \u201cinactive.\u201d  When we draw a graph, we can indicate observed variables by shading them in", "5e603639-c47a-4858-afa4-be00bcc21f5d": "Regarding the update of augmentation parameters \u03c6 (Eq.8), since text samples are discrete, to enable ef\ufb01cient gradient propagation through \u03b8\u2032 to \u03c6, we use a gumbel-softmax approximation  to x when sampling substitution words from the LM. Learning Data Weights We now demonstrate the instantiation of data weighting. We aim to assign an importance weight to each training example to adapt its effect on model training. We automate the process by learning the data weights. This is achieved by parameterizing R\u03c6 as: In practice, when minibatch stochastic optimization is used, we approximate the weighted sampling by taking the softmax over the weights of only the minibatch examples. The data weights \u03c6 are updated with Eq.(8). It is worth noting that the previous work  similarly derives data weights based on their gradient directions on a validation set.\n\nOur algorithm differs in that the data weights are parameters maintained and updated throughout the training, instead of re-estimated from scratch in each iteration. Experiments show the parametric treatment achieves superior performance in various settings. There are alternative parameterizations of R\u03c6 other than Eq.(11)", "25eeab9d-0359-470d-af16-e15874bf6285": "Speci\ufb01cally, suppose that, for any time step, the true values of actions 1 and 2 are respectively 0.1 and 0.2 with probability 0.5 (case A), and 0.9 and 0.8 with probability 0.5 (case B). If you are not able to tell which case you face at any step, what is the best expectation of success you can achieve and how should you behave to achieve it? Now suppose that on each step you are told whether you are facing case A or case B (although you still don\u2019t know the true action values). This is an associative search task. What is the best expectation of success you can achieve in this task, and how should you behave to achieve it? \u21e4 We have presented in this chapter several simple ways of balancing exploration and exploitation. The \"-greedy methods choose randomly a small fraction of the time, whereas UCB methods choose deterministically but achieve exploration by subtly favoring at each step the actions that have so far received fewer samples. Gradient bandit algorithms estimate not action values, but action preferences, and favor the more preferred actions in a graded, probabilistic manner using a soft-max distribution.\n\nThe simple expedient of initializing estimates optimistically causes even greedy methods to explore signi\ufb01cantly", "8f43f9fb-f362-47d4-a9d6-79350747e906": "We can think of a location-based read instruction as saying \u201cRetrieve the lyrics of the song in slot 347.\u201d Location-based addressing can often be a perfectly sensible mechanism even when the memory cells are small. If the content of a memory cell is copied (not forgotten) at most time steps, then the information it contains can be propagated forward in time and the gradients propagated backward in time without either vanishing or exploding. The explicit memory approach is illustrated in figure 10.18, where we see that a \u201ctask neural network\u201d is coupled with a memory.\n\nAlthough that task neural network could be feedforward or recurrent, the overall system is a recurrent network. The task network can choose to read from or write to specific memory addresses. 413  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  https://www.deeplearningbook.org/contents/rnn.html    Memory cells  Reading Writing  mechanism  mechanism  Task network, controlling the memory  Figure 10.18: A schematic of a network with an explicit memory, capturing some of the key design elements of the neural Turing machine", "d715012b-14e2-40a3-bdb6-6f7d36d89d51": "Some variants of ICA avoid this problematic operation by constraining W to be orthogonal. All variants of ICA require that p(h) be non-Gaussian. This is because if p(h)  https://www.deeplearningbook.org/contents/linear_factors.html    is an independent prior with Gaussian components, then W is not identifiable. We can obtain the same distribution over p(#) for many values of W. This is very different from other linear factor models like probabilistic PCA and factor analysis,  which often require p(h) to be Gaussian in order to make many operations on the model have closed form solutions.\n\nIn the maximum likelihood approach, where the user explicitly specifies the distribution, a typical choice is to use p(h;) = toh): Typical choices of these non-Gaussian distributions have larger peaks near 0 than does the Gaussian distribution, so we can also see most implementations of ICA as learning sparse features. Many variants of ICA are not generative models in the sense that we use the  488  CHAPTER 13. LINEAR FACTOR MODELS  phrase", "5320df97-5853-4e00-aa88-fe420d1fc4ba": "In Proceedings of the 2nd Berkeley Symposium on Mathematical Statistics and Probabilities, pp. 481\u2013492. University of California Press. Kullback, S. and R. A. Leibler . On information and suf\ufb01ciency. Annals of Mathematical Statistics 22(1), 79\u201386. K\u02d9urkov\u00b4a, V. and P. C. Kainen . Functionally equivalent feed-forward neural networks. Neural Computation 6(3), 543\u2013558. Kuss, M. and C. Rasmussen . Assessing approximations for Gaussian process classi\ufb01cation. LeCun, Y., L. Bottou, Y. Bengio, and P. Haffner . Gradient-based learning applied to document recognition. Proceedings of the IEEE 86, 2278\u20132324. Lee, Y., Y. Lin, and G. Wahba", "4c12ce5c-aff8-4acf-a21f-2dbdbc4dccc2": "Thus the model predicts a noise variance orthogonal to the principal subspace, which, from (12.46), is just the average of the discarded eigenvalues. Now suppose that v = Ui where Ui is one of the retained eigenvectors defining the principal subspace. Then vTCv = (Ai (J'2) +  or through the EM algorithm, then the resulting value of R is essentially arbitrary. This implies that the columns of W need not be orthogonal. If an orthogonal basis is required, the matrix W can be post-processed appropriately . Alternatively, the EM algorithm can be modified in such a way as to yield orthonormal principal directions, sorted in descending order of the corresponding eigenvalues, directly . The rotational invariance in latent space represents a form of statistical nonidentifiability, analogous to that encountered for mixture models in the case of discrete latent variables", "4f6b5c73-1cc9-4162-91f1-779cced896c0": "Known Problems  Exploration-Exploitation Dilemma  The problem of exploration vs exploitation dilemma has been discussed in my previous post.\n\nWhen the RL problem faces an unknown environment, this issue is especially a key to finding a good solution: without enough exploration, we cannot learn the environment well enough; without  https://lilianweng.github.io/posts/2018-02-19-rl-overview/  A (Long) Peek into Reinforcement Learning | Lil'Log  enough exploitation, we cannot complete our reward optimization task. Different RL algorithms balance between exploration and exploitation in different ways. In MC methods, Q-learning or many on-policy algorithms, the exploration is commonly implemented by \u00a2- greedy; In ES, the exploration is captured by the policy parameter perturbation. Please keep this into consideration when develop a new RL algorithm. Deadly Triad Issue  We do seek the efficiency and flexibility of TD methods that involve bootstrapping. However, when off-policy, nonlinear function approximation, and bootstrapping are combined in one RL algorithm, the training could be unstable and hard to converge. This issue is known as the deadly triad", "d135f37d-dd02-422d-90c7-2ff1d47408e4": "The frame was preprocessed and added to the four-frame stack that became the next input to the network. Skipping for the moment the changes to the basic Q-learning procedure made by Mnih et al., DQN used the following semi-gradient form of Q-learning to update the network\u2019s weights: where wt is the vector of the network\u2019s weights, At is the action selected at time step t, and St and St+1 are respectively the preprocessed image stacks input to the network at time steps t and t + 1. The gradient in (16.3) was computed by backpropagation. Imagining again that there was a separate network for each action, for the update at time step t, backpropagation was applied only to the network corresponding to At. Mnih et al.\n\ntook advantage of techniques shown to improve the basic backpropagation algorithm when applied to large networks. They used a mini-batch method that updated weights only after accumulating gradient information over a small batch of images (here after 32 images). This yielded smoother sample gradients compared to the usual procedure that updates weights after each action", "e2972032-300d-4d58-9de2-bc7d3bf7a78d": "An extension to mixtures of conditional Gaussian distributions, which permit multimodal conditional distributions, will be discussed in Section 14.5.1. Now consider a data set of inputs X = {x1, . , xN} with corresponding target values t1, . , tN. We group the target variables {tn} into a column vector that we denote by t where the typeface is chosen to distinguish it from a single observation of a multivariate target, which would be denoted t. Making the assumption that these data points are drawn independently from the distribution (3.8), we obtain the following expression for the likelihood function, which is a function of the adjustable parameters w and \u03b2, in the form where we have used (3.3). Note that in supervised learning problems such as regression (and classi\ufb01cation), we are not seeking to model the distribution of the input variables", "f1c3621d-8d8d-4a70-940e-3c5d2e536047": "Memory-based methods such as the weighted average and locally weighted regression methods described above depend on assigning weights to examples s0 7! g in the database depending on the distance between s0 and a query states s. The function that assigns these weights is called a kernel function, or simply a kernel. In the weighted average and locally weighted regressions methods, for example, a kernel function k : R ! R assigns weights to distances between states. More generally, weights do not have to depend on distances; they can depend on some other measure of similarity between states. In this case, k : S \u21e5 S ! R, so that k(s, s0) is the weight given to data about s0 in its in\ufb02uence on answering queries about s. Viewed slightly di\u21b5erently, k(s, s0) is a measure of the strength of generalization from s0 to s", "1f6d4d47-72e8-495d-88ea-bf9e197f36f6": "Let \u03b8 and \u03b8\u2032 be two parameter vectors in Rd. Then, we will \ufb01rst attempt to bound W(P\u03b8, P\u03b8\u2032), from where the theorem will come easily. The main element of the proof is the use of the coupling \u03b3, the distribution of the joint (g\u03b8(Z), g\u03b8\u2032(Z)), which clearly has \u03b3 \u2208 \u03a0(P\u03b8, P\u03b8\u2032). By the de\ufb01nition of the Wasserstein distance, we have If g is continuous in \u03b8, then g\u03b8(z) \u2192\u03b8\u2192\u03b8\u2032 g\u03b8\u2032(z), so \u2225g\u03b8 \u2212 g\u03b8\u2032\u2225 \u2192 0 pointwise as functions of z. Since X is compact, the distance of any two elements in it has to be uniformly bounded by some constant M, and therefore \u2225g\u03b8(z) \u2212 g\u03b8\u2032(z)\u2225 \u2264 M for all \u03b8 and z uniformly. By the bounded convergence theorem, we therefore have whenever (\u03b8\u2032, z) \u2208 U. Therefore, we can de\ufb01ne U\u03b8 = {\u03b8\u2032|(\u03b8\u2032, z) \u2208 U}", "36026407-02c9-4ec9-b4f9-74a6de65a105": "On the Feret dataset, accuracy improved from 83.52 to 88.46%. The audience dataset responded with an improvement of 70.02% to 76.06%.\n\nMost interestingly, results from another face dataset increased from 88.15 to 95.66%. This was compared with traditional augmentation techniques which increased the accuracy from 88.15 to 89.08%. Addition- ally, this experiment derived the same accuracy when using two Network-As in the aug-  mentation framework as was found with one Network-A. This experiment demonstrates    6:60  Shorten and Khoshgoftaar J Big Data  s0U9 uoljezjeW0U qno-doig 1nez|| auenbs ueaw yozeg (vq) vonsuny sso7  =|  Adosyua -$s019 jesu08aye> (87) vonzuny sso7  yo yndyri", "43132575-a7ce-4c4d-97ce-1043ef297cd7": "Once again, the derivative of the error function with respect to the activation for In summary, there is a natural choice of both output unit activation function and matching error function, according to the type of problem being solved. For regression we use linear outputs and a sum-of-squares error, for (multiple independent) binary classi\ufb01cations we use logistic sigmoid outputs and a cross-entropy error function, and for multiclass classi\ufb01cation we use softmax outputs with the corresponding multiclass cross-entropy error function. For classi\ufb01cation problems involving two classes, we can use a single logistic sigmoid output, or alternatively we can use a network with two outputs having a softmax output activation function. We turn next to the task of \ufb01nding a weight vector w which minimizes the chosen function E(w).\n\nAt this point, it is useful to have a geometrical picture of the error function, which we can view as a surface sitting over weight space as shown in Figure 5.5", "3ff62cd0-32fe-4952-bd97-c51fa132c3df": "Dro opping terms of \u00a3 that do not vary with ys, we are left with the optimization prob  p* = arg max log p(h = p, v), (19.12) wb  which is equivalent to the MAP inference problem  h* = argmax p(h | v). (19.13) h  We can thus justify a learning procedure similar to EM, in which we alternate between performing MAP inference to infer h* and then update @ to increase log p(h*,v). As with EM, this is a form of coordinate ascent on \u00a3, where we alternate between using inference to optimize \u00a3 with respect to q and using parameter updates to optimize \u00a3 with respect to 6. The procedure as a whole can be justified by the fact that Lis a lower bound on log p(v). In the case of MAP inference, this justification is rather vacuous, because the bound is infinitely loose, due to the Dirac distribution\u2019s differential entropy of negative infinity. Adding noise to would make the bound meaningful again", "2bea502b-4cf2-4454-a564-f05bf55293f9": "Let us write the joint distribution over a set of variables in the form of a product of factors p(x) = \ufffd where xs denotes a subset of the variables. For convenience, we shall denote the individual variables by xi, however, as in earlier discussions, these can comprise groups of variables (such as vectors or matrices). Each factor fs is a function of a corresponding set of variables xs. Directed graphs, whose factorization is de\ufb01ned by (8.5), represent special cases of (8.59) in which the factors fs(xs) are local conditional distributions. Similarly, undirected graphs, given by (8.39), are a special case in which the factors are potential functions over the maximal cliques (the normalizing coef\ufb01cient 1/Z can be viewed as a factor de\ufb01ned over the empty set of variables).\n\nIn a factor graph, there is a node (depicted as usual by a circle) for every variable in the distribution, as was the case for directed and undirected graphs. There are also additional nodes (depicted by small squares) for each factor fs(xs) in the joint distribution", "b87bf92a-1618-460f-a359-d6330c88bd22": "When dis an eigenvector of H,, the second derivative in that direction is given by the corresponding eigenvalue.\n\nFor other directions of d, the directional second derivative is a weighted average of all the eigenvalues, with weights between 0 and 1, and eigenvectors that have a smaller angle with d receiving more weight. The maximum eigenvalue determines the maximum second derivative, and the minimum eigenvalue determines the minimum second derivative. The (directional) second derivative tells us how well we can expect a gradient descent step to perform. We can make a second-order Taylor series approximation  (0)  4a 40 Latta LN 3-4-2 A LL Arent Att me  https://www.deeplearningbook.org/contents/numerical.html    LO LUG LULCLIOL J (we) ALOULLU LUG CULLELL PULL w&  fe) ~ fa) +(e-2)\"g+3(e-2)' Hwa), (48) where g is the gradient and H is the Hessian at 2)", "df3b514e-f7f1-487b-9afa-3662d95e632f": "8.21 (\u22c6 \u22c6) www Show that the marginal distributions p(xs) over the sets of variables xs associated with each of the factors fx(xs) in a factor graph can be found by \ufb01rst running the sum-product message passing algorithm and then evaluating the required marginals using (8.72).\n\n8.22 (\u22c6) Consider a tree-structured factor graph, in which a given subset of the variable nodes form a connected subgraph (i.e., any variable node of the subset is connected to at least one of the other variable nodes via a single factor node). Show how the sum-product algorithm can be used to compute the marginal distribution over that subset. 8.23 (\u22c6 \u22c6) www In Section 8.4.4, we showed that the marginal distribution p(xi) for a variable node xi in a factor graph is given by the product of the messages arriving at this node from neighbouring factor nodes in the form (8.63). Show that the marginal p(xi) can also be written as the product of the incoming message along any one of the links with the outgoing message along the same link", "7cade3e0-5748-4370-8adf-91a817646936": "Further insight into the role of the equivalent kernel can be obtained by considering the covariance between y(x) and y(x\u2032), which is given by where we have made use of (3.49) and (3.62). From the form of the equivalent kernel, we see that the predictive mean at nearby points will be highly correlated, whereas for more distant pairs of points the correlation will be smaller. The predictive distribution shown in Figure 3.8 allows us to visualize the pointwise uncertainty in the predictions, governed by (3.59).\n\nHowever, by drawing samples from the posterior distribution over w, and plotting the corresponding model functions y(x, w) as in Figure 3.9, we are visualizing the joint uncertainty in the posterior distribution between the y values at two (or more) x values, as governed by the equivalent kernel. The formulation of linear regression in terms of a kernel function suggests an alternative approach to regression as follows. Instead of introducing a set of basis functions, which implicitly determines an equivalent kernel, we can instead de\ufb01ne a localized kernel directly and use this to make predictions for new input vectors x, given the observed training set. This leads to a practical framework for regression (and classi\ufb01cation) called Gaussian processes, which will be discussed in detail in Section 6.4", "03c5e6d7-2f19-4e69-a2cc-5992f46558c2": "Perhaps the \ufb01rst to succinctly express the essence of trial-and-error learning as a principle of learning was Edward Thorndike: Of several responses made to the same situation, those which are accompanied or closely followed by satisfaction to the animal will, other things being equal, be more \ufb01rmly connected with the situation, so that, when it recurs, they will be more likely to recur; those which are accompanied or closely followed by discomfort to the animal will, other things being equal, have their connections with that situation weakened, so that, when it recurs, they will be less likely to occur.\n\nThe greater the satisfaction or discomfort, the greater the strengthening or weakening of the bond. (Thorndike, 1911, p. 244) Thorndike called this the \u201cLaw of E\u21b5ect\u201d because it describes the e\u21b5ect of reinforcing events on the tendency to select actions. Thorndike later modi\ufb01ed the law to better account for subsequent data on animal learning (such as di\u21b5erences between the e\u21b5ects of reward and punishment), and the law in its various forms has generated considerable controversy among learning theorists", "7d9fe85b-865d-468d-901d-b2f32563531e": "In this setting, each synapse has its own eligibility trace that records past activity involving that synapse. The only di\u21b5erence between the actor and critic learning rules is that they use di\u21b5erent kinds of eligibility traces: the critic unit\u2019s traces are non-contingent because they do not involve the critic unit\u2019s output, whereas the actor unit\u2019s traces are contingent because in addition to the actor unit\u2019s input, they depend on the actor unit\u2019s output.\n\nIn the hypothetical implementation of an actor\u2013critic system in the brain, these learning rules respectively correspond to rules governing plasticity of corticostriatal synapses that convey signals from the cortex to the principal neurons in the dorsal and ventral striatal subdivisions, synapses that also receive inputs from dopamine neurons. The learning rule of an actor unit in the actor\u2013critic network closely corresponds to reward-modulated spike-timing-dependent plasticity. In spike-timing-dependent plasticity (STDP), the relative timing of pre- and postsynaptic activity determines the direction of synaptic change", "4d80d86f-4755-4b92-8bfe-07f12f0f740b": "Figure 10.4 shows the results of a more detailed study of the e\u21b5ect of the parameters \u21b5 and n on the rate of learning on this task. Exercise 10.1 We have not explicitly considered or given pseudocode for any Monte Carlo methods in this chapter. What would they be like? Why is it reasonable not to give pseudocode for them?\n\nHow would they perform on the Mountain Car task? \u21e4 Exercise 10.2 Give pseudocode for semi-gradient one-step Expected Sarsa for control. \u21e4 We now introduce a third classical setting\u2014alongside the episodic and discounted settings\u2014 for formulating the goal in Markov decision problems (MDPs). Like the discounted setting, the average reward setting applies to continuing problems, problems for which the interaction between agent and environment goes on and on forever without termination or start states. Unlike that setting, however, there is no discounting\u2014the agent cares just as much about delayed rewards as it does about immediate reward. The average-reward setting is one of the major settings commonly considered in the classical theory of dynamic programming and less-commonly in reinforcement learning. As we discuss in the next section, the discounted setting is problematic with function approximation, and thus the average-reward setting is needed to replace it", "77c0f777-ea97-49dd-916d-71a832188a87": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19\u201327.\n\nAppendix for \u201cBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u201d We organize the appendix into three sections: \u2022 Additional implementation details for BERT are presented in Appendix A; \u2022 Additional details for our experiments are presented in Appendix B; and We provide examples of the pre-training tasks in the following. hairy, and during the random masking procedure we chose the 4-th token (which corresponding to hairy), our masking procedure can be further illustrated by \u2022 80% of the time: Replace the word with the  token, e.g., my dog is hairy \u2192 \u2022 10% of the time: Replace the word with a random word, e.g., my dog is hairy \u2192 my is hairy. The purpose of this is to bias the representation towards the actual observed word. The advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predict or which have been replaced by random words, so it is forced to keep a distributional contextual representation of every input token", "f96b09c6-e227-4211-b35e-3752f2b02aff": "Instead of using the raw embeddings directly, we need to refine the embedding with further fine-tuning. Natural Language Inference (NLI) tasks are the main data sources to provide supervised signals for learning sentence embedding; such as SNLI, MNLI, and QQP.\n\nSentence-BERT  SBERT (Sentence-BERT)  relies on siamese and triplet network architectures to learn sentence embeddings such that the sentence similarity can be estimated by cosine similarity between pairs of embeddings. Note that learning SBERT depends on supervised data, as it is fine-tuned on several NLI datasets. They experimented with a few different prediction heads on top of BERT model:  Softmax classification objective: The classification head of the siamese network is built on the concatenation of two embeddings f(x), f(x\u2019) and | f(x) \u2014 f(x\u2019)|. The predicted output is 9 = softmax(W,|f(x); f(x\u2019); | f(x) \u2014 f(\u00ab\u2019)|])", "c38531b0-c07b-491d-bba2-f61098294e3e": "Thus, by (9.8), the approximate value function will be a\u21b5ected at all states within the union of the circles, with a greater e\u21b5ect the more circles a point has \u201cin common\u201d with the state, as shown in Figure 9.6. If the circles are small, then the generalization will be over a short distance, as in Figure 9.7 (left), whereas if they are large, it will be over a large distance, as in Figure 9.7 (middle). Moreover, the shape of the features will determine the nature of the generalization. For example, if they are not strictly circular, but are elongated in one direction, then generalization will be similarly a\u21b5ected, as in Figure 9.7 (right).\n\nFeatures with large receptive \ufb01elds give broad generalization, but might also seem to limit the learned function to a coarse approximation, unable to make discriminations much \ufb01ner than the width of the receptive \ufb01elds. Happily, this is not the case. Initial generalization from one point to another is indeed controlled by the size and shape of the receptive \ufb01elds, but acuity, the \ufb01nest discrimination ultimately possible, is controlled more by the total number of features", "df1ac8eb-34b8-473c-b32b-5fd879b6025a": "An analysis of temporal-di\u21b5erence learning with function approximation. IEEE Transactions on Automatic Control, 42(5):674\u2013690. Tsitsiklis, J. N., Van Roy, B. Average cost temporal-di\u21b5erence learning. Automatica, Turing, A. M. Intelligent machinery. In B. Jack Copeland (Ed.) , The Essential Turing, pp. 410\u2013432. Oxford University Press, Oxford. Ungar, L. H. A bioreactor benchmark for adaptive network-based process control. In W. T. Miller, R. S. Sutton, and P. J. Werbos (Eds. ), Neural Networks for Control, pp. 387\u2013402. MIT Press, Cambridge, MA. Unnikrishnan, K. P., Venugopal, K. P", "228d81b6-8cfb-47f7-843d-4d80eecfac8c": "We therefore need to solve the inverse problem, which has two solutions as seen in Figure 5.18. Forward problems often corresponds to causality in a physical system and generally have a unique solution. For instance, a speci\ufb01c pattern of symptoms in the human body may be caused by the presence of a particular disease. In pattern recognition, however, we typically have to solve an inverse problem, such as trying to predict the presence of a disease given a set of symptoms. If the forward problem involves a many-to-one mapping, then the inverse problem will have multiple solutions. For instance, several different diseases may result in the same symptoms. In the robotics example, the kinematics is de\ufb01ned by geometrical equations, and the multimodality is readily apparent. However, in many machine learning problems the presence of multimodality, particularly in problems involving spaces of high dimensionality, can be less obvious.\n\nFor tutorial purposes, however, we shall consider a simple toy problem for which we can easily visualize the multimodality", "c20da818-b6a6-4e4a-8103-5697264c0acf": "However, the greater \ufb02exibility of the variational approximation leads to improved accuracy compared to the Laplace method.\n\nFurthermore (unlike the Laplace method), the variational approach is optimizing a well de\ufb01ned objective function given by a rigourous bound on the model evidence. Logistic regression has also been treated by Dybowski and Roberts  from a Bayesian perspective using Monte Carlo sampling techniques. Here we shall make use of a variational approximation based on the local bounds introduced in Section 10.5. This allows the likelihood function for logistic regression, which is governed by the logistic sigmoid, to be approximated by the exponential of a quadratic form. It is therefore again convenient to choose a conjugate Gaussian prior of the form (4.140). For the moment, we shall treat the hyperparameters m0 and S0 as \ufb01xed constants. In Section 10.6.3, we shall demonstrate how the variational formalism can be extended to the case where there are unknown hyperparameters whose values are to be inferred from the data. In the variational framework, we seek to maximize a lower bound on the marginal likelihood", "9b8786d7-b754-41c5-9d5f-1e2d4ecea5a4": "Such a hybrid criterion had previously been introduced for RBMs  https://www.deeplearningbook.org/contents/generative_models.html    by Larochelle and Bengio . They show improved classification performance using this scheme. 20.13 Other Generation Schemes  The methods we have described so far use either MCMC sampling, ancestral sampling, or some mixture of the two to generate samples. While these are the most popular approaches to generative modeling, they are by no means the only approaches. Sohl-Dickstein ef al. developed a diffusion inversion training scheme for learning a generative model, based on nonequilibrium thermodynamics.\n\nThe approach is based on the idea that the probability distributions we wish to sample from have structure. This structure can gradually be destroyed by a diffusion process that incrementally changes the probability distribution to have more entropy. To form a generative model, we can run the process in reverse, by training a model that gradually restores the structure to an unstructured distribution. By iteratively applying a process that brings a distribution closer to the target one, we can gradually approach that target distribution", "00713bde-d951-4873-91d6-418b7af98227": "However, this can result in much slower training because, instead of solving K separate optimization problems each over N data points with an overall cost of O(KN 2), a single optimization problem of size (K \u22121)N must be solved giving an overall cost of O(K2N 2). Another approach is to train K(K \u22121)/2 different 2-class SVMs on all possible pairs of classes, and then to classify test points according to which class has the highest number of \u2018votes\u2019, an approach that is sometimes called one-versus-one. Again, we saw in Figure 4.2 that this can lead to ambiguities in the resulting classi\ufb01cation. Also, for large K this approach requires signi\ufb01cantly more training time than the one-versus-the-rest approach. Similarly, to evaluate test points, signi\ufb01cantly more computation is required. The latter problem can be alleviated by organizing the pairwise classi\ufb01ers into a directed acyclic graph (not to be confused with a probabilistic graphical model) leading to the DAGSVM", "766217a7-a6b4-4075-8442-224370c083fc": "Then the associative strengths of the stimulus components change according to these expressions: where \u21b5A\u03b2Y and \u21b5X\u03b2Y are the step-size parameters, which depend on the identities of the CS components and the US, and RY is the asymptotic level of associative strength that the US Y can support. (Rescorla and Wagner used \u03bb here instead of R, but we use R to avoid confusion with our use of \u03bb and because we usually think of this as the magnitude of a reward signal, with the caveat that the US in classical conditioning is not necessarily rewarding or penalizing.) A key assumption of the model is that the aggregate associative strength VAX is equal to VA + VX. The associative strengths as changed by these \u2206s become the associative strengths at the beginning of the next trial. To be complete, the model needs a response-generation mechanism, which is a way of mapping values of V s to CRs. Because this mapping would depend on details of the experimental situation, Rescorla and Wagner did not specify a mapping but simply assumed that larger V s would produce stronger or more likely CRs, and that negative V s would mean that there would be no CRs", "f7b6d4c7-275a-433c-944c-c3c44186e5b2": "This presumably is why the exhaustive, unfocused approach does better in the long run, at least for small problems.\n\nThese results are not conclusive because they are only for problems generated in a particular, random way, but they do suggest that sampling according to the on-policy distribution can be a great advantage for large problems, in particular for problems in which a small subset of the state\u2013action space is visited under the on-policy distribution. Exercise 8.7 Some of the graphs in Figure 8.8 seem to be scalloped in their early portions, particularly the upper graph for b = 1 and the uniform distribution. Why do you think this is? What aspects of the data shown support your hypothesis? \u21e4 Real-time dynamic programming, or RTDP, is an on-policy trajectory-sampling version of the value-iteration algorithm of dynamic programming (DP). Because it is closely related to conventional sweep-based policy iteration, RTDP illustrates in a particularly clear way some of the advantages that on-policy trajectory sampling can provide. RTDP updates the values of states visited in actual or simulated trajectories by means of expected tabular value-iteration updates as de\ufb01ned by (4.10)", "123e4887-ce7d-4dc1-958e-fd388ff71e12": "Another form of dynamic experience comes from the changing environments, such as the data stream in online learning  whose distribution can change over time, or the experience in lifelong learning  that di\ufb00ers across a series of tasks. In particular, we consider an online setting: at each time \u03c4 \u2208 {1, ..., T}, a predictor is given an input and is required to make a prediction (e.g., if the stock market will go up or down tomorrow where). We have access to the recommended prediction by each of the K experts t = {1, ..., K}, and make our prediction accordingly. As a result, the environment reveals a reward based on the discrepancy between the prediction and the true answer.\n\nThe sequence of data instances follows a dynamic that is unknown and can even be adversarially adaptive to the predictor\u2019s behavior (e.g., in the problem of spam email \ufb01ltering, or other strategic game environments) . In such cases, we can only hope the predictor to achieve some relative performance guarantee, in particular w.r.t. the best single expert in hindsight. This is formally captured by regret, which is the di\ufb00erence between the cumulative reward of the predictor and that of the best single expert", "f0854b40-eb46-4b8c-ac0a-ae492cb0c2d8": "(20.4 The energy function for an RBM is given by E(v,h) =\u2014b'v\u2014clh\u2014v' Wh, (20.5  and Z is the normalizing constant known as the partition function:  Z=S_Soexp{-E(v,h)}. (20.6 v oh  It is apparent from the definition of the partition function Z that the naive method of computing Z (exhaustively summing over all states) could be computationally intractable, unless a cleverly designed algorithm could exploit regularities in the probability distribution to compute Z faster. In the case of restricted Boltzmann machines, Long and Servedio  formally proved that the partition function Z is intractable. The intractable partition function Z implies that the normalized joint probability distribution P(v) is also intractable to evaluate", "1d771a12-ccd1-4af3-aec3-2f13dda38699": "MACHINE LEARNING BASICS  introduce some dependencies between the regions through additional assumptions  https://www.deeplearningbook.org/contents/ml.html    about the underlying data-generating distribution. In this way, we can actually generalize nonlocally . Many ifferent deep learning algorithms provide implicit or explicit assumptions that are reasonable for a broad range of AI tasks in order to capture these advantages. Other approaches to machine learning often make stronger, task-specific as- sumptions. For example, we could easily solve the checkerboard task by providing the assumption that the target function is periodic. Usually we do not include such strong, task-specific assumptions in neural networks so that they can generalize to a much wider variety of structures. AI tasks have structure that is much too complex to be limited to simple, manually specified properties such as periodicity, so we want learning algorithms that embody more general-purpose assumptions.\n\nThe core idea in deep learning is that we assume that the data was generated by the composition of factors, or features, potentially at multiple levels in a hierar- chy. Many other similarly generic assumptions can further improve deep learning algorithms. These apparently mild assumptions allow an exponential gain in the relationship between the number of examples and the number of regions that can be distinguished", "2a848beb-3728-4b13-bdf6-97618cbd3222": "If the target and behavior policies are very di\u21b5erent it probably needs some new algorithmic ideas before it can be e\ufb03cient and practical. The other, based on tree-backup updates, is the natural extension of Q-learning to the multi-step case with stochastic target policies. It involves no importance sampling but, again if the target and behavior policies are substantially di\u21b5erent, the bootstrapping may span only a few steps even if n is large. The notion of n-step returns is due to Watkins , who also \ufb01rst discussed their error reduction property. n-step algorithms were explored in the \ufb01rst edition of this book, in which they were treated as of conceptual interest, but not feasible in practice. The work of Cichosz  and particularly van Seijen  showed that they are actually completely practical algorithms.\n\nGiven this, and their conceptual clarity and simplicity, we have chosen to highlight them here in the second edition. In particular, we now postpone all discussion of the backward view and of eligibility traces until Chapter 12. 7.1\u20132 The results in the random walk examples were made for this text based on work of Sutton  and Singh and Sutton . The use of backup diagrams to describe these and other algorithms in this chapter is new", "5a381a79-cd59-43db-906f-9f9733203fc4": "This has the same column space as a 2 X 1 matrix containing only one copy. of the replicated column. In other words, the column space is still Just a line and fails to  encompass all of R?, even though there are two columns. Formally, this kind of redundancy is known as linear dependence. A set of vectors is linearly independent if no vector in the set is a linear combination of the other vectors. If we add a vector to a set that is a linear combination of the other vectors in the set, the new vector does not add any points to the set\u2019s span. This means that for the column space of the matrix to encompass all of R\u201d, the matrix must contain at least one set of m linearly independent columns. This condition is both necessary and sufficient for equation 2.11 to have a solution for every value of b. Note that the requirement is for a set to have exactly m linearly independent columns, not at least m", "085b447d-8a5f-42f9-8559-3267a7d33291": "Dougal J Sutherland, Hsiao-Yu Tung, Heiko Strathmann, Soumyajit De, Aaditya Ramdas, Alex Smola, and Arthur Gretton. Generative models and model criticism via optimized maximum mean discrepancy. In International Conference on Learning Representations, 2017. Under review. T. Tieleman and G. Hinton. Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012. C\u00b4edric Villani. Optimal Transport: Old and New. Grundlehren der mathematischen Wissenschaften. Springer, Berlin, 2009. Fisher Yu, Yinda Zhang, Shuran Song, Ari Se\ufb00, and Jianxiong Xiao. LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop. Corr, abs/1506.03365, 2015. We now introduce our notation", "b440ff5d-a199-4dec-b589-5d04a3699b31": "For example, if we use a two-dimensional image J as our input, we probably also want to use a two-dimensional kernel K:  S(i,j) = (I* K)(i, j) = I(m,n)K(i\u2014m,j \u2014n). (9.4)  mn  Convolution is commutative, meaiyngywe can equivalently write  S(i,j) = (K \u00abI(t = SST 1 i\u2014m,j \u2014n)K(m,n). (9.5)  https://www.deeplearningbook.org/contents/convnets.html    Usually the latter formula is more straightforward to implement in a machine learning library, because there is less variation in the range of valid values of \u2122 and n.  The commutative property of convolution arises because we have flipped the kernel relative to the input, in the sense that as m increases, the index into the input increases, but the index into the kernel decreases. The only reason to flip the kernel is to obtain the commutative property. While the commutative property  328  CHAPTER 9", "762a0926-6569-40d0-bce1-f5782e2e38e4": "In this essay, we use techniques such as the photochemical transfer military: arms defense battalion battalion cavalry This essay discusses three main themes:\\n\\n 1) Lack of uniformed soldiers is an unacceptable and unconscionable strategy for the Army.\\n 2) Poor and inadequate training does not compensate the soldiers, and may deprive them of the necessary and competitive training from their instructors legal: there liable injunction In summary, the court decided that defendants had a right to petition the high court to intervene, and therefore granted the injunction.\n\nHowever, a Court of Appeal decision in the US District Court ruled with no effect on the petitioned by the plaintiffs, citing two reasons.\\n\\n The US politics: the primary referendum was In summary, the majority of Russians would support the idea of banning all human rights, and that would be a major part of the government\u2019s effort to build a new liberal economy, which President Vladimir Putin says would boost Russian tourism.\\n\\n The results were published in the computers: the macintoshintosh In summary, the kernel has the best quality of life, and kernel security is at the heart of any OS development. The \ufb01rst time Linux released was when i386 released the Macintosh version, which had a rather low standard", "b875c196-7059-4c4c-8662-893c3571ec19": "The greedy policy takes the action that looks best in the short term\u2014after one step of lookahead\u2014according to v\u21e1. By construction, the greedy policy meets the conditions of the policy improvement theorem (4.7), so we know that it is as good as, or better than, the original policy. The process of making a new policy that improves on an original policy, by making it greedy with respect to the value function of the original policy, is called policy improvement. Suppose the new greedy policy, \u21e10, is as good as, but not better than, the old policy \u21e1. Then v\u21e1 = v\u21e10, and from (4.9) it follows that for all s 2 S: But this is the same as the Bellman optimality equation (4.1), and therefore, v\u21e10 must be v\u21e4, and both \u21e1 and \u21e10 must be optimal policies. Policy improvement thus must give us a strictly better policy except when the original policy is already optimal. So far in this section we have considered the special case of deterministic policies.\n\nIn the general case, a stochastic policy \u21e1 speci\ufb01es probabilities, \u21e1(a|s), for taking each action, a, in each state, s", "6858d19c-12b8-404b-8398-371c709b7a71": "The divergence function can have a variety of choices, ranging from the family of f-divergence (e.g., KL divergence), or Bregman divergence, to optimal transport distance (e.g., Wasserstein distance), and so on. We discuss the divergence term in Section 5 in more detail. Uncertainty function. The uncertainty function H(q) describes the uncertainty of the auxiliary distribution q and thus controls the complexity of the learning system. It conforms with the maximum entropy principle discussed in Section 2 that one should pick the most uncertain solution among those that \ufb01t all experience. Like other components in SE, the uncertainty measure H(\u00b7) can take di\ufb00erent forms, such as the popular Shannon entropy, as well as other generalized ones such as Tsallis entropy. In this article, we assume Shannon entropy by default. For the discussion in the following sections, it is often convenient to consider a special case of the SE in Equation 3.1. Speci\ufb01cally, we assume a common choice of the penalty U(\u03be) = \ufffd which can be easily seen by optimizing Equation 3.1 over \u03be", "c27747ae-6ccf-433a-9def-b2d3c449e2a5": "Show that the solution for the components an of the vector a can be expressed as a linear combination of the elements of the vector \u03c6(xn).\n\nDenoting these coef\ufb01cients by the vector w, show that the dual of the dual formulation is given by the original representation in terms of the parameter vector w. 6.2 (\u22c6 \u22c6) In this exercise, we develop a dual formulation of the perceptron learning algorithm. Using the perceptron learning rule (4.55), show that the learned weight vector w can be written as a linear combination of the vectors tn\u03c6(xn) where tn \u2208 {\u22121, +1}. Denote the coef\ufb01cients of this linear combination by \u03b1n and derive a formulation of the perceptron learning algorithm, and the predictive function for the perceptron, in terms of the \u03b1n. Show that the feature vector \u03c6(x) enters only in the form of the kernel function k(x, x\u2032) = \u03c6(x)T\u03c6(x\u2032)", "6da64f31-7f24-4103-a5da-afc6c6ce4438": "For sequence prediction tasks, rather than modeling the future observations pr(Xtrk|Ce) directly (which could be fairly expensive), CPC models a density function to preserve the mutual information between x\u00a2;% and cz:  D(X 4|C%)  + = WwW Fe (Xt+ks Ce) exp(Zi+4 Kez) OC D(Xt.6)  https://lilianweng.github.io/posts/2021-05-31-contrastive/  Contrastive Representation Learning | Lil'Log   where Z;,% is the encoded input and W, is a trainable weight matrix.\n\nSoft-Nearest Neighbors Loss  Soft-Nearest Neighbors Loss  extends it to include multiple positive samples", "2a37aeab-9c80-47fc-8540-adf17d03f8f8": "If the model were Gaussian, then these interactions could be modeled efficiently via the covariance matrix, but the sparse prior makes these interactions non-Gaussian. Because p( ) is intractable, so is the computation of the log-likelihood and  hla  https://www.deeplearningbook.org/contents/inference.html    its gradient. We thus cannot use exact maximum likelihood learning.\n\nInstead, we use MAP inference and learn the parameters by maximizing the ELBO defined by the Dirac distribution around the MAP estimate of h.  If we concatenate all the h vectors in the training set into a matrix H, and concatenate all the v vectors into a matrix V, then the sparse coding learning process consists of minimizing  = i|+o(v-aw') (19.16) a . tJ i,j a  Most applications of sparse coding also involve weight decay or a constraint on the norms of the columns of W, to prevent the pathological solution with extremely small H and large W.  We can minimize J by alternating between minimization with respect to H and minimization with respect to W. Both subproblems are convex", "847fbab5-7550-488f-b7af-9e3f4482f7e9": "this will in gencr~1 not be flO'slble, To see thl', OOIe Ihat the mapping 4'(x) maps the D-dimensional x space i\"t\" 0 D-dimensioo.l manijQiII in lhe M-dimemioo.l femure space <1>. TlIe: . 'ector x i' koown a< lhe f'\",.imagr of lhe c\",\"\"\"ponding poi\"l 4'(x). However, fhe projec1ioo of poinl> in feature <J'3C\" \"\"to the linear rcA ,ub,p\"\"\" in that 'pace will typically\"''' lie On fhe nonlinear Ddimensional manifold and !iO will nul ha.,. a c\"\"\"\",pondlng p\",.lmo~ein dOlO spa<", "27165a61-6159-4359-af43-2686b2c9c643": "This indicates that the algorithms have converged long before the end of the run for all \u21b5 values, since we do not see any effect of the initial learning phase. For Sarsa the performance comes close to the performance of Expected Sarsa only for \u21b5 = 0.1, while for large \u21b5, the performance for n = 100, 000 even drops below the performance for n = 100. The reason is that for large values of \u21b5 the Q values of Sarsa diverge. Although the policy is still improved over the initial random policy during the early stages of learning, divergence causes the policy to get worse in the long run. for n = 100 and n = 100, 000 using an \u03f5-greedy policy with \u03f5 = 0.1. The big dots indicate the maximal values. We turn to the windy grid world task to further test Hypothesis 2. The windy grid world task is another navigation task, where the agent has to \ufb01nd its way from start to goal. over Sarsa over a wide range of values for the step-size parameter \u21b5. In cli\u21b5 walking the state transitions are all deterministic and all randomness comes from the policy", "4dd49dd8-7039-4890-b852-7209bfdb8cb0": "Extend this result to the case of multiple outputs. where y(x, w) is a parametric function such as a neural network. The result (1.89) shows that the function y(x, w) that minimizes this error is given by the conditional expectation of t given x. Use this result to show that the second derivative of E with respect to two elements wr and ws of the vector w, is given by Note that, for a \ufb01nite sample from p(x), we obtain (5.84). 5.18 (\u22c6) Consider a two-layer network of the form shown in Figure 5.1 with the addition of extra parameters corresponding to skip-layer connections that go directly from the inputs to the outputs. By extending the discussion of Section 5.3.2, write down the equations for the derivatives of the error function with respect to these additional parameters", "21ba0d7a-6bbb-4738-805e-e941ca1849bc": "To make a more radical departure from the feedforward networks we have seen previously, we can also generalize the notion of an encoding function f(a) to an encoding distribution peycoder(h | x), as illustrated in figure 14.2. Any latent variable model pyodel(h, \u00ab) defines a stochastic encoder  Pencoder (Rh | x) = Pmodel (A | x) (14.12)  and a stochastic decoder  Paecoder(\u00ae | h) = Pmodel (\u00a9 | h).\n\n(14.13)  In general, the encoder and decoder distributions are not necessarily conditional distributions compatible with a unique joint distribution pmodel(@,h). Alain ef al. showed that training the encoder and decoder as a denoising autoencoder will tend to make them compatible asymptotically (with enough capacity and examples). 14.5 Denoising Autoencoders  The denoising autoencoder (DAE) is an autoencoder that receives a corrupted data point as input and is trained to predict the original, uncorrupted data point as its output", "6b0f930f-1426-4653-bfa3-f83272ffd900": "By using the standard results for the mean and variance of the gamma distribution given by (B.27) and (B.28), show that if we let N \u2192 \u221e, this variational posterior distribution has a mean given by the inverse of the maximum likelihood estimator for the variance of the data, and a variance that goes to zero.\n\n10.9 (\u22c6 \u22c6) By making use of the standard result E = aN/bN for the mean of a gamma distribution, together with (10.26), (10.27), (10.29), and (10.30), derive the result (10.33) for the reciprocal of the expected precision in the factorized variational treatment of a univariate Gaussian. 10.10 (\u22c6) www Derive the decomposition given by (10.34) that is used to \ufb01nd approximate posterior distributions over models using variational inference. 10.12 (\u22c6 \u22c6) Starting from the joint distribution (10.41), and applying the general result (10.9), show that the optimal variational distribution q\u22c6(Z) over the latent variables for the Bayesian mixture of Gaussians is given by (10.48) by verifying the steps given in the text", "87e64c79-dabb-4042-8d2b-83528614537a": "12.2.1.1 Contrast Normalization  One of the most obvious sources of variation that can be safely removed for many tasks is the amount of contrast in the image. Contrast simply refers to the magnitude of the difference between the bright and the dark pixels in an image. There are many ways of quantifying the contrast of an image. In the context of deep learning, contrast usually refers to the standard deviation of the pixels in an image or region of an image. Suppose we have an image represented by a tensor XE R\u2122 3, with Xj,j,1 being the red intensity at row i, and column j, X;,;,2 giving the green intensity, and X;,;,3 giving the blue intensity.\n\nThen the contrast of the entire image is given by  roc 3  LLd (Xi,je \u2014 X)\u201d, (12.1)  i=1 j=1 k=1  where X is the mean intensity of the entire image: < 1 x= EYE xin 022)  Global contrast normalization (GCN) aims to prevent images from having varying amounts of contrast by subtracting the mean from each image, then rescaling it so that the standard deviation across its pixels is equal to some constant s", "f55e81e1-87ae-43ef-8869-7acac272e1ec": "These evaluation methods complemented one another: the value network evaluated the high-performance RL policy that was too slow to be used in live play, while rollouts using the weaker but much faster rollout policy were able to add precision to the value network\u2019s evaluations for speci\ufb01c states that occurred during games.\n\nOverall, AlphaGo\u2019s remarkable success fueled a new round of enthusiasm for the promise of arti\ufb01cial intelligence, speci\ufb01cally for systems combining reinforcement learning with deep ANNs, to address problems in other challenging domains. Building upon the experience with AlphaGo, a DeepMind team developed AlphaGo Zero . In contrast to AlphaGo, this program used no human data or guidance beyond the basic rules of the game (hence the Zero in its name). It learned exclusively from self-play reinforcement learning, with input giving just \u201craw\u201d descriptions of the placements of stones on the Go board. AlphaGo Zero implemented a form of policy iteration (Section 4.3), interleaving policy evaluation with policy improvement. Figure 16.7 is an overview of AlphaGo Zero\u2019s algorithm", "92cfc189-97ef-4de4-8eb2-6b8ee6cc2046": "Each feature map in a subsampling layer consists of units that average over a receptive \ufb01eld of units in the feature maps of the preceding convolutional layer. For example, each unit in each of the 6 feature maps in the \ufb01rst subsampling layer of the network of Figure 9.15 averages over a 2 \u21e5 2 non-overlapping receptive \ufb01eld over one of the feature maps produced by the \ufb01rst convolutional layer, resulting in six 14 \u21e5 14 feature maps. Subsampling layers reduce the network\u2019s sensitivity to the spatial locations of the features detected, that is, they help make the network\u2019s responses spatially invariant. This is useful because a feature detected at one place in an image is likely to be useful at other places as well. Advances in the design and training of ANNs\u2014of which we have only mentioned a few\u2014all contribute to reinforcement learning.\n\nAlthough current reinforcement learning theory is mostly limited to methods using tabular or linear function approximation methods, the impressive performances of notable reinforcement learning applications owe much of their success to nonlinear function approximation by multi-layer ANNs. We discuss several of these applications in Chapter 16", "e92fd635-071a-47c0-8471-87d7ddd769e6": "And gradually all the other non-successful impulses will be stamped out and the particular impulse leading to the successful act will be stamped in by the resulting pleasure, until, after many trials, the cat will, when put in the box, immediately claw the button or loop in a de\ufb01nite way. (Thorndike 1898, p. 13) These and other experiments (some with dogs, chicks, monkeys, and even \ufb01sh) led Thorndike to formulate a number of \u201claws\u201d of learning, the most in\ufb02uential being the Law of E\u21b5ect, a version of which we quoted in Chapter 1 (page 15). This law describes what is generally known as learning by trial and error. As mentioned in Chapter 1, many aspects of the Law of E\u21b5ect have generated controversy, and its details have been modi\ufb01ed over the years. Still the law\u2014in one form or another\u2014expresses an enduring principle of learning. Essential features of reinforcement learning algorithms correspond to features of animal learning described by the Law of E\u21b5ect.\n\nFirst, reinforcement learning algorithms are selectional, meaning that they try alternatives and select among them by comparing their consequences", "35444eef-6e53-4648-b83b-9b622c2c4340": "The oil \ufb02ow data set is generated using realistic known values for the absorption properties of oil, water, and gas at the two gamma energies used, and with a speci\ufb01c choice of integration time (10 seconds) chosen as characteristic of a typical practical setup. Each point in the data set is generated independently using the following steps: 1. Choose one of the three phase con\ufb01gurations at random with equal probability. 2.\n\nChoose three random numbers f1, f2 and f3 from the uniform distribution over (0, 1) and de\ufb01ne This treats the three phases on an equal footing and ensures that the volume fractions add to one. 3. For each of the six beam lines, calculate the effective path lengths through oil and water for the given phase con\ufb01guration. 4. Perturb the path lengths using the Poisson distribution based on the known beam intensities and integration time to allow for the effect of photon statistics. Each point in the data set comprises the 12 path length measurements, together with the fractions of oil and water and a binary label describing the phase con\ufb01guration. The data set is divided into training, validation, and test sets, each of which comprises 1, 000 independent data points. Details of the data format are available from the book web site", "98085820-3664-4644-98b4-204e9e30a57e": "The actor eligibility trace vector z\u2713 t is a trace (average of recent values) of r ln \u21e1(At|St, \u2713). To understand this eligibility trace refer to Exercise 13.5, which de\ufb01nes this kind of unit and asks you to give a learning rule for it.\n\nThat exercise asked you to express r ln \u21e1(a|s, \u2713) in terms of a, x(s), and \u21e1(a|s, \u2713) (for arbitrary state s and action a) by calculating the gradient. For the action and state actually occurring at time t, the answer is Unlike the non-contingent eligibility trace of a critic synapse that only accumulates the presynaptic activity x(St), the eligibility trace of an actor unit\u2019s synapse in addition depends on the activity of the actor unit itself. We call this a contingent eligibility trace because it is contingent on this postsynaptic activity. The eligibility trace at each synapse continually decays, but increments or decrements depending on the activity of the presynaptic neuron and whether or not the postsynaptic neuron \ufb01res", "508c6dcb-c5e3-4da2-a6c8-a5e9b0c41dbc": "The z-axis is the coordinate of the initial state along a random direction in the 100-dimensional space. We can thus view this plot as a linear cross-section of a high-dimensional function. The plots show the function after each time step, or equivalently, after each number of times the transition function has been composed. 397  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  and lacking inputs a. As described in section 8.2.5, this recurrence relation essentially describes the power method. It may be simplified to  nd = (Ww)! pn, (10.37 and if W admits an eigendecomposition of the form W =QAQ\u2019, (10.38  with orthogonal Q, the recurrence may be simplified further to  hn =Q\u2019aAtQn\u00ae. (10.39  The eigenvalues are raised to the power of t, causing eigenvalues with magnitude less than one to decay to zero and eigenvalues with magnitude greater than one to explode", "cbb09228-9e12-402f-83eb-18b2f9282e74": "Actively supervised data instances.\n\nInstead of access to data instances x\u2217 with readily available labels y\u2217, in the active supervision setting, we are presented with a large pool of unlabeled instances D = {x\u2217} as well as a certain budget for querying an oracle (e.g., human annotators) for labeling a limited set of instances. To minimize the need for labeled instances, we need to strategically select queries from the pool according to an informativeness measure u(x) \u2208 R. For example, u(x) can be the predictive uncertainty on the instance x, quanti\ufb01ed by the Shannon entropy of the predictive distribution or the vote entropy based on a committee of predictors . Mapping the standard equation to this setting, we show the informativeness measure u(x) is subsumed as part of the experience. Intuitively, u(x) encodes our heuristic belief about sample \u2018informativeness\u2019. This heuristic is a form of information we inject into the learning system. Denote the oracle as o from which we can draw a label y\u2217 \u223c o(x\u2217)", "3d9f6035-e1db-47e5-9c36-e683f18f0dde": "The tasks can be any well-defined family of machine learning problems: supervised learning, reinforcement learning, etc. For example, here are a couple concrete meta-learning tasks:  A classifier trained on non-cat images can tell whether a given image contains a cat after seeing a handful of cat pictures.\n\nA game bot is able to quickly master a new game. A mini robot completes the desired task on an uphill surface during test even through it was only trained in a flat surface environment. Define the Meta-Learning Problem  https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Log   In this post, we focus on the case when each desired task is a supervised learning problem like image classification. There is a lot of interesting literature on meta-learning with reinforcement learning problems (aka \u201cMeta Reinforcement Learning\u201d), but we would not cover them here. A Simple View  A good meta-learning model should be trained over a variety of learning tasks and optimized for the best performance on a distribution of tasks, including potentially unseen tasks. Each task is  associated with a dataset D, containing both feature vectors and true labels", "1ff9cf74-fd75-4e4b-a145-1050d1011810": "This representation allows the TD error to mimic the fact that dopamine neuron activity not only predicts a future reward, but that it is also sensitive to when after a predictive cue that reward is expected to arrive. There has to be some way to keep track of the time between sensory cues and the arrival of reward. If a stimulus initiates a sequence of internal signals that continues after the stimulus ends, and if there is a di\u21b5erent signal for each time step following the stimulus, then each time step after the stimulus is represented by a distinct state. Thus, the TD error, being state-dependent, can be sensitive to the timing of events within a trial", "1cf48f04-635c-49b8-bff4-afd73f037172": "By a shallow transformation, we mean a transformation that would be represented by a single layer within a deep MLP. Typically this is a transformation represented by a learned affine  c i eu WW \u00b0 1 .o  https://www.deeplearningbook.org/contents/rnn.html    ULrallslormlatlou LOLLIOWEd DY a UxXed LOMINeAarity. Would it be advantageous to introduce depth in each of these operations? Experimental evidence  strongly suggests so. The experimental evidence is in agreement with the idea that we need enough  393  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  depth to perform the required mappings.\n\nSee also Schmidhuber , El Hihi and Bengio , or Jaeger  for earlier work on deep RNNs. Graves et al. were the first to show a significant benefit of decomposing the state of an RNN into multiple layers, as in figure 10.13 (left)", "3c863b1b-4c94-48b5-a589-944c845a9340": "The naive algorithm could have exponential runtime due to these repeated subexpressions.\n\nNow that we have specified the back-propagation algorithm, we can understand its computational cost. If we assume that each operation evaluation has roughly the  212  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  https://www.deeplearningbook.org/contents/mlp.html    Algorithm 6.5 The outermost skeleton of the back-propagation algorithm. This portion does simple setup and cleanup work. Most of the important work happens  in the build_grad subroutine of algorithm 6.6  Require: T, the target set of variables whose gradients must be computed. Require: G, the computational graph Require: z, the variable to be differentiated Let G\u2019 be G pruned to contain only nodes that are ancestors of z and descendents of nodes in T", "8a592813-f0c2-46bd-8177-3ba3b1643a93": "We see that the weight parameters in the \ufb01rst layer of the network are shared between the various outputs, whereas in the linear model each classi\ufb01cation problem is solved independently. The \ufb01rst layer of the network can be viewed as performing a nonlinear feature extraction, and the sharing of features between the different outputs can save on computation and can also lead to improved generalization. Finally, we consider the standard multiclass classi\ufb01cation problem in which each input is assigned to one of K mutually exclusive classes.\n\nThe binary target variables tk \u2208 {0, 1} have a 1-of-K coding scheme indicating the class, and the network outputs are interpreted as yk(x, w) = p(tk = 1|x), leading to the following error function Following the discussion of Section 4.3.4, we see that the output unit activation function, which corresponds to the canonical link, is given by the softmax function k yk = 1. Note that the yk(x, w) are unchanged if a constant is added to all of the ak(x, w), causing the error function to be constant for some directions in weight space. This degeneracy is removed if an appropriate regularization term (Section 5.5) is added to the error function", "2e916cfe-a02f-4905-b66d-f7f3ac964787": "However, we can \ufb01rst seek invertible transformations either of the function or of its argument which change it into a convex form. We then calculate the conjugate function and then transform back to the original variables. An important example, which arises frequently in pattern recognition, is the logistic sigmoid function de\ufb01ned by As it stands this function is neither convex nor concave. However, if we take the logarithm we obtain a function which is concave, as is easily veri\ufb01ed by \ufb01nding the second derivative. From (10.133) the corresponding conjugate function then takes Exercise 10.30 which we recognize as the binary entropy function for a variable whose probability of having the value 1 is \u03bb.\n\nUsing (10.132), we then obtain an upper bound on the log Appendix B with two examples of the exponential upper bound (10.137) shown in blue. The right-hand plot shows the logistic sigmoid again in red together with the Gaussian lower bound (10.144) shown in blue. Here the parameter \u03be = 2.5, and the bound is exact at x = \u03be and x = \u2212\u03be, denoted by the dashed green lines", "fdc73231-0a5b-4fcf-8cdf-4e12648e447c": "A Gaussian mixture output with n components is defined by the conditional probability distribution:  ply |x) = Yo rle= il Ny; nw? (x), B\u00ae (a). (6.35)  The neural network must have three outputs: a vector defining p(c =i| x), a matrix providing (a) for all i, and a tensor providing \u00a9 (a) for all i. These outputs must satisfy different constraints:  185  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  1. Mixture components p(c = i | x): these form a multinoulli distribution over the n different components associated with latent variable! c, and can typically be obtained by a softmax over an n-dimensional vector, to guarantee that these outputs are positive and sum to 1. 2. Means pa): these indicate the center or mean associated with the i-th  https://www.deeplearningbook.org/contents/mlp.html    Gaussian component and are unconstrained (typically with no nonlinearity at all for these output units)", "ab3c2c39-d8e3-42dd-9753-a5213a24a2f4": "(In the undiscounted episodic case, it is possible that there are some orderings of updates that do not result in convergence, but it is relatively easy to avoid these.) Similarly, it is possible to intermix policy evaluation and value iteration updates to produce a kind of asynchronous truncated policy iteration. Although the details of this and other more unusual DP algorithms are beyond the scope of this book, it is clear that a few di\u21b5erent updates form building blocks that can be used \ufb02exibly in a wide variety of sweepless DP algorithms. Of course, avoiding sweeps does not necessarily mean that we can get away with less computation. It just means that an algorithm does not need to get locked into any hopelessly long sweep before it can make progress improving a policy.\n\nWe can try to take advantage of this \ufb02exibility by selecting the states to which we apply updates so as to improve the algorithm\u2019s rate of progress. We can try to order the updates to let value information propagate from state to state in an e\ufb03cient way. Some states may not need their values updated as often as others. We might even try to skip updating some states entirely if they are not relevant to optimal behavior. Some ideas for doing this are discussed in Chapter 8", "63efb4ac-41ee-4e21-b0e5-f06a6f53d11d": "The computational cost of this algorithm is proportional to the number of edges in  https://www.deeplearningbook.org/contents/mlp.html    the graph, een that the partial derivative associated with each edge requires a constant time. This is of t he ssagne order as the number of corp atten for the forward propagation. Each 9,@ is a function of the parents u~\u2019 of u\u2019\u2019, thus linking the nodes of the forward graph to those added for the back-propagation  graph.\n\nRun forward propagation (algorithm 6.1 for this example) to obtain the activa- tions of the network. Initialize grad_table, a data structure that will store the derivatives that have  been computed. The entry grad_table \u2014 1 for j = n\u20141 down to 1 do  The next line computes 4 Su = yijePa(ul) bury Burs using stored values:  iy) du  grad_table \u2014 Tejepaquoy erad_table 200 end for return {grad_table | i =1,..., ni}  207  CHAPTER 6", "b62a1980-565f-4696-9932-639fc7437af9": "A neuron\u2019s background activity is its level of activity, usually its \ufb01ring rate, when the neuron does not appear to be driven by synaptic input related to the task of interest to the experimenter, for example, when the neuron\u2019s activity is not correlated with a stimulus delivered to a subject as part of an experiment. Background activity can be irregular due to input from the wider network, or due to noise within the neuron or its synapses. Sometimes background activity is the result of dynamic processes intrinsic to the neuron. A neuron\u2019s phasic activity, in contrast to its background activity, consists of bursts of spiking activity usually caused by synaptic input. Activity that varies slowly and often in a graded manner, whether as background activity or not, is called a neuron\u2019s tonic activity", "224ee220-7ff7-483a-af4a-060555896040": "The RNN in this figure is trained to put a specific output value into o, and o is the only information it is allowed to send to the future. There are no direct connections from h going forward. The previous h is connected to the present only indirectly, via the predictions it was used to produce. Unless o is very high-dimensional and rich, it will usually lack important information from the past. This makes the RNN in this figure less powerful, but it may be easier to train because each time step can be trained in isolation from the others, allowing greater parallelization during training, as described in section 10.2.1. 375  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  MN L IN.\n\nDNL O\u2122..  https://www.deeplearningbook.org/contents/rnn.html    1(T)  Figure 10.5: Time-unfolded recurrent neural network with a single output at the end of the sequence. Such a network can be used to summarize a sequence and produce a fixed-size representation used as input for further processing", "e5513a02-13e7-4ceb-8100-5181a4e1f286": "Proper resolution of this issue awaits a more thorough understanding of the theory of function approximation in reinforcement learning.\n\nIn the multi-step generalizations of these algorithms, both the state-value and actionwt+n .= wt+n\u22121 + \u21b5\u21e2t+1 \u00b7 \u00b7 \u00b7 \u21e2t+n\u22121  r\u02c6q(St, At, wt+n\u22121) Gt:t+n .= Rt+1 \u2212 \u00afRt + \u00b7 \u00b7 \u00b7 + Rt+n \u2212 \u00afRt+n\u22121 + \u02c6q(St+n, At+n, wt+n\u22121), (continuing) where here we are being slightly informal in our treatment of the ends of episodes. In the \ufb01rst equation, the \u21e2ks for k \u2265 T (where T is the last time step of the episode) should be taken to be 1, and Gt:n should be taken to be Gt if t + n \u2265 T. Recall that we also presented in Chapter 7 an o\u21b5-policy algorithm that does not involve with \u03b4t as de\ufb01ned at the top of this page for Expected Sarsa", "72202496-bc68-41eb-a0a6-54de87c8bd29": "LSTD is the most data-e\ufb03cient linear TD prediction method, but requires computation proportional to the square of the number of weights, whereas all the other methods are of complexity linear in the number of weights. Nonlinear methods include arti\ufb01cial neural networks trained by backpropagation and variations of SGD; these methods have become very popular in recent years under the name deep reinforcement learning. Linear semi-gradient n-step TD is guaranteed to converge under standard conditions, for all n, to a VE that is within a bound of the optimal error (achieved asymptotically by Monte Carlo methods). This bound is always tighter for higher n and approaches zero as n ! 1. However, in practice very high n results in very slow learning, and some degree of bootstrapping (n < 1) is usually preferrable, just as we saw in comparisons of tabular n-step methods in Chapter 7 and in comparisons of tabular TD and Monte Carlo methods in Chapter 6.\n\nGeneralization and function approximation have always been an integral part of reinforcement learning. Bertsekas and Tsitsiklis , Bertsekas , and Sugiyama et al", "8fac79c2-45f8-43e6-afa0-36b968e995d5": "(In reality, Alice\u2019s performance probably influences Bob\u2019s performance\u2014depending on Bob\u2019s personality, if Alice runs especially fast in a given race, this might encourage Bob to push hard and match her exceptional performance, or it might make him overconfident and lazy). Then the only effect Alice has on Bob\u2019s finishing time is that we must add Alice\u2019s finishing time to the total amount of time we think Bob needs to run. This observation allows us to define a model with O(k) parameters instead of O(k?). However, note that tg and t; are still directly dependent with this assumption, because t; represents the absolute time at which Bob finishes, not the total time he spends running. This means our graph must still contain an arrow from to to ti. The assumption that Bob\u2019s personal running time is independent from all other factors cannot be encoded in a graph over to, t1, and tz. Instead, we encode this information in the definition of the conditional distribution itself", "b801ae54-b07f-4af4-a3f1-2dfe0faaadf1": "Within each row, the arithmetic is element-wise, so H;; is normalized by subtracting 1; and dividing by o;. The rest of the network then operates on H\u2019 in exactly the same way that the original network operated on H.  At training time,  w= SH, (8.36)  and  o= 5+ Hn );, (8.37)  where 6 is a small positive value such as 10~8, imposed to avoid encountering the undefined gradient of \\/z at z = 0. Crucially, we back-propagate through these operations for computing the mean and the standard deviation, and for applying them to normalize H. This means that the gradient will never propose an operation that acts simply to increase the standard deviation or mean of  he a1 a at at ad roo c 1 at 1  https://www.deeplearningbook.org/contents/optimization.html    ree LUE HOLMAN Zavlou OPeLrabllOlUs LEMLOVE LIE CLLECL OL SUCLIL all ACLIOLI ALLU ZELO out its component in the gradient.\n\nThis was a major innovation of the batch normalization approach", "376b97aa-9354-4122-87a4-584ff3d6fe4a": "We say that the earlier states are given less credit for the TD error. If \u03bb = 1, then the credit given to earlier states falls only by \u03b3 per step. This turns out to be just the right thing to do to achieve Monte Carlo behavior. For example, remember that the TD error, \u03b4t, includes an undiscounted term of Rt+1. In passing this back k steps it needs to be discounted, like any reward in a return, by \u03b3k, which is just what the falling eligibility trace achieves. If \u03bb = 1 and \u03b3 = 1, then the eligibility traces do not decay at all with time. In this case the method behaves like a Monte Carlo method for an undiscounted, episodic task. If \u03bb = 1, the algorithm is also known as TD(1).\n\nTD(1) is a way of implementing Monte Carlo algorithms that is more general than those presented earlier and that signi\ufb01cantly increases their range of applicability. Whereas the earlier Monte Carlo methods were limited to episodic tasks, TD(1) can be applied to discounted continuing tasks as well. Moreover, TD(1) can be performed incrementally and online", "8f41f0bb-72f2-4473-aff8-5a15587c0639": "Once an action is taken, the environment delivers a reward (r \u20ac 7) as feedback. The model defines the reward function and transition probabilities. We may or may not know how the model works and this differentiate two circumstances:  Know the model: planning with perfect information; do model-based RL. When we fully know the environment, we can find the optimal solution by Dynamic Programming (DP). Do you still remember \u201clongest increasing subsequence\u201d or \u201ctraveling salesmen problem\" from your Algorithms 101 class? LOL. This is not the focus of this post though.\n\nDoes not know the model: learning with incomplete information; do model-free RL or try to learn the model explicitly as part of the algorithm. Most of the following content serves the scenarios when the model is unknown. The agent's policy 7(s) provides the guideline on what is the optimal action to take in a certain state with the goal to maximize the total rewards", "e42c755e-e82a-4214-bb57-ece91f24bedd": "It is convenient to consider low-level actions to be special cases of options\u2014each action a corresponds to an option h\u21e1!, \u03b3!i whose policy picks the action (\u21e1! (s)=a for all s 2 S) and whose termination function is zero (\u03b3! (s) = 0 for all s 2 S+). Options e\u21b5ectively extend the action space. The agent can either select a low-level action/option, terminating after one time step, or select an extended option that might execute for many time steps before terminating. Options are designed so that they are interchangable with low-level actions. For example, the notion of an action-value function q\u21e1 naturally generalizes to an optionvalue function that takes a state and option as input and returns the expected return starting from that state, executing that option to termination, and thereafter following the policy, \u21e1.\n\nWe can also generalize the notion of policy to a hierarchical policy that selects from options rather than actions, where options, when selected, execute until termination. With these ideas, many of the algorithms in this book can be generalized to learn approximate option-value functions and hierarchical policies", "da337795-ff23-48a9-8720-baa6f6534249": "SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  https://www.deeplearningbook.org/contents/rnn.html    Figure 10.11: Computation of a typical bidirectional recurrent neural network, meant to learn to map input sequences \u00ab to target sequences y, with loss L\u00ae at each step t The h recurrence propagates information forward in time (toward the right), while theg recurrence propagates information backward in time (toward the left). Thus at each point t, the output units o\u201d) can benefit from a relevant summary of the past in its ne) input and from a relevant summary of the future in its g\u201d) input. 389  CHAPTER 10", "c84b0714-251b-4be0-802f-e0b2652da8cf": "In this sense, DP is exponentially faster than any direct search in policy space could be, because direct search would have to exhaustively examine each policy to provide the same guarantee. Linear programming methods can also be used to solve MDPs, and in some cases their worst-case convergence guarantees are better than those of DP methods. But linear programming methods become impractical at a much smaller number of states than do DP methods (by a factor of about 100).\n\nFor the largest problems, only DP methods are feasible. DP is sometimes thought to be of limited applicability because of the curse of dimensionality, the fact that the number of states often grows exponentially with the number of state variables. Large state sets do create di\ufb03culties, but these are inherent di\ufb03culties of the problem, not of DP as a solution method. In fact, DP is comparatively better suited to handling large state spaces than competing methods such as direct search and linear programming. In practice, DP methods can be used with today\u2019s computers to solve MDPs with millions of states. Both policy iteration and value iteration are widely used, and it is not clear which, if either, is better in general", "ce30dec8-12eb-42a0-ad68-78224e95a839": "That\u2019s why we\u2019re committed to working collaboratively with the broader Al community to achieve our goal of, one day, building machines with human-level  https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/  Self-supervised learning: The dark matter of intelligence   intelligence. Our research has been made publicly available and published at top conferences. And we\u2019ve organized workshops and released libraries to help accelerate the research in this area.\n\nWritten By  Yann LeCun Ishan Misra VP and Chief Al Scientist Research Scientist  https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/  Self-supervised learning: The dark matter of intelligence  Who We Are  About Meta Al People Careers Events  Latest Work  Research Infrastructure Blog Resources  Our Actions  Responsibilities  Newsletter  Sign Up  Privacy Policy Terms Cookies  https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/    as @O000  Meta \u00a9 2023", "4a1830c4-598e-42de-8540-2870adf3875a": "This may seem paradoxical because a polynomial of given order contains all lower order polynomials as special cases.\n\nThe M = 9 polynomial is therefore capable of generating results at least as good as the M = 3 polynomial. Furthermore, we might suppose that the best predictor of new data would be the function sin(2\u03c0x) from which the data was generated (and we shall see later that this is indeed the case). We know that a power series expansion of the function sin(2\u03c0x) contains terms of all orders, so we might expect that results should improve monotonically as we increase M. We can gain some insight into the problem by examining the values of the coef\ufb01cients w\u22c6 obtained from polynomials of various order, as shown in Table 1.1. We see that, as M increases, the magnitude of the coef\ufb01cients typically gets larger. In particular for the M = 9 polynomial, the coef\ufb01cients have become \ufb01nely tuned to the data by developing large positive and negative values so that the correspondpolynomial for N = 15 data points (left plot) and N = 100 data points (right plot). We see that increasing the size of the data set reduces the over-\ufb01tting problem", "9b628e02-e2b5-4a52-818f-d15a10c70cca": "S\u00b8im\u00b8sek, \u00a8O., Alg\u00b4orta, S., Kothiyal, A. Why most decisions are easy in tetris\u2014And perhaps in other sequential decision problems, as well. In Proceedings of the 33rd International Conference on Machine Learning , pp. 1757-1765. Simon, H. Lecture at the Earthware Symposium, Carnegie Mellon University. https://www.youtube.com/w Singh, S. P. Reinforcement learning with a hierarchy of abstract models. In Proceedings of the Tenth National Conference on Arti\ufb01cial Intelligence (AAAI-92), pp. 202\u2013207. AAAI/MIT Press, Menlo Park, CA. Singh, S. P. Scaling reinforcement learning algorithms by learning variable temporal resolution models. In Proceedings of the 9th International Workshop on Machine Learning, pp. 406\u2013415. Morgan Kaufmann. Singh, S. P. Learning to Solve Markovian Decision Processes. Ph.D", "05e57253-6b2f-494d-9c16-3ecac6599608": "Pseudocode for the full algorithm is shown in the box below.\n\nInput: an arbitrary behavior policy b such that b(a|s) > 0, for all s 2 S, a 2 A Initialize Q(s, a) arbitrarily, for all s 2 S, a 2 A Initialize \u21e1 to be greedy with respect to Q, or as a \ufb01xed given policy Algorithm parameters: step size \u21b5 2 (0, 1], a positive integer n All store and access operations (for St, At, and Rt) can take their index mod n + 1 The o\u21b5-policy version of n-step Expected Sarsa would use the same update as above for n-step Sarsa except that the importance sampling ratio would have one less factor in it. That is, the above equation would use \u21e2t+1:t+n\u22121 instead of \u21e2t+1:t+n, and of course it would use the Expected Sarsa version of the n-step return (7.7). This is because in Expected Sarsa all possible actions are taken into account in the last state; the one actually taken has no e\u21b5ect and does not have to be corrected for", "b63cb3d9-bcc5-4089-9be4-ee340a4dcde8": "The back-propagation algorithm is designed to reduce the number of common subexpressions without regard to memory. Specifically, it performs on the order of one Jacobian product per node in the graph.\n\nThis can be seen from the fact that backprop (algorithm 6.2) visits each edge from node uD to node u\u2122 of the graph exactly once in order to obtain the associated partial derivative au Back-propagation thus avoids the exponential explosion in repeated subexpres- sions. Other algorithms may be able to avoid more subexpressions by performing simplifications on the computational graph, or may be able to conserve memory by recomputing rather than storing some subexpressions. We revisit these ideas  after describing the back-propagation algorithm itself. Algorithm 6.2 Simplified version of the back-propagation algorithm for computing the derivatives of u!\u2122 with respect to the variables in the graph. This example is intended to further understanding by showing a simplified case where all variables are scalars, and we wish to compute the derivatives with respect to uw, Lee Un),  This simplified version computes the derivatives of all nodes in the graph", "53cdf9b6-08fe-45ab-8d87-deb15e6ebb1d": "Thus, contrastive loss takes a pair of inputs (x;, x;) and minimizes the embedding distance when they are from the same class but maximizes the distance otherwise. Leont (is X51 9) = 1 max(0, \u20ac \u2014 || fo(%:) \u2014 fa(x,) Ilo)?\n\nwhere \u20ac is a hyperparameter, defining the lower bound distance between samples of different classes. Triplet Loss  Triplet loss was originally proposed in the FaceNet  paper and was used to learn face recognition of the same person at different poses and angles. https://lilianweng.github.io/posts/2021-05-31-contrastive/  Contrastive Representation Learning | Lil'Log  Negative  Anchor LEARNING Negative Anchor  Positive Positive  Given one anchor input x, we select one positive sample x* and one negative x~, meaning that xt and x belong to the same class and x\u201d is sampled from another different class", "8a57bfcd-7e23-442f-8e72-ee07649f4a0c": "In the second column there are M - 2 independent parameters, because the column must be normalized and also must be orthogonal to the previous column, and so on. Summing this arithmetic series, we see that R has a total of M(M -1)/2 independent parameters. Thus the number of degrees of freedom in the covariance matrix C is given by DM + 1 - M(M - 1)/2. (12.51) The number of independent parameters in this model therefore only grows linearly with D, for fixed M. If we take M = D - 1, then we recover the standard result for a full covariance Gaussian. In this case, the variance along D - 1 linearly independent directions is controlled by the columns of W, and the variance along the remaining direction is given by a 2 . If M = 0, the model is equivalent to the isotropic covariance case. As we have seen, the probabilistic PCA model can be expressed in terms of a marginalization over a continuous latent space z in which for each data point X n , there is a corresponding latent variable Zn", "ea03962c-3389-4f9e-837d-357096685842": "The cartoon view of a complex cell is that it computes the L? norm of the 2-D vector containing two simple cells\u2019 responses: c(I) = \\/so(I)? + 5,(J)?. An important special case occurs when 5; has all the same parameters as s9 except for \u00a2, and \u00a2 is set such that s1 is one quarter cycle out of phase with so. In this case, so and s; form a quadrature pair. A complex cell defined in this way responds when the Gaussian reweighted image I(a,y) exp(\u2014,x\u201d\" \u2014 Byy\u201d) contains a high-amplitude sinusoidal wave with frequency f in direction 7 near (29, yo), regardless of the phase offset of this wave. In other words, the complex cell is invariant to small translations of the image in direction 7, or to negating the image (replacing black with white and vice versa).\n\nSome of the most striking correspondences between neuroscience and machine learning come from visually comparing the features learned by machine learning models with those employed by V1", "e761a861-5a60-4e4c-9d87-9ffd2edeb5f1": "25 Illustration of style and content reconstructions in Neural Style Transfer   Style Target porelut2 pbrelu2.2 pdrelu3.3 y6,relud.3  style \u201cstyle style style ce fw LEBY) ----FF---FE-----f]------F fh;  oeeea  Input | H Image Tage Transform Net | ' Loss Network vee-1)| Gg! Fe1u3.3 Content Target efit i. Fig. 26 Illustration of the Fast neural style algorithm by Johnson et al. (35,  data into a night-to-day scale, winter-to-summer, or rainy-to-sunny scale. However, in other application domains, the set of styles to transfer into is not so obvious. For ease of implementation, data augmentation via Neural Style Transfer could be done by selecting a set of k styles and applying them to all images in the training set. The work of Style Augmentation , avoids introducing a new form of style bias into the dataset by deriving styles at random from a distribution of 79,433 artistic images", "2337db9f-c290-424e-8d24-c7c652517d66": "Snorkel\u2019s deployments in industry, research laboratories, and government agencies show that it has real-world impact, offering developers an improved way to build models. Acknowledgements Alison Callahan and Nigam Shah of Stanford, and Nicholas Giori of the US Dept. of Veterans Affairs developed the electronic health records application. Emily Mallory, Ambika Acharya, and Russ Altman of Stanford, and Roselie Bright and Elaine Johanson of the US Food and Drug Administration developed the scienti\ufb01c articles application. Joy Ku of the Mobilize Center organized the user study. Nishith Khandwala developed the radiograph application. We thank the contributors to Snorkel including Bryan He, Theodoros Rekatsinas, and Braden Hancock. We gratefully acknowledge the support of DARPA under Nos. N66001-15-C-4043 (SIMPLEX), FA875017-2-0095 (D3M), FA8750-12-2-0335, and FA8750-13-2-0039, DOE 108845, NIH U54EB020405, ONR under Nos", "05d93cf3-4ada-45b0-8ea2-aac088492bad": "Dynamic programming has been extensively developed since the late 1950s, including extensions to partially observable MDPs , many applications , approximation methods , and asynchronous methods . Many excellent modern treatments of dynamic programming are available . Bryson  provides an authoritative history of optimal control. Connections between optimal control and dynamic programming, on the one hand, and learning, on the other, were slow to be recognized.\n\nWe cannot be sure about what accounted for this separation, but its main cause was likely the separation between the disciplines involved and their di\u21b5erent goals. Also contributing may have been the prevalent view of dynamic programming as an o\u270fine computation depending essentially on accurate system models and analytic solutions to the Bellman equation. Further, the simplest form of dynamic programming is a computation that proceeds backwards in time, making it di\ufb03cult to see how it could be involved in a learning process that must proceed in a forward direction. Some of the earliest work in dynamic programming, such as that by Bellman and Dreyfus , might now be classi\ufb01ed as following a learning approach. Witten\u2019s  work (discussed below) certainly quali\ufb01es as a combination of learning and dynamic-programming ideas", "2a31de3f-4fb9-459a-9108-df795bb406e6": "For example, in natural images, pixels that are widely separated in space also have weak correlation, so the generalized pseudolikelihood  can be applied with each S set being a small, spatially localized window. One weakness of the pseudolikelihood estimator is that it cannot be used with other approximations that provide only a lower bound on p(x), such as variational inference, which is covered in chapter 19. This is because p appears  614  https://www.deeplearningbook.org/contents/partition.html    CHAPTER 18, CONFRONTING THE PARTITION FUNCTION  in the denominator. A lower bound on the denominator provides only an upper bound on the expression as a whole, and there is no benefit to maximizing an upper bound. This makes it difficult to apply pseudolikelihood approaches to deep models such as deep Boltzmann machines, since variational methods are one of the dominant approaches to approximately marginalizing out the many layers of hidden variables that interact with each other.\n\nNonetheless, pseudolikelihood is still useful for deep learning, because it can be used to train single-layer models or deep models using approximate inference methods that are not based on lower bounds", "c7737d9e-9d38-451c-ac63-4e44cf497a88": "If we were to draw examples in order from this list, then each of our minibatches would  LA nvetannen alee LinnnA Lananeenn 14 wee nanan 4 wwe nntle Ann RAti nnd Ae04 Af ELA  https://www.deeplearningbook.org/contents/optimization.html    vo CAULLTLUCLY VilascuUu, VOCAUSE LL WUULU LEOPLESCLL pililllaliry VLLO PaviTilt UUL VIL LLCO many patients in the dataset. In cases such as these, where the order of the dataset holds some significance, it is necessary to shufle the examples before selecting minibatches. B or very large datasets, for example, datasets containing billions o  examples in a data center, it can be impractical to sample examples truly uniformly at random every time we want to construct a minibatch", "ab6b33dd-d852-4a0e-a8bf-fe330c769622": "These additional limitations, such as the imperfection of the optimization algorithm, mean that the learning algorithm\u2019s effective capacity may be less than the representational capacity of the model family. Our modern ideas about improving the generalization of machine learning models are refinements of thought dating back to philosophers at least as early as Ptolemy. Many early scholars invoke a principle of parsimony that is now most widely known as Occam\u2019s razor . This principle states that among competing hypotheses that explain known observations equally well, we should choose the \u201csimplest\u201d one. This idea was formalized and made more precise in the twentieth century by the founders of statistical learning theory . Statistical learning theory provides various means of quantifying model capacity.\n\nAmong these, the most well known is the Vapnik-Chervonenkis dimension, or VC dimension. The VC dimension measures the capacity of a binary classifier. The VC dimension is defined as being the largest possible value of m for which there exists a training set of m different 2 points that the classifier can label arbitrarily. Quantifying the capacity of the model enables statistical learning theory to make quantitative predictions", "fa312780-a19b-4503-a99c-27213c8c25b1": "The rough idea is that when a component of wt participates in producing an estimated value, then the corresponding component of zt is bumped up and then begins to fade away. Learning will then occur in that component of wt if a nonzero TD error occurs before the trace falls back to zero. The trace-decay parameter \u03bb 2  determines the rate at which the trace falls. The primary computational advantage of eligibility traces over n-step methods is that only a single trace vector is required rather than a store of the last n feature vectors. Learning also occurs continually and uniformly in time rather than being delayed and then catching up at the end of the episode. In addition learning can occur and a\u21b5ect behavior immediately after a state is encountered rather than being delayed n steps. Eligibility traces illustrate that a learning algorithm can sometimes be implemented in a di\u21b5erent way to obtain computational advantages.\n\nMany algorithms are most naturally formulated and understood as an update of a state\u2019s value based on events that follow that state over multiple future time steps", "1d088448-6601-4c63-b273-7eb115fda2f4": "Let p\u03b1(\u03b8) be some hyperprior for the parameters introduced above, parameterized by \u03b1. The marginal likelihood can be written as: where the \ufb01rst RHS term denotes a KL divergence of the approximate from the true posterior, and where L(\u03c6; X) denotes the variational lower bound to the marginal likelihood: Note that this is a lower bound since the KL divergence is non-negative; the bound equals the true marginal when the approximate and true posteriors match exactly. The term log p\u03b8(X) is composed of a sum over the marginal likelihoods of individual datapoints log p\u03b8(X) = \ufffdN i=1 log p\u03b8(x(i)), which can each be rewritten as: where again the \ufb01rst RHS term is the KL divergence of the approximate from the true posterior, and L(\u03b8, \u03c6; x) is the variational lower bound of the marginal likelihood of datapoint i: The expectations on the RHS of eqs (14) and (16) can obviously be written as a sum of three separate expectations, of which the second and third component can sometimes be analytically solved, e.g. when both p\u03b8(x) and q\u03c6(z|x) are Gaussian", "e7a9abe8-c875-499c-a3eb-33a05b174f2e": "The most serious limitation of the Laplace framework, however, is that it is based purely on the aspects of the true distribution at a speci\ufb01c value of the variable, and so can fail to capture important global properties. In Chapter 10 we shall consider alternative approaches which adopt a more global perspective. As well as approximating the distribution p(z) we can also obtain an approximation to the normalization constant Z.\n\nUsing the approximation (4.133) we have where we have noted that the integrand is Gaussian and made use of the standard result (2.43) for a normalized Gaussian distribution. We can use the result (4.135) to obtain an approximation to the model evidence which, as discussed in Section 3.4, plays a central role in Bayesian model comparison. Consider a data set D and a set of models {Mi} having parameters {\u03b8i}. For each model we de\ufb01ne a likelihood function p(D|\u03b8i, Mi). If we introduce a prior p(\u03b8i|Mi) over the parameters, then we are interested in computing the model evidence p(D|Mi) for the various models", "7f3201ed-d19d-49a9-8083-7a46be1b6c6d": "Now suppose we \ufb01rst run K-means on the image data, and then instead of transmitting the original pixel intensity vectors we transmit the identity of the nearest vector \u00b5k. Because there are K such vectors, this requires log2 K bits per pixel. We must also transmit the K code book vectors \u00b5k, which requires 24K bits, and so the total number of bits required to transmit the image is 24K + N log2 K (rounding up to the nearest integer). The original image shown in Figure 9.3 has 240 \u00d7 180 = 43, 200 pixels and so requires 24 \u00d7 43, 200 = 1, 036, 800 bits to transmit directly. By comparison, the compressed images require 43, 248 bits (K = 2), 86, 472 bits (K = 3), and 173, 040 bits (K = 10), respectively, to transmit. These represent compression ratios compared to the original image of 4.2%, 8.3%, and 16.7%, respectively. We see that there is a trade-off between degree of compression and image quality", "7994b4e6-7dbf-4a97-8496-474d07c25385": "The idea follows the same route of GPI. Within one episode, it works as follows:  Initialize t = 0. Start with So and choose action Ay = arg MaX,<4 Q(So, a), where e-greedy is commonly applied. At time t, after applying action A;, we observe reward R;,, and get into the next state S;,,. Then pick the next action in the same way as in step 2: Ay,; = argmax,cy Q(S1,1, 4). Update the Q-value function: Q(S:, Ar) \u2014 Q(S:, At) + a( Rey + YQ(Si41, Atv) \u2014 Q(S:, At). Set t = t+ 1 and repeat from step 3. In each step of SARSA, we need to choose the next action according to the current policy. Q-Learning: Off-policy TD control  The development of Q-learning  is a big breakout in the early days of Reinforcement Learning", "b171da0c-7e9f-4d3b-947f-eed5bc43db24": "Equation (13.6) involves an appropriate sum over actions, but each term is not weighted by \u21e1(a|St, \u2713) as is needed for an expectation under \u21e1. So we introduce such a weighting, without changing the equality, by multiplying and then dividing the summed terms by \u21e1(a|St, \u2713). Continuing from (13.6), we have where Gt is the return as usual. The \ufb01nal expression in brackets is exactly what is needed, a quantity that can be sampled on each time step whose expectation is equal to the gradient. Using this sample to instantiate our generic stochastic gradient ascent algorithm (13.1) yields the REINFORCE update: This update has an intuitive appeal. Each increment is proportional to the product of a return Gt and a vector, the gradient of the probability of taking the action actually taken divided by the probability of taking that action.\n\nThe vector is the direction in parameter space that most increases the probability of repeating the action At on future visits to state St. The update increases the parameter vector in this direction proportional to the return, and inversely proportional to the action probability", "d5852cc6-2cd5-4c3e-8436-e45609130008": "To demonstrate this, we consider the emerging task of prompting a large pretrained LM for controllable generation . The goal is to learn to generate text prompts that steer the LM to generate sentences of certain desired attributes (e.g., topics). The problem of controlling the generation of pretrained LMs was previously approached through specialized algorithms such as modifying the LM hidden states during decoding . Here we show that prompts offer an easier, faster, more effective way for controlled generation.\n\nLearning to generate/tune prompts is gaining increasing attention recently. It side-steps the needs for expensive LM \ufb01ne-tuning, and adapts LMs to new scenarios with prompt as the (computefriendly) interface. Most existing approaches  rely on gradient backpropagation and are applicable only when the whole training pipeline is differentiable. This does not hold for the text generation setting, as illustrated in Figure 5. In contrast, the RL framework is generally applicable to any differentiable or discrete pipelines. Setup (more in \u00a7A.2.3)", "80574f9d-f0fc-4fd0-8467-bb309e78644f": "The result is a distribution that in general has longer \u2018tails\u2019 than a Gaussian, as was seen in Figure 2.15. This gives the tdistribution an important property called robustness, which means that it is much less sensitive than the Gaussian to the presence of a few data points which are outliers. The robustness of the t-distribution is illustrated in Figure 2.16, which compares the maximum likelihood solutions for a Gaussian and a t-distribution. Note that the maximum likelihood solution for the t-distribution can be found using the expectationmaximization (EM) algorithm.\n\nHere we see that the effect of a small number of Exercise 12.24 distribution of 30 data points drawn from a Gaussian distribution, together with the maximum likelihood \ufb01t obtained from a t-distribution (red curve) and a Gaussian (green curve, largely hidden by the red curve). Because the t-distribution contains the Gaussian as a special case it gives almost the same solution as the Gaussian", "e60a013b-570f-46c7-8295-6181a63453fe": "\u201cRepresentation Learning with Contrastive Predictive Coding\" arXiv preprint arXiv:1807.03748 . 23] Jason Wei and Kai Zou. \u201cEDA: Easy data augmentation techniques for boosting performance on text classification tasks.\" EMNLP-IJCNLP 2019. 24] Sosuke Kobayashi. \u201cContextual Augmentation: Data Augmentation by Words with Paradigmatic Relations.\" NAACL 2018  25] Hongchao Fang et al. \"CERT: Contrastive self-supervised learning for language understanding.\" arXiv preprint arXiv:2005.12766 . 26] Dinghan Shen et al. \u201cA Simple but Tough-to-Beat Data Augmentation Approach for Natural Language Understanding and Generation.\" arXiv preprint arXiv:2009.13818    27] Tianyu Gao et al. \u201cSimCSE: Simple Contrastive Learning of Sentence Embeddings.\" arXiv preprint arXiv:2104.08821 . 28] Nils Reimers and Iryna Gurevych", "1cd341bc-209a-46f6-ac55-40aa784dfd07": "These constraints explain why the MDP has a NoOp action and why the reward signal is 0 except when a read or write command is issued. NoOp is issued when it is the sole legal action in a state.\n\nTo maximize utilization of the memory system, the controller\u2019s task is to drive the system to states in which either a read or a write action can be selected: only these actions result in sending data over the external data bus, so it is only these that contribute to the throughput of the system. Although precharge and activate produce no immediate reward, the agent needs to select these actions to make it possible to later select the rewarded read and write actions. The scheduling agent used Sarsa (Section 6.4) to learn an action-value function. States were represented by six integer-valued features. To approximate the action-value function, the algorithm used linear function approximation implemented by tile coding with hashing (Section 9.5.4). The tile coding had 32 tilings, each storing 256 action values as 16-bit \ufb01xed point numbers. Exploration was \"-greedy with \" = 0.05", "34c7072e-611b-4f10-9b5f-90c05b2ff80c": "We might ask whether it is possible to de\ufb01ne an alternative graphical semantics for probability distributions such that conditional independence is determined by simple graph separation.\n\nThis is indeed the case and corresponds to undirected graphical models. By removing the directionality from the links of the graph, the asymmetry between parent and child nodes is removed, and so the subtleties associated with head-to-head nodes no longer arise. Suppose that in an undirected graph we identify three sets of nodes, denoted A, B, and C, and that we consider the conditional independence property To test whether this property is satis\ufb01ed by a probability distribution de\ufb01ned by a graph we consider all possible paths that connect nodes in set A to nodes in set B. If all such paths pass through one or more nodes in set C, then all such paths are \u2018blocked\u2019 and so the conditional independence property holds. However, if there is at least one such path that is not blocked, then the property does not necessarily hold, or more precisely there will exist at least some distributions corresponding to the graph that do not satisfy this conditional independence relation. This is illustrated with an example in Figure 8.27", "55d1a023-4f50-4357-b951-3db437bdadb2": "In this diagram we distinguish the \u201crepresentation\u201d part of the model (the \u201ctask network,\u201d here a recurrent net in the bottom) from the \u201cmemory\u201d part of the model (the set of cells), which can store facts. The task network learns to \u201ccontrol\u201d the memory, deciding where to read from and where to write to within the memory (through the reading and writing mechanisms, indicated by bold arrows pointing at the reading and writing addresses). 414  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  Explicit memory seems to allow models to learn tasks that ordinary RNNs or LSTM RNNs cannot learn. One reason for this advantage may be that information and  roo 1 ,04\u00a2 trout 1 1 trout ard  https://www.deeplearningbook.org/contents/rnn.html    TAUIELLLS Call DE propagated (4oL wald Ll Lune OF DaCKWalrU Il LUE, LESPECLLVEly ) or very long durations.\n\nAs an alternative to back-propagation through weighted averages of memory cells, we can interpret the memory addressing coefficients as probabilities and  stochastically read just one cell", "1c7bdd83-6733-4bb4-9726-728f7bc6c508": "The geometrical interpretation of the sum-of-squares error function is illustrated in Figure 1.3. We can solve the curve \ufb01tting problem by choosing the value of w for which E(w) is as small as possible. Because the error function is a quadratic function of the coef\ufb01cients w, its derivatives with respect to the coef\ufb01cients will be linear in the elements of w, and so the minimization of the error function has a unique solution, denoted by w\u22c6, which can be found in closed form. The resulting polynomial is Exercise 1.1 There remains the problem of choosing the order M of the polynomial, and as we shall see this will turn out to be an example of an important concept called model comparison or model selection. In Figure 1.4, we show four examples of the results of \ufb01tting polynomials having orders M = 0, 1, 3, and 9 to the data set shown in Figure 1.2.\n\nWe notice that the constant (M = 0) and \ufb01rst order (M = 1) polynomials give rather poor \ufb01ts to the data and consequently rather poor representations of the function sin(2\u03c0x). The third order (M = 3) polynomial seems to give the best \ufb01t to the function sin(2\u03c0x) of the examples shown in Figure 1.4", "97c1523e-8da6-452e-a1c2-5c4d4449c59e": "We recommend a terminology in which Figure 5.1 is called a two-layer network, because it is the number of layers of adaptive weights that is important for determining the network properties.\n\nAnother generalization of the network architecture is to include skip-layer connections, each of which is associated with a corresponding adaptive parameter. For instance, in a two-layer network these would go directly from inputs to outputs. In principle, a network with sigmoidal hidden units can always mimic skip layer connections (for bounded input values) by using a suf\ufb01ciently small \ufb01rst-layer weight that, over its operating range, the hidden unit is effectively linear, and then compensating with a large weight value from the hidden unit to the output. In practice, however, it may be advantageous to include skip-layer connections explicitly. Furthermore, the network can be sparse, with not all possible connections within a layer being present. We shall see an example of a sparse network architecture when we consider convolutional neural networks in Section 5.5.6. Because there is a direct correspondence between a network diagram and its mathematical function, we can develop more general network mappings by considering more complex network diagrams", "fa5ea8c1-f32d-4959-aa7c-26be7c26f95b": "It has been reported that gradient-based optimization of conditional Gaussian mixtures (on the output of neural networks) can be unreliable, in part because one gets divisions (by the variance) which can be numerically unstable (when some variance gets to be small for a particular example, yielding very large gradients). One solution is to clip gradients (see section 10.11.1), while another is to scale the gradients heuristically .\n\nGaussian mixture outputs are particularly effective in generative models of speech  and movements of physical objects . The mixture density strategy gives a way for the network to represent multiple output modes and to control the variance of its output, which is crucial for obtaining  1 We consider c to be latent because we do not observe it in the data: given input x and target y, it is not possible to know with certainty which Gaussian component was responsible fory, but we can imagine that y was generated by picking one of them, and we can make that unobserved choice a random variable. 186  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  cn  https://www.deeplearningbook.org/contents/mlp.html     Figure 6.4: Samples drawn from a neural network with a mixture density output layer", "2ef4ef6b-c378-4c37-bdca-76316b3c81f1": "Comparing the predicted returns of simulated paths is a simple form of planning, which can be done in a variety of ways as discussed in Chapter 8. When the environment of a model-free agent changes the way it reacts to the agent\u2019s actions, the agent has to acquire new experience in the changed environment during which it can update its policy and/or value function. In the model-free strategy shown in Figure 14.5 (lower left), for example, if one of the goal boxes were to somehow shift to delivering a di\u21b5erent reward, the rat would have to traverse the maze, possibly many times, to experience the new reward upon reaching that goal box, all the while updating either its policy or its action-value function (or both) based on this experience.\n\nThe key point is that for a model-free agent to change the action its policy speci\ufb01es for a state, or to change an action value associated with a state, it has to move to that state, act from it, possibly many times, and experience the consequences of its actions. A model-based agent can accommodate changes in its environment without this kind of \u2018personal experience\u2019 with the states and actions a\u21b5ected by the change. A change in its model automatically (through planning) changes its policy", "01074af7-a127-4859-8533-be5524f46e2b": "They showed that the most important component is the element-wise difference | f(x) \u2014 f(x\u2019). Regression objective: This is the regression loss on cos( f(x), f(x\u2019)), in which the pooling strategy has a big impact. In the experiments, they observed that max performs much worse than mean and cis -token. https://lilianweng.github.io/posts/2021-05-31-contrastive/     Contrastive Representation Learning | Lil'Log  In the experiments, which objective function works the best depends on the datasets, so there is no  Triplet objective: max(0, | f(x)  Sf(x*)|  fx)  f(x~)| + \u20ac), where x,x*,x~ are  embeddings of the anchor, positive and negative sentences. universal winner", "4fcbbd18-0ea5-4c18-9b5b-17a244f7d344": "After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.\n\nThis is the original form of the k-armed bandit problem, so named by analogy to a slot machine, or \u201cone-armed bandit,\u201d except that it has k levers instead of one. Each action selection is like a play of one of the slot machine\u2019s levers, and the rewards are the payo\u21b5s for hitting the jackpot. Through repeated action selections you are to maximize your winnings by concentrating your actions on the best levers. Another analogy is that of a doctor choosing between experimental treatments for a series of seriously ill patients. Each action is the selection of a treatment, and each reward is the survival or well-being of the patient. Today the term \u201cbandit problem\u201d is sometimes used for a generalization of the problem described above, but in this book we use it to refer just to this simple case. In our k-armed bandit problem, each of the k actions has an expected or mean reward given that that action is selected; let us call this the value of that action", "d9693618-ef83-49df-b8fc-cc7383cc6cd0": "Transferring style in training data has been tested on the transition from simulated environments to the real-world.\n\nThis is very useful for robotic manipulation tasks using Reinforcement Learning because of potential damages to hardware when train- ing in the real-world. Many constraints such as low-fidelity cameras cause these models to generalize poorly when trained in physics simulations and deployed in the real-world. Tobin et al. explore the effectiveness of using different styles in training simula- tion and achieve within 1.5 cm accuracy in the real-world on the task of object localiza-  tion. Their experiments randomize the position and texture of the objects to be detected Shorten and Khoshgoftaar J Big Data  6:60   Fig. 27 Examples of different styles simulated by Tobin et al. on the table in the simulation, as well as the texture, lighting, number of lights, and ran- dom noise in the background. They found that with enough variability in the training data style, the real-world simply appears as another variation to the model. Interestingly, they found that diversity in styles was more effective than simulating in as realistic of an environment as possible. This is in contrast to the work of Shrivastava et al", "94f0a705-25c8-4690-9e84-e1712ed42612": "One option is to have a tree structure that does not depend on the data,  ?We suggest not abbreviating \u201crecursive neural network\u201d as \u201cRNN\u201d to avoid confusion with \u201crecurrent neural network.\u201d  394  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  Figure 10.14: A recursive network has a computational graph that generalizes that of the  my 9N \u201c4 https://www.deeplearningbook.org/contents/rnn.html     recurrent network from a chain to a tree.\n\nA variable-size sequence \u00ae*\u201d\u2019, @*\u2019,...,@*\u201d can be mapped to a fixed-size representation (the output \u00a9), witha fixed set of parameters . Ideally, one would like the learner itself to discover and infer the tree structure that is appropriate for any given input, as suggested by Bottou . Many variants of the recursive net idea are possible. For example, Frasconi et al. and Frasconi et al. associate the data with a tree structure, and associate the inputs and targets with individual nodes of the tree", "cd895f3e-f8ee-4326-ae5a-dda234c7a3e9": "For example, if you are one cell to the right of the goal and you move left, then one-third of the time you move one cell above the goal, one-third of the time you move two cells above the goal, and one-third of the time you move to the goal.\n\n\u21e4 One of the early breakthroughs in reinforcement learning was the development of an o\u21b5-policy TD control algorithm known as Q-learning , de\ufb01ned by In this case, the learned action-value function, Q, directly approximates q\u21e4, the optimal action-value function, independent of the policy being followed. This dramatically simpli\ufb01es the analysis of the algorithm and enabled early convergence proofs. The policy still has an e\u21b5ect in that it determines which state\u2013action pairs are visited and updated. However, all that is required for correct convergence is that all pairs continue to be updated. As we observed in Chapter 5, this is a minimal requirement in the sense that any method guaranteed to \ufb01nd optimal behavior in the general case must require it. Under this assumption and a variant of the usual stochastic approximation conditions on the sequence of step-size parameters, Q has been shown to converge with probability 1 to q\u21e4. The Q-learning algorithm is shown below in procedural form", "67ea78bc-a840-4119-a05d-0fac30508f1b": "\u201cA Survey on Contrastive Self-Supervised Learning.\" arXiv preprint arXiv:2011.00362   17] Jure Zbontar et al. \u201cBarlow Twins: Self-Supervised Learning via Redundancy Reduction.\" arXiv preprint arXiv:2103.03230   18  Alec Radford, et al. \u201cLearning Transferable Visual Models From Natural Language Supervision\u201d arXiv preprint arXiv:2103.00020   19] Mathilde Caron et al. \u201cUnsupervised Learning of Visual Features by Contrasting Cluster  https://lilianweng.github.io/posts/2021-05-31-contrastive/   Contrastive Representation Learning | Lil'Log   Assignments (SWAV).\"\n\nNeuriPS 2020. 20] Mathilde Caron et al. \u201cDeep Clustering for Unsupervised Learning of Visual Features.\" ECCV 2018. 21] Prannay Khosla et al. \u201cSupervised Contrastive Learning.\" NeurlPS 2020. 22] Aaron van den Oord, Yazhe Li & Oriol Vinyals", "84b4cb3d-87d2-4fe6-bd29-c8e4f5037c11": "However, the \ufb01nal function learned was a\u21b5ected only slightly by the width of the features. Receptive \ufb01eld shape tends to have a strong e\u21b5ect on generalization but little e\u21b5ect on asymptotic solution quality. Tile coding is a form of coarse coding for multi-dimensional continuous spaces that is \ufb02exible and computationally e\ufb03cient. It may be the most practical feature representation for modern sequential digital computers. In tile coding the receptive \ufb01elds of the features are grouped into partitions of the state space. Each such partition is called a tiling, and each element of the partition is called a tile. For example, the simplest tiling of a two-dimensional state space is a uniform grid such as that shown on the left side of Figure 9.9. The tiles or receptive \ufb01eld here are squares rather than the circles in Figure 9.6. If just this single tiling were used, then the state indicated by the white spot would be represented by the single feature whose tile it falls within; generalization would be complete to all states within the same tile and nonexistent to states outside it", "c2abb443-651c-4f9b-88ad-9546c4163caf": "In this case, the network can contain as many convolutional layers as the available hardware can support, since the operation of convolution does not modify the architectural possibilities  343  CHAPTER 9. CONVOLUTIONAL NETWORKS  ()  Strided convolution Downsampling  Convolution  https://www.deeplearningbook.org/contents/convnets.html    OCRORORO  Figure 9.12: Convolution with a stride. In this example, we use a stride of two. (Top)Convolution with a stride length of two implemented in a single operation. (Bot- tom)Convolution with a stride greater than one pixel is mathematically equivalent to convolution with unit stride followed by downsampling. Obviously, the two-step approach involving downsampling is computationally wasteful, because it computes many values that are then discarded. available to the next layer. The input pixels near the border, however, influence fewer output pixels than the input pixels near the center", "30a4018e-9371-4946-840a-4d278e52e486": "Linear models, such as logistic regression and linear regression, are appealing because they can be fit efficiently and reliably, either in closed form or with convex optimization. Linear models also have the obvious defect that the model capacity is limited to linear functions, so  https://www.deeplearningbook.org/contents/mlp.html    the model cannot understand the interaction between any two input variables. To extend linear models to represent nonlinear functions of \u00a9, we can apply the linear model not to 2 itself but to a transformed input \u00a2(#), where \u00a2 is a  165  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  nonlinear transformation. Equivalently, we can apply the kernel trick described in section 5.7.2, to obtain a nonlinear learning algorithm based on implicitly applying the @ mapping. We can think of \u00a2 as providing a set of features describing x, or as providing a new representation for a. The question is then how to choose the mapping @. 1. One option is to use a very generic \u00a2, such as the infinite-dimensional \u00a2 that is implicitly used by kernel machines based on the RBF kernel", "4ebe2b5f-8268-44f8-99ed-9c1dcecb1a04": "This means that immediately neighboring locations will have different filters, as in a locally connected layer, but the memory requirements for storing the parameters will increase only by a factor of the size of this set of kernels, rather than by the size of the entire output feature map.\n\nSee figure 9.16 for a comparison of locally connected layers, tiled convolution, and standard convolution. To define tiled convolution algebraically, let K be a 6-D tensor, where two of the dimensions correspond to different locations in the output map. Rather than having a separate index for each location in the output map, output locations cycle through a set of t different choices of kernel stack in each direction. If t is equal to  346  CHAPTER 9. CONVOLUTIONAL NETWORKS  https://www.deeplearningbook.org/contents/convnets.html    Figure 9.14: Comparison of local connections, convolution, and full connections. (Top)A locally connected layer with a patch size of two pixels. Each edge is labeled with a unique letter to show that each edge is associated with its own weight parameter. (Center)A convolutional layer with a kernel width of two pixels", "d7af4005-1754-4ff8-a7de-2f0a6bf3a49a": "Because we cannot use the complete-data log likelihood, we consider instead its expected value under the posterior distribution of the latent variable, which corresponds (as we shall see) to the E step of the EM algorithm. In the subsequent M step, we maximize this expectation. If the current estimate for the parameters is denoted \u03b8old, then a pair of successive E and M steps gives rise to a revised estimate \u03b8new. The algorithm is initialized by choosing some starting value for the parameters \u03b80. The use of the expectation may seem somewhat arbitrary. However, we shall see the motivation for this choice when we give a deeper treatment of EM in Section 9.4. In the E step, we use the current parameter values \u03b8old to \ufb01nd the posterior distribution of the latent variables given by p(Z|X, \u03b8old)", "ef07aa81-b565-4d91-8182-301bcb5612d1": ", xN+1 and corresponding target observations t1, . , tN. 6.24 (\u22c6) Show that a diagonal matrix W whose elements satisfy 0 < Wii < 1 is positive de\ufb01nite. Show that the sum of two positive de\ufb01nite matrices is itself positive de\ufb01nite. 6.27 (\u22c6 \u22c6 \u22c6) Derive the result (6.90) for the log likelihood function in the Laplace approximation framework for Gaussian process classi\ufb01cation. Similarly, derive the results (6.91), (6.92), and (6.94) for the terms in the gradient of the log likelihood. In the previous chapter, we explored a variety of learning algorithms based on nonlinear kernels.\n\nOne of the signi\ufb01cant limitations of many such algorithms is that the kernel function k(xn, xm) must be evaluated for all possible pairs xn and xm of training points, which can be computationally infeasible during training and can lead to excessive computation times when making predictions for new data points. In this chapter we shall look at kernel-based algorithms that have sparse solutions, so that predictions for new inputs depend only on the kernel function evaluated at a subset of the training data points", "2950a372-c591-4898-86c3-cb623cd8f337": "In a traditional supervised learning setup, we would learn h\u03b8 by \ufb01tting it to a training set of labeled data points. However, in our setting, we assume that we only have access to unlabeled data for training. We do assume access to a small set of labeled data used during development, called the development set, and a blind, held-out labeled test set for evaluation. These sets can be orders of magnitudes smaller than a training set, making them economical to obtain.\n\nThe user of Snorkel aims to generate training labels by providing a set of labeling functions, which are black-box functions, \u03bb : X \u2192 Y \u222a {\u2205}, that take in a data point and output a label where we use \u2205 to denote that the labeling function abstains. Given m unlabeled data points and n labeling functions, Snorkel applies the labeling functions over the unlabeled data to produce a matrix of labeling function outputs \ufffd \u2208 (Y \u222a {\u2205})m\u00d7n. The goal of the remaining Fig. 4 Labeling functions take as input a Candidate object, representing a data point to be classi\ufb01ed", "0cae7e82-cc92-496d-b248-19e71944c863": "In section 3.3.2, we saw that the probability of a continuous vector-valued x lying in some set S is given by the integral of p(x) over the set S. Some choices of set S can produce paradoxes. For example, it is possible to construct two sets S; and Sg such that p@ \u20ac S1) + p(w \u20ac Sz) > 1 but S;MS2 = @.\n\nThese sets are generally constructed making very heavy use of the infinite precision of real numbers, for example by making fractal-shaped sets or sets that are defined  https://www.deeplearningbook.org/contents/prob.html    by transforming the set of rational numbers.\u201d One of the key contributions of measure theory is to provide a characterization of the set of sets we can compute he probability of without encountering paradoxes. In this book, we integrate only  over sets with relatively simple descriptions, so this aspect of measure theory never becomes a relevant concern. For our purposes, measure theory is more useful for describing theorems that apply to most points in R\u201d but do not apply to some corner cases. Measure theory provides a rigorous way of describing that a set of points is negligibly small. Such a set is said to have measure zero", "39f1857f-bd29-457a-b8bf-9489037a73e4": "We can derive the EM algorithm for the linear dynamical system as follows. Let us denote the estimated parameter values at some particular cycle of the algorithm by \u03b8old. For these parameter values, we can run the inference algorithm to determine the posterior distribution of the latent variables p(Z|X, \u03b8old), or more precisely those local posterior marginals that are required in the M step. In particular, we shall require the following expectations where we have used (13.104). Now we consider the complete-data log likelihood function, which is obtained by taking the logarithm of (13.6) and is therefore given by in which we have made the dependence on the parameters explicit.\n\nWe now take the expectation of the complete-data log likelihood with respect to the posterior distribution p(Z|X, \u03b8old) which de\ufb01nes the function In the M step, this function is maximized with respect to the components of \u03b8. Consider \ufb01rst the parameters \u00b50 and V0. If we substitute for p(z1|\u00b50, V0) in (13.108) using (13.77), and then take the expectation with respect to Z, we obtain where all terms not dependent on \u00b50 or V0 have been absorbed into the additive constant", "c8c8c0a9-a815-4e35-bc60-25fea37db1cf": "Xinyun Chen, Chen Liang, Adams Wei Yu, Dawn Song, and Denny Zhou. 2020e. Compositional generalization via neural-symbolic stack machines. Advances in Neural Information Processing Systems, 33. Yong Cheng, Lu Jiang, and Wolfgang Macherey. 2019.\n\nRobust neural machine translation with doubly adversarial inputs. Proceedings of the 57th Annual Yong Cheng, Lu Jiang, Wolfgang Macherey, and Jacob Eisenstein. 2020b. AdvAug: Robust adversarial augmentation for neural machine translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5961\u2013 5970, Online. Association for Computational Linguistics. Yong Cheng, Wei Xu, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. 2016. Semisupervised learning for neural machine translation. Kevin Clark, Minh-Thang Luong, Christopher D. Manning, and Quoc Le. 2018", "ad8c4ba2-1c3b-40ab-9d0e-3538dd226723": "This points to a direction in which the reinforcement learning framework might be developed in the future to exploit computational advantages of separate appetitive and aversive systems, but for now we are passing over these possibilities. Another discrepancy in terminology is how we use the word action. To many cognitive scientists, an action is purposeful in the sense of being the result of an animal\u2019s knowledge about the relationship between the behavior in question and the consequences of that behavior. An action is goal-directed and the result of a decision, in contrast to a response, which is triggered by a stimulus; the result of a re\ufb02ex or a habit.\n\nWe use the word action without di\u21b5erentiating among what others call actions, decisions, and responses. These are important distinctions, but for us they are encompassed by di\u21b5erences between model-free and model-based reinforcement learning algorithms, which we discussed above in relation to habitual and goal-directed behavior in Section 14.6. Dickinson  discusses the distinction between responses and actions. A term used a lot in this book is control. What we mean by control is entirely di\u21b5erent from what it means to animal learning psychologists", "8f729e91-097b-40c1-b7b3-9dd8937ebf16": "No triggering stimuli were presented, and after the monkey reached for and ate the food morsel, the experimenter usually (though not always), silently and unseen by the monkey, replaced food in the bin by sticking it onto a rigid wire.\n\nHere too, the activity of the dopamine neurons Romo and Schultz monitored was not related to the monkey\u2019s movements, but a large percentage of these neurons produced phasic responses whenever the monkey \ufb01rst touched a food morsel. These neurons did not respond when the monkey touched just the wire or explored the bin when no food was there. This was good evidence that the neurons were responding to the food and not to other aspects of the task. The purpose of Romo and Schultz\u2019s second task was to see what happens when movements are triggered by stimuli. This task used a di\u21b5erent bin with a movable cover. The sight and sound of the bin opening triggered reaching movements to the bin. In this case, Romo and Schultz found that after some period of training, the dopamine neurons no longer responded to the touch of the food but instead responded to the sight and sound of the opening cover of the food bin", "fdcb51bb-f08b-4d3d-9c31-d313192d4cfc": "This equation should include an expectation conditioned on following the policy, \u21e1. Then give a second equation in which the expected value is written out explicitly in terms of \u21e1(a|s) such that no expected value notation appears in the equation. \u21e4 Exercise 3.19 The value of an action, q\u21e1(s, a), depends on the expected next reward and the expected sum of the remaining rewards. Again we can think of this in terms of a small backup diagram, this one rooted at an action (state\u2013action pair) and branching to the possible next states: Give the equation corresponding to this intuition and diagram for the action value, q\u21e1(s, a), in terms of the expected next reward, Rt+1, and the expected next state value, v\u21e1(St+1), given that St =s and At =a. This equation should include an expectation but not one conditioned on following the policy.\n\nThen give a second equation, writing out the expected value explicitly in terms of p(s0, r|s, a) de\ufb01ned by (3.2), such that no expected value notation appears in the equation. \u21e4 Solving a reinforcement learning task means, roughly, \ufb01nding a policy that achieves a lot of reward over the long run", "16c380d1-9e51-4437-8f0e-f2e5f7c23bf1": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3-4):285\u2013294. Thompson, W. R. On the theory of apportionment. American Journal of Mathematics, Thon, M. Spectral Learning of Sequential Systems. Ph.D. thesis, Jacobs University Thon, M., Jaeger, H. Links between multiplicity automata, observable operator models and predictive state representations: a uni\ufb01ed learning framework. The Journal of Machine Learning Research, 16(1):103\u2013147. Thorndike, E. L. Animal intelligence: An experimental study of the associative processes in animals. The Psychological Review, Series of Monograph Supplements, II(4). Tian, T. (in preparation) An Empirical Study of Sliding-Step Methods in Temporal Di\u21b5erence Learning. M.Sc thesis, University of Alberta, Edmonton. Tieleman, T., Hinton, G. Lecture 6.5\u2013RMSProp", "844df381-2354-4eb9-98c4-d6eebf1afb6b": "Besides, compared to both the conventional synonym substitution and the approach that keeps the augmentation network \ufb01xed, our adaptive method that \ufb01ne-tunes the augmentation network jointly with model training achieves superior results. Indeed, the heuristic-based synonym approach can sometimes harm the model performance (e.g., SST-5 and IMDB), as also observed in previous work . This can be because the heuristic rules do not \ufb01t the task or datasets well. In contrast, learning-based augmentation has the advantage of adaptively generating useful samples to improve model training. It is interesting to see from Table 1 that our augmentation method consistently outperforms the weighting method, showing that data augmentation can be a more suitable technique than data weighting for manipulating small-size data.\n\nOur approach provides the generality to instantiate diverse manipulation types and learn with the same single procedure. To investigate the augmentation model and how the \ufb01ne-tuning affects the augmentation results, we show in Figure 2 the top-5 most probable word substitutions predicted by the augmentation model for two masked tokens, respectively. Comparing the results of epoch 1 and epoch 3, we can see the augmentation model evolves and dynamically adjusts the augmentation behavior as the training proceeds", "5817884d-b4a7-4fce-a478-d04fc852ce1b": "Maximum likelihood thus becomes minimization of the negative log-likelihood NLL), or equivalently, minimization of the cross-entropy.\n\nThe perspective of maximum likelihood as minimum KL divergence becomes helpful in this case because the KL divergence has a known minimum value of zero. The negative og-likelihood can actually become negative when 2 is real-valued. 130  CHAPTER 5. MACHINE LEARNING BASICS  5.5.1 Conditional Log-Likelihood and Mean Squared Error  The maximum likelihood estimator can readily be generalized to estimate a condi- tional probability P( ;@) in order to predict y given x. This is actually the yx  https://www.deeplearningbook.org/contents/ml.html  most common situation because 1t forms the basis tor most supervised learning. It represents all our inputs and Y all our observed targets, then the conditional maximum likelihood estimator is  Oy, = arg max P(Y | X; 6)", "ce63ddf9-52d6-4622-8fc2-0ee92ed4b7f6": "input: batch size N, constant rT, structure of f, g, T. for sampled minibatch {a,}/\u2019_, do for all k \u20ac {1,...,N} do draw two augmentation functions t~T, t!~T # the first augmentation  Fox-1 = t(xp)  hop-1 = f(\u00ae2x-1) # representation Zor\u20141 = g(hox-1) # projection # the second augmentation Ea. = t' (x) hoa, = f (Xx) # representation Zo", "88ff02c0-3e32-4a5e-b0dc-ed1ed36887b5": "If we call the ?P\u00a5\u00b0P method to request the gradient with respect to  given that the gradient on the output is G, then the PPrOP method of the matrix multiplication operation must state that the gradient with respect to A  is given by GB\". Likewise, if we call the bprop method to request the gradient with respect to B, then the matrix operation is responsible for implementing the bprop method and specifying that the desired gradient is given by A'G. The back-propagation algorithm itself does not need to know any differentiation rules. It only needs to call each operation\u2019s bprop rules with the right arguments. Formally, op.bprop(inputs, X,G) must return  S- (Vxop.f(inputs),;) G;, (6.54)  a  which is just an implementation of the chain rule as expressed in equation 6.47.\n\nHere, inputs is a list of inputs that are supplied to the operation, op.f is the mathematical function that the operation implements, X is the input whose gradient we wish to compute, and G is the gradient on the output of the operation", "9afdaaf7-eb91-41ca-8a67-d927b8137836": "Note that this holds independently of the values of the \u03c0k so long as none of the \u03c0k is zero.\n\nThus, in this limit, we obtain a hard assignment of data points to clusters, just as in the K-means algorithm, so that \u03b3(znk) \u2192 rnk where rnk is de\ufb01ned by (9.2). Each data point is thereby assigned to the cluster having the closest mean. The EM re-estimation equation for the \u00b5k, given by (9.17), then reduces to the K-means result (9.4). Note that the re-estimation formula for the mixing coef\ufb01cients (9.22) simply re-sets the value of \u03c0k to be equal to the fraction of data points assigned to cluster k, although these parameters no longer play an active role in the algorithm. Finally, in the limit \u03f5 \u2192 0 the expected complete-data log likelihood, given by (9.40), becomes Exercise 9.11 Thus we see that in this limit, maximizing the expected complete-data log likelihood is equivalent to minimizing the distortion measure J for the K-means algorithm given by (9.1). Note that the K-means algorithm does not estimate the covariances of the clusters but only the cluster means", "d4e4fbe4-36a0-4452-b0ef-59a178ed3caf": "In reward-modulated STDP, changes in synapses in addition depend on a neuromodulator, such as dopamine, arriving within a time window that can last up to 10 seconds after the conditions for STDP are met. Evidence is accumulating that reward-modulated STDP occurs at corticostriatal synapses, where the actor\u2019s learning takes place in the hypothetical neural implementation of an actor\u2013critic system, adds to the plausibility of the hypothesis that something like an actor\u2013critic system exists in the brains of some animals. The idea of synaptic eligibility and basic features of the actor learning rule derive from Klopf\u2019s hypothesis of the \u201chedonistic neuron\u201d . He conjectured that individual neurons seek to obtain reward and to avoid punishment by adjusting the e\ufb03cacies of their synapses on the basis of rewarding or punishing consequences of their action potentials.\n\nA neuron\u2019s activity can a\u21b5ect its later input because the neuron is embedded in many feedback loops, some within the animal\u2019s nervous system and body and others passing through the animal\u2019s external environment", "eadef837-8de7-4c4e-9094-2ee211d7c82c": "Finally, there is o\u21b5-policy learning; can we give that up? On-policy methods are often adequate. For model-free reinforcement learning, one can simply use Sarsa rather than Q-learning. O\u21b5-policy methods free behavior from the target policy. This could be considered an appealing convenience but not a necessity. However, o\u21b5-policy learning is essential to other anticipated use cases, cases that we have not yet mentioned in this book but may be important to the larger goal of creating a powerful intelligent agent. In these use cases, the agent learns not just a single value function and single policy, but large numbers of them in parallel. There is extensive psychological evidence that people and animals learn to predict many di\u21b5erent sensory events, not just rewards. We can be surprised by unusual events, and correct our predictions about them, even if they are of neutral valence (neither good nor bad). This kind of prediction presumably underlies predictive models of the world such as are used in planning.\n\nWe predict what we will see after eye movements, how long it will take to walk home, the probability of making a jump shot in basketball, and the satisfaction we will get from taking on a new project", "c8f15a5f-ed65-4cb3-9466-9d81b4d264dc": "Write down the minimum misclassi\ufb01cation-rate decision rule assuming the two classes have equal prior probability.\n\nShow also that, if the kernel is chosen to be k(x, x\u2032) = xTx\u2032, then the classi\ufb01cation rule reduces to simply assigning a new input vector to the class having the closest mean. Finally, show that, if the kernel takes the form k(x, x\u2032) = \u03c6(x)T\u03c6(x\u2032), that the classi\ufb01cation is based on the closest mean in the feature space \u03c6(x). 7.3 (\u22c6 \u22c6) Show that, irrespective of the dimensionality of the data space, a data set consisting of just two data points, one from each class, is suf\ufb01cient to determine the location of the maximum-margin hyperplane. where {an} are given by maximizing (7.10) subject to the constraints (7.11) and (7.12). 7.5 (\u22c6 \u22c6) Show that the values of \u03c1 and {an} in the previous exercise also satisfy where \ufffdL(a) is de\ufb01ned by (7.10). Similarly, show that 7.6 (\u22c6) Consider the logistic regression model with a target variable t \u2208 {\u22121, 1}", "f9d06b52-3f76-4c75-a704-d6b44ce4ee3b": "Once the SamplePairing images are added to the training set, they run in cycles between 8:2 epochs, 8 with SamplePairing images, 2 without. Jaderberg et al. train exclusively with synthetic data for natural scene text recognition. The synthetic data produced the training data by enumerating through different fonts and augmentations. This produced sets of training images for size 50 k and 90 k lexicons.\n\nMikolajczyk and Grochowski  draw comparisons from transfer learning. They suggest that training on augmented data to learn the initial weights of a deep convolutional network is similar to transferring weights trained on other datasets such as ImageNet. These weights are then fine-tuned only with the original training data. Curriculum learning decisions are especially important for One-Shot Learning sys- tems such as FaceNet, presented by Schroff et al. It is important to find faces which are somewhat similar to the new face such that the learned distance function is actually useful. In this sense, the concept of curriculum learning shares many similarities with adversarial search algorithms or learning only on hard examples", "e306bddc-bfc0-4b3e-9b0f-ba0fcd46fa99": ", N, spaced uniformly in range , and the target data set t was obtained by \ufb01rst computing the corresponding values of the function sin(2\u03c0x) and then adding a small level of random noise having a Gaussian distribution (the Gaussian distribution is discussed in Section 1.2.4) to each such point in order to obtain the corresponding value tn. By generating data in this way, we are capturing a property of many real data sets, namely that they possess an underlying regularity, which we wish to learn, but that individual observations are corrupted by random noise. This noise might arise from intrinsically stochastic (i.e. random) processes such as radioactive decay but more typically is due to there being sources of variability that are themselves unobserved. Our goal is to exploit this training set in order to make predictions of the value \ufffdt of the target variable for some new value \ufffdx of the input variable. As we shall see later, this involves implicitly trying to discover the underlying function sin(2\u03c0x)", "e6258fbe-f5f2-4efb-9086-595b0e75fe21": "The graphical models research community is large and has developed many different models, training algorithms, and inference algorithms.\n\nIn this chapter, we provide basic background on some of the most central ideas of graphical models, with an emphasis on the concepts that have proved most useful to the deep learning research community. If you already have a strong background in graphical models,  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    you May wisn to SKlp Most OI this Chapter. However, even a graphical Model 4 555  CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  expert may benefit from reading the final section of this chapter, section 16.7, in which we highlight some of the unique ways in which graphical models are used for deep learning algorithms. Deep learning practitioners tend to use very different model structures, learning algorithms and inference procedures than are commonly used by the rest of the graphical models research community. In this chapter, we identify these differences in preferences and explain the reasons for them. We first describe the challenges of building large-scale probabilistic models. Next, we describe how to use a graph to describe the structure of a probability distribution", "b38dd58e-403a-452e-b9d2-70117e53ec24": "CONVOLUTIONAL NETWORKS  Figure 9.19: Many machine learning algorithms learn features that detect edges or specific colors of edges when applied to natural images.\n\nThese feature detectors are reminiscent of the Gabor functions known to be present in the primary visual cortex. (Left)Weights learned by an unsupervised learning algorithm (spike and slab sparse coding) applied to small image patches. (Right)Convolution kernels learned by the first layer of a fully supervised convolutional maxout network. Neighboring pairs of filters drive the same maxout unit. 9.11 Convolutional Networks and the History of Deep Learning  Convolutional networks have played an important role in the history of deep learning. They are a key example of a successful application of insights obtained by studying the brain to machine learning applications. They were also some of the first deep models to perform well, long before arbitrary deep models were considered viable. Convolutional networks were also some of the first neural networks to solve important commercial applications and remain at the forefront of commercial applications of deep learning today. For example, in the 1990s, the neural network research group at AT&T developed a convolutional network for reading checks", "f1d21b2e-e90a-4559-ab74-22159b4a5202": "A~! is primarily useful as a theoretical tool, however, and should not actually be used in practice for most software applications. Because A 7! can be represented with only limited precision on a digital computer, algorithms that make use of the value of b can usually obtain more accurate estimates of x. 2.4 Linear Dependence and Span  For A7! to exist, equation 2.11 must have exactly one solution for every value of b. It is also possible for the system of equations to have no solutions or infinitely many solutions for some values of b. It is not possible, however, to have more than one but less than infinitely many solutions for a particular 6; if both aw and y are  https://www.deeplearningbook.org/contents/linear_algebra.html    solutions, then z=ax+(l\u2014a)y (2.26)  is also a solution for any real a", "c2804b9c-6c90-4c83-a5e4-9e477ca26613": "L., Szepesvri, C. Convergence results for single-step on-policy reinforcement-learning algorithms. Machine Learning, 38(3):287\u2013308. aggregation. In Advances in Neural Information Processing Systems 7 , pp. 359\u2013 368. MIT Press, Cambridge, MA. Singh, S., Lewis, R. L., Barto, A. G. Where do rewards come from? In N. Taatgen and H. van Rijn (Eds. ), Proceedings of the 31st Annual Conference of the Cognitive Science Society, pp. 2601\u20132606. Cognitive Science Society. Singh, S., Lewis, R. L., Barto, A. G., Sorg, J. Intrinsically motivated reinforcement learning: An evolutionary perspective. IEEE Transactions on Autonomous Mental Development, 2(2):70\u201382. Special issue on Active Learning and Intrinsically Motivated Exploration in Robots: Advances and Challenges. Skinner, B", "78800acc-9e5f-4768-a005-7fa7d7745c8e": "In many follow-up works, contrastive loss incorporating multiple negative samples is also broadly referred to as NCE. InfoNCE  The InfoNCE loss in CPC , inspired by NCE, uses categorical cross-entropy loss to identify the positive sample amongst a set of unrelated noise samples.\n\nGiven a context vector c, the positive sample should be drawn from the conditional distribution p(x|c), while VV \u2014 1 negative samples are drawn from the proposal distribution p(x), independent from the context c. For brevity, let us label all the samples as X = {x;}%, among which only one of them Xpos is a positive sample", "c22ea91f-84c9-44f5-a834-3f913e165004": "https://www.deeplearningbook.org/contents/generative_models.html    Figure 20.9: A neural auto-regressive network predicts the i-th variable x; from the i \u2014 1 previous ones, but is parametrized so that features (groups of hidden units denoted h; ) that are functions of x1,...,2; can be reused in predicting all the subsequent variables Vit, Vi42,..-,Xa. 704  CHAPTER 20. DEEP GENERATIVE MODELS  Each P(x; | %j-1,...,21) can represent a conditional distribution by having outputs of the neural network predict parameters of the conditional distribution of 2;, as discussed in section 6.2.1.1. Although the original neural auto-regressive networks were initially evaluated in the context of purely discrete multivariate data (with a sigmoid output for a Bernoulli variable or softmax output for a multinoulli variable), it is natural to extend such models to continuous variables or joint distributions involving both discrete and continuous variables. 20.10.10 NADE  The neural auto-regressive density estimator (NADE) is a very successful recent form of neural auto-regressive network", "b94509bf-7076-4ae4-a4fe-e1a9a1c6bd3b": "Note that the left-hand mode of the class-conditional density p(x|C1), shown in blue on the left plot, has no effect on the posterior probabilities. The vertical green line in the right plot shows the decision boundary in x that gives the minimum misclassi\ufb01cation rate. be of low accuracy, which is known as outlier detection or novelty detection . However, if we only wish to make classi\ufb01cation decisions, then it can be wasteful of computational resources, and excessively demanding of data, to \ufb01nd the joint distribution p(x, Ck) when in fact we only really need the posterior probabilities p(Ck|x), which can be obtained directly through approach (b). Indeed, the classconditional densities may contain a lot of structure that has little effect on the posterior probabilities, as illustrated in Figure 1.27. There has been much interest in exploring the relative merits of generative and discriminative approaches to machine learning, and in \ufb01nding ways to combine them", "aabb213d-b9bc-47e3-9c64-0645dcf9bfe6": "Similarly, the corresponding planning algorithms also have no \u03b3.\n\nFor example, the value iteration algorithm with options, analogous to (4.10), is If \u2326(s) includes all the low-level actions available in each s, then this algorithm converges to the conventional v\u21e4, from which the optimal policy can be computed. However, it is particularly useful to plan with options when only a subset of the possible options are considered (in \u2326(s)) in each state. Value iteration will then converge to the best hierarchical policy limited to the restricted set of options. Although this policy may be sub-optimal, convergence can be much faster because fewer options are considered and because each option can jump over many time steps. To plan with options, one must either be given the option models, or learn them. One natural way to learn an option model is to formulate it as a collection of GVFs (as de\ufb01ned in the preceding section) and then learn the GVFs using the methods presented in this book. It is not di\ufb03cult to see how this could be done for the reward part of the option model", "5a9e6d06-0e04-42ca-98c0-daa1d0fbae11": "In many ways these categories respectively correspond to categories of learning extensively studied by psychologists: classical, or Pavlovian, conditioning and instrumental, or operant, conditioning. These correspondences are not completely accidental because of psychology\u2019s in\ufb02uence on reinforcement learning, but they are nevertheless striking because they connect ideas arising from di\u21b5erent objectives. The prediction algorithms presented in this book estimate quantities that depend on how features of an agent\u2019s environment are expected to unfold over the future.\n\nWe speci\ufb01cally focus on estimating the amount of reward an agent can expect to receive over the future while it interacts with its environment. In this role, prediction algorithms are policy evaluation algorithms, which are integral components of algorithms for improving policies. But prediction algorithms are not limited to predicting future reward; they can predict any feature of the environment . The correspondence between prediction algorithms and classical conditioning rests on their common property of predicting upcoming stimuli, whether or not those stimuli are rewarding (or punishing). The situation in an instrumental, or operant, conditioning experiment is di\u21b5erent. Here, the experimental apparatus is set up so that an animal is given something it likes (a reward) or something it dislikes (a penalty) depending on what the animal did", "994c00da-17b8-477d-b9f5-cda00af307d7": "However, imagine using color aug- mentations exclusively. If the initial training dataset consists of 50 dogs and 50 cats, and each image is augmented with 100 color filters to produce 5000 dogs and 5000 cats, this dataset will be heavily biased towards the spatial characteristics of the original 50 dogs and 50 cats. This over-extensive color-augmented data will cause a deep model to overfit even worse than the original. From this anecdote, we can conceptualize the existence of an optimal size for post-augmented data. Additionally, there is no consensus about the best strategy for combining data warping and oversampling techniques. One important consideration is the intrinsic bias in the initial, limited dataset.\n\nThere are no existing augmentation techniques that can correct a dataset that has very poor diversity with respect to the testing data. All these augmenta- tion algorithms perform best under the assumption that the training data and testing data are both drawn from the same distribution. If this is not true, it is very unlikely that  these methods will be useful", "f694f9a3-b3cd-4f95-b1c0-4da74e044723": "In particular, squared error is a poor loss function for softmax units and can fail to train the model to change its output,  even when the model makes highly confident incorrect predictions . To understand why these other loss functions can fail, we need to examine the softmax function itself. Like the sigmoid, the softmax activation can saturate. The sigmoid function has a single output that saturates when its input is extremely negative or extremely positive. The softmax has multiple output values.\n\nThese output values can saturate when the differences between input values become extreme. When the softmax saturates, many cost functions based on the softmax also saturate, unless they are able to invert the saturating activating function. To see that the softmax function responds to the difference between its inputs, observe that the softmax output is invariant to adding the same scalar to all its  inputs: softmax(z) = softmax(z + c). (6.32) Using this property, we can derive a numerically stable variant of the softmax: softmax(z) = softmax(z \u2014 max z;)", "c792c5c0-f3f7-43e8-a9f1-da83a286ce55": "For any \ufb01xed policy \u21e1, TD(0) has been proved to converge to v\u21e1, in the mean for a constant step-size parameter if it is su\ufb03ciently small, and with probability 1 if the step-size parameter decreases according to the usual stochastic approximation conditions (2.7). Most convergence proofs apply only to the table-based case of the algorithm presented above (6.2), but some also apply to the case of general linear function approximation. These results are discussed in a more general setting in Section 9.4. If both TD and Monte Carlo methods converge asymptotically to the correct predictions, then a natural next question is \u201cWhich gets there \ufb01rst?\u201d In other words, which method learns faster? Which makes the more e\ufb03cient use of limited data? At the current time this is an open question in the sense that no one has been able to prove mathematically that one method converges faster than the other. In fact, it is not even clear what is the most appropriate formal way to phrase this question!\n\nIn practice, however, TD methods have usually been found to converge faster than constant-\u21b5 MC methods on stochastic tasks, as illustrated in Example 6.2", "dbb7c6e0-62bc-4951-92d9-9cc7fcaeab88": "Now\u00b4e, Vrancx, and De Hauwere  reviewed more recent developments in the wider \ufb01eld of multi-agent reinforcement learning 15.11 Yin and Knowlton  reviewed \ufb01ndings from outcome-devaluation experiments with rodents supporting the view that habitual and goal-directed behavior (as psychologists use the phrase) are respectively most associated with processing in the dorsolateral striatum (DLS) and the dorsomedial striatum (DMS).\n\nResults of functional imaging experiments with human subjects in the outcomedevaluation setting by Valentin, Dickinson, and O\u2019Doherty  suggest that the orbitofrontal cortex (OFC) is an important component of goal-directed choice. Single unit recordings in monkeys by Padoa-Schioppa and Assad  support the role of the OFC in encoding values guiding choice behavior. Rangel, Camerer, and Montague  and Rangel and Hare  reviewed \ufb01ndings from the perspective of neuroeconomics about how the brain makes goal-directed decisions. Pezzulo, van der Meer, Lansink, and Pennartz  reviewed the neuroscience of internally generated sequences and presented a model of how these mechanisms might be components of model-based planning", "f9eacb38-a759-499a-a30a-d8b0778bf6f3": "One of prioritized sweeping\u2019s limitations is that it uses expected updates, which in stochastic environments may waste lots of computation on low-probability transitions.\n\nAs we show in the following section, sample updates right is shown the obstacles and the shortest solution from start to goal, found by prioritized sweeping. This problem is deterministic, but has four actions and 14,400 potential states (some of these are unreachable because of the obstacles). This problem is probably too large to be solved with unprioritized methods. Figure reprinted from Moore and Atkeson . can in many cases get closer to the true value function with less computation despite the variance introduced by sampling. Sample updates can win because they break the overall backing-up computation into smaller pieces\u2014those corresponding to individual transitions\u2014which then enables it to be focused more narrowly on the pieces that will have the largest impact. This idea was taken to what may be its logical limit in the \u201csmall backups\u201d introduced by van Seijen and Sutton . These are updates along a single transition, like a sample update, but based on the probability of the transition without sampling, as in an expected update", "34553c65-1862-4afb-a57c-cb84168073e2": "Smart Augmentation The Smart Augmentation  approach utilizes a similar con- cept as the Neural Augmentation technique presented above. However, the combina- tion of images is derived exclusively from the learned parameters of a prepended CNN, rather than using the Neural Style Transfer algorithm. Smart Augmentation is another approach to meta-learning augmentations. This is done by having two networks, Network-A and Network-B. Network-A is an augmen- tation network that takes in two or more input images and maps them into a new  image or images to train Network-B. The change in the error rate in Network-B is then Shorten and Khoshgoftaar J Big Data  6:60   Table 7 Results comparing augmentations   Quantitative results on dogs vs. goldfish  Dogs vs goldfish  Augmentation Val. acc.\n\nNone 0.855 Traditional 0.890 GANs 0.865 Neural + no loss 0.915 Neural + content loss 0.900 Neural + style 0.890 Control 0.840  Quantitative results on dogs vs cats  Dogs vs cat  Augmentation Val. acc", "1cc55bc8-2770-4193-bf49-f4047b345ac2": "A Lipschitz continuous function is a function f whose rate of change is bounded by a Lipschitz constant CL:  Va,Vy,|f(@) \u2014 f(y)| < L\\|a\u2014 ylle- (4.13)  This property is useful because it enables us to quantify our assumption that a small change in the input made by an algorithm such as gradient descent will have  90  https://www.deeplearningbook.org/contents/numerical.html    CHAPTER 4.\n\nNUMERICAL COMPUTATION  a small change in the output. Lipschitz continuity is also a fairly weak constraint, and many optimization problems in deep learning can be made Lipschitz continuous with relatively minor modifications. Perhaps the most successful field of specialized optimization is convex op- timization. Convex optimization algorithms are able to provide many more guarantees by making stronger restrictions. These algorithms are applicable only to convex functions\u2014functions for which the Hessian is positive semidefinite ev- erywhere. Such functions are well-behaved because they lack saddle points, and all their local minima are necessarily global minima. However, most problems in deep learning are difficult to express in terms of convex optimization", "4a9576ab-bd80-42a5-8128-de98f3f30fff": "We turn now to the second major class of graphical models that are described by undirected graphs and that again specify both a factorization and a set of conditional independence relations. A Markov random \ufb01eld, also known as a Markov network or an undirected graphical model , has a set of nodes each of which corresponds to a variable or group of variables, as well as a set of links each of which connects a pair of nodes. The links are undirected, that is they do not carry arrows. In the case of undirected graphs, it is convenient to begin with a discussion of conditional independence properties. In the case of directed graphs, we saw that it was possible to test whether a parSection 8.2 ticular conditional independence property holds by applying a graphical test called d-separation. This involved testing whether or not the paths connecting two sets of nodes were \u2018blocked\u2019. The de\ufb01nition of blocked, however, was somewhat subtle due to the presence of paths having head-to-head nodes", "f83c8fa0-08c1-4cb2-b95c-417b675216cf": "1 1 1 a wo soo  https://www.deeplearningbook.org/contents/applications.html    1oOKup.\n\n1 OlMer Words, IU Call De viewed as a local NOUparamevric preaicvor, sumuar to k-nearest neighbors. The statistical problems facing these extremely local pre- dictors are described in section 5.11.2. 'The problem for a language model is even more severe than usual, because any two different words have the same distance  from each other in one-hot vector space. It is thus difficult to leverage much information from any \u201cneighbors\u2019\u2014only training examples that repeat literally the same context are useful for local generalization. To overcome these problems, a language model must be able to share knowledge between one word and other semantically similar words. To improve the statistical efficiency of n-gram models, class-based language models  introduce the notion of word categories and then share statistical strength between words that are in the same category. The idea is to use a clustering algorithm to partition the set of words into clusters or classes, based on their co-occurrence frequencies with other words", "20019da6-4a91-4f06-a844-131a6f5b3bff": ", |T|, with leaf node \u03c4 representing a region R\u03c4 of input space having N\u03c4 data points, and |T| denoting the total number of leaf nodes.\n\nThe optimal prediction for region R\u03c4 is then given by and the corresponding contribution to the residual sum-of-squares is then The pruning criterion is then given by The regularization parameter \u03bb determines the trade-off between the overall residual sum-of-squares error and the complexity of the model as measured by the number |T| of leaf nodes, and its value is chosen by cross-validation. For classi\ufb01cation problems, the process of growing and pruning the tree is similar, except that the sum-of-squares error is replaced by a more appropriate measure of performance. If we de\ufb01ne p\u03c4k to be the proportion of data points in region R\u03c4 assigned to class k, where k = 1, . , K, then two commonly used choices are the cross-entropy These both vanish for p\u03c4k = 0 and p\u03c4k = 1 and have a maximum at p\u03c4k = 0.5. They encourage the formation of regions in which a high proportion of the data points are assigned to one class", "6a2f4b26-53fa-433f-b30c-9541cea22ddb": "The dashed lines in Fig. 9 show that as \u03f5 decreases, the number of selected correlations follows a pattern. Generally, the number of correlations grows slowly at \ufb01rst, then hits an \u201celbow point\u201d beyond which the number explodes, which \ufb01ts the assumption that the correlation structure is sparse. In all three cases, setting \u03f5 to this elbow point is a safe trade-off between predictive performance and computational cost. In cases where performance grows consistently (left and right), the elbow point achieves most of the predictive performance gains at a small fraction of the computational cost. For example,onSpouses(right),choosing\u03f5 = 0.08achievesascoreof 56.6 F1\u2014within one point of the best score\u2014but only takes 8 min for parameter estimation. In cases where predictive performance eventually degrades (middle), the elbow point also selects a relatively small number of correlations, giving an 0.7 F1 point improvement and avoiding over\ufb01tting", "961a190f-92f4-4e2f-ab7a-9c7ea38baffd": "We can express any probability distribution of the form p(y; 0) or p( ;0)  https://www.deeplearningbook.org/contents/generative_models.html    as Ply |W), Where W 1s a varlable contaiming both parameters 9, and 11 applicanle, the inputs #. Given a value y sampled from distribution p(y | w), where W may in turn be a function of other variables, we can rewrite  y ~ vly | w) (20.56)  as y = f(z), (20.57)  where z is a source of randomness. We may then compute the derivatives of y with respect to w using traditional tools such as the back-propagation algorithm applied to f, as long as f is continuous and differentiable almost everywhere. Crucially, w must not be a function of z, and z must not be a function of w. This technique is often called the reparametrization trick, stochastic back-propagation, or perturbation analysis. The requirement that f be continuous and differentiable of course requires y to be continuous", "2a9d4901-f201-428f-87f0-d9fc68e7b55e": "\"\".Ioos, which \"'l'fesoots!he s.um-ol\u00b7SQ\"\",es distortlon J i<*~ by projecti<Xl the data onto a p<incipal componenl slll>spaee '\" dimensionalitv M. FIIIUr. 1:1:.5 An \",>gi\",,1 ~mpIe Irom lI>e 011\u00b7_ digils data ...ttOll\"1her with its PeA re<:onstnxlions oblair...:! by 'e1aio\"li!Xl ,If j)<incipal ~n1S 10< various val,,\" 01 ,If. As ,II increason !tie re<:onst,uctiOfI ~s more ao::urate and woukl ~ portee! when .-If K D ~ 28 x 28 ~ . \"-1", "42eaf789-d459-4b13-bf52-09a569316a72": "Gaussians, we \ufb01rst chose one of the components at random with probability given by the mixing coef\ufb01cients \u03c0k and then generate a sample vector x from the corresponding Gaussian component.\n\nThis process is repeated N times to generate a data set of N independent samples. In the case of the hidden Markov model, this procedure is modi\ufb01ed as follows. We \ufb01rst choose the initial latent variable z1 with probabilities governed by the parameters \u03c0k and then sample the corresponding observation x1. Now we choose the state of the variable z2 according to the transition probabilities p(z2|z1) using the already instantiated value of z1. Thus suppose that the sample for z1 corresponds to state j. Then we choose the state k of z2 with probabilities Ajk for k = 1, . , K. Once we know z2 we can draw a sample for x2 and also sample the next latent variable z3 and so on. This is an example of ancestral sampling for a directed graphical model", "1668a9f2-5d7e-4cca-9db4-5796bfd48773": "We can then interpret the average over these one-hot codes as giving a probability distribution over classes. As a nonparametric learning algorithm, k-nearest neighbor can achieve very high capacity.\n\nFor example, suppose we have a multiclass classification task and measure performance with 0-1 loss. In this setting, 1-nearest neighbor converges to double the Bayes error as the number of training examples approaches infinity. The error in excess of the Bayes error results from choosing a single neighbor by breaking ties between equally distant neighbors randomly. When there is infinite training data, all test points x will have infinitely many training set neighbors at distance zero. If we allow the algorithm to use all these neighbors to vote, rather than randomly choosing one of them, the procedure converges to the Bayes error rate. The high capacity of k-nearest neighbors enables it to obtain high accuracy given a large training set. It does so at high computational cost, however, and it may generalize very badly given a small finite training set. One weakness of k-nearest neighbors is that it cannot learn that one feature is more discriminative than another", "1e8e90d7-f261-4ee3-bd82-e9250ec61535": "Whereas the tile coding input was derived from the contents of the transaction queue, the constraint sets depended on a host of other features related to timing and resource constraints that had to be satis\ufb01ed by the hardware implementation of the entire system. In this way, the action constraints ensured that the learning algorithm\u2019s exploration could not endanger the integrity of the physical system, while learning was e\u21b5ectively limited to a \u201csafe\u201d region of the much larger state space of the hardware implementation. Because an objective of this work was that the learning controller could be implemented on a chip so that learning could occur online while a computer is running, hardware implementation details were important considerations.\n\nThe design included two \ufb01ve-stage pipelines to calculate and compare two action values at every processor clock cycle, and to update the appropriate action value. This included accessing the tile coding which was stored on-chip in static RAM. For the con\ufb01guration \u02d9Ipek et al. simulated, which was a 4GHz 4-core chip typical of high-end workstations at the time of their research, there were 10 processor cycles for every DRAM cycle. Considering the cycles needed to \ufb01ll the pipes, up to 12 actions could be evaluated in each DRAM cycle", "81e1c4fe-8ba5-445e-9d2a-9ce92c3bd21e": "We develop them \ufb01rst for the on-policy case then extend them to o\u21b5-policy learning.\n\nOur treatment pays special attention to the case of linear function approximation, for which the results with eligibility traces are stronger. All these results apply also to the tabular and state aggregation cases because these are special cases of linear function approximation. In Chapter 7 we de\ufb01ned an n-step return as the sum of the \ufb01rst n rewards plus the estimated value of the state reached in n steps, each appropriately discounted (7.1). The general form of that equation, for any parameterized function approximator, is Gt:t+n .= Rt+1 +\u03b3Rt+2 +\u00b7 \u00b7 \u00b7+\u03b3n\u22121Rt+n +\u03b3n\u02c6v(St+n,wt+n\u22121), 0 \uf8ff t \uf8ff T \u2212n, (12.1) where \u02c6v(s,w) is the approximate value of state s given weight vector w (Chapter 9), and T is the time of episode termination, if any", "c65f1f15-ed59-43ef-bced-af39234420e8": "shows that increasing the number of parameters in layers of convolutional networks without increasing their depth is not nearly as effective at increasing test set performance, as illustrated in this figure. The legend indicates the depth of network used to make each curve and whether the curve represents variation in the size of the convolutional or the fully connected layers. We observe that shallow models in this context overfit at around 20 million parameters while deep ones can benefit from having over 60 million. This suggests that using a deep model expresses a useful preference over the space of functions the model can learn. Specifically, it expresses a belief that the function should consist of many simpler functions composed together. This could result either in learning a representation that is composed in turn of simpler representations (e.g., corners defined in terms of edges) or in learning a program with sequentially dependent steps (e.g., first locate a set of objects, then segment them from each other, then recognize them)", "382de4c2-a448-4b98-b39c-a86e87b5aa60": "This multivariate probability density must satisfy in which the integral is taken over the whole of x space. We can also consider joint probability distributions over a combination of discrete and continuous variables. Note that if x is a discrete variable, then p(x) is sometimes called a probability mass function because it can be regarded as a set of \u2018probability masses\u2019 concentrated at the allowed values of x. The sum and product rules of probability, as well as Bayes\u2019 theorem, apply equally to the case of probability densities, or to combinations of discrete and continuous variables.\n\nFor instance, if x and y are two real variables, then the sum and product rules take the form A formal justi\ufb01cation of the sum and product rules for continuous variables  requires a branch of mathematics called measure theory and lies outside the scope of this book. Its validity can be seen informally, however, by dividing each real variable into intervals of width \u2206 and considering the discrete probability distribution over these intervals. Taking the limit \u2206 \u2192 0 then turns sums into integrals and gives the desired result. One of the most important operations involving probabilities is that of \ufb01nding weighted averages of functions", "10372319-1b1b-48a7-86a2-3492dd840ea6": "If color space transforms repeatedly change the color space such that the model cannot recognize red blood from green paint, the model will perform poorly on Image Sentiment Analysis. In effect, color space transformations will eliminate color biases present in the dataset in favor of spa- tial characteristics. However, for some tasks, color is a very important distinctive  feature.\n\nShorten and Khoshgoftaar J Big Data  6:60   Table 1 Results of Taylor and Nitschke\u2019s Data Augmentation experiments on Caltech101   Top-1 accuracy (%) Top-5 accuracy (%)  Baseline 48.134042 64.50+0.65 Flipping 49.7341.13 67.36 + 138 Rotating 50.80 + 0.63 69.414048 Cropping 61.95+ 1.01 79.104 0.80 Color Jittering 49.57 \u00a30.53 67.1840.42 Edge Enhancement 49.29 + 1.16 66.49 + 0.84 Fancy PCA 49.41 40.84 67.544 1.01  Their results find that the cropping geometric transformation results in the most accurate classifier The italic value denote high performance according to the comparative metrics  Geometric versus photometric transformations  Taylor and Nitschke  provide a comparative study on the effectiveness of geometric and photometric (color space) transformations", "f543a538-3ddf-4dac-a786-8fa335cd0808": "The resulting value F(\u03be\u22c6) represents the tightest bound within this family of bounds and can be used as an approximation to I. This optimized bound, however, will in general not be exact. Although the bound \u03c3(a) \u2a7e f(a, \u03be) on the logistic sigmoid can be optimized exactly, the required choice for \u03be depends on the value of a, so that the bound is exact for one value of a only. Because the quantity F(\u03be) is obtained by integrating over all values of a, the value of \u03be\u22c6 represents a compromise, weighted by the distribution p(a). We now illustrate the use of local variational methods by returning to the Bayesian logistic regression model studied in Section 4.5. There we focussed on the use of the Laplace approximation, while here we consider a variational treatment based on the approach of Jaakkola and Jordan . Like the Laplace method, this also leads to a Gaussian approximation to the posterior distribution", "097bd2cf-88ff-4a55-825f-3ee86dcbe701": "The other is to develop true gradient methods that do not rely on any special distribution for stability. We present methods based on both approaches. This is a cutting-edge research area, and it is not clear which of these approaches is most e\u21b5ective in practice. We begin by describing how the methods developed in earlier chapters for the o\u21b5policy case extend readily to function approximation as semi-gradient methods. These methods address the \ufb01rst part of the challenge of o\u21b5-policy learning (changing the update targets) but not the second part (changing the update distribution). Accordingly, these methods may diverge in some cases, and in that sense are not sound, but still they are often successfully used. Remember that these methods are guaranteed stable and asymptotically unbiased for the tabular case, which corresponds to a special case of function approximation. So it may still be possible to combine them with feature selection methods in such a way that the combined system could be assured stable. In any event, these methods are simple and thus a good place to start", "db8a3af2-5e28-4b54-b535-4a4687b61fbd": "In an artificial neural network, we can just display an image of the convolution kernel to see what the corresponding channel of a convolutional layer responds to. In a biological neural network, we do not have access to the weights themselves. Instead, we put an electrode in the neuron, display several samples of white noise images in front of the animal\u2019s retina, and record how each of these samples causes the neuron to activate. We can then fit a linear model to these responses to obtain an approximation of the neuron\u2019s weights. This approach is known as reverse correlation . Reverse correlation shows us that most V1 cells have weights that are described by Gabor functions. The Gabor function describes the weight at a 2-D point in the image. We can think of an image as being a function of 2-D coordinates, I(x,y). Likewise, we can think of a simple cell as sampling the image at a set of locations, defined by a set of x coordinates X and a set of y coordinates Y, then applying weights that are also a function of the location, w(z,y)", "d5720a9f-77cd-4b5d-b4d4-30cadede7c98": "TD errors like (6.5) are special kinds of RPEs that signal discrepancies between current and earlier expectations of reward over the long-term. When neuroscientists refer to RPEs they generally (though not always) mean TD RPEs, which we simply call TD errors throughout this chapter. Also in this chapter, a TD error is generally one that does not depend on actions, as opposed to TD errors used in learning action-values by algorithms like Sarsa and Q-learning. This is because the most well-known links to neuroscience are stated in terms of action-free TD errors, but we do not mean to rule out possible similar links involving action-dependent TD errors. (TD errors for predicting signals other than rewards are useful too, but that case will not concern us here. See, for example, Modayil, White, and Sutton, 2014.) One can ask many questions about links between neuroscience data and these theoreticallyde\ufb01ned signals", "024f51c3-9e3b-47f2-b108-014f3b638012": "500 epochs 14) to obtain the optimal result, while ResNet-50 (4\u00d7) does not bene\ufb01t from longer training. B.4. Understanding The Non-Linear Projection Head 13It is 80.1% top-1 / 95.2% top-5 without broader augmentations for pretraining SimCLR. 14With AutoAugment , optimal test accuracy can be achieved between 900 and 500 epochs. A Simple Framework for Contrastive Learning of Visual Representations B.5. Semi-supervised Learning via Fine-Tuning Fine-tuning Procedure We \ufb01ne-tune using the Nesterov momentum optimizer with a batch size of 4096, momentum of 0.9, and a learning rate of 0.8 (following LearningRate = 0.05\u00d7BatchSize/256) without warmup. Only random cropping (with random left-to-right \ufb02ipping and resizing to 224x224) is used for preprocessing. We do not use any regularization (including weight decay). For 1% labeled data we \ufb01ne-tune for 60 epochs, and for 10% labeled data we \ufb01ne-tune for 30 epochs", "7e3f3dc6-5c3c-403a-8879-3f1876f4d04b": "Di\u21b5erential semi-gradient n-step Sarsa for estimating \u02c6q \u21e1 q\u21e1 or q\u21e4 Input: a di\u21b5erentiable function \u02c6q : S \u21e5 A \u21e5 Rd ! R, a policy \u21e1 Initialize value-function weights w 2 Rd arbitrarily (e.g., w = 0) Initialize average-reward estimate \u00afR 2 R arbitrarily (e.g., \u00afR = 0) Algorithm parameters: step size \u21b5, \u03b2 > 0, a positive integer n All store and access operations (St, At, and Rt) can take their index mod n + 1 parameter on the average reward, \u03b2, needs to be quite small so that \u00afR becomes a good long-term estimate of the average reward. Unfortunately, \u00afR will then be biased by its initial value for many steps, which may make learning ine\ufb03cient.\n\nAlternatively, one could use a sample average of the observed rewards for \u00afR. That would initially adapt rapidly but in the long run would also adapt slowly. As the policy slowly changed, \u00afR would also change; the potential for such long-term nonstationarity makes sample-average methods ill-suited", "2e19b7e2-a867-4bbb-98eb-c0485cfe7666": "Policy Evaluation Policy Evaluation is to compute the state-value V_ for a given policy 7:  https://lilianweng.github.io/posts/2018-02-19-rl-overview/  A (Long) Peek into Reinforcement Learning | Lil'Log   Visa(s) = Eg = $0 a(a|s) $0 P(s',r|s,a)(r + 9Vi(s'))  a 3! Policy Improvement  Based on the value functions, Policy Improvement generates a better policy 7\u2019 > 7 by acting greedily. Q,(8, a) = eRe + W-(St41) St = 8,Ar= a] = S> P(s',r\\s, a)(r + 1V,(8'))  3! Policy Iteration  The Generalized Policy Iteration (GPI) algorithm refers to an iterative procedure to improve the policy when combining policy evaluation and improvement. evaluation improve evaluation improve evaluation improve evaluation  Tr 70 > Ty > Vi, > 1, dee > Ts V,  In GPI, the value function is approximated repeatedly to be closer to the true value of the current policy and in the meantime, the policy is improved repeatedly to approach optimality", "c9ab1b84-d5bb-4585-bb9e-ed6d1b653257": "For the given value of slope \u03bb the contact point of the tangent line having the same slope is found by minimizing with respect to x the discrepancy (shown by the green dashed lines) given by f(x) \u2212 \u03bbx. This de\ufb01nes the dual function g(\u03bb), which corresponds to the (negative of the) intercept of the tangent line having slope \u03bb. exp(\u2212x), we therefore obtain the tangent line in the form which is a linear function parameterized by \u03be. For consistency with subsequent discussion, let us de\ufb01ne \u03bb = \u2212 exp(\u2212\u03be) so that Different values of \u03bb correspond to different tangent lines, and because all such lines are lower bounds on the function, we have f(x) \u2a7e y(x, \u03bb). Thus we can write the function in the form We have succeeded in approximating the convex function f(x) by a simpler, linear function y(x, \u03bb).\n\nThe price we have paid is that we have introduced a variational parameter \u03bb, and to obtain the tightest bound we must optimize with respect to \u03bb. We can formulate this approach more generally using the framework of convex duality", "dec4ca5f-8acb-4882-b24d-d89fc4eb4588": "Since the classifier should be invariant to the local factors of variation that correspond to movement on the manifold, it would make sense to use as nearest neighbor distance between points a, and a the distance between the manifolds M, and M) to which they respectively belong.\n\nAlthough that may be computationally difficult (it would require solving an optimization problem, to find the nearest pair of points on M, and Mp), a cheap alternative that makes sense locally is to approximate M; by its tangent plane at a; and measure the distance between the two tangents, or between  a banned Hla 2d 2 Ate MLA Ane Le 2 Ane dd Lee net 2 1a At tnd  https://www.deeplearningbook.org/contents/regularization.html    @ LALBCUL Plalle allLU a PUILLLL. Liab Call VC ACLUICVEU Vy SUILVILL @ 1OW-ULLEMSIONAL linear system (in the dimension of the manifolds). Of course, this algorithm requires one to specify the tangent vectors", "0e5fbd15-36ca-49e4-a028-d4ee2f30cbc1": "Furthermore, the network must also exhibit invariance to more subtle transformations such as elastic deformations of the kind illustrated in Figure 5.14. One simple approach would be to treat the image as the input to a fully connected network, such as the kind shown in Figure 5.1. Given a suf\ufb01ciently large training set, such a network could in principle yield a good solution to this problem and would learn the appropriate invariances by example.\n\nHowever, this approach ignores a key property of images, which is that nearby pixels are more strongly correlated than more distant pixels. Many of the modern approaches to computer vision exploit this property by extracting local features that depend only on small subregions of the image. Information from such features can then be merged in later stages of processing in order to detect higher-order features and ultimately to yield information about the image as whole. Also, local features that are useful in one region of the image are likely to be useful in other regions of the image, for instance if the object of interest is translated. These notions are incorporated into convolutional neural networks through three mechanisms: (i) local receptive \ufb01elds, (ii) weight sharing, and (iii) subsampling. The structure of a convolutional network is illustrated in Figure 5.17", "9c6c2809-386d-407e-86ef-a05471a00912": "The usual approach to problems of this kind is to put a grid over the area covered by the surface and solve for its height at the grid points by an iterative computation. Grid points at the boundary are forced to the wire frame, and all others are adjusted toward the average of the heights of their four nearest neighbors. This process then iterates, much like DP\u2019s iterative policy evaluation, and ultimately converges to a close approximation to the desired surface. This is similar to the kind of problem for which Monte Carlo methods were originally designed. Instead of the iterative computation described above, imagine standing on the surface and taking a random walk, stepping randomly from grid point to neighboring grid point, with equal probability, until you reach the boundary. It turns out that the expected value of the height at the boundary is a close approximation to the height of the desired surface at the starting point (in fact, it is exactly the value computed by the iterative method described above).\n\nThus, one can closely approximate the height of the surface at a point by simply averaging the boundary heights of many walks started at the point", "387465a2-536c-453d-87a0-87ddfa92baa3": "The gradient and the Hessian for the vector wk are given by Section 4.3.3 where \u2207k denotes the gradient with respect to wk.\n\nFor \ufb01xed \u03b3nk, these are independent of {wj} for j \u0338= k and so we can solve for each wk separately using the IRLS algorithm. Thus the M-step equations for component k correspond simply to \ufb01tting Section 4.3.3 a single logistic regression model to a weighted data set in which data point n carries a weight \u03b3nk. Figure 14.10 shows an example of the mixture of logistic regression models applied to a simple classi\ufb01cation problem. The extension of this model to a mixture of softmax models for more than two classes is straightforward. Exercise 14.16 In Section 14.5.1, we considered a mixture of linear regression models, and in Section 14.5.2 we discussed the analogous mixture of linear classi\ufb01ers. Although these simple mixtures extend the \ufb02exibility of linear models to include more complex (e.g., multimodal) predictive distributions, they are still very limited", "c12177f8-66ec-4990-b09c-db307ed99471": "An early important experiment of this type was conducted by Adams and Dickinson . They trained rats via instrumental conditioning until the rats energetically pressed a lever for sucrose pellets in a training chamber. The rats were then placed in the same chamber with the lever retracted and allowed non-contingent food, meaning that pellets were made available to them independently of their actions. After 15-minutes of this free-access to the pellets, rats in one group were injected with the nausea-inducing poison lithium chloride. This was repeated for three sessions, in the last of which none of the injected rats consumed any of the non-contingent pellets, indicating that the reward value of the pellets had been decreased\u2014the pellets had been devalued. In the next stage taking place a day later, the rats were again placed in the chamber and given a session of extinction training, meaning that the response lever was back in place but disconnected from the pellet dispenser so that pressing it did not release pellets.\n\nThe question was whether the rats that had the reward value of the pellets decreased would lever-press less than rats that did not have the reward value of the pellets decreased, even without experiencing the devalued reward as a result of lever-pressing", "f1a429e7-8a0b-448e-94d8-efba1f8242a3": "We can think of the penalty Q(h) simply as a regularizer term added to a feedforward network whose primary task is to copy the input to the output (unsupervised learning objective) and possibly also perform some supervised task (with a supervised learning objective) that depends on these sparse features. Unlike other regularizers, such as weight decay, there is not a straightforward Bayesian interpretation to this regularizer.\n\nAs described in section 5.6.1, training with weight decay and other regularization penalties can be interpreted as a MAP approximation to Bayesian inference, with the added regularizing penalty  https://www.deeplearningbook.org/contents/autoencoders.html    corresponding to a prior probability distribution over the model parameters, In his view, regularized maximum likelihood corresponds to maximizing P x), which is equivalent to maximizing log p(x | 8) + logp(@). The log p(a | 6) term is the usual data log-likelihood term, and the log p(@) term, the log-prior over parameters, incorporates the preference over particular values of 0. This view is described in section 5.6", "d557b405-330d-4dbc-96c9-17e2f34030d1": ", Gn\u22121, all starting in the same state and keep it up-to-date as we obtain a single additional return Gn. In addition to keeping track of Vn, we must maintain for each state the cumulative sum Cn of the weights given to the \ufb01rst n returns. The update rule for Vn is where C0 .= 0 (and V1 is arbitrary and thus need not be speci\ufb01ed). The box on the next page contains a complete episode-by-episode incremental algorithm for Monte Carlo policy evaluation. The algorithm is nominally for the o\u21b5-policy case, using weighted importance sampling, but applies as well to the on-policy case just by choosing the target and behavior policies as the same (in which case (\u21e1 = b), W is always 1). The approximation Q converges to q\u21e1 (for all encountered state\u2013action pairs) while actions are selected according to a potentially di\u21b5erent policy, b", "efb3df89-ed04-4ff8-88da-e7930d207d3c": "Russell, S., Norvig, P. Arti\ufb01cial Intelligence: A Modern Approach, 3rd edition. PrenticeRusso, D. J., Van Roy, B., Kazerouni, A., Osband, I., Wen, Z. A tutorial on Thompson sampling, Foundations and Trends in Machine Learning. ArXiv:1707.02038. Rust, J. Numerical dynamic programming in economics. In H. Amman, D. Kendrick, and J. Rust (Eds. ), Handbook of Computational Economics, pp. 614\u2013722. Elsevier, Amsterdam. dopamine release dynamics in the nucleus accumbens core and shell reveal complementary signals for error prediction and incentive motivation. The Journal of Neuroscience, 35(33):11572\u201311582. Saksida, L. M., Raymond, S. M., Touretzky, D. S. Shaping robot behavior using principles from instrumental conditioning", "f3c9e099-50ac-4d5a-919b-814c925208e6": "MA we dawnt nnd 4h AMR AF ALR AR RA HAAR RAR AI AAR ALR BI RIA RAR AE  https://www.deeplearningbook.org/contents/rnn.html    LU ULLUCLSLALLU LLC C1LCCL VI LLC Specular LAULUDS, CULLSIUCL LLC SUL pie Case VL back-propagation with a Jacobian matrix J that does not change with t. \u2018This case happens, for example, when the network is purely linear. Suppose that J has an eigenvector v with corresponding eigenvalue 1. Consider what happens as we  propagate a gradient vector backward through time. If we begin with a gradient vector g, then after one step of back-propagation, we will have Jg, and after n steps we will have J\u201dg.\n\nNow consider what happens if we instead back-propagate a perturbed version of g. If we begin with g + dv, then after one step, we will have J(g + dv). After n steps, we will have J\"(g + dv)", "d367a149-8af3-434e-bb09-749c974af91c": "The precise form of the function y(x) is determined during the training phase, also known as the learning phase, on the basis of the training data.\n\nOnce the model is trained it can then determine the identity of new digit images, which are said to comprise a test set. The ability to categorize correctly new examples that differ from those used for training is known as generalization. In practical applications, the variability of the input vectors will be such that the training data can comprise only a tiny fraction of all possible input vectors, and so generalization is a central goal in pattern recognition. For most practical applications, the original input variables are typically preprocessed to transform them into some new space of variables where, it is hoped, the pattern recognition problem will be easier to solve. For instance, in the digit recognition problem, the images of the digits are typically translated and scaled so that each digit is contained within a box of a \ufb01xed size. This greatly reduces the variability within each digit class, because the location and scale of all the digits are now the same, which makes it much easier for a subsequent pattern recognition algorithm to distinguish between the different classes. This pre-processing stage is sometimes also called feature extraction", "d40f7ef5-8545-4211-b88a-73041b73297f": "What would the sequence of Rt+1 \u2212 \u00afRt errors be? What would the sequence of \u03b4t errors be (using (10.10))? Which error sequence would produce a more stable estimate of the average reward if the estimates were allowed to change in response to the errors? Why? \u21e4 access control to a set of 10 servers. Customers of four di\u21b5erent priorities arrive at a single queue. If given access to a server, the customers pay a reward of 1, 2, 4, or 8 to the server, depending on their priority, with higher priority customers paying more.\n\nIn each time step, the customer at the head of the queue is either accepted (assigned to one of the servers) or rejected (removed from the queue, with a reward of zero). In either case, on the next time step the next customer in the queue is considered. The queue never empties, and the priorities of the customers in the queue are equally randomly distributed. Of course a customer cannot be served if there is no free server; the customer is always rejected in this case. Each busy server becomes free with probability p = 0.06 on each time step. Although we have just described them for de\ufb01niteness, let us assume the statistics of arrivals and departures are unknown", "90d6a371-a3ec-4e43-aa1d-b979998ec39b": "Section 15.4 describes how this view motivates the use of distributed representations, with separate directions in representation space corresponding to separate factors of variation.\n\ne Causal factors: The model is constructed in such a way that it treats the factors of variation described by the learned representation h as the causes of the observed data x, and not vice versa. As discussed in section 15.3, this  https://www.deeplearningbook.org/contents/representation.html    1s advantageous tor semi-supervised learning and makes the learned model more robust when the distribution over the underlying causes changes or when we use the model for a new task. e Depth, or a hierarchical organization of explanatory factors: High-level, abstract concepts can be defined in terms of simple concepts, forming a hierarchy. From another point of view, the use of a deep architecture expresses our belief that the task should be accomplished via a multistep program, with each step referring back to the output of the processing accomplished via previous steps", "1921098b-a381-4225-9aa0-4daa53aa2844": "Any local minimum is  https://www.deeplearningbook.org/contents/optimization.html    uaranteed to be a global minimum. Some convex functions have a flat region at the bottom rather than a single global minimum point, but any point within such a flat region is an acceptable solution. When optimizing a convex function, we  know that we have reached a good solution if we find a critical point of any kind.\n\nWith nonconvex functions, such as neural nets, it is possible to have many local minima. Indeed, nearly any deep model is essentially guaranteed to have an extremely large number of local minima. As we will see, however, this is not necessarily a major problem. Neural networks and any models with multiple equivalently parametrized latent variables all have multiple local minima because of the model identifiability problem. A model is said to be identifiable if a sufficiently large training set can rule out all but one setting of the model\u2019s parameters. Models with latent variables are often not identifiable because we can obtain equivalent models by exchanging latent variables with each other", "a0ac8575-9338-4d32-b49e-e6b7ab1be114": "The policy gradient theorem gives an exact expression proportional to the gradient; all that is needed is some way of sampling whose expectation equals or approximates this expression. Notice that the right-hand side of the policy gradient theorem is a sum over states weighted by how often the states occur under the target policy \u21e1; if \u21e1 is followed, then states will be encountered in these proportions. Thus We could stop here and instantiate our stochastic gradient-ascent algorithm (13.1) as where \u02c6q is some learned approximation to q\u21e1.\n\nThis algorithm, which has been called an all-actions method because its update involves all of the actions, is promising and deserving of further study, but our current interest is the classical REINFORCE algorithm  whose update at time t involves just At, the one action actually taken at time t. We continue our derivation of REINFORCE by introducing At in the same way as we introduced St in (13.6)\u2014by replacing a sum over the random variable\u2019s possible values by an expectation under \u21e1, and then sampling the expectation", "d93beefc-2746-4390-8a13-763c76e4ffc7": "Frey and Morris  proposed the idea of a \u201csynaptic tag\u201d for the induction of long-lasting strengthening of synaptic e\ufb03cacy. Though not unlike Klopf\u2019s eligibility, their tag was hypothesized to consist of a temporary strengthening of a synapse that could be transformed into a long-lasting strengthening by subsequent neuron activation. The model of O\u2019Reilly and Frank  and O\u2019Reilly, Frank, Hazy, and Watz  uses working memory to bridge temporal intervals instead of eligibility traces. Wickens and Kotter  discuss possible mechanisms for synaptic eligibility. He, Huertas, Hong, Tie, Hell, Shouval, Kirkwood  provide evidence supporting the existence of contingent eligibility traces in synapses of cortical neurons with time courses like those of the eligibility traces Klopf postulated. The metaphor of a neuron using a learning rule related to bacterial chemotaxis was discussed by Barto .\n\nKoshland\u2019s extensive study of bacterial chemotaxis was in part motivated by similarities between features of bacteria and features of neurons . See also Berg", "9795695e-55ab-4efd-bdce-5a761f0c8559": "It can readily be extended to multiple target variables represented by the vector t, in which case the optimal solution is the conditional average y(x) = Et. Exercise 1.25 We can also derive this result in a slightly different way, which will also shed light on the nature of the regression problem. Armed with the knowledge that the optimal solution is the conditional expectation, we can expand the square term as follows {y(x) \u2212 t}2 = {y(x) \u2212 E + E \u2212 t}2 where, to keep the notation uncluttered, we use E to denote Et. Substituting into the loss function and performing the integral over t, we see that the cross-term vanishes and we obtain an expression for the loss function in the form The function y(x) we seek to determine enters only in the \ufb01rst term, which will be minimized when y(x) is equal to E, in which case this term will vanish. This is simply the result that we derived previously and that shows that the optimal least squares predictor is given by the conditional mean. The second term is the variance of the distribution of t, averaged over x.\n\nIt represents the intrinsic variability of the target data and can be regarded as noise", "fadb4025-edfd-490a-9789-fdaf04e88b19": "The value of this way of behaving is The key criterion is whether this is greater than or less than v\u21e1(s).\n\nIf it is greater\u2014that is, if it is better to select a once in s and thereafter follow \u21e1 than it would be to follow \u21e1 all the time\u2014then one would expect it to be better still to select a every time s is encountered, and that the new policy would in fact be a better one overall. That this is true is a special case of a general result called the policy improvement theorem. Let \u21e1 and \u21e10 be any pair of deterministic policies such that, for all s 2 S, Then the policy \u21e10 must be as good as, or better than, \u21e1. That is, it must obtain greater or equal expected return from all states s 2 S: Moreover, if there is strict inequality of (4.7) at any state, then there must be strict inequality of (4.8) at that state", "7b6bae98-e6cb-4663-8634-fc250fe04bf9": "Taking the sentiment attribute, for example, given a sentence x (e.g., a customer\u2019s review \u201cthe manager is a horrible person\u201d) and a target sentiment a (e.g., positive), the goal of the problem is to generate a new sentence y that (1) possesses the target sentiment, (2) preserves all other characteristics of the original sentence, and (3) is \ufb02uent (e.g., the transferred sentence \u201cthe manager is a perfect person\u201d).\n\nTo learn an attribute transfer model p\u03b8(y|x, a), a key challenge of the problem is the lack of direct supervision data (i.e., pairs of sentences that are exact the same except for sentiment), making it necessary to use other forms of experience. Here we brie\ufb02y describe an approach originally presented in Hu et al. and Yang et al. , highlighting how the approach can be built mechanically, by formulating relevant experience directly based on the problem de\ufb01nition and then plugging them into the SE. We can identify three types of experience, corresponding to the above three desiderata, respectively. First, the model needs to learn the concept of \u2018sentiment\u2019 to be able to modify the attribute of text", "98fc676b-c893-4a39-a3c1-71ea84841d60": "The molecular mechanisms producing these traces, as well as the much shorter traces that likely underly STDP, are not yet understood, but research focusing on time-dependent and neuromodulator-dependent synaptic plasticity is continuing.\n\nThe neuron-like actor unit that we have described here, with its Law-of-E\u21b5ect-style learning rule, appeared in somewhat simpler form in the actor\u2013critic network of Barto et al. That network was inspired by the \u201chedonistic neuron\u201d hypothesis proposed by physiologist A. H. Klopf . Not all the details of Klopf\u2019s hypothesis are consistent with what has been learned about synaptic plasticity, but the discovery of STDP and the growing evidence for a reward-modulated form of STDP suggest that Klopf\u2019s ideas may not have been far o\u21b5 the mark. We discuss Klopf\u2019s hedonistic neuron hypothesis next", "a1248576-e6ae-4b81-88c2-b078f062d8ac": "Let us simply assume that all models are given equal prior probability. The interesting term is the model evidence p(D|Mi) which expresses the preference shown by the data for different models, and we shall examine this term in more detail shortly. The model evidence is sometimes also called the marginal likelihood because it can be viewed as a likelihood function over the space of models, in which the parameters have been marginalized out. The ratio of model evidences p(D|Mi)/p(D|Mj) for two models is known as a Bayes factor", "0a3a07ce-228e-48be-8057-a1eeb49069d8": "d., Li, Y., and Vinyals, O. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. Parkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar, C. Cats and dogs. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3498\u20133505. IEEE, 2012. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211\u2013252, 2015. Schroff, F., Kalenichenko, D., and Philbin, J. Facenet: A uni\ufb01ed embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp", "baf4ba93-2421-4b55-8083-478fcf6d4ab3": "PCA learns a representation that has lower dimensionality than the original input. It also learns a representation whose elements have no linear correlation with each other. This is a first step toward the criterion of learning representations whose elements are statistically independent. To achieve full independence, a representation learning algorithm must also remove the nonlinear relationships between variables. PCA learns an orthogonal, linear transformation of the data that projects an input x to a representation z as shown in figure 5.8. In section 2.12, we saw that we could learn a one-dimensional representation that best reconstructs the original data (in the sense of mean squared error) and that this representation actually corresponds to the first principal component of the data. Thus we can use PCA as a simple and effective dimensionality reduction method that preserves as much of the information in the data as possible (again, as measured by least-squares reconstruction error). In the following, we will study how the PCA representation decorrelates the original data representation X. Let us consider the m x n design matrix X", "085bba2d-fbe6-49a9-88c0-907b8e5114d6": "Swee KL, Yi L, Ngoc-Trung T, Ngai-Man C, Gemma R, Yuval E. DOPING: generative data augmentation for unsuper- vised anomaly detection with GAN. arXiv preprint. 2018. Alireza M, Jonathon S, Navdeep J, lan G, Brendan F. Adversarial autoencoders. arXiv preprint. 2015.\n\nTim S, lan G, Wojciech Z, Vicki C, Alec R, Xi C. Improved techniques for training GANs. arXiv preprint. 2016. Yanghao L, Naiyan W, Jiaying L, Xiaodi H. Demistifying neural style transfer. arXiv preprint. 2017. Khizar H. Super-resolution via deep learning. arXiv preprint. 2017. Dmitry U, Andrea V, Victor L. Instance normalization: the missing ingredient for fast stylization", "df14777b-a39d-4f4e-b862-e96e778ef011": "Finally, in many of the application domains covered such as medical image analysis, the biases distancing the training data from the testing data are more complex than positional and transla- tional variances. Therefore, the scope of where and when geometric transformations  can be applied is relatively limited. Color space transformations  Image data is encoded into 3 stacked matrices, each of size height x width. These matri- ces represent pixel values for an individual RGB color value. Lighting biases are amongst the most frequently occurring challenges to image recognition problems. Therefore, the effectiveness of color space transformations, also known as photometric transforma- tions, is fairly intuitive to conceptualize. A quick fix to overly bright or dark images is to loop through the images and decrease or increase the pixel values by a constant value. Another quick color space manipulation is to splice out individual RGB color matrices. Another transformation consists of restricting pixel values to a certain min or max value.\n\nThe intrinsic representation of color in digital images lends itself to many strategies of augmentation. Color space transformations can also be derived from image-editing apps", "0d00de81-696a-4828-88ce-b0a4088cf5ef": "Simulation, learning, and optimization techniques in Watson\u2019s game strategies.\n\nIBM Journal of Research and Development, 56(3-4):16\u20131\u201316\u201311. Tesauro, G., Gondek, D. C., Lenchner, J., Fan, J., Prager, J. M. Analysis of Watson\u2019s strategies for playing Jeopardy! Journal of Arti\ufb01cial Intelligence Research, 47:205\u2013251. Tham, C. K. Modular On-Line Function Approximation for Scaling up Reinforcement Thathachar, M. A. L., Sastry, P. S. A new approach to the design of reinforcement Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, 36(6):711\u2013722. Thathachar, M., Sastry, P. S. Networks of Learning Automata: Techniques for Online Stochastic Optimization. Springer Science & Business Media. Theocharous, G., Thomas, P. S., Ghavamzadeh, M", "8ddabb6b-9df3-48ee-a93c-68e82199303b": "(a) The restricted Boltzmann machine itself is an undirected graphical model based on a bipartite graph, with visible units in one part of the graph and hidden units in the other part.\n\nThere are no connections among the visible units, nor any connections among the hidden units. Typically every visible unit is connected to every hidden unit, but it is possible to construct sparsely connected RBMs such as convolutional RBMs. (b) A  https://www.deeplearningbook.org/contents/generative_models.html    deep beliet network 1s a hybrid graphical model involving both directed and undirected connections. Like an RBM, it has no intralayer connections. However, a. DBN has multiple hidden layers, and thus connections between hidden units that are in separate layers. All the local conditional probability distributions needed by the deep belief network are  copied directly from the local conditional probability distributions of its constituent RBMs. Alternatively, we could also represent the deep belief network with a completely undirected graph, but it would need intralayer connections to capture the dependencies between parents. (c) A deep Boltzmann machine is an undirected graphical model with several layers of latent variables", "81dd0688-fcbd-4862-b2fc-0ec207dada9b": "Using the transformation rule (1.27) for densities we see that p(ln \u03c3) = const. Thus, for this prior there is the same probability mass in the range 1 \u2a7d \u03c3 \u2a7d 10 as in the range 10 \u2a7d \u03c3 \u2a7d 100 and in 100 \u2a7d \u03c3 \u2a7d 1000. An example of a scale parameter would be the standard deviation \u03c3 of a Gaussian distribution, after we have taken account of the location parameter \u00b5, because where \ufffdx = x \u2212 \u00b5. As discussed earlier, it is often more convenient to work in terms of the precision \u03bb = 1/\u03c32 rather than \u03c3 itself. Using the transformation rule for densities, we see that a distribution p(\u03c3) \u221d 1/\u03c3 corresponds to a distribution over \u03bb of the form p(\u03bb) \u221d 1/\u03bb. We have seen that the conjugate prior for \u03bb was the gamma distribution Gam(\u03bb|a0, b0) given by (2.146). The noninformative prior is obtained Section 2.3 as the special case a0 = b0 = 0", "95644829-e7ea-401c-8b31-6fddad14a66e": "Before giving a proof, we \ufb01rst discuss a generalization, known as the Metropolis-Hastings algorithm , to the case where the proposal distribution is no longer a symmetric function of its arguments. In particular at step \u03c4 of the algorithm, in which the current state is z(\u03c4), we draw a sample z\u22c6 from the distribution qk(z|z(\u03c4)) and then accept it with probability Ak(z\u22c6, z\u03c4) where Here k labels the members of the set of possible transitions being considered. Again, the evaluation of the acceptance criterion does not require knowledge of the normalizing constant Zp in the probability distribution p(z) = \ufffdp(z)/Zp. For a symmetric proposal distribution the Metropolis-Hastings criterion (11.44) reduces to the standard Metropolis criterion given by (11.33).\n\nWe can show that p(z) is an invariant distribution of the Markov chain de\ufb01ned by the Metropolis-Hastings algorithm by showing that detailed balance, de\ufb01ned by (11.40), is satis\ufb01ed. Using (11.44) we have as required. The speci\ufb01c choice of proposal distribution can have a marked effect on the performance of the algorithm", "c37eb213-2a82-4ff7-ac02-850ed85d984b": "Another option is to take only a single vector x as input. https://www.deeplearningbook.org/contents/rnn.html    When & 1s a nxed-size vector, we can simply make It an extra Input Of the HININ that generates the y sequence.\n\nSome common ways of providing an extra input to an RNN are  1. as an extra input at each time step, or 2. as the initial state h \u00a9), or  3. both. The first and most common approach is illustrated in figure 10.9. The interaction between the input x and each hidden unit vector h) is parametrized by a newly introduced weight matrix R that was absent from the model of only the sequence of y values. The same product a! R is added as additional input to the hidden units at every time step. We can think of the choice of a as determining the value of a! R that is effectively a new bias parameter used for each of the hidden units. The weights remain independent of the input", "5303f67d-54a9-4ac7-88b1-f4e602a32baa": "This approach has been successfully applied to neural language models .\n\n12.4.4 Combining Neural Language Models with n-grams  A major advantage of n-gram models over neural networks is that n-gram models achieve high model capacity (by storing the frequencies of very many tuples), while requiring very little computation to process an example (by looking up only a few tuples that match the current context). If we use hash tables or trees to access the counts, the computation used for n-grams is almost independent of capacity. In comparison, doubling a neural network\u2019s number of parameters typically also roughly doubles its computation time. Exceptions include models that avoid using all parameters on each pass. Embedding layers index only a single embedding in each pass, so we can increase the vocabulary size without increasing the computation time per example. Some other models, such as tiled convolutional networks, can add parameters while reducing the degree of parameter sharing to maintain the same amount of computation. Typical neural network layers based on matrix multiplication, however, use an amount of computation proportional to  https://www.deeplearningbook.org/contents/applications.html    che number of parameters", "37b2671d-df93-440b-8980-6999127d180c": "crafted a deep convolutional GAN (DCGAN) that performs very well for image synthesis tasks, and showed that its latent repre- sentation space captures important factors of variation, as shown in figure 15.9. See figure 20.7 for examples of images generated by a DCGAN generator. The GAN learning problem can also be simplified by breaking the generation process into many levels of detail.\n\nIt is possible to train conditional GANs  that learn to sample from a distribution p(x | y) rather than simply sampling from a marginal distribution p(a). Denton et al. showed that a series of conditional GANs can be trained to first generate a very low-resolution version of an image, then incrementally add details to the image. This technique is called the LAPGAN model, due to the use of a Laplacian pyramid to generate the images containing varying levels of detail. LAPGAN generators are able to fool not only discriminator networks but also human observers, with experimental subjects identifying up to 40 percent of the outputs of the network as being real data. See figure 20.7 for examples of images generated by a LAPGAN generator", "1a97a35d-2811-4202-b280-71a86b942a5f": "If we change the sign of all of the weights and the bias feeding into a particular hidden unit, then, for a given input pattern, the sign of the activation of the hidden unit will be reversed, because \u2018tanh\u2019 is an odd function, so that tanh(\u2212a) = \u2212 tanh(a). This transformation can be exactly compensated by changing the sign of all of the weights leading out of that hidden unit. Thus, by changing the signs of a particular group of weights (and a bias), the input\u2013output mapping function represented by the network is unchanged, and so we have found two different weight vectors that give rise to the same mapping function. For M hidden units, there will be M such \u2018sign-\ufb02ip\u2019 symmetries, and thus any given weight vector will be one of a set 2M equivalent weight vectors .\n\nSimilarly, imagine that we interchange the values of all of the weights (and the bias) leading both into and out of a particular hidden unit with the corresponding values of the weights (and bias) associated with a different hidden unit. Again, this clearly leaves the network input\u2013output mapping function unchanged, but it corresponds to a different choice of weight vector", "e9068c75-7c8e-4c1c-abe6-056ceec4f4ff": "We recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two models, as ELMo does.\n\nHowever: (a) this is twice as expensive as a single bidirectional model; (b) this is non-intuitive for tasks like QA, since the RTL model would not be able to condition the answer on the question; (c) this it is strictly less powerful than a deep bidirectional model, since it can use both left and right context at every layer. In this section, we explore the effect of model size on \ufb01ne-tuning task accuracy. We trained a number of BERT models with a differing number of layers, hidden units, and attention heads, while otherwise using the same hyperparameters and training procedure as described previously. Results on selected GLUE tasks are shown in Table 6. In this table, we report the average Dev Set accuracy from 5 random restarts of \ufb01ne-tuning. We can see that larger models lead to a strict accuracy improvement across all four datasets, even for MRPC which only has 3,600 labeled training examples, and is substantially different from the pre-training tasks", "e0ebac1f-7775-4a4f-a359-d8e7c8e92ca2": "Our driver is a bit unreliable, as expressed through the following probabilities Suppose that the driver tells us that the fuel gauge shows empty, in other words that we observe D = 0. Evaluate the probability that the tank is empty given only this observation.\n\nSimilarly, evaluate the corresponding probability given also the observation that the battery is \ufb02at, and note that this second probability is lower. Discuss the intuition behind this result, and relate the result to Figure 8.54. M distinct random variables. Draw the 8 possibilities for the case of M = 3. function given by (8.42). Write down an expression for the difference in the values of the energy associated with the two states of a particular variable xj, with all other variables held \ufb01xed, and show that it depends only on quantities that are local to xj in the graph. coef\ufb01cients \u03b2 = h = 0. Show that the most probable con\ufb01guration of the latent variables is given by xi = yi for all i. nodes in the graph shown in Figure 8.38 is given by an expression of the form (8.58)", "946b3f3e-dd89-4abe-92f6-803b80618055": "Suppose we have a joint distribution p(x, y) from which we draw pairs of values of x and y. If a value of x is already known, then the additional information needed to specify the corresponding value of y is given by \u2212 ln p(y|x). Thus the average additional information needed to specify y can be written as which is called the conditional entropy of y given x.\n\nIt is easily seen, using the product rule, that the conditional entropy satis\ufb01es the relation Exercise 1.37 where H is the differential entropy of p(x, y) and H is the differential entropy of the marginal distribution p(x). Thus the information needed to describe x and y is given by the sum of the information needed to describe x alone plus the additional information required to specify y given x. So far in this section, we have introduced a number of concepts from information theory, including the key notion of entropy. We now start to relate these ideas to pattern recognition. Consider some unknown distribution p(x), and suppose that we have modelled this using an approximating distribution q(x)", "8afcfa8c-2ab6-4123-bee7-1bbfa9d17d4c": "In addition to accounting for many features of the phasic activity of dopamine neurons, the reward prediction error hypothesis links neuroscience to other aspects of reinforcement learning, in particular, to learning algorithms that use TD errors as reinforcement signals. Neuroscience is still far from reaching complete understanding of the circuits, molecular mechanisms, and functions of the phasic activity of dopamine neurons, but evidence supporting the reward prediction error hypothesis, along with evidence that phasic dopamine responses are reinforcement signals for learning, suggest that the brain might implement something like an actor\u2013critic algorithm in which TD errors play critical roles.\n\nOther reinforcement learning algorithms are plausible candidates too, but actor\u2013critic algorithms \ufb01t the anatomy and physiology of the mammalian brain particularly well, as we describe in the following two sections. component that learns policies, and the \u2018critic\u2019 is the component that learns about whatever policy is currently being followed by the actor in order to \u2018criticize\u2019 the actor\u2019s action choices. The critic uses a TD algorithm to learn the state-value function for the actor\u2019s current policy. The value function allows the critic to critique the actor\u2019s action choices by sending TD errors, \u03b4, to the actor", "a28979d5-4406-4ae5-a588-2979b8e10cec": "Minimizing this KL divergence corresponds exactly to minimizing the cross- entropy between the distributions. Many authors use the term \u201ccross-entropy\u201d to identify specifically the negative log-likelihood of a Bernoulli or softmax distribution, but that is a misnomer. Any loss consisting of a negative log-likelihood is a cross- entropy between the empirical distribution defined by the training set and the probability distribution defined by model. For example, mean squared error is the cross-entropy between the empirical distribution and a Gaussian model. We can thus see maximum likelihood as an attempt to make the model dis- ribution match the empirical distribution pgata. Ideally, we would like to match he true data-generating distribution pgata, but we have no direct access to this distribution. While the optimal @ is the same regardless of whether we are maximizing the ikelihood or minimizing the KL divergence, the values of the objective functions are different. In software, we often phrase both as minimizing a cost function", "8b9b551a-12bb-49f1-8428-492713fc7547": "2.25 (\u22c6 \u22c6) In Sections 2.3.1 and 2.3.2, we considered the conditional and marginal distributions for a multivariate Gaussian. More generally, we can consider a partitioning of the components of x into three groups xa, xb, and xc, with a corresponding partitioning of the mean vector \u00b5 and of the covariance matrix \u03a3 in the form By making use of the results of Section 2.3, \ufb01nd an expression for the conditional distribution p(xa|xb) in which xc has been marginalized out. By multiplying both sides by (A + BCD) prove the correctness of this result. 2.27 (\u22c6) Let x and z be two independent random vectors, so that p(x, z) = p(x)p(z). Show that the mean of their sum y = x + z is given by the sum of the means of each of the variable separately. Similarly, show that the covariance matrix of y is given by the sum of the covariance matrices of x and z. Con\ufb01rm that this result agrees with that of Exercise 1.10", "dfffc46b-895c-4760-9ff7-d01067ad8618": "This random variable can take one of two possible values, namely r (corresponding to the red box) or b (corresponding to the blue box).\n\nSimilarly, the identity of the fruit is also a random variable and will be denoted by F. It can take either of the values a (for apple) or o (for orange). To begin with, we shall de\ufb01ne the probability of an event to be the fraction of times that event occurs out of the total number of trials, in the limit that the total number of trials goes to in\ufb01nity. Thus the probability of selecting the red box is 4/10 considering two random variables, X, which takes the values {xi} where i = 1, . , M, and Y , which takes the values {yj} where j = 1, . , L. In this illustration we have M = 5 and L = 3. If we consider a total number N of instances of these variables, then we denote the number of instances where X = xi and Y = yj by nij, which is the number of points in the corresponding cell of the array", "52b673a2-67fa-47bb-be55-82b86c413d3a": "G., Veness, J., Bowling, M. Investigating contingency awareness using Intelligence (AAAI-12), pp. 864\u2013871. AAAI Press, Menlo Park, CA. Bellman, R. E. A problem in the sequential design of experiments. Sankhya, 16:221\u2013229. Bellman, R. E. Dynamic Programming. Princeton University Press, Princeton. Bellman, R. E. A Markov decision process. Journal of Mathematics and Mechanics, Bellman, R. E., Dreyfus, S. E. Functional approximations and dynamic programming. Mathematical Tables and Other Aids to Computation, 13:247\u2013251. Bellman, R. E., Kalaba, R., Kotkin, B. Polynomial approximation\u2014A new computational Bengio, Y. Learning deep architectures for AI", "d82e3627-7875-4854-887e-e1a7d103a18e": "PAC-Bayesian stochastic model selection. Machine Learning 51(1), 5\u201321. Minka, T. Divergence measures and message passing. Technical Report MSR-TR-2005173, Microsoft Research Cambridge. Minka, T. P. Automatic choice of dimensionality for PCA. In T. K. Leen, T. G. Dietterich, and V. Tresp (Eds. ), Advances in Neural Information Processing Systems, Volume 13, pp. 598\u2013604. MIT Press. Minsky, M. L. and S. A. Papert . Perceptrons. MIT Press. Expanded edition 1990. Miskin, J. W. and D. J. C. MacKay . Ensemble learning for blind source separation. In S. J. Roberts and R. M", "46092513-7e62-4318-a9b3-105f701e3259": "CoLA The Corpus of Linguistic Acceptability is a binary single-sentence classi\ufb01cation task, where the goal is to predict whether an English sentence is linguistically \u201cacceptable\u201d or not . STS-B The Semantic Textual Similarity Benchmark is a collection of sentence pairs drawn from news headlines and other sources . They were annotated with a score from 1 to 5 denoting how similar the two sentences are in terms of semantic meaning. MRPC Microsoft Research Paraphrase Corpus consists of sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in the pair are semantically equivalent . RTE Recognizing Textual Entailment is a binary entailment task similar to MNLI, but with much less training data .14 WNLI Winograd NLI is a small natural language inference dataset .\n\nThe GLUE webpage notes that there are issues with the construction of this dataset, 15 and every trained system that\u2019s been submitted to GLUE has performed worse than the 65.1 baseline accuracy of predicting the majority class. We therefore exclude this set to be fair to OpenAI GPT", "4defa996-741d-418b-88d7-dc92d8ae9c3d": "Both velocity components are restricted to be nonnegative and less than 5, and they cannot both be zero except at the starting line. Each episode begins in one of the randomly selected start states with both velocity components zero and ends when the car crosses the \ufb01nish line. The rewards are \u22121 for each step until the car crosses the \ufb01nish line. If the car hits the track boundary, it is moved back to a random position on the starting line, both velocity components are reduced to zero, and the episode continues. Before updating the car\u2019s location at each time step, check to see if the projected path of the car intersects the track boundary. If it intersects the \ufb01nish line, the episode ends; if it intersects anywhere else, the car is considered to have hit the track boundary and is sent back to the starting line. To make the task more challenging, with probability 0.1 at each time step the velocity increments are both zero, independently of the intended increments. Apply a Monte Carlo control method to this task to compute the optimal policy from each starting state", "8020404c-a40c-41bf-89cc-44bb663e9219": "| 84.13 | 91.4 71.13 84.66 InferSent - GloVe 81.57 | 86.54 | 92.50 | 90.38 | 84.18 | 88.2 75.77 85.59 Universal Sentence Encoder | 80.09 | 85.19 | 93.98 86.70 | 86.38 93.2 70.14 85.10 SBERT-NLI-base 83.64 | 89.43 | 94.39 | 89.86 | 88.96 | 89.6 76.00 87.41 SBERT-NLI-large 84.88 | 90.07 | 94.52 | 90.33 | 90.66 | 87.4 75.94 87.69  BERT-flow  The embedding representation space is deemed isotropic if embeddings are uniformly distributed on each dimension; otherwise, it is anisotropic. Li et al,  showed that a pre-trained BERT learns a non-smooth anisotropic semantic space of sentence embeddings and thus leads to poor  performance for text similarity tasks without fine-tuning", "53caab8a-3ee0-4275-8289-a097533f9621": "We can then express the expectation in the form of a \ufb01nite sum over The quantities rl = p(z(l))/q(z(l)) are known as importance weights, and they correct the bias introduced by sampling from the wrong distribution. Note that, unlike rejection sampling, all of the samples generated are retained. It will often be the case that the distribution p(z) can only be evaluated up to a normalization constant, so that p(z) = \ufffdp(z)/Zp where \ufffdp(z) can be evaluated easily, whereas Zp is unknown.\n\nSimilarly, we may wish to use an importance sampling distribution q(z) = \ufffdq(z)/Zq, which has the same property. We then have where \ufffdrl = \ufffdp(z(l))/\ufffdq(z(l)). We can use the same sample set to evaluate the ratio Zp/Zq with the result As with rejection sampling, the success of the importance sampling approach depends crucially on how well the sampling distribution q(z) matches the desired distribution p(z)", "80c50ac7-1a57-4c13-9fbc-e5580ed2b735": "On the other hand, when q(x) < p(a )| f(a)|, whic  will happen more rarely, the ratio can be huge.\n\nBecause these latter events are rare, they may not show up in a typical sample, yielding typical underestimation of s, compensated rarely by gross overestimation. Such very large or very small numbers are typical when & is high dimensional, because in high dimension the dynamic range of joint probabilities can be very large. In spite of this danger, importance sampling and its variants have been found very useful in many machine learning algorithms, including deep learning algo- rithms. For example, see the use of importance sampling to accelerate training in neural language models with a large vocabulary (section 12.4.3.3) or other neural nets with a large number of outputs. See also how importance sampling has been used to estimate a partition function (the normalization constant of a probability  591  CHAPTER 17. MONTE CARLO METHODS  distribution) in section 18.7, and to estimate the log-likelihood in deep directed models, such as the variational autoencoder, in section 20.10.3", "8bedc8ec-1c1d-4306-9529-c76799e6e54f": "x'|h, =h,);  The probability of observing a positive example for x is p* (x') = p( The probability of getting a negative sample for x is p; (x\u2019) = p(x\u2019|h, #h,). When we are sampling x~ , we cannot access the true p> (x-) and thus x~ may be sampled from the (undesired) anchor class c with probability 7+. The actual sampling data distribution becomes:  P(x\u2019) = 1\u00b0 pz (x\u2019) + 7p; (x\u2019')  Thus we can use p, (x\u2019) = (p(x\u2019) \u2014 nt p{(x\u2019))/n~ for sampling x~ to debias the loss", "abeae0c6-aa37-47a9-ad4e-c409fbd3d817": "The architectures were constructed such that many of the parameters in the classifier model could be paired to corresponding parameters in the unsupervised model.\n\nWhile a parameter norm penalty is one way to regularize parameters to be close to one another, the more popular way is to use constraints: to force sets of parameters to be equal. This method of regularization is often referred to as parameter sharing, because we interpret the various models or model components as sharing a unique set of parameters. A significant advantage of parameter sharing over regularizing the parameters to be close (via a norm penalty) is that only a subset of the parameters (the unique set) needs to be stored in memory. In certain models\u2014such as the convolutional neural network\u2014this can lead to significant reduction in the memory footprint of the model. 7.9.1 Convolutional Neural Networks By far the most popular and extensive use of parameter sharing occurs in convo- lutional neural networks (CNNs) applied to computer vision. Natural images have many statistical properties that are invariant to translation. For example, a photo of a cat remains a photo of a cat if it is translated one pixel to the right", "24f955bb-2939-4571-a36d-c8a158e6cfbe": "We see that the solution for the maximum likelihood estimator depends on the data only through \ufffd n u(xn), which is therefore called the suf\ufb01cient statistic of the distribution (2.194). We do not need to store the entire data set itself but only the value of the suf\ufb01cient statistic. For the Bernoulli distribution, for example, the function u(x) is given just by x and so we need only keep the sum of the data points {xn}, whereas for the Gaussian u(x) = (x, x2)T, and so we should keep both the sum of {xn} and the sum of {x2 n}. If we consider the limit N \u2192 \u221e, then the right-hand side of (2.228) becomes E, and so by comparing with (2.226) we see that in this limit \u03b7ML will equal the true value \u03b7", "1f12da11-11bd-405a-bb72-9d16fc3d0df6": "The second estimates used to compute action values gave the \u201cin-category DD con\ufb01dence,\u201d pDD, which estimated the likelihood that Watson would respond correctly to the as-yet unrevealed DD clue. Tesauro et al. used the reinforcement learning approach of TD-Gammon described above to learn \u02c6v(\u00b7,w): a straightforward combination of nonlinear TD(\u03bb) using a multilayer ANN with weights w trained by backpropagating TD errors during many simulated games. States were represented to the network by feature vectors speci\ufb01cally designed for Jeopardy!. Features included the current scores of the three players, how many DDs remained, the total dollar value of the remaining clues, and other information related to the amount of play left in the game. Unlike TD-Gammon, which learned by self-play, Watson\u2019s \u02c6v was learned over millions of simulated games against carefully-crafted models of human players. In-category con\ufb01dence estimates were conditioned on the number of right responses r and wrong responses w that Watson gave in previously-played clues in the current category", "1c08494b-961a-441a-a1c4-976f4fb1f768": "Sometimes we annotate the edges in this graph with the name of the parameters that describe the  relationship between two layers. Here, we indicate that a matrix W describes the mapping from x to h, and a vector w describes the mapping from h to y. We typically omit the  https://www.deeplearningbook.org/contents/mlp.html    intercept parameters associated with each layer when labeling this kind ot drawing. max{0, z}  9(2)  Figure 6.3: The rectified linear activation function. This activation function is the default activation function recommended for use with most feedforward neural networks. Applying this function to the output of a linear transformation yields a nonlinear transformation. The function remains very close to linear, however, in the sense that it is a piecewise linear function with two linear pieces. Because rectified linear units are nearly linear, they preserve many of the properties that make linear models easy to optimize with gradient-based methods. They also preserve many of the properties that make linear models generalize well.\n\nA common principle throughout computer science is that we can build complicated systems from minimal components", "6dec25d7-9053-4024-aac8-332bc7c6f6e9": "An, P. C. E. .\n\nAn Improved Multi-dimensional CMAC Neural network: Receptive Field Function and Placement. Ph.D. thesis, University of New Hampshire, Durham. An, P. C. E., Miller, W. T., Parks, P. C. Design improvements in associative memories for cerebellar model articulation controllers (CMAC). Arti\ufb01cial Neural Networks, pp. 1207\u20131210, Elsevier North-Holland. http://www.incompleteideas.net/papers/AnMillerParks1991.pdf Anderson, C. W. Learning and Problem Solving with Multilayer Connectionist Systems. Anderson, C. W. Strategy learning with multilayer connectionist representations. In Anderson, C. W. Learning to control an inverted pendulum using neural networks. IEEE categorical perception, and probability learning: Some applications of a neural model. Psychological Review, 84(5):413\u2013451", "6166f17d-c29c-475d-86c8-faf760261462": "The existence of the ping pong ball and all its spatial coordinates are important underlying causal factors that generate the image and are relevant to the robotics task. Unfortunately, the autoencoder has limited capacity, and the training with mean squared error did not identify the ping pong ball as being salient enough to encode. Images graciously provided by Chelsea Finn. of a robotics task in which an autoencoder has failed to learn to encode a small  https://www.deeplearningbook.org/contents/representation.html    ping pong ball. This same robot is capable of successfully interacting with larger objects, such as baseballs, which are more salient according to mean squared error. Other definitions of salience are possible. For example, if a group of pixels follows a highly recognizable pattern, even if that pattern does not involve extreme brightness or darkness, then that pattern could be considered extremely salient.\n\nOne way to implement such a definition of salience is to use a recently developed approach called generative adversarial networks . In this approach, a generative model is trained to fool a feedforward classifier", "47573819-75ec-4a3b-92c4-ea722e1ca255": "However, we see that while EHR and Chem have equivalent label densities, our optimizer correctly predicts that Chem can be modeled with majority vote, speeding up each pipeline execution by 1.8\u00d7. 3.1.3 Accelerating initial development cycles We \ufb01nd in our applications that the optimizer can save execution time especially during the initial cycles of iterative development. To illustrate this empirically, in Fig. 8 we measure the modeling advantage of the generative model versus a majority vote of the labeling functions on increasingly large random subsets of the CDR labeling functions. We see that 11 Note that in Sect. 4, due to known negative class imbalance in relation extraction problems, we default to a negative value if majority vote yields a tie-vote label of 0. Thus, our reported F1 score metric hides instances in which the generative model learns to correctly (or incorrectly) break ties", "e04dac51-f97e-4802-ba07-36a2db59bb6d": "A typical way to do this is to introduce assumptions about how q factorizes. A common approach to variational learning is to impose the restriction that q  https://www.deeplearningbook.org/contents/inference.html    is a factorial distribution:  qh | v) = II (hi | v). (19.17)  This is called the mean field approach. More generally, we can impose any graphi- cal model structure we choose on gq, to flexibly determine how many interactions we want our approximation to capture. This fully general graphical model approach is called structured variational inference . The beauty of the variational approach is that we do not need to specify a specific parametric form for g. We specify how it should factorize, but then the optimization problem determines the optimal probability distribution within those factorization constraints. For discrete latent variables, this just means that we use traditional optimization techniques to optimize a finite number of variables describing the q distribution.\n\nFor continuous latent variables, this means that we use a branch of mathematics called calculus of variations to perform optimization over a space of functions and actually determine which function should be used to represent qg", "b2f691ab-fb65-46e0-a26b-da426216951d": "Besides, our data manipulation approach is derived based on a different perspective of reward learning, instead of meta-learning as in . Another popular type of data manipulation involves data synthesis, which creates entire arti\ufb01cial samples from scratch. GAN-based approaches have achieved impressive results for synthesizing conditional image data .\n\nIn the text domain, controllable text generation  presents a way of co-training the data generator and classi\ufb01er in a cyclic manner within a joint VAE  and wake-sleep  framework. It is interesting to explore the instantiation of the present approach for adaptive data synthesis in the future. We \ufb01rst present the relevant work upon which our automated data manipulation is built. This section also establishes the notations used throughout the paper. Let x denote the input and y the output. For example, in text classi\ufb01cation, x can be a sentence and y is the sentence label. Denote the model of interest as p\u03b8(y|x), where \u03b8 is the model parameters to be learned. In supervised setting, given a set of training examples D = {(x\u2217, y\u2217)}, we learn the model by maximizing the data log-likelihood", "a964a698-f283-4636-97f8-d95701b128d4": "An important class of such approximations, that can broadly be called variational methods, will be discussed in detail in Chapter 10. Complementing these deterministic approaches is a wide range of sampling methods, also called Monte Carlo methods, that are based on stochastic numerical sampling from distributions and that will be discussed at length in Chapter 11. Here we consider one simple approach to approximate inference in graphs with loops, which builds directly on the previous discussion of exact inference in trees. The idea is simply to apply the sum-product algorithm even though there is no guarantee that it will yield good results. This approach is known as loopy belief propagation  and is possible because the message passing rules (8.66) and (8.69) for the sum-product algorithm are purely local.\n\nHowever, because the graph now has cycles, information can \ufb02ow many times around the graph. For some models, the algorithm will converge, whereas for others it will not. In order to apply this approach, we need to de\ufb01ne a message passing schedule. Let us assume that one message is passed at a time on any given link and in any given direction", "8b073643-2ccc-4334-baec-211a39393406": "This heuristic serves as an upper bound to the true expected advantage, and thus, we can use it to determine when we can safely skip training the generative model (see Algorithm 1).\n\nLet cy(\ufffdi) = \ufffdn j=1 1 \ufffd \ufffdi, j = y \ufffd be the counts of labels of class y for xi, and assume that the true labeling function weights lie within a \ufb01xed range, w j \u2208  and have a mean \u00afw.10 Then, de\ufb01ne: where \u03c3(\u00b7) is the sigmoid function, f \u00afw is majority vote with all weights set to the mean \u00afw, and \u02dcA\u2217(\ufffd) is the predicted modeling advantage used by our optimizer. Essentially, we are taking the expected counts of instances in which a weighted majority vote could possibly \ufb02ip the incorrect predictions of unweighted majority vote under best-case conditions, which is an upper bound for the expected advantage: Proposition 3 (Optimizer Upper Bound) Assume that the labeling functions have accuracy parameters (log-odds weights) w j \u2208 , and have E = \u00afw", "150b544a-02f5-496d-9113-61e68e856d25": "We \ufb01rst note that the leapfrog integration scheme (11.64), (11.65), and (11.66) is time-reversible, so that integration for L steps using step size \u2212\u03f5 will exactly undo the effect of integration for L steps using step size \u03f5.\n\nNext we show that the leapfrog integration preserves phase-space volume exactly. This follows from the fact that each step in the leapfrog scheme updates either a zi variable or an ri variable by an amount that is a function only of the other variable. As shown in Figure 11.14, this has the effect of shearing a region of phase space while not altering its volume. Finally, we use these results to show that detailed balance holds. Consider a small region R of phase space that, under a sequence of L leapfrog iterations of step size \u03f5, maps to a region R\u2032. Using conservation of volume under the leapfrog iteration, we see that if R has volume \u03b4V then so too will R\u2032", "2ac30d57-88f8-402e-ae5d-7af13811c167": "The development of goal-directed behavioral control was likely a major advance in the evolution of animal intelligence.\n\nstrategies in a hypothetical task in which a rat has to navigate a maze that has distinctive goal boxes, each delivering an associated reward of the magnitude shown (Figure 14.5 top). Starting at S1, the rat has to \ufb01rst select left (L) or right (R) and then has to select L or R again at S2 or S3 to reach one of the goal boxes. The goal boxes are the terminal states of each episode of the rat\u2019s episodic task. A model-free strategy (Figure 14.5 lower left) relies on stored values for state\u2013action pairs. These action values are estimates of the highest return the rat can expect for each action taken from each (nonterminal) state. They are obtained over many trials of running the maze from start to \ufb01nish. When the action values have become good enough estimates of the optimal returns, the rat just has to select at each state the action with the largest action value in order to make optimal decisions. In this case, when the action-value estimates become accurate enough, the rat selects L from S1 and R from S2 to obtain the maximum return of 4", "657d1b38-883c-4011-8148-13f8c99dccb1": "An important class of such models, known as independent component analysis, or leA, arises when we consider a distribution over the latent variables that factorizes, so that M p(z) = IIp(Zj). j=l (12.89) To understand the role of such models, consider a situation in which two people are talking at the same time, and we record their voices using two microphones. If we ignore effects such as time delay and echoes, then the signals received by the microphones at any point in time will be given by linear combinations of the amplitudes of the two voices. The coefficients of this linear combination will be constant, and if we can infer their values from sample data, then we can invert the mixing process (assuming it is nonsingular) and thereby obtain two clean signals each of which contains the voice ofjust one person.\n\nThis is an example of a problem called blind source separation in which 'blind' refers to the fact that we are given only the mixed data, and neither the original sources nor the mixing coefficients are observed . This type of problem is sometimes addressed using the following approach  in which we ignore the temporal nature of the signals and treat the successive samples as i.i.d", "a8ed459e-43f8-4051-8013-2abeec206fb6": "It could be tackled using handcrafted rules or heuristics for distinguishing the digits based on the shapes of the strokes, but in practice such an approach leads to a proliferation of rules and of exceptions to the rules and so on, and invariably gives poor results. Far better results can be obtained by adopting a machine learning approach in which a large set of N digits {x1, . , xN} called a training set is used to tune the parameters of an adaptive model. The categories of the digits in the training set are known in advance, typically by inspecting them individually and hand-labelling them. We can express the category of a digit using target vector t, which represents the identity of the corresponding digit. Suitable techniques for representing categories in terms of vectors will be discussed later. Note that there is one such target vector t for each digit image x. The result of running the machine learning algorithm can be expressed as a function y(x) which takes a new digit image x as input and that generates an output vector y, encoded in the same way as the target vectors", "77121de8-c220-492b-b786-abc6161d21fc": "They find classifica- tion accuracy differences of 70.18% versus 74.42% on the CIFAR-10 dataset and 74.61% versus 80.35% on the problem of classifying dogs versus cats.\n\nFurther, they explore the  robustness of classifiers with respect to test-time augmentation and find that the model Shorten and Khoshgoftaar J Big Data  6:60  Table 8 AutoAugment augmentation policy found on the reduced CIFAR-10 dataset   Operation 1 Operation 2 Sub-policy 0 (Invert,0.1,7) (Contrast,0.2,6) Sub-policy 1 (Rotate,0.7,2) (TranslateX,0.3,9) Sub-policy 2 (Sharpness,0.8,1) (Sharpness,0.9,3) Sub-policy 3 (ShearY,0.5,8) (TranslateY,0.7,9) Sub-policy 4 (AutoContrast,0.5,8) (Equalize,0.9,2) Sub-policy 5 (ShearY,0.2,7) (Posterize,0.3,7) Sub-policy 6 (Color,0.4,3) (Brightness,0.6,7) Sub-policy 7 (Sharpness,0.3,9) (Brightness,0.7,9) Sub-policy 8& (Equalize,0.6,5) (Equalize,0.5,1) Sub-policy 9 (Contrast,0.6,7) (Sharpness,0.6,5) Sub-policy 10 (Color,0.7,7) (TranslateX,0.5,8) Sub-policy 11 (Equalize,0.3,7) (AutoContrast,0.4,8) Sub-policy 12 (Translatey,0.4,3) (Sharpness,0.2,6) Sub-policy 13 (Brightness,0.9,6) (Color,0.2,8)\n\nSub-policy 14 (Solarize,0.5,2) (Invert,0.0,3) Sub-policy 15 (Equalize,0.2,0) (AutoContrast,0.6,0) Sub-policy 16 (Equalize,0.2,8) (Equalize,0.6,4) Sub-policy 17 (Color,0.9,9) (Equalize,0.6,6) Sub-policy 18 (AutoContrast,0.8,4) (Solarize,0.2,8) Sub-policy 19 (Brightness,0.1,3) (Color,0.7,0) Sub-policy 20 (Solarize,0.4,5) (AutoContrast,0.9,3) Sub-policy 21 (TranslateY,0.9,9) (TranslateY,0.7,9) Sub-policy 22 (AutoContrast,0.9,2) (Solarize,0.8,3) Sub-policy 23 (Equalize,0.8,8) (Invert,0.1,3) Sub-policy 24 (Translatey,0.7,9) (AutoContrast,0.9,1)  Table 9 The performance of ARS on continuous space vs", "e2d538fd-085e-4ee4-b716-485a484af395": "G., Finn, C., Lee, L., Neiswanger, W., Qin, L., Berg-Kirkpatrick, T., Salakhutdinov, R., & Xing, E. P. The NeurIPS workshop on learning with rich experience: Integration of learning paradigms. https://sites.google.com/view/neurips2019lire Hu, Z., Yang, Z., Liang, X., Salakhutdinov, R., & Xing, E. P. Toward controlled generation of text. International Conference on Machine Learning, 1587\u20131596. Hu, Z., Yang, Z., Salakhutdinov, R., Liang, X., Qin, L., Dong, H., & Xing, E. P. Deep generative models with learnable knowledge constraints. Proceedings of the 32nd International Conference on Neural Information Processing Systems, 10522\u201310533. Jaakkola, T., Meila, M., & Jebara, T. Maximum entropy discrimination", "88e49456-7f0a-49cb-af96-7dd4838eff94": "Repurposing Learning Algorithms for New Problems. The standardized formalism sheds new light on fundamental relationships between a number of learning problems in di\ufb00erent research areas, showing that they are essentially the same under the SE perspective. This opens up a wide range of opportunities for generalizing existing algorithms, which were originally designed for specialized problems, to a much broader set of new problems. It is also made easy to exchange between the diverse research areas in aspects of modeling, theoretical understanding, approximation, and optimization. For example, an earlier successful approach to challenges in one area can now be readily applied to address challenges in another. Similarly, a future progress made in one problem could immediately unlock progresses in many others", "b5b90cc7-4414-4084-905a-88c3456e5c4d": "Our discussion so far can be summarized by saying that the danger of instability and divergence arises whenever we combine all of the following three elements, making up what we call the deadly triad: Function approximation A powerful, scalable way of generalizing from a state space Bootstrapping Update targets that include existing estimates (as in dynamic programming or TD methods) rather than relying exclusively on actual rewards and complete returns (as in MC methods). O\u21b5-policy training Training on a distribution of transitions other than that produced by the target policy.\n\nSweeping through the state space and updating all states uniformly, as in dynamic programming, does not respect the target policy and is an example of o\u21b5-policy training. In particular, note that the danger is not due to control or to generalized policy iteration. Those cases are more complex to analyze, but the instability arises in the simpler prediction case whenever it includes all three elements of the deadly triad. The danger is also not due to learning or to uncertainties about the environment, because it occurs just as strongly in planning methods, such as dynamic programming, in which the environment is completely known. If any two elements of the deadly triad are present, but not all three, then instability can be avoided", "116c45eb-7e52-4ad6-ae43-9d1931647242": "We also assume that the loss is the negative log-likelihood of the true target y given the input so far.\n\nThe gradient Vj) L on the outputs at time step \u00a2, for all 7,t, is as follows:  ott  aL AL OLY ow  (VoL); = ao = aL () ao) =U  1 (10.18)  jay *  We work our way backward, starting from the end of the sequence. At the final time step T, h 7) only has o(7) as a descendent, so its gradient is simple:  Vin b=V! Von L- 10.19)  We can then iterate backward in time to back-propagate gradients through time, from t =r \u2014 1 down to t = 1, noting that AY (for t < 7) has as descendents both o) and A+)", "a627efb6-04a1-4dd4-8b45-5864499293e0": "In this case no \u03b2 recursion is required, and we simply have Let us take a moment to interpret this result for p(X). Recall that to compute the likelihood we should take the joint distribution p(X, Z) and sum over all possible values of Z. Each such value represents a particular choice of hidden state for every time step, in other words every term in the summation is a path through the lattice diagram, and recall that there are exponentially many such paths.\n\nBy expressing the likelihood function in the form (13.42), we have reduced the computational cost from being exponential in the length of the chain to being linear by swapping the order of the summation and multiplications, so that at each time step n we sum the contributions from all paths passing through each of the states znk to give the intermediate quantities \u03b1(zn). Next we consider the evaluation of the quantities \u03be(zn\u22121, zn), which correspond to the values of the conditional probabilities p(zn\u22121, zn|X) for each of the K \u00d7 K settings for (zn\u22121, zn)", "0023e207-d2e5-4142-8618-6d04b9480661": "Asynchronous algorithms also make it easier to intermix computation with real-time interaction. To solve a given MDP, we can run an iterative DP algorithm at the same time that an agent is actually experiencing the MDP. The agent\u2019s experience can be used to determine the states to which the DP algorithm applies its updates. At the same time, the latest value and policy information from the DP algorithm can guide the agent\u2019s decision making. For example, we can apply updates to states as the agent visits them. This makes it possible to focus the DP algorithm\u2019s updates onto parts of the state set that are most relevant to the agent. This kind of focusing is a repeated theme in reinforcement learning. Policy iteration consists of two simultaneous, interacting processes, one making the value function consistent with the current policy (policy evaluation), and the other making the policy greedy with respect to the current value function (policy improvement).\n\nIn policy iteration, these two processes alternate, each completing before the other begins, but this is not really necessary. In value iteration, for example, only a single iteration of policy evaluation is performed in between each policy improvement", "42a6e364-7dac-4b07-8bb4-1050fe9ecb57": "Having found the marginal and conditional distributions, we effectively expressed the joint distribution p(z) = p(x)p(y|x) in the form p(x|y)p(y). These results are summarized below. Given a marginal Gaussian distribution for x and a conditional Gaussian distribution for y given x in the form the marginal distribution of y and the conditional distribution of x given y are given by Given a data set X = (x1, . , xN)T in which the observations {xn} are assumed to be drawn independently from a multivariate Gaussian distribution, we can estimate the parameters of the distribution by maximum likelihood. The log likelihood function is given by By simple rearrangement, we see that the likelihood function depends on the data set only through the two quantities These are known as the suf\ufb01cient statistics for the Gaussian distribution.\n\nUsing (C.19), the derivative of the log likelihood with respect to \u00b5 is given by Appendix C and setting this derivative to zero, we obtain the solution for the maximum likelihood estimate of the mean given by which is the mean of the observed set of data points. The maximization of (2.118) with respect to \u03a3 is rather more involved", "ff099b3f-2a43-4b10-8e0b-c6ba5ae6ef34": "Steinkrau ef al. implemented a two-layer fully connected neural network on a GPU and reported a three-times speedup over their CPU-based baseline. Shortly thereafter, Chellapilla et al. demonstrated that the same technique could be used to  1 1 \u20184 ras 1 1 1  https://www.deeplearningbook.org/contents/applications.html    accelerate SUPErVIsSed CONVOLULIONAL LLELWOIKS. The popularity of graphics cards for neural network training exploded after the advent of general purpose GPUs, These GP-GPUs could execute arbitrary  code, not just rendering subroutines. NVIDIA\u2019s CUDA programming language provided a way to write this arbitrary code in a C-like language.\n\nWith their relatively convenient programming model, massive parallelism, and high memory bandwidth, GP-GPUs now offer an ideal platform for neural network programming. 440  CHAPTER 12. APPLICATIONS  This platform was rapidly adopted by deep learning researchers soon after it became available . Writing efficient code for GP-GPUs remains a difficult task best left to special- ists", "9ca64577-83bc-4ae8-9d36-859a62510b58": "In this elegant approach, there is no need to construct explicit targets for the inference network. Instead, the inference network is simply used to define \u00a3, and then the parameters of the inference network are adapted to increase \u00a3. This model is described in depth in section 20.10.3. Using approximate inference, it is possible to train and use a wide variety of models. Many of these models are described in the next chapter.\n\n650  https://www.deeplearningbook.org/contents/inference.html", "4c531b26-b32e-4f40-8530-0557b397c072": "Furthermore, we shall assume that the covariance of this Gaussian is small so that the network function is approximately linear with respect to the parameters over the region of parameter space for which the posterior probability is signi\ufb01cantly nonzero. With these two approximations, we will obtain models that are analogous to the linear regression and classi\ufb01cation models discussed in earlier chapters and so we can exploit the results obtained there. We can then make use of the evidence framework to provide point estimates for the hyperparameters and to compare alternative models (for example, networks having different numbers of hidden units). To start with, we shall discuss the regression case and then later consider the modi\ufb01cations needed for solving classi\ufb01cation tasks. Consider the problem of predicting a single continuous target variable t from a vector x of inputs (the extension to multiple targets is straightforward). We shall suppose that the conditional distribution p(t|x) is Gaussian, with an x-dependent mean given by the output of a neural network model y(x, w), and with precision (inverse variance) \u03b2 For an i.i.d. data set of N observations x1, . , xN, with a corresponding set of target values D = {t1,", "5d408743-0207-4b1b-8c94-920697fb6e54": "This gives rise to two terms, one of which cancels KL(q\u2225p) while the other gives the required log likelihood ln p(X|\u03b8) after noting that q(Z) is a normalized distribution that sums to 1.\n\nFrom (9.72), we see that KL(q\u2225p) is the Kullback-Leibler divergence between q(Z) and the posterior distribution p(Z|X, \u03b8). Recall that the Kullback-Leibler divergence satis\ufb01es KL(q\u2225p) \u2a7e 0, with equality if, and only if, q(Z) = p(Z|X, \u03b8). It Section 1.6.1 therefore follows from (9.70) that L(q, \u03b8) \u2a7d ln p(X|\u03b8), in other words that L(q, \u03b8) is a lower bound on ln p(X|\u03b8). The decomposition (9.70) is illustrated in FigThe EM algorithm is a two-stage iterative optimization technique for \ufb01nding maximum likelihood solutions. We can use the decomposition (9.70) to de\ufb01ne the EM algorithm and to demonstrate that it does indeed maximize the log likelihood", "57cbd845-e564-4903-8caf-b3a1aa692b02": "Throughout the book, we use two simple synthetic data sets to illustrate many of the algorithms. The \ufb01rst of these is a regression problem, based on the sinusoidal function, shown in Figure A.6. The input values {xn} are generated uniformly in range (0, 1), and the corresponding target values {tn} are obtained by \ufb01rst computing the corresponding values of the function sin(2\u03c0x), and then adding random noise with a Gaussian distribution having standard deviation 0.3. Various forms of this data set, having different numbers of data points, are used in the book. The second data set is a classi\ufb01cation problem having two classes, with equal prior probabilities, and is shown in Figure A.7. The blue class is generated from a single Gaussian while the red class comes from a mixture of two Gaussians.\n\nBecause we know the class priors and the class-conditional densities, it is straightforward to evaluate and plot the true posterior probabilities as well as the minimum misclassi\ufb01cation-rate decision boundary, as shown in Figure A.7. function from which the data points were generated", "65d29a12-679f-40e1-9351-4fcd2beb7e3a": "In this case, we can let the variational approximate posterior be a multivariate Gaussian with a diagonal covariance structure2: where the mean and s.d. of the approximate posterior, \u00b5(i) and \u03c3(i), are outputs of the encoding MLP, i.e. nonlinear functions of datapoint x(i) and the variational parameters \u03c6 (see appendix C). As explained in section 2.4, we sample from the posterior z(i,l) \u223c q\u03c6(z|x(i)) using z(i,l) = g\u03c6(x(i), \u03f5(l)) = \u00b5(i) + \u03c3(i) \u2299 \u03f5(l) where \u03f5(l) \u223c N(0, I). With \u2299 we signify an element-wise product. In this model both p\u03b8(z) (the prior) and q\u03c6(z|x) are Gaussian; in this case, we can use the estimator of eq. (7) where the KL divergence can be computed and differentiated without estimation (see appendix B)", "4be235eb-7caa-4c2c-95b1-7af72c94e1e0": "tla tte t ie 1 ML Ny  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    a set of 1UCaL CULULUIULaL pruvaviily UIDLLIVULIOLIS P(x\u2019 | \u00a9UY(x\")), where P aol x!) gives the parents of x? in Y. The probability distribution over X is given  by p(x) = Tips | Pag(x)). (16.1) In our relay race example, this means that, using the graph drawn in figure 16.2, p(to, ti, t2) = p(to)p(ti | to)p(te | t1). (16.2)  This is our first time seeing a structured probabilistic model in action. We can examine the cost of using it, to observe how structured modeling has many advantages relative to unstructured modeling. Suppose we represented time by discretizing time ranging from minute 0 to minute 10 into 6-second chunks. This would make to, t; and t2 each be a discrete variable with 100 possible values", "d28e6fc7-ce51-471e-8095-218db70df6e6": "Our probabilistic model speci\ufb01es the joint distribution p(X, Z), and our goal is to \ufb01nd an approximation for the posterior distribution p(Z|X) as well as for the model evidence p(X). As in our discussion of EM, we can decompose the log marginal probability using This differs from our discussion of EM only in that the parameter vector \u03b8 no longer appears, because the parameters are now stochastic variables and are absorbed into Z.\n\nSince in this chapter we will mainly be interested in continuous variables we have used integrations rather than summations in formulating this decomposition. However, the analysis goes through unchanged if some or all of the variables are discrete simply by replacing the integrations with summations as required. As before, we can maximize the lower bound L(q) by optimization with respect to the distribution q(Z), which is equivalent to minimizing the KL divergence. If we allow any possible choice for q(Z), then the maximum of the lower bound occurs when the KL divergence vanishes, which occurs when q(Z) equals the posterior distribution p(Z|X)", "dece5c6b-b10b-44e7-a2ce-906a0719de61": "Rather than training the model to maximize the likelihood, the model is trained to make each recurrent network obtain an accurate answer to the corresponding inference problem. The training process is illustrated in figure 20.5. It consists of randomly sampling a training example, randomly sampling a subset of inputs to the inference network, and then training the inference network to predict the values of the remaining units. This general principle of back-propagating through the computational graph for approximate inference has been applied to other models . In these models and in the MP-DBM, the final loss is not the lower bound on the likelihood. Instead, the final loss is typically based on  vd . 1 tae Pow. as sya ad . ote 1 1  https://www.deeplearningbook.org/contents/generative_models.html    Le Approximate CONCIUOLAL GISLLIDULION Lal Le approximate Wuerence LeEUWOrk imposes over the missing values. This means that the training of these models is somewhat heuristically motivated", "3b9a3e09-79c7-45e0-9dc1-6110456cbb56": "In this  https://www.deeplearningbook.org/contents/optimization.html    case, the generalization error (equation 8.2) can be written as a sum  = So Paata(aw, y)L( f(a: 9), y), (8.7) ey with the exact gradient  g =VoJ*( = y Paatalx, y)V oL( f(x; 9), y). (8.8)  We have already seen the same fact demonstrated for the log-likelihood in equa- tion 8.5 and equation 8.6; we observe now that this holds for other functions L besides the likelihood.\n\nA similar result can be derived when x and y are continuous, under mild assumptions regarding pgata and L.  Hence, we can obtain an unbiased estimator of the exact gradient of the generalization error by sampling a minibatch of examples {a), Lee al} with cor- responding targets y from the data-generating distribution pgata, then computing the gradient of the loss with respect to the parameters for that minibatch:  an oe v6), y). (8.9)  Updating @ in the direction of g performs SGD on the generalization error", "42fc62d0-878e-4df4-b3c0-7cb12745169c": "Conditional independence properties play an important role in using probabilistic models for pattern recognition by simplifying both the structure of a model and the computations needed to perform inference and learning under that model. We shall see examples of this shortly.\n\nIf we are given an expression for the joint distribution over a set of variables in terms of a product of conditional distributions (i.e., the mathematical representation underlying a directed graph), then we could in principle test whether any potential conditional independence property holds by repeated application of the sum and product rules of probability. In practice, such an approach would be very time consuming. An important and elegant feature of graphical models is that conditional independence properties of the joint distribution can be read directly from the graph without having to perform any analytical manipulations. The general framework for achieving this is called d-separation, where the \u2018d\u2019 stands for \u2018directed\u2019 . Here we shall motivate the concept of d-separation and give a general statement of the d-separation criterion. A formal proof can be found in Lauritzen . We begin our discussion of the conditional independence properties of directed graphs by considering three simple examples each involving graphs having just three nodes", "96799363-9e40-4335-8fb2-e954beb2746a": "441  CHAPTER 12. APPLICATIONS  12.1.3.\n\nLarge-Scale Distributed Implementations  In many cases, the computational resources available on a single machine are insufficient. We therefore want to distribute the workload of training and inference across many machines. Distributing inference is simple, because each input example we want to process can be run by a separate machine. This is known as data parallelism. It is also possible to get model parallelism, where multiple machines work together on a single data point, with each machine running a different part of the model. This is feasible for both inference and training. Data parallelism during training is somewhat harder. We can increase the size of the minibatch used for a single SGD step, but usually we get less than linear returns in terms of optimization performance. It would be better to allow multiple machines to compute multiple gradient descent steps in parallel. Unfortunately, the standard definition of gradient descent is as a completely sequential algorithm: the gradient at step t is a function of the parameters produced by step \u00a2t \u2014 1. This can be solved using asynchronous stochastic gradient descent . In this approach, several processor cores share the memory representing the parameters", "8c953bf1-4f04-4c09-9f03-1051948854ba": "For each data point xn, we introduce a corresponding set of binary indicator variables rnk \u2208 {0, 1}, where k = 1, . , K describing which of the K clusters the data point xn is assigned to, so that if data point xn is assigned to cluster k then rnk = 1, and rnj = 0 for j \u0338= k. This is known as the 1-of-K coding scheme. We can then de\ufb01ne an objective function, sometimes called a distortion measure, given by which represents the sum of the squares of the distances of each data point to its assigned vector \u00b5k. Our goal is to \ufb01nd values for the {rnk} and the {\u00b5k} so as to minimize J. We can do this through an iterative procedure in which each iteration involves two successive steps corresponding to successive optimizations with respect to the rnk and the \u00b5k. First we choose some initial values for the \u00b5k. Then in the \ufb01rst phase we minimize J with respect to the rnk, keeping the \u00b5k \ufb01xed.\n\nIn the second phase we minimize J with respect to the \u00b5k, keeping rnk \ufb01xed", "ad2820fc-6c76-4b55-9f76-955d4913f5b8": "Having solved the quadratic programming problem and found a value for a, we can then determine the value of the threshold parameter b by noting that any support vector xn satis\ufb01es tny(xn) = 1. Using (7.13) this gives where S denotes the set of indices of the support vectors. Although we can solve this equation for b using an arbitrarily chosen support vector xn, a numerically more stable solution is obtained by \ufb01rst multiplying through by tn, making use of t2 n = 1, and then averaging these equations over all support vectors and solving for b to give where NS is the total number of support vectors.\n\nFor later comparison with alternative models, we can express the maximummargin classi\ufb01er in terms of the minimization of an error function, with a simple quadratic regularizer, in the form where E\u221e(z) is a function that is zero if z \u2a7e 0 and \u221e otherwise and ensures that the constraints (7.5) are satis\ufb01ed. Note that as long as the regularization parameter satis\ufb01es \u03bb > 0, its precise value plays no role", "a4ee3b03-f0a8-457b-b241-122263364057": "For example, in Goethe\u2019s poem \u201cThe Sorcerer\u2019s Apprentice\u201d , the apprentice uses magic to enchant a broom to do his job of fetching water, but the result is an unintended \ufb02ood due to the apprentice\u2019s inadequate knowledge of magic. In the engineering context, Norbert Wiener, the founder of cybernetics, warned of this problem more than half a century ago by relating the supernatural story of \u201cThe Monkey\u2019s Paw\u201d : \u201c... it grants what you ask for, not what you should have asked for or what you intend\u201d (p. 59). The problem has also been discussed at length in a modern context by Nick Bostrom . Anyone having experience with reinforcement learning has likely seen their systems discover unexpected ways to obtain a lot of reward. Sometimes the unexpected behavior is good: it solves a problem in a nice new way. In other instances, what the agent learns violates considerations that the system designer may never have thought about", "27948716-fdd5-4aea-ba62-57674de288ff": "Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 2672\u20132680. Curran Associates, Inc. Demi Guo, Yoon Kim, and Alexander Rush. 2020. Sequence-level mixed sample data augmentation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5547\u20135552, Online. Association for Computational Linguistics. Ankush Gupta, Arvind Agarwal, Prawaan Singh, and Piyush Rai. 2017. A deep generative framework for paraphrase generation. Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, and Maosong Sun. 2018. FewRel: A large-scale supervised few-shot relation classi\ufb01cation dataset with state-of-the-art evaluation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4803\u2013 4809, Brussels, Belgium", "8f54fa09-24c7-4535-90f0-a8ae2773d564": "We can represent this data set as an N \u00d7 D from the joint distribution p(z)p(x|z) in which the three states of z, corresponding to the three components of the mixture, are depicted in red, green, and blue, and (b) the corresponding samples from the marginal distribution p(x), which is obtained by simply ignoring the values of z and just plotting the x values. The data set in (a) is said to be complete, whereas that in (b) is incomplete.\n\n(c) The same samples in which the colours represent the value of the responsibilities \u03b3(znk) associated with data point xn, obtained by plotting the corresponding point using proportions of red, blue, and green ink given by \u03b3(znk) for k = 1, 2, 3, respectively matrix X in which the nth row is given by xT n. Similarly, the corresponding latent variables will be denoted by an N \u00d7 K matrix Z with rows zT the data points are drawn independently from the distribution, then we can express the Gaussian mixture model for this i.i.d. data set using the graphical representation shown in Figure 9.6", "7f9f9ef9-16af-4da0-98da-1530a35d3d8f": "Everson (Eds. ), Independent Component Analysis: Principles and Practice. Cambridge University Press. Moody, J. and C. J. Darken . Fast learning in networks of locally-tuned processing units. Neural Computation 1(2), 281\u2013294. Moore, A. W. The anchors hierarch: using the triangle inequality to survive high dimensional data. In Proceedings of the Twelfth Conference on Uncertainty in Arti\ufb01cial Intelligence, pp. 397\u2013405. M\u00a8uller, K. R., S. Mika, G. R\u00a8atsch, K. Tsuda, and B. Sch\u00a8olkopf . An introduction to kernelbased learning algorithms. IEEE Transactions on Neural Networks 12(2), 181\u2013202. Nag, R., K. Wong, and F. Fallside . Script recognition using hidden markov models. In ICASSP86, pp", "4a522759-f721-46e0-9dcb-3c04451da13e": "Fast parameterization  ot = Fu (Voli, tee Vole) 7, = fo,o(2;)  Meta learner:  Slow weights @  Base learner:  Slow weights @  Fast parameterization $7 = Go(Vele\u2122 Meta Info  fo,o+  \u201cKey\u201d memory R= {ri}hy  /  Input  Output  \u2018Value\u201d Memory M= {7 Ha  https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Log   Training Process  Sample a random pair of inputs at each time step t from the support set S, (x), y/) and (xi, Yj). Let Xq@1) = x, and X(t2) = xi. fort =1,...,K:  a", "fa3b1689-9240-4744-bac9-3a73dd0d2c4f": "MCTS is largely responsible for the improvement in computer Go from a weak amateur level in 2005 to a grandmaster level (6 dan or more) in 2015. Many variations of the basic algorithm have been developed, including a variant that we discuss in Section 16.6 that was critical for the stunning 2016 victories of the program AlphaGo over an 18-time world champion Go player. MCTS has proved to be e\u21b5ective in a wide variety of competitive settings, including general game playing , but it is not limited to games; it can be e\u21b5ective for single-agent sequential decision problems if there is an environment model simple enough for fast multistep simulation.\n\nMCTS is executed after encountering each new state to select the agent\u2019s action for that state; it is executed again to select the action for the next state, and so on. As in a rollout algorithm, each execution is an iterative process that simulates many trajectories starting from the current state and running to a terminal state (or until discounting makes any further reward negligible as a contribution to the return). The core idea of MCTS is to successively focus multiple simulations starting at the current state by extending the initial portions of trajectories that have received high evaluations from earlier simulations", "4fbdbffa-9839-4977-b21c-641fe815865c": "= argmax V,(s), 7. = argmaxQ,(s, a) And of course, we have V,,. (s) = V.(s) and Q,,(s,a) = Q. (s, a). Markov Decision Processes  In more formal terms, almost all the RL problems can be framed as Markov Decision Processes (MDPs)", "d3e5b373-bb40-455d-9ec2-538e71bf83aa": "The optimal test error is found by trading off these quantities.\n\nNeural networks typically perform best when the training error is very low (and thus, when capacity is high) and the test error is primarily driven by the gap between training and test error. Your goal is to reduce this gap without increasing training error faster than the gap decreases. To reduce the gap, change regularization hyperparameters to reduce effective model capacity, such as by adding dropout or weight decay. Usually the best performance comes from a large model that is regularized well, for example, by using dropout. Most hyperparameters can be set by reasoning about whether they increase or decrease model capacity. Some examples are included in table 11.1. While manually tuning hyperparameters, do not lose sight of your end goal: good performance on the test set. Adding regularization is only one way to achieve this goal. As long as you have low training error, you can always reduce general- ization error by collecting more training data. The brute force way to practically guarantee success is to continually increase model capacity and training set size until the task is solved. This approach does of course increase the computational cost of training and inference, so it is only feasible given appropriate resources", "75b63e14-48c2-4807-b781-742465373d3f": "Another useful strategy for generative modeling worth mentioning is variational auto-encoders. The GAN framework can be extended to improve the quality of samples produced with variational auto-encoders . Variational auto-encoders learn a low- dimensional representation of data points. In the image domain, this translates an image tensor of size height x width x color channels down into a vector of size n x 1, identi- cal to what was discussed with respect to feature space augmentation.\n\nLow-dimensional constraints in vector representations will result in a poorer representation, although these constraints are better for visualization using methods such as t-SNE . Imag- ine a vector representation of size 5 x 1 created by an autoencoder. These autoencoders can take in a distribution of labeled data and map them into this space. These classes could include \u2018head turned left; \u2018centered head, and \u2018head turned right: The auto-encoder learns a low-dimensional representation of these data points such that vector operations such as adding and subtracting can be used to simulate a front view-3D rotation of a new instance", "dc0274c2-35f7-40a2-8f74-36e12f42a516": "If a value 7 is missing, and all the other values, denoted \u00ae-i, are given, then we know the distribution over it is given by p(2; | xi). In practice, density estimation does not always enable us to solve all these related tasks, because in many cases the required operations on p(a) are computationally intractable. Of course, many other tasks and types of tasks are possible. The types of tasks we list here are intended only to provide examples of what machine learning can do, not to define a rigid taxonomy of tasks. 5.1.2. The Performance Measure, P  To evaluate the abilities of a machine learning algorithm, we must design a quantitative measure of its performance. Usually this performance measure P is specific to the task T being carried out by the system. For tasks such as classification, classification with missing inputs, and tran- scription, we often measure the accuracy of the model. Accuracy is just the  101  CHAPTER 5. MACHINE LEARNING BASICS  proportion of examples for which the model produces the correct output.\n\nWe can also obtain equivalent information by measuring the error rate, the proportion of examples for which the model produces an incorrect output", "9431e36c-b87e-485f-a254-4f9ffb0d0aa7": "It has become increasingly ubiquitous to manipulate data to improve learning, especially in low data regime or in presence of low-quality datasets (e.g., imbalanced labels).\n\nFor example, data augmentation applies label-preserving transformations on original data points to expand the data size; data weighting assigns an importance weight to each instance to adapt its effect on learning; and data synthesis generates entire arti\ufb01cial examples. Different types of manipulation can be suitable for different application settings. Common data manipulation methods are usually designed manually, e.g., augmenting by \ufb02ipping an image or replacing a word with synonyms, and weighting with inverse class frequency or loss values . Recent work has studied automated approaches, such as learning the composition of augmentation operators with reinforcement learning , deriving sample weights adaptively from a validation set via meta learning , or learning a weighting network by inducing a curriculum . These learning-based approaches have alleviated the engineering burden and produced impressive results. However, the algorithms are usually designed speci\ufb01cally for certain types of manipulation (e.g., either augmentation or weighting) and thus have limited application scope in practice. In this work, we propose a new approach that enables learning for different manipulation schemes with the same single algorithm", "a2a8f339-3009-4432-a154-0b3824bc2616": "Softmax classifier  4  MLP(f(z), f(2'),|f(@) \u2014 f(@\u2019)))  siamese network: two towers have shared weights  f(x) f(e') eoropeooes E Sitaklake stage tetera!\n\nC siaietatolateieateeh ' pooling pooling | z x ' BERT BERT Sentence x Sentence x\u2019  -1..  1  4 cos( f(z), f(2\u2019))  f(x) f(z')  ry ry pooling pooling  r 3  BERT BERT Sentence \u00a9 Sentence \u00ab\u2019  The SentEval library  is commonly used for evaluating the quality of  earned sentence embedding. SBERT outperformed other baselines at that time  on 5  out of 7 tasks. Model MR CR | SUBJ | MPQA | SST | TREC | MRPC || Avg. Avg", "cc480821-6662-4937-ac15-1cf0615304a8": "Figure 8.6 illustrates how the method of steepest descent, when applied in a quadratic bowl, progresses in a rather ineffective back-and-forth zig-zag pattern. This happens because each line search direction, when given by the gradient, is guaranteed to be orthogonal to the previous line search direction. Let the previous search direction be dt-1. At the minimum, where the line search terminates, the directional derivative is zero in direction dy: VeJ(0) - d,-1 = 0. Since the gradient at this point defines the current search direction, d, = VoJ(@) will have no contribution in the direction d,_,. Thus d; is orthogonal to d,_,;.\n\nThis relationship between d;_; and d; is illustrated in figure 8.6 for multiple iterations of steepest descent. As demonstrated in the figure, the choice of orthogonal directions of descent do not preserve the minimum along the previous search directions", "5ea0d289-c5d0-4985-b569-3d373adf9166": "The use of cross-entropy losses greatly improved the performance of models with sigmoid and softmax outputs, which had previously suffered from saturation and slow learning when using the mean squared error loss. The other major algorithmic change that has greatly improved the performance  https://www.deeplearningbook.org/contents/mlp.html    of feedforward networks was the replacement of sigmoid hidden units with piecewis linear hidden units, such as rectified linear units. Rectification using the max{0, 2 function was introduced in early neural network models and dates back at least as far  as the cognitron and neocognitron . These early models did not use rectified linear units but instead applied rectification to nonlinear functions. Despite the early popularity of rectification, it was largely replaced by sigmoids in the 1980s, perhaps because sigmoids perform better when neural networks are very small. As of the early 2000s, rectified linear units were avoided because of a somewhat superstitious belief that activation functions with nondifferentiable points must be avoided. This began to change in about 2009", "b9b42bd1-47b5-4c62-9b0f-592863b27f05": "MACHINE LEARNING BASICS  oc exp (=H ~Xw)' (y\u2014 Xw)) exp (-3tw ~ po) AQ w \u2014 H0)) (5.75)  1 x exp (-3 (-2y\"Xw +wiX'Xw+ w! Ag w \u2014 241) Ay\") ) : (5.76)  We now define Ay, = (XTX +A51) 1 and py = Am (XTy + Ap !uo). Us- ing these new variables, we find that the posterior may be rewritten as a Gaussian distribution:  1 _ 1 _ view | X.y) oc exp (= 5(0 = pn)\" ApH) + 5H Anlst) (5.7)  x exp (= 5(1 = Hn)\" AGM Hm) (5.78)  All terms that do not include the parameter vector w have been omitted; they are implied by the fact that the distribution must be normalized to integrate to 1.\n\nEquation 3.23 shows how to normalize a multivariate Gaussian distribution. Examining this posterior distribution enables us to gain some intuition for the effect of Bayesian inference", "fddd4c55-8c47-4f8a-8b8d-50123b2ea726": "An image can be quickly converted into its representation in one color channel by isolating that matrix and adding 2 zero matrices from the other color channels.\n\nAdditionally, the RGB values can be easily manipulated with simple matrix operations to increase or decrease the brightness of the image. More advanced color augmentations come from deriving a color histogram describing the image. Changing the intensity values in these histograms results in lighting alterations  such as what is used in photo editing applications. Cropping  Cropping images can be used as a practical processing step for image data with mixed height and width dimensions by cropping a central patch of each image. Additionally, random cropping can also be used to provide an effect very similar to translations. The contrast between random cropping and translations is that cropping will reduce the size of the input such as (256,256) \u2014 (224, 224), whereas translations preserve the spatial dimensions of the image. Depending on the reduction threshold chosen for  cropping, this might not be a label-preserving transformation. Rotation  Rotation augmentations are done by rotating the image right or left on an axis between 1\u00b0 and 359\u00b0. The safety of rotation augmentations is heavily determined by the rotation degree parameter", "80d69e64-bda8-4df3-ae34-49406b94fac2": "\u21e4 The importance sampling that we have used in this section, the previous section, and in Chapter 5, enables sound o\u21b5-policy learning, but also results in high variance updates, forcing the use of a small step-size parameter and thereby causing learning to be slow. It is probably inevitable that o\u21b5-policy training is slower than on-policy training\u2014after all, the data is less relevant to what is being learned. However, it is probably also true that these methods can be improved on. The control variates are one way of reducing the variance. Another is to rapidly adapt the step sizes to the observed variance, as in the Autostep method .\n\nYet another promising approach is the invariant updates of Karampatziakis and Langford  as extended to TD by Tian (in preparation). The usage technique of Mahmood (2017; Mahmood and Sutton, 2015) may also be part of the solution. In the next section we consider an o\u21b5-policy learning method that does not use importance sampling", "a90dec4c-1277-45d7-8438-cab4240dded5": "Many real-world use cases of machine learning involve multiple related classi\ufb01cation tasks\u2014both because there are multiple tasks of interest, and because available weak supervision sources may in fact label different related tasks. Handling this multi-task weak supervision setting has been the focus of recent work on a new version of Snorkel, Snorkel MeTaL,21 which handles labeling functions that label different tasks, and in turn can be used to supervise popular multi-task learning (MTL) discriminative models .\n\nFor example, we might be aiming to train a \ufb01ne-grained named entity recognition (NER) system which tags speci\ufb01c types of people, places, and things, and have access to both \ufb01ne-grained labeling functions\u2014e.g., that label doctors versus lawyers\u2014and coarse-grained ones, e.g., that label people versus organizations. By representing these as different logically-related tasks, we can model and combine these In addition to working on the core directions outlined\u2014realworld deployment, higher-level interfaces, and multi-task supervision\u2014severalotherdirectionsarenaturalandexciting extensions of Snorkel", "d2777764-8389-49f1-937f-e4a16d29e512": "70.16 = 69.21 64.25 66.58 + SimCSE-BERT\u00bbase 66.68 81.43 71.38 78.43 78.47 75.49 69.92 74.54 RoBERTapase (first-last avg.)\n\n40.88 58.74 49.07 65.63 61.48 58.55 61.63 56.57 RoBERTa,s\u00a2-whitening 46.99 63.24 57.23 71.36 68.99 61.36 62.91 61.73 * SimCSE-RoBERTayase 68.68 82.62 73.56 81.49 80.82 80.48 6787 76.50 * SimCSE-RoBERTajarge 69.87 82.97 74.25 83.01 79.52 81.23 71.47 77.47 Supervised models InferSent-GloVe* 52.86 66.75 62.15 72.77 66.87 68.03 65.65 65.01 Universal Sentence Encoder* 64.49 67.80 64.61 76.83 73.18 74.92 76.69 71.22 SBERTyase* 70.97 76.53 73.19 79.09 74.30 77.03 72.91 74.89 SBERT>2s\u00a2-flow 69.78 7127 74.35 82.01 7746 79.12 76.21 76.60 SBERT;,", "d54e5854-c986-428e-b5ef-a68713994e0f": "The promise of machine learning as a means for achieving this has been frustrated by the need to craft problem-speci\ufb01c representations. DeepMind\u2019s DQN stands as a major step forward by demonstrating that a single agent can learn problem-speci\ufb01c features enabling it to acquire humancompetitive skills over a range of tasks. This demonstration did not produce one agent that simultaneously excelled at all the tasks (because learning occurred separately for each task), but it showed that deep learning can reduce, and possibly eliminate, the need for problem-speci\ufb01c design and tuning. As Mnih et al. point out, however, DQN is not a complete solution to the problem of task-independent learning. Although the skills needed to excel on the Atari games were markedly diverse, all the games were played by observing video images, which made a deep convolutional ANN a natural choice for this collection of tasks. In addition, DQN\u2019s performance on some of the Atari 2600 games fell considerably short of human skill levels on these games", "94515580-343a-43c1-9f1d-5227db93218f": "The input always sends a fixed message of v' 8W to the hidden units, but the hidden units constantly update the message they send to each other. Specifically, two units hi and h j inhibit each other when their weight vectors are aligned. This is a form of competition\u2014between two hidden units that both explain the input, only the one that explains the input best will be allowed to remain active. This competition is the mean field approximation\u2019s attempt to capture the explaining away interactions in the binary sparse coding posterior. The explaining away effect actually should cause a multimodal posterior, so that if we draw samples from the posterior, some samples will have one unit active, other samples will have the other unit active, but very few samples will have both active. Unfortunately, explaining away interactions cannot be modeled by the factorial g used for mean field, so the mean field approximation is forced to choose one mode to model. This is an instance of the behavior illustrated in figure 3.6.\n\nWe can rewrite equation 19.44 into an equivalent form that reveals some further  insights:  +  1 hj=o b+ v\u2014  Wajh; BW.4\u2014 5W. BW", "34fdba1a-0137-4769-b41d-c125b7dd5bde": "In (b), the joint distribution is given by a general form p(x) = f(x1, x2, x3), whereas in (c), it is given by the more speci\ufb01c factorization p(x) = fa(x1, x2)fb(x1, x3)fc(x2, x3).\n\nIt should be emphasized that the factorization in (c) does not correspond to any conditional independence properties. We shall now make use of the factor graph framework to derive a powerful class of ef\ufb01cient, exact inference algorithms that are applicable to tree-structured graphs. Here we shall focus on the problem of evaluating local marginals over nodes or subsets of nodes, which will lead us to the sum-product algorithm. Later we shall modify the technique to allow the most probable state to be found, giving rise to the max-sum algorithm. Also we shall suppose that all of the variables in the model are discrete, and so marginalization corresponds to performing sums. The framework, however, is equally applicable to linear-Gaussian models in which case marginalization involves integration, and we shall consider an example of this in detail when we discuss linear dynamical systems", "ac448a9e-93f8-4585-8138-446ddec0c0b5": "Including higher-order interactions and average-pooling of the slab variables  enables the model to learn excellent features for a classifier when labeled data is scarce. Adding a term to the energy function that prevents the partition function from becoming  https://www.deeplearningbook.org/contents/generative_models.html    undefined results in a sparse coding model, spike and slab sparse coding , also known as S3C  20.6 Convolutional Boltzmann Machines  As we discuss in chapter 9, extremely high-dimensional inputs such as images place great strain on the computation, memory and statistical requirements of machine learning models. Replacing matrix multiplication by discrete convolution with a small kernel is the standard way of solving these problems for inputs that have translation invariant spatial or temporal structure. Desjardins and Bengio   679  CHAPTER 20. DEEP GENERATIVE MODELS  showed that this approach works well when applied to RBMs.\n\nDeep convolutional networks usually require a pooling operation so that the spatial size of each successive layer decreases. Feedforward convolutional networks often use a pooling function such as the maximum of the elements to be pooled", "973dbd16-1aca-49d0-bf9e-414c5dd3aba7": "Let x be the target sample ~ P(x|C = 1; 6) = po(x) and X be the noise sample  ~ P(&|C = 0) = \u00a2q(X). Note that the logistic regression models the logit (i.e. log-odds) and in this case we would like to model the logit of a sample u from the target data distribution instead of the noise distribution:  \u00a3(u) = log ot = log pe(u) \u2014 log q(u)  https://lilianweng.github.io/posts/2021-05-31-contrastive/  Contrastive Representation Learning | Lil'Log   After converting logits into probabilities with sigmoid o(. ), we can apply cross entropy loss:  Cree = - xd log o(lo(x,)) + log(1 \u2014 o(E0(%:)))]  i=l  Hb  Po  where o(\u00a3) = 1+ exp(\u2014d) = beng  Here | listed the original form of NCE loss which works with only one positive and one noise sample", "650a01ea-197e-4398-872b-fb92b3a1aaa0": "Section 13.3 The hidden Markov model can be viewed as a speci\ufb01c instance of the state space model of Figure 13.5 in which the latent variables are discrete. However, if we examine a single time slice of the model, we see that it corresponds to a mixture distribution, with component densities given by p(x|z). It can therefore also be interpreted as an extension of a mixture model in which the choice of mixture component for each observation is not selected independently but depends on the choice of component for the previous observation. The HMM is widely used in speech recognition , natural language modelling , on-line handwriting recognition , and for the analysis of biological sequences such as proteins and DNA . As in the case of a standard mixture model, the latent variables are the discrete multinomial variables zn describing which component of the mixture is responsible for generating the corresponding observation xn. Again, it is convenient to use a 1-of-K coding scheme, as used for mixture models in Chapter 9", "a7bd6c13-1f8f-4db5-92a8-67a183a0f200": "In the bandit algorithms the baseline was just a number (the average of the rewards seen so far), but for MDPs the baseline should vary with state.\n\nIn some states all actions have high values and we need a high baseline to di\u21b5erentiate the higher valued actions from the less highly valued ones; in other states all actions will have low values and a low baseline is appropriate. One natural choice for the baseline is an estimate of the state value, \u02c6v(St,w), where w 2 Rd is a weight vector learned by one of the methods presented in previous chapters. Because REINFORCE is a Monte Carlo method for learning the policy parameter, \u2713, it seems natural to also use a Monte Carlo method to learn the state-value weights, w. A complete pseudocode algorithm for REINFORCE with baseline using such a learned state-value function as the baseline is given in the box below", "4d2569ad-2247-444c-9627-c9bbc0916d5d": "In relevant recent work on autoencoders  it was shown that the training criterion of unregularized autoencoders corresponds to maximization of a lower bound (see the infomax principle ) of the mutual information between input X and latent representation Z. Maximizing (w.r.t. parameters) of the mutual information is equivalent to maximizing the conditional entropy, which is lower bounded by the expected loglikelihood of the data under the autoencoding model , i.e. the negative reconstrution error. However, it is well known that this reconstruction criterion is in itself not suf\ufb01cient for learning useful representations . Regularization techniques have been proposed to make autoencoders learn useful representations, such as denoising, contractive and sparse autoencoder variants . The SGVB objective contains a regularization term dictated by the variational bound (e.g. eq. (10)), lacking the usual nuisance regularization hyperparameter required to learn useful representations", "87227887-868b-4720-812d-606ccf719916": "The prediction of the ensemble is given by the arithmetic mean of all  these distributions, k  i p k SS (y|@). (7.52)  https://www.deeplearningbook.org/contents/regularization.html    \u2014  In the case of dropout, each submodel defined by mask vector H defines a probability distribution p(y (y | wp) pu). The arithmetic mean over all masks is given by  rw) ply | x, 1), (7.53)  where p(s) is the probability distribution that was used to sample yz at training time. Because this sum includes an exponential number of terms, it is intractable to evaluate except when the structure of the model permits some form of simplification. So far, deep neural nets are not known to permit any tractable simplification. Instead, we can approximate the inference with sampling, by averaging together he output from many masks.\n\nEven 10-20 masks are often sufficient to obtain good performance. An even better approach, however, allows us to obtain a good approximation to he predictions of the entire ensemble, at the cost of only one forward propagation", "2a79d43d-5048-4c8a-aef7-6ea3028e5cff": "What is the space of methods lying between Monte Carlo and TD methods? Consider estimating v\u21e1 from sample episodes generated using \u21e1. Monte Carlo methods perform an update for each state based on the entire sequence of observed rewards from that state until the end of the episode. The update of one-step TD methods, on the other hand, is based on just the one next reward, bootstrapping from the value of the state one step later as a proxy for the remaining rewards. One kind of intermediate method, then, would perform an update based on an intermediate number of rewards: more than one, but less than all of them until termination. For example, a two-step update would be based on the \ufb01rst two rewards and the estimated value of the state two steps later.\n\nSimilarly, we could have three-step updates, four-step updates, and so on. Figure 7.1 shows the backup diagrams of the spectrum of n-step updates for v\u21e1, with the one-step TD update on the left and the up-until-termination Monte Carlo update on the right. The methods that use n-step updates are still TD methods because they still change an earlier estimate based on how it di\u21b5ers from a later estimate", "8aab867b-cb51-44fd-8c50-22444eb3afdb": "To do so, we change to using the geometric mean rather than the arithmetic mean of he ensemble members\u2019 predicted distributions. Warde-Farley et al. present arguments and empirical evidence that the geometric mean performs comparably (0 the arithmetic mean in this context. The geometric mean of multiple probability distributions is not guaranteed to be a probability distribution. To guarantee that the result is a probability distribution, we impose the requirement that none of the submodels assigns probability 0 to any event, and we renormalize the resulting distribution. The unnormalized probability distribution defined directly by the geometric mean is given by  Pensemble( y | x) = 2d Ifo y | x Ht); (7.54)  where d is the number of units that may be dropped. Here we use a uniform distribution over yz to simplify the presentation, but nonuniform distributions are also possible. To make predictions we must renormalize the ensemble:  Pensemble (y | x) Ly Pensemble(y\u2019 | xv) 259  Pensemble(Y | x) = (7.55)  CHAPTER 7", "eade1691-a381-4fdd-aaf6-0d4a57d356e8": "it eventually makes $$\\text{SGD}(\\mathbb{E} |tau, \\theta, k)diverge from\\mathbb{E}|tau $$ when k > 1. The Optimization Assumption  https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Log   Assuming that a task T ~ p(T) has a manifold of optimal network configuration, W>. The model fg achieves the best performance for task 7 when @ lays on the surface of W*. To find a solution that is good across tasks, we would like to find a parameter close to all the optimal manifolds of all tasks:  0* = arg min En p(r) = Vol; dist(6, W:(6))?] 1 * = Vol 5 (0\u2014 W7(9))\" =60- W;. (0) ; See notes", "56f4affd-cdc4-4a8a-b90b-19e027ae882c": "Chapter 12  Applications  In this chapter, we describe how to use deep learning to solve applications in computer vision, speech recognition, natural language processing, and other areas of commercial interest. We begin by discussing the large-scale neural network implementations required for most serious AI applications. Next, we review several specific application areas that deep learning has been used to solve. While one goal of deep learning is to design algorithms that are capable of solving a broad variety of tasks, so far some degree of specialization is needed. For example, vision tasks require processing a large number of input features (pixels) per example. Language tasks require modeling a large number of possible values (words in the vocabulary) per input feature. 12.1 Large-Scale Deep Learning  Deep learning is based on the philosophy of connectionism: while an individual biological neuron or an individual feature in a machine learning model is not intelligent, a large population of these neurons or features acting together can exhibit intelligent behavior. It truly is important to emphasize the fact that the number of neurons must be large", "fcf866bd-a348-4d44-b89f-ca7e0f02b050": "Because each component is either 0 or 1, the weighted sum making up the approximate value function (9.8) is almost trivial to compute. Rather than performing d multiplications and additions, one simply computes the indices of the n \u2327 d active features and then adds up the n corresponding components of the weight vector. Generalization occurs to states other than the one trained if those states fall within any of the same tiles, proportional to the number of tiles in common. Even the choice of how to o\u21b5set the tilings from each other a\u21b5ects generalization. If they are o\u21b5set uniformly in each dimension, as they were in Figure 9.9, then di\u21b5erent states can generalize in qualitatively di\u21b5erent ways, as shown in the upper half of Figure 9.11. Each of the eight sub\ufb01gures show the pattern of generalization from a trained state to nearby points.\n\nIn this example there are eight tilings, thus 64 subregions within a tile that generalize distinctly, but all according to one of these eight patterns. Note how uniform o\u21b5sets result in a strong e\u21b5ect along the diagonal in many patterns", "55c729b4-be21-47ca-a95c-dd48ca5dd1f1": "Reinforcement Learning. Kluwer Academic Press. Reprinting of a special double issue on reinforcement learning, Machine Learning, 8(3-4). Sutton, R.S. Adapting bias by gradient descent: An incremental version of delta-barSutton, R.S. Gain adaptation beats least squares? Proceedings of the Seventh Yale Workshop on Adaptive and Learning Systems, pp.\n\n161\u2013166, Yale University, New Haven, CT. Sutton, R. S. TD models: Modeling the world at a mixture of time scales. In Proceedings Proceedings of the Workshop on Value Function Approximation at The 12th International Conference on Machine Learning . sparse coarse coding. In Advances in Neural Information Processing Systems 8 , Sutton, R. S. The grand challenge of predictive empirical abstract knowledge. Working Notes of the IJCAI-09 Workshop on Grand Challenges for Reasoning from Experiences. Sutton, R. S.  Introduction to reinforcement learning with function approximation. Sutton, R. S", "ee369105-f594-4baa-9695-69b1b34ae393": "Some other unsupervised learning algorithms  https://www.deeplearningbook.org/contents/ml.html    perform other roles, like clustering, which consists of dividing the dataset into clusters of similar examples. Supervised learning algorithms experience a dataset containing features, but each example is also associated with a label or target. For example, the Iris dataset is annotated with the species of each iris plant. A supervised learning algorithm can study the Iris dataset and learn to classify iris plants into three different species based on their measurements.\n\nRoughly speaking, unsupervised learning involves observing several examples of a random vector x and attempting to implicitly or explicitly learn the proba- bility distribution p(x), or some interesting properties of that distribution; while supervised learning involves observing several examples of a random vector x and an associated value or vector y, then learning to predict y from x, usually by estimating p(y | x). The term supervised learning originates from the view of the target y being provided by an instructor or teacher who shows the machine learning system what to do. In unsupervised learning, there is no instructor or teacher, and the algorithm must learn to make sense of the data without this guide", "987494a8-9a88-468e-82a3-a4cec7a28c5f": "Maji, S., Kannala, J., Rahtu, E., Blaschko, M., and Vedaldi, A. Fine-grained visual classi\ufb01cation of aircraft. Technical report, 2013. Mikolov, T., Chen, K., Corrado, G., and Dean, J. Ef\ufb01cient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n\nNilsback, M.-E. and Zisserman, A. Automated \ufb02ower classi\ufb01cation over a large number of classes. In Computer Vision, Graphics & Image Processing, 2008. ICVGIP\u201908. Sixth Indian Conference on, pp. 722\u2013729. IEEE, 2008. Noroozi, M. and Favaro, P. Unsupervised learning of visual representations by solving jigsaw puzzles. In European Conference on Computer Vision, pp. 69\u201384. Springer, 2016. Oord, A. v", "c206669f-2a70-474c-a4db-78aa9a6eaeae": "Neuroscience, 8(4):791\u2013797. Programming for Feedback Control. John Wiley and Sons. Lewis, R. L., Howes, A., Singh, S. Computational rationality: Linking mechanism and behavior through utility maximization. Topics in Cognitive Science, 6(2):279\u2013311. Li, L. Sample complexity bounds of exploration. In M. Wiering and M. van Otterlo (Eds. ), Reinforcement Learning: State-of-the-Art, pp. 175\u2013204. Springer-Verlag Berlin Heidelberg. Li, L., Chu, W., Langford, J., Schapire, R. E. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th International Conference on World Wide Web, pp. 661\u2013670. ACM, New York. Lin, C.-S., Kim, H. CMAC-based adaptive critic self-learning control. IEEE Transactions Lin, L.-J", "26cc0b25-29b1-470d-b9ba-fa27857ac957": "The conditional distribution is no longer a k x k \u2014 1 element table indexed by to and t1 but is now a slightly more complicated formula using only k \u2014 1 parameters.\n\nThe directed graphical model syntax does not place any constraint on how we define our conditional distributions. It only defines which variables they are allowed to take in as arguments. 16.2.2 Undirected Models  Directed graphical models give us one language for describing structured probabilis- tic models. Another popular language is that of undirected models, otherwise  562  CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  known as Markov random fields (MRFs) or Markov networks . As their name implies, undirected models use graphs whose edges are undirected. Directed models are most naturally applicable to situations where there is a clear reason to draw each arrow in one particular direction. Often these are situations where we understand the causality, and the causality flows in only one direction. One such situation is the relay race example. Earlier runners affect the finishing times of later runners; later runners do not affect the finishing times of earlier runners", "1d8ebd89-7512-40e8-873a-6d8eee4dbb63": "When we go to a much higher order polynomial (M = 9), we obtain an excellent \ufb01t to the training data. In fact, the polynomial passes exactly through each data point and E(w\u22c6) = 0. However, the \ufb01tted curve oscillates wildly and gives a very poor representation of the function sin(2\u03c0x). This latter behaviour is known as over-\ufb01tting. As we have noted earlier, the goal is to achieve good generalization by making accurate predictions for new data. We can obtain some quantitative insight into the dependence of the generalization performance on M by considering a separate test set comprising 100 data points generated using exactly the same procedure used to generate the training set points but with new choices for the random noise values included in the target values. For each choice of M, we can then evaluate the residual value of E(w\u22c6) given by (1.2) for the training data, and we can also evaluate E(w\u22c6) for the test data set.\n\nIt is sometimes more convenient to use the root-mean-square in which the division by N allows us to compare different sizes of data sets on an equal footing, and the square root ensures that ERMS is measured on the same scale (and in the same units) as the target variable t", "c97edd71-41b4-41dd-9d52-fad5ac1cde11": "In many contexts, the squared L? norm may be undesirable because it increases very slowly near the origin. In several machine learning applications, it is important to discriminate between elements that are exactly zero and elements that are small but nonzero. In these cases, we turn to a function that grows at the  37  CHAPTER 2. LINEAR ALGEBRA  same rate in all locations, but that retains mathematical simplicity: the L! norm. The L! norm may be simplified to  \\|a||. = |2; |- (2.31)  4  The L! norm is commonly used in machige/learning when the difference between zero and nonzero elements is very important.\n\nEvery time an element of 2 moves  https://www.deeplearningbook.org/contents/linear_algebra.html    away from 0 by e, the L* norm increases by \u20ac. We sometimes measure the size of the vector by counting its number of nonzero  elements. Some authors refer to this function as the \u201c LY norm,\u201d but this is incorrect  terminology. The number of nonzero entries in a vector is not a norm, because scaling the vector by @ does not change the number of nonzero entries", "c9eea858-59ff-4f92-8f1e-7306e704e48f": "--.4bn- A Ann nbn 2M iL 2 ne 1 alee dt Lath an 22d LL -e eet  https://www.deeplearningbook.org/contents/regularization.html    LLC VECLUL Y UCLIULES All LUC PaLALUCLeLs, LUCLUUINY DULL Ww alld LUE ULLLCYULALIZCU parameters.\n\nIn the context of neural networks, it is sometimes desirable to use a separate penalty with a different a coefficient for each layer of the network. Because it can  be expensive to search for the correct value of multiple hyperparameters, it is still reasonable to use the same weight decay at all layers just to reduce the size of search space. 226  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  7.1.1 L? Parameter Regularization  We have already seen, in section 5.2.2, one of the simplest and most common kinds of parameter norm penalty: the L? parameter norm penalty commonly known as weight decay. This regularization strategy drives the weights closer to the origin! by adding a regularization term 2(6) = 3 ||w||3 to the objective function", "78bbe931-364a-4820-a477-a6c19ad5d6f2": "This large cost would be incurred because the same computation for ous would be redone many times.\n\nTo avoid such recomputation, we can think of back-propagation as a table-filling algorithm that takes advantage of storing au table to store the gradient for that node. By filling in these table entries in order, back-propagation avoids repeating many common subexpressions. This table-filling strategy is sometimes called dynamic programming. intermediate results  Each node in the graph has a corresponding slot in a  https://www.deeplearningbook.org/contents/mlp.html    214  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  6.5.7 Example: Back-Propagation for MLP Training  As an example, we walk through the back-propagation algorithm as it is used to train a multilayer perceptron. Here we develop a very simple multilayer perceptron with a single hidden layer. To train this model, we will use minibatch stochastic gradient descent. The back-propagation algorithm is used to compute the gradient of the cost on a single minibatch", "070f5f0b-5975-4113-b3af-535055aeedd6": "Without a specific way to do so, the two networks  https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/  Self-supervised learning: The dark matter of intelligence   could happily ignore their inputs and always produce identical output embeddings. This phenomenon is called a collapse. When a collapse occurs, the energy is not higher for nonmatching x and y than it is for matching x and y. There are two categories of techniques to avoid collapse: contrastive methods and regularization methods. Contrastive energy-based SSL  Contrastive methods are based on the simple idea of constructing pairs of x and y that are not compatible, and adjusting the parameters of the model so that the corresponding output energy is large", "54e5ac80-408a-4b22-8cea-e5a2fe4d3dd3": "Nevertheless, for most practical purposes it may be adequate simply to order policies according to their average reward per time step, in other words, according to their r(\u21e1). This quantity is essentially the average reward under \u21e1, as suggested by (10.7). In particular, we consider all policies that attain the maximal value of r(\u21e1) to be optimal. Note that the steady state distribution is the special distribution under which, if you select actions according to \u21e1, you remain in the same distribution. That is, for which In the average-reward setting, returns are de\ufb01ned in terms of di\u21b5erences between This is known as the di\u21b5erential return, and the corresponding value functions are known as di\u21b5erential value functions. They are de\ufb01ned in the same way and we will use the same notation for them as we have all along: v\u21e1(s) .= E\u21e1 and q\u21e1(s, a) .= E\u21e1 (similarly for v\u21e4 and q\u21e4)", "7e26a6c7-b83e-47a0-9ff8-9cf6981fa3dd": "Chapter 4  Numerical Computation  Machine learning algorithms usually require a high amount of numerical compu- tation. This typically refers to algorithms that solve mathematical problems by methods that update estimates of the solution via an iterative process, rather than analytically deriving a formula to provide a symbolic expression for the correct solution. Common operations include optimization (finding the value of an argument that minimizes or maximizes a function) and solving systems of linear equations. Even just evaluating a mathematical function on a digital computer can be difficult when the function involves real numbers, which cannot be represented precisely using a finite amount of memory. 4.1 Overflow and Underflow  The fundamental difficulty in performing continuous math on a digital computer is that we need to represent infinitely many real numbers with a finite number of bit patterns. This means that for almost all real numbers, we incur some approximation error when we represent the number in the computer. In many cases, this is just rounding error. Rounding error is problematic, especially when it compounds across many operations, and can cause algorithms that work in theory to fail in practice if they are not designed to minimize the accumulation of rounding error. One form of rounding error that is particularly devastating is underflow", "9f5575fb-ef85-40ff-bde5-7a197c14bdf6": "Usually, when we work with data on a computer, time will be discretized, and our sensor will provide data at regular intervals. In our example, it might be more realistic to assume that our laser provides a measurement once per second. The time index t can then take on only integer values. If we now assume that x and w are defined only on integer t, we can define the discrete convolution:  s(t) =(v*w)(t)= SY > x(a)jw(t\u2014a). (9.3)  In machine learning applications, the input is usually a multidimensional array of data, and the kernel is usually a multidimensional array of parameters that are adapted by the learning algorithm. We will refer to these multidimensional arrays as tensors. Because each element of the input and kernel must be explicitly stored separately, we usually assume that these functions are zero everywhere but in the finite set of points for which we store the values. This means that in practice, we can implement the infinite summation as a summation over a finite number of array elements.\n\nFinally, we often use convolutions over more than one axis at a time", "b8e62230-d2ef-44ec-bfb1-e72e1dc99db5": "o* = arg min \u00bb Li (far) ~ arg min \u00bb LY (fy avvoc! ir) Ti~p(T) Ti~p(T)  64+ 6- BV 6 S LY (Fy avyc gy) ; updating rule  Ti~p(T)  Algorithm 1 Model-Agnostic Meta-Learning Require: p(7): distribution over tasks Require: a, @: step size hyperparameters  1: randomly initialize 0  2: while not done do  3: Sample batch of tasks 7; ~ p(7)  4 for all 7; do 5: Evaluate V \u00a37; (fo) with respect to K examples 6 Compute adapted parameters with gradient de-  scent: 0! = 0 \u2014 aVoLl7,(fo) end for Note: the meta-update is using different set of data", "d299bcb3-e13a-4660-befd-2500fd0cb4e4": "It is important to note that a factor node can send a message to a variable node once it has received incoming messages from all other neighbouring variable nodes. Finally, we derive an expression for evaluating the messages from variable nodes to factor nodes, again by making use of the (sub-)graph factorization. From Figure 8.48, we see that term Gm(xm, Xsm) associated with node xm is given by a product of terms Fl(xm, Xml) each associated with one of the factor nodes fl that is linked to node xm (excluding node fs), so that where the product is taken over all neighbours of node xm except for node fs.\n\nNote that each of the factors Fl(xm, Xml) represents a subtree of the original graph of precisely the same kind as introduced in (8.62). Substituting (8.68) into (8.67), we where we have used the de\ufb01nition (8.64) of the messages passed from factor nodes to variable nodes", "9a487e35-64b5-44a0-a6a5-799d173748d6": "12.4.6 Historical Perspective  mi +a eC toast c tod m4 1 1a n u  https://www.deeplearningbook.org/contents/applications.html    Lule 10ea OL GISULIDULEd LepresenLavlONs LOL SYWDOIS Was LLroduced Dy LHULeLUArt et al.\n\nin one of the first explorations of back-propagation, with symbols corresponding to the identity of family members, and the neural network capturing the relationships between family members, with training examples forming triplets  such as (Colin, Mother, Victoria). The first layer of the neural network learned a representation of each family member. For example, the features for Colin might represent which family tree Colin was in, what branch of that tree he was in, what generation he was from, and so on. One can think of the neural network as computing learned rules relating these attributes together to obtain the desired predictions. The model can then make predictions such as inferring who is the mother of Colin. The idea of forming an embedding for a symbol was extended to the idea of an embedding for a word by Deerwester ef al", "20003f13-8e97-4650-b372-93c2aca6d3dc": "2.34 (\u22c6 \u22c6) www To \ufb01nd the maximum likelihood solution for the covariance matrix of a multivariate Gaussian, we need to maximize the log likelihood function (2.118) with respect to \u03a3, noting that the covariance matrix must be symmetric and positive de\ufb01nite. Here we proceed by ignoring these constraints and doing a straightforward maximization. Using the results (C.21), (C.26), and (C.28) from Appendix C, show that the covariance matrix \u03a3 that maximizes the log likelihood function (2.118) is given by the sample covariance (2.122). We note that the \ufb01nal result is necessarily symmetric and positive de\ufb01nite (provided the sample covariance is nonsingular). where xn denotes a data point sampled from a Gaussian distribution with mean \u00b5 and covariance \u03a3, and Inm denotes the (n, m) element of the identity matrix", "12c72c06-d2f2-4b4e-9c26-37bfe60321a6": "At each moment we look at the current TD error and assign it backward to each prior state according to how much that state contributed to the current eligibility trace at that time. We might imagine ourselves riding along the stream of states, computing TD errors, and shouting them back to the previously visited states, as suggested by Figure 12.5.\n\nWhere the TD error and traces come together, we get the update given by (12.7), changing the values of those past states for when they occur again in the future. To better understand the backward view of TD(\u03bb), consider what happens at various corresponding to St. Thus the TD(\u03bb) update (12.7) reduces to the one-step semi-gradient TD update treated in Chapter 9 (and, in the tabular case, to the simple TD rule (6.2)). This is why that algorithm was called TD(0). In terms of Figure 12.5, TD(0) is the case in which only the one state preceding the current one is changed by the TD error. For larger values of \u03bb, but still \u03bb < 1, more of the preceding states are changed, but each more temporally distant state is changed less because the corresponding eligibility trace is smaller, as suggested by the \ufb01gure", "8b548a0f-f492-4bf6-9c5e-86fb202e9e3f": ", yN is speci\ufb01ed completely by the second-order statistics, namely the mean and the covariance. In most applications, we will not have any prior knowledge about the mean of y(x) and so by symmetry we take it to be zero. This is equivalent to choosing the mean of the prior over weight values p(w|\u03b1) to be zero in the basis function viewpoint.\n\nThe speci\ufb01cation of the Gaussian process is then completed by giving the covariance of y(x) evaluated at any two values of x, which is given by the kernel function For the speci\ufb01c case of a Gaussian process de\ufb01ned by the linear regression model (6.49) with a weight prior (6.50), the kernel function is given by (6.54). We can also de\ufb01ne the kernel function directly, rather than indirectly through a choice of basis function. Figure 6.4 shows samples of functions drawn from Gaussian processes for two different choices of kernel function", "f7fec4a0-f5d0-4024-9e77-d9f05cb04440": "Taking the negative logarithm of (1.66) and combining with (1.62) and (1.65), we \ufb01nd that the maximum of the posterior is given by the minimum of Thus we see that maximizing the posterior distribution is equivalent to minimizing the regularized sum-of-squares error function encountered earlier in the form (1.4), with a regularization parameter given by \u03bb = \u03b1/\u03b2. Although we have included a prior distribution p(w|\u03b1), we are so far still making a point estimate of w and so this does not yet amount to a Bayesian treatment. In a fully Bayesian approach, we should consistently apply the sum and product rules of probability, which requires, as we shall see shortly, that we integrate over all values of w. Such marginalizations lie at the heart of Bayesian methods for pattern recognition. In the curve \ufb01tting problem, we are given the training data x and t, along with a new test point x, and our goal is to predict the value of t", "dcf31674-edf8-48e9-9a30-4173bded47fc": "Explanation-based learning and reinforcement learning: A uni\ufb01ed view. In A. Prieditis and S. Russell (Eds. ), Proceedings of the 12th International Conference on Machine Learning, pp. 176\u2013184. Morgan Kaufmann. Dietterich, T. G., Wang, X. Batch value function approximation via support vectors. reinforcement learning. In Proceedings of the 25th International Conference on Machine Learning, pp. 240\u2013247. ACM, New York. Dolan, R. J., Dayan, P. Goals and habits in the brain. Neuron, 80(2):312\u2013325. Doll, B. B., Simon, D. A., Daw, N. D. The ubiquity of model-based reinforcement learning. Current Opinion in Neurobiology, 22(6):1\u20137. Dorigo, M., Colombetti, M", "3de60cc9-73b0-45ae-a05f-00f309439417": "EDA: Easy data augmentation techniques for boosting performance on text classi\ufb01cation tasks. arXiv preprint arXiv:1901.11196, 2019. X. Wu, S. Lv, L. Zang, J. Han, and S. Hu. Conditional BERT contextual augmentation. arXiv preprint arXiv:1812.06705, 2018. Q. Xie, Z. Dai, E. Hovy, M.-T. Luong, and L. Q. V. Unsupervised data augmentation. arXiv preprint arXiv 1904.12848, 2019. Z. Xie, S. I. Wang, J. Li, D. L\u00e9vy, A. Nie, D. Jurafsky, and A. Y. Ng. Data noising as smoothing in neural network language models. In ICLR, 2017.", "5af7d302-697e-4247-8648-9fbec6f8660a": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1 2 everywhere. In the case where G and D are de\ufb01ned by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples. The promise of deep learning is to discover rich, hierarchical models  that represent probability distributions over the kinds of data encountered in arti\ufb01cial intelligence applications, such as natural images, audio waveforms containing speech, and symbols in natural language corpora", "009e3d06-d924-4ee4-8b33-74802853005e": "Adversarial augmentation policy search for domain and cross-lingual generalization in reading comprehension. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics. Nikolaos Malandrakis, Minmin Shen, Anuj Goyal, Shuyang Gao, Abhishek Sethi, and Angeliki Metallinou. 2019. Controlled text generation for data augmentation in intelligent arti\ufb01cial agents. In Proceedings of the 3rd Workshop on Neural Generation and Translation, pages 90\u201398, Hong Kong. Association for Computational Linguistics. R. Thomas McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. Zhengjie Miao, Yuliang Li, Xiaolan Wang, and WangChiew Tan. 2020. Snippext: Semi-supervised opinion mining with augmented data.\n\nIn Proceedings of The Web Conference 2020, pages 617\u2013628", "162c299f-54aa-426f-86a4-1c0c90012842": "Note that the {zn}, as well as the missing data values that are components of the vectors {xn }, are now latent variables. Show that in the special case in which all of the data values are observed, this reduces to the EM algorithm for probabilistic PCA derived in Section 12.2.2.\n\n12.17 (**) IIiI!I Let W be a D x M matrix whose columns define a linear subspace of dimensionality M embedded within a data space of dimensionality D, and let J1 be a D-dimensional vector. Given a data set {xn } where n = 1, ... , N, we can approximate the data points using a linear mapping from a set of M -dimensional vectors {zn}, so that Xn is approximated by W Zn + J1. The associated sum-ofsquares reconstruction cost is given by First show that minimizing J with respect to J1leads to an analogous expression with X n and Zn replaced by zero-mean variables X n - x and Zn - Z, respectively, where x and Z denote sample means", "fad49005-ea7b-4f25-9443-ba86bbd532a7": "Specifically, the M-step assumes that the same value of q can be used for all values of 6. This will introduce a gap between \u00a3 and the true log p(v) as the M-step moves further and further away from the value 6) used in the E-step. Fortunately, the E-step reduces the gap to zero again as we enter the loop for the next time. The EM algorithm contains a few different insights. First, there is the basic structure of the learning process, in which we update the model parameters to improve the likelihood of a completed dataset, where all missing variables have their values provided by an estimate of the posterior distribution. This particular insight is not unique to the EM algorithm. For example, using gradient descent to maximize the log-likelihood also has this same property; the log-likelihood gradient computations require taking expectations with respect to the posterior distribution over the hidden units. Another key insight in the EM algorithm is that we can  https://www.deeplearningbook.org/contents/inference.html    continue to use one value of 7 even after we have moved to a different value of 9", "ee257daa-907d-444a-8112-f4ffdf9b3f8e": "Because almost all of the conventional methods require complete knowledge of the system to be controlled, it feels a little unnatural to say that they are part of reinforcement learning. On the other hand, many dynamic programming algorithms are incremental and iterative. Like learning methods, they gradually reach the correct answer through successive approximations.\n\nAs we show in the rest of this book, these similarities are far more than super\ufb01cial. The theories and solution methods for the cases of complete and incomplete knowledge are so closely related that we feel they must be considered together as part of the same subject matter. Let us return now to the other major thread leading to the modern \ufb01eld of reinforcement learning, the thread centered on the idea of trial-and-error learning. We only touch on the major points of contact here, taking up this topic in more detail in Section 14.3. According to American psychologist R. S. Woodworth  the idea of trial-and-error learning goes as far back as the 1850s to Alexander Bain\u2019s discussion of learning by \u201cgroping and experiment\u201d and more explicitly to the British ethologist and psychologist Conway Lloyd Morgan\u2019s 1894 use of the term to describe his observations of animal behavior", "979be654-bbcb-47f7-abf7-5e964eaa592b": "Thus the pair is rejected if it lies in the grey shaded region in Figure 11.4.\n\nThe remaining pairs then have uniform distribution under the curve of \ufffdp(z), and hence the corresponding z values are distributed according to p(z), as desired. Exercise 11.6 The original values of z are generated from the distribution q(z), and these samples are then accepted with probability \ufffdp(z)/kq(z), and so the probability that a sample will be accepted is given by Thus the fraction of points that are rejected by this method depends on the ratio of the area under the unnormalized distribution \ufffdp(z) to the area under the curve kq(z). We therefore see that the constant k should be as small as possible subject to the limitation that kq(z) must be nowhere less than \ufffdp(z). As an illustration of the use of rejection sampling, consider the task of sampling which, for a > 1, has a bell-shaped form, as shown in Figure 11.5. A suitable proposal distribution is therefore the Cauchy (11.8) because this too is bell-shaped and because we can use the transformation method, discussed earlier, to sample from it. We need to generalize the Cauchy slightly to ensure that it nowhere has a smaller value than the gamma distribution", "0513344b-9297-475a-be4b-33dfb4cb5886": "Here \u03bd > 0 is called the number of degrees of freedom of the distribution.\n\nThe particular case of \u03bd = 1 is called the Cauchy distribution. For a D-dimensional variable x, Student\u2019s t-distribution corresponds to marginalizing the precision matrix of a multivariate Gaussian with respect to a conjugate Wishart prior and takes the form where \u22062 is the squared Mahalanobis distance de\ufb01ned by In the limit \u03bd \u2192 \u221e, the t-distribution reduces to a Gaussian with mean \u00b5 and precision \u039b. Student\u2019s t-distribution provides a generalization of the Gaussian whose maximum likelihood parameter values are robust to outliers. This is a simple distribution for a continuous variable x de\ufb01ned over a \ufb01nite interval x \u2208  where b > a. If x has distribution U(x|0, 1), then a + (b \u2212 a)x will have distribution U(x|a, b). The von Mises distribution, also known as the circular normal or the circular Gaussian, is a univariate Gaussian-like periodic distribution for a variable \u03b8 \u2208 [0, 2\u03c0)", "b2dfdccf-551a-4d5d-a2a1-5aa79e36ca7a": "A lower than expected wind at a (for a hurricane) would not change our expectation of winds at b (knowing there is a hurricane). However, if s is not observed, then a and b are dependent, i.e., the path is  a a  <eT 4 at  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    acts. ( Variables a and b are both parents of s. \u2018his 1s called a V-Strucvure, oy Lue collider case. The V-structure causes a and b to be related by the explaining away effect, In this case, the path is actually active when s is observed. For example, suppose s is a variable indicating that your colleague is not at work. The variablea represents  her being sick, while b represents her being on vacation. If you observe that she is not at work, you can presume she is probably sick or on vacation, but it is not especially likely that both have happened at the same time. If you find out that she is on vacation, this fact is sufficient to explain her absence", "9237e2d1-7afe-4624-a782-fde120a92f8f": "Unfortunately, both conventions are widely used in the literature.\n\nbut here it just reminds us that p speci\ufb01es a probability distribution for each choice of s and a, that is, that In a Markov decision process, the probabilities given by p completely characterize the environment\u2019s dynamics. That is, the probability of each possible value for St and Rt depends only on the immediately preceding state and action, St\u22121 and At\u22121, and, given them, not at all on earlier states and actions. This is best viewed a restriction not on the decision process, but on the state. The state must include information about all aspects of the past agent\u2013environment interaction that make a di\u21b5erence for the future. If it does, then the state is said to have the Markov property. We will assume the Markov property throughout this book, though starting in Part II we will consider approximation methods that do not rely on it, and in Chapter 17 we consider how a Markov state can be learned and constructed from non-Markov observations", "69210e2c-ca19-436e-af81-c620e8e106b4": "Hengyi Cai, Hongshen Chen, Yonghao Song, Cheng Zhang, Xiaofang Zhao, and Dawei Yin. 2020. Data manipulation: Towards effective instance learning for neural dialogue generation via learning to augment and reweight. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6334\u20136343, Online. Association for Computational Linguistics. Rui Cai and Mirella Lapata. 2019. Semi-supervised semantic role labeling with cross-view training. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1018\u2013 1027, Hong Kong, China. Association for Computational Linguistics. Ming-Wei Chang, Lev Ratinov, Dan Roth, and Vivek Srikumar. 2008. Importance of semantic representation: Dataless classi\ufb01cation. In Proceedings of the 23rd National Conference on Arti\ufb01cial Intelligence - Volume 2, AAAI\u201908, pages 830\u2013835", "a1fa4ff7-e03f-4ab3-9b0f-c980e504e916": "The generator network directly produces samples x = g(2z; 9(9)). Its adversary, the discriminator network, attempts to distinguish between samples drawn from the training data and samples drawn from the generator. The discriminator emits a probability value given by d(x; a), indicating the probability that x is a real training example rather than a fake sample drawn from the model. The simplest way to formulate learning in generative adversarial networks is as a zero-sum game, in which a function v(09), a4) determines the payoff of the  696  CHAPTER 20. DEEP GENERATIVE MODELS  discriminator. The generator receives (99), 9%) as its own payoff.\n\nDuring learning, each player attempts to maximize its own payoff, so that at convergence  g =argmin max v(g, d). (20.80) g  The default choice for v is V0, 8 ) = Ex paaial08 U(#) + Ex~pmoaer 10g (1 \u2014 d(a))", "13eef519-e2d7-44cb-86bb-5e11411753cd": "Now consider a variational distribution that factorizes between the latent variables and the parameters, so that q(Z, \u03b7) = q(Z)q(\u03b7). Using the general result (10.9), we can solve for the two factors as follows Thus we see that this decomposes into a sum of independent terms, one for each value of n, and hence the solution for q\u22c6(Z) will factorize over n so that q\u22c6(Z) = \ufffd n q\u22c6(zn). This is an example of an induced factorization. Taking the exponential Section 10.2.5 where the normalization coef\ufb01cient has been re-instated by comparison with the standard form for the exponential family. Similarly, for the variational distribution over the parameters, we have Again, taking the exponential of both sides, and re-instating the normalization coef\ufb01cient by inspection, we have Note that the solutions for q\u22c6(zn) and q\u22c6(\u03b7) are coupled, and so we solve them iteratively in a two-stage procedure", "93f42838-030e-47a6-88de-7914adb97dbf": "(7.50) i j#i = tot itte (7.51)  In the case where the errors are perfectly correlated and c= v, the mean squared  error reduces to v, so the model averaging does not help at all. In the case where  the errors are perfectly uncorrelated and c = 0, the expected squared error of the 1  https://www.deeplearningbook.org/contents/regularization.html    ensemble is only kU. \u2018I\u2019his means that the expected squared error of the ensemble is inversely proportional to the ensemble size. In other words, on average, the ensemble will perform at least as well as any of its members, and if the members make independent errors, the ensemble will perform significantly better than its  members.\n\nDifferent ensemble methods construct the ensemble of models in different ways. For example, each member of the ensemble could be formed by training a completely different kind of model using a different algorithm or objective function. Bagging is a method that allows the same kind of model, training algorithm and objective function to be reused several times. Specifically, bagging involves constructing k different datasets", "4144b7e9-07b5-435c-9225-4115c653a2aa": "J., Pouget, A., Sejnowski, T. J. Using aperiodic Information Processing Systems 5 , pp. 969\u2013976. Morgan Kaufmann. Montague, P. R., Dayan, P., Person, C., Sejnowski, T. J. Bee foraging in uncertain environments using predictive hebbian learning. Nature, 377:725\u2013728. Montague, P. R., Dayan, P., Sejnowski, T. J. A framework for mesencephalic dopamine systems based on predictive Hebbian learning. The Journal of Neuroscience, 16(5):1936\u20131947. Montague, P. R., Dolan, R. J., Friston, K. J., Dayan, P. Computational psychiatry. temporal order in synaptic learningmechanisms. Learning & Memory, 1(1):1\u201333. Moore, A", "fa903edc-444a-4e87-9227-53c91eb38f79": "10.4 The recognition of the limitations of discounting as a formulation of the reinforcement learning problem with function approximation became apparent to the authors shortly after the publication of the \ufb01rst edition of this text. Singh, Jaakkola, and Jordan  may have been the \ufb01rst to observe it in print.\n\nThis book has treated on-policy and o\u21b5-policy learning methods since Chapter 5 primarily as two alternative ways of handling the con\ufb02ict between exploitation and exploration inherent in learning forms of generalized policy iteration. The two chapters preceding this have treated the on-policy case with function approximation, and in this chapter we treat the o\u21b5 -policy case with function approximation. The extension to function approximation turns out to be signi\ufb01cantly di\u21b5erent and harder for o\u21b5-policy learning than it is for on-policy learning. The tabular o\u21b5-policy methods developed in Chapters 6 and 7 readily extend to semi-gradient algorithms, but these algorithms do not converge as robustly as they do under on-policy training", "c3eed49a-27ee-4339-8819-8334d23ef15b": "Storing datasets in memory can be extremely problematic depending on how heavily the dataset size has been inflated. Storing augmented datasets in memory is especially problematic when augmenting big data. This decision is generally categorized as online or offline data augmentation, (with online augmentation referring to on the fly augmentations and offline augmentation referring to editing and storing data on the disk). In the design of a massively distributed training system, Chilimbi et al. augment images before training to speed up image serving. By augmenting images in advance, the distributed system is able to request and pre-cache training batches.\n\nAugmentations can also be built into the computational graph used to construct Deep Learning models and facilitate fast differentiation. These augmentations process images immediately after the input image tensor. Additionally, it is also interesting to explore a subset of the inflated data that will result in higher or similar performance to the entire training set. This is a similar concept to curriculum learning, since the central idea is to find an optimal ordering of training data. This idea is also very related to final dataset size and the considerations of transforma-  tion compute and available memory for storing augmented images", "9e5f1a52-48a1-4df9-be33-6b69fa08a0f4": "Learning with multiple types of experience is a necessary capability of AI agents to be deployed in a real dynamic environment and to improve from heterogeneous signals in di\ufb00erent forms, at varying granularities, from diverse sources, and with di\ufb00erent intents (e.g., adversaries).\n\nSuch versatile learning capability is best exempli\ufb01ed by human learning\u2014for instance, we learn a new language by reading and hearing examples, studying grammars and rules, practicing and interacting with others, receiving feedback, associating with prior languages we already mastered, and so forth. To build a similar panoramic-learning AI agent, having a standardized learning formalism is perhaps an indispensable step. The standard equation we have presented is naturally designed to \ufb01t the needs. In particular, the experience function provides a straightforward vehicle for experience combination. For example, the simplest approach is perhaps to make a weighted composition of multiple individual experience functions: where each fi characterizes a speci\ufb01c source of experience, and \u03bbi > 0 is the respective weight. One can readily plug in arbitrary available experience, such as data, logical rules, constraints, rewards, and auxiliary models, as components of the experience function", "821d5481-da8a-45ad-ac3b-e16d39ce97e9": "A common problem in conventional calculus is to \ufb01nd a value of x that maximizes (or minimizes) a function y(x). Similarly, in the calculus of variations we seek a function y(x) that maximizes (or minimizes) a functional F. That is, of all possible functions y(x), we wish to \ufb01nd the particular function for which the functional F is a maximum (or minimum). The calculus of variations can be used, for instance, to show that the shortest path between two points is a straight line or that the maximum entropy distribution is a Gaussian. If we weren\u2019t familiar with the rules of ordinary calculus, we could evaluate a conventional derivative dy/ dx by making a small change \u03f5 to the variable x and then expanding in powers of \u03f5, so that and \ufb01nally taking the limit \u03f5 \u2192 0.\n\nSimilarly, for a function of several variables y(x1, . , xD), the corresponding partial derivatives are de\ufb01ned by y(x1 + \u03f51, . , xD + \u03f5D) = y(x1,", "caacc58b-1446-4efe-bce0-37973e0a0fa5": "While we do not know the exact size of the constraint region, we can control it roughly by increasing or decreasing a in order to grow or shrink the constraint region.\n\nLarger q will result in a smaller constraint region. Smaller a will result in a larger constraint region. Sometimes we may wish to use explicit constraints rather than penalties. As described in section 4.4, we can modify algorithms such as stochastic gradient descent to take a step downhill on J(@) and then project 6 back to the nearest point that satisfies (0) < k. This can be useful if we have an idea of what value of k is appropriate and do not want to spend time searching for the value of a that corresponds to this k.  Another reason to use explicit constraints and reprojection rather than enforcing constraints with penalties is that penalties can cause nonconvex optimization procedures to get stuck in local minima corresponding to small @. When training neural networks, this usually manifests as neural networks that train with several \u201cdead units.\u201d These are units that do not contribute much to the behavior of the function learned by the network because the weights going into or out of them are all very small", "062f0279-e743-4efd-8cc0-0c668a54d10e": "As in all of arti\ufb01cial intelligence, there is a tension between breadth of applicability and mathematical tractability. In this chapter we introduce this tension and discuss some of the trade-o\u21b5s and challenges that it implies.\n\nSome ways in which reinforcement learning can be taken beyond MDPs are treated in Chapter 17. MDPs are meant to be a straightforward framing of the problem of learning from interaction to achieve a goal. The learner and decision maker is called the agent. The thing it interacts with, comprising everything outside the agent, is called the environment. These interact continually, the agent selecting actions and the environment responding to these actions and presenting new situations to the agent.1 The environment also gives rise to rewards, special numerical values that the agent seeks to maximize over time through its choice of actions. More speci\ufb01cally, the agent and environment interact at each of a sequence of discrete time steps, t = 0, 1, 2, 3,", "74575881-1dcb-4728-9844-980dd7f2ee17": "Fortunately, in practice it is usually sufficient to shuffle the order of the dataset once and then store it in shuffled fashion.\n\nThis will impose a fixed set of possible minibatches of consecutive examples that all models trained thereafter will use, and each individual model will be forced to reuse this ordering every time it passes through the training data. This deviation from true random selection does not seem to have a significant detrimental effect. Failing to ever shuffle the examples in any way can seriously reduce the effectiveness of the algorithm. Many optimization problems in machine learning decompose over examples well enough that we can compute entire separate updates over different examples in parallel. In other words, we can compute the update that minimizes J(X) for one minibatch of examples X at the same time that we compute the update for several other minibatches. Such asynchronous parallel distributed approaches are discussed further in section 12.1.3. An interesting motivation for minibatch stochastic gradient descent is that it  follows the gradient of the true generalization error (equation 8.2) as long as no  277  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  examples are repeated", "ffc2a09f-4976-41db-b338-800392a76a53": "we see that the marginal distribution for the observed ,-\"riabl, i' gi,-,n by 1'(x) ~ N( Xlj', C) whe... now NOie th\"t thi' i' e.pre,<ed in a for'\" thai in,-oh'es inycrsi\"n ofmalrices \"f SilO ,\\ I x ,If rathe'lhan D x D (ex\"\",,,, for tbe D x D diagooal matrix oJ' \"'hose in,-erse i.' 'ri\"ial Exercise 12.22 to compute in O(D) steps), which is convenient because often M \u00ab D. Similarly, the M-step equations take the form where the 'diag' operator sets all of the nondiagonal elements of a matrix to zero. A Bayesian treatment of the factor analysis model can be obtained by a straightforward application of the techniques discussed in this book. Another difference between probabilistic PCA and factor analysis concerns their Exercise 12.25 different behaviour under transformations of the data set", "8a28d3ca-2c1f-4b5f-a2e2-27bb7e79e788": "Zhiting Hu1,2\u2217, Bowen Tan1\u2217, Ruslan Salakhutdinov1, Tom Mitchell1, Eric P. Xing1,2 Manipulating data, such as weighting data examples or augmenting with new instances, has been increasingly used to improve model training. Previous work has studied various rule- or learning-based approaches designed for speci\ufb01c types of data manipulation. In this work, we propose a new method that supports learning different manipulation schemes with the same gradient-based algorithm. Our approach builds upon a recent connection of supervised learning and reinforcement learning (RL), and adapts an off-the-shelf reward learning algorithm from RL for joint data manipulation learning and model training. Different parameterization of the \u201cdata reward\u201d function instantiates different manipulation schemes. We showcase data augmentation that learns a text transformation network, and data weighting that dynamically adapts the data sample importance. Experiments show the resulting algorithms signi\ufb01cantly improve the image and text classi\ufb01cation performance in low data regime and class-imbalance problems. The performance of machines often crucially depend on the amount and quality of the data used for training", "8fcc56af-1921-4009-9d3d-0c3fbf4bb989": "If k = 2 and k = 3 each represent possible values of xmax N , then starting from either state and tracing back along the black lines, which corresponds to iterating (8.102), we obtain a valid global maximum con\ufb01guration. Note that if we had run a forward pass of max-sum message passing followed by a backward pass and then applied (8.98) at each node separately, we could end up selecting some states from one path and some from the other path, giving an overall con\ufb01guration that is not a global maximizer.\n\nWe see that it is necessary instead to keep track of the maximizing states during the forward pass using the functions \u03c6(xn) and then use back-tracking to \ufb01nd a consistent solution. The extension to a general tree-structured factor graph should now be clear. If a message is sent from a factor node f to a variable node x, a maximization is performed over all other variable nodes x1, . , xM that are neighbours of that factor node, using (8.93). When we perform this maximization, we keep a record of which values of the variables x1, . , xM gave rise to the maximum", "332f0ae1-1666-4b3d-acef-834e33aabf85": "Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to the masked language model, we also use a \u201cnext sentence prediction\u201d task that jointly pretrains text-pair representations. The contributions of our paper are as follows: \u2022 We demonstrate the importance of bidirectional pre-training for language representations. Unlike Radford et al. , which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al.\n\n, which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs. \u2022 We show that pre-trained representations reduce the need for many heavily-engineered taskspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01netuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-speci\ufb01c architectures", "304b674f-8c23-423e-8525-fe1d041ed946": "1052\u20131058. MIT Press, Cambridge, MA. Carnegie Mellon University, Pittsburgh PA. Pittsburgh, PA. Gordon, G. J. Reinforcement learning with function approximation converges to a region. In Advances in Neural Information Processing Systems 13 , pp. 1040\u20131046. MIT Press, Cambridge, MA. Graybiel, A. M. The basal ganglia. Current Biology, 10(14):R509\u2013R511. Greensmith, E., Bartlett, P. L., Baxter, J. Variance reduction techniques for gradient estimates in reinforcement learning. In Advances in Neural Information Processing Systems 14 , pp. 1507\u20131514. MIT Press, Cambridge, MA. Greensmith, E., Bartlett, P. L., Baxter, J. Variance reduction techniques for gradient Technical Report Project MAC, Arti\ufb01cial Intelligence Memo 94. Massachusetts Institute of Technology, Cambridge, MA", "b49a18fa-58b9-484e-b572-3367a8519d67": "Vanishing gradients  https://www.deeplearningbook.org/contents/optimization.html    make it difficult to know which direction the parameters should move to improve the cost function, while exploding gradients can make learning unstable.\n\nThe cliff structures described earlier that motivate gradient clipping are an example of the exploding gradient phenomenon. The repeated multiplication by W at each time step described here is very similar to the power method algorithm used to find the largest eigenvalue of a matrix W and the corresponding eigenvector. From this point of view it is not surprising that \u00ab' W? will eventually discard all components of x that are orthogonal to the principal eigenvector of W.  286  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  Recurrent networks use the same matrix W at each time step, but feedforward networks do not, so even very deep feedforward networks can largely avoid the vanishing and exploding gradient problem . We defer further discussion of the challenges of training recurrent networks until section 10.7, after recurrent networks have been described in more detail. 8.2.6 Inexact Gradients  Most optimization algorithms are designed with the assumption that we have access to the exact gradient or Hessian matrix", "fd261c1f-c031-4d92-a9c5-bfdc0e1b9eeb": "A Exercise 1.36 function is called strictly convex if the equality is satis\ufb01ed only for \u03bb = 0 and \u03bb = 1. If a function has the opposite property, namely that every chord lies on or below the function, it is called concave, with a corresponding de\ufb01nition for strictly concave. If a function f(x) is convex, then \u2212f(x) will be concave. Using the technique of proof by induction, we can show from (1.114) that a Exercise 1.38 i \u03bbi = 1, for any set of points {xi}.\n\nThe result (1.115) is known as Jensen\u2019s inequality. If we interpret the \u03bbi as the probability distribution over a discrete variable x taking the values {xi}, then (1.115) can be written where E denotes the expectation. For continuous variables, Jensen\u2019s inequality takes the form We can apply Jensen\u2019s inequality in the form (1.117) to the Kullback-Leibler where we have used the fact that \u2212 ln x is a convex function, together with the normalization condition \ufffd q(x) dx = 1", "0bc5fcdb-0e97-4487-abcb-507bb9c475a6": "Therefore, it is usually best to monitor the first several iterations and use a learning rate that is higher than the best-performing learning rate at this time, but not so high that it causes severe instability. The most important property of SGD and related minibatch or online gradient- based optimization is that computation time per update does not grow with the number of training examples. This allows convergence even when the number of training examples becomes very large. For a large enough dataset, SGD may converge to within some fixed tolerance of its final test set error before it has  https://www.deeplearningbook.org/contents/optimization.html    processed the entire training set. To study the convergence rate of an optimization algorithm it is common to measure the excess error J(@) \u2014 ming J(@), which is the amount by which the  current cost function exceeds the minimum possible cost. When SGD is applied to a convex problem, the excess error is OF) after k iterations, while in the strongly  convex case, it is O(Z): These bounds cannot be improved unless extra conditions are assumed. Batch gradient descent enjoys better convergence rates than stochastic gradient descent in theory", "5edf64bb-6a40-4906-999f-2dc209ab8b73": "The training of the student network is made easier by training the student network not only to predict he output for the original task, but also to predict the value of the middle layer of he teacher network. This extra task provides a set of hints about how the hidden layers should be used and can simplify the optimization problem. Additional parameters are introduced to regress the middle layer of the five layer teacher network from the middle layer of the deeper student network.\n\nInstead of predicting he final classification target, however, the objective is to predict the middle hidden ayer of the teacher network. The lower layers of the student networks thus have \u2018wo objectives: to help the outputs of the student network accomplish their task, as well as to predict the intermediate layer of the teacher network. Although a thin and deep network appears to be more difficult to train than a wide and shallow  network, the thin and deep network may generalize better and certainly has lower computational cost if it is thin enough to have far fewer parameters. Without the hints on the hidden layer, the student network performs very poorly in the experiments, on both the training and the test set. Hints on middle layers may thus be one of the tools to help train neural networks that otherwise seem difficult to train, but other optimization techniques or changes in the architecture may also solve the problem", "a29f401f-cd1d-4963-9588-a2c93c6e2cf7": "13.15 (\u22c6 \u22c6) Use the expressions (13.33) and (13.43) for the marginals in a hidden Markov model to derive the corresponding results (13.64) and (13.65) expressed in terms of re-scaled variables. 13.16 (\u22c6 \u22c6 \u22c6) In this exercise, we derive the forward message passing equation for the Viterbi algorithm directly from the expression (13.6) for the joint distribution. This involves maximizing over all of the hidden variables z1, . , zN. By taking the logarithm and then exchanging maximizations and summations, derive the recursion (13.68) where the quantities \u03c9(zn) are de\ufb01ned by (13.70). Show that the initial condition for this recursion is given by (13.69).\n\n13.17 (\u22c6) www Show that the directed graph for the input-output hidden Markov model, given in Figure 13.18, can be expressed as a tree-structured factor graph of the form shown in Figure 13.15 and write down expressions for the initial factor h(z1) and for the general factor fn(zn\u22121, zn) where 2 \u2a7d n \u2a7d N", "325cc5cc-6a72-4854-9cb0-abf2c4e95479": "Many possibilities exist for the particular parametrization associated with such a model. Early work on learning about relations between entities  posited highly constrained parametric forms (\u201clinear relational embeddings\u201d), often using a different form of representation for the relation than for the entities.\n\nFor example, Paccanaro and Hinton  and Bordes et al. used vectors for entities and matrices for relations, with the idea that a relation acts like an operator on entities. Alternatively, relations can be considered as any other entity , allowing us to make statements about relations, but more flexibility is  https://www.deeplearningbook.org/contents/applications.html    put in the machinery that combines them in order to model their joint distribution. _ A practical short-term application of such models is link prediction; predict- ing missing arcs in the knowledge graph. This is a form of generalization to new facts based on old facts. Most of the knowledge bases that currently exist have been constructed through manual labor, which tends to leave many and probably the majority of true relations absent from the knowledge base. See Wang et al. , Lin ef al. , and Garcia-Duran ef al", "8cced30e-eb2a-4639-94c9-f28fe4bccd47": "Compare this to the non-distributed representations in figure 15.8. In the general case of d input dimensions, a distributed representation divides R\u00a2 by intersecting half-spaces rather than half-planes. The distributed representation with n features assigns unique codes to O(n\u00ae) different regions, while the nearest neighbor algorithm withn examples assigns unique codes to only n regions. The distributed representation is thus able to distinguish exponentially many more regions than the nondistributed one", "47602583-5423-41c8-82e6-a750bcf148ad": "Then the predicted time to go after exiting the highway would be revised upward by four minutes as a result of this experience. This is probably too large a change in this case; the truck was probably just an unlucky break. In any event, the change can only be made o\u270fine, that is, after you have reached home. Only at this point do you know any of the actual returns. Is it necessary to wait until the \ufb01nal outcome is known before learning can begin? Suppose on another day you again estimate when leaving your o\ufb03ce that it will take 30 minutes to drive home, but then you become stuck in a massive tra\ufb03c jam. Twenty-\ufb01ve minutes after leaving the o\ufb03ce you are still bumper-to-bumper on the highway. You now 1If this were a control problem with the objective of minimizing travel time, then we would of course make the rewards the negative of the elapsed time. But because we are concerned here only with prediction (policy evaluation), we can keep things simple by using positive numbers. estimate that it will take another 25 minutes to get home, for a total of 50 minutes. As you wait in tra\ufb03c, you already know that your initial estimate of 30 minutes was too optimistic", "958c612c-bc6f-435d-9a2f-32e3262fdbf6": "Users were presented with 4 tutorial Jupyter notebooks providing skeleton code for evaluating labeling functions, along with a small labeled development candidate set, and were given 2.5 hours of dedicated development time in aggregate to write their labeling functions. All workshop materials are available online.19 Baseline To compare our users\u2019 performance against models trained on hand-labeled data, we collected a large handlabeled dataset via Amazon Mechanical Turk (the same set used in the previous subsection).\n\nWe then split this into 15 datasets representing 7 hours worth of hand-labeling time each\u2014based on the crowdworker average of 10 s per label\u2014 simulating the alternative scenario where users skipped both instruction and labeling function development sessions and instead spent the full day hand-labeling data. Partitions were created by drawing a uniform random sample of 2500 labels from the total Amazon Mechanical Turk-generated Spouse dataset. For 15 such random samples, the mean F1 score was 20.9 (min:11.7, max: 29.5). Scaling to 55 random partitions, the mean F1 score was 22.5 (min:11.7, max: 34.1)", "6122c366-5565-48f6-a1be-b237e642d321": "Chunking can be implemented using protected conjugate gradients . Although chunking reduces the size of the matrix in the quadratic function from the number of data points squared to approximately the number of nonzero Lagrange multipliers squared, even this may be too big to \ufb01t in memory for large-scale applications. Decomposition methods  also solve a series of smaller quadratic programming problems but are designed so that each of these is of a \ufb01xed size, and so the technique can be applied to arbitrarily large data sets. However, it still involves numerical solution of quadratic programming subproblems and these can be problematic and expensive. One of the most popular approaches to training support vector machines is called sequential minimal optimization, or SMO .\n\nIt takes the concept of chunking to the extreme limit and considers just two Lagrange multipliers at a time. In this case, the subproblem can be solved analytically, thereby avoiding numerical quadratic programming altogether. Heuristics are given for choosing the pair of Lagrange multipliers to be considered at each step. In practice, SMO is found to have a scaling with the number of data points that is somewhere between linear and quadratic depending on the particular application", "a7d9b5bc-858b-43d7-ba1a-763e976333b9": "One way to allow earlier observations to have an in\ufb02uence is to move to higher-order Markov chains. If we allow the predictions to depend also on the previous-but-one value, we obtain a second-order Markov chain, represented by the graph in Figure 13.4. The joint distribution is now given by Again, using d-separation or by direct evaluation, we see that the conditional distribution of xn given xn\u22121 and xn\u22122 is independent of all observations x1, . xn\u22123. Each observation is now in\ufb02uenced by two previous observations. We can similarly consider extensions to an M th order Markov chain in which the conditional distribution for a particular variable depends on the previous M variables. However, we have paid a price for this increased \ufb02exibility because the number of parameters in the model is now much larger. Suppose the observations are discrete variables having K states. Then the conditional distribution p(xn|xn\u22121) in a \ufb01rst-order Markov chain will be speci\ufb01ed by a set of K \u22121 parameters for each of the K states of xn\u22121 giving a total of K(K \u2212 1) parameters", "1887b6e2-edd7-496e-828d-f6aa4fedb241": "These posterior distributions are usually not explicitly specified and parametrized in the model. Inferring these posterior distributions can be costly. In models where this is the case, ancestral sampling is no longer efficient. Unfortunately, ancestral sampling is applicable only to directed models.\n\nWe can sample from undirected models by converting them to directed models, but this often requires solving intractable inference problems (to determine the marginal distribution over the root nodes of the new directed graph) or requires introducing so many edges that the resulting directed model becomes intractable. Sampling from an undirected model without first converting it to a directed model seems to require resolving cyclical dependencies. Every variable interacts with every other variable, so there is no clear beginning point for the sampling process. Unfortunately, drawing samples from an undirected graphical model is an expensive, multipass process. The conceptually simplest approach is Gibbs sampling. Suppose we have a graphical model over an n-dimensional vector of random variables x. We iteratively visit each variable x; and draw a sample conditioned on all the other variables, from p(x; | x_;)", "083d657c-151e-423e-8be1-6596a49cfe51": "Datasets We investigated transfer learning performance on the Food-101 dataset , CIFAR-10 and CIFAR-100 , Birdsnap , the SUN397 scene dataset , Stanford Cars , FGVC Aircraft , the PASCAL VOC 2007 classi\ufb01cation task , the Describable Textures Dataset (DTD) , Oxford-IIIT Pets , Caltech-101 , and Oxford 102 Flowers .\n\nWe follow the evaluation protocols in the papers introducing these datasets, i.e., we report top-1 accuracy for Food-101, CIFAR-10, CIFAR-100, Birdsnap, SUN397, Stanford Cars, and DTD; mean per-class accuracy for FGVC Aircraft, Oxford-IIIT Pets, Caltech-101, and Oxford 102 Flowers; and the 11-point mAP metric as de\ufb01ned in Everingham et al. for PASCAL VOC 2007. For DTD and SUN397, the dataset creators de\ufb01ned multiple train/test splits; we report results only for the \ufb01rst split. Caltech-101 de\ufb01nes no train/test split, so we randomly chose 30 images per class and test on the remainder, for fair comparison with previous work", "e478695b-1ff3-4ce4-b087-0e34877aa7ef": "(Left)A linear function fit to the data suffers from underfitting\u2014it cannot capture the curvature that is present in the data. (Center)A quadratic function fit to the data generalizes well to unseen points.\n\nIt does not suffer from a significant amount of overfitting or underfitting. (Right)A polynomial of degree 9 fit to the data suffers from overfitting. Here we used the Moore-Penrose pseudoinverse to solve the underdetermined normal equations. The solution passes through all the training points exactly, but we have not been lucky enough for it to extract the correct structure. It now has a deep valley between two training points that does not appear in the true underlying function. It also increases sharply on the left side of the data, while the true function decreases in this area. function is quadratic. The linear function is unable to capture the curvature in the true underlying problem, so it underfits. The degree-9 predictor is capable of representing the correct function, but it is also capable of representing infinitely many other functions that pass exactly through the training points, because we have more parameters than training examples", "6c0b3f74-e803-4229-900d-2c77f1cce36a": "Evaluate the misclassi\ufb01cation rates for the two trees and hence show that they are equal.\n\nSimilarly, evaluate the cross-entropy (14.32) and Gini index (14.33) for the two trees and show that they are both lower for tree B than for tree A. 14.12 (\u22c6 \u22c6) Extend the results of Section 14.5.1 for a mixture of linear regression models to the case of multiple target values described by a vector t. To do this, make use of the results of Section 3.1.5. 14.14 (\u22c6) Use the technique of Lagrange multipliers (Appendix E) to show that the M-step re-estimation equation for the mixing coef\ufb01cients in the mixture of linear regression models trained by maximum likelihood EM is given by (14.38). 14.15 (\u22c6) www We have already noted that if we use a squared loss function in a regression problem, the corresponding optimal prediction of the target variable for a new input vector is given by the conditional mean of the predictive distribution. Show that the conditional mean for the mixture of linear regression models discussed in Section 14.5.1 is given by a linear combination of the means of each component distribution. Note that if the conditional distribution of the target data is multimodal, the conditional mean can give poor predictions", "c953772c-cbdf-4ea1-ab92-dc1d45b1f0a3": "By contrast, the advantage of the mixture density network approach is that the component densities and the mixing coef\ufb01cients share the hidden units of the neural network. Furthermore, in the mixture density network, the splits of the input space are further relaxed compared to the hierarchical mixture of experts in that they are not only soft, and not constrained to be axis aligned, but they can also be nonlinear. Exercises 14.1 (\u22c6 \u22c6) www Consider a set models of the form p(t|x, zh, \u03b8h, h) in which x is the input vector, t is the target vector, h indexes the different models, zh is a latent variable for model h, and \u03b8h is the set of parameters for model h. Suppose the models have prior probabilities p(h) and that we are given a training set X = {x1, . , xN} and T = {t1, . , tN}. Write down the formulae needed to evaluate the predictive distribution p(t|x, X, T) in which the latent variables and the model index are marginalized out", "5c754664-eb27-49de-a93c-fae7c27a93be": "The transcription project began with a choice of performance metrics and desired values for these metrics. An important general principle is to tailor the choice of metric to the business goals for the project. Because maps are only useful if they have high accuracy, it was important to set a high accuracy requirement for this project. Specifically, the goal was to obtain human-level, 98 percent accuracy. This level of accuracy may not always be feasible to obtain. To reach this level of accuracy, the Street View transcription system sacrificed coverage.\n\nCoverage thus became the main performance metric optimized during the project, with accuracy held at 98 percent. As the convolutional network improved, it became possible to reduce the confidence threshold below which the network refused to transcribe the input, eventually exceeding the goal of 95 percent coverage. After choosing quantitative goals, the next step in our recommended methodol-  435  CHAPTER 11. PRACTICAL METHODOLOGY  ogy is to rapidly establish a sensible baseline system. For vision tasks, this means a convolutional network with rectified linear units. The transcription project began with such a model. At the time, it was not common for a convolutional network (0 output a sequence of predictions", "edfada0a-822c-4568-877d-a9b5944d1c29": "For example, Roweis and Ghahramani  uni\ufb01ed various unsupervised learning algorithms with a linear Gaussian model; Wainwright and Jordan (n.d.) presented the variational method for inference in general exponential-family graphical models; Domingos  and Richardson and Domingos  presented Markov logic networks that combine Bayesian Markov network with \ufb01rst-order logic for uncertain inference; Knoblauch et al. developed a generalized form of variational Bayesian inference by allowing losses and divergences beyond the standard likelihood and KL divergence, which subsumes existing variants for Bayesian posterior approximation; Altun and Smola  showed the duality between regularized divergence (e.g., Bregman and f-divergence) minimization and statistical inference (e.g., MAP estimation); Arora et al.\n\npresented a common formulation of di\ufb00erent multiplicative weights update methods; Mohamed and Lakshminarayanan  connected generative adversarial learning with a rich set of other statistical learning principles; Y. N. Wu et al. discussed connections between generative, discriminative, and energybased models. Ma et al", "865a8ac2-2621-4ce6-b273-90a2cbcea878": "Using the orthonormality property of the matrix U, we see that the square of the determinant of the Jacobian matrix is and hence |J| = 1. Also, the determinant |\u03a3| of the covariance matrix can be written as the product of its eigenvalues, and hence Thus in the yj coordinate system, the Gaussian distribution takes the form which is the product of D independent univariate Gaussian distributions. The eigenvectors therefore de\ufb01ne a new set of shifted and rotated coordinates with respect to which the joint probability distribution factorizes into a product of independent distributions. The integral of the distribution in the y coordinate system is then where we have used the result (1.48) for the normalization of the univariate Gaussian. This con\ufb01rms that the multivariate Gaussian (2.43) is indeed normalized.\n\nWe now look at the moments of the Gaussian distribution and thereby provide an interpretation of the parameters \u00b5 and \u03a3. The expectation of x under the Gaussian distribution is given by where we have changed variables using z = x \u2212 \u00b5", "6187d76f-c0cc-4e51-9c9b-8b91a62b1606": "One is  https://www.deeplearningbook.org/contents/convnets.html    the extreme case in which no zero padding is used whatsoever, and the convolution kernel is allowed to visit only Positions where the entire kernel is contained entirely within the image. In MATLAB terminology, this is called valid convolution. In  this case, all pixels in the output are a function of the same number of pixels in the input, so the behavior of an output pixel is somewhat more regular. However, the size of the output shrinks at each layer. If the input image has width m and the kernel has width k, the output will be of width m\u2014k-+ 1. The rate of this shrinkage can be dramatic if the kernels used are large. Since the shrinkage is greater than 0, it limits the number of convolutional layers that can be included in the network.\n\nAs layers are added, the spatial dimension of the network will eventually drop to 1 x 1, at which point additional layers cannot meaningfully be considered convolutional. Another special case of the zero-padding setting is when just enough zero padding is added to keep the size of the output equal to the size of the input. MATLAB calls this same convolution", "15d94e30-350d-428f-9068-23e4825ac750": "MIT Press, Cambridge, MA. Kakade, S. M. On the Sample Complexity of Reinforcement Learning. Ph.D. thesis, Kakutani, S. Markov processes and the Dirichlet problem. Proceedings of the Japan Kalos, M. H., Whitlock, P. A. Monte Carlo Methods. Wiley, New York. Kamin, L. J. \u201cAttention-like\u201d processes in classical conditioning. In M. R. Jones (Ed. ), Miami Symposium on the Prediction of Behavior, 1967: Aversive Stimulation, pp. 9\u201331. University of Miami Press, Coral Gables, Florida. Kamin, L. J. Predictability, surprise, attention, and conditioning. In B. A. Campbell and Kandel, E. R., Schwartz, J. H., Jessell, T. M., Siegelbaum, S", "1a8cb714-a054-4844-bc50-ddf5b7f3e570": "LINEAR ALGEBRA  = arg min Tr ((x \u2014 Xda\") (x - xad')) 2.74  (by equation 2.49)  = argminTr(X'X \u2014 X'Xdd' \u2014dd'X'X+dd'X'Xdd') 2.75 d  = argmin Tr(X' X) \u2014 Tr(X'Xdd')\u2014 Tr(dd' X' X) + Tr(dd'X' Xdd')  d 2.76 = argmin\u2014Tr(X! Xdd') \u2014 Tr(dd' X'X)+Tr(dd' X'Xdd!) (2.77 d  (because terms not involving d do not affect the arg min)  https://www.deeplearningbook.org/contents/linear_algebra.html  = argynin \u20142'Ir(X Add\u2019 )+'1lr(\\dd\u2019 A Aad )  (2.78)  (because we can cycle the order of the matrices inside a trace, equation 2.52)  = argmin \u20142Tr(X ' Xdd') + Tr(X 'Xdd'dd') d  using the same property again).\n\ng property ag;  At this point, we reintroduce the constraint: argmin \u20142Tr(X'Xdd') + Tr(X'Xdd'dd\u00b0\u2018) subject to d'd=1 d = argmin\u20142 Tr(X! Xdd') + Tr(X 'Xdd') subject to d'd=1 d (due to the constraint)  = argmin \u2014 Tr(X' Xdd\") subject to d'd=1 d  = arg max Tr(X 'Xdd\") subject to d'd=1 d  = arg max Tr(d' X | Xd) subject to d'd =1", "ce69ed26-ff36-487d-8e64-5d3bae4b996e": "As we shall see shortly, the columns of W span a linear subspace within the data space that corresponds to the principal subspace. The other parameter in this model is the scalar a 2 governing the variance of the conditional distribution.\n\nNote that there is no Flgu.", "31886001-9807-47c5-a81a-576ca744d3b6": "An infinitely strong prior places zero probability on some parameters and says 339  CHAPTER 9", "8dc32bfd-6ebe-4c1f-867f-17e1e83bdd0b": "Springer-Verlag, Berlin. Korf, R. E. Real-time heuristic search. Arti\ufb01cial Intelligence, 42(2\u20133), 189\u2013211. Koshland, D. E. Bacterial Chemotaxis as a Model Behavioral System. Raven Press, New Koza, J. R. Genetic Programming: On the Programming of Computers by Means of Natural Selection (Vol. 1). MIT Press., Cambridge, MA.\n\nKraft, L. G., Campagna, D. P. A summary comparison of CMAC neural network and traditional adaptive control systems. In T. Miller, R. S. Sutton, and P. J. Werbos (Eds. ), Neural Networks for Control, pp. 143\u2013169. MIT Press, Cambridge, MA. Kraft, L. G., Miller, W. T., Dietz, D", "beb43c20-fda4-43c3-802f-5d606b6d7278": "defines a linear subspa::e \",hose dimensi\"nality is at \"\"'st N - 1, and SO there is linle point in applying PeA for ,'alue< of M thaI\"'\" greater than N I, Indeed, if \"'e pelf\"\",, PeA we will find that at least D - N + I of the eigen\".lues art lero. eorrespnnding tQ eigenvectors aloog \",hose direclioos the data <el has 10m varianee. Funhem>ore. typical algOl\"ithm, for finding the eigen,'eet\"\" ofa D x D matrix ha\"e a computatiooal eosl thm scales like O( D~J. aOO so for appliealions such as the image e,ample. a direc' application of PeA will be computatiooally infe,,-sibJe.\n\nW", "9fe18745-c6eb-4fbb-a80a-33dbd7a849ec": "This property is useful because it means that gradient-based learning can act to quickly correct a mistaken z. When we use other loss functions, such as mean squared error, the loss can saturate anytime o(z) saturates. The sigmoid activation function saturates to 0 when z becomes very negative and saturates to 1 when z becomes very positive. The gradient can shrink too small to be useful for learning when this happens, whether the model has the correct answer or the incorrect answer. For this reason, maximum likelihood is almost always the preferred approach to training sigmoid output units.\n\nAnalytically, the logarithm of the sigmoid is always defined and finite, because the sigmoid returns values restricted to the open interval (0, 1), rather than using  4 n andi. Alan. tet ne--.1 2 2-012 2A Liat. PA 11) TR. 7A fe 11 dt nee  https://www.deeplearningbook.org/contents/mlp.html    LUC CLLUILe CLOSCU IULEL Val OL Valu PLOVAVIIULLLES | Ust}", "2b68c710-31d4-4f0c-b30e-e82e08f949c6": "These two Monte Carlo (MC) methods are very similar but have slightly di\u21b5erent theoretical properties.\n\nFirst-visit MC has been most widely studied, dating back to the 1940s, and is the one we focus on in this chapter. Every-visit MC extends more naturally to function approximation and eligibility traces, as discussed in Chapters 9 and 12. First-visit MC is shown in procedural form in the box. Every-visit MC would be the same except without the check for St having occurred earlier in the episode. Both \ufb01rst-visit MC and every-visit MC converge to v\u21e1(s) as the number of visits (or \ufb01rst visits) to s goes to in\ufb01nity. This is easy to see for the case of \ufb01rst-visit MC. In this case each return is an independent, identically distributed estimate of v\u21e1(s) with \ufb01nite variance. By the law of large numbers the sequence of averages of these estimates converges to their expected value. Each average is itself an unbiased estimate, and the standard deviation of its error falls as 1/pn, where n is the number of returns averaged", "4b4af9fa-f560-4c76-88f4-78a2c7625949": "The input can be provided as the initial state of the RNN, or the input can be connected to the hidden units at each time step. These two ways can also be combined. There is no constraint that the encoder must have the same size of hidden layer as the decoder. One clear limitation of this architecture is when the context C output by the encoder RNN has a dimension that is too small to properly summarize a long sequence. This phenomenon was observed by Bahdanau et al. in the context of machine translation. They proposed to make C a variable-length sequence rather than a fixed-size vector. Additionally, they introduced an attention mechanism that learns to associate elements of the sequence C to elements of the output sequence. See section 12.4.5.1 for more details. 10.5 Deep Recurrent Networks  The computation in most RNNs can be decomposed into three blocks of parameters and associated transformations:  1", "7af00ce0-7a62-45a3-b4f4-6992abed00c3": "Write down the differential equation satisfied by f (x) and draw a diagram illustrating the transformation of the density. 12.29 (**)Em Suppose that two variables Zl and Z2 are independent so thatp(zl' Z2) = P(Zl)P(Z2)' Show that the covariance matrix between these variables is diagonal. This shows that independence is a sufficient condition for two variables to be uncorrelated. Now consider two variables Yl and Y2 in which -1 :0;; Yl :0;; 1 and Y2 = yg. Write down the conditional distribution p(Y2IYl) and observe that this is dependent on Yb showing that the two variables are not independent. Now show that the covariance matrix between these two variables is again diagonal. To do this, use the relation P(Yl, Y2) = P(YI )p(Y2IYl) to show that the off-diagonal terms are zero. This counter-example shows that zero correlation is not a sufficient condition for independence", "16a8185d-2e88-4a68-a723-e9b0987c1d50": ", L we \ufb01rst draw a sample \u03b8(l) from the current estimate for p(\u03b8|X), and then use this to draw a sample Z(l) from p(Z|\u03b8(l), X). we use the samples {Z(l)} obtained from the I-step to compute a revised estimate of the posterior distribution over \u03b8 given by By assumption, it will be feasible to sample from this approximation in the I-step. Note that we are making a (somewhat arti\ufb01cial) distinction between parameters \u03b8 and hidden variables Z. From now on, we blur this distinction and focus simply on the problem of drawing samples from a given posterior distribution. In the previous section, we discussed the rejection sampling and importance sampling strategies for evaluating expectations of functions, and we saw that they suffer from severe limitations particularly in spaces of high dimensionality.\n\nWe therefore turn in this section to a very general and powerful framework called Markov chain Monte Carlo (MCMC), which allows sampling from a large class of distributions, and which scales well with the dimensionality of the sample space. Markov chain Monte Carlo methods have their origins in physics , and it was only towards the end of the 1980s that they started to have a signi\ufb01cant impact in the \ufb01eld of statistics", "cd08d7d7-8837-40d9-b6d9-1e3435eeee9f": "A contestant selects a square, the host reads the square\u2019s clue, and each contestant may choose to respond to the clue by sounding a buzzer (\u201cbuzzing in\u201d). The \ufb01rst contestant to buzz in gets to try responding to the clue. If this contestant\u2019s response is correct, their score increases by the dollar value of the square; if their response is not correct, or if they do not respond within \ufb01ve seconds, their score decreases by that amount, and the other contestants get a chance to buzz in to respond to the same clue. One or two squares (depending on the game\u2019s current round) are special DD squares.\n\nA contestant who selects one of these gets an exclusive opportunity to respond to the square\u2019s clue and has to decide\u2014before the clue is revealed\u2014on how much to wager, or bet. The bet has to be greater than \ufb01ve dollars but not greater than the contestant\u2019s current score. If the contestant responds correctly to the DD clue, their score increases by the bet amount; otherwise it decreases by the bet amount", "76c90e49-57c0-4418-9eb5-cfb1c7c77520": "replacing many 3x3 Convs with 1x1 Convs). In our framework, we decouple the prediction task and encoder architecture, by random cropping (with resizing) and using the \ufb01nal A Simple Framework for Contrastive Learning of Visual Representations representations of two augmented views for prediction, so we can use standard and more powerful ResNets. Our NT-Xent loss function leverages normalization and temperature to restrict the range of similarity scores, whereas they use a tanh function with regularization. We use a simpler data augmentation policy, while they use FastAutoAugment for their best result. \u2022 CPC v1 and v2  de\ufb01ne the context prediction task using a deterministic strategy to split examples into patches, and a context aggregation network (a PixelCNN) to aggregate these patches. The base encoder network sees only patches, which are considerably smaller than the original image", "5f58efd5-491e-41a5-abec-85853e80d68d": "Now suppose that training is halted after a \ufb01nite number \u03c4 of steps. Show that the components of the weight vector parallel to the eigenvectors of the Hessian satisfy Compare this result with the discussion in Section 3.5.3 of regularization with simple weight decay, and hence show that (\u03c1\u03c4)\u22121 is analogous to the regularization parameter \u03bb. The above results also show that the effective number of parameters in the network, as de\ufb01ned by (3.91), grows as the training progresses", "bc5ec7c6-8d11-4f6f-8645-377f64875036": "Using Bayes\u2019 theorem, we have Note that the denominator p(X) is implicitly conditioned on the parameters \u03b8old of the HMM and hence represents the likelihood function. Using the conditional independence property (13.24), together with the product rule of probability, we obtain \u03b3(zn) = p(x1, . , xn, zn)p(xn+1, . , xN|zn) The quantity \u03b1(zn) represents the joint probability of observing all of the given data up to time n and the value of zn, whereas \u03b2(zn) represents the conditional probability of all future data from time n + 1 up to N given the value of zn.\n\nAgain, \u03b1(zn) and \u03b2(zn) each represent set of K numbers, one for each of the possible settings of the 1-of-K coded binary vector zn. We shall use the notation \u03b1(znk) to denote the value of \u03b1(zn) when znk = 1, with an analogous interpretation of \u03b2(znk)", "1583bfb0-b11e-4e86-845d-15fe73e6681b": "The term action value is due to Watkins . The \ufb01rst to use \"-greedy methods may also have been Watkins (1989, p. 187), but the idea is so simple that some earlier use seems likely. 2.7 Early work on using estimates of the upper con\ufb01dence bound to select actions was done by Lai and Robbins , Kaelbling , and Agrawal . The UCB algorithm we present here is called UCB1 in the literature and was \ufb01rst developed by Auer, Cesa-Bianchi and Fischer .\n\n2.8 Gradient bandit algorithms are a special case of the gradient-based reinforcement learning algorithms introduced by Williams , and that later developed into the actor\u2013critic and policy-gradient algorithms that we treat later in this book. Our development here was in\ufb02uenced by that by Balaraman Ravindran (personal communication). Further discussion of the choice of baseline is provided there and by Greensmith, Bartlett, and Baxter  and Dick . Early systematic studies of algorithms like this were done by Sutton . The term soft-max for the action selection rule (2.11) is due to Bridle . This rule appears to have been \ufb01rst proposed by Luce", "527b41af-0fbe-4541-86bd-2152deee22b8": "Then: 10 We \ufb01x these at defaults of (wmin, \u00afw, wmax) = (0.5, 1.0, 1.5), which corresponds to assuming labeling functions have accuracies between 62 and 82%, and an average accuracy of 73%. Fig. 8 The predicted ( \u02dcA\u2217) and actual (Aw) advantage of using the generative labeling model (GM) over majority vote (MV) on the CDR application as the number of LFs is increased. At 9 LFs, the optimizer switches from choosing MV to choosing GM; this leads to faster modeling in early development cycles, and more accurate results in later cycles We apply \u02dcA\u2217 to a synthetic dataset and plot in Fig. 7. Next, we compute \u02dcA\u2217 for the labeling matrices from experiments in Sect.\n\n4.1 and compare with the empirical advantage of the trained generative models (Table 1).11 We see that our approximate quantity \u02dcA\u2217 serves as a correct guide in all cases for determining which modeling strategy to select, which for the mature applications reported on is indeed most often the generative model", "4eb7b541-2997-4220-b987-dfcdfa0c5dd2": "Ralph Linsker. An application of the principle of maximum information preservation to linear systems.\n\nMorgan Kaufmann Publishers Inc., 1989. Rajesh Ranganath, Sean Gerrish, and David M Blei. Black Box Variational Inference. arXiv preprint arXiv:1401.0118, 2013. Sam Roweis. EM algorithms for PCA and SPCA. Advances in neural information processing systems, pages 626\u2013632, 1998. Tim Salimans and David A Knowles. Fixed-form variational posterior approximation through stochastic linear regression. Bayesian Analysis, 8(4), 2013. Ruslan Salakhutdinov and Hugo Larochelle. Ef\ufb01cient learning of deep boltzmann machines. In International Conference on Arti\ufb01cial Intelligence and Statistics, pages 693\u2013 700, 2010. Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "f6f70077-1757-4b3a-b9d2-5a47ea0180d1": "Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., Chen, Y., Lillicrap, L., Hui, F., Sifre, L., van den Driessche, G., Graepel, T., Hassibis, D. Mastering the game of Go without human knowledge. Nature, 550:354\u2013359.\n\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., Lillicrap, T., Simoyan, K., Hassibis, D. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. ArXiv:1712.01815", "ad7edaea-61cc-421c-9962-f857cd34e080": "The L! norm is often used as a substitute for the number of nonzero entries. One other norm that commonly arises in machine learning is the [\u00b0\u00b0 norm, also known as the max norm. This norm simplifies to the absolute value of the element with the largest magnitude in the vector,  I|2I|o0 = max |i. (2.32)  Sometimes we may also wish to measure the size of a matrix. In the context of deep learning, the most common way to do this is with the otherwise obscure  Frobenius norm: _ 2 Alle = [7 AR; (2.33) ij  which is analogous to the L? norm of a vector. The dot product of two vectors can be rewritten in terms of norms. Specifically, x! y = ||2|2/|yll2 cos, (2.34)  where @ is the angle between x and y. 2.6 Special Kinds of Matrices and Vectors  Some special kinds of matrices and vectors are particularly useful. Diagonal matrices consist mostly of zeros and have nonzero entries only along the main diagonal", "0a948dd0-688c-4a35-b0bb-dde0f8f143e7": "Not surprisingly, however, Fourier features have trouble with discontinuities because it is di\ufb03cult to avoid \u201cringing\u201d around points of discontinuity unless very high frequency basis functions are included. The number of features in the order-n Fourier basis grows exponentially with the dimension of the state space, but if that dimension is small enough (e.g., k \uf8ff 5), then one can select n so that all of the order-n Fourier features can be used. This makes the selection of features more-or-less automatic. For high dimension state spaces, however, it is necessary to select a subset of these features. This can be done using prior beliefs about the nature of the function to be approximated, and some automated selection methods can be adapted to deal with the incremental and nonstationary nature of reinforcement learning", "dbb40f39-5239-4fdc-976b-47f14675f843": "This is an application of a more general technique called importance sampling, which we describe in more detail in section 17.2. Unfortunately, even exact importance sampling is not efficient because it requires computing weights p;/q;, where pj = P(i | C), which can only be computed if all the scores a; are computed. The solution adopted for this application is called biased importance sampling, where the importance weights are normalized to sum to 1. When negative word n; is sampled, the associated gradient is weighted by  Pil Gri  wi =\u2014yv . j=l Pr; /Qn; >  (12.17)  https://www.deeplearningbook.org/contents/applications.html    La  These weights are used to give the appropriate importance to the \u2122 negative samples from \u00a2 used to form the estimated negative phase contribution to the  gradient: lv| m Oa 1 0an, \u00bb P(i| C) mm e wing\u201d (12.18) i=l 465  CHAPTER 12. APPLICATIONS  A unigram or a bigram distribution works well as the proposal distribution q. It is easy to estimate the parameters of such a distribution from data", "a5fce2f5-2d3e-4a23-a541-2289630dcde9": "DEEP GENERATIVE MODELS  a new form of Boltzmann machine requires some more care and creativity than developing a new neural network layer, because it is often difficult to find an energy function that maintains tractability of all the different conditional distributions needed to use the Boltzmann machine. Despite this required effort, the field remains open to innovation. 20.9 Back-Propagation through Random Operations  Traditional neural networks implement a deterministic transformation of some input variables a. When developing generative models, we often wish to extend  https://www.deeplearningbook.org/contents/generative_models.html    neural networks to Implement stochastic transtormations of \u00a3. Une straighttorward way to do this is to augment the neural network with extra inputs 2 that are sampled from some simple probability distribution, such as a uniform or Gaussian distribution. The neural network can then continue to perform deterministic computation internally, but the function f(a, z) will appear stochastic to an observer who does not have access to z", "b5e4a5b4-c309-4fd0-a062-f69665326402": "State features included the number of read requests in the transaction queue, the number of write requests in the transaction queue, the number of write requests in the transaction queue waiting for their row to be opened, and the number of read requests in the transaction queue waiting for their row to be opened that are the oldest issued by their requesting processors. (The other features depended on how the DRAM interacts with cache memory, details we omit here.) The selection of the state features was based on \u02d9Ipek et al.\u2019s understanding of factors that impact DRAM performance.\n\nFor example, balancing the rate of servicing reads and writes based on how many of each are in the transaction queue can help avoid stalling the DRAM system\u2019s interaction with cache memory. The authors in fact generated a relatively long list of potential features, and then pared them down to a handful using simulations guided by stepwise feature selection. An interesting aspect of this formulation of the scheduling problem as an MDP is that the features input to the tile coding for de\ufb01ning the action-value function were di\u21b5erent from the features used to specify the action-constraint sets A(St)", "7587e93a-2db1-404b-9af7-bbc349aadb3f": "Note that our discussion will apply equally to concave functions with \u2018min\u2019 and \u2018max\u2019 interchanged and with lower bounds replaced by upper bounds.\n\nLet us begin by considering a simple example, namely the function f(x) = exp(\u2212x), which is a convex function of x, and which is shown in the left-hand plot of Figure 10.10. Our goal is to approximate f(x) by a simpler function, in particular a linear function of x. From Figure 10.10, we see that this linear function will be a lower bound on f(x) if it corresponds to a tangent. We can obtain the tangent line y(x) at a speci\ufb01c value of x, say x = \u03be, by making a \ufb01rst order Taylor expansion so that y(x) \u2a7d f(x) with equality when x = \u03be. For our example function f(x) = linear function \u03bbx, which is a lower bound on f(x) because f(x) > \u03bbx for all x", "dc6b3381-987c-4ff1-a31f-dff28d017bff": "Show that the contours of constant error are ellipses whose axes are aligned with the eigenvectors ui, with lengths that are inversely proportional to the square root of the corresponding eigenvalues \u03bbi. 5.12 (\u22c6 \u22c6) www By considering the local Taylor expansion (5.32) of an error function about a stationary point w\u22c6, show that the necessary and suf\ufb01cient condition for the stationary point to be a local minimum of the error function is that the Hessian matrix H, de\ufb01ned by (5.30) with \ufffdw = w\u22c6, be positive de\ufb01nite. 5.14 (\u22c6) By making a Taylor expansion, verify that the terms that are O(\u03f5) cancel on the right-hand side of (5.69). 5.15 (\u22c6 \u22c6) In Section 5.3.4, we derived a procedure for evaluating the Jacobian matrix of a neural network using a backpropagation procedure. Derive an alternative formalism for \ufb01nding the Jacobian based on forward propagation equations.\n\n5.16 (\u22c6) The outer product approximation to the Hessian matrix for a neural network using a sum-of-squares error function is given by (5.84)", "a9c6c8d8-0267-410d-aa81-34ad02f80aae": "The targets used to train the network are simply the input vectors themselves, so that the network is attempting to map each input vector onto itself. Such a network is said to form an autoassociative mapping. Since the number of hidden units is smaller than the number of inputs, a perfect reconstruction of all input vectors is not in general possible. We therefore determine the network parameters w by minimizing an error function which captures the degree of mismatch between the input vectors and their reconstructions. In particular, we shall choose a sum-of-squares error of the form If the hidden units have linear activations functions, then it can be shown that the error function has a unique global minimum, and that at this minimum the network performs a projection onto the M -dimensional subspace which is spanned by the first M principal components of the data . Thus, the vectors of weights which lead into the hidden units in Figure 12.18 form a basis set which spans the principal subspace. Note, however, that these vectors need not be orthogonal or normalized", "8717700f-3ebd-4c62-b98c-1777a7d8a387": "9.23 (\u22c6 \u22c6) www In Section 7.2.1 we used direct maximization of the marginal likelihood to derive the re-estimation equations (7.87) and (7.88) for \ufb01nding values of the hyperparameters \u03b1 and \u03b2 for the regression RVM.\n\nSimilarly, in Section 9.3.4 we used the EM algorithm to maximize the same marginal likelihood, giving the re-estimation equations (9.67) and (9.68). Show that these two sets of re-estimation equations are formally equivalent. 9.25 (\u22c6) www Show that the lower bound L(q, \u03b8) given by (9.71), with q(Z) = p(Z|X, \u03b8(old)), has the same gradient with respect to \u03b8 as the log likelihood function ln p(X|\u03b8) at the point \u03b8 = \u03b8(old). 9.26 (\u22c6) www Consider the incremental form of the EM algorithm for a mixture of Gaussians, in which the responsibilities are recomputed only for a speci\ufb01c data point xm. Starting from the M-step formulae (9.17) and (9.18), derive the results (9.78) and (9.79) for updating the component means", "fb8eb06f-4d09-483d-acc3-cc709573a548": "In a similar vein we will refer to p\u03b8(x|z) as a probabilistic decoder, since given a code z it produces a distribution over the possible corresponding values of x.\n\nThe marginal likelihood is composed of a sum over the marginal likelihoods of individual datapoints log p\u03b8(x(1), \u00b7 \u00b7 \u00b7 , x(N)) = \ufffdN i=1 log p\u03b8(x(i)), which can each be rewritten as: log p\u03b8(x(i)) = DKL(q\u03c6(z|x(i))||p\u03b8(z|x(i))) + L(\u03b8, \u03c6; x(i)) (1) The \ufb01rst RHS term is the KL divergence of the approximate from the true posterior. Since this KL-divergence is non-negative, the second RHS term L(\u03b8, \u03c6; x(i)) is called the (variational) lower bound on the marginal likelihood of datapoint i, and can be written as: We want to differentiate and optimize the lower bound L(\u03b8, \u03c6; x(i)) w.r.t. both the variational parameters \u03c6 and generative parameters \u03b8", "a824deeb-7f7c-4389-9f3e-eb313e96d435": "When using an extremely large training set, overfitting is not an issue, so underfitting and computational efficiency become the predominant concerns.\n\nSee also Bottou and Bousquet  for a discussion of the effect of computational  Latta nclen 2 wan nian dian sen 25 ALA nee ne Af LA ttn nen en een  https://www.deeplearningbook.org/contents/optimization.html    VULLICLICCKS ULL BCHECLANUZALION CLLUL, ad LIC HUILIVEL UL LIANE CAALUIpPIES BLOWS. 8.2 Challenges in Neural Network Optimization  Optimization in general is an extremely difficult task. Traditionally, machine learning has avoided the difficulty of general optimization by carefully designing the objective function and constraints to ensure that the optimization problem is convex. When training neural networks, we must confront the general nonconvex case. Even convex optimization is not without its complications. In this section, we summarize several of the most prominent challenges involved in optimization for training deep models. 8.2.1 Ill-Conditioning  Some challenges arise even when optimizing convex functions", "10b4f3c4-de52-46ef-9f29-120af7730052": "The use of value functions distinguishes reinforcement learning methods from evolutionary methods that search directly in policy space guided by evaluations of entire policies. The early history of reinforcement learning has two main threads, both long and rich, that were pursued independently before intertwining in modern reinforcement learning.\n\nOne thread concerns learning by trial and error, and originated in the psychology of animal learning. This thread runs through some of the earliest work in arti\ufb01cial intelligence and led to the revival of reinforcement learning in the early 1980s. The second thread concerns the problem of optimal control and its solution using value functions and dynamic programming. For the most part, this thread did not involve learning. The two threads were mostly independent, but became interrelated to some extent around a third, less distinct thread concerning temporal-di\u21b5erence methods such as that used in the tic-tac-toe example in this chapter. All three threads came together in the late 1980s to produce the modern \ufb01eld of reinforcement learning as we present it in this book. The thread focusing on trial-and-error learning is the one with which we are most familiar and about which we have the most to say in this brief history. Before doing that, however, we brie\ufb02y discuss the optimal control thread", "9f695841-72e2-493e-997f-37c485e6b99a": "For this one needs the conjunctive rectangular tiles such as originally shown in Figure 9.9.\n\nWith multiple tilings\u2014some horizontal, some vertical, and some conjunctive\u2014one can get everything: a preference for generalizing along each dimension, yet the ability to learn speci\ufb01c values for conjunctions (see Sutton, 1996 for examples). The choice of tilings determines generalization, and until this choice can be e\u21b5ectively automated, it is important that tile coding enables the choice to be made \ufb02exibly and in a way that makes sense to people. Another useful trick for reducing memory requirements is hashing\u2014a consistent pseudorandom collapsing of a large tiling into a much smaller set of tiles. Hashing produces tiles consisting of noncontiguous, disjoint regions randomly spread throughout the state space, but that still form an exhaustive partition. For example, one tile might consist of the four subtiles shown to the right. Through hashing, memory requirements are often reduced by large factors with little loss of performance. This is possible because high resolution is needed in only a small fraction of the state space", "337e3210-3099-4cf6-a7e0-f843c249a6eb": "With no observed data points, we have the prior variance, whereas if the number of data points N \u2192 \u221e, the variance \u03c32 N goes to zero and the posterior distribution becomes in\ufb01nitely peaked around the maximum likelihood solution.\n\nWe therefore see that the maximum likelihood result of a point estimate for \u00b5 given by (2.143) is recovered precisely from the Bayesian formalism in the limit of an in\ufb01nite number of observations. Note also that for \ufb01nite N, if we take the limit \u03c32 0 \u2192 \u221e in which the prior has in\ufb01nite variance then the posterior mean (2.141) reduces to the maximum likelihood result, while from (2.142) the posterior variance is given by \u03c32 N = \u03c32/N. We illustrate our analysis of Bayesian inference for the mean of a Gaussian distribution in Figure 2.12. The generalization of this result to the case of a Ddimensional Gaussian random variable x with known covariance and unknown mean is straightforward", "0f8115e2-f3bc-4f5e-b959-756d0f9241a5": "To explore all possibilities, we require that the behavior policy be soft (i.e., that it select all actions in all states with nonzero probability). The box on the next page shows an o\u21b5-policy Monte Carlo control method, based on GPI and weighted importance sampling, for estimating \u21e1\u21e4 and q\u21e4. The target policy \u21e1 \u21e1 \u21e1\u21e4 is the greedy policy with respect to Q, which is an estimate of q\u21e1.\n\nThe behavior policy b can be anything, but in order to assure convergence of \u21e1 to the optimal policy, an in\ufb01nite number of returns must be obtained for each pair of state and action. This can be assured by choosing b to be \"-soft. The policy \u21e1 converges to optimal at all encountered states even though actions are selected according to a di\u21b5erent soft policy b, which may change between or even within episodes. A potential problem is that this method learns only from the tails of episodes, when all of the remaining actions in the episode are greedy. If nongreedy actions are common, then learning will be slow, particularly for states appearing in the early portions of long episodes. Potentially, this could greatly slow learning", "a8d3e711-25d4-485e-8e8d-22a70f147af2": "Also shown are three examples of histogram density estimates corresponding to three different choices for the bin width \u2206. We see that when \u2206 is very small (top \ufb01gure), the resulting density model is very spiky, with a lot of structure that is not present in the underlying distribution that generated the data set. Conversely, if \u2206 is too large (bottom \ufb01gure) then the result is a model that is too smooth and that consequently fails to capture the bimodal property of the green curve. The best results are obtained for some intermediate value of \u2206 (middle \ufb01gure). In principle, a histogram density model is also dependent on the choice of edge location for the bins, though this is typically much less signi\ufb01cant than the value of \u2206.\n\nNote that the histogram method has the property (unlike the methods to be discussed shortly) that, once the histogram has been computed, the data set itself can be discarded, which can be advantageous if the data set is large. Also, the histogram approach is easily applied if the data points are arriving sequentially. In practice, the histogram technique can be useful for obtaining a quick visualization of data in one or two dimensions but is unsuited to most density estimation applications", "d8b4f518-9f1e-4708-8ab2-7fd2ed17d7a7": "In the direction w1 the eigenvalue \u03bb1, de\ufb01ned by (3.87), is small compared with \u03b1 and so the quantity \u03bb1/(\u03bb1 + \u03b1) is close to zero, and the corresponding MAP value of w1 is also close to zero. By contrast, in the direction w2 the eigenvalue \u03bb2 is large compared with \u03b1 and so the quantity \u03bb2/(\u03bb2 +\u03b1) is close to unity, and the MAP value of w2 is close to its maximum likelihood value. The result (3.92) has an elegant interpretation , which provides insight into the Bayesian solution for \u03b1. To see this, consider the contours of the likelihood function and the prior as illustrated in Figure 3.15. Here we have implicitly transformed to a rotated set of axes in parameter space aligned with the eigenvectors ui de\ufb01ned in (3.87).\n\nContours of the likelihood function are then axis-aligned ellipses", "37052ccb-b7c1-4bab-ab49-201610d79684": "(12.7)  A fundamental limitation of maximum likelihood for n-gram models is that P,, as estimated from training set counts is very likely to be zero in many cases, even though the tuple (%;~n41,...,vz) May appear in the test set. This can cause two different kinds of catastrophic outcomes. When P,_ is zero, the ratio is undefined, so the model does not even produce a sensible output. When P,,\u20141 is nonzero but P, is zero, the test log-likelihood is \u2014oo.\n\nTo avoid such catastrophic outcomes, most n-gram models employ some form of smoothing. Smoothing techniques shift probability mass from the observed tuples to unobserved ones that are similar. See Chen and Goodman  for a review and empirical comparisons. One basic technique consists of adding nonzero probability mass to all the possible next symbol values. This method can be justified as Bayesian inference with a uniform  457  CHAPTER 12. APPLICATIONS  or Dirichlet prior over the count parameters", "a7467ead-4353-464f-93c6-1024e5918ece": "learning of the size of the receptive \ufb01elds in coarse coding. Linear function approximation based on coarse coding and (9.7) was used to learn a one-dimensional square-wave function (shown at the top of Figure 9.8). The values of this function were used as the targets, Ut. With just one dimension, the receptive \ufb01elds were intervals rather than circles. Learning was repeated with three di\u21b5erent sizes of the intervals: narrow, medium, and broad, as shown at the bottom of the \ufb01gure. All three cases had the same density of features, about 50 over the extent of the function being learned. Training examples were generated uniformly at random over this extent. The step-size parameter was \u21b5 = 0.2 the number of features that were present at one time. Figure 9.8 shows the functions learned in all three cases over the course of learning. Note that the width of the features had a strong e\u21b5ect early in learning.\n\nWith broad features, the generalization tended to be broad; with narrow features, only the close neighbors of each trained point were changed, causing the function learned to be more bumpy", "09ba03c9-4c18-45ec-84d1-81570b3efc01": "Additionally, the bottom RBM must be trained using two \u201ccopies\u201d of each visible unit and the weights tied to be equal between the two copies. This means that the weights are effectively doubled during the upward pass. Similarly, the top RBM should be trained with two copies of the topmost layer. Obtaining the state of the art results with the deep Boltzmann machine requires a modification of the standard SML algorithm, which is to use a small amount of mean field during the negative phase of the joint PCD training step .\n\nSpecifically, the expectation of the energy gradient should  La aaencn--bnd eb nn nk be AL nn GAIA Alsat eettae te tak AW ihn. https://www.deeplearningbook.org/contents/generative_models.html    VE CULLLPULEU WILLE LESPeCl LO LIC MWiCall WEL UISLLIDULIOLL Hl WIMCI all LLC ULLLLS are independent from each other, The parameters of this mean field distribution should be obtained by running the mean field fixed-point equations for just one step. See Goodfellow et al", "08b8d7c7-6c2d-4f6a-a42e-21947aecd7c7": "Must we wait for exact convergence, or can we stop short of that? The example in Figure 4.1 certainly suggests that it may be possible to truncate policy evaluation. In that example, policy evaluation iterations beyond the \ufb01rst three have no e\u21b5ect on the corresponding greedy policy.\n\nIn fact, the policy evaluation step of policy iteration can be truncated in several ways without losing the convergence guarantees of policy iteration. One important special case is when policy evaluation is stopped after just one sweep (one update of each state). This algorithm is called value iteration. It can be written as a particularly simple update operation that combines the policy improvement and truncated policy evaluation steps: for all s 2 S. For arbitrary v0, the sequence {vk} can be shown to converge to v\u21e4 under the same conditions that guarantee the existence of v\u21e4. Another way of understanding value iteration is by reference to the Bellman optimality equation (4.1). Note that value iteration is obtained simply by turning the Bellman optimality equation into an update rule. Also note how the value iteration update is identical to the policy evaluation update (4.5) except that it requires the maximum to be taken over all actions", "0fc3d01e-de64-483a-b787-d467adb72dc0": "Shorten and Khoshgoftaar J Big Data  6:60   Fig.\n\n21 t-SNE visualization demonstrating the improved decision boundaries when using CycleGAN-generated samples. a original CNN model, b adding GAN-generated disgust images, \u00a2 adding GAN-generated sad images, d adding both GAN-generated disgust and sad images   Discriminator  aX X XX GSE  Fig. 22 Illustration from Mirza and Osindero  showing how the conditional y vector is integrated into the GAN framework  Lucic et al. sought out to compare newly developed GAN loss functions. They conducted a series of tests that determined most loss functions can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that increased computational power is a more promising area of focus than algorithmic changes in the generator versus discriminator loss function. Most of the research done in applying GANs to Data Augmentation and reporting the resulting classification performance has been done in biomedical image analysis Shorten and Khoshgoftaar J Big Data  6:60   . These papers have shown improved classification boundaries derived from train- ing with real and generated data from GAN models", "f236f7b7-6ced-4f21-b66f-517c1d166e46": "JMLR Workshop and Conference Proceedings, 2015.\n\nLuke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial networks. Corr, abs/1611.02163, 2016. Volodymyr Mnih, Adri`a Puigdom`enech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pages 1928\u20131937, 2016. Gr\u00b4egoire Montavon, Klaus-Robert M\u00a8uller, and Marco Cuturi. Wasserstein training of restricted boltzmann machines. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems 29, pages 3718\u20133726", "e07ba247-4a53-42a7-acbc-3782065e667f": "for function approximation and by Nair and Hinton  for the conditional distributions of undirected probabilistic models. Glorot ef al. compared the softplus and rectifier and found better results with the latter. The use of the softplus is generally discouraged. The softplus demonstrates that the performance of hidden unit types can be very counterintuitive\u2014one might expect it to have an advantage over the rectifier due to being differentiable everywhere or due to saturating less completely, but empirically it does not. .\n\nThis is shaped similarly to the tanh and the rectifier, but unlike e Hard tanh  oN ca + ofa ee  https://www.deeplearningbook.org/contents/mlp.html    the latter, it is bounded, 9(@) = max(\u20141,mim(1,4@)). It was introduced by Collobert . Hidden unit design remains an active area of research, and many useful hidden unit types remain to be discovered. 6.4 Architecture Design  Another key design consideration for neural networks is determining the architecture. The word architecture refers to the overall structure of the network: how many units it should have and how these units should be connected to each other", "5df94c18-2e49-4695-9c27-959c8c7db0dd": "However, in the original GMMN paper  they indeed used a minibatch of size 1000, much larger than the standard 32 or 64 (even when this incurred in quadratic computational cost). While estimates that have linear computational cost as a function of the number of samples exist , they have worse sample complexity, and to the best of our knowledge they haven\u2019t been yet applied in a generative context such as in GMMNs. On another great line of research, the recent work of  has explored the use of Wasserstein distances in the context of learning for Restricted Boltzmann Machines for discrete spaces. The motivations at a \ufb01rst glance might seem quite di\ufb00erent, since the manifold setting is restricted to continuous spaces and in \ufb01nite discrete spaces the weak and strong topologies (the ones of W and JS respectively) coincide. However, in the end there is more in commmon than not about our motivations.\n\nWe both want to compare distributions in a way that leverages the geometry of the underlying space, and Wasserstein allows us to do exactly that. Finally, the work of  shows new algorithms for calculating Wasserstein distances between di\ufb00erent distributions. We believe this direction is quite important, and perhaps could lead to new ways of evaluating generative models", "f1bd6c4a-97c0-4c66-850f-74de92f6f84b": "In fact, the evidence is suf\ufb01ciently strong that it outweighs the prior and makes it more likely that the red box was chosen rather than the blue one. Finally, we note that if the joint distribution of two variables factorizes into the product of the marginals, so that p(X, Y ) = p(X)p(Y ), then X and Y are said to be independent.\n\nFrom the product rule, we see that p(Y |X) = p(Y ), and so the conditional distribution of Y given X is indeed independent of the value of X. For instance, in our boxes of fruit example, if each box contained the same fraction of apples and oranges, then p(F|B) = P(F), so that the probability of selecting, say, an apple is independent of which box is chosen. As well as considering probabilities de\ufb01ned over discrete sets of events, we also wish to consider probabilities with respect to continuous variables. We shall limit ourselves to a relatively informal discussion", "428dfd89-b80e-406f-a478-2bb7f517728e": "This is what we have done to produce the tilings in the lower half of Figure 9.11, in which k = 2, n = 23 \u2265 4k, and the displacement vector is (1, 3). In a three-dimensional case, the \ufb01rst four tilings would be o\u21b5set in total from a base position by (0, 0, 0), (1, 3, 5), (2, 6, 10), and (3, 9, 15). Open-source software that can e\ufb03ciently make tilings like this for any k is readily available. In choosing a tiling strategy, one has to pick the number of the tilings and the shape of the tiles. The number of tilings, along with the size of the tiles, determines the resolution or \ufb01neness of the asymptotic approximation, as in general coarse coding and illustrated in Figure 9.8. The shape of the tiles will determine the nature of generalization as in Figure 9.7.\n\nSquare tiles will generalize roughly equally in each dimension as indicated in Figure 9.11 (lower). Tiles that are elongated along one dimension, such as the stripe tilings in Figure 9.12 (middle), will promote generalization along that dimension", "76c12d53-ec79-4648-9053-556cd4ac6346": "As a concrete example, consider the experiences that do not have an analytic form, but instead are de\ufb01ned in a variational way (i.e., as a A particular example is the adversarial experience emergingly used in many generation and representation learning problems.\n\nSpeci\ufb01cally, recall the data instance experience fdata(t; D) that measures the closeness between a con\ufb01guration t with the true data D based on data instance matching (Equation 4.2). Such manually de\ufb01ned measures could be subjective, suboptimal, or demanding expertise or heavy engineering to be properly speci\ufb01ed. An alternative way that sidesteps the drawbacks is to automatically induce a closeness measure f\u03c6(t), where \u03c6 denotes any free parameters associated with the experience and is to be learned. For example, one can measure the closeness of a con\ufb01guration t to the data set D based on a discriminator (or critic) that evaluates how easily t can be di\ufb00erentiated from the instances in D. A concrete application is in image generation, where a binary discriminator takes as input an image sample and tells whether the input image is a real instance from the observed image corpus D or a fake one produced by the model", "c2524bd0-ed38-42eb-a614-d44dd3c330ec": "199  CHAPTER 6.\n\nDEEP FEEDFORWARD NETWORKS  wv adeeen eden Aan neth ad tant A lene 2 natn nd Abb nn Af ane AA tne  https://www.deeplearningbook.org/contents/mlp.html    LICLWULKS, UCSCLIVCEU Ll CHAPLEL Y, USC SPCCLAMUZCU PALLELLUS OL Spalse CULLLCCLIOUS that are very effective for computer vision problems. In this chapter, it is difficult to give more specific advice concerning the architecture of a generic neural network. In subsequent chapters we develop the particular architectural strategies that have  been found to work well for different application domains. 6.5 Back-Propagation and Other Differentiation Algorithms  When we use a feedforward neural network to accept an input a and produce an output y, information flows forward through the network. The input a provides the initial information that then propagates up to the hidden units at each layer and finally produces y. This is called forward propagation", "2bcc241d-7a40-4ca4-80ed-c161a603436a": "More generally, introducing transition or emission models that depart from the linear-Gaussian (or other exponential family) model leads to an intractable inference problem.\n\nWe can make deterministic approximations such as assumed density \ufb01ltering or expectation propagation, or we can make use of sampling methods, Chapter 10 as discussed in Section 13.3.4. One widely used approach is to make a Gaussian approximation by linearizing around the mean of the predicted distribution, which gives rise to the extended Kalman \ufb01lter . As with hidden Markov models, we can develop interesting extensions of the basic linear dynamical system by expanding its graphical representation. For example, the switching state space model  can be viewed as a combination of the hidden Markov model with a set of linear dynamical systems. The model has multiple Markov chains of continuous linear-Gaussian latent variables, each of which is analogous to the latent chain of the linear dynamical system discussed earlier, together with a Markov chain of discrete variables of the form used in a hidden Markov model. The output at each time step is determined by stochastically choosing one of the continuous latent chains, using the state of the discrete latent variable as a switch, and then emitting an observation from the corresponding conditional output distribution", "3071a2bf-d9ed-4bc2-83ff-cc6b8a0b52eb": "When latent variables are used in the context of traditional graphical models, they are  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    oiten designed with some specific semantics im mind\u2014the topic ot a document, the intelligence of a student, the disease causing a patient\u2019s symptoms, and so forth.\n\nThese models are often much more interpretable by human practitioners and often have more theoretical guarantees, yet they are less able to scale to complex  problems and are not reusable in as many different contexts as deep models are. Another obvious difference is the kind of connectivity typically used in the deep learning approach. Deep graphical models typically have large groups of units that  582  CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  are all connected to other groups of units, so that the interactions between two groups may be described by a single matrix. Traditional graphical models have very few connections, and the choice of connections for each variable may be individually designed. The design of the model structure is tightly linked with the choice of inference algorithm. Traditional approaches to graphical models typically aim to maintain the tractability of exact inference", "c9e0cc5d-8eea-44db-8637-edb0185cc361": "The left-hand plot shows the original distribution (yellow) along with the Laplace (red), global variational (green), and EP (blue) approximations, and the right-hand plot shows the corresponding negative logarithms of the distributions.\n\nNote that the EP distribution is broader than that variational inference, as a consequence of the different form of KL divergence. where Zj is the normalization constant given by We now determine a revised factor \ufffdfj(\u03b8) by minimizing the Kullback-Leibler divergence This is easily solved because the approximating distribution qnew(\u03b8) is from the exponential family, and so we can appeal to the result (10.187), which tells us that the parameters of qnew(\u03b8) are obtained by matching its expected suf\ufb01cient statistics to the corresponding moments of (10.196). We shall assume that this is a tractable operation. For example, if we choose q(\u03b8) to be a Gaussian distribution N(\u03b8|\u00b5, \u03a3), then \u00b5 is set equal to the mean of the (unnormalized) distribution fj(\u03b8)q\\j(\u03b8), and \u03a3 is set to its covariance", "e36d0660-bfe0-4f16-bd3e-d60ea95da2da": "By de\ufb01nition, f is convex i\ufb00 f(\u03bby+(1\u2212\u03bb)x) \u2264 \u03bbf(y)+(1\u2212\u03bb)f(x). Writing z = \u03bby+(1\u2212\u03bb)x, and noting that f(z) = \u03bbf(z)+(1\u2212\u03bb)f(z) we have that f(z) = \u03bbf(z)+(1\u2212\u03bb)f(z) \u2264 \u03bbf(y)+(1\u2212\u03bb)f(x). By rearranging terms, an equivalent de\ufb01nition for convexity can be obtained: f is convex if By the mean value theorem, \u2203s, x \u2264 s \u2264 z s.t. Similarly, applying the mean value theorem to f(y) \u2212 f(z), \u2203t, z \u2264 t \u2264 y s.t. Thus we have the situation, x \u2264 s \u2264 z \u2264 t \u2264 y. By assumption, f \u2032\u2032(x) \u2265 0 on  so f \u2032(s) \u2264 f \u2032(t) since s \u2264 t", "a125bb86-7db4-4df6-adf2-04181954d883": "8.27 (\u22c6 \u22c6) Consider two discrete variables x and y each having three possible states, for example x, y \u2208 {0, 1, 2}. Construct a joint distribution p(x, y) over these variables having the property that the value \ufffdx that maximizes the marginal p(x), along with the value \ufffdy that maximizes the marginal p(y), together have probability zero under the joint distribution, so that p(\ufffdx,\ufffdy) = 0. 8.28 (\u22c6 \u22c6) www The concept of a pending message in the sum-product algorithm for a factor graph was de\ufb01ned in Section 8.4.7. Show that if the graph has one or more cycles, there will always be at least one pending message irrespective of how long the algorithm runs. 8.29 (\u22c6 \u22c6) www Show that if the sum-product algorithm is run on a factor graph with a tree structure (no loops), then after a \ufb01nite number of messages have been sent, there will be no pending messages", "ad07ff47-0f4a-448f-847b-5da9a3bbdade": "The polynomial regression example in figure 5.2 has a single hyperparameter: the degree of the polynomial, which acts as a capacity hyperparameter. The X value used to control the strength of weight decay is another example of a  hyperparameter.\n\nUnderfitting Appropriate weight decay Overfitting (Excessive ) (Medium A) (A0) T% Tt, Ts  Figure 5.5: We fit a high-degree polynomial regression model to our example training set from figure 5.2. The true function is quadratic, but here we use only models with degree 9. We vary the amount of weight decay to prevent these high-degree models from overfitting. (Left)With very large A, we can force the model to learn a function with no slope at all. This underfits because it can only represent a constant function. (Center)With a medium value of 4, the learning algorithm recovers a curve with the right general shape. Even though the model is capable of representing functions with much more complicated shapes, weight decay has encouraged it to use a simpler function described by smaller coefficients", "bf5a7307-cb16-4155-9fa0-369c170d8e77": "The slowness principle predates slow feature analysis and has been applied to a wide variety of models . In general, we can apply the slowness principle to any differentiable model trained with gradient descent.\n\nThe slowness principle may be introduced by adding a term to the cost function of the form  A L( f(a), f(a), (13.7)  where Ais ahyperparameter d\u00a5f\u00e9rmining the strength of the slowness regularization  term, t is the index into a ti\u00e9\u2019sequence of examples, f is the feature extractor  to be regularized, and L is a loss function measuring the distance between f (a) (t+1)  https://www.deeplearningbook.org/contents/linear_factors.html    and f(x ). A common choice tor L 1s the mean squared difference. Slow feature analysis is a particularly efficient application of the slowness principle. It is efficient because it is applied to a linear feature extractor and can thus be trained in closed form", "bf921d8d-d46e-43dd-b61b-db01402cdf4c": "The hypotheses have an average entailment probability of only 50%, and over 2/5 of them less than 20% (negative/contradictive examples) \u2013 a signi\ufb01cant challenge for the models to learn from the noises. The rewards include (1) the entailment score of the generation measured by a robust entailment classi\ufb01er , (2) the log-likelihood of the generation as an indicator of language quality measured by a GPT-2 language model , and (3) BLEU score w.r.t the input premises as another language quality reward that avoids trivial outputs. We sum together all rewards with weights 1.0", "e0b69550-6445-4313-bac2-3181cd7110c3": "Everyone we acknowledged for their inspiration and help with the \ufb01rst edition deserve our deepest gratitude for this edition as well, which would not exist were it not for their contributions to edition number one. To that long list we must add many others who contributed speci\ufb01cally to the second edition. Our students over the many years that we have taught this material contributed in countless ways: exposing errors, o\u21b5ering \ufb01xes, and\u2014not the least\u2014being confused in places where we could have explained things better. We especially thank Martha Steenstrup for reading and providing detailed comments throughout.\n\nThe chapters on psychology and neuroscience could not have been written without the help of many experts in those \ufb01elds. We thank John Moore for his patient tutoring over many many years on animal learning experiments, theory, and neuroscience, and for his careful reading of multiple drafts of Chapters 14 and 15. We also thank Matt Botvinick, Nathaniel Daw, Peter Dayan, and Yael Niv for their penetrating comments on drafts of these chapter, their essential guidance through the massive literature, and their interception of many of our errors in early drafts. Of course, the remaining errors in these chapters\u2014and there must still be some\u2014are totally our own", "6b717bbb-a5a6-4db7-b4c5-c9efa8440198": "Still, Redish\u2019s model illustrates how reinforcement learning theory can be enlisted in the e\u21b5ort to understand a major health problem. In a similar manner, reinforcement learning theory has been in\ufb02uential in the development of the new \ufb01eld of computational psychiatry, which aims to improve understanding of mental disorders through mathematical and computational methods. The neural pathways involved in the brain\u2019s reward system are complex and incompletely understood, but neuroscience research directed toward understanding these pathways and their roles in behavior is progressing rapidly. This research is revealing striking correspondences between the brain\u2019s reward system and the theory of reinforcement learning as presented in this book. The reward prediction error hypothesis of dopamine neuron activity was proposed by scientists who recognized striking parallels between the behavior of TD errors and the activity of neurons that produce dopamine, a neurotransmitter essential in mammals for reward-related learning and behavior", "2715e55c-4c2d-4711-80f2-3f236bb15621": "A complete algorithm is given in the box.\n\nInput: an arbitrary behavior policy b such that b(a|s) > 0, for all s 2 S, a 2 A Initialize Q(s, a) arbitrarily, for all s 2 S, a 2 A Initialize \u21e1 to be \"-greedy with respect to Q, or as a \ufb01xed given policy Algorithm parameters: step size \u21b5 2 (0, 1], small \" > 0, a positive integer n All store and access operations can take their index mod n + 1 In this chapter we have developed a range of temporal-di\u21b5erence learning methods that lie in between the one-step TD methods of the previous chapter and the Monte Carlo methods of the chapter before. Methods that involve an intermediate amount of bootstrapping are important because they will typically perform better than either extreme. Our focus in this chapter has been on n-step methods, which look ahead to the next n rewards, states, and actions. The two 4-step backup diagrams to the right together summarize most of the methods introduced", "5c5acd07-c3cb-42be-bf16-cbf60b9a4905": "CONFRONTING THE PARTITION FUNCTION  18.1. The Log-Likelihood Gradient  What makes learning undirected models by maximum likelihood particularly difficult is that the partition function depends on the parameters. The gradient of the log-likelihood with respect to the parameters has a term corresponding to the gradient of the partition function:  Vo log p(x; 8) = Vo log p(x; 8) \u2014 Vo log Z(8).\n\n(18.4)  This is a well-known decomposition into the positive phase and negative phase of learning. For most undirected models of interest, the negative phase is difficult. Models with no latent variables or with few interactions between latent variables typically have a tractable positive phase. The quintessential example of a model with a straightforward positive phase and a difficult negative phase is the RBM, which has hidden units that are conditionally independent from each other given the visible units. The case where the positive phase is difficult, with complicated interactions between latent variables, is primarily covered in chapter 19. This chapter focuses on the difficulties of the negative phase", "75c84e11-7cd7-404d-9d59-cc3206002b4c": "Mixing images  Mixing images together by averaging their pixel values is a very counterintuitive approach to Data Augmentation. The images produced by doing this will not look like a useful transformation to a human observer. However, Ionue  demonstrated how the pairing of samples could be developed into an effective augmentation strategy. In this experiment, two images are randomly cropped from 256 x 256 to 224 x 224 and ran- domly flipped horizontally. These images are then mixed by averaging the pixel values for each of the RGB channels. This results in a mixed image which is used to train a clas- sification model.\n\nThe label assigned to the new image is the same as the first randomly selected image (Fig. 7). On the CIFAR-10 dataset, Ionue reported a reduction in error rate from 8.22 to 6.93% when using the SamplePairing Data Augmentation technique. The researcher found even better results when testing a reduced size dataset, reducing CIFAR-10 to 1000 total samples with 100 in each class. With the reduced size dataset, SamplePairing resulted in an error rate reduction from 43.1 to 31.0%", "55222eb8-cc95-40e5-aad4-687f48a9d124": "Hedderich et al. provide a broad overview of techniques for NLP in low resource scenarios and brie\ufb02y cover data augmentation as one of several techniques. In contrast, we focus on data augmentation and provide a more comprehensive review on recent data augmentation methods in this work. While Feng et al. also survey task-speci\ufb01c data augmentation approaches for NLP, our work summarizes recent data augmentation methods in a more \ufb01ne-grained categorization.\n\nWe also focus on their application to learning from limited data by providing an empirical study over different augmentation methods on various benchmark datasets in both supervised and semi-supervised settings, so as to hint data augmentation selections in future research. Data augmentation increases both the amount (the number of data points) and the diversity (the variety of data) of a given dataset . Limited labeled data often leads to over\ufb01tting on the training set and data augmentation works to alleviate this issue by manipulating data either automatically or manually to create additional augmented data.Such techniques have been widely explored in the computer vision \ufb01eld, with methods like geometric/color space transformations , mixup , and random erasing", "19081181-556f-4b2d-9859-27cfd9c77161": "Speci\ufb01cally, the experience functions contribute to the optimization objective via the penalty term U(\u03be) over slack variables \u03be \u2208 RK applied to the expectation Eq . The e\ufb00ect of maximizing the expectation is such that the auxiliary model q is encouraged to produce samples of high quality in light of the experience (i.e., samples receiving high scores as evaluated by the experience function). Divergence function. The divergence function D (q, p\u03b8) measures the \u2018quality\u2019 of the target model p\u03b8 in terms of its distance (divergence) with the auxiliary model q. Intuitively, we want to minimize the distance from p\u03b8 to q, which is optimized to \ufb01t the experience as above. Section 5 gives a concrete example of how the divergence term would directly impact the model training: with a certain speci\ufb01cation of the di\ufb00erent components (e.g., the experience function, \u03b1/\u03b2), the SE in Equation 3.1 would reduce to min\u03b8 D(pd, p\u03b8).\n\nThat is, the learning objective is to minimize the divergence between the target model distribution p\u03b8 and the data distribution pd, and the divergence function D(\u00b7, \u00b7) determines the speci\ufb01c optimization problem", "cdb78bbe-f085-4d57-a0ac-7b4e131a7ae9": "By de\ufb01nition, the marginal is obtained by summing the joint distribution over all variables except x so that where x \\ x denotes the set of variables in x with variable x omitted. The idea is to substitute for p(x) using the factor graph expression (8.59) and then interchange summations and products in order to obtain an ef\ufb01cient algorithm. Consider the fragment of graph shown in Figure 8.46 in which we see that the tree structure of the graph allows us to partition the factors in the joint distribution into groups, with one group associated with each of the factor nodes that is a neighbour of the variable node x.\n\nWe see that the joint distribution can be written as a product of the form ne(x) denotes the set of factor nodes that are neighbours of x, and Xs denotes the set of all variables in the subtree connected to the variable node x via the factor node fs, and Fs(x, Xs) represents the product of all the factors in the group associated with factor fs", "07876a28-4f80-40e5-b8f1-4c2ef8f4f8db": "Empowerment: A universal agent-centric measure of control. In Proceedings of the 2005 IEEE Congress on Evolutionary Computation (Vol. 1, pp. 128\u2013135). IEEE. Kober, J., Peters, J. Reinforcement learning in robotics: A survey.\n\nIn M. Wiering, M. van Otterlo (Eds. ), Reinforcement Learning: State-of-the-Art, pp. 579\u2013610. Springer-Verlag. Kocsis, L., Szepesv\u00b4ari, Cs. Bandit based Monte-Carlo planning. In Proceedings of the European Conference on Machine Learning, pp. 282\u2013293. Springer-Verlag Berlin Heidelberg. Koller, D., Friedman, N. Probabilistic Graphical Models: Principles and Techniques. di\u21b5erential Hebbian and temporal di\u21b5erence learning. Neural Computation, 21(4):1173\u20131202. Kolter, J. Z", "0a4655ef-fa18-44e4-8f47-af51f06a7100": "This is easily veri\ufb01ed by noting that every path from node zn\u22121 to node zn+1 passes through at least one observed node that is head-to-tail with respect to that path. As a consequence, we can again use a forward-backward recursion in the E step of the EM algorithm to determine the posterior distributions of the latent variables in a computational time that is linear in the length of the chain. Similarly, the M step involves only a minor modi\ufb01cation of the standard M-step equations. In the case of Gaussian emission densities this involves estimating the parameters using the standard linear regression equations, discussed in Chapter 3. We have seen that the autoregressive HMM appears as a natural extension of the standard HMM when viewed as a graphical model.\n\nIn fact the probabilistic graphical modelling viewpoint motivates a plethora of different graphical structures based on the HMM. Another example is the input-output hidden Markov model , in which we have a sequence of observed variables u1, . , uN, in addition to the output variables x1,", "5392b9f1-87db-4b06-baf0-1f7cadf1e1ff": "Similarly Note that the distribution of a takes the same form as the predictive distribution (3.58) for the linear regression model, with the noise variance set to zero. Thus our variational approximation to the predictive distribution becomes This result can also be derived directly by making use of the results for the marginal of a Gaussian distribution given in Section 2.3.2. Exercise 4.24 The integral over a represents the convolution of a Gaussian with a logistic sigmoid, and cannot be evaluated analytically. We can, however, obtain a good approximation  by making use of the close similarity between the logistic sigmoid function \u03c3(a) de\ufb01ned by (4.59) and the probit function \u03a6(a) de\ufb01ned by (4.114).\n\nIn order to obtain the best approximation to the logistic function we need to re-scale the horizontal axis, so that we approximate \u03c3(a) by \u03a6(\u03bba). We can \ufb01nd a suitable value of \u03bb by requiring that the two functions have the same slope at the origin, which gives \u03bb2 = \u03c0/8. The similarity of the logistic sigmoid and the probit function, for this Exercise 4.25 choice of \u03bb, is illustrated in Figure 4.9", "a4733972-f375-4c0f-a306-4b9573ef96d0": "Along the axis corresponding to x2, the function curves downward. This direction is an eigenvector of the Hessian with negative eigenvalue.\n\nThe name \u201csaddle point\u201d derives from the saddle-like shape of this function. This is the quintessential example of a function with a saddle point. In more than one dimension, it is not necessary to have an eigenvalue of 0 to get a saddle point: it is only necessary to have both positive and negative eigenvalues. We can think of a saddle point with both signs of eigenvalues as being a local maximum within one cross section and a local minimum within another cross section. 88  CHAPTER 4. NUMERICAL COMPUTATION  https://www.deeplearningbook.org/contents/numerical.html    20  z2 Oo  \u201430 \u201420 -10 0 10 20  1  Figure 4.6: Gradient descent fails to exploit the curvature information contained in the Hessian matrix. Here we use gradient descent to minimize a quadratic function f(a) whose Hessian matrix has condition number 5", "8c9785ad-ae6e-4bfe-9de6-c5f20a4b972f": "In a footnote, Minsky mentioned that it is possible to apply DP to problems in which Samuel\u2019s backing-up process can be handled in closed analytic form.\n\nThis remark may have misled arti\ufb01cial intelligence researchers into believing that DP was restricted to analytically tractable problems and therefore largely irrelevant to arti\ufb01cial intelligence. Andreae  mentioned DP in the context of reinforcement learning, speci\ufb01cally policy iteration, although he did not make speci\ufb01c connections between DP and learning algorithms. Werbos  suggested an approach to approximating DP called \u201cheuristic dynamic programming\u201d that emphasizes gradient-descent methods for continuous-state problems . These methods are closely related to the reinforcement learning algorithms that we discuss in this book. Watkins  was explicit in connecting reinforcement learning to DP, characterizing a class of reinforcement learning methods as \u201cincremental dynamic programming.\u201d 4.1\u20134 These sections describe well-established DP algorithms that are covered in any of the general DP references cited above. The policy improvement theorem and the policy iteration algorithm are due to Bellman  and Howard . Our presentation was in\ufb02uenced by the local view of policy improvement taken by Watkins", "8f24ce5a-d054-4d00-bef3-0f4925f751a8": "Update the training loss: Ltrain \u2014 Ltrain +L (94,4+(*i), Yi)  https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Log   Update all the parameters (0, \u00a2, w, v) using Lirain-  Optimization-Based  Deep learning models learn through backpropagation of gradients. However, the gradient-based optimization is neither designed to cope with a small number of training samples, nor to converge within a small number of optimization steps.\n\nIs there a way to adjust the optimization algorithm so that the model can be good at learning with a few examples? This is what optimization-based approach meta-learning algorithms intend for. LSTM Meta-Learner  The optimization algorithm can be explicitly modeled. Ravi & Larochelle  did so and named it \u201cmeta-learner\", while the original model for handling the task is called \u201clearner\u201d. The goal of the meta-learner is to efficiently update the learner's parameters using a small support set so that the learner can adapt to the new task quickly", "b67d3252-a07c-4b6a-b1f5-8d8bd1f7bc28": "A reinforcement learning problem can be posed in a variety of di\u21b5erent ways depending on assumptions about the level of knowledge initially available to the agent.\n\nIn problems of complete knowledge, the agent has a complete and accurate model of the environment\u2019s dynamics. If the environment is an MDP, then such a model consists of the complete fourargument dynamics function p (3.2). In problems of incomplete knowledge, a complete and perfect model of the environment is not available. Even if the agent has a complete and accurate environment model, the agent is typically unable to perform enough computation per time step to fully use it. The memory available is also an important constraint. Memory may be required to build up accurate approximations of value functions, policies, and models. In most cases of practical interest there are far more states than could possibly be entries in a table, and approximations must be made. A well-de\ufb01ned notion of optimality organizes the approach to learning we describe in this book and provides a way to understand the theoretical properties of various learning algorithms, but it is an ideal that reinforcement learning agents can only approximate to varying degrees. In reinforcement learning we are very much concerned with cases in which optimal solutions cannot be found but must be approximated in some way", "3669abf6-70cd-4bb2-8cb8-f094d7c35520": "https://www.deeplearningbook.org/contents/numerical.html    95  https://www.deeplearningbook.org/contents/numerical.html", "8f0ea450-2d57-42f7-a655-fe5edec8280c": "In Section 6.1, we introduced kernels by applying the concept of duality to a nonprobabilistic model for regression. Here we extend the role of kernels to probabilistic discriminative models, leading to the framework of Gaussian processes. We shall thereby see how kernels arise naturally in a Bayesian setting. In Chapter 3, we considered linear regression models of the form y(x, w) = wT\u03c6(x) in which w is a vector of parameters and \u03c6(x) is a vector of \ufb01xed nonlinear basis functions that depend on the input vector x. We showed that a prior distribution over w induced a corresponding prior distribution over functions y(x, w).\n\nGiven a training data set, we then evaluated the posterior distribution over w and thereby obtained the corresponding posterior distribution over regression functions, which in turn (with the addition of noise) implies a predictive distribution p(t|x) for new input vectors x. In the Gaussian process viewpoint, we dispense with the parametric model and instead de\ufb01ne a prior probability distribution over functions directly. At \ufb01rst sight, it might seem dif\ufb01cult to work with a distribution over the uncountably in\ufb01nite space of functions", "9abdc3ce-ac65-47e7-8fd9-73b124c3e6ed": "(Though it has often been pointed out that even when the reinforcing US in a classical conditioning experiment is not contingent on the subject\u2019s preceding behavior, its reinforcing value can be in\ufb02uenced by this behavior, an example being that a closed eye makes an air pu\u21b5 to the eye less aversive.) The distinction between reward signals and reinforcement signals is a crucial point when we discuss neural correlates of these signals in the next chapter. Like a reward signal, for us, the reinforcement signal at any speci\ufb01c time is a positive or negative number, or zero. A reinforcement signal is the major factor directing changes a learning algorithm makes in an agent\u2019s policy, value estimates, or environment models. The de\ufb01nition that makes the most sense to us is that a reinforcement signal at any time is a number that multiplies (possibly along with some constants) a vector to determine parameter updates in some learning algorithm", "5182521c-1f8c-4503-8cd4-0fef4c0c25cf": "However, we can easily adapt Gaussian processes to classi\ufb01cation problems by transforming the output of the Gaussian process using an appropriate nonlinear activation function. Consider \ufb01rst the two-class problem with a target variable t \u2208 {0, 1}.\n\nIf we de\ufb01ne a Gaussian process over a function a(x) and then transform the function using a logistic sigmoid y = \u03c3(a), given by (4.59), then we will obtain a non-Gaussian stochastic process over functions y(x) where y \u2208 (0, 1). This is illustrated for the case of a one-dimensional input space in Figure 6.11 in which the probability distrishows the result of transforming this sample using a logistic sigmoid function. bution over the target variable t is then given by the Bernoulli distribution As usual, we denote the training set inputs by x1, . , xN with corresponding observed target variables t = (t1, . , tN)T. We also consider a single test point xN+1 with target value tN+1", "9124d79f-c38d-45b1-b0db-7e71511ef623": "Although eligibility traces are closely associated historically with TD learning, in fact they have nothing to do with it. In fact, eligibility traces arise even in Monte Carlo learning, as we show in this section. We show that the linear MC algorithm (Chapter 9), taken as a forward view, can be used to derive an equivalent yet computationally cheaper backward-view algorithm using dutch traces. This is the only equivalence of forward- and backward-views that we explicitly demonstrate in this book. It gives some of the \ufb02avor of the proof of equivalence of true online TD(\u03bb) and the online \u03bb-return algorithm, but is much simpler.\n\nThe linear version of the gradient Monte Carlo prediction algorithm (page 202) makes the following sequence of updates, one for each time step of the episode: To simplify the example, we assume here that the return G is a single reward received at the end of the episode (this is why G is not subscripted by time) and that there is no discounting. In this case the update is also known as the Least Mean Square (LMS) rule. As a Monte Carlo algorithm, all the updates depend on the \ufb01nal reward/return, so none can be made until the end of the episode", "3f7bc131-77dd-4208-b705-a0fbd78faa28": "AutoAugment AutoAugment , developed by Cubuk et al., is a much different approach to meta-learning than Neural Augmentation or Smart Augmentation. Auto- Augment is a Reinforcement Learning algorithm  that searches for an optimal augmentation policy amongst a constrained set of geometric transformations with mis- cellaneous levels of distortions.\n\nFor example, \u2018translateX 20 pixels\u2019 could be one of the transformations in the search space (Table 8). In Reinforcement Learning algorithms, a policy is analogous to the strategy of the learning algorithm. This policy determines what actions to take at given states to achieve some goal. The AutoAugment approach learns a policy which consists of many sub- policies, each sub-policy consisting of an image transformation and a magnitude of transformation. Reinforcement Learning is thus used as a discrete search algorithm of augmentations. The authors also suggest that evolutionary algorithms or random search would be effective search algorithms as well. AutoAugment found policies which achieved a 1.48% error rate on CIFAR-10. Auto- Augment also achieved an 83.54% Top-1 accuracy on the ImageNet dataset", "1232bca5-0721-40bc-a77a-5baf12cdbf15": "(b) The graph for the logistic regression prediction \u00a5 = 0 (a\u2019 w + b). Some of the intermediate expressions do not have names in the algebraic expression but need names in the graph. We simply name the \u00e9th such variable u.\n\n(c) The computational graph for the expression H = max{0, XW +}, which computes a design matrix of rectified linear unit activations H given a design matrix containing a minibatch of inputs X. (d) Examples a-c applied at most one operation to each variable, but it is possible to apply more than one operation. Here we show a computation graph that applies more than one operation to the weights w of a linear regression model. The weights are used to make both the prediction 7 and the weight decay penalty \\_,w?. >  https://www.deeplearningbook.org/contents/mlp.html    202  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  the chain rule states that dz dzdy  = . 6.44 dx dydz ( )  We can generalize this beyond the scalar case", "e9762c76-eb98-4d5b-bda8-ccbf8bd7e69d": "To be a probability density function, a function p must satisfy the following properties:  e The domain of p must be the set of all possible states of x.  e Vx \u20ac x,p(x) > 0. Note that we do not require p(x) < 1. e f p(x)da = 1. A probability density function p(x) does not give the probability of a specific state directly; instead the probability of landing inside an infinitesimal region with volume dz is given by p(x)dz. We can integrate the density function to find the actual probability mass of a set of points.\n\nSpecifically, the probability that x lies in some set S is given by the integral of p(a) over that set. In the univariate example, the probability that x lies in the interval  is given by  P(r) da. For an example of a PDF corresponding to a specific probability density over a continuous random variable, consider a uniform distribution on an interval of  \u2014\u2014 : - > a . \u2018 1h :", "4b571ab3-6169-4b90-af67-56bc61792143": "In these cases, kernel regression is much less complex than directly using a linear parametric method with states represented by these feature vectors. This is the so-called \u201ckernel trick\u201d that allows e\u21b5ectively working in the high-dimension of an expansive feature space while actually working only with the set of stored training examples. The kernel trick is the basis of many machine learning methods, and researchers have shown how it can sometimes bene\ufb01t reinforcement learning. The algorithms we have considered so far in this chapter have treated all the states encountered equally, as if they were all equally important. In some cases, however, we are more interested in some states than others.\n\nIn discounted episodic problems, for example, we may be more interested in accurately valuing early states in the episode than in later states where discounting may have made the rewards much less important to the value of the start state. Or, if an action-value function is being learned, it may be less important to accurately value poor actions whose value is much less than the greedy action. Function approximation resources are always limited, and if they were used in a more targeted way, then performance could be improved", "8720f3dd-21c3-43cd-b740-690c319cc77b": "https://www.deeplearningbook.org/contents/ml.html    In some cases, this is because it is difficult to decide what should be measured.\n\nFor example, when performing a transcription task, should we measure the accuracy of the system at transcribing entire sequences, or should we use a more fine- grained performance measure that gives partial credit for getting some elements of the  sequence correct? When performing a regression task, should we penalize the system more if it frequently makes medium-sized mistakes or if it rarely makes very large mistakes? These kinds of design choices depend on the application. In other cases, we know what quantity we would ideally like to measure, but measuring it is impractical. For example, this arises frequently in the context of density estimation. Many of the best probabilistic models represent probability distributions only implicitly. Computing the actual probability value assigned to a specific point in space in many such models is intractable. In these cases, one must design an alternative criterion that still corresponds to the design objectives, or design a good approximation to the desired criterion. 5.1.3", "60a04bac-1211-4540-93b1-0c6017395022": "These visualizations show a flattening of the cost function near a prominent saddle point, where the weights are all zero, but they also show the gradient descent trajectory rapidly escaping this region. Goodfellow et al. 283  https://www.deeplearningbook.org/contents/optimization.html    CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  (a)c  Projecs: . Jection 1 of @ ro  Figure 8.2: A visualization of the cost function of a neural network. These visualizations appear similar for feedforward neural networks, convolutional networks, and recurrent networks applied to real object recognition and natural language processing tasks. Sur- prisingly, these visualizations usually do not show many conspicuous obstacles. Prior to the success of stochastic gradient descent for training very large models beginning in roughly 2012, neural net cost function surfaces were generally believed to have much more nonconvex structure than is revealed by these projections", "07ea0229-b3b8-49b6-acbc-ac421af2e792": "In this case, an autoencoder  https://www.deeplearningbook.org/contents/autoencoders.html    trained to perform the copying task has learned the principal subspace of the training data as a side effect. Autoencoders with nonlinear encoder functions f and nonlinear decoder func- tions g can thus learn a more powerful nonlinear generalization of PCA. Unfortu-  500  CHAPTER 14. AUTOENCODERS  nately, if the encoder and decoder are allowed too much capacity, the autoencoder can learn to perform the copying task without extracting useful information about the distribution of the data. Theoretically, one could imagine that an autoencoder with a one-dimensional code but a very powerful nonlinear encoder could learn to represent each training example a with the code i. The decoder could learn to map these integer indices back to the values of specific training examples.\n\nThis specific scenario does not occur in practice, but it illustrates clearly that an autoen- coder trained to perform the copying task can fail to learn anything useful about the dataset if the capacity of the autoencoder is allowed to become too great", "1415799c-f638-455f-9dea-8838ab0afca4": "For example, the simple rule \u201cMost birds fly\u201d is cheap to develop and is broadly useful, while a rule of the form, \u201cBirds fly, except for very young birds that have not yet learned to fly, sick or injured birds that have lost the ability to fly, flightless species of birds including the cassowary, ostrich and kiwi...\u201d is expensive to develop, maintain and communicate and, after all this effort, is still brittle and prone to failure. While it should be clear that we need a means of representing and reasoning about uncertainty, it is not immediately obvious that probability theory can provide all the tools we want for artificial intelligence applications. Probability theory was originally developed to analyze the frequencies of events. It is easy to see how probability theory can be used to study events like drawing a certain hand of cards in a poker game. These kinds of events are often repeatable.\n\nWhen we say that an outcome has a probability p of occurring, it means that if we repeated the experiment (e.g., drawing a hand of cards) infinitely many times, then a proportion p of the repetitions would result in that outcome. This kind of reasoning does not seem immediately applicable to propositions that are not repeatable", "b66c5592-37f1-4ca1-a8b9-9d2f45530578": "Note that this is exactly the same as the d-separation criterion except that there is no \u2018explaining away\u2019 phenomenon. Testing for conditional independence in undirected graphs is therefore simpler than in directed graphs. An alternative way to view the conditional independence test is to imagine removing all nodes in set C from the graph together with any links that connect to those nodes.\n\nWe then ask if there exists a path that connects any node in A to any node in B. If there are no such paths, then the conditional independence property must hold. The Markov blanket for an undirected graph takes a particularly simple form, because a node will be conditionally independent of all other nodes conditioned only on the neighbouring nodes, as illustrated in Figure 8.28. We now seek a factorization rule for undirected graphs that will correspond to the above conditional independence test. Again, this will involve expressing the joint distribution p(x) as a product of functions de\ufb01ned over sets of variables that are local to the graph. We therefore need to decide what is the appropriate notion of locality in this case", "520a9d8f-5415-451f-9a1f-da4ff8f6b40a": "Learned associations over long delays. In G. Bower (Ed. ), The Psychology of Learning and Motivation, v. 4, pp. 1\u201384. Academic Press, Inc., New York. Ring, M. B. (in preparation). Representing knowledge as forecasts (and state as knowledge). Ripley, B. D. Pattern Recognition and Neural Networks. Cambridge University Press. Rixner, S. Memory controller optimizations for web servers. In Proceedings of the Robertie, B. Carbon versus silicon: Matching wits with TD-Gammon. Inside BackgamRomo, R., Schultz, W. Dopamine neurons of the monkey midbrain: Contingencies of Ross, S. Introduction to Stochastic Dynamic Programming. Academic Press, New York. Ross, T. Machines that think. Scienti\ufb01c American, 148(4):206\u2013208. Rubinstein, R. Y", "f21dcb97-4da0-47db-a79e-7c01f5f99279": "The mean and covariance of this distribution are easily seen to be Now let us consider a \ufb01nite mixture of these distributions given by where \u00b5 = {\u00b51, . , \u00b5K}, \u03c0 = {\u03c01, . , \u03c0K}, and The mean and covariance of this mixture distribution are given by Exercise 9.12 where \u03a3k = diag {\u00b5ki(1 \u2212 \u00b5ki)}. Because the covariance matrix cov is no longer diagonal, the mixture distribution can capture correlations between the variables, unlike a single Bernoulli distribution. If we are given a data set X = {x1, . , xN} then the log likelihood function for this model is given by Again we see the appearance of the summation inside the logarithm, so that the maximum likelihood solution no longer has closed form. We now derive the EM algorithm for maximizing the likelihood function for the mixture of Bernoulli distributions. To do this, we \ufb01rst introduce an explicit latent variable z associated with each instance of x. As in the case of the Gaussian mixture, z = (z1, .", "80f20599-afc6-4dfb-914a-52e67391208d": "The quantity \ufffdfjm(\u03b8m) corresponds to the message \u00b5fj\u2192\u03b8m(\u03b8m), which factor node j sends to variable node m, and the product over k in (10.240) is over all factors that depend on the variables \u03b8m that have variables (other than variable \u03b8l) in common with factor fj(\u03b8j).\n\nIn other words, to compute the outgoing message from a factor node, we take the product of all the incoming messages from other factor nodes, multiply by the local factor, and then marginalize. Thus, the sum-product algorithm arises as a special case of expectation propagation if we use an approximating distribution that is fully factorized. This suggests that more \ufb02exible approximating distributions, corresponding to partially disconnected graphs, could be used to achieve higher accuracy. Another generalization is to group factors fi(\u03b8i) together into sets and to re\ufb01ne all the factors in a set together at each iteration. Both of these approaches can lead to improvements in accuracy . In general, the problem of choosing the best combination of grouping and disconnection is an open research issue", "1ca506c5-4b9d-4d92-bfa3-0a3bb07ceca6": "Before delving into the regularization behavior of different norms, we note that for neural networks, we typically choose to use a parameter norm penalty Q that penalizes only the weights of the affine transformation at each layer and leaves the biases unregularized. The biases typically require less data than the weights to fit accurately. Each weight specifies how two variables interact. Fitting the weight well requires observing both variables in a variety of conditions. Each bias controls only a single variable. This means that we do not induce too much variance by leaving the biases unregularized. Also, regularizing the bias parameters can introduce a significant amount of underfitting. We therefore use the vector w to indicate all the weights that should be affected by a norm penalty, while  aL", "1f337d13-b833-4892-8b35-63e8a3d9f1fc": "A few moments thought will show that the required joint distribution can be written Exercise 8.15 Thus we can obtain the joint distributions over all of the sets of variables in each of the potentials directly once we have completed the message passing required to obtain the marginals. This is a useful result because in practice we may wish to use parametric forms for the clique potentials, or equivalently for the conditional distributions if we started from a directed graph. In order to learn the parameters of these potentials in situations where not all of the variables are observed, we can employ the EM algorithm, Chapter 9 and it turns out that the local joint distributions of the cliques, conditioned on any observed data, is precisely what is needed in the E step. We shall consider some examples of this in detail in Chapter 13. We have seen that exact inference on a graph comprising a chain of nodes can be performed ef\ufb01ciently in time that is linear in the number of nodes, using an algorithm that can be interpreted in terms of messages passed along the chain", "fa616684-c31d-491a-92bd-6324e46a69cc": "Of course, one could select according to a soft-max distribution based on action values, but this alone would not allow the policy to approach a deterministic policy. Instead, the action-value estimates would converge to their corresponding true values, which would di\u21b5er by a \ufb01nite amount, translating to speci\ufb01c probabilities other than 0 and 1.\n\nIf the soft-max distribution included a temperature parameter, then the temperature could be reduced over time to approach determinism, but in practice it would be di\ufb03cult to choose the reduction schedule, or even the initial temperature, without more prior knowledge of the true action values than we would like to assume. Action preferences are di\u21b5erent because they do not approach speci\ufb01c values; instead they are driven to produce the optimal stochastic policy. If the optimal policy is deterministic, then the preferences of the optimal actions will be driven in\ufb01nitely higher than all suboptimal actions (if permitted by the parameterization). A second advantage of parameterizing policies according to the soft-max in action preferences is that it enables the selection of actions with arbitrary probabilities. In problems with signi\ufb01cant function approximation, the best approximate policy may be stochastic", "b0c615cb-5c46-4438-807c-b4380ee9eadb": "Note that for the above con\ufb01gurations, if f\u03c6 is parameterized as a neural network with a \ufb01xed architecture (e.g., ConvNet), its space F is not necessarily convex (i.e., a linear combination of two neural networks in F is not necessarily in F).\n\nIn such cases we formulate the optimization of the experience function over conv(F), the convex hull of F containing any convex combination of neural network functions in F , and see the various GAN algorithms as approximations by considering only the subset F \u2286 conv(F). Besides the above examples of divergence D that each leads to a di\ufb00erent GAN algorithm, we can consider even more options, such as the hybrid f-divergence and Wasserstein distance studied in . Of particular interest is to set D to the KL divergence D(q, p\u03b8) = KL(q\u2225p\u03b8), motivated by the simplicity in the sense that the auxiliary distribution q has a closed-form solution:1 where importance sampling is used to estimate the expectation under q(n+1), using the generator p(n) \u03b8 as the proposal distribution", "093f39c8-89fa-4609-a362-9c2cd2dc1f0a": "This approach will only yield accurate results if the importance sampling distribution pG is closely matched to the distribution pE, so that the ratio pE/pG does not have wide variations. In practice, suitable analytically speci\ufb01ed importance sampling distributions cannot readily be found for the kinds of complex models considered in this book. An alternative approach is therefore to use the samples obtained from a Markov chain to de\ufb01ne the importance-sampling distribution.\n\nIf the transition probability for the Markov chain is given by T(z, z\u2032), and the sample set is given by z(1), . , z(L), then the sampling distribution can be written as which can be used directly in (11.72). Methods for estimating the ratio of two partition functions require for their success that the two corresponding distributions be reasonably closely matched. This is especially problematic if we wish to \ufb01nd the absolute value of the partition function for a complex distribution because it is only for relatively simple distributions that the partition function can be evaluated directly, and so attempting to estimate the ratio of partition functions directly is unlikely to be successful. This problem can be tackled using a technique known as chaining , which involves introducing a succession of intermediate distributions p2,", "1b655155-6416-473c-8541-22bbb8d26d8a": "the approximate posterior, which are also intractable in the general case.\n\nWe show how a reparameterization of the variational lower bound yields a simple differentiable unbiased estimator of the lower bound; this SGVB (Stochastic Gradient Variational Bayes) estimator can be used for ef\ufb01cient approximate posterior inference in almost any model with continuous latent variables and/or parameters, and is straightforward to optimize using standard stochastic gradient ascent techniques. For the case of an i.i.d. dataset and continuous latent variables per datapoint, we propose the AutoEncoding VB (AEVB) algorithm. In the AEVB algorithm we make inference and learning especially ef\ufb01cient by using the SGVB estimator to optimize a recognition model that allows us to perform very ef\ufb01cient approximate posterior inference using simple ancestral sampling, which in turn allows us to ef\ufb01ciently learn the model parameters, without the need of expensive iterative inference schemes (such as MCMC) per datapoint. The learned approximate posterior inference model can also be used for a host of tasks such as recognition, denoising, representation and visualization purposes. When a neural network is used for the recognition model, we arrive at the variational auto-encoder", "c1b0c68e-39b5-4411-91fd-42cd092fe795": "In on-policy control methods the policy is generally soft, meaning that \u21e1(a|s) > 0 for all s 2 S and all a 2 A(s), but gradually shifted closer and closer to a deterministic optimal policy. Many of the methods discussed in Chapter 2 provide mechanisms for this. The on-policy method we present in this section uses \"-greedy policies, meaning that most of the time they choose an action that has maximal estimated action value, but with probability \" they instead select an action at random. That is, all nongreedy actions are given the minimal probability of selection, \" |A(s)|, is given to the greedy action. The \"-greedy policies are actions, for some \" > 0. Among \"-soft policies, \"-greedy policies are in some sense those The overall idea of on-policy Monte Carlo control is still that of GPI. As in Monte Carlo ES, we use \ufb01rst-visit MC methods to estimate the action-value function for the current policy.\n\nWithout the assumption of exploring starts, however, we cannot simply improve the policy by making it greedy with respect to the current value function, because that would prevent further exploration of nongreedy actions", "533c44e5-2e9a-4b05-934f-9c5627475a0b": "Training Data as the Interface to ML The system should model label sources to produce a single, probabilistic label for each data point and train any of a wide range of classi\ufb01ers to generalize beyond those sources. 3. Supervision as Interactive Programming The system should provide rapid results in response to user supervision. We envision weak supervision as the REPL-like interface for machine learning. Our work makes the following technical contributions: A Flexible Interface for Sources We observe that the heterogeneity of weak supervision strategies is a stumbling block for developers. Different types of weak supervision operate on different scopes of the input data. For example, distant supervision has to be mapped programmatically to speci\ufb01c spans of text. Crowd workers and weak classi\ufb01ers often operate over entire documents or images. Heuristic rules are open Fig.\n\n3 An overview of the Snorkel system. (1) SME users write labeling functions (LFs) that express weak supervision sources like distant supervision, patterns, and heuristics. (2) Snorkel applies the LFs over unlabeled data and learns a generative model to combine the LFs\u2019 outputs into probabilistic labels", "82e12515-4b25-48da-aeb5-cf47f019383e": "2014. Mark E, Luc VG, Christopher KIW, John W, Andrew Z. The pascal visual object classes (VOC) challenge. http://www. pascal-network.org/challenges/VOC/voc2008/workshop/. 2008. Aranzazu J, Miguel P, Mikel G, Carlos L-M, Daniel P. A comparison study of different color spaces in clustering based image segmentation. IPMU; 2010. Quanzeng J, Jiebo L, Hailin J, Jianchao Y. Robust image sentiment analysis using progressively trained and domain transferred deep networks. In: AAAI. 2015, p. 381-8. Luke T, Geoff N. Improving deep learning using generic data augmentation. arXiv preprint. 2017. Guoliang K, Xuanyi D, Liang Z, Yi Y. PatchShuffle regularization. arXiv preprint. 2017. Hiroshi |", "ad7aca6c-5c63-4a60-8840-b28d3faac66e": "The chosen number of hidden units is based on prior literature on auto-encoders, and the relative performance of different algorithms was not very sensitive to these choices. Figure 2 shows the results when comparing the lower bounds. Interestingly, super\ufb02uous latent variables did not result in over\ufb01tting, which is explained by the regularizing nature of the variational bound. Marginal likelihood For very low-dimensional latent space it is possible to estimate the marginal likelihood of the learned generative models using an MCMC estimator. More information about the marginal likelihood estimator is available in the appendix. For the encoder and decoder we again used neural networks, this time with 100 hidden units, and 3 latent variables; for higher dimensional latent space the estimates became unreliable. Again, the MNIST dataset was used. The AEVB and Wake-Sleep methods were compared to Monte Carlo EM (MCEM) with a Hybrid Monte Carlo (HMC)  sampler; details are in the appendix. We compared the convergence speed for the three algorithms, for a small and large training set size. Results are in \ufb01gure 3", "b6701fbc-8ebc-46a3-aed7-112ae4d4a84b": "The square root of the variance, given by \u03c3, is called the standard deviation, and the reciprocal of the variance, written as \u03b2 = 1/\u03c32, is called the precision. We shall see the motivation for these terms shortly. Figure 1.13 shows a plot of the Gaussian distribution.\n\nFrom the form of (1.46) we see that the Gaussian distribution satis\ufb01es Also it is straightforward to show that the Gaussian is normalized, so that Exercise 1.7 It is said that Laplace was seriously lacking in modesty and at one point declared himself to be the best mathematician in France at the time, a claim that was arguably true. As well as being proli\ufb01c in mathematics, he also made numerous contributions to astronomy, including the nebular hypothesis by which the earth is thought to have formed from the condensation and cooling of a large rotating disk of gas and dust. In 1812 he published the \ufb01rst edition of Th\u00b4eorie Analytique des Probabilit\u00b4es, in which Laplace states that \u201cprobability theory is nothing but common sense reduced to calculation\u201d", "f0c8099a-a580-45e7-a6d6-e534316a672e": "Another difference between variational Bayes and EP arises from the form of KL divergence that is minimized by the two algorithms, because the former minimizes KL(q\u2225p) whereas the latter minimizes KL(p\u2225q). As we saw in Figure 10.3, for distributions p(\u03b8) which are multimodal, minimizing KL(p\u2225q) can lead to poor approximations. In particular, if EP is applied to mixtures the results are not sensible because the approximation tries to capture all of the modes of the posterior distribution.\n\nConversely, in logistic-type models, EP often out-performs both local variational methods and the Laplace approximation . Following Minka , we illustrate the EP algorithm using a simple example in which the goal is to infer the mean \u03b8 of a multivariate Gaussian distribution over a variable x given a set of observations drawn from that distribution. To make the problem more interesting, the observations are embedded in background clutter, which itself is also Gaussian distributed, as illustrated in Figure 10.15", "801ceca8-90f2-4773-85bd-aa44bd12c56e": "Note that these states are not necessarily the integers; they can also just be named states that are not considered to have any numerical value. A continuous random variable is associated with a real value. 3.3 Probability Distributions  aoe, wen ed  https://www.deeplearningbook.org/contents/prob.html    A probability distribution js a description of how likely a random variable or set of random variables is to take on each of its possible states. The way we describe probability distributions depends on whether the variables are discrete or continuous. 3.3.1 Discrete Variables and Probability Mass Functions  A probability distribution over discrete variables may be described using a proba- bility mass function (PMF). We typically denote probability mass functions with a capital P. Often we associate each random variable with a different probability mass function and the reader must infer which PMF to use based on the identity  54  CHAPTER 3.\n\nPROBABILITY AND INFORMATION THEORY  of the random variable, rather than on the name of the function; P(x) is usually not the same as P(y). The probability mass function maps from a state of a random variable to the probability of that random variable taking on that state", "60007b90-94f7-447a-8463-28a5e44f51a6": "These methods correspond to using an outer approximation M\u2032\u2032 \u2287 M of the marginal polytope, and do not guarantee upper bounds on the negative marginal log-likelihood. Another approach to restrict the family of q is to assume a parametric distribution q\u03c9(y|x) and optimize the parameters \u03c9 in the E-step. The approach has been used in black-box variational inference , and variational auto-encoders (VAEs)  where q is parameterized as a neural network (a.k.a \u2018inference network,\u2019 or \u2018encoder\u2019). It is worth mentioning that the variational approach has also been used for approximate Gaussian processes (GPs, as a nonparametric methods) , where y is the inducing points and the variational distribution q(y) is parameterized as a Gaussian distribution with a nondiagonal covariance matrix that preserves the structures within the true covariance (and hence is di\ufb00erent from the above mean-\ufb01eld approximation, which assumes a diagonal variational covariance matrix). We refer interested readers to  for more details. Wake-Sleep", "fade5b84-aa84-4939-b114-411017f7c509": "In the context of deep learning, we also use some less conventional notation. We allow the addition of a matrix and a vector, yielding another matrix: C =A +b, where Cj,; = Ajj +b;. In other words, the vector b is added to each row of the matrix. This shorthand eliminates the need to define a matrix with b copied into each row before doing the addition. This implicit copying of b to many locations is called broadcasting. 2.2 Multiplying Matrices and Vectors  One of the most important operations involving matrices is multiplication of two matrices. The matrix product of matrices A and B is a third matrix C. In order for this product to be defined, A must have the same number of columns as B has rows. If A is of shape m x n and B is of shape n x p, then C is of shape m x p. We can write the matrix product just by placing two or more matrices together, for example,  C= AB", "c563dbad-1f8a-489b-8103-b65d3fab0748": "Martin Arjovsky1, Soumith Chintala2, and L\u00b4eon Bottou1,2 The problem this paper is concerned with is that of unsupervised learning. Mainly, what does it mean to learn a probability distribution? The classical answer to this is to learn a probability density. This is often done by de\ufb01ning a parametric family of densities (P\u03b8)\u03b8\u2208Rd and \ufb01nding the one that maximized the likelihood on our data: if we have real data examples {x(i)}m i=1, we would solve the problem If the real data distribution Pr admits a density and P\u03b8 is the distribution of the parametrized density P\u03b8, then, asymptotically, this amounts to minimizing the Kullback-Leibler divergence KL(Pr\u2225P\u03b8). For this to make sense, we need the model density P\u03b8 to exist. This is not the case in the rather common situation where we are dealing with distributions supported by low dimensional manifolds. It is then unlikely that the model manifold and the true distribution\u2019s support have a non-negligible intersection (see ), and this means that the KL distance is not de\ufb01ned (or simply in\ufb01nite)", "4025b4b8-cef1-4905-9dc5-bc3ba92697d5": "It follows then that this \u2018naive\u2019 solution must also be the one that minimizes the BE, and so it is.\n\nOn a deterministic problem, the Bellman errors and TD errors are all the same, so the BE is always the same as the TDE. Optimizing the BE on this example gives rise to the same failure mode as with the naive residual-gradient algorithm on the A-split example. equation is possible. But if we examine examples with genuine function approximation, then the residual-gradient algorithm, and indeed the BE objective, seem to \ufb01nd the wrong value functions. One of the most telling such examples is the variation on the A-split example known as the A-presplit example, shown on the preceding page, in which the residual-gradient algorithm \ufb01nds the same poor solution as its naive version. This example shows intuitively that minimizing the BE (which the residual-gradient algorithm surely does) may not be a desirable goal. The third way in which the convergence of the residual-gradient algorithm is not satisfactory is explained in the next section. Like the second way, the third way is also a problem with the BE objective itself rather than with any particular algorithm for achieving it", "cda35334-de63-403b-a555-33b527e26376": "We evaluated the performance of our self-supervised representation for transfer learning in two settings: linear evaluation, where a logistic regression classi\ufb01er is trained to classify a new dataset based on the self-supervised representation learned on ImageNet, and \ufb01ne-tuning, where we allow all weights to vary during training. In both cases, we follow the approach described by Kornblith et al. , although our preprocessing differs slightly", "73a53933-c8dc-43d2-9d60-533192aa5751": "We can \ufb01nd the probability distribution of y as follows. First of all we note that y is a linear combination of Gaussian distributed variables given by the elements of w and hence is itself Gaussian.\n\nWe therefore need only to \ufb01nd its mean and covariance, which are given from Exercise 2.31 where K is the Gram matrix with elements and k(x, x\u2032) is the kernel function. This model provides us with a particular example of a Gaussian process. In general, a Gaussian process is de\ufb01ned as a probability distribution over functions y(x) such that the set of values of y(x) evaluated at an arbitrary set of points x1, . , xN jointly have a Gaussian distribution. In cases where the input vector x is two dimensional, this may also be known as a Gaussian random \ufb01eld. More generally, a stochastic process y(x) is speci\ufb01ed by giving the joint probability distribution for any \ufb01nite set of values y(x1), . , y(xN) in a consistent manner. A key point about Gaussian stochastic processes is that the joint distribution over N variables y1,", "b7013490-cce2-40a8-8aec-ede6390357fd": "We then make use of the knowledge-based experience such as frule(t) to drive learning. The standard equation rediscovers classical algorithms for learning with symbolic knowledge. Posterior regularization and extensions.\n\nBy setting \u03b1 = \u03b2 = 1 and f to a constraint function such as frule, the SE with cross entropy naturally leads to a generalized posterior regularization framework : which extends the conventional Bayesian inference formulation (Section 2.3) by permitting regularization on arbitrary random variables of arbitrary models (e.g., deep neural networks) with complex rule constraints. The trade-o\ufb00 hyperparameters can also take other values. For example, by allowing arbitrary \u03b1 \u2208 R, the objective corresponds to the uni\ufb01ed expectation maximization (UEM) algorithm  that extends the posterior regularization for added \ufb02exibility. 4.3. Reward Experience. We now consider a very di\ufb00erent learning setting commonly seen in robotic control and other sequential decision making problems. In this setting, experience is gained by the agent interacting with external environment and collecting feedback in the form of rewards", "ad63488e-f9a0-49bc-956d-4800b1645b8d": "Figure 7.2 illustrates a very common form of multitask learning, in which  different supervised tasks (predicting y) given x) share the same input x, as well as some intermediate-level representation p(shared) | capturing a common pool of factors. The model can generally be divided into two kinds of parts and associated parameters:  1. Task-specific parameters (which only benefit from the examples of their task to achieve good generalization). These are the upper layers of the neural network in figure 7.2. 2. Generic parameters, shared across all the tasks (which benefit from the pooled data of all the tasks). These are the lower layers of the neural network in figure 7.2. Ti", "d31e410c-a616-48c9-821f-052673e94c1a": "MACHINE LEARNING BASICS  in its hypothesis space. This means that both functions are eligible, but one is preferred. The unpreferred solution will be chosen only if it fits the training data significantly better than the preferred solution. For example, we can modify the training criterion for linear regression to include weight decay. To perform linear regression with weight decay, we minimize a sum J(w) comprising both the mean squared error on the training and a criterion that expresses a preference for the weights to have smaller squared L? norm. Specifically,  J(w) = MSEtrain + Aw i (5.18)  https://www.deeplearningbook.org/contents/ml.html    where is a value chosen ahead of time that controls the strength of our preference for smaller weights. When A = 0, we impose no preference, and larger 4 forces the weights to become smaller.\n\nMinimizing J(w) results in a choice of weights that  make a tradeoff between fitting the training data and being small. This gives us solutions that have a smaller slope, or that put weight on fewer of the features", "9bc34ab8-d226-4e51-bb28-af5b8d9c48b3": "Once again, our strategy for evaluating this distribution ef\ufb01ciently will be to focus on the quadratic form in the exponent of the joint distribution and thereby to identify the mean and covariance of the marginal distribution p(xa). The quadratic form for the joint distribution can be expressed, using the partitioned precision matrix, in the form (2.70). Because our goal is to integrate out xb, this is most easily achieved by \ufb01rst considering the terms involving xb and then completing the square in order to facilitate integration.\n\nPicking out just those terms that involve xb, we have We see that the dependence on xb has been cast into the standard quadratic form of a Gaussian distribution corresponding to the \ufb01rst term on the right-hand side of (2.84), plus a term that does not depend on xb (but that does depend on xa). Thus, when we take the exponential of this quadratic form, we see that the integration over xb required by (2.83) will take the form \ufffd exp \ufffd \u22121 This integration is easily performed by noting that it is the integral over an unnormalized Gaussian, and so the result will be the reciprocal of the normalization coef\ufb01cient", "3df85100-24d2-4992-b6a5-e662e91d4b2a": "\u21e4 Di\u21b5erential semi-gradient Sarsa for estimating \u02c6q \u21e1 q\u21e4 Input: a di\u21b5erentiable action-value function parameterization \u02c6q : S \u21e5 A \u21e5 Rd ! R Algorithm parameters: step sizes \u21b5, \u03b2 > 0 Initialize value-function weights w 2 Rd arbitrarily (e.g., w = 0) Initialize average reward estimate \u00afR 2 R arbitrarily (e.g., \u00afR = 0) Exercise 10.6 Suppose there is an MDP that under any policy produces the deterministic sequence of rewards +1, 0, +1, 0, +1, 0, . going on forever. Technically, this is not allowed because it violates ergodicity; there is no stationary limiting distribution \u00b5\u21e1 and the limit (10.7) does not exist. Nevertheless, the average reward (10.6) is well de\ufb01ned; What is it? Now consider two states in this MDP.\n\nFrom A, the reward sequence is exactly as described above, starting with a +1, whereas, from B, the reward sequence starts with a 0 and then continues with +1, 0, +1, 0, . .", "9bb427b9-f712-40ab-8ea2-c3af1680bb0d": "The goal of achieving (11) exactly is common; less common is to consider approximating it as an objective. The early work on gradient-TD  appears to be \ufb01rst to have explicitly proposed minimizing the d-weighted norm of the error in (11), which we here call the projected Bellman error: This objective is best understood by looking at the left side of Figure 1. Starting at v\u03b8, the Bellman operator takes us outside the subspace, and the projection operator takes us back into it. The distance between where we end up and where we started is the PBE. The distance is minimal (zero) when the trip up and back leaves us in the same place. dimensional space of all value functions over three states, while shown as a plane is the subspace of all value functions representable by a linear function approximator with parameter w = (w1, w2)>", "b5da3b55-1682-49b0-b2dd-fffc5414a285": "The input to the network, whose architecture we describe below, consisted of raw representations of board positions, and its output had two parts: a scalar value, v, an estimate of the probability that the current player will win from from the current board position, and a vector, p, of move probabilities, one for each possible stone placement on the current board, plus the pass, or resign, move. Instead of selecting self-play actions according to the probabilities p, however, AlphaGo Zero used these probabilities, together with the network\u2019s value output, to direct each execution of MCTS, which returned new move probabilities, shown in Figure 16.7 as the policies \u21e1i. These policies bene\ufb01tted from the many simulations that MCTS conducted s1, ..., sT against itself. In each position st, a Monte-Carlo tree search (MCTS) is executed (see ities computed by the MCTS, at \u21e0 \u03c0\u03c0\u03c0t. The terminal position sT is scored to compute the game winner z", "f4c154be-0f3b-45e4-ad73-7b82c5fa9b87": "AUTOENCODERS  autoencoders thus provide yet another example of how useful properties can emerge as a byproduct of minimizing reconstruction error.\n\nThey are also an example of how overcomplete, high-capacity models may be used as autoencoders as long as care is taken to prevent them from learning the identity function. Denoising autoencoders are presented in more detail in section 14.5. 14.2.3. Regularizing by Penalizing Derivatives  Another strategy for regularizing an autoencoder is to use a penalty Q, as in sparse autoencoders,  L(x, 9(f(#))) + Q(h, x), (14.10) but with a different form of Q: Q(h,z) =X ||Vahill. (14.11)  This forces the model to learn a functidy that does not change much when \u00abx changes slightly. Because this penalty is applied only at training examples, it forces  https://www.deeplearningbook.org/contents/autoencoders.html    the autoencoder to learn features that capture information about the training distribution. An autoencoder regularized in this way is called a contractive autoencoder, or CAE", "9f3843bc-c681-4aef-86dc-7b05c6fe9f5b": "As we now discuss, this poses some serious challenges and is an important factor in\ufb02uencing the design of pattern recognition techniques. In order to illustrate the problem we consider a synthetically generated data set representing measurements taken from a pipeline containing a mixture of oil, water, and gas . These three materials can be present in one of three different geometrical con\ufb01gurations known as \u2018homogenous\u2019, \u2018annular\u2019, and \u2018laminar\u2019, and the fractions of the three materials can also vary. Each data point comprises a 12-dimensional input vector consisting of measurements taken with gamma ray densitometers that measure the attenuation of gamma rays passing along narrow beams through the pipe. This data set is described in detail in Appendix A.\n\nFigure 1.19 shows 100 points from this data set on a plot showing two of the measurements x6 and x7 (the remaining ten input values are ignored for the purposes of this illustration). Each data point is labelled according to which of the three geometrical classes it belongs to, and our goal is to use this data as a training set in order to be able to classify a new observation (x6, x7), such as the one denoted by the cross in Figure 1.19", "23fe7b65-9953-4474-8c89-3489322f985e": "Other knowledge can be explicit, declarative, and relatively straightforward to put into words\u2014everyday commonsense knowledge, like \u201ca cat is a kind of animal,\u201d or very specific facts that you need to know to accomplish your current goals, like \u201cthe meeting with the sales team is at 3:00 PM in room 141.\u201d  Neural networks excel at storing implicit knowledge, but they struggle to memorize facts. Stochastic gradient descent requires many presentations of the same input before it can be stored in neural network parameters, and even then,  https://www.deeplearningbook.org/contents/rnn.html    that input will not be stored especially precisely. Graves ef al. , hypothesized that this is because neural networks lack the equivalent of the working memory system that enables human beings to explicitly hold and manipulate pieces of  information that are relevant to achieving some goal. Such explicit memory components would allow our systems not only to rapidly and \u201cintentionally\u201d store and retrieve specific facts but also to sequentially reason with them.\n\nThe need for neural networks that can process information in a sequence of steps, changing the way the input is fed into the network at each step, has long been recognized as important for the ability to reason rather than to make automatic, intuitive responses to the input", "8b911df5-37cf-4554-835e-09c21f1c57f8": "which is clearly a nonlinear function of the digit position. In this example. !.he lranslation and rotation parameters are latent variables because we observe only the image vectors and are not told which values of the translation or rotation variables were used to create them. For real digit image data, there will be a funher degree of freedom arising from scaling.\n\nMoreover there will be multiple addilional degrees of freedom associaled wilh more complex deformations due to the variability in an individual's wriling 3S well as lhe differences in writing slyles between individuals. evenheless. the number of such degrees of freedom will be small compared to the dimensionality of Ihe data set. AppendiX A Another example is provided by the oil flow data set. in which (for a given geometrical configuration of the gas, WOller, and oil phases) there are only two degrees of freedom of variability corresponding to the fraction of oil in the pipe and the fraction of water (the fraction of gas Ihen being determined)", "afa06977-4261-4878-9d71-db6436892982": "The simplicity of using a linear decoder made these models some of the first latent variable models to be extensively studied. A linear factor model describes the data-generation process as follows.\n\nFirst, we sample the explanatory factors h from a distribution  h~ p(h), (13.1)  (r\\- eo. re rs fad (1 TN  https://www.deeplearningbook.org/contents/linear_factors.html    where P\\/4) 1S a factorial distribution, with Pll) = It PU), So that It 18 easy 485  CHAPTER 13. LINEAR FACTOR MODELS  x= Wh-+ 6+ noise  Figure 13.1: The directed graphical model describing the linear factor model family, in which we assume that an observed data vector x is obtained by a linear combination of independent latent factors h, plus some noise. Different models, such as probabilistic PCA, factor analysis or ICA, make different choices about the form of the noise and of the prior p(h). to sample from", "61e3b50f-b6be-477f-be69-c5a7b1480b44": "While unbiased estimators are clearly desirable, they are not always the \u201cbest\u201d estimators.\n\nAs we will see we often use biased estimators that possess other important properties. 5.4.3 Variance and Standard Error Another property of the estimator that we might want to consider is how much  we expect it to vary as a function of the data sample. Just as we computed the expectation of the estimator to determine its bias, we can compute its variance. https://www.deeplearningbook.org/contents/ml.html    The variance of an estimator is simply the variance  Var(6) (5.45)  where the random variable is the training set. Alternately, the square root of the variance is called the standard error, denoted SE(6). The variance, or the standard error, of an estimator provides a measure of how we would expect the estimate we compute from data to vary as we independently resample the dataset from the underlying data-generating process. Just as we might like an estimator to exhibit low bias, we would also like it to have relatively low variance", "6069c8f2-4cb7-48f0-916d-0660d7b52ca1": "Hence this estimator is called asymptotically unbiased. Although a good choice of q can greatly improve the efficiency of Monte Carlo estimation, a poor choice of q can make the efficiency much worse. Vet back to equation 17.12, we see that if there are samples of gq for which ee IM@)T ig large,  then the variance of the estimator can get very large. This may hapron w when q(a) is tiny while neither p(a) nor f(a) are small enough to cancel it. The g distribution  https://www.deeplearningbook.org/contents/monte_carlo.html    is usually CHLOSELL LO veda SUUple GISUL 1WULLON 5U0 ULLAL it 1S Cady LO Salllple POLL. VV Le IL x ish gh di eqsions ly his simplicity in \u00a2 causes it to match Pp or pl f| poorly. When q(x ' > p(x We Ca |, importance sampling collects useless samples (summin  tiny numbers or zeros)", "8438c78e-9896-4b7c-b5ce-4d3b5e7d8025": "This is the action MCTS actually selects.\n\nAfter the environment transitions to a new state, MCTS is run again, sometimes starting with a tree of a single root node representing the new state, but often starting with a tree containing any descendants of this node left over from the tree constructed by the previous execution of MCTS; all the remaining nodes are discarded, along with the action values associated with them. MCTS was \ufb01rst proposed to select moves in programs playing two-person competitive games, such as Go. For game playing, each simulated episode is one complete play of the game in which both players select actions by the tree and rollout policies. Section 16.6 describes an extension of MCTS used in the AlphaGo program that combines the Monte Carlo evaluations of MCTS with action values learned by a deep arti\ufb01cial neural network via self-play reinforcement learning. Relating MCTS to the reinforcement learning principles we describe in this book provides some insight into how it achieves such impressive results. At its base, MCTS is a decision-time planning algorithm based on Monte Carlo control applied to simulations that start from the root state; that is, it is a kind of rollout algorithm as described in the previous section", "85fb9781-ca66-4581-960f-c95a2234d39c": "(7.40) Now, the expression for Q'w in equation 7.13 for L? regularization can be rear- ranged as  Qlw = (Atal) 1AQ' w*, (7.41)  Ql w = Q'w*.\n\n(7.42)  Comparing equation 7.40 and equation 7.42, we see that if the hyperparameters e, a, and 7 are chosen such that  (I\u2014 eA)\u2019 =(A+alI)~\u2018a, (7.43)  3For neural networks, to obtain symmetry breaking between hidden units, we cannot initialize all the parameters to 0, as discussed in section 6.2. However, the argument holds for any other initial value wo). 248  https://www.deeplearningbook.org/contents/regularization.html    CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  then L? regularization and early stopping can be seen as equivalent (at least under the quadratic approximation of the objective function)", "66a5ea27-c876-4d39-8bb4-b8b67095283e": "Given a support set S' and a test image x, the final predicted class is:  \u00e9s(x) = c(arg max P(x, x;)) xiE  https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Log  where c(x) is the class label of an image x and \u00e9(. ) is the predicted label. The assumption is that the learned embedding can be generalized to be useful for measuring the distance between images of unknown categories. This is the same assumption behind transfer learning via the adoption of a pre-trained model; for example, the convolutional features learned in the model pre-trained with ImageNet are expected to help other image tasks. However, the benefit of a pre-trained model decreases when the new task diverges from the original task that the model was trained on. Matching Networks  The task of Matching Networks  is to learn a classifier cg for any given (small) support set S = {x;, Yi han (k-shot classification).\n\nThis classifier defines a probability distribution over output labels y given a test example x", "38fcb5c9-c275-463e-a78e-60117a37e06f": "This allows the individual steps of the algorithm to be implemented with efficient matrix product operations, or sparsely connected generalizations, like block diagonal matrix products or convolutions. Finally, the deep learning approach to graphical modeling is characterized by a marked tolerance of the unknown.\n\nRather than simplifying the model until all quantities we might want can be computed exactly, we increase the power of  ae 2 2 VD Leet ta te tee Lede etd ee ete eT et ee ee  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    LHe MOUeL ULLLIL IL IS JUS Darely POSsivie LO Lralll OF USE. VWVE OLLEL USE ILLOUCIS whose marginal distributions cannot be computed and are satisfied simply to draw approximate samples from these models. We often train models with an intractable objective function that we cannot even approximate in a reasonable amount of  time, but we are still able to approximately train the model if we can efficiently obtain an estimate of the gradient of such a function", "eb0a7a94-9e33-4de5-be9b-b72b88361cd5": "Invariance to translation means that if we translate the input by a small amount, the values of most of the pooled outputs do not change. See figure 9.8 for an example of how this works. Invariance to local translation can be a useful property if we care more about whether some feature is present than exactly where it is. For example, when determining whether an image contains a face, we need not know the location of the eyes with pixel-perfect accuracy, we just need to know that there is an eye on the left side of the face and an eye on the right side of the face. In other contexts, it is more important to preserve the location of a feature. For example, if we want to find a corner defined  336  CHAPTER 9. CONVOLUTIONAL NETWORKS  POOLING STAGE \u2014 \u2014 \u2014  https://www.deeplearningbook.org/contents/convnets.html    MOLOEOuOY EE  DETECTOR STAGE  POOLING STAGE  QAQAQ. vt  *  DETECTOR STAGE  Figure 9.8: Max pooling introduces invariance. (Top)A view of the middle of the output of a convolutional layer", "8403c16d-8ed5-4d88-b7dd-96691b1f02d6": "In this case, the logistic sigmoid function becomes in\ufb01nitely steep in feature space, corresponding to a Heaviside step function, so that every training point from each class k is assigned a posterior probability p(Ck|x) = 1. Furthermore, there is typically a continuum Exercise 4.14 of such solutions because any separating hyperplane will give rise to the same posterior probabilities at the training data points, as will be seen later in Figure 10.13.\n\nMaximum likelihood provides no way to favour one such solution over another, and which solution is found in practice will depend on the choice of optimization algorithm and on the parameter initialization. Note that the problem will arise even if the number of data points is large compared with the number of parameters in the model, so long as the training data set is linearly separable. The singularity can be avoided by inclusion of a prior and \ufb01nding a MAP solution for w, or equivalently by adding a regularization term to the error function. In the case of the linear regression models discussed in Chapter 3, the maximum likelihood solution, on the assumption of a Gaussian noise model, leads to a closed-form solution. This was a consequence of the quadratic dependence of the log likelihood function on the parameter vector w", "d4ee369d-5303-4566-815f-63af1993d025": "The dataset is directly observed and so is not random. On the other hand, the true parameter 0 is unknown or uncertain and thus is represented as a random variable. Before observing the data, we represent our knowledge of 8 using the prior probability distribution, p(@) (sometimes referred to as simply \u201cthe prior\u201d). Generally, the machine learning practitioner selects a prior distribution that is quite broad (i.e., with high entropy) to reflect a high degree of uncertainty in the value of 8 before observing any data.\n\nFor example, one might assume a priori that @ lies in some finite range or volume, with a uniform distribution. Many priors instead reflect a preference for \u201csimpler\u201d solutions (such as smaller magnitude coefficients, or a function that is closer to being constant). Now consider that we have a set of data samples {2), eng al}, We can recover the effect of data on our belief about @ by combining the data likelihood p(a),...,2 | @) with the prior via Bayes\u2019 rule:  pa), ... 0) | @)p(8) pia,", "5f08e571-1d07-4613-ac5d-b69b9602bcfb": "Here Gaussian kernels of the form exp (\u2212\u03b3\u2225x \u2212 x\u2032\u22252) have been used, with \u03b3 = 0.45. Although predictions for new inputs are made using only the support vectors, the training phase (i.e., the determination of the parameters a and b) makes use of the whole data set, and so it is important to have ef\ufb01cient algorithms for solving the quadratic programming problem. We \ufb01rst note that the objective function \ufffdL(a) given by (7.10) or (7.32) is quadratic and so any local optimum will also be a global optimum provided the constraints de\ufb01ne a convex region (which they do as a consequence of being linear).\n\nDirect solution of the quadratic programming problem using traditional techniques is often infeasible due to the demanding computation and memory requirements, and so more practical approaches need to be found. The technique of chunking  exploits the fact that the value of the Lagrangian is unchanged if we remove the rows and columns of the kernel matrix corresponding to Lagrange multipliers that have value zero. This allows the full quadratic programming problem to be broken down into a series of smaller ones, whose goal is eventually to identify all of the nonzero Lagrange multipliers and discard the others", "e5caab4d-59e3-411b-9756-a30b9dda7d87": "J. C. and R. M. Neal . Good errorcorrecting codes based on very sparse matrices. IEEE Transactions on Information Theory 45, 399\u2013431. MacQueen, J. Some methods for classi\ufb01cation and analysis of multivariate observations. In L. M. LeCam and J. Neyman (Eds. ), Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability, Volume I, pp. 281\u2013297. University of California Press. Magnus, J. R. and H. Neudecker . Matrix Differential Calculus with Applications in Statistics and Econometrics. Wiley. Mallat, S. A Wavelet Tour of Signal Processing (Second ed.). Academic Press. Maybeck, P. S. .\n\nStochastic models, estimation and control. Academic Press. McAllester, D. A", "0f9119cc-927e-420d-b8ad-0e9edd86c920": "Since text generation models intrinsically trade off diversity and quality , we vary the generation diversity by generating samples via top-p sampling  with different p values, and plot the entailment rate and perplexity against diversity, resp. We also evaluate the samples produced by beam-search decoding. Results. Figure 3 (left) shows the results, and Table A.5 shows samples. First, notice that MLE performs poorly, while MLE+reward improves upon it. This is not surprising as the training data contain noisy/negative examples. Similarly, since the pure off-policy algorithm GOLD-s relies heavily on the data distribution, we observed that it achieves suboptimal performance. The on-policy MLE+PG with MLE initialization gives better entailment rate. In comparison, our full SQL framework achieves the best entailment-diversity trade-off. The comparison between SQL and SQL(single) highlights the importance of having the multi-step objective which directly uses the end reward rather than bootstrapping intermediate Q-values for supervision.\n\nWe next study the application in text adversarial attacks, where again no supervised data is available", "27a10828-cc1d-4e31-8381-64b40d5e86f2": "The primary obstacle revealed by this projection is a saddle point of high cost near where the parameters are initialized, but, as indicated by the blue path, the SGD training trajectory escapes this saddle point readily.\n\nMost of training time is spent traversing the relatively flat valley of the cost function, perhaps because of high noise in the gradient, poor conditioning of the Hessian matrix in this region, or simply the need to circumnavigate the tall \u201cmountain\u201d visible in the figure via an indirect arcing path. Image adapted with permission from Goodfellow et al. also argue that continuous-time gradient descent may be shown analytically to be repelled from, rather than attracted to, a nearby saddle point, but the situation may be different for more realistic uses of gradient descent. For Newton\u2019s method, saddle points clearly constitute a problem. Gradient descent is designed to move \u201cdownhill\u201d and is not explicitly designed to seek a critical point. Newton\u2019s method, however, is designed to solve for a point where the gradient is zero. Without appropriate modification, it can jump to a saddle point. The proliferation of saddle points in high-dimensional spaces presumably explains why second-order methods have not succeeded in replacing gradient descent for neural network training", "1996b2e5-e7a6-462d-b2c7-b52eb65fc645": "STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    \\L/ \\/ \\L/  Figure 16.13: An example of how a factor graph can resolve ambiguity in the interpretation of undirected networks. (Left) An undirected network with a clique involving three variables: a, b and c. (Center) A factor graph corresponding to the same undirected model. This factor graph has one factor over all three variables. (Right) Another valid factor graph for the same undirected model. This factor graph has three factors, each over only two variables. Representation, inference, and learning are all asymptotically cheaper in this factor graph than in the factor graph depicted in the center, even though both require the same undirected graph to represent.\n\nin the graph if and only if the variable is one of the arguments to the factor in the unnormalized probability distribution. No factor may be connected to another factor in the graph, nor can a variable be connected to a variable", "990fa16d-2bb4-4afd-b281-c3a8e935975e": "If, for instance, we have a model in which the diagoSection 8.1.2 nal transition elements Akk are much larger than the off-diagonal elements, then a typical data sequence will have long runs of points generated from a single component, with infrequent transitions from one component to another. The generation of samples from a hidden Markov model is illustrated in Figure 13.8. There are many variants of the standard HMM model, obtained for instance by imposing constraints on the form of the transition matrix A .\n\nHere we mention one of particular practical importance called the left-to-right HMM, which is obtained by setting the elements Ajk of A to zero if k < j, as illustrated in the state transition diagram for a 3-state HMM in Figure 13.9. Typically for such models the initial state probabilities for p(z1) are modi\ufb01ed so that p(z11) = 1 and p(z1j) = 0 for j \u0338= 1, in other words every sequence is constrained to start in state j = 1. The transition matrix may be further constrained to ensure that large changes in the state index do not occur, so that Ajk = 0 if k > j + \u2206", "30a98d8e-c62b-4311-a3cb-c3c7cb731865": "A di\u21b5erent model-free strategy might simply rely on a cached policy instead of action values, making direct links from S1 to L and from S2 to R. In neither of these strategies do decisions rely on an environment model. There is no need to consult a state-transition model, and no connection is required between the features of the goal boxes and the rewards they deliver. model consisting of a state-transition model and a reward model.\n\nThe state-transition model is shown as a decision tree, and the reward model associates the distinctive features of the goal boxes with the rewards to be found in each. (The rewards associated with states S1, S2, and S3 are also part of the reward model, but here they are zero and are not shown.) A model-based agent can decide which way to turn at each state by using the model to simulate sequences of action choices to \ufb01nd a path yielding the highest return. In this case the return is the reward obtained from the outcome at the end of the path. Here, with a su\ufb03ciently accurate model, the rat would select L and then R to obtain reward of 4", "109f89b8-6bbd-4c20-91c9-9c17f4f839a8": "(4.19)  We may also convert this to a problem with maximization in the outer loop:  max jn ti min n f(x y+ ye rig (a =~ Deayhl . (4.20) The sign of the term for the equality constraints does not matter; we may define it with addition or subtraction as we wish, because the optimization is free to choose any sign for each );. The inequality constraints are particularly interesting. We say that a constraint h\u00ae (a) is active if h (w*) = 0. If a constraint is not active, then the solution to the problem found using that constraint would remain at least a local solution if that constraint were removed. It is possible that an inactive constraint excludes other solutions. For example, a convex problem with an entire region of globally optimal points (a wide, flat region of equal cost) could have a subset of this region eliminated by constraints, or a nonconvex problem could have better local stationary points excluded by a constraint that is inactive at convergence.\n\nYet the point found at convergence remains a stationary point whether or not the inactive constraints are included", "947c40a8-2d91-4f4e-91b4-4d1140118906": "(Later, in Chapter 10, we will introduce a formulation that is both continuing and undiscounted.) Almost all reinforcement learning algorithms involve estimating value functions\u2014functions of states (or of state\u2013action pairs) that estimate how good it is for the agent to be in a given state (or how good it is to perform a given action in a given state). The notion of \u201chow good\u201d here is de\ufb01ned in terms of future rewards that can be expected, or, to be precise, in terms of expected return. Of course the rewards the agent can expect to receive in the future depend on what actions it will take. Accordingly, value functions are de\ufb01ned with respect to particular ways of acting, called policies. Formally, a policy is a mapping from states to probabilities of selecting each possible action. If the agent is following policy \u21e1 at time t, then \u21e1(a|s) is the probability that At = a if St = s. Like p, \u21e1 is an ordinary function; the \u201c|\u201d in the middle of \u21e1(a|s) merely reminds that it de\ufb01nes a probability distribution over a 2 A(s) for each s 2 S", "5fb01bb0-b5cd-479e-b530-0d8009be5e56": "Other work preserves the labels of the text by conditioning on the label when generating the LMs\u2019 predictions . In addition, different sampling strategies for word replacement have been explored. For example, instead of sampling one speci\ufb01c word from candidates by LMs, Gao et al. propose to compute a weighted average over embeddings of possible words predicted by LMs as the replaced input since the averaged representations could augment text with richer information. Random Insertion, Replacement, Deletion and Swapping. While well-designed local modi\ufb01cations can preserve the syntax and semantic meaning of a sentence , random local modi\ufb01cations such as deleting certain tokens , inserting random tokens , replacing non-important tokens with random tokens  or randomly swapping tokens in one sentence  can preserve the meaning in practice. Different Kolomiyets et al. , Zhang et al. , Yang , Miao et al. , Wei and Zou  Kolomiyets et al.\n\n, Gao et al. Kobayashi , Wu et al. Fadaee et al", "ece0a361-be40-4073-bdbf-33742fca57ba": "Speci\ufb01cally, the ith component of the belief-state update function is for all a 2 A, o 2 O, and belief states s 2 Rd with components s, where the fourargument p function here is not the usual one for MDPs (as in Chapter 3), but the analogous one for POMDPs, in terms of the latent state: p(x0, o|x, a) .= Pr{Xt =x0, Ot =o |Xt\u22121 =x, At\u22121 =a}. This approach is popular in theoretical work and has many signi\ufb01cant applications, but its assumptions and computational complexity scale poorly and we do not recommend it as an approach to arti\ufb01cial intelligence. Another example of Markov states is provided by Predictive State Representations, or PSRs. PSRs address the weakness of the POMDP approach that the semantics of its agent state St are grounded in the environment state, Xt, which is never observed and thus is di\ufb03cult to learn about", "10ab4033-0033-491f-b040-46c4cce3ccfc": "It has the advantage of being easy to train because many loss functions result in convex optimization problems when  https://www.deeplearningbook.org/contents/mlp.html    applied to linear models. Unfortunately, we often want our systems to learn nonlinear functions. At first glance, we might presume that learning a nonlinear function requires designing a specialized model family for the kind of nonlinearity we want to learn. Fortunately, feedforward networks with hidden layers provide a universal approxi- mation framework. Specifically, the universal approximation theorem  states that a feedforward network with a linear output layer and at least one hidden layer with any \u201csquashing\u201d activation function (such as the logistic sigmoid activation function) can approximate any Borel measurable function from one finite-dimensional space to another with any desired nonzero amount of error, provided that the network is given enough hidden units", "04521a32-780c-4dae-9e65-443be0541bd1": "One approach is to choose a feature space mapping \u03c6(x) and then use this to \ufb01nd the corresponding kernel, as is illustrated in Figure 6.1. Here the kernel function is de\ufb01ned for a one-dimensional input space by where \u03c6i(x) are the basis functions. An alternative approach is to construct kernel functions directly. In this case, we must ensure that the function we choose is a valid kernel, in other words that it corresponds to a scalar product in some (perhaps in\ufb01nite dimensional) feature space. As a simple example, consider a kernel function given by tions.\n\nIn each column the lower plot shows the kernel function k(x, x\u2032) de\ufb01ned by (6.10) plotted as a function of x for x\u2032 = 0, while the upper plot shows the corresponding basis functions given by polynomials (left column), \u2018Gaussians\u2019 (centre column), and logistic sigmoids (right column)", "6c9d878c-64e1-45e6-8a59-c4a4e05a7435": "Bottou and Vapnik  demonstrated surprising e\ufb03ciency of several local learning algorithms compared to non-local algorithms in some pattern recognition tasks, discussing the impact of local learning on generalization. Bentley  introduced k-d trees and reported observing average running time of O(log n) for nearest neighbor search over n records. Friedman, Bentley, and Finkel  clari\ufb01ed the algorithm for nearest neighbor search with k-d trees. Omohundro  discussed e\ufb03ciency gains possible with hierarchical data structures such as k-d-trees. Moore, Schneider, and Deng  introduced the use of k-d trees for e\ufb03cient locally weighted regression. 9.10 The origin of kernel regression is the method of potential functions of Aizerman, Braverman, and Rozonoer . They likened the data to point electric charges of various signs and magnitudes distributed over space. The resulting electric potential over space produced by summing the potentials of the point charges corresponded to the interpolated surface", "73cd3675-abea-4288-b9bc-2b12c862e09b": "An even simpler approach is (c) in which we use the training data to \ufb01nd a discriminant function f(x) that maps each x directly onto a class label, thereby combining the inference and decision stages into a single learning problem.\n\nIn the example of Figure 1.27, this would correspond to \ufb01nding the value of x shown by the vertical green line, because this is the decision boundary giving the minimum probability of misclassi\ufb01cation. With option (c), however, we no longer have access to the posterior probabilities p(Ck|x). There are many powerful reasons for wanting to compute the posterior probabilities, even if we subsequently use them to make decisions. These include: Minimizing risk. Consider a problem in which the elements of the loss matrix are subjected to revision from time to time (such as might occur in a \ufb01nancial application). If we know the posterior probabilities, we can trivially revise the minimum risk decision criterion by modifying (1.81) appropriately. If we have only a discriminant function, then any change to the loss matrix would require that we return to the training data and solve the classi\ufb01cation problem afresh. Reject option", "d44a5d62-2f33-4830-b183-f843bab00cf1": "We compare our SQL with MLE+PG.\n\nWe use all hypotheses in the MultiNLI dataset as the training data for the MLE training in MLE+PG and the offpolicy updates for our SQL. We do not compare with previous specialized adversarial text attack methods, because they either are not applicable to the universal attack setting , or were not designed to generate human-readable sentences . Besides, it is worth noting that the general RL algorithms have an additional advantage of doing black-box attacks. That is, the algorithms only require the ability to query the entailment classi\ufb01er for entailment probability, without need of knowing the internal structure of the classi\ufb01er (e.g., for computing gradients) as in previous attack algorithms . For top-p sampling results, we sample a hypothesis for each premise and measure the average attack rate across the dataset. This is because sampling multiple hypotheses, each for all premises, and measure performance are expensive. Since the hypotheses are sampled input-independently, this should be a good approximation", "930b076c-8902-45f5-9483-db30143f42bb": "Also, referring back to Figure 11.1, we note that if f(z) is small in regions where p(z) is large, and vice versa, then the expectation may be dominated by regions of small probability, implying that relatively large sample sizes will be required to achieve suf\ufb01cient accuracy. For many models, the joint distribution p(z) is conveniently speci\ufb01ed in terms of a graphical model. In the case of a directed graph with no observed variables, it is straightforward to sample from the joint distribution (assuming that it is possible to sample from the conditional distributions at each node) using the following ancestral sampling approach, discussed brie\ufb02y in Section 8.1.2.\n\nThe joint distribution is speci\ufb01ed by where zi are the set of variables associated with node i, and pai denotes the set of variables associated with the parents of node i. To obtain a sample from the joint distribution, we make one pass through the set of variables in the order z1, . , zM sampling from the conditional distributions p(zi|pai). This is always possible because at each step all of the parent values will have been instantiated. After one pass through the graph, we will have obtained a sample from the joint distribution", "087ab826-f532-4fdf-bd28-fb43ebbdc646": "As with rejection and importance sampling, we again sample from a proposal distribution. This time, however, we maintain a record of the current state z(\u03c4), and the proposal distribution q(z|z(\u03c4)) depends on this current state, and so the sequence of samples z(1), z(2), . forms a Markov chain. Again, if we write p(z) = \ufffdp(z)/Zp, Section 11.2.1 we will assume that \ufffdp(z) can readily be evaluated for any given value of z, although the value of Zp may be unknown. The proposal distribution itself is chosen to be suf\ufb01ciently simple that it is straightforward to draw samples from it directly. At each cycle of the algorithm, we generate a candidate sample z\u22c6 from the proposal distribution and then accept the sample according to an appropriate criterion", "ad9009e7-7559-4693-83f8-9d0fded16f17": ", xD) + The analogous de\ufb01nition of a functional derivative arises when we consider how much a functional F changes when we make a small change \u03f5\u03b7(x) to the function y(x), where \u03b7(x) is an arbitrary function of x, as illustrated in Figure D.1. We denote the functional derivative of E with respect to f(x) by \u03b4F/\u03b4f(x), and de\ufb01ne it by the following relation: This can be seen as a natural extension of (D.2) in which F now depends on a continuous set of variables, namely the values of y at all points x. Requiring that the functional be stationary with respect to small variations in the function y(x) gives Because this must hold for an arbitrary choice of \u03b7(x), it follows that the functional derivative must vanish. To see this, imagine choosing a perturbation \u03b7(x) that is zero everywhere except in the neighbourhood of a point \ufffdx, in which case the functional derivative must be zero at x = \ufffdx. However, because this must be true for every choice of \ufffdx, the functional derivative must vanish for all values of x", "8ee9e1fd-8313-4704-bcec-db4be4150148": "The max pooling unit then has a large activation regardless of which detector unit was activated. We show here how the network processes two different inputs, resulting in two different detector units being activated. The effect on the pooling unit is roughly the same either way. This principle is leveraged by maxout networks  and other convolutional networks. Max pooling over spatial positions is naturally invariant to translation; this multichannel approach is only necessary for learning other transformations. SEWEWE  Figure 9.10: Pooling with downsampling. Here we use max pooling with a pool width of three and a stride between pools of two.\n\nThis reduces the representation size by a factor of two, which reduces the computational and statistical burden on the next layer. Note that the rightmost pooling region has a smaller size but must be included if we do not want to ignore some of the detector units. Large response in detector unit 3  5 |e  the number of parameters in the next layer is a function of its input size (such as when the next layer is fully connected and based on matrix multiplication), this  338  https://www.deeplearningbook.org/contents/convnets.html    CHAPTER 9", "6d639053-969e-4679-8407-0b74b7585199": "D. Cubuk, and Q. V. Le. Specaugment: A simple data augmentation method for automatic speech recognition. arXiv preprint arXiv 1904.08779. X. Peng, Z. Tang, F. Yang, R. S. Feris, and D. Metaxas. Jointly optimize data augmentation and network training: Adversarial data augmentation in human pose estimation. In CVPR, 2018. A. J. Ratner, H. Ehrenberg, Z. Hussain, J. Dunnmon, and C. R\u00e9. Learning to compose domain-speci\ufb01c transformations for data augmentation. In NeurIPS, 2017. M. Ren, W. Zeng, B. Yang, and R. Urtasun. Learning to reweight examples for robust deep learning. In ICML, 2018. S. Roweis and Z", "07da967a-b917-4bba-aebb-fd5850f7e5e0": "We turn now to the subject of learning when reinforcing stimuli occur well after the events they reinforce.\n\nThe mechanisms used by reinforcement learning algorithms to enable learning with delayed reinforcement\u2014eligibility traces and TD learning\u2014closely correspond to psychologists\u2019 hypotheses about how animals can learn under these conditions. The Law of E\u21b5ect requires a backward e\u21b5ect on connections, and some early critics of the law could not conceive of how the present could a\u21b5ect something that was in the past. This concern was ampli\ufb01ed by the fact that learning can even occur when there is a considerable delay between an action and the consequent reward or penalty. Similarly, in classical conditioning, learning can occur when US onset follows CS o\u21b5set by a non-negligible time interval. We call this the problem of delayed reinforcement, which is related to what Minsky  called the \u201ccredit-assignment problem for learning systems\u201d: how do you distribute credit for success among the many decisions that may have been involved in producing it? The reinforcement learning algorithms presented in this book include two basic mechanisms for addressing this problem", "da3f4878-fb82-4ae9-8c97-69d8cf9a1528": "Because the parameter vector w is marginalized out, we can regard it as a latent variable, and hence we can optimize this marginal likelihood function using EM.\n\nIn the E step, we compute the posterior distribution of w given the current setting of the parameters \u03b1 and \u03b2 and then use this to \ufb01nd the expected complete-data log likelihood. In the M step, we maximize this quantity with respect to \u03b1 and \u03b2. We have already derived the posterior distribution of w because this is given by (3.49). The complete-data log likelihood function is then given by where the likelihood p(t|w, \u03b2) and the prior p(w|\u03b1) are given by (3.10) and (3.52), respectively, and y(x, w) is given by (3.3). Taking the expectation with respect to the posterior distribution of w then gives Note that this re-estimation equation takes a slightly different form from the corresponding result (3.92) derived by direct evaluation of the evidence function. However, they each involve computation and inversion (or eigen decomposition) of an M \u00d7 M matrix and hence will have comparable computational cost per iteration", "7df2162c-0812-4cd4-b70f-9f70bc6dd38e": "Starting from the upper bound of negative marginal log-likelihood (Equation 2.9) with maximum entropy and minimum cross entropy, the originally intractable MLE problem gets simpli\ufb01ed, and a series of optimization algorithms, ranging from (variational) EM to wake-sleep, arise naturally as an approximation to the original solution. 2.2. Bayesian Inference. Now we revisit another classical learning framework, Bayesian inference, and examine its intriguing connections with the maximum entropy principle. Interestingly, the the maximum entropy principle can also help to reformulate Bayesian inference as a constraint optimization problem, as for MLE.\n\nDi\ufb00erent from MLE, Bayesian approach for statistical inference treats the hypotheses (parameters \u03b8) to be inferred as random variables", "9dfd4d43-dbcd-439a-9de9-b5879032715a": "Note that, although the polynomial function y(x, w) is a nonlinear function of x, it is a linear function of the coef\ufb01cients w. Functions, such as the polynomial, which are linear in the unknown parameters have important properties and are called linear models and will be discussed extensively in Chapters 3 and 4. The values of the coef\ufb01cients will be determined by \ufb01tting the polynomial to the training data. This can be done by minimizing an error function that measures the mis\ufb01t between the function y(x, w), for any given value of w, and the training set data points.\n\nOne simple choice of error function, which is widely used, is given by the sum of the squares of the errors between the predictions y(xn, w) for each data point xn and the corresponding target values tn, so that we minimize where the factor of 1/2 is included for later convenience. We shall discuss the motivation for this choice of error function later in this chapter. For the moment we simply note that it is a nonnegative quantity that would be zero if, and only if, the function y(x, w) were to pass exactly through each training data point", "39f8377b-8617-4805-89fa-69640829d440": "Naively, we could consider explicitly all of the exponentially many paths through the lattice, evaluate the probability for each, and then select the path having the highest probability. However, we notice that we can make a dramatic saving in computational cost as follows. Suppose that for each path we evaluate its probability by summing up products of transition and emission probabilities as we work our way forward along each path through the lattice. Consider a particular time step n and a particular state k at that time step. There will be many possible paths converging on the corresponding node in the lattice diagram. However, we need only retain that particular path that so far has the highest probability. Because there are K states at time step n, we need to keep track of K such paths.\n\nAt time step n + 1, there will be K2 possible paths to consider, comprising K possible paths leading out of each of the K current states, but again we need only retain K of these corresponding to the best path for each state at time n+1. When we reach the \ufb01nal time step N we will discover which state corresponds to the overall most probable path", "3349ac6b-a355-47a7-b69c-5a5ab92eeddb": "We then introduce Lagrange multipliers {\u03bbj} and {\u00b5k}, and then optimize the Lagrangian function given by subject to \u00b5k \u2a7e 0 and \u00b5khk(x) = 0 for k = 1, . , K. Extensions to constrained functional derivatives are similarly straightforward. For a more detailed discussion Appendix D of the technique of Lagrange multipliers, see Nocedal and Wright . Amari, S., A. Cichocki, and H. H. Yang . A new learning algorithm for blind signal separation. In D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo (Eds. ), Advances in Neural Information Processing Systems, Volume 8, pp. 757\u2013763. MIT Press. Anderson, T. W. Asymptotic theory for principal component analysis. Annals of Mathematical Statistics 34, 122\u2013148. Andrieu, C., N", "13c78930-4924-4132-88d1-3887afb53b34": "Without the topological sorting, we might attempt to sample a variable before its parents are available. 577  CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  For some graphs, more than one topological ordering is possible.\n\nAncestral sampling may be used with any of these topological orderings. file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    Ancestral sampling is generally very fast (assuming sampling from each condi- tional is easy) and. convenient. One drawback to ancestral sampling is that it only applies to directed graphical models. Another drawback is that it does not support every conditional sampling operation. When we wish to sample from a subset of the variables in a directed graphical model, given some other variables, we often require that all the condition- ing variables come earlier than the variables to be sampled in the ordered graph. In this case, we can sample from the local conditional probability distributions specified by the model distribution. Otherwise, the conditional distributions we need to sample from are the posterior distributions given the observed variables", "97cee526-4b8b-48e0-879d-69fea6a4206f": "Hence the eigenvectors can be chosen to be orthogonal, and by normalizing can be set to unit length. Because there are M eigenvalues, the corresponding M orthogonal eigenvectors form a complete set and so any M-dimensional vector can be expressed as a linear combination of the eigenvectors. We can take the eigenvectors ui to be the columns of an M \u00d7 M matrix U, which from orthonormality satis\ufb01es Such a matrix is said to be orthogonal. Interestingly, the rows of this matrix are also orthogonal, so that UUT = I. To show this, note that (C.37) implies UTUU\u22121 = U\u22121 = UT and so UU\u22121 = UUT = I. Using (C.12), it also follows that |U| = 1.\n\nThe eigenvector equation (C.29) can be expressed in terms of U in the form then the length of the vector is preserved because and similarly the angle between any two such vectors is preserved because Thus, multiplication by U can be interpreted as a rigid rotation of the coordinate system", "fdb52a6b-57e3-4056-aaef-61f42b0e438f": "In order to ensure that the predictions yCOM(x) remain within sensible limits, suppose that we require that they be bounded at each value of x by the minimum and maximum values given by any of the members of the committee, so that Show that a necessary and suf\ufb01cient condition for this constraint is that the coef\ufb01cients \u03b1m satisfy 14.6 (\u22c6) www By differentiating the error function (14.23) with respect to \u03b1m, show that the parameters \u03b1m in the AdaBoost algorithm are updated using (14.17) in which \u03f5m is de\ufb01ned by (14.16).\n\n14.7 (\u22c6) By making a variational minimization of the expected exponential error function given by (14.27) with respect to all possible functions y(x), show that the minimizing function is given by (14.28). 14.8 (\u22c6) Show that the exponential error function (14.20), which is minimized by the AdaBoost algorithm, does not correspond to the log likelihood of any well-behaved probabilistic model. This can be done by showing that the corresponding conditional distribution p(t|x) cannot be correctly normalized", "ebeb45bc-d6cd-484c-8766-a8918278433f": "Equating these two results, we obtain Eq(z) = Ep(z). (10.187) We see that the optimum solution simply corresponds to matching the expected suf\ufb01cient statistics. So, for instance, if q(z) is a Gaussian N(z|\u00b5, \u03a3) then we minimize the Kullback-Leibler divergence by setting the mean \u00b5 of q(z) equal to the mean of the distribution p(z) and the covariance \u03a3 equal to the covariance of p(z). This is sometimes called moment matching. An example of this was seen in Figure 10.3(a). Now let us exploit this result to obtain a practical algorithm for approximate inference.\n\nFor many probabilistic models, the joint distribution of data D and hidden variables (including parameters) \u03b8 comprises a product of factors in the form This would arise, for example, in a model for independent, identically distributed data in which there is one factor fn(\u03b8) = p(xn|\u03b8) for each data point xn, along with a factor f0(\u03b8) = p(\u03b8) corresponding to the prior", "46caf246-f9ad-415b-9c8c-d79a581284e1": "This example and the more general argument in the box show that if we optimized discounted value over the on-policy distribution, then the e\u21b5ect would be identical to optimizing undiscounted average reward; the actual value of \u03b3 would have no e\u21b5ect. This strongly suggests that discounting has no role to play in the de\ufb01nition of the control problem with function approximation. One can nevertheless go ahead and use discounting in solution methods. The discounting parameter \u03b3 changes from a problem parameter to a solution method parameter!\n\nUnfortunately, discounting algorithms with function approximation do not optimize discounted value over the on-policy distribution, and thus are not guaranteed to optimize average reward. The root cause of the di\ufb03culties with the discounted control setting is that with function approximation we have lost the policy improvement theorem (Section 4.2). It is no longer true that if we change the policy to improve the discounted value of one state then we are guaranteed to have improved the overall policy in any useful sense. That guarantee was key to the theory of our reinforcement learning control methods", "a5c12bff-397b-404d-99d8-4f4e65537ff4": "Temporal-di\u21b5erence learning methods are distinctive in being driven by the di\u21b5erence between temporally successive estimates of the same quantity\u2014for example, of the probability of winning in the tic-tac-toe example. This thread is smaller and less distinct than the other two, but it has played a particularly important role in the \ufb01eld, in part because temporal-di\u21b5erence methods seem to be new and unique to reinforcement learning. The origins of temporal-di\u21b5erence learning are in part in animal learning psychology, in particular, in the notion of secondary reinforcers. A secondary reinforcer is a stimulus that has been paired with a primary reinforcer such as food or pain and, as a result, has come to take on similar reinforcing properties. Minsky  may have been the \ufb01rst to realize that this psychological principle could be important for arti\ufb01cial learning systems. Arthur Samuel  was the \ufb01rst to propose and implement a learning method that included temporal-di\u21b5erence ideas, as part of his celebrated checkers-playing program (Section 16.2).\n\nSamuel made no reference to Minsky\u2019s work or to possible connections to animal learning", "bc75c101-0436-4498-aefe-8561aec95ba4": "This was a deep convolutional ANN with the same structure as the SL policy network. It was initialized with the \ufb01nal weights of the SL policy network that were learned via supervised learning, and then policy-gradient reinforcement learning was used to improve upon the SL policy. In the second stage of training the value network, the team used Monte Carlo policy evaluation on data obtained from a large number of simulated self-play games with moves selected by the RL policy network. in what the DeepMind team called the \u201cAlphaGo pipeline.\u201d All these networks were trained before any live game play took place, and their weights remained \ufb01xed throughout live play. sampled state-action pairs (s, a), using stochastic gradient ascent to  maximize the likelihood of the human move a selected in state s We trained a 13-layer policy network, which we call the SL policy  network, from 30 million positions from the KGS Go Server", "adc18582-45b2-49db-a30e-5a01ae9b965f": "Then the aggregate associative strength corresponding to a state s is given by (14.1), the same as for the Rescorla-Wagner model, but the TD model updates the associative strength vector, w, di\u21b5erently. With t now labeling a time step instead of a complete trial, the TD model governs learning according to this update: which replaces xt(St) in the Rescorla\u2013Wagner update (14.2) with zt, a vector of eligibility traces, and instead of the \u03b4t of (14.3), here \u03b4t is a TD error: where \u03b3 is a discount factor (between 0 and 1), Rt is the prediction target at time t, and \u02c6v(St+1,wt) and \u02c6v(St,wt) are aggregate associative strengths at t + 1 and t as de\ufb01ned by (14.1)", "25f31433-fc02-471a-ab0e-e83b610544ce": "In such cases, Expected Sarsa can safely set \u21b5 = 1 without su\u21b5ering any degradation of asymptotic performance, whereas Sarsa can only perform well in the long run at a small value of \u21b5, at which short-term performance is poor.\n\nIn this and other examples there is a consistent empirical advantage of Expected Sarsa over Sarsa. In these cli\u21b5 walking results Expected Sarsa was used on-policy, but in general it might use a policy di\u21b5erent from the target policy \u21e1 to generate behavior, in which case it becomes an o\u21b5-policy algorithm. For example, suppose \u21e1 is the greedy policy while behavior is more exploratory; then Expected Sarsa is exactly Q-learning. In this sense Expected Sarsa subsumes and generalizes Q-learning while reliably improving over Sarsa. Except for the small additional computational cost, Expected Sarsa may completely dominate both of the other more-well-known TD control algorithms. All the control algorithms that we have discussed so far involve maximization in the construction of their target policies", "1efe180c-4750-4df5-9af9-536d913b75a7": "The policy improvement theorem applies to the two policies that we considered at the beginning of this section, an original deterministic policy: \u21e1, and a changed policy, \u21e10, that is identical to \u21e1 except that \u21e10(s) = a 6= \u21e1(s). For states other than s, (4.7) holds because the two sides are equal. Thus, if q\u21e1(s, a) > v\u21e1(s), then the changed policy is indeed better than \u21e1. The idea behind the proof of the policy improvement theorem is easy to understand. = E\u21e10 | St =s] So far we have seen how, given a policy and its value function, we can easily evaluate a change in the policy at a single state to a particular action.\n\nIt is a natural extension to consider changes at all states and to all possible actions, selecting at each state the action that appears best according to q\u21e1(s, a). In other words, to consider the new greedy policy, \u21e10, given by where argmaxa denotes the value of a at which the expression that follows is maximized (with ties broken arbitrarily)", "ff62d53c-0968-43de-bc25-6088272cee19": "A practical Bayesian framework for back-propagation networks. Neural Computation 4(3), 448\u2013472. MacKay, D. J. C. Bayesian neural networks and density networks. Nuclear Instruments and Methods in Physics Research, A 354(1), 73\u201380. MacKay, D. J. C. Ensemble learning for hidden Markov models. Unpublished manuscript, MacKay, D. J. C. Comparison of approximate methods for handling hyperparameters. Neural Computation 11(5), 1035\u20131068. MacKay, D. J. C. and M. N. Gibbs . Density networks. In J. W. Kay and D. M. Titterington (Eds. ), Statistics and Neural Networks: Advances at the Interface, Chapter 5, pp. 129\u2013145. Oxford University Press. MacKay, D", "6f441657-43d6-49b8-b82c-430ed778ec25": "paper) Classi\ufb01cation: LCC Q325.6 .R45 2018 | DDC 006.3/1--dc23 LC record available 9.10 Kernel-based Function Approximation . 232 9.11 Looking Deeper at On-policy Learning: Interest and Emphasis . 234 16.6 Mastering the Game of Go . .\n\n441 The twenty years since the publication of the \ufb01rst edition of this book have seen tremendous progress in arti\ufb01cial intelligence, propelled in large part by advances in machine learning, including advances in reinforcement learning. Although the impressive computational power that became available is responsible for some of these advances, new developments in theory and algorithms have been driving forces as well. In the face of this progress, a second edition of our 1998 book was long overdue, and we \ufb01nally began the project in 2012. Our goal for the second edition was the same as our goal for the \ufb01rst: to provide a clear and simple account of the key ideas and algorithms of reinforcement learning that is accessible to readers in all the related disciplines. The edition remains an introduction, and we retain a focus on core, online learning algorithms. This edition includes some new topics that rose to importance over the intervening years, and we expanded coverage of topics that we now understand better", "c1eed209-78d7-45f2-97ff-a0c7852285d9": "Each step of the Gibbs sampling procedure involves replacing the value of one of the variables by a value drawn from the distribution of that variable conditioned on the values of the remaining variables. Thus we replace zi by a value drawn from the distribution p(zi|z\\i), where zi denotes the ith component of z, and z\\i denotes z1, . , zM but with zi omitted. This procedure is repeated either by cycling through the variables and so on, cycling through the three variables in turn. States at Yale, a post for which he received no salary because at the time he had no publications.\n\nHe developed the \ufb01eld of vector analysis and made contributions to crystallography and planetary orbits. His most famous work, entitled On the Equilibrium of Heterogeneous Substances, laid the foundations for the science of physical chemistry. To show that this procedure samples from the required distribution, we \ufb01rst of all note that the distribution p(z) is an invariant of each of the Gibbs sampling steps individually and hence of the whole Markov chain", "7f66aa88-cdcd-4292-993d-5be589720b8a": "The amount of information can be viewed as the \u2018degree of surprise\u2019 on learning the value of x. If we are told that a highly improbable event has just occurred, we will have received more information than if we were told that some very likely event has just occurred, and if we knew that the event was certain to happen we would receive no information.\n\nOur measure of information content will therefore depend on the probability distribution p(x), and we therefore look for a quantity h(x) that is a monotonic function of the probability p(x) and that expresses the information content. The form of h(\u00b7) can be found by noting that if we have two events x and y that are unrelated, then the information gain from observing both of them should be the sum of the information gained from each of them separately, so that h(x, y) = h(x) + h(y). Two unrelated events will be statistically independent and so p(x, y) = p(x)p(y)", "3acb4b97-11ac-4115-a738-1620995c8e41": "Second, reinforcement learning algorithms are associative, meaning that the alternatives found by selection are associated with particular situations, or states, to form the agent\u2019s policy. Like learning described by the Law of E\u21b5ect, reinforcement learning is not just the process of \ufb01nding actions that produce a lot of reward, but also of connecting these actions to situations or states. Thorndike used the phrase learning by \u201cselecting and connecting\u201d . Natural selection in evolution is a prime example of a selectional process, but it is not associative (at least as it is commonly understood); supervised learning is associative, but it is not selectional because it relies on instructions that directly tell the agent how to change its behavior. In computational terms, the Law of E\u21b5ect describes an elementary way of combining search and memory: search in the form of trying and selecting among many actions in each situation, and memory in the form of associations linking situations with the actions found\u2014so far\u2014to work best in those situations. Search and memory are essential components of all reinforcement learning algorithms, whether memory takes the form of an agent\u2019s policy, value function, or environment model", "503e6a1a-5aa4-4667-b9b1-55fdd30a6499": "We herefore divide by a normalizing constant Z, defined to be the sum or integral over all states of the product of the \u00a2 functions, in order to obtain a normalized  probability distribution: 1 . = \u00ae (e@ vx) = [Jo (). (3.55)  See figure 3.8 for an example of an undirected graph and the factorization of probability distributions it represents. Keep in mind that these graphical representations of factorizations are a language for describing probability distributions. They are not mutually exclusive families of probability distributions. Being directed or undirected is not a property of a probability distribution; it is a property of a particular description of a probability distribution, but any probability distribution may be described in both ways. Throughout parts I and II of this book, we use structured probabilistic models  https://www.deeplearningbook.org/contents/prob.html    76  CHAPTER 3. PROBABILITY AND INFORMATION THEORY  Figure 3.8: An undirected graphical model over random variables a, b,c, d and e", "3b87f117-d16b-4b9f-8f2d-c86827a9b7fc": "The network is again trained by minimization of the error function (12.91). We can view this network as two successive functional mappings F] and F 2 , as indicated in Figure 12.19. The first mapping F] projects the original Ddimensional data onto an AI-dimensional subspace S defined by the activations of the units in the second hidden layer. Because of the presence of the first hidden layer of nonlinear units. this mapping is very general.\n\nand in particular is not restricted to being linear. Similarly. the second half of the network defines an arbitrary functional mapping from the M -dimensional space back into the original D-dimensional input space. This has a simple geometrical interpretation. as indicated for the case D = 3 and M = 2 in Figure 12.20. Such a network effectively perfonns a nonlinear principal component analysis. It has the advantage of not being limited to linear transformations, although it contains standard principal component analysis as a special case. However, training the network now involves a nonlinear optimization problem, since the error function (12.91) is no longer a quadratic function of the network parameters", "6fc6f7a2-9365-4681-9d06-04c76bab0216": "Kapur, J. Maximum entropy methods in science and engineering. Wiley. Karush, W. Minima of functions of several variables with inequalities as side constraints. Master\u2019s thesis, Department of Mathematics, University of Chicago. Kass, R. E. and A. E. Raftery . Bayes factors. Journal of the American Statistical Association 90, 377\u2013395. Kindermann, R. and J. L. Snell . Markov Random Fields and Their Applications. American Mathematical Society. Kittler, J. and J. F\u00a8oglein . Contextual classi\ufb01cation of multispectral pixel data. Image and Vision Computing 2, 13\u201329. Kohonen, T. Self-organized formation of topologically correct feature maps. Biological Cybernetics 43, 59\u201369. Kolmogorov, V. and R. Zabih", "80e1ddba-fcf5-4d96-bd58-83b9b6e56570": "The similar idea of discriminator-based closeness measure was also explored in the likelihood-free inference literature . The discriminator/critic as the experience can be learned or adapted together with the target model training in an iterative way, as in Equation 6.2. We show below that the discriminator/critic-based experience, in combination of certain choices of the divergence function D, re-derives the generative adversarial learning  from a di\ufb00erent perspective than Section 5.2. Generative adversarial learning: The variational experience view.\n\nThe functional descent view of generative adversarial learning presented in Section 5.2 is based on the treatment that the experience is the given static data instances, and the various GAN algorithms are due to the di\ufb00erent choices of the divergence function. The extended view of SE in this section also allows an alternative viewpoint of the learning paradigm, that gives more \ufb02exibility in not only choosing the divergence function but also the experience function, leading to a richer set of GAN variants. In this viewpoint, we consider experience that is de\ufb01ned variationally as mentioned above. That is, the experience function f, as a measure of the goodness of a sample t, is not speci\ufb01ed a priori but rather de\ufb01ned through an optimization problem", "e9e09c69-8d1e-4c4f-a125-a7fa96bc947a": "However, from a Bayesian perspective it makes little sense to limit the number of parameters in the network according to the size of the training set. In a Bayesian neural network, the prior distribution over the parameter vector w, in conjunction with the network function f(x, w), produces a prior distribution over functions from y(x) where y is the vector of network outputs. Neal  has shown that, for a broad class of prior distributions over w, the distribution of functions generated by a neural network will tend to a Gaussian process in the limit M \u2192 \u221e. It should be noted, however, that in this limit the output variables of the neural network become independent. One of the great merits of neural networks is that the outputs share the hidden units and so they can \u2018borrow statistical strength\u2019 from each other, that is, the weights associated with each hidden unit are in\ufb02uenced by all of the output variables not just by one of them. This property is therefore lost in the Gaussian process limit.\n\nWe have seen that a Gaussian process is determined by its covariance (kernel) function", "242007a8-5302-43f4-98b3-583356592148": "O\u21b5-policy methods also have a variety of additional uses in applications. For example, they can often be applied to learn from data generated by a conventional non-learning controller, or from a human expert. O\u21b5-policy learning is also seen by some as key to learning multi-step predictive models of the world\u2019s dynamics . In this section we begin the study of o\u21b5-policy methods by considering the prediction problem, in which both target and behavior policies are \ufb01xed. That is, suppose we wish to estimate v\u21e1 or q\u21e1, but all we have are episodes following another policy b, where b 6= \u21e1. In this case, \u21e1 is the target policy, b is the behavior policy, and both policies are considered \ufb01xed and given. In order to use episodes from b to estimate values for \u21e1, we require that every action taken under \u21e1 is also taken, at least occasionally, under b. That is, we require that \u21e1(a|s) > 0 implies b(a|s) > 0. This is called the assumption of coverage. It follows from coverage that b must be stochastic in states where it is not identical to \u21e1", "757650be-62c2-467c-bc6e-7da27398c3dc": "In that case only n different configurations of the representation space are possible, carving n different regions in input space, as illustrated in figure 15.8. Such a symbolic representation is also called a one-hot representation, since it can be captured by a binary vector with n bits that are mutually exclusive (only one of them can be active).\n\nA symbolic representation is a specific example of the broader class of nondistributed representations, which are representations that may contain many entries but without significant meaningful separate control over each entry. The following examples of learning algorithms are based on nondistributed  representations:  e Clustering methods, including the k-means algorithm: each input point is assigned to exactly one cluster. e k-nearest neighbors algorithms: one or a few templates or prototype examples are associated with a given input. In the case of k > 1, multiple values describe each input, but they cannot be controlled separately from each other, so this does not qualify as a true distributed representation. 544  CHAPTER 15", "e4e0b4c9-baef-4527-b673-1248c2cc0a97": "There are sixteen possible subsets of these four units. We show all sixteen subnetworks that may be formed by dropping out different subsets of units from the original network. In this small example, a large proportion of the resulting networks have no input units or no path connecting the input to the output. This problem becomes insignificant for networks with wider layers, where the probability of dropping all possible paths from inputs to outputs becomes smaller. 256  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  different datasets by sampling from the training set with replacement, and then train model 7 on dataset 7. Dropout aims to approximate this process, but with an exponentially large number of neural networks. Specifically, to train with dropout, we use a minibatch-based learning algorithm that makes small steps, such as stochastic gradient descent. Each time we load an example into a minibatch, we randomly sample a different binary mask to apply to all the input and hidden units in the network. The mask for each unit is sampled independently from all the others.\n\nThe probability of sampling a mask value of one (causing a unit to be included) is a hyperparameter fixed before training begins", "3647a3dd-b01e-4475-96e2-cfb023c490bf": "Dopamine also can function di\u21b5erently in non-mammals. But no one doubts that dopamine is essential for reward-related processes in mammals, including humans. An early, traditional view is that dopamine neurons broadcast a reward signal to multiple brain regions implicated in learning and motivation. This view followed from a famous 1954 paper by James Olds and Peter Milner that described the e\u21b5ects of electrical stimulation on certain areas of a rat\u2019s brain. They found that electrical stimulation to particular regions acted as a very powerful reward in controlling the rat\u2019s behavior: \u201c...the control exercised over the animal\u2019s behavior by means of this reward is extreme, possibly exceeding that exercised by any other reward previously used in animal experimentation\u201d . Later research revealed that the sites at which stimulation was most e\u21b5ective in producing this rewarding e\u21b5ect excited dopamine pathways, either directly or indirectly, that ordinarily are excited by natural rewarding stimuli.\n\nE\u21b5ects similar to these were also observed with human subjects. These observations strongly suggested that dopamine neuron activity signals reward", "6cb7adf5-7176-42e1-9d7e-8ce5826c2707": "By making the weight of this self-loop gated (controlled by another hidden unit), the time scale of integration can be changed dynamically.\n\nIn this case, we mean that even for an LSTM with fixed parameters, the time scale of integration can change based on the input sequence, because the time constants are output by the model itself. The LSTM has been found extremely successful in many applications, such as unconstrained handwriting recognition , speech recognition , handwriting generation , machine translation , image captioning , and parsing . The LSTM block diagram is illustrated in figure 10.16. The corresponding forward propagation equations are given below, for a shallow recurrent network  404  https://www.deeplearningbook.org/contents/rnn.html    CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  output  self-loop  Figure 10.16: Block diagram of the LSTM recurrent network \u201ccell.\u201d Cells are connected recurrently to each other, replacing the usual hidden units of ordinary recurrent networks. An input feature is computed with a regular artificial neuron unit. Its value can be accumulated into the state if the sigmoidal input gate allows it", "f494ef80-7b55-4fed-a377-10a11b08860e": "All of the methods they used resulted in better  performance compared to the baseline models (Fig. 9). Amongst these non-linear augmentations tested, the best technique resulted in a  reduction from 5.4 to 3.8% error on CIFAR-10 and 23.6% to 19.7% on CIFAR-100. In like manner, Liang et al. used GANs to produce mixed images. They found that the inclusion of mixed images in the training data reduced training time and increased the diversity of GAN-samples. Takahashi and Matsubara  experiment   Shorten and Khoshgoftaar J Big Data  6:60   Linear Methods  Fig. 9 Non-linearly mixing images   a  co  Fig.\n\n10 Mixing images through random image cropping and patching   with another approach to mixing images that randomly crops images and concate- nates the croppings together to form new images as depicted below. The results of their technique, as well as SamplePairing and mixup augmentation, demonstrate the sometimes unreasonable effectiveness of big data with Deep Learning models (Fig. 10). An obvious disadvantage of this technique is that it makes little sense from a human perspective", "8901a027-7223-4555-bcbd-2561bf7f2fe6": ", zK)T is a binary K-dimensional variable having a single component equal to 1, with all other components equal to 0. We can then write the conditional distribution of x, given the latent variable, as while the prior distribution for the latent variables is the same as for the mixture of Gaussians model, so that In order to derive the EM algorithm, we \ufb01rst write down the complete-data log likelihood function, which is given by where X = {xn} and Z = {zn}. Next we take the expectation of the complete-data log likelihood with respect to the posterior distribution of the latent variables to give where \u03b3(znk) = E is the posterior probability, or responsibility, of component k given data point xn. In the E step, these responsibilities are evaluated using Bayes\u2019 theorem, which takes the form If we consider the sum over n in (9.55), we see that the responsibilities enter only through two terms, which can be written as where Nk is the effective number of data points associated with component k. In the M step, we maximize the expected complete-data log likelihood with respect to the parameters \u00b5k and \u03c0", "5adfa89e-8380-4b82-a3d3-1e4667a451e9": "Working with Supervised Datasets  CLIP  CLIP  jointly trains a text encoder  and an image feature extractor over the pretraining task that predicts which caption goes with which image.\n\n(1) Contrastive pre-training (2) Create dataset classifier from label text \u2014=) : aussie pup > irae ~ A photo of Text i TIinl ol] - |W \u2014 _ > hot IpT) | Ty | |\" Ty {3) Use for zero-shot prediction sal IgT, | ty Ty | ly Ty by ty | hi) ty - Ty a - ee By jen | bt | bT btw h |) wt |i ws | ITs > iy WT Wh wTs) \u2014 [IvTw. Given a batch of NV (image, text) pairs, CLIP computes the dense cosine similarity matrix between all NV x N possible (image, text) candidates within this batch", "26d0129f-17c7-44f1-94fb-700f96c686bc": "Image classification of melanoma, nevus and seborrheic keratosis by deep neural network ensemble. In: International skin imaging collaboration (ISIC) 2017 challenge at the interna- tional symposium on biomedical imaging (SBI). 2017. Max J, Karen S, Andrea V, Andrew Z. Synthetic data and artificial neural networks for natural scene text recognition. arXiv preprint. 2014. Florian S, Dmitry K, James P. FaceNet: a unified embedding for face recognition and clustering. In: CVPR'15. 2015. Xudong M, Qing L, Haoran X, Raymond YKL, Zhen W, Stephen PS. Least squares generative adversarial networks. In: International conference on computer vision (ICCV), 2017. Ren W, Shengen Y, Yi S, Qingqing D, Gang S. Deep image: scaling up image recognition. CoRR, abs/1501.02876, 2015. Chao D, Chen CL, Kaiming H, Ziaoou T", "3dba340b-8eca-4556-87c0-249a45498d92": "Arti\ufb01cial neural networks and deep learning (Section 9.6) are not the only, or necessarily the best, way to do this. In this tic-tac-toe example, learning started with no prior knowledge beyond the rules of the game, but reinforcement learning by no means entails a tabula rasa view of learning and intelligence. On the contrary, prior information can be incorporated into reinforcement learning in a variety of ways that can be critical for e\ufb03cient learning (e.g., see Sections 9.5, 17.4, and 13.1). We also have access to the true state in the tic-tac-toe example, whereas reinforcement learning can also be applied when part of the state is hidden, or when di\u21b5erent states appear to the learner to be the same. Finally, the tic-tac-toe player was able to look ahead and know the states that would result from each of its possible moves.\n\nTo do this, it had to have a model of the game that allowed it to foresee how its environment would change in response to moves that it might never make. Many problems are like this, but in others even a short-term model of the e\u21b5ects of actions is lacking", "15ec3283-4b33-40f6-95a7-e8d0ac4b6aa6": "Provided p(X, Z|\u03b8) is a continuous function of \u03b8 then, by continuity, any local maximum of L(q, \u03b8) will also be a local maximum of ln p(X|\u03b8). Consider the case of N independent data points x1, . , xN with corresponding latent variables z1, . , zN. The joint distribution p(X, Z|\u03b8) factorizes over the data points, and this structure can be exploited in an incremental form of EM in which at each EM cycle only one data point is processed at a time. In the E step, instead of recomputing the responsibilities for all of the data points, we just re-evaluate the responsibilities for one data point.\n\nIt might appear that the subsequent M step would require computation involving the responsibilities for all of the data points. However, if the mixture components are members of the exponential family, then the responsibilities enter only through simple suf\ufb01cient statistics, and these can be updated ef\ufb01ciently", "8cac3e54-1ce7-4598-88c1-121d07b08f80": "Our greedy, \"-greedy, and UCB (Section 2.7) action-selection methods are not unlike heuristic search, albeit on a smaller scale. For example, to compute the greedy action given a model and a state-value function, we must look ahead from each possible action to each possible next state, take into account the rewards and estimated values, and then pick the best action. Just as in conventional heuristic search, this process computes backed-up values of the possible actions, but does not attempt to save them. Thus, heuristic search can be viewed as an extension of the idea of a greedy policy beyond a single step. The point of searching deeper than one step is to obtain better action selections. If one has a perfect model and an imperfect action-value function, then in fact deeper search will usually yield better policies.2 Certainly, if the search is all the way to the end of the episode, then the e\u21b5ect of the imperfect value function is eliminated, and the action determined in this way must be optimal", "e555d939-3c50-419a-a840-a001fa83d0e8": "However, in a Bayesian treatment we need to marginalize over the distribution of parameters in order to make predictions. In Section 3.3, we developed a Bayesian solution for a simple linear regression model under the assumption of Gaussian noise. We saw that the posterior distribution, which is Gaussian, could be evaluated exactly and that the predictive distribution could also be found in closed form. In the case of a multilayered network, the highly nonlinear dependence of the network function on the parameter values means that an exact Bayesian treatment can no longer be found. In fact, the log of the posterior distribution will be nonconvex, corresponding to the multiple local minima in the error function. The technique of variational inference, to be discussed in Chapter 10, has been applied to Bayesian neural networks using a factorized Gaussian approximation to the posterior distribution  and also using a fullcovariance Gaussian .\n\nThe most complete treatment, however, has been based on the Laplace approximation  and forms the basis for the discussion given here. We will approximate the posterior distribution by a Gaussian, centred at a mode of the true posterior", "6add811e-1cba-4b43-8e33-637907f01681": "In all of these algorithms, updates are delayed by n steps and only take into account the \ufb01rst n rewards, but now all the k-step returns are included for 1 \uf8ff k \uf8ff n (whereas the earlier n-step algorithms used only the n-step return), weighted geometrically as in Figure 12.2.\n\nIn the state-value case, this family of algorithms is known as truncated TD(\u03bb), or TTD(\u03bb). The compound backup diagram, shown in Figure 12.7, is similar to that for TD(\u03bb) (Figure 12.1) except that the longest component update is at most n steps rather than always going all the way to the end of the episode. TTD(\u03bb) is de\ufb01ned by (cf. (9.15)): This algorithm can be implemented e\ufb03ciently so that per-step computation does not scale with n (though of course memory must). Much as in n-step TD methods, no updates are made on the \ufb01rst n \u2212 1 time steps of each episode, and n \u2212 1 additional updates are made upon termination", "5454732b-4a9f-417c-99e3-6cb808143023": "If the clipping parameter is large, then it can take a long time for any weights to reach their limit, thereby making it harder to train the critic till optimality. If the clipping is small, this can easily lead to vanishing gradients when the number of layers is big, or batch normalization is not used (such as in RNNs). We experimented with simple variants (such as projecting the weights to a sphere) with little di\ufb00erence, and we stuck with weight clipping due to its simplicity and already good performance. However, we do leave the topic of enforcing Lipschitz constraints in a neural network setting for further investigation, and we actively encourage interested researchers to improve on this method. Algorithm 1 WGAN, our proposed algorithm. All experiments in the paper used the default values \u03b1 = 0.00005, c = 0.01, m = 64, ncritic = 5. Require: : \u03b1, the learning rate. c, the clipping parameter.\n\nm, the batch size. ncritic, the number of iterations of the critic per generator iteration. Require: : w0, initial critic parameters", "4c01cda2-cd84-4b3f-981d-d94c57b7d5b2": "Other examples of kernel substitution include nearest-neighbour classi\ufb01ers and the kernel Fisher discriminant . There are numerous forms of kernel functions in common use, and we shall encounter several examples in this chapter. Many have the property of being a function only of the difference between the arguments, so that k(x, x\u2032) = k(x \u2212 x\u2032), which are known as stationary kernels because they are invariant to translations in input space.\n\nA further specialization involves homogeneous kernels, also known as radial basis functions, which depend only on the magnitude of the distance (typically Section 6.3 Euclidean) between the arguments so that k(x, x\u2032) = k(\u2225x \u2212 x\u2032\u2225). For recent textbooks on kernel methods, see Sch\u00a8olkopf and Smola , Herbrich , and Shawe-Taylor and Cristianini . Many linear models for regression and classi\ufb01cation can be reformulated in terms of a dual representation in which the kernel function arises naturally. This concept will play an important role when we consider support vector machines in the next chapter", "bc1d86b1-202e-42d3-8a10-b3fe468aebc2": "RegNet models are ConvNets capable of scaling to billions or potentially even trillions of parameters, and can be optimized to fit different runtime and memory limitations. But we do not know how to efficiently represent uncertainty when we predict missing frames in a video or missing patches in an image. We cannot list all possible video frames and associate a score to each of them, because there is an infinite number of them. While this problem has limited the performance improvement from SSL in vision, new techniques SSL techniques such as SwAV are starting to beat accuracy records in vision tasks.\n\nThis is best demonstrated by the SEER system that uses a large convolutional network trained with billions of examples. Modeling the uncertainty in prediction  https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/  Self-supervised learning: The dark matter of intelligence   To better understand this challenge, we first need to understand the prediction uncertainty and the way it\u2019s modeled in NLP compared with CV. In NLP, predicting the missing words involves computing a prediction score for every possible word in the vocabulary", "d3e47a72-d432-4e11-bc07-b96a58e4688c": "This is most easily done using the 1-of-K coding scheme in which the target vector tn for a feature vector \u03c6n belonging to class Ck is a binary vector with all elements zero except for element k, which equals one. The likelihood function is then given by where ynk = yk(\u03c6n), and T is an N \u00d7 K matrix of target variables with elements tnk. Taking the negative logarithm then gives E(w1, . , wK) = \u2212 ln p(T|w1, . , wK) = \u2212 which is known as the cross-entropy error function for the multiclass classi\ufb01cation problem. We now take the gradient of the error function with respect to one of the parameter vectors wj. Making use of the result (4.106) for the derivatives of the softmax function, we obtain Exercise 4.18 k tnk = 1", "7ae729f2-deec-4f61-87a1-40f21982437c": "For instance, the multilayer perceptron architecture is sometimes called a backpropagation network. The term backpropagation is also used to describe the training of a multilayer perceptron using gradient descent applied to a sum-of-squares error function. In order to clarify the terminology, it is useful to consider the nature of the training process more carefully. Most training algorithms involve an iterative procedure for minimization of an error function, with adjustments to the weights being made in a sequence of steps.\n\nAt each such step, we can distinguish between two distinct stages. In the \ufb01rst stage, the derivatives of the error function with respect to the weights must be evaluated. As we shall see, the important contribution of the backpropagation technique is in providing a computationally ef\ufb01cient method for evaluating such derivatives. Because it is at this stage that errors are propagated backwards through the network, we shall use the term backpropagation speci\ufb01cally to describe the evaluation of derivatives. In the second stage, the derivatives are then used to compute the adjustments to be made to the weights. The simplest such technique, and the one originally considered by Rumelhart et al. , involves gradient descent", "1837ab3d-1447-4b5e-b2ce-bbb91ee071e3": "en GA MIEe Ane  https://www.deeplearningbook.org/contents/mlp.html    LECLIULIGQUES LU LIAHIUILY ALLIUNClalL UCULAL WCUWULKS. Le 1Ued Wad Lillally USVeELUpecu in practice after being independently rediscovered in different ways . The book Parallel Distributed Pro* cessing presented the results of some of the first successful experiments with  back-propagation in a chapter  that contributed greatly to the popularization of back-propagation and initiated a very active period of re- search in multilayer neural networks. The ideas put forward by the authors of that book, particularly by Rumelhart and Hinton, go much beyond back-propagation.\n\nThey include crucial ideas about the possible computational implementation of several central aspects of cognition and learning, which came under the name \u201cconnectionism\u201d because of the importance this school of thought places on the connections between neurons as the locus of learning and memory. In particular, these ideas include the notion of distributed representation . Following the success of back-propagation, neural network research gained pop- ularity and reached a peak in the early 1990s", "c643141f-59c0-4053-a619-8e266d8824a0": "For a single representative state s, let its two numbers be s1 2 R and s2 2 R. You might choose to represent s simply by its two state dimensions, so that x(s) = (s1, s2)>, but then you would not be able to take into account any interactions between these dimensions. In addition, if both s1 and s2 were zero, then the approximate value would have to also be zero. Both limitations can be overcome by instead representing s by the four-dimensional feature vector x(s) = (1, s1, s2, s1s2)>. The initial 1 feature allows the representation of a\ufb03ne functions in the original state numbers, and the \ufb01nal product feature, s1s2, enables interactions to be taken into account. Or you might choose to use higher-dimensional feature vectors like x(s) = (1, s1, s2, s1s2, s2 take more complex interactions into account", "bb69066a-05cb-47d3-9d62-656c9cce1ded": "Williams  has given explicit forms for the covariance in the case of two speci\ufb01c choices for the hidden unit activation function (probit and Gaussian). These kernel functions k(x, x\u2032) are nonstationary, i.e. they cannot be expressed as a function of the difference x \u2212 x\u2032, as a consequence of the Gaussian weight prior being centred on zero which breaks translation invariance in weight space. By working directly with the covariance function we have implicitly marginalized over the distribution of weights. If the weight prior is governed by hyperparameters, then their values will determine the length scales of the distribution over functions, as can be understood by studying the examples in Figure 5.11 for the case of a \ufb01nite number of hidden units. Note that we cannot marginalize out the hyperparameters analytically, and must instead resort to techniques of the kind discussed in Section 6.4. Exercises 6.1 (\u22c6 \u22c6) www Consider the dual formulation of the least squares linear regression problem given in Section 6.1", "b23a5565-9f18-4d16-ad3e-b706b9155365": "With just one tiling, we would not have coarse coding but just a case of state aggregation.\n\nTo get the strengths of coarse coding requires overlapping receptive \ufb01elds, and by de\ufb01nition the tiles of a partition do not overlap. To get true coarse coding with tile coding, multiple tilings are used, each o\u21b5set by a fraction of a tile width. A simple case with four tilings is shown on the right side of Figure 9.9. Every state, such as that indicated by the white spot, falls in exactly one tile in each of the four tilings. These four tiles correspond to four features that become active when the state occurs. Speci\ufb01cally, the feature vector x(s) has one component for each tile in each tiling. In this example there are 4 \u21e5 4 \u21e5 4 = 64 components, all of which will be 0 except for the four corresponding to the tiles that s falls within. Figure 9.10 shows the advantage of multiple o\u21b5set tilings (coarse coding) over a single tiling on the 1000-state random walk example", "fc7356da-b679-461c-9f10-56c366243e82": "A Gwth a Anarene nn AE Aa lae nee bee Ln een ee nn tn tieen Af dL 282 ete-  https://www.deeplearningbook.org/contents/monte_carlo.html    JA UIE SCYUCLLES UL Salllples lay LUUS HUOL De VELY LEPLESeLllLallve UL LLC CYULLULIULIL distribution. One way to mitigate this problem is to return only every \u201d successive samples, so that our estimate of the statistics of the equilibrium distribution is not as biased by the correlation between an MCMC sample and the next several samples. Markov chains are thus expensive to use because of the time required to burn in to the equilibrium distribution and the time required to transition from one sample to another reasonably decorrelated sample after reaching equilibrium. If one desires truly independent samples, one can run multiple Markov chains in parallel", "ef571077-8134-49c1-bd8d-919ad78cb1e5": "Craik, K. J. W. The Nature of Explanation. Cambridge University Press, Cambridge. Cross, J. G. A stochastic learning model of economic behavior. The Quarterly Journal Crow, T. J. Cortical synapses and reinforcement: a hypothesis. Nature, 219:736\u2013 Curtiss, J. H. A theoretical comparison of the e\ufb03ciencies of two classical methods and a Monte Carlo method for computing one component of the solution of a set of linear algebraic equations. In H. A. Meyer (Ed. ), Symposium on Monte Carlo Methods, pp. 191\u2013233. Wiley, New York. Cybenko, G. Approximation by superpositions of a sigmoidal function. Mathematics of Dabney, W. Adaptive step-sizes for reinforcement learning. PhD thesis, University of Dabney, W., Barto, A. G", "ee7cb0f2-2e6f-47e2-81cf-f3508f25a40b": "f is said to be convex on I if \u2200x1, x2 \u2208 I, \u03bb \u2208 , f(\u03bbx1 + (1 \u2212 \u03bb)x2) \u2264 \u03bbf(x1) + (1 \u2212 \u03bb)f(x2). f is said to be strictly convex if the inequality is strict. Intuitively, this de\ufb01nition states that the function falls below (strictly convex) or is never above (convex) the straight line (the secant) from points (x1, f(x1)) to (x2, f(x2)). See Figure (1).\n\nDe\ufb01nition 2 f is concave (strictly concave) if \u2212f is convex (strictly convex). Theorem 1 If f(x) is twice di\ufb00erentiable on  and f \u2032\u2032(x) \u2265 0 on  then f(x) is convex on . Proof: For x \u2264 y \u2208  and \u03bb \u2208  let z = \u03bby+(1\u2212\u03bb)x", "7a750a54-ac4c-4595-a6c2-89ea64357ff8": "J Mach Learn Res. 2008;9:2431-56. ._ Jeff D, Philipp K, Trevor D. Adversarial feature learning. In: CVPR'16. 2016. Lin Z, Shi Y, Xue Z. IDSGAN: Generative Adversarial Networks for Attack Generation against Intrusion Detection. arXiv preprint; 2018.\n\nWilliam F, Mihaela R, Balaji L, Andrew MD, Shakir M, lan G. Many paths to equilibrium: GANs do not need to decrease a divergence at every step. In: International conference on learning representations (ICLR); 2017. Alec R, Luke M, Soumith C. Unsupervised representation learning with deep convolutional generative adversarial networks. ICLR, 2016. Jun-Yan Z, Taesung P, Phillip |, Alexei AE. Unpaired image-to-image translation using cycle-consistent adversarial networks. In: International conference on cmoputer vision (ICCV), 2017", "f003c9d4-3733-4246-94bf-ac1ddf2d0c04": ".\n\nAdjustment of an inverse matrix corresponding to changes in the elements of a given column or a given row of the original matrix (abstract). Annals of Mathematical Statistics, 20(4):621. dimensionality. In Proceedings of the Fifth IEEE International Symposium on Intelligent Control, pp. 383\u2013388. IEEE Computer Society Press, Los Alamitos, CA. bacterial chemotaxis. Biological Cybernetics, 101(5-6):379\u2013385. Si, J., Barto, A., Powell, W., Wunsch, D. (Eds.) . Handbook of Learning and Approximate Silver, D. Reinforcement Learning and Simulation Based Search in the Game of Go. Silver, D., Huang, A., Maddison, C", "2be373d6-347a-4eae-b63e-eb7f2600ebca": "We also examine the impact of different types of labeling functions on end predictive performance, using the CDR application as a representative example of three common categories of labeling functions: \u2022 Text Patterns Basic word, phrase, and regular expression labeling functions.\n\nDistant Supervision External knowledge bases mapped to candidates, either directly or \ufb01ltered by a heuristic. \u2022 Structure-Based Labeling functions expressing heuristics over the context hierarchy, e.g., reasoning about position in the document or relative to other candidates. We show an ablation in Table 7, sorting by stand-alone score. We see that distant supervision adds recall at the cost of some precision, as we would expect, but ultimately improves F1 score by 2 points; and that structure-based labeling functions, enabled by Snorkel\u2019s context hierarchy data representation, add an additional F1 point. We conducted a formal study of Snorkel to (i) evaluate how quickly subject matter expert (SME) users could learn to write labeling functions, and (ii) empirically validate the core hypothesis that writing labeling functions is more timeef\ufb01cient than hand-labeling data. Users were given instruction on Snorkel, and then asked to write labeling functions for the Spouses task described in the previous subsection", "f4c0bcf7-2292-4439-ad7b-eaf51b4a127b": "AUTOENCODERS  contractive penalty to f(a) rather than to g(f(a)). A contractive penalty on f(x) also has close connections to score matching, as discussed in section 14.5.1. The name contractive arises from the way that the CAE warps space. Specifi- cally, because the CAE is trained to resist perturbations of its input, it is encouraged to map a neighborhood of input points to a smaller neighborhood of output points. We can think of this as contracting the input neighborhood to a smaller output neighborhood. To clarify, the CAE is contractive only locally\u2014all perturbations of a training point 2 are mapped near to f(a). Globally, two different points x and a\u2019 may be mapped to f(a) and f(x\u2019) points that are farther apart than the original points", "ca797c28-9086-450a-921b-f0f4746cb721": "Recall that in the boxes of fruit example, the observation of the identity of the fruit provided relevant information that altered the probability that the chosen box was the red one. In that example, Bayes\u2019 theorem was used to convert a prior probability into a posterior probability by incorporating the evidence provided by the observed data.\n\nAs we shall see in detail later, we can adopt a similar approach when making inferences about quantities such as the parameters w in the polynomial curve \ufb01tting example. We capture our assumptions about w, before observing the data, in the form of a prior probability distribution p(w). The effect of the observed data D = {t1, . , tN} is expressed through the conditional probability p(D|w), and we shall see later, in Section 1.2.5, how this can be represented explicitly. Bayes\u2019 theorem, which takes the form then allows us to evaluate the uncertainty in w after we have observed D in the form of the posterior probability p(w|D)", "c281e902-1cf2-4ff8-a061-581218876ead": "4.11 (\u22c6 \u22c6) Consider a classi\ufb01cation problem with K classes for which the feature vector \u03c6 has M components each of which can take L discrete states. Let the values of the components be represented by a 1-of-L binary coding scheme. Further suppose that, conditioned on the class Ck, the M components of \u03c6 are independent, so that the class-conditional density factorizes with respect to the feature vector components. Show that the quantities ak given by (4.63), which appear in the argument to the softmax function describing the posterior class probabilities, are linear functions of the components of \u03c6. Note that this represents an example of the naive Bayes model which is discussed in Section 8.2.2. 4.13 (\u22c6) www By making use of the result (4.88) for the derivative of the logistic sigmoid, show that the derivative of the error function (4.90) for the logistic regression model is given by (4.91).\n\n4.14 (\u22c6) Show that for a linearly separable data set, the maximum likelihood solution for the logistic regression model is obtained by \ufb01nding a vector w whose decision boundary wT\u03c6(x) = 0 separates the classes and then taking the magnitude of w to in\ufb01nity", "dff2cac8-507f-4c1f-852a-f63caf578209": "We can either run finite differencing mn times to evaluate all che partial derivatives of g, or apply the test to a new function that uses random projections at both the input and the output of g. For example, we can apply our test of the implementation of the derivatives to f(x), where f(z) = u\" g(vz), and wu and v are randomly chosen vectors. Computing f\u2019(x) correctly requires being able to back-propagate through g correctly yet is efficient to do with finite differences because f has only a single input and a single output. It is usually a good idea to repeat this test for more than one value of u and v to reduce he chance of the test overlooking mistakes that are orthogonal to the random projection.\n\nIf one has access to numerical computation on complex numbers, then there is a very efficient way to numerically estimate the gradient by using complex numbers as input to the function", "95f03722-5eef-481d-80c1-f69255b1b0d3": "As shown in the \ufb01gure, this value function is generally di\u21b5erent from those minimizing VE or BE. Methods that are guaranteed to converge to it are discussed in Sections 11.7 and 11.8. Armed with a better understanding of value function approximation and its various objectives, we return now to the challenge of stability in o\u21b5-policy learning.\n\nWe would like to apply the approach of stochastic gradient descent (SGD, Section 9.3), in which updates are made that in expectation are equal to the negative gradient of an objective function. These methods always go downhill (in expectation) in the objective and because of this are typically stable with excellent convergence properties. Among the algorithms investigated so far in this book, only the Monte Carlo methods are true SGD methods. These methods converge robustly under both on-policy and o\u21b5-policy training as well as for general nonlinear (di\u21b5erentiable) function approximators, though they are often slower than semi-gradient methods with bootstrapping, which are not SGD methods. Semi-gradient methods may diverge under o\u21b5-policy training, as we have seen earlier in this chapter, and under contrived cases of nonlinear function approximation", "9615174d-8d4b-4e6f-aac9-8ad9d3d9e22e": "Maxwell I. Nye, Armando Solar-Lezama, Joshua B. Tenenbaum, and Brenden M. Lake. 2020. Learning compositional rules via neural program synthesis. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2227\u20132237. Ngoc-Quan Pham, Jan Niehues, Thanh-Le Ha, and Alexander Waibel. 2019. Improving zero-shot translation with language-independent constraints. In Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers), pages 13\u2013 23, Florence, Italy. Association for Computational Linguistics. Aaditya Prakash, Sadid A", "1fca3826-0180-45cb-829c-c30d60f3e240": "Maximization with respect to \u00b50 and V0 is easily performed by making use of the maximum likelihood solution for a Gaussian distribution discussed in Section 2.3.4, giving Exercise 13.32 Similarly, to optimize A and \u0393, we substitute for p(zn|zn\u22121, A, \u0393) in (13.108) using (13.75) giving in which the constant comprises terms that are independent of A and \u0393. Maximizing with respect to these parameters then gives Exercise 13.33 Note that Anew must be evaluated \ufb01rst, and the result can then be used to determine \u0393new. Finally, in order to determine the new values of C and \u03a3, we substitute for p(xn|zn, C, \u03a3) in (13.108) using (13.76) giving We have approached parameter learning in the linear dynamical system using maximum likelihood. Inclusion of priors to give a MAP estimate is straightforward, and a fully Bayesian treatment can be found by applying the analytical approximation techniques discussed in Chapter 10, though a detailed treatment is precluded here due to lack of space.\n\nAs with the hidden Markov model, there is considerable interest in extending the basic linear dynamical system in order to increase its capabilities", "095a1d72-1e18-487c-a60f-148a59d9a509": "A key observation is that the summation over the latent variables appears inside the logarithm. Even if the joint distribution p(X, Z|\u03b8) belongs to the exponential family, the marginal distribution p(X|\u03b8) typically does not as a result of this summation. The presence of the sum prevents the logarithm from acting directly on the joint distribution, resulting in complicated expressions for the maximum likelihood solution. Now suppose that, for each observation in X, we were told the corresponding value of the latent variable Z. We shall call {X, Z} the complete data set, and we shall refer to the actual observed data X as incomplete, as illustrated in Figure 9.5.\n\nThe likelihood function for the complete data set simply takes the form ln p(X, Z|\u03b8), and we shall suppose that maximization of this complete-data log likelihood function is straightforward. In practice, however, we are not given the complete data set {X, Z}, but only the incomplete data X. Our state of knowledge of the values of the latent variables in Z is given only by the posterior distribution p(Z|X, \u03b8)", "a3f7e0c0-2b37-4f80-89e2-32dca47c6722": "Very inter- estingly as well, the policies learned on the ImageNet dataset were successful when transferred to the Stanford Cars and FGVC Aircraft image recognition tasks. In this case, the ImageNet policy applied to these other datasets reduced error rates by 1.16% and 1.76% respectively. Geng et al. expanded on AutoAugment by replacing the Reinforcement Learning search algorithm with Augmented Random Search (ARS) . The authors point out that the sub-policies learned from AutoAugment are inherently flawed because of the discrete search space. They convert the probability and magnitude of augmentations into a continuous space and search for sub-policies with ARS.\n\nWith this, they achieve lower error rates on CIFAR-10, CIFAR-100, and ImageNet (Table 9). Minh et al. also experimented with using Reinforcement Learning  to search for Data Augmentations. They further explore the effectiveness of learning trans- formations for individual instances rather than the entire dataset", "ce7aa649-f274-42e2-a3c4-ea933f3be7fb": "This is vividly apparent in the TD-Gammon results.\n\nTD-Gammon 0.0, whose network input was essentially a \u201craw\u201d representation of the backgammon board, meaning that it involved very little knowledge of backgammon, learned to play approximately as well as the best previous backgammon computer programs. Adding specialized backgammon features produced TD-Gammon 1.0 which was substantially better than all previous backgammon programs and competed well against human experts. Mnih et al. developed a reinforcement learning agent called deep Q-network (DQN) that combined Q-learning with a deep convolutional ANN, a many-layered, or deep, ANN specialized for processing spatial arrays of data such as images. We describe deep convolutional ANNs in Section 9.6. By the time of Mnih et al.\u2019s work with DQN, deep ANNs, including deep convolutional ANNs, had produced impressive results in many applications, but they had not been widely used in reinforcement learning. Mnih et al", "1499f7f2-9744-4856-a1ef-bb3f4c183c37": "), Advances in Neural Information Processing Systems, Volume 15, pp. 455\u2013 462. MIT Press. Williams, C. K. I. Prediction with Gaussian processes: from linear regression to linear prediction and beyond. In M. I. Jordan (Ed. ), Learning in Graphical Models, pp. 599\u2013621. MIT Press. Williams, C. K. I. and D. Barber . Bayesian classi\ufb01cation with Gaussian processes. IEEE Transactions on Pattern Analysis and Machine Intelligence 20, 1342\u20131351.\n\nWilliams, C. K. I. and M. Seeger . Using the Nystrom method to speed up kernel machines. In T. K. Leen, T. G. Dietterich, and V. Tresp (Eds. ), Advances in Neural Information Processing Systems, Volume 13, pp. 682\u2013688", "d0cf7a13-f993-46b6-b618-701b713e1e88": "The curious case of neural text degeneration. In International Conference on Learning Representations. Zhiting Hu, Haoran Shi, Bowen Tan, Wentao Wang, Zichao Yang, Tiancheng Zhao, Junxian He, Lianhui Qin, Di Wang, Xuezhe Ma, et al. 2019. Texar: A modularized, versatile, and extensible toolkit for text generation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 159\u2013164. Zhiting Hu, Zichao Yang, Xiaodan Liang, R. Salakhutdinov, and E. Xing. 2017. Toward controlled generation of text. In International Conference on Machine Learning (ICML). Natasha Jaques, Shixiang Gu, Dzmitry Bahdanau, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Richard E Turner, and Douglas Eck. 2017. Sequence tutor: Conservative \ufb01ne-tuning of sequence generation models with kl-control", "fa4d7e27-2f90-42f4-93fc-f6b1162beec1": "In contrast to basic MCTS, which expands its current search tree by using stored action values to select an unexplored edge from a leaf node, APV-MCTS, as implemented in AlphaGo, expanded its tree by choosing an edge according to probabilities supplied by a 13-layer deep convolutional ANN, called the SL-policy network, trained previously by supervised learning to predict moves contained in a database of nearly 30 million human expert moves. Then, also in contrast to basic MCTS, which evaluates the newly-added state node solely by the return of a rollout initiated from it, APV-MCTS evaluated the node in two ways: by this return of the rollout, but also by a value function, v\u2713, learned previously by a reinforcement learning method.\n\nIf s was the newly-added node, its value became where G was the return of the rollout and \u2318 controlled the mixing of the values resulting from these two evaluation methods. In AlphaGo, these values were supplied by the value network, another 13-layer deep convolutional ANN that was trained as we describe below to output estimated values of board positions", "dae49f1f-a79d-4104-a66a-9b01455c5a03": "The Tsetlin collection also includes studies of learning automata in team and game problems, which led to later work in this area using stochastic learning automata as described by Narendra and Thathachar , Viswanathan and Narendra , Lakshmivarahan and Narendra , Narendra and Wheeler , and Thathachar and Sastry . Thathachar and Sastry  is a more recent comprehensive account.\n\nThese studies were mostly restricted to non-associative learning automata, meaning that they did not address associative, or contextual, bandit problems (Section 2.9). The second phase began with the extension of learning automata to the associative, or contextual, case. Barto, Sutton, and Brouwer  and Barto and Sutton  experimented with associative stochastic learning automata in singlelayer ANNs to which a global reinforcement signal was broadcast. The learning algorithm was an associative extension of the Alopex algorithm of Harth and Tzanakou . Barto et al. called neuron-like elements implementing this kind of learning associative search elements (ASEs)", "468328f0-aff9-478a-b4ef-9246a3eeec54": "PROBABILITY AND INFORMATION THEORY  With probability 3, we choose the value of s to be 1. Otherwise, we choose the value of s to be \u20141. We can then generate a random variable y by assigning y = sx. Clearly, x and y are not independent, because x completely determines the magnitude of y. However, Cov(2,y) = 0. The covariance matrix of a random vector x \u20ac R\u201d is an n x n matrix, such that  Cov(x);,; = Cov(x;, x;). (3.14)  https://www.deeplearningbook.org/contents/prob.html    The diagonal elements of the covariance give the variance:  Cov(xi, xi) = Var(x\u00e9). (3.15)  3.9 Common Probability Distributions  Several simple probability distributions are useful in many contexts in machine learning. 3.9.1 Bernoulli Distribution  The Bernoulli distribution is a distribution over a single binary random variable. It is controlled by a single parameter \u00a2 \u20ac , which gives the probability of the random variable being equal to 1", "b0e708f0-e466-4a34-b077-9e1bfd09b8d3": "The DCGAN  architecture was proposed to expand on the internal complex- ity of the generator and discriminator networks. This architecture uses CNNs for the generator and discriminator networks rather than multilayer perceptrons. The DCGAN was tested to generate results on the LSUN interior bedroom image data- set, each image being 64 x 64 x 3, for a total of 12,288 pixels, (compared to 784 in MNIST).\n\nThe idea behind DCGAN is to increase the complexity of the generator network to project the input into a high dimensional tensor and then add decon- volutional layers to go from the projected tensor to an output image. These decon- volutional layers will expand on the spatial dimensions, for example, going from 14x 14x 6 to 28 x 28x 1, whereas a convolutional layer will decrease the spatial dimensions such as going from 14 x 14 x 32 to 7 x 7 x 64. The DCGAN architecture presents a strategy for using convolutional layers in the GAN framework to produce higher resolution images (Figs. 17, 18). Stride 2 16  CONV 2  Fig. 17 DCGAN, generator architecture presented by Radford et al", "f1f27166-6638-4688-ba97-c95f488dccef": "We therefore turn to the expectation maximization algorithm to \ufb01nd an ef\ufb01cient framework for maximizing the likelihood function in hidden Markov models. The EM algorithm starts with some initial selection for the model parameters, which we denote by \u03b8old. In the E step, we take these parameter values and \ufb01nd the posterior distribution of the latent variables p(Z|X, \u03b8old).\n\nWe then use this posterior distribution to evaluate the expectation of the logarithm of the complete-data likelihood function, as a function of the parameters \u03b8, to give the function Q(\u03b8, \u03b8old) de\ufb01ned by Q(\u03b8, \u03b8old) = \ufffd At this point, it is convenient to introduce some notation. We shall use \u03b3(zn) to denote the marginal posterior distribution of a latent variable zn, and \u03be(zn\u22121, zn) to denote the joint posterior distribution of two successive latent variables, so that For each value of n, we can store \u03b3(zn) using a set of K nonnegative numbers that sum to unity, and similarly we can store \u03be(zn\u22121, zn) using a K \u00d7 K matrix of nonnegative numbers that again sum to unity", "53407bb9-09a0-4ebe-888b-227421f81d4c": "When the force due to the gradient of the cost function is small but nonzero, the constant force due to friction can cause the particle to come to rest before reaching a local minimum. Viscous drag avoids both of these problems\u2014it is weak enough that the gradient can continue to cause motion until a minimum is reached, but strong enough to prevent motion if the gradient does not justify moving. 8.3.3 Nesterov Momentum  Sutskever ef al. introduced a variant of the momentum algorithm that was inspired by Nesterov\u2019s accelerated gradient method . The update rules in this case are given by  1i< . ve av\u2014\u20acVe F \u00a5 2(s10%:0-+00).4)| ; (8.21) i=l 0+ 6+, (8.22)  where the parameters qa and \u00ab play a similar role as in the standard momentum method. The difference between Nesterov momentum and standard momentum is where the gradient is evaluated. With Nesterov momentum, the gradient is evaluated after the current velocity is applied. Thus one can interpret Nesterov momentum as attempting to add a correction factor to the standard method of momentum", "f11d9f90-8c2e-40e1-b721-80b664cf804d": "Alternatively, multiple tasks (some supervised, some unsupervised) can be learned together with some shared internal representation. Most representation learning problems face a trade-off between preserving as much information about the input as possible and attaining nice properties (such as independence).\n\nRepresentation learning is particularly interesting because it provides one way to perform unsupervised and semi-supervised learning. We often have very large amounts of unlabeled training data and relatively little labeled training data. Training with supervised learning techniques on the labeled subset often results in severe overfitting. Semi-supervised learning offers the chance to resolve  ade eae oil 1 1 1 . c wd Prwrqvs fal \u2018eo  https://www.deeplearningbook.org/contents/representation.html    LIUS OVeFLLLINg proviem Dy also learning Lroml We ULabeled Gala. SpeccalLy, we can learn good representations for the unlabeled data, and then use these representations to solve the supervised learning task. Humans and animals are able to learn from very few labeled examples. We do  525  CHAPTER 15", "9127e429-bc6a-4b03-8628-c28f381804c0": "Most neural networks are organized into groups of units called layers. Most neural network architectures arrange these layers in a chain structure, with each  193  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  layer being a function of the layer that preceded it. In this structure, the first layer is given by  nO) =o) (WOT a +B): (6.40)  the second layer is given by a) = 9g (wer Ao + b\u00b0)) ; (6.41)  and so on. In these chain-based architectures, the main architectural considerations are choosing the depth of the network and the width of each layer. As we will see, a network with even one hidden layer is sufficient to fit the training set. Deeper networks are often able to use far fewer units per layer and far fewer parameters, as well as frequently generalizing to the test set, but they also tend to be harder to optimize.\n\nThe ideal network architecture for a task must be found via experimentation guided by monitoring the validation set error. 6.4.1 Universal Approximation Properties and Depth  A linear model, mapping from features to outputs via matrix multiplication, can by definition represent only linear functions", "89ea60ab-0694-4814-a592-b1e66272f4dd": "Assume that the di\u21b5erence between the action preferences is given by a weighted sum of the unit\u2019s input vector, that is, assume that h(s, 1, \u2713) \u2212 h(s, 0, \u2713) = \u2713>x(s), where \u2713 is the unit\u2019s weight vector.\n\n(a) Show that if the exponential soft-max distribution (13.2) is used to convert action preferences to policies, then Pt = \u21e1(1|St, \u2713t) = 1/(1 + exp(\u2212\u2713> (b) What is the Monte-Carlo REINFORCE update of \u2713t to \u2713t+1 upon receipt of return (c) Express the eligibility r ln \u21e1(a|s, \u2713) for a Bernoulli-logistic unit, in terms of a, x(s), Hint for part (c): De\ufb01ne P = \u21e1(1|s, \u2713) and compute the derivative of the logarithm, for each action, using the chain rule on P", "9682f71a-652b-491a-b28f-9ce19341e065": "If even one element of the vector is unusual, the system must assign it a low probability. e Denoising: Given a damaged or incorrectly observed input z, the machine learning system returns an estimate of the original or correct x. For example, the machine learning system might be asked to remove dust or scratches from an old photograph. This requires multiple outputs (every element of the estimated clean example x) and an understanding of the entire input (since even one damaged area will still reveal the final estimate as being damaged). e Missing value imputation: Given the observations of some elements of x the model is asked to return estimates of or a probability distribution over  \u2019  some or all of the unobserved elements of x.\n\nThis requires multiple outputs. Because the model could be asked to restore any of the elements of x, it must understand the entire input. e Sampling: The model generates new samples from the distribution p(a). Applications include speech synthesis, that is, producing new waveforms that sound like natural human speech. This requires multiple output values and a good model of the entire input. If the samples have even one element drawn from the wrong distribution, then the sampling process is wrong", "ece62976-fda2-4fed-93e4-97efc0423934": "8.6\u20137 Trajectory sampling has implicitly been a part of reinforcement learning from the outset, but it was most explicitly emphasized by Barto, Bradtke, and Singh  in their introduction of RTDP. They recognized that Korf\u2019s  learning real-time A* (LRTA*) algorithm is an asynchronous DP algorithm that applies to stochastic problems as well as the deterministic problems on which Korf focused. Beyond LRTA*, RTDP includes the option of updating the values of many states in the time intervals between the execution of actions. Barto et al. proved the convergence result described here by combining Korf\u2019s  convergence proof for LRTA* with the result of Bertsekas   ensuring convergence of asynchronous DP for stochastic shortest path problems in the undiscounted case. Combining model-learning with RTDP is called Adaptive RTDP, also presented by Barto et al. and discussed by Barto", "4418def3-f412-4bf1-8bff-0cccb7d0c0b3": "Bengio and Delalleau  show that CD can be interpreted as discarding the smallest terms of the correct MCMC update gradient, which explains the bias. CD is useful for training shallow models like RBMs. These can in turn be stacked to initialize deeper models like DBNs or DBMs. But CD does not provide  609  CHAPTER 18. CONFRONTING THE PARTITION FUNCTION  much help for training deeper models directly. This is because it is difficult to obtain samples of the hidden units given samples of the visible units. Since the hidden units are not included in the data, initializing from training points cannot solve the problem. Even if we initialize the visible units from the data, we will still need to burn in a Markov chain sampling from the distribution over the hidden units conditioned on those visible samples.\n\nThe CD algorithm can be thought of as penalizing the model for having a Markov chain that changes the input rapidly when the input comes from the data. This means training with CD somewhat resembles autoencoder training. Even though CD is more biased than some of the other training methods, it can be useful for pretraining shallow models that will later be stacked", "ac2361e1-4c05-42b2-b44d-13f029fc2f49": "____'c2=.~1. Principal Component Analysis Principal compooem analy,;\" or rcA. ;s a technique tha! is \"'idely u<ed for appli. cations such as dimensionality .-eduction, lossy data comprc\"ion, feature e>tracti\"\". and data v;,ualizatiOll (Jolliffe, 2(02). It;s also kno....\" as tile Karoan.n\u00b7I..,;\"\" tran,\u00b7 f~. lbcrc an: t....o commonly used definitions of PeA that giye rise to the >arne algorithm. PeA can be defined as the unhog<lnal projtttion of the data O/1tO a lo....er dimensionallincar space. kno....n as the pri/lcip.al $uh.\u2022p.aa. soch that the \\'ariance of the projttted data i' ma~imi,e<J", "80879c73-a4a6-43d4-9e21-41d16e92900e": "In fact, the step-size parameter on the average reward is a perfect place to use the unbiased constant-step-size trick from Exercise 2.7. Describe the speci\ufb01c changes needed to the boxed algorithm for di\u21b5erential semi-gradient n-step Sarsa to use this trick. \u21e4 In this chapter we have extended the ideas of parameterized function approximation and semi-gradient descent, introduced in the previous chapter, to control. The extension is immediate for the episodic case, but for the continuing case we have to introduce a whole new problem formulation based on maximizing the average reward setting per time step. Surprisingly, the discounted formulation cannot be carried over to control in the presence of approximations. In the approximate case most policies cannot be represented by a value function. The arbitrary policies that remain need to be ranked, and the scalar average reward r(\u21e1) provides an e\u21b5ective way to do this. The average reward formulation involves new di\u21b5erential versions of value functions, Bellman equations, and TD errors, but all of these parallel the old ones, and the conceptual changes are small.\n\nThere is also a new parallel set of di\u21b5erential algorithms for the average-reward case", "542c2715-8117-483c-8d87-b5a6ba1646ab": "In this case, the switch variable determines whether we draw x from the data or from the noise distribution. Formally, ptrain(y = 1) = 3 + Ptrain(x | y = 1) = Paata(x), and Ptrain (x | y= 0) = Pnoise (x). We can now just use standard maximum likelihood learning on the supervised learning problem of fitting pjoint tO Ptrain:  9,c=argmaxE  ,,,, log Pjoint Be xaynp (y | x). (18.32)  https://www.deeplearningbook.org/contents/partition.html    The distribution picint is essentially a logistic regression model applied to the difference in log probabilities of the model and the noise distribution:  Pmodel (x) (18.33) Pp model(X) +p noise(X)  1 -\u2014__, (18.34) 1+ Pmodel (x)  = ! (18.35)  1+ exp (log Boom )  Pmodel\\*  Pjoint(Y =1 | x) =  619  CHAPTER 18", "d46cf398-452e-4048-b7d9-f851cd7fbd70": "How might this be de\ufb01ned formally? If the sequence of rewards received after time step t is denoted Rt+1, Rt+2, Rt+3, . .\n\n., then what precise aspect of this sequence do we wish to maximize? In general, we seek to maximize the expected return, where the return, denoted Gt, is de\ufb01ned as some speci\ufb01c function of the reward sequence. In the simplest case the return is the sum of the rewards: where T is a \ufb01nal time step. This approach makes sense in applications in which there is a natural notion of \ufb01nal time step, that is, when the agent\u2013environment interaction breaks naturally into subsequences, which we call episodes,7 such as plays of a game, trips through a maze, or any sort of repeated interaction. Each episode ends in a special state called the terminal state, followed by a reset to a standard starting state or to a sample from a standard distribution of starting states. Even if you think of episodes as ending in di\u21b5erent ways, such as winning and losing a game, the next episode begins independently of how the previous one ended. Thus the episodes can all be considered to end in the same terminal state, with di\u21b5erent rewards for the di\u21b5erent outcomes", "16537032-05be-477d-94c6-6ca3bf2371ff": "3. Incomplete modeling. When we use a model that must discard some of  https://www.deeplearningbook.org/contents/prob.html    the information we have observed, the discarded information results in uncertainty in the model\u2019s predictions. For example, suppose we build a robot that can exactly observe the location of every ob ject around it.\n\nIf the robot discretizes space when predicting the future location of these objects,  52  CHAPTER 3. PROBABILITY AND INFORMATION THEORY  then the discretization makes the robot immediately become uncertain about the precise position of objects: each object could be anywhere within the discrete cell that it was observed to occupy. In many cases, it is more practical to use a simple but uncertain rule rather than a complex but certain one, even if the true rule is deterministic and our modeling system has the fidelity to accommodate a complex rule", "c58b0b9d-d9d1-43c0-bc9c-99a33bb867ad": "We have already seen that the minimizer y(x) of the cross-entropy error (4.90) for two-class classi\ufb01cation is given by the posterior class probability. In the case of a target variable t \u2208 {\u22121, 1}, we have seen that the error function is given by Section 7.1.2 ln(1 + exp(\u2212yt)). This is compared with the exponential error function in Figure 14.3, where we have divided the cross-entropy error by a constant factor ln(2) so that it passes through the point (0, 1) for ease of comparison. We see that both can be seen as continuous approximations to the ideal misclassi\ufb01cation error function. An advantage of the exponential error is that its sequential minimization leads to the simple AdaBoost scheme. One drawback, however, is that it penalizes large negative values of ty(x) much more strongly than cross-entropy. In particular, we see that for large negative values of ty, the cross-entropy grows linearly with |ty|, whereas the exponential error function grows exponentially with |ty|", "8e9096d5-ee07-4722-95ea-1f3798b65b64": "Likewise, when there is no ambiguity, we may omit the square brackets. Expectations are linear, for example,  Exlaf (x) + 69(x)] = oF x + BExlg(a)), (3.11)  when a and \u00a3 are not dependent on x.  https://www.deeplearningbook.org/contents/prob.html    The variance gives a measure of how much the values of a function of a random variable x vary as we sample different values of % from its probability distribution:  Var( f(x) = | (f(x) \u2014 Ef(@))] (3.12)  C  When the variance is low, the values of f(a) cluster near their expected value. The square root of the variance is known as the standard deviation.\n\nThe covariance gives some sense of how much two values are linearly related to each other, as well as the scale of these variables:  Cov(f (x), 9(y)) = E) (gy) - Ela) - (3.18)  High absolute values of the covariance mean that the values change very much and are both far from their respective means at the same time", "0116355c-4d16-49c8-b311-f77e42bfae74": "We initialize the EM algorithm by choosing an initial value \u03b8old for the model parameters. In the E step, we then use these parameter values to evaluate the posterior probabilities of the components k for each data point n, which are given by These responsibilities are then used to \ufb01nd the expected complete-data log likelihood as a function of \u03b8, given by The M step involves maximization of this function with respect to \u03b8, keeping \u03b8old, and hence \u03b3nk, \ufb01xed. Maximization with respect to \u03c0k can be done in the usual way, with a Lagrange multiplier to enforce the summation constraint \ufffd To determine the {wk}, we note that the Q(\u03b8, \u03b8old) function comprises a sum over terms indexed by k each of which depends only on one of the vectors wk, so that the different vectors are decoupled in the M step of the EM algorithm. In other words, the different components interact only via the responsibilities, which are \ufb01xed during the M step. Note that the M step does not have a closed-form solution and must be solved iteratively using, for instance, the iterative reweighted least squares (IRLS) algorithm", "fc8e1d25-bed9-4cfe-a7c8-c174ae6cea31": "Two designs in SimCLR, namely, (1) an MLP projection head and (2) stronger data augmentation, are proved to be very efficient. MoCo V2  combined these two designs, achieving even better transfer performance with no dependency on a very large batch size. CURL  CURL  applies the above ideas in Reinforcement Learning. It learns a visual representation for RL tasks by matching embeddings of two data-~augmented versions, 0, and 0,, of the raw observation o via contrastive loss. CURL primarily relies on random crop data augmentation. The key encoder is implemented as a momentum encoder with weights as EMA of the query encoder weights, same as in MoCo.\n\nOne significant difference between RL and supervised visual tasks is that RL depends on temporal consistency between consecutive frames. Therefore, CURL applies augmentation consistently on each stack of frames to retain information about the temporal structure of the observation. Feature Clustering  DeepCluster  DeepCluster  iteratively clusters features via k-means and uses cluster assignments as pseudo labels to provide supervised signals", "af8a9bb3-cf4b-43a6-9a7d-e80bd6792259": "Then the tangent to the curve M is given by the directional derivative \u03c4 = \u2202s/\u2202\u03be, and the tangent vector at the point xn is given by Under a transformation of the input vector, the network output vector will, in general, change. The derivative of output k with respect to \u03be is given by where Jki is the (k, i) element of the Jacobian matrix J, as discussed in Section 5.3.4.\n\nThe result (5.126) can be used to modify the standard error function, so as to encourage local invariance in the neighbourhood of the data points, by the addition to the original error function E of a regularization function \u2126 to give a total error function of the form \ufffdE = E + \u03bb\u2126 (5.127) where \u03bb is a regularization coef\ufb01cient and The regularization function will be zero when the network mapping function is invariant under the transformation in the neighbourhood of each pattern vector, and the value of the parameter \u03bb determines the balance between \ufb01tting the training data and learning the invariance property. In a practical implementation, the tangent vector \u03c4 n can be approximated using \ufb01nite differences, by subtracting the original vector xn from the corresponding vector after transformation using a small value of \u03be, and then dividing by \u03be", "04e93b67-bc1b-4a47-8da8-82c966b85f5f": "The first-order Taylor series approximation of g predicts that the value of 7 will decrease by eg!\n\ng- If we wanted to decrease g by 0.1, this first-order information available in the gradient suggests we could set the learning rate \u00ab to 24. Yet, the actual update will include second-order and third-order effects, on up to effects of order |. The new value of \u00a5 is given by  x(wy _ \u20ac91) (we _ \u20acg2) wae (wi _ \u20acg1). (8.34)  An example of one second-order term arising from this update is \u20ac2g1 m 3 Wi. This term might be negligible if 3 w; is small, or might be exponentially large if the weights on layers 3 through / are greater than 1. This makes it vac hard to choose an appropriate learnitg rate, because the effects of an update to the  https://www.deeplearningbook.org/contents/optimization.html    parameters for one layer depend so strongly on all the other layers. Second-order optimization algorithms address this issue by computing an update that takes these second-order interactions into account, but we can see that in very deep networks,  even higher-order interactions can be significant", "037b1abd-f70c-4800-a64e-ed13ac8e4cb4": "For example, if estimating the probability density at some point 2, we can just return the number of training examples in the same unit volume cell as x, divided by the total number of training examples. If we wish to classify an example, we can return the most common class of training examples in the same cell. If we are doing regression, we can average the target values observed over the examples in that cell. But what about the cells for which we have seen no example? Because in high-dimensional spaces, the number of configurations is huge, much larger than our number of examples, a typical grid cell has no training example associated with it. How could we possibly say something meaningful about these new configurations? Many traditional machine learning  153  CHAPTER 5. MACHINE LEARNING BASICS  algorithms simply assume that the output at a new point should be approximately the same as the output at the nearest training point. 5.11.2 Local Constancy and Smoothness Regularization  To generalize well, machine learning algorithms need to be guided by prior beliefs about what kind of function they should learn.\n\nWe have seen these priors incorpo- rated as explicit beliefs in the form of probability distributions over parameters of the model", "cfbe0051-8f87-4b07-8c28-e256645e3d30": "Schultz, W., Dayan, P., Montague, P. R. A neural substrate of prediction and reward. Schultz, W., Romo, R. Dopamine neurons of the monkey midbrain: contingencies of Schultz, W., Romo, R., Ljungberg, T., Mirenowicz, J., Hollerman, J. R., Dickinson, A. Reward-related signals carried by dopamine neurons. In J. C. Houk, J. L. Davis, and D. G. Beiser (Eds. ), Models of Information Processing in the Basal Ganglia, pp. 233\u2013248. MIT Press, Cambridge, MA. Schwartz, A. A reinforcement learning method for maximizing undiscounted rewards. In Proceedings of the 10th International Conference on Machine Learning , pp. 298\u2013305. Morgan Kaufmann. Schweitzer, P. J., Seidmann, A", "e551ea39-2080-4804-997d-aadf95e7128c": "The Tiny-imagenet-200 dataset contains  only 500 images in each of the classes, with 100 set aside for validation.\n\nThis problem Shorten and Khoshgoftaar J Big Data  6:60   Augmented and target image are Second network generates compared to generate a loss (or none) a classification loss EY 2 a, = A pair of random 9 images from the i/ l training set is fed into g a CNN. a : ~ a  \u2018\u201c  Classification  Augmentation Network  Network  Fig. 29 Illustration of augmentation network   limits this dataset to 2 classes. Thus there are only 800 images for training. Each of the Tiny-imagenet-200 images is 64 x 64 x 3, and the MNIST images are 28 x 28 x 1. The experiment compares their proposed Neural Augmentation  approach with traditional augmentation techniques such as cropping and rotation, as well as with a style transfer approach with a predetermined set of styles such as Night/Day and Winter/Summer. The traditional baseline study transformed images by choosing an augmentation from a set (shifted, zoomed in/out, rotated, flipped, distorted, or shaded with a hue)", "b33ce61f-4281-485a-b542-72ac56fbd6de": "Instead, it analytically regularizes the model to resist perturbation in the directions corresponding to the specified transformation. While this analytical approach is intellectually elegant, it has two major drawbacks. First, it only regularizes the model to resist infinitesimal perturbation.\n\nExplicit dataset augmentation confers resistance to  https://www.deeplearningbook.org/contents/regularization.html    larger perturbations. Second, the infinitesimal approach poses difficulties for models based on rectified linear units. These models can only shrink their derivatives by turning units off or shrinking their weights. They are not able to shrink their  derivatives by saturating at a high value with large weights, as sigmoid or tanh units can. Dataset augmentation works well with rectified linear units because different subsets of rectified units can activate for different transformed versions of each original input. Tangent propagation is also related to double backprop  and adversarial training . Double backprop regularizes the Jacobian to be small, while adversarial training finds inputs near the original inputs and trains the model to produce the same output on these as on the original inputs", "e55a0072-949b-452f-9588-2cae814e57a3": "In fact, because local contrast normalization typically acts on smaller windows, it is even more important to regularize. Smaller windows are more likely to contain values that are all nearly the same as each other, and thus more likely to have zero standard deviation.\n\nhttps://www.deeplearningbook.org/contents/applications.html    12.2.1.2 Dataset Augmentation  As described in section 7.4, it is easy to improve the generalization of a classifier by increasing the size of the training set by adding extra copies of the training  452  CHAPTER 12. APPLICATIONS  examples that have been modified with transformations that do not change the class. Object recognition is a classification task that is especially amenable to this form of dataset augmentation because the class is invariant to so many transformations and the input can be easily transformed with many geometric operations. As described before, classifiers can benefit from random translations, rotations, and in some cases, flips of the input to augment the dataset. In specialized computer vision applications, more advanced transformations are commonly used for dataset augmentation. These schemes include random perturbation of the colors in an image  and nonlinear geometric distortions of the input . 12.3", "ad3ad655-5d59-4cea-a88f-d21e8f700d2a": "S., Mahmood, A. R., Precup, D., van Hasselt, H. A new Q(\u03bb) with interim forward view and Monte Carlo equivalence. In Proceedings of the International Conference on Machine Learning, 31. JMLR W&CP 32(2). Sutton, R. S., Mahmood, A. R., White, M. An emphatic approach to the problem of o\u21b5-policy temporal-di\u21b5erence learning. Journal of Machine Learning Research, 17(73):1\u201329. Sutton, R. S., McAllester, D. A., Singh, S. P., Mansour, Y. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems 12 , pp. 1057\u20131063. MIT Press, Cambridge, MA. Sutton, R. S., Modayil, J., Delp, M., Degris, T., Pilarski, P", "af3a2936-d5ae-422b-bba3-8c98cb6b67a7": "14.2.2 Denoising Autoencoders  Rather than adding a penalty \u00a9 to the cost function, we can obtain an autoencoder  https://www.deeplearningbook.org/contents/autoencoders.html    that learns something useful by changing the reconstruction error term of the cost function. Traditionally, autoencoders minimize some function  L(x, 9(f(\u00ab))), (14.8)  where L is a loss function penalizing g(f(a)) for being dissimilar from a, such as the L? norm of their difference. This encourages go f to learn to be merely an identity function if they have the capacity to do so. A denoising autoencoder (DAE) instead minimizes  L(x, 9(f(#))), (14.9)  where & is a copy of # that has been corrupted by some form of noise. Denoising autoencoders must therefore undo this corruption rather than simply copying their input. Denoising training forces f and g to implicitly learn the structure of pgata(\u00a3), as shown by Alain and Bengio  and Bengio ef al. Denoising 504  CHAPTER 14", "6b4f313f-aa43-4420-8bdb-28d825d9c667": "As pure symbols, \u201ccat\u201d and \u201cdog\u201d are as far from each other as any other two symbols. However, if one associates them with a meaningful distributed representation, then many of the things that can be said about cats can generalize to dogs and vice versa. For example, our distributed representation may contain entries such as \u201chas_fur\u201d or \u201cnumber_of_legs\u201d that have the same value for the embedding of both \u201ccat\u201d and \u201cdog.\u201d Neural language models that operate on distributed representations of words generalize much better than other models that operate directly on one-hot representations of words, as discussed in section 12.4. Distributed representations induce a rich similarity space, in which semantically close concepts (or inputs) are close in distance, a property that is absent from purely symbolic representations. When and why can there be a statistical advantage from using a distributed representation as part of a learning algorithm?\n\nDistributed representations can have a statistical advantage when an apparently complicated structure can be compactly represented using a small number of parameters. Some traditional nondistributed learning algorithms generalize only due to the smoothness assumption, which  546  CHAPTER 15", "30317e1d-ac06-4557-9f9d-d3dbeb54f5fb": "Making use of (2.105) and (2.108) we see that the mean and covariance of the marginal distribution p(y) are given by A special case of this result is when A = I, in which case it reduces to the convolution of two Gaussians, for which we see that the mean of the convolution is the sum of the mean of the two Gaussians, and the covariance of the convolution is the sum of their covariances. Finally, we seek an expression for the conditional p(x|y).\n\nRecall that the results for the conditional distribution are most easily expressed in terms of the partitioned precision matrix, using (2.73) and (2.75). Applying these results to (2.105) and Section 2.3 (2.108) we see that the conditional distribution p(x|y) has mean and covariance given by The evaluation of this conditional can be seen as an example of Bayes\u2019 theorem. We can interpret the distribution p(x) as a prior distribution over x. If the variable y is observed, then the conditional distribution p(x|y) represents the corresponding posterior distribution over x", "0301befa-6c48-4460-b9d0-16e7125e1e47": "This procedure can be justified as increasing a variational lower bound on the log-likelihood of the data under the DBN . In most applications, no effort is made to jointly train the DBN after the greedy layer-wise procedure is complete. However, it is possible to perform generative fine-tuning using the wake-sleep algorithm. 658  CHAPTER 20. DEEP GENERATIVE MODELS  The trained DBN may be used directly as a generative model, but most of the interest in DBNs arose from their ability to improve classification models.\n\nWe can take the weights from the DBN and use them to define an MLP:  AY =o BD4u0T WO , (20.22)  RO =o BO LRY-YMTWO V6 2,...,m. (20.23)  After initializing this MLP/with the weights and biases learned via generative training of the DBN, we cary train the MLP toh additional training of the MLP is an example of discriminative fine-tuning. erform a classification task", "aea3bab8-e4a3-46ae-924f-18ece50f0b7a": "We would like to quantify information in a way that formalizes this intuition. e Likely events should have low information content, and in the extreme case, events that are guaranteed to happen should have no information content whatsoever. e Less likely events should have higher information content. e Independent events should have additive information. For example, finding out that a tossed coin has come up as heads twice should convey twice as much information as finding out that a tossed coin has come up as heads once. To satisfy all three of these properties, we define the self-information of an event x = x to be  I(x) = \u2014 log P(z). (3.48)  In this book, we always use log to mean the natural logarithm, with base e. Our definition of I(x) is therefore written in units of nats.\n\nOne nat is the amount of information gained by observing an event of probability i. Other texts use base-2  71  CHAPTER 3. PROBABILITY AND INFORMATION THEORY  logarithms and units called bits or shannons; information measured in bits is just a rescaling of information measured in nats", "0df0cf9b-c9d3-4a5f-9a22-0fa30ba5a97e": "In particular, we introduce and adapt the principled path consistency learning  to text generation, that (1) offers a natural way to train the model with both on- and off-policy updates, hence combining the best of the two strategies, (2) bridges the sparse reward signal to directly supervise the Q function learning, leading to more accurate Q estimation and credit assignment, and (3) makes ef\ufb01cient updates to Q-values by considering all candidate actions together. The generality and ef\ufb01ciency of the proposed method allows us to train text generation in a wide range of applications: (1) With noisy and negative training examples, our approach learns to generate accurate entailment text that greatly improves upon the data itself as well as other various training methods; (2) Our approach also manages to train an effective adversarial text generator for robustness test for classi\ufb01ers; (3) We train a prompt generator with our algorithm to achieve controllable generation of pretrained LMs in terms of topics.2 On all the three tasks, our approach consistently improves over not only previous RL algorithms for text generation, but also diverse task-specialized methods designed speci\ufb01cally for each of the problems, respectively.\n\nIn the appendix (\u00a7A.1.4), we also show that on standard supervised tasks where MLE prevails, our approach is competitive to train text generation models from scratch, which was usually impossible for previous RL algorithms", "b6cb029e-0653-40b7-9225-856661402d39": "Using Bayes\u2019 theorem, these probabilities can be expressed in the form Note that any of the quantities appearing in Bayes\u2019 theorem can be obtained from the joint distribution p(x, Ck) by either marginalizing or conditioning with respect to the appropriate variables. We can now interpret p(Ck) as the prior probability for the class Ck, and p(Ck|x) as the corresponding posterior probability. Thus p(C1) represents the probability that a person has cancer, before we take the X-ray measurement. Similarly, p(C1|x) is the corresponding probability, revised using Bayes\u2019 theorem in light of the information contained in the X-ray. If our aim is to minimize the chance of assigning x to the wrong class, then intuitively we would choose the class having the higher posterior probability. We now show that this intuition is correct, and we also discuss more general criteria for making decisions. Suppose that our goal is simply to make as few misclassi\ufb01cations as possible", "47b618b8-ede8-48f1-a14a-53388177e2f0": "Currently, we cannot unambiguously recommend Bayesian hyperparameter optimization as an established tool for achieving better deep learning results or  https://www.deeplearningbook.org/contents/guidelines.html    tor obtaining those results with less ettort. Bayesian hyperparameter optimization sometimes performs comparably to human experts, sometimes better, but fails catastrophically on other problems. It may be worth trying to see if it works on a particular problem but is not yet sufficiently mature or reliable. That being said,  hyperparameter optimization is an important field of research that, while often driven primarily by the needs of deep learning, holds the potential to benefit not  430  CHAPTER 11. PRACTICAL METHODOLOGY  only the entire field of machine learning but also the discipline of engineering in general. One drawback common to most hyperparameter optimization algorithms with more sophistication than random search is that they require for a training ex- periment to run to completion before they are able to extract any information from the experiment", "c3432817-618e-4444-b4dc-095a67784c6f": "This approach resembles MCMC methods in the sense that it involves many iterations to produce a sample. However, the model is defined to be the probability distribution produced by the final step of the chain. In this sense, there is no approximation induced by the iterative procedure. The approach introduced by Sohl-Dickstein et al. is also very close to the generative interpretation of the denoising autoencoder (section 20.11.1). As with the denoising autoencoder, diffusion inversion trains a transition operator that attempts to probabilistically undo the effect of adding some noise. The difference is that diffusion inversion requires undoing only one step of the diffusion process, rather than traveling all the way back to a clean data point.\n\nThis addresses the following dilemma present with the ordinary reconstruction log-likelihood objective of denoising autoencoders: with small levels of noise the learner only sees configurations near the data points, while with large levels of noise it is asked to do an almost impossible job (because the denoising distribution is highly complex and multimodal)", "e3e594b1-8efc-46a8-9ae6-c6af412be7a3": "Pavlov (or more exactly, his translators) called inborn responses (e.g., salivation in his demonstration described above) \u201cunconditioned responses\u201d (URs), their natural triggering stimuli (e.g., food) \u201cunconditioned stimuli\u201d (USs), and new responses triggered by predictive stimuli (e.g., here also salivation) \u201cconditioned responses\u201d (CRs). A stimulus that is initially neutral, meaning that it does not normally elicit strong responses (e.g., the metronome sound), becomes a \u201cconditioned stimulus\u201d (CS) as the animal learns that it predicts the US and so comes to produce a CR in response to the CS. These terms are still used in describing classical conditioning experiments (though better translations would have been \u201cconditional\u201d and \u201cunconditional\u201d instead of conditioned and unconditioned).\n\nThe US is called a reinforcer because it reinforces producing a CR in response to the CS. sound of a metronome is just one example of classical conditioning, which has been intensively studied across many response systems of many species of animals", "2bc3eea2-04de-4fbc-8ff8-a66320e17093": "As we discussed above, even if we have a complete and accurate model of the environment\u2019s dynamics, it is usually not possible to simply compute an optimal policy by solving the Bellman optimality equation. For example, board games such as chess are a tiny fraction of human experience, yet large, customdesigned computers still cannot compute the optimal moves. A critical aspect of the problem facing the agent is always the computational power available to it, in particular, the amount of computation it can perform in a single time step. The memory available is also an important constraint. A large amount of memory is often required to build up approximations of value functions, policies, and models. In tasks with small, \ufb01nite state sets, it is possible to form these approximations using arrays or tables with one entry for each state (or state\u2013action pair). This we call the tabular case, and the corresponding methods we call tabular methods. In many cases of practical interest, however, there are far more states than could possibly be entries in a table.\n\nIn these cases the functions must be approximated, using some sort of more compact parameterized function representation. Our framing of the reinforcement learning problem forces us to settle for approximations", "de3a5f53-d4f2-4797-8d75-991daa83601d": "A plot of the cost function J given by (9.1) for the Old Faithful example is shown in Figure 9.2. Note that we have deliberately chosen poor initial values for the cluster centres so that the algorithm takes several steps before convergence. In practice, a better initialization procedure would be to choose the cluster centres \u00b5k to be equal to a random subset of K data points. It is also worth noting that the K-means algorithm itself is often used to initialize the parameters in a Gaussian mixture model before applying the EM algorithm. Section 9.2.2 A direct implementation of the K-means algorithm as discussed here can be relatively slow, because in each E step it is necessary to compute the Euclidean distance between every prototype vector and every data point. Various schemes have been proposed for speeding up the K-means algorithm, some of which are based on precomputing a data structure such as a tree such that nearby points are in the same subtree . Other approaches make use of the triangle inequality for distances, thereby avoiding unnecessary distance calculations .\n\nSo far, we have considered a batch version of K-means in which the whole data set is used together to update the prototype vectors", "d550e1de-e837-4ae0-a8d7-fb7756afcd12": "Later, Martens and Medabalimi   551  CHAPTER 15. REPRESENTATION LEARNING  showed that there are significant differences between every two finite depths of SPN, and that some of the constraints used to make SPNs tractable may limit their representational power. Another interesting development is a set of theoretical results for the expressive power of families of deep circuits related to convolutional nets, highlighting an exponential advantage for the deep circuit even when the shallow circuit is allowed to only approximate the function computed by the deep circuit . By comparison, previous theoretical work made claims regarding only the case where the shallow circuit must exactly replicate particular functions. 15.6 Providing Clues to Discover Underlying Causes  To close this chapter, we come back to one of our original questions: what makes one representation better than another? One answer, first introduced in section 15.3, is that an ideal representation is one that disentangles the underlying causal factors of variation that generated the data, especially those factors that are relevant to our applications.\n\nMost strategies for representation learning are based on introducing clues that help the learning find these underlying factors of variations. The clues can help the learner separate these observed factors from the others", "a9c52578-a231-4a88-82cc-07d0b125bd2e": "tile se<:o<l<l M S!flp.\n\n<') After l!Ie MC()<>;l E st\"l' S,uion I.J Be<:au,\", th~ pm/xlhi li>lic PeA modd has a well\u00b7defined likelillood f\"flCtion, we <wId employ cros,-,-.1idation to delermine the \\\"aJue of di\"\",nsiooa!ity by \"'Iecting tit<: large,t log likelihood t>I1 a '-alidation data set Such an opprooch. hov.\u00b7~\\-er. can become computationally ro<lly. p3rticularl)' if we CQnsid<:, \u2022 probabilistic miXlUre of PeA modds  in \"hich we seek 10 <!etermi'\" the appropriate dimen,ionalily \",paraltly for toch componenl in lt1e mixm\"\" Gi'-en thai w", "63523c5b-6852-4f33-a59a-3c2203173e79": "One popular idea is to formalize an MDP at a detailed level, with a small time step, yet enable planning at higher levels using extended courses of action that correspond to many base-level time steps.\n\nTo do this we need a notion of course of action that extends over many time steps and includes a notion of termination. A general way to formulate these two ideas is as a policy, \u21e1, and a state-dependent termination function, \u03b3, as in GVFs. We de\ufb01ne a pair of these as a generalized notion of action termed an option. To execute an option ! = h\u21e1!, \u03b3!i at time t is to obtain the action to take, At, from \u21e1! (\u00b7|St), then terminate at time t + 1 with probability 1 \u2212 \u03b3!(St+1). If the option does not terminate at t+1, then At+1 is selected from \u21e1! (\u00b7|St+1), and the option terminates at t + 2 with probability 1 \u2212 \u03b3! (St+2), and so on until eventual termination", "fe1b9e8c-5366-42b8-8f38-7afc9bc9fefc": "This leaves the issue of deciding the appropriate model complexity for the particular problem, which cannot be decided simply by maximizing the likelihood function, because this always leads to excessively complex models and over-\ufb01tting. Independent hold-out data can be used to determine model complexity, as discussed in Section 1.3, but this can be both computationally expensive and wasteful of valuable data. We therefore turn to a Bayesian treatment of linear regression, which will avoid the over-\ufb01tting problem of maximum likelihood, and which will also lead to automatic methods of determining model complexity using the training data alone. Again, for simplicity we will focus on the case of a single target variable t. Extension to multiple target variables is straightforward and follows the discussion of Section 3.1.5. We begin our discussion of the Bayesian treatment of linear regression by introducing a prior probability distribution over the model parameters w. For the moment, we shall treat the noise precision parameter \u03b2 as a known constant.\n\nFirst note that the likelihood function p(t|w) de\ufb01ned by (3.10) is the exponential of a quadratic function of w", "4bc252ad-2b2c-4d23-806f-6099c914884b": "Averages of random variables tend to a Gaussian, by the central limit theorem, and the sum of two Gaussian variables is again Gaussian. The Gaussian is the distribution that maximizes the entropy for a given variance (or covariance). Any linear transformation of a Gaussian random variable is again Gaussian. The marginal distribution of a multivariate Gaussian with respect to a subset of the variables is itself Gaussian, and similarly the conditional distribution is also Gaussian", "795484ab-a7b2-4b2d-ae41-a47f664bee34": "9.4 Sutton  proved convergence of linear TD(0) in the mean to the minimal VE solution for the case in which the feature vectors, {x(s) : s 2 S}, are linearly independent. Convergence with probability 1 was proved by several researchers at about the same time .\n\nIn addition, Jaakkola, Jordan, and Singh  proved convergence under online updating. All of these results assumed linearly independent feature vectors, which implies at least as many components to wt as there are states. Convergence for the more important case of general (dependent) feature vectors was \ufb01rst shown by Dayan . A signi\ufb01cant generalization and strengthening of Dayan\u2019s result was proved by Tsitsiklis and Van Roy . They proved the main result presented in this section, the bound on the asymptotic error of linear bootstrapping methods. 9.5.2 Konidaris, Osentoski, and Thomas  introduced the Fourier basis in a simple form suitable for reinforcement learning problems with multi-dimensional continuous state spaces and functions that do not have to be periodic. 9.5.3 The term coarse coding is due to Hinton , and our Figure 9.6 is based on one of his \ufb01gures", "97bbe852-9b94-46e0-8a61-f93f1c46aff0": "We denote the prior class probability p(C1) = \u03c0, so that p(C2) = 1 \u2212 \u03c0.\n\nFor a data point xn from class C1, we have tn = 1 and hence p(xn, C1) = p(C1)p(xn|C1) = \u03c0N(xn|\u00b51, \u03a3). Similarly for class C2, we have tn = 0 and hence p(xn, C2) = p(C2)p(xn|C2) = (1 \u2212 \u03c0)N(xn|\u00b52, \u03a3). Thus the likelihood function is given by where t = (t1, . , tN)T. As usual, it is convenient to maximize the log of the likelihood function. Consider \ufb01rst the maximization with respect to \u03c0. The terms in the log likelihood function that depend on \u03c0 are Setting the derivative with respect to \u03c0 equal to zero and rearranging, we obtain where N1 denotes the total number of data points in class C1, and N2 denotes the total number of data points in class C2. Thus the maximum likelihood estimate for \u03c0 is simply the fraction of points in class C1 as expected", "4a7c0d47-2045-47d2-906a-dcdac5793107": "We want the agent to explore to \ufb01nd changes in the environment, but not so much that performance is greatly degraded.\n\nAs in the earlier exploration/exploitation con\ufb02ict, there probably is no solution that is both perfect and practical, but simple heuristics are often e\u21b5ective. The Dyna-Q+ agent that did solve the shortcut maze uses one such heuristic. This agent keeps track for each state\u2013action pair of how many time steps have elapsed since the pair was last tried in a real interaction with the environment. The more time that has elapsed, the greater (we might presume) the chance that the dynamics of this pair has changed and that the model of it is incorrect. To encourage behavior that tests long-untried actions, a special \u201cbonus reward\u201d is given on simulated experiences involving these actions. In particular, if the modeled reward for a transition is r, and the transition has not been tried in \u2327 time steps, then planning updates are done as if that transition produced a reward of r + \uf8ffp\u2327, for some small \uf8ff", "68c9d145-479b-4733-adf4-1d18326a73a8": "Training a Discriminative Model The output of Snorkel is a set of probabilistic labels that can be used to train a wide variety of state-of-the-art machine learning models, such as popular deep learning models.\n\nWhile the generative model is essentially a re-weighted combination of the user-provided labeling functions\u2014which tend to be precise but low-coverage\u2014modern discriminative models can retain this precision while learning to generalize beyond the labeling functions, increasing coverage and robustness on unseen data. Next, we set up the problem Snorkel addresses and describe its main components and design decisions. Setup Our goal is to learn a parameterized classi\ufb01cation model h\u03b8 that, given a data point x \u2208 X, predicts its label y \u2208 Y, where the set of possible labels Y is discrete. For simplicity, we focus on the binary setting Y = {\u22121, 1}, though we include a multi-class application in our experiments. For example, x might be a medical image, and y a label indicating normal versus abnormal. In the relation extraction examples we look at, we often refer to x as a candidate", "47fc57ed-a082-4869-b4e9-de207dd4bdf9": "We don't need to worry about the infinite loops in the state transition graph.\n\nThe state-value of a state s is the expected return if we are in this state at time t, S; = s:  V,(s) ~ 1 1G4|S: ~ 8]  Similarly, we define the action-value (\u201cQ-value\u201d; Q as \u201cQuality\u201d | believe?) of a state-action pair as:  Q,(s; a) = oo G|S; \u2014 8, A; = a]  Additionally, since we follow the target policy 7, we can make use of the probility distribution over possible actions and the Q-values to recover the state-value:  V,(s) = }\u00b0Q,(s,a)m(als)  acA  The difference between action-value and state-value is the action advantage function (\u201cA-value\u201d): A,(s, a) = Q,(s, a) ~~ V,(s) Optimal Value and Policy  The optimal value function produces the maximum return:  V.(s) = max V,(s); Q. (s, a) = max Q,(s; a)  The optimal policy achieves optimal value functions:  TT", "aa29e75d-e846-44d4-9269-1ef6e56d2cf3": "Action preferences were computed from the current estimates of the action values: the action with the maximum estimated action value was given preference 1/\u2327, the action with the minimum estimated action value was given preference 0, and the preferences of the other actions were scaled between these extremes. The step-size and discount-rate parameters were \ufb01xed at 0.1 and 0.98 respectively. Each learning episode took place with the agent controlling simulated \ufb02ight in an independently generated period of simulated turbulent air currents. Each episode lasted 2.5 minutes simulated with a 1 second time step.\n\nLearning e\u21b5ectively converged after a few hundred episodes. The left panel of Figure 16.10 shows a sample trajectory before learning where the agent selects actions randomly. Starting at the top of the volume shown, the glider\u2019s trajectory is in the direction indicated by the arrow and quickly loses altitude. Figure 16.10\u2019s right panel is a trajectory after learning. The glider starts at the same place (here appearing at the bottom of the volume) and gains altitude by spiraling 3Reddy et al. described this slightly di\u21b5erently, but our version is equivalent to theirs. \ufb02ight from the same starting point (note that the altitude scales are shifted)", "6596e747-4daa-4c01-9b16-7bda7475de80": "This is the case surprisingly often; the ability of Monte Carlo methods to work with sample episodes alone can be a signi\ufb01cant advantage even when one has complete knowledge of the environment\u2019s dynamics. Can we generalize the idea of backup diagrams to Monte Carlo algorithms? The general idea of a backup diagram is to show at the top the root node to be updated and to show below all the transitions and leaf nodes whose rewards and estimated values contribute to the update. For Monte Carlo estimation of v\u21e1, the root is a state node, and below it is the entire trajectory of transitions along a particular single episode, ending at the terminal state, as shown to the right.\n\nWhereas the DP diagram (page 59) shows all possible transitions, the Monte Carlo diagram shows only those sampled on the one episode. Whereas the DP diagram includes only one-step transitions, the Monte Carlo diagram goes all the way to the end of the episode. These di\u21b5erences in the diagrams accurately re\ufb02ect the fundamental di\u21b5erences between the algorithms. An important fact about Monte Carlo methods is that the estimates for each state are independent. The estimate for one state does not build upon the estimate of any other state, as is the case in DP", "2e933bb4-7c32-42a8-9fa6-21e8ce9c0397": "13.4 Sparse Coding  Sparse coding  is a linear factor model that has  https://www.deeplearningbook.org/contents/linear_factors.html    DECLL LICaVIly SLUCICU ad all ULSUPELVI5CU LEALULE LEAL ALG LEALULE CXLLACLIOIL mechanism. Strictly speaking, the term \u201csparse coding\u201d refers to the process of inferring the value of hin this model, while \u201csparse modeling\u201d refers to the process of designing and learning the model, but the term \u201csparse coding\u201d is often used to  refer to both. Like most other linear factor models, it uses a linear decoder plus noise to obtain reconstructions of a, as specified in equation 13.2.\n\nMore specifically, sparse coding models typically assume that the linear factors have Gaussian noise with isotropic precision (:  p(a | h) =N(esWh+b, 50). (13.12)  The distribution p(h) is chosen to be one with sharp peaks near 0 . Common choices include factorized Laplace, Cauchy or factorized Student \u00a2distributions", "37e1099e-f19d-42c8-b12d-2a5a16295f86": "However, the least-squares solution gives poor results, with only a small region of the input space assigned to the green class. The failure of least squares should not surprise us when we recall that it corresponds to maximum likelihood under the assumption of a Gaussian conditional distribution, whereas binary target vectors clearly have a distribution that is far from Gaussian. By adopting more appropriate probabilistic models, we shall obtain classi\ufb01cation techniques with much better properties than least squares. For the moment, however, we continue to explore alternative nonprobabilistic methods for setting the parameters in the linear classi\ufb01cation models. One way to view a linear classi\ufb01cation model is in terms of dimensionality reduction. Consider \ufb01rst the case of two classes, and suppose we take the D(\u00d7), green (+), and blue (\u25e6).\n\nLines denote the decision boundaries, and the background colours denote the respective classes of the decision regions. On the left is the result of using a least-squares discriminant. We see that the region of input space assigned to the green class is too small and so most of the points from this class are misclassi\ufb01ed. On the right is the result of using logistic regressions as described in Section 4.3.2 showing correct classi\ufb01cation of the training data", "8f4bdd46-9af2-4fec-8dca-b5fc7c2f759e": "Plugging the approximation of in\ufb02uence function into the above functional descent 7.2.2. The Student Step. The student step optimizes the SE objective w.r.t. the target model parameters \u03b8, given q(n+1) from the teacher step. The optimization is to minimize the divergence between the student p\u03b8 and the teacher q(n+1): Section 5 discussed the di\ufb00erent choices of the divergence function D. In particular, in the same setting of Equation 3.3 where D is the common cross entropy (or KL divergence as discussed in Section 5.1), the student step is written as: where q(n+1) is in the form of Equation 7.1 above. The optimization then amounts to \ufb01rst drawing samples from the teacher \u02dct \u223c q(n+1), and then updating \u03b8 by maximizing the log-likelihood of those samples under p\u03b8. We could apply various sampling methods to draw samples from q(n+1), such as Markov chain Monte Carlo (MCMC) methods, like Gibbs sampling when t is discrete and Hamiltonian or Langevin MC for continuous t , and sampling based on stochastic di\ufb00erential equations", "865c0708-1428-4028-90f7-323bef78335b": "For example, consider the classical form of a dynamical system: 8) = f(s); 6), (10.1)  where s\u201c) is called the state of the system. Equation 10.1 is recurrent because the definition of s at time t refers back to the same definition at time t \u2014 1.\n\nFor a finite number of time steps 7, the graph can be unfolded by applying the definition 7 \u2014 1 times. For example, if we unfold equation 10.1 for r = 3 time steps, we obtain  https://www.deeplearningbook.org/contents/rnn.html    Unfolding the equation by repeatedly applying the definition in this way has yielded an expression that does not involve recurrence. Such an expression can now be represented by a traditional directed acyclic computational graph. The unfolded computational graph of equation 10.1 and equation 10.3 is illustrated in figure 10.1. As another example, let us consider a dynamical system driven by an external  369  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  f Unfold  Figure 10.2: A recurrent network with no outputs", "fe2c8925-e10a-48e9-9d3c-f3439bfb976c": "His \ufb01rst learning program was completed in 1955 and was demonstrated on television in 1956. Later versions of the program achieved good, though not expert, playing skill. Samuel was attracted to game-playing as a domain for studying machine learning because games are less complicated than problems \u201ctaken from life\u201d while still allowing fruitful study of how heuristic procedures and learning can be used together. He chose to study checkers instead of chess because its relative simplicity made it possible to focus more strongly on learning. Samuel\u2019s programs played by performing a lookahead search from each current position. They used what we now call heuristic search methods to determine how to expand the search tree and when to stop searching. The terminal board positions of each search were evaluated, or \u201cscored,\u201d by a value function, or \u201cscoring polynomial,\u201d using linear function approximation. In this and other respects Samuel\u2019s work seems to have been inspired by the suggestions of Shannon", "0d41cbae-b5e3-4d80-b990-991754332b3f": "Note that this is not a probabilistic graphical model because the nodes represent individual states of variables, while each variable corresponds to a column of such states in the diagram.\n\nFor each state of a given variable, there is a unique state of the previous variable that maximizes the probability (ties are broken either systematically or at random), corresponding to the function \u03c6(xn) given by (8.101), and this is indicated by the lines connecting the nodes. Once we know the most probable value of the \ufb01nal node xN, we can then simply follow the link back to \ufb01nd the most probable state of node xN\u22121 and so on back to the initial node x1. This corresponds to propagating a message back down the chain using and is known as back-tracking. Note that there could be several values of xn\u22121 all of which give the maximum value in (8.101). Provided we chose one of these values when we do the back-tracking, we are assured of a globally consistent maximizing con\ufb01guration. In Figure 8.53, we have indicated two paths, each of which we shall suppose corresponds to a global maximum of the joint probability distribution", "2cb2b9fe-a60c-4522-94d4-66da36620abc": "MACHINE LEARNING BASICS  Underfitting zone Overfitting zone  Generalization  error Variance  Optimal Capacity capacity  Figure 5.6: As capacity increases (a-axis), bias (dotted) tends to decrease and variance (dashed) tends to increase, yielding another U-shaped curve for generalization error (bold  https://www.deeplearningbook.org/contents/ml.html    curve).\n\nIf we vary capacity along one axis, there is an optimal capacity, with underfitting when the capacity is below this optimum and overfitting when it is above. This relationship is similar to the relationship between capacity, underfitting, and overfitting, discussed in  section 5.2 and figure 5.3.  error is measured by the MSE (where bias and variance are meaningful components of generalization error), increasing capacity tends to increase variance and decrease bias. This is illustrated in figure 5.6, where we see again the U-shaped curve of generalization error as a function of capacity. 5.4.5 Consistency  So far we have discussed the properties of various estimators for a training set of fixed size. Usually, we are also concerned with the behavior of an estimator as the amount of training data grows", "90603369-2f8c-4499-a94c-934a67abe77a": "Thus maximizing (10.6) is equivalent to minimizing the Kullback-Leibler contributions, he formulated the modern theory of the function, he developed (together with Lagrange) the calculus of variations, and he discovered the formula ei\u03c0 = \u22121, which relates four of the most important numbers in mathematics. During the last 17 years of his life, he was almost totally blind, and yet he produced nearly half of his results during this period. divergence, and the minimum occurs when qj(Zj) = \ufffdp(X, Zj).\n\nThus we obtain a general expression for the optimal solution q\u22c6 j (Zj) given by It is worth taking a few moments to study the form of this solution as it provides the basis for applications of variational methods. It says that the log of the optimal solution for factor qj is obtained simply by considering the log of the joint distribution over all hidden and visible variables and then taking the expectation with respect to all of the other factors {qi} for i \u0338= j. The additive constant in (10.9) is set by normalizing the distribution q\u22c6 j (Zj)", "7ab61bab-2eb1-470e-afba-0715540a8e3f": "Crucially, notice that the gradient update is applied to \u03b8 through the log \u03c0\u03b8 term which explicitly involves the Q\u03b8-values of all tokens a in the vocabulary. This shows an important difference from the above vanilla training in conventional Q-learning (\u00a72) where Q\u03b8 is updated only through the particular at token. The PCL training thus offers more ef\ufb01cient updates for the Q\u03b8 function. In the appendix (\u00a7A.3.1), we also discuss the difference from the MLE objective. Intuitively, MLE trains the model to (blindly) increase the probability of the observed tokens, while PCL encourages the (log) probability of the tokens to match the approximate advantage values. Multi-step PCL for Sparse Reward. The above PCL objective Eq. (7) alone does not resolve the potential instability issue due to the bootstrapped V\u00af\u03b8(st+1) value and the sparse reward (i.e., r(st, at) = 0 for t < T)", "1760438d-073b-404e-b677-5950e86752dc": "Given a sentence, EDA randomly chooses and applies one of four simple operations:  Synonym replacement (SR): Replace n random non-stop words with their synonyms. Random insertion (RI): Place a random synonym of a randomly selected non-stop word in the sentence at a random position. Random swap (RS): Randomly swap two words and repeat n times. Random deletion (RD): Randomly delete each word in the sentence with probability p.  where p = aandn = a@ X sentence_length, with the intuition that longer sentences can absorb more noise while maintaining the original label. The hyperparameter a roughly indicates the percent of words in one sentence that may be changed by one augmentation. EDA is shown to improve the classification accuracy on several classification benchmark datasets compared to baseline without EDA. The performance lift is more significant on a smaller training set", "23d39bc1-7d27-41f1-a49e-98d6d417a741": "In the next few sections we will consider extensions of it that do guarantee stability. A very similar series of steps can be followed to derive the o\u21b5-policy eligibility traces for action-value methods and corresponding general Sarsa(\u03bb) algorithms. One could start with either recursive form for the general action-based \u03bb-return, (12.19) or (12.20), but the latter (the Expected Sarsa form) works out to be simpler. We extend (12.20) to the where \u00afVt(St+1) is as given by (12.21).\n\nAgain the \u03bb-return can be written approximately as the sum of TD errors, using the expectation form of the action-based TD error: Exercise 12.10 Prove that (12.27) becomes exact if the value function does not change. To save writing, consider the case of t = 0, and use the notation Qk = \u02c6q(Sk, Ak, w)", "46e36539-2125-46e7-8b7d-92d69afdafe7": "A critical point is a point with zero slope.\n\nSuch a point can either be a local minimum, which is lower than the neighboring points; a local maximum, which is higher than the neighboring points; or a saddle point, which has neighbors that are both higher and lower than the point itself. so it is not possible to increase f(x) by making infinitesimal steps. Some critical points are neither maxima nor minima. These are known as saddle points. See figure 4.2 for examples of each type of critical point. A point that obtains the absolute lowest value of f(x) is a global minimum. There can be only one global minimum or multiple global minima of the function. It is also possible for there to be local minima that are not globally optimal. In the  ed 1 : aroot c at a1 1 1 1  https://www.deeplearningbook.org/contents/numerical.html    COLLEXL OL GCCp leary, WE OPLUIUZe LULICLIOUS Lal Way Lave Wally local UW1tdita that are not optimal and many saddle points surrounded by very flat regions", "d4224afa-8e2a-45f6-b2c5-77676c13f59f": "Proofs of this theorem can be found for example in Rosenblatt , Block , Nilsson , Minsky and Papert , Hertz et al. , and Bishop . Note, however, that the number of steps required to achieve convergence could still be substantial, and in practice, until convergence is achieved, we will not be able to distinguish between a nonseparable problem and one that is simply slow to converge. Even when the data set is linearly separable, there may be many solutions, and which one is found will depend on the initialization of the parameters and on the order of presentation of the data points. Furthermore, for data sets that are not linearly separable, the perceptron learning algorithm will never converge. classes (red and blue) in a two-dimensional feature space (\u03c61, \u03c62). The top left plot shows the initial parameter vector w shown as a black arrow together with the corresponding decision boundary (black line), in which the arrow points towards the decision region which classi\ufb01ed as belonging to the red class.\n\nThe data point circled in green is misclassi\ufb01ed and so its feature vector is added to the current weight vector, giving the new decision boundary shown in the top right plot", "44cf6e75-547a-4d2f-bce5-52d82d91af8f": "In the state case, our \ufb01nal de\ufb01nition of the \u03bb-return generalizes (12.18), after the model b(At|St) is the usual single-step importance sampling ratio. Much like the other returns we have seen in this book, the truncated version of this return can be approximated simply in terms of sums of the state-based TD error, with the approximation becoming exact if the approximate value function does not change. Exercise 12.8 Prove that (12.24) becomes exact if the value function does not change. To save writing, consider the case of t = 0, and use the notation Vk .= \u02c6v(Sk,w). \u21e4 Exercise 12.9 The truncated version of the general o\u21b5-policy return is denoted G\u03bbs The above form of the \u03bb-return (12.24) is convenient to use in a forward-view update, which to the experienced eye looks like an eligibility-based TD update\u2014the product is like an eligibility trace and it is multiplied by TD errors. But this is just one time step of a forward view", "59bc0e8e-8896-4e7e-ac39-45731a288388": "Once again, we see the same form arising for the gradient as was found for the sum-of-squares error function with the linear model and the cross-entropy error for the logistic regression model, namely the product of the error (ynj \u2212 tnj) times the basis function \u03c6n. Again, we could use this to formulate a sequential algorithm in which patterns are presented one at a time, in which each of the weight vectors is updated using (3.22).\n\nWe have seen that the derivative of the log likelihood function for a linear regression model with respect to the parameter vector w for a data point n took the form of the \u2018error\u2019 yn \u2212 tn times the feature vector \u03c6n. Similarly, for the combination of logistic sigmoid activation function and cross-entropy error function (4.90), and for the softmax activation function with the multiclass cross-entropy error function (4.108), we again obtain this same simple form. This is an example of a more general result, as we shall see in Section 4.3.6. To \ufb01nd a batch algorithm, we again appeal to the Newton-Raphson update to obtain the corresponding IRLS algorithm for the multiclass problem", "2484da27-a90a-41af-a512-32168df55397": "However, for the purposes of \ufb01nding a good density model, it is irrelevant because any of the equivalent solutions is as good as any other. Maximizing the log likelihood function (9.14) for a Gaussian mixture model turns out to be a more complex problem than for the case of a single Gaussian. The dif\ufb01culty arises from the presence of the summation over k that appears inside the logarithm in (9.14), so that the logarithm function no longer acts directly on the Gaussian.\n\nIf we set the derivatives of the log likelihood to zero, we will no longer obtain a closed form solution, as we shall see shortly. One approach is to apply gradient-based optimization techniques (Fletcher, 1987; Nocedal and Wright, 1999; Bishop and Nabney, 2008). Although gradient-based techniques are feasible, and indeed will play an important role when we discuss mixture density networks in Chapter 5, we now consider an alternative approach known as the EM algorithm which has broad applicability and which will lay the foundations for a discussion of variational inference techniques in Chapter 10. An elegant and powerful method for \ufb01nding maximum likelihood solutions for models with latent variables is called the expectation-maximization algorithm, or EM algorithm", "de82771e-6c2d-4b62-a535-9190515ec69d": "If a function h(\u03bb) is constant, and we change variables to \u03bb = \u03b72, then \ufffdh(\u03b7) = h(\u03b72) will also be constant. However, if we choose the density p\u03bb(\u03bb) to be constant, then the density of \u03b7 will be given, from (1.27), by and so the density over \u03b7 will not be constant. This issue does not arise when we use maximum likelihood, because the likelihood function p(x|\u03bb) is a simple function of \u03bb and so we are free to use any convenient parameterization. If, however, we are to choose a prior distribution that is constant, we must take care to use an appropriate representation for the parameters. Here we consider two simple examples of noninformative priors . First of all, if a density takes the form then the parameter \u00b5 is known as a location parameter. This family of densities exhibits translation invariance because if we shift x by a constant to give \ufffdx = x + c, then p(\ufffdx|\ufffd\u00b5) = f(\ufffdx \u2212 \ufffd\u00b5) (2.233) where we have de\ufb01ned \ufffd\u00b5 = \u00b5 + c", "0fee47bf-7ef2-46da-9a26-21829a0f248b": "OPTIMIZATION FOR TRAINING DEEP MODELS  Algorithm 8.3 Stochastic gradient descent (SGD) with Nesterov momentum  Require: Learning rate \u00ab, momentum parameter a Require: Initial parameter 0, initial velocity v while stopping criterion not met do Sample a minibatch of m examples from the training set {a), a (m) with corresponding labels y\u2122. Apply interim update: 6+ O+av. Compute gradient (at interim point): g <\u2014 4V6 LF (@ 6), y\u00ae). Compute velocity update: v \u00ab+ av \u2014 eg. Apply update: 0+ 6+ v. end while  deep learning models are usually iterative and thus require the user to specify some initial point from which to begin the iterations.\n\nMoreover, training deep models is a sufficiently difficult task that most algorithms are strongly affected by the choice of initialization. The initial point can determine whether the algorithm converges at all, with some initial points being so unstable that the algorithm encounters numerical difficulties and fails altogether", "0cd1a30d-35a6-4674-8f2c-5c191c4dcddf": "Given this batch of data, what would you say are the optimal predictions, the best values for the estimates V (A) and V (B)?\n\nEveryone would probably agree that the optimal value for V (B) is 3 out of the eight times in state B the process terminated immediately with a return of 1, and the other two times in B the process terminated immediately with a return of 0. But what is the optimal value for the estimate V (A) given this data? Here there are One way of viewing this answer is that it is based on \ufb01rst modeling the Markov process, in this case as shown to the right, and then computing the correct estimates given the model, which indeed in this case gives V (A) = 3 The other reasonable answer is simply to observe that we have seen A once and the return that followed it was 0; we therefore estimate V (A) as 0. This is the answer that batch Monte Carlo methods give. Notice that it is also the answer that gives minimum squared error on the training data. In fact, it gives zero error on the data. But still we expect the \ufb01rst answer to be better. If the process is Markov, we expect that the \ufb01rst answer will produce lower error on future data, even though the Monte Carlo answer is better on the existing data", "41a92669-a039-4ff2-9124-34acd51560d2": "Furthermore, (7.31) together with \u00b5n \u2a7e 0 implies an \u2a7d C. We therefore have to minimize (7.32) with respect to the dual variables {an} subject to for n = 1, . , N, where (7.33) are known as box constraints. This again represents a quadratic programming problem. If we substitute (7.29) into (7.1), we see that predictions for new data points are again made by using (7.13). We can now interpret the resulting solution. As before, a subset of the data points may have an = 0, in which case they do not contribute to the predictive model (7.13). The remaining data points constitute the support vectors. These have an > 0 and hence from (7.25) must satisfy If an < C, then (7.31) implies that \u00b5n > 0, which from (7.28) requires \u03ben = 0 and hence such points lie on the margin", "7222823a-5cae-4995-b8fa-36ec8d0901d1": "The result was that if a position that had already been encountered were to occur again as a terminal position of a search tree, the depth of the search was e\u21b5ectively ampli\ufb01ed because this position\u2019s stored value cached the results of one or more searches conducted earlier. One initial problem was that the program was not encouraged to move along the most direct path to a win. Samuel gave it a \u201ca sense of direction\u201d by decreasing a position\u2019s value a small amount each time it was backed up a level (called a ply) during the minimax analysis.\n\n\u201cIf the program is now faced with a choice of board positions whose scores di\u21b5er only by the ply number, it will automatically make the most advantageous choice, choosing a low-ply alternative if winning and a high-ply alternative if losing\u201d (Samuel, 1959, p. 80). Samuel found this discounting-like technique essential to successful learning. Rote learning produced slow but continual improvement that was most e\u21b5ective for opening and endgame play", "6a071c4c-42f6-4ed7-abba-ea926efd7d42": "All of them are capable of interacting with the environment and only candidates with high fitness scores can survive (only the fittest can survive in a competition for limited resources). A new generation is then created by recombining the settings (gene mutation) of high-fitness survivors. This process is repeated until the new solutions are good enough. Very different from the popular MDP-based approaches as what we have introduced above, ES aims to learn the policy parameter 6 without value approximation. Let's assume the distribution over the parameter 0 is an isotropic multivariate Gaussian with mean yp and fixed covariance o*I. The gradient of F(8) is calculated:  Vek gin (u,02)F (9)  =Vo | F (8) Pr(@) Pr(.) is the Gaussian density function. 8 V, Pr(6) = | F(6)Pr(\u00e9   \u2014 6 _ 2 =Eo.v(u0?) [F(6)Vo( \u2014log V 210? \u2014 ( = )|", "78acf956-d30b-4537-bb1c-c82aefaeec8a": "In Chapter 10 we describe how to use recurrent neural networks to define such models over sequences, and in part II] we describe advanced techniques for modeling arbitrary probability distributions. 6.3. Hidden Units  So far we have focused our discussion on design choices for neural networks that are common to most parametric machine learning models trained with gradient- based optimization. Now we turn to an issue that is unique to feedforward neural  187  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  https://www.deeplearningbook.org/contents/mlp.html    networks: how to choose the type of hidden unit to use in the hidden layers of the model. The design of hidden units is an extremely active area of research and does not yet have many definitive guiding theoretical principles. Rectified linear units are an excellent default choice of hidden unit. Many other types of hidden units are available. It can be difficult to determine when to use which kind (though rectified linear units are usually an acceptable choice).\n\nWe describe here some of the basic intuitions motivating each type of hidden unit. These intuitions can help decide when to try out which unit. Predicting in advance which will work best is usually impossible", "ff06d960-dee7-48a5-bf95-1e19630aae5f": "The backup diagram on the left graphically represents the Bellman optimality equation (3.19) and the backup diagram on the right graphically represents (3.20). For \ufb01nite MDPs, the Bellman optimality equation for v\u21e4 (3.19) has a unique solution. The Bellman optimality equation is actually a system of equations, one for each state, so if there are n states, then there are n equations in n unknowns. If the dynamics p of the environment are known, then in principle one can solve this system of equations for v\u21e4 using any one of a variety of methods for solving systems of nonlinear equations. One can solve a related set of equations for q\u21e4. Once one has v\u21e4, it is relatively easy to determine an optimal policy. For each state s, there will be one or more actions at which the maximum is obtained in the Bellman optimality equation. Any policy that assigns nonzero probability only to these actions is an optimal policy. You can think of this as a one-step search.\n\nIf you have the optimal value function, v\u21e4, then the actions that appear best after a one-step search will be optimal actions", "c991e64c-2081-4338-8cbe-e3d542acc737": "First End-to-End System for Data Programming Snorkel is the \ufb01rst system to implement our recent work on data programming .\n\nPrevious ML systems that we and others developed  required extensive feature engineering and model speci\ufb01cation, leading to confusion about where to injectrelevantdomainknowledge.Whileprogrammingweak supervision seems super\ufb01cially similar to feature engineering, we observe that users approach the two processes very differently. Our vision\u2014weak supervision as the sole port of interaction for machine learning\u2014implies radically different work\ufb02ows, requiring a proof of concept. Snorkel demonstrates that this paradigm enables users to develop high-quality models for a wide range of tasks. We report on two deployments of Snorkel, in collaboration with the US Department of Veterans Affairs and Stanford Hospital and Clinics, and the US Food and Drug Administration, where Snorkel improves over heuristic baselines by an average 110%. We also report results on four open-source datasets that are representative of other Snorkel deployments, including bioinformatics, medical image analysis, and crowdsourcing; on which Snorkel beats heuristics by an average 153% and comes within an average 3.60% of the predictive performance of large hand-curated training sets", "9aabe03f-e087-4504-970c-7ccf5dcaacc3": "We noted in Chapter 7 that each n-step return, for n \u2265 1, is a valid update target for a tabular learning update, just as it is for an approximate SGD learning update such as (9.7). Now we note that a valid update can be done not just toward any n-step return, but 2Gt:t+4. Any set of n-step returns can be averaged in this way, even an in\ufb01nite set, as long as the weights on the component returns are positive and sum to 1. The composite return possesses an error reduction property similar to that of individual n-step returns (7.3) and thus can be used to construct updates with guaranteed convergence properties. Averaging produces a substantial new range of algorithms.\n\nFor example, one could average one-step and in\ufb01nite-step returns to obtain another way of interrelating TD and Monte Carlo methods. In principle, one could even average experience-based updates with DP updates to get a simple combination of experience-based and model-based methods (cf. Chapter 8). An update that averages simpler component updates is called a compound update", "73a41394-9f2f-417d-92e4-8625b5f2f216": "Such a procedure can be motivated from a frequentist perspective by considering the trade-off between bias and variance, which decomposes the erSection 3.2 ror due to a model into the bias component that arises from differences between the model and the true function to be predicted, and the variance component that represents the sensitivity of the model to the individual data points. Recall from Figure 3.5 that when we trained multiple polynomials using the sinusoidal data, and then averaged the resulting functions, the contribution arising from the variance term tended to cancel, leading to improved predictions. When we averaged a set of low-bias models (corresponding to higher order polynomials), we obtained accurate predictions for the underlying sinusoidal function from which the data were generated. In practice, of course, we have only a single data set, and so we have to \ufb01nd a way to introduce variability between the different models within the committee. One approach is to use bootstrap data sets, discussed in Section 1.2.3.\n\nConsider a regression problem in which we are trying to predict the value of a single continuous variable, and suppose we generate M bootstrap data sets and then use each to train a separate copy ym(x) of a predictive model where m = 1, . , M", "515d8eb4-8299-40fb-a656-0e92c33b9308": "Supervised Contrastive Loss  aims to leverage label information more effectively than cross entropy, imposing that normalized embeddings from the same class are closer together than embeddings from different classes.\n\nAnchor Negatives Anchor Negatives  \u00e9  Positive Ta  ie  Self Supervised Contrastive Supervised Contrastive  Given a set of randomly sampled n (image, label) pairs, {x;, y;}\" 2n training pairs can be  i=l! created by applying two random augmentations of every sample, {x;, Gi}7r,  Supervised contrastive loss L utilizes multiple positive and negative samples, very similar to  supcon  soft nearest-neighbor loss: --\u00a5 Se en expen IN |= 1 do Hive ker kyi XP(B 4/7)  where Z;, = P(E(2)), in which E(. ) is an encoder network (augmented image mapped to vector) P(. ) is a projection network (one vector mapped to another). N; = {j \u20ac I: 9; = gi} contains a set of indices of samples with label y;", "ff2310a9-6369-41e8-821e-db8e3bf9d284": "This can be interpreted from a representation learning point of view as saying that we believe the learning problem consists of discovering a set of underlying factors of variation that can in turn be described in terms of other, simpler underlying factors of variation. Alternately, we can interpret the use of a deep architecture as expressing a belief that the function we want to learn is a computer program consisting of multiple steps, where each step makes use of the previous step\u2019s output.\n\nThese intermediate outputs are not necessarily factors of variation but can instead be analogous to counters or pointers that the network uses to organize its internal processing. Empirically, greater depth does seem to result in better generalization for a wide variety of tasks . See figure 6.6 and figure 6.7 for examples of some of these empirical results. These results suggest that using deep architectures does indeed express a useful prior over the space of functions the model learns. 6.4.2 Other Architectural Considerations  So far we have described neural networks as being simple chains of layers, with the main considerations being the depth of the network and the width of each layer. In practice, neural networks show considerably more diversity. 197  CHAPTER 6", "6759671a-67c9-4bbf-84fb-38265019a9c6": "(18.40)  ZO) =7Z(Oa) =  https://www.deeplearningbook.org/contents/partition.html   Z(OA)  A simple way to estimate the partition function is to use a Monte Carlo method such as simple importance sampling.\n\nWe present the approach in terms of continuous variables using integrals, but it can be readily applied to discrete variables by replacing the integrals with summation. We use a proposal distribution  po(x) = Z po(x), which supports tractable sampling and tractable evaluation o both the partition function Zp and the unnormalized distribution fg (x). Z\\ = [nn (x) dx 18.41 pol) = pi(x) dx 18.42 cae =% [roc Bee dx, 18.43 Po(x) Z, =% y Pulx) (k) 18.44 1= a (ky s.t. :X ~ po. K at Bo(x (*)) In the last line, we make a Monte Carlo estimator, Z 1, of the integral using samples drawn from po(x), and then weight each sample with the ratio of the unnormalized  p, and the proposal pp", "c3cae251-3d2e-44d8-a721-7324837e2e6e": "Though certainly not infallible (indeed, possibly detrimental in environments that di\u21b5er in certain ways from ancestral environments), this compensates for many of our limitations: our limited sensory abilities, the limited time over which we can learn, and the risks involved in \ufb01nding a healthy diet through personal experimentation. Similarly, because an animal cannot observe its own evolutionary \ufb01tness, that objective function does not work as a reward signal for learning. Evolution instead provides reward signals that are sensitive to observable predictors of evolutionary \ufb01tness.\n\nFinally, remember that a reinforcement learning agent is not necessarily like a complete organism or robot; it can be a component of a larger behaving system. This means that reward signals may be in\ufb02uenced by things inside the larger behaving agent, such as motivational states, memories, ideas, or even hallucinations. Reward signals may also depend on properties of the learning process itself, such as measures of how much progress learning is making. Making reward signals sensitive to information about internal factors such as these makes it possible for an agent to learn how to control the \u201ccognitive architecture\u201d of which it is a part, as well as to acquire knowledge and skills that would be di\ufb03cult to learn from a reward signal that depended only on external events", "366b7bc8-b209-4d16-95a9-4a21f565e1b3": "That is why the early results  are specialized to particular parametrizations, where g(f(x)) \u2014 @ may be obtained by taking the derivative of another function. Kamyshanska and Memisevic  generalized the results of Vincent  by identifying a family of shallow autoencoders such that g(f(a)) \u2014 x corresponds to a score for all members of the family. So far we have described only how the denoising autoencoder learns to represent a probability distribution. More generally, one may want to use the autoencoder as a generative model and draw samples from this distribution. This is described in section 20.11. 14.5.1.1 Historical Perspective  The idea of using MLPs for denoising dates back to the work of LeCun  and Gallinari et al. Behnke  also used recurrent networks to denoise images", "ad8fa8ec-53b6-401b-9937-942a91bd085e": "To understand why drawing samples from an energy-based model is difficult, consider an EBM over just two variables, defining a distribution p(a, b). In order to sample a, we must draw a from p(a| b), and in order to sample b, we must draw it from p(b | a). It seems to be an intractable chicken-and-egg problem. Directed models avoid this because their graph is directed and acyclic. To perform ancestral sampling, one simply samples each of the variables in topological order, conditioning on each variable\u2019s parents, which are guaranteed to have already been  592  CHAPTER 17. MONTE CARLO METHODS  sampled (section 16.3). Ancestral sampling defines an efficient, single-pass method of obtaining a sample. In an EBM, we can avoid this chicken-and-egg problem by sampling using a Markov chain. The core idea of a Markov chain is to have a state a that begins as an arbitrary value. Over time, we randomly update x repeatedly. Eventually x becomes (very nearly) a fair sample from p(a)", "ebcf2958-4efa-4a4c-b4d8-626b6b976d19": "8.10 Abramson\u2019s  expected-outcome model is a rollout algorithm applied to twoperson games in which the play of both simulated players is random.\n\nHe argued that even with random play, it is a \u201cpowerful heuristic\u201d that is \u201cprecise, accurate, easily estimable, e\ufb03ciently calculable, and domain-independent.\u201d Tesauro and Galperin  demonstrated the e\u21b5ectiveness of rollout algorithms for improving the play of backgammon programs, adopting the term \u201crollout\u201d from its use in evaluating backgammon positions by playing out positions with di\u21b5erent randomly generating sequences of dice rolls. Bertsekas, Tsitsiklis, and Wu  examine rollout algorithms applied to combinatorial optimization problems, and Bertsekas  surveys their use in discrete deterministic optimization problems, remarking that they are \u201coften surprisingly e\u21b5ective.\u201d 8.11 The central ideas of MCTS were introduced by Coulom  and by Kocsis and Szepesv\u00b4ari . They built upon previous research with Monte Carlo planning algorithms as reviewed by these authors", "0018ff19-6dfe-4762-90ce-75b14ac59280": "Because this is a polynomial of order M in \u03bbi, it must have M solutions (though these need not all be distinct). The rank of A is equal to the number of nonzero eigenvalues. Of particular interest are symmetric matrices, which arise as covariance matrices, kernel matrices, and Hessians. Symmetric matrices have the property that Aij = Aji, or equivalently AT = A. The inverse of a symmetric matrix is also symmetric, as can be seen by taking the transpose of A\u22121A = I and using AA\u22121 = I together with the symmetry of I. In general, the eigenvalues of a matrix are complex numbers, but for symmetric matrices the eigenvalues \u03bbi are real. This can be seen by \ufb01rst left multiplying (C.29) by (u\u22c6 i )T, where \u22c6 denotes the complex conjugate, to give Next we take the complex conjugate of (C.29) and left multiply by uT i to give where we have used A\u22c6 = A because we consider only real matrices A", "9febcd80-7b3c-4e72-8474-dbbe77705a66": "This particular insight is used throughout classical machine learning to derive large M-step updates.\n\nIn the context of deep learning, most models are too complex to admit a tractable solution for an optimal large M-step update, so this second  insight, which is more unique to the EM algorithm, is rarely used. 19.3. MAP Inference and Sparse Coding  We usually use the term inference to refer to computing the probability distribution over one set of variables given another. When training probabilistic models with latent variables, we are usually interested in computing p(h | v). An alternative form of inference is to compute the single most likely value of the missing variables, rather than to infer the entire distribution over their possible values. In the context  633  CHAPTER 19. APPROXIMATE INFERENCE  of latent variable models, this means computing  h* = arg max p(h | v). (19.9) h  This is known as maximum a posteriori inference, abbreviated as MAP inference. MAP inference is usually not thought of as approximate inference\u2014it does compute the exact most likely value of h*", "60469fca-996c-4a13-86b3-3fe829a8dcd3": "In order to formulate a variational treatment of this model, we next write down the joint distribution of all of the random variables, which is given by in which the various factors are de\ufb01ned above. The reader should take a moment to verify that this decomposition does indeed correspond to the probabilistic graphical model shown in Figure 10.5. Note that only the variables X = {x1, . , xN} are observed. We now consider a variational distribution which factorizes between the latent variables and the parameters so that It is remarkable that this is the only assumption that we need to make in order to obtain a tractable practical solution to our Bayesian mixture model. In particular, the functional form of the factors q(Z) and q(\u03c0, \u00b5, \u039b) will be determined automatically by optimization of the variational distribution. Note that we are omitting the subscripts on the q distributions, much as we do with the p distributions in (10.41), and are relying on the arguments to distinguish the different distributions. The corresponding sequential update equations for these factors can be easily derived by making use of the general result (10.9)", "af68f820-678d-4525-9b4f-2426bd23a216": "Whereas before we considered random training examples of the form St 7! Ut, now we consider examples of the form St, At 7! Ut. The update target Ut can be any approximation of q\u21e1(St, At), including the usual backed-up values such as the full Monte Carlo return (Gt) or any of the n-step Sarsa returns (7.4). The general gradient-descent update for action-value For example, the update for the one-step Sarsa method is We call this method episodic semi-gradient one-step Sarsa.\n\nFor a constant policy, this method converges in the same way that TD(0) does, with the same kind of error bound (9.14). To form control methods, we need to couple such action-value prediction methods with techniques for policy improvement and action selection. Suitable techniques applicable to continuous actions, or to actions from large discrete sets, are a topic of ongoing research with as yet no clear resolution. On the other hand, if the action set is discrete and not too large, then we can use the techniques already developed in previous chapters", "36f5869f-68d1-4306-acb4-a5a893a72d07": "Reinforcement learning has features in common with Hull\u2019s theory, which included eligibility-like mechanisms and secondary reinforcement to account for the ability to learn when there is a signi\ufb01cant time interval between an action and the consequent reinforcing stimulus (see Section 14.4). Randomness also played a role in Hull\u2019s theory through what he called \u201cbehavioral oscillation\u201d to introduce exploratory behavior. Skinner did not fully subscribe to the memory aspect of the Law of E\u21b5ect. Being averse to the idea of associative linkages, he instead emphasized selection from spontaneouslyemitted behavior.\n\nHe introduced the term \u201coperant\u201d to emphasize the key role of an action\u2019s e\u21b5ects on an animal\u2019s environment. Unlike the experiments of Thorndike and others, which consisted of sequences of separate trials, Skinner\u2019s operant conditioning experiments allowed animal subjects to behave for extended periods of time without interruption. He invented the operant conditioning chamber, now called a \u201cSkinner box,\u201d the most basic version of which contains a lever or key that an animal can press to obtain a reward, such as food or water, which would be delivered according to a well-de\ufb01ned rule, called a reinforcement schedule", "39684007-f9ee-4ef7-b4cb-3eeaf395d9b5": "These classes are imbalanced and the CycleGAN is used as a method of intelligent oversampling. CycleGANs learned an unpaired image-to-image translation between domains. An example of the domains in this problem is neutral to disgust. The CycleGAN learns to translate an image representing a neutral image into an image representing the dis-  gust emotion (Figs. 19, 20). Shorten and Khoshgoftaar J Big Data  6:60  neutral AES tar BEY angry \"WR disgust FES) ad ER happy fq suse 9  Generated images Coe sere qeu s HIG REE ERa s Sb ARES ert tt BSS =) BERR: \u201cPOT EPE: :  Fig.\n\n19 Some examples of synthetic data created with CycleGANs for emotion classification  Classification  t  Original Data Supplementary Data  Reference class Target class  1 == Domain T  F Leye \u2014\u00bb FG(R))  See F(T) \u00ab\u201d  LR Fig. 20 Architecture overview: G and F consist of two separate GANs composing the CycleGAN", "7e67cef3-1e96-4b9c-b936-3bdab8420fa6": "First we take a closer look at the exact performance gradient: where Bt, called the baseline , can be any scalar that does not depend on x.\n\nWe can include a baseline here without changing the equality because the gradient sums to zero over all the actions, P probabilities go up and some go down, but the sum of the changes must be zero because the sum of the probabilities is always one. Next we multiply each term of the sum by \u21e1t(x)/\u21e1t(x): The equation is now in the form of an expectation, summing over all possible values x of the random variable At, then multiplying by the probability of taking those values. Thus: Recall that our plan has been to write the performance gradient as an expectation of something that we can sample on each step, as we have just done, and then update on each step proportional to the sample. Substituting a sample of the expectation above for the performance gradient in (2.13) yields: which you may recognize as being equivalent to our original algorithm (2.12). We have just shown that the expected update of the gradient bandit algorithm is equal to the gradient of expected reward, and thus that the algorithm is an instance of stochastic gradient ascent. This assures us that the algorithm has robust convergence properties", "1610f422-1b65-4a65-815a-f2ca5924b0a3": "If we use a line search, we can search only over step sizes \u20ac that yield new \u00abx points that are feasible, or we can project each point on the line back into the constraint region. When possible, this method can be made more efficient by projecting the gradient into the tangent space of the feasible region before taking the step or beginning the line search . A more sophisticated approach is to design a different, unconstrained opti- mization problem whose solution can be converted into a solution to the original,  https://www.deeplearningbook.org/contents/numerical.html    constrained optimization problem. For example, if we want to minimize f (2) for  91  CHAPTER 4. NUMERICAL COMPUTATION  x \u20ac R? with \u00ab constrained to have exactly unit L? norm, we can instead minimize g(9) = f ({cos@, sin 6] ') with respect to 0, then return  as the solution to the original problem.\n\nThis approach requires creativity; the transformation between optimization problems must be designed specifically for each case we encounter. The Karush\u2014Kuhn\u2014Tucker (KKT) approach! provides a very general so- lution to constrained optimization", "04f5ce5c-891a-481c-a127-ce8c13f76f8c": "The name score matching comes from terminology in which the derivatives of a log density with respect to its argument, Vz log p(a), are called its score.\n\nThe strategy used by score matching is to minimize the expected squared difference between the derivatives of the model\u2019s log density with respect to the input and the derivatives of the data\u2019s log density with respect to the input:  1 La, i) = D) ||Vaz log Pmodel (X; 8), \u2014Va log paata(x)||3, (18.22) 1 J(0) = 7D Fpaata(@)E(@, 9), (18.23) Or = min J(0). (18.24)  This objective function avoids the difficulties associated with differentiating  615  https://www.deeplearningbook.org/contents/partition.html    CHAPTER 18. CONFRONTING THE PARTITION FUNCTION  the partition function Z because Z is not a function of a and therefore VxZ = 0. Initially, score matching appears to have a new difficulty: computing the score of the data distribution requires knowledge of the true distribution generating the training data, paata", "5612171a-d41a-475a-a009-2d976b018838": "The result (2.126) will clearly give the same answer as the batch result (2.121) because the two formulae are equivalent. However, we will not always be able to derive a sequential algorithm by this route, and so we seek a more general formulation of sequential learning, which leads us to the Robbins-Monro algorithm. Consider a pair of random variables \u03b8 and z governed by a joint distribution p(z, \u03b8).\n\nThe conditional expectation of z given \u03b8 de\ufb01nes a deterministic function f(\u03b8) that is given by and is illustrated schematically in Figure 2.10. Functions de\ufb01ned in this way are called regression functions. Our goal is to \ufb01nd the root \u03b8\u22c6 at which f(\u03b8\u22c6) = 0. If we had a large data set of observations of z and \u03b8, then we could model the regression function directly and then obtain an estimate of its root. Suppose, however, that we observe values of z one at a time and we wish to \ufb01nd a corresponding sequential estimation scheme for \u03b8\u22c6", "57a6a812-3c36-46c2-996a-83147f96611f": "There is a long history of pre-training general language representations, and we brie\ufb02y review the most widely-used approaches in this section. Learning widely applicable representations of words has been an active area of research for decades, including non-neural  and neural  methods. Pre-trained word embeddings are an integral part of modern NLP systems, offering signi\ufb01cant improvements over embeddings learned from scratch . To pretrain word embedding vectors, left-to-right language modeling objectives have been used , as well as objectives to discriminate correct from incorrect words in left and right context . These approaches have been generalized to coarser granularities, such as sentence embeddings  or paragraph embeddings . To train sentence representations, prior work has used objectives to rank candidate next sentences , left-to-right generation of next sentence words given a representation of the previous sentence , or denoising autoencoder derived objectives .\n\nELMo and its predecessor (Peters et al., 2017, 2018a) generalize traditional word embedding research along a different dimension. They extract context-sensitive features from a left-to-right and a right-to-left language model", "5dc11f7b-a28f-4dd5-b4e7-4de5f03cc247": "Design considerations for image Data Augmentation This section will briefly describe some additional design decisions with respect to Data  Augmentation techniques on image data.\n\nTest-time augmentation  In addition to augmenting training data, many research reports have shown the effec- tiveness of augmenting data at test-time as well. This can be seen as analogous to ensem- ble learning techniques in the data space. By taking a test image and augmenting it in the same way as the training images, a more robust prediction can be derived. This comes at a computational cost depending on the augmentations performed, and it can restrict the speed of the model. This could be a very costly bottleneck in models that require real-time prediction. However, test-time augmentation is a promising practice for appli- cations such as medical image diagnosis. Radosavovic et al. denote test-time aug- mentation as data distillation to describe the use of ensembled predictions to get a better representation of the image. Wang et al. sought out to develop a mathematical framework to formulate test- time augmentation", "141d8454-6417-46e6-8759-374489f128de": "alld ''4 1s Lue number of the training examples.\n\nComparing the log-likelihood with the mean squared error,  m MSEtrain = = Selig = yO, (5.66) i=1 we immediately see that maximizing the log-likelihood with respect to w yields the same estimate of the parameters w as does minimizing the mean squared error. The two criteria have different values but the same location of the optimum. This justifies the use of the MSE as a maximum likelihood estimation procedure. As we will see, the maximum likelihood estimator has several desirable properties. 5.5.2 Properties of Maximum Likelihood  The main appeal of the maximum likelihood estimator is that it can be shown to be the best estimator asymptotically, as the number of examples m \u2014 oo, in terms of its rate of convergence as m increases. Under appropriate conditions, the maximum likelihood estimator has the property of consistency (see section 5.4.5), meaning that as the number of training examples approaches infinity, the maximum likelihood estimate of a parameter converges to the true value of the parameter. These conditions are as follows:  e The true distribution paata must lie within the model family pmodel(-; 9)", "7b0e49f8-59b6-4537-a707-bad2cfef67e7": "This rapid parameter update can be achieved by its internal architecture or controlled by another meta-learner model. Memory-Augmented Neural Networks  A family of model architectures use external memory storage to facilitate the learning process of neural networks, including Neural Turing Machines and Memory Networks. With an explicit storage buffer, it is easier for the network to rapidly incorporate new information and not to forget in the future. Such a model is known as MANN, short for \u201cMemory-Augmented Neural Network\u201d. Note that recurrent neural networks with only internal memory such as vanilla RNN or LSTM are not MANNSs. Because MANN is expected to encode new information fast and thus to adapt to new tasks after only a few samples, it fits well for meta-learning. Taking the Neural Turing Machine (NTM) as the base model, Santoro et al. proposed a set of modifications on the training setup and the memory retrieval mechanisms (or \u201caddressing mechanisms\", deciding how to assign attention weights to memory vectors)", "3a081a51-7d8a-4aa1-ab8d-b09605432872": "In the dual formulation, we determine the parameter vector a by inverting an N \u00d7 N matrix, whereas in the original parameter space formulation we had to invert an M \u00d7 M matrix in order to determine w. Because N is typically much larger than M, the dual formulation does not seem to be particularly useful. However, the advantage of the dual formulation, as we shall see, is that it is expressed entirely in terms of the kernel function k(x, x\u2032).\n\nWe can therefore work directly in terms of kernels and avoid the explicit introduction of the feature vector \u03c6(x), which allows us implicitly to use feature spaces of high, even in\ufb01nite, dimensionality. The existence of a dual representation based on the Gram matrix is a property of many linear models, including the perceptron. In Section 6.4, we will develop a dualExercise 6.2 ity between probabilistic linear models for regression and the technique of Gaussian processes. Duality will also play an important role when we discuss support vector machines in Chapter 7. In order to exploit kernel substitution, we need to be able to construct valid kernel functions", "ccf8ef24-e35f-4725-95ff-b2f277169d3d": "Many forms of dimensionality reduction place semantically related examples near each other, as observed by Salakhutdinov and Hinton  and Torralba e\u00a2 al. .\n\nThe hints provided by the mapping to the lower-dimensional space aid generalization. One task that benefits even more than usual from dimensionality reduction is information retrieval, the task of finding entries in a database that resemble a query entry. This task derives the usual benefits from dimensionality reduction that other tasks do, but also derives the additional benefit that search can become extremely efficient in certain kinds of low-dimensional spaces. Specifically, if we train the dimensionality reduction algorithm to produce a code that is low- dimensional and binary, then we can store all database entries in a hash table that maps binary code vectors to entries. This hash table allows us to perform information retrieval by returning all database entries that have the same binary code as the query. We can also search over slightly less similar entries very efficiently, just by flipping individual bits from the encoding of the query. This approach to information retrieval via dimensionality reduction and binarization is called semantic hashing  and has been applied to both textual input  and  522  CHAPTER 14", "e7c1ea40-ac2a-4ac6-9f00-4013e39f6f16": "God and Golem, Inc: A Comment on Certain Points where Cybernetics Wiewiora, E. Potential-based shaping and Q-value initialization are equivalent. Journal analysis. Technical Report ICS 8605. Institute for Cognitive Science, University of California at San Diego, La Jolla. NU-CCS-87-3. College of Computer Science, Northeastern University, Boston. Williams, R. J. On the use of backpropagation in associative reinforcement learning. In Proceedings of the IEEE International Conference on Neural Networks, pp. I263\u2013I270. IEEE San Diego section and IEEE TAB Neural Network Committee. reinforcement learning. Machine Learning, 8(3-4):229\u2013256. Williams, R. J., Baird, L. C. A mathematical analysis of actor\u2013critic architectures for learning optimal controls through incremental dynamic programming. In Proceedings of the Sixth Yale Workshop on Adaptive and Learning Systems, pp. 96\u2013101. Center for Systems Science, Dunham Laboratory, Yale University, New Haven", "c850eced-8b0c-4d0b-a414-e8f62157fa7b": "The additional term \u03b3V (St) \u2212 V (St\u22121) is the higher-order reinforcement part of \u03b4t\u22121, and even if reward occurs (Rt 6= 0), the TD error can be silent if the reward is fully predicted (which is fully explained in Section 15.6 below).\n\nA closer look at Olds\u2019 and Milner\u2019s 1954 paper, in fact, reveals that it is mainly about the reinforcing e\u21b5ect of electrical stimulation in an instrumental conditioning task. Electrical stimulation not only energized the rats\u2019 behavior\u2014through dopamine\u2019s e\u21b5ect on motivation\u2014it also led to the rats quickly learning to stimulate themselves by pressing a lever, which they would do frequently for long periods of time. The activity of dopamine neurons triggered by electrical stimulation reinforced the rats\u2019 lever pressing. More recent experiments using optogenetic methods clinch the role of phasic responses of dopamine neurons as reinforcement signals. These methods allow neuroscientists to precisely control the activity of selected neuron types at a millisecond timescale in awake behaving animals", "2eaf9a6a-3a39-4f8f-b3a0-d9c52445fb60": "For example, the Monte Carlo update for value prediction is St 7! Gt, the TD(0) update is St 7!\n\nRt+1 +\u03b3\u02c6v(St+1,wt), and the n-step TD update is St 7! Gt:t+n. In the DP (dynamic programming) policy-evaluation update, s 7! E\u21e1, an arbitrary state s is updated, whereas in the other cases the state encountered in actual experience, St, is updated. It is natural to interpret each update as specifying an example of the desired input\u2013 output behavior of the value function. In a sense, the update s 7! u means that the estimated value for state s should be more like the update target u. Up to now, the actual update has been trivial: the table entry for s\u2019s estimated value has simply been shifted a fraction of the way toward u, and the estimated values of all other states were left unchanged. Now we permit arbitrarily complex and sophisticated methods to implement the update, and updating at s generalizes so that the estimated values of many other states are changed as well", "5b1d30d8-2496-4f59-ac98-8c64c9192eb3": "We could therefore proceed to formulate and solve complicated probabilistic models purely by algebraic manipulation. However, we shall \ufb01nd it highly advantageous to augment the analysis using diagrammatic representations of probability distributions, called probabilistic graphical models. These offer several useful properties: 1.\n\nThey provide a simple way to visualize the structure of a probabilistic model and can be used to design and motivate new models. 2. Insights into the properties of the model, including conditional independence properties, can be obtained by inspection of the graph. 3. Complex computations, required to perform inference and learning in sophisticated models, can be expressed in terms of graphical manipulations, in which underlying mathematical expressions are carried along implicitly. A graph comprises nodes (also called vertices) connected by links (also known as edges or arcs). In a probabilistic graphical model, each node represents a random variable (or group of random variables), and the links express probabilistic relationships between these variables. The graph then captures the way in which the joint distribution over all of the random variables can be decomposed into a product of factors each depending only on a subset of the variables", "83824e39-d107-4ddc-af34-16199e924d51": "As mentioned in Section 4.4, a natural form of experience that has encoded the concept is a pretrained sentiment classi\ufb01er (SC). The \ufb01rst experience function can then be de\ufb01ned as fsc(x, a, y) = SC(a, y), which evaluates the log likelihood of the transferred sentence y possessing the sentiment a. The higher value the y achieves, the higher quality it is considered in light of the experience. The second desideratum requires the model to reconstruct as much of the input text x as possible.\n\nWe combine the second experience fdata(x, a, y|D) (Equation 4.2) de\ufb01ned by a set of simple reconstruction data instances D = {(x\u2217, a\u2217 = ax\u2217, y\u2217 = x\u2217)}, where the target sentiment a\u2217 is set to the sentiment of the original sentence x\u2217, and by the problem de\ufb01nition the ground-truth output y\u2217 is exact the same as the input x\u2217. Such data thus carry the information of preserving the input content", "cb1d70a8-0a80-437f-b31c-10f2986b0790": "The best value function for this purpose is not necessarily the best for minimizing VE. Nevertheless, it is not yet clear what a more useful alternative goal for value prediction might be. For now, we will focus on VE.\n\nAn ideal goal in terms of VE would be to \ufb01nd a global optimum, a weight vector w\u21e4 for which VE(w\u21e4) \uf8ff VE(w) for all possible w. Reaching this goal is sometimes possible for simple function approximators such as linear ones, but is rarely possible for complex function approximators such as arti\ufb01cial neural networks and decision trees. Short of this, complex function approximators may seek to converge instead to a local optimum, a weight vector w\u21e4 for which VE(w\u21e4) \uf8ff VE(w) for all w in some neighborhood of w\u21e4. Although this guarantee is only slightly reassuring, it is typically the best that can be said for nonlinear function approximators, and often it is enough. Still, for many cases of interest in reinforcement learning there is no guarantee of convergence to an optimum, or even to within a bounded distance of an optimum", "0bbbd4e0-e757-4451-8903-369288095c3f": "is simply reconstruction log-probability on the visible units, just as for denoising autoencoders.\n\nThis is achieved by clamping x(\u00b0) = & to the observed example and maximizing the probability of generating x at some subsequent time steps, that is, maximizing log p(x\u201d) =z | h(*)), where h*) is sampled from the chain, given x9) =a. In order to estimate the gradient of log p(x) = a | h)) with respect to the other pieces of the model, Bengio ef al. use the reparametrization trick, introduced in section 20.9. The walk-back training procedure (described in section 20.11.3) was used  to improve training convergence of GSNs. 20.12.1 Discriminant GSNs  The original formulation of GSNs  was meant for unsupervised learning and implicitly modeling p(x) for observed data x, but it is possible to modify the framework to optimize p(y | x). For example, Zhou and Troyanskaya  generalize GSNs in this way, by only back-propagating the reconstruction log-probability over the output variables, keeping the input variables fixed", "a0476129-faf3-4c54-9c42-bbfa77155816": "PRACTICAL METHODOLOGY  determine whether the underlying software is correctly implemented. Some clues can be obtained from the training and test errors. If training error is low but test error is high, then it is likely that that the training procedure works correctly, and the model is overfitting for fundamental algorithmic reasons. An alternative possibility is that the test error is measured incorrectly because of a problem with saving the model after training then reloading it for test set evaluation, or because the test data was prepared differently from the training data. If both training and test errors are high, then it is difficult to determine whether there is a software defect or whether the model is underfitting due to fundamental algorithmic reasons. This scenario requires further tests, described next. Fit a tiny dataset: If you have high error on the training set, determine whether it is due to genuine underfitting or due to a software defect.\n\nUsually even small models can be guaranteed to be able fit a sufficiently small dataset. For example, a classification dataset with only one example can be fit just by setting the biases of the output layer correctly", "7e0108e2-e09e-4cd5-8eb0-6fffc29a90a1": "This requires evaluation of the second derivatives of the log posterior, which is equivalent to \ufb01nding the Hessian matrix.\n\nBecause we seek a Gaussian representation for the posterior distribution, it is natural to begin with a Gaussian prior, which we write in the general form where t = (t1, . , tN)T. Taking the log of both sides, and substituting for the prior distribution using (4.140), and for the likelihood function using (4.89), we obtain where yn = \u03c3(wT\u03c6n). To obtain a Gaussian approximation to the posterior distribution, we \ufb01rst maximize the posterior distribution to give the MAP (maximum posterior) solution wMAP, which de\ufb01nes the mean of the Gaussian. The covariance is then given by the inverse of the matrix of second derivatives of the negative log likelihood, which takes the form The Gaussian approximation to the posterior distribution therefore takes the form Having obtained a Gaussian approximation to the posterior distribution, there remains the task of marginalizing with respect to this distribution in order to make predictions", "d1b3102b-4a51-44e0-98a8-c2a83da524c5": "Both nonparametric approaches (such as nearest neighbor methods based on the estimated similarity between patterns of preferences) and parametric methods are possible.\n\nParametric methods often rely on learning a distributed representation (also called an embedding) for each user and for each item. Bilinear prediction of the target variable (such as a rating) is a simple parametric method that is highly successful and often found as a component of state-of-the-art systems. The prediction is obtained by the dot product between the user embedding and the item embedding (possibly corrected by constants that depend only on either the user ID or the item ID). Let R be the matrix containing our predictions, A a matrix with user embeddings in its rows, and B a matrix with item embeddings in its columns. Let b and c be vectors that contain respectively a kind of bias for each user (representing how grumpy or positive that user is in general) and for each item (representing its general popularity). The bilinear prediction is thus obtained as follows:  j  Typically one wants to minimize the squared error between predicted ratings Rui and actual ratings R,,;", "9aa3fbc1-9d59-4175-bb92-ec20c55173d9": "Even second-order optimization algorithms are expensive and usually require numerous approximations that prevent them from truly accounting for all significant second-order interactions. Building an n-th order optimization algorithm for n > 2 thus seems hopeless. What can we do instead? Batch normalization provides an elegant way of reparametrizing almost any deep network. The reparametrization significantly reduces the problem of coordinating updates across many layers.\n\nBatch normalization can be applied to any input or hidden layer in a network. Let H be a minibatch of activations of the layer to normalize, arranged as a design matrix, with the activations for each example appearing in a row of the matrix. To normalize H, we replace it with  wi (8.35)  Oo  where yp is a vector containing the mean of each unit and o is a vector containing the standard deviation of each unit. The arithmetic here is based on broadcasting  314  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  the vector yz and the vector o to be applied to every row of the matrix H", "76315a45-4e21-4dc5-98b3-480944b7ae3f": "We have introduced two graphical frameworks for representing probability distributions, corresponding to directed and undirected graphs, and it is instructive to discuss the relation between these. Consider \ufb01rst the problem of taking a model that is speci\ufb01ed using a directed graph and trying to convert it to an undirected graph. In some cases this is straightforward, as in the simple example in Figure 8.32. Here the joint distribution for the directed graph is given as a product of conditionals in the form Now let us convert this to an undirected graph representation, as shown in Figure 8.32. In the undirected graph, the maximal cliques are simply the pairs of neighbouring nodes, and so from (8.39) we wish to write the joint distribution in the form where we have absorbed the marginal p(x1) for the \ufb01rst node into the \ufb01rst potential function. Note that in this case, the partition function Z = 1. Let us consider how to generalize this construction, so that we can convert any distribution speci\ufb01ed by a factorization over a directed graph into one speci\ufb01ed by a factorization over an undirected graph", "7f323903-f3de-4aca-bc57-27b1b94fb750": "Proper data augmentation setup is critical for learning good and generalizable embedding features. It introduces the non-essential variations into examples without modifying semantic meanings and thus encourages the model to learn the essential part of the representation. For example, experiments in SimCLR showed that the composition of random cropping and random color distortion is crucial for good performance on learning visual representation of images. Large Batch Size  Using a large batch size during training is another key ingredient in the success of many contrastive learning methods (e.g. SimCLR, CLIP), especially when it relies on in-batch negatives.\n\nOnly when the batch size is big enough, the loss function can cover a diverse enough collection of negative samples, challenging enough for the model to learn meaningful representation to distinguish different examples. Hard Negative Mining  Hard negative samples should have different labels from the anchor sample, but have embedding features very close to the anchor embedding. With access to ground truth labels in supervised datasets, it is easy to identify task-specific hard negatives. For example when learning sentence embedding, we can treat sentence pairs labelled as \u201ccontradiction\u201d in NLI datasets as hard negative pairs", "d8af36c9-fdd6-48bd-aabe-3e6666dc001f": "LINEAR FACTOR MODELS  https://www.deeplearningbook.org/contents/linear_factors.html    we lay \u00a9 |S  a |S  |W] Gs |e  3  Ee] @ i t 3  Rp  S | SF] HIND |  Figure 13.2: Example samples and weights from a spike and slab sparse coding model trained on the MNIST dataset. (Left) The samples from the model do not resemble the training examples. At first glance, one might assume the model is poorly fit. (Right) The weight vectors of the model have learned to represent penstrokes and sometimes complete digits. The model has thus learned useful features. The problem is that the factorial prior over features results in random subsets of features being combined. Few such subsets are appropriate to form a recognizable MNIST digit. This motivates the development of generative models that have more powerful distributions over their latent codes. Figure reproduced with permission from Goodfellow et al. 495  CHAPTER 13", "3a67c276-b8d7-43ac-ae0b-be2c0b83c141": "Morgan Kaufmann. See also Technical Report CS-95-10, Brown University, Department of Computer Science, 1995. Degris, T., White, M., Sutton, R. S. O\u21b5-policy actor\u2013critic. In Proceedings of the 29th International Conference on Machine Learning . ArXiv:1205.4839, 2012. Denardo, E. V. Contraction mappings in the theory underlying dynamic programming. Dennett, D. C. Why the Law of E\u21b5ect Will Not Go Away. Brainstorms, pp. 71\u201389. Derthick, M. Variations on the Boltzmann machine learning algorithm. Carnegie-Mellon University Department of Computer Science Technical Report No. CMU-CS-84-120. Deutsch, J. A. A new type of behaviour theory. British Journal of Psychology. General Deutsch, J. A. A machine with insight. Quarterly Journal of Experimental Psychology, Dickinson, A", "d2db72a8-5dde-4770-b4f1-c366634b97f2": "This is appealing because TD(\u03bb) is often faster than GTD(\u03bb) when both algorithms converge, and TD(\u03bb) requires setting only a single step size. HTD(\u03bb) is de\ufb01ned by where \u03b2 > 0 again is a second step-size parameter.\n\nIn addition to the second set of weights, vt, HTD(\u03bb) also has a second set of eligibility traces, zb accumulating eligibility traces for the behavior policy and become equal to zt if all the \u21e2t are 1, which causes the last term in the wt update to be zero and the overall update to reduce to TD(\u03bb). Emphatic TD(\u03bb) is the extension of the one-step Emphatic-TD algorithm (Sections 9.11 and 11.8) to eligibility traces. The resultant algorithm retains strong o\u21b5-policy convergence guarantees while enabling any degree of bootstrapping, albeit at the cost of high variance and potentially slow convergence. Emphatic TD(\u03bb) is de\ufb01ned by where Mt \u2265 0 is the general form of emphasis, Ft \u2265 0 is termed the followon trace, and It \u2265 0 is the interest, as described in Section 11.8", "8bb732ba-edf0-436d-a5de-0e1c87da24f6": "We use a simple idealized version of this task, but we go into a lot more detail than is usual because we want to emphasize the theoretical basis of the parallel between TD errors and dopamine neuron activity. The \ufb01rst simplifying assumption is that the agent has already learned the actions required to obtain reward. Then its task is just to learn accurate predictions of future reward for the sequence of states it experiences. This is then a prediction task, or more technically, a policy-evaluation task: learning the value function for a \ufb01xed policy (Sections 4.1 and 6.1). The value function to be learned assigns to each state a value that predicts the return that will follow that state if the agent selects actions according to the given policy, where the return is the (possibly discounted) sum of all the future rewards", "4210e41d-b5cb-4085-bab4-5e7ab3986a73": "AAAI Press. O. Chapelle, B. Scholkopf, and Eds A. Zien. 2009. Semi-supervised learning  . IEEE Transactions on Neural Networks, 20(3):542\u2013542. Jiaao Chen, Dinghan Shen, Weizhu Chen, and Diyi Yang. 2021. Hiddencut: Simple data augmentation for natural language understanding with better generalization. In ACL. Jiaao Chen, Zhenghui Wang, Ran Tian, Zichao Yang, and Diyi Yang. 2020a.\n\nLocal additivity based data augmentation for semi-supervised NER. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1241\u20131251, Online. Association for Computational Linguistics. Jiaao Chen, Yuwei Wu, and Diyi Yang. 2020b. Semisupervised models via data augmentation for classifying interactive affective responses. In AffCon@ AAAI", "a19d2ae2-8709-4169-8937-8b56c62f3fc3": "For a \u2a7e 1 the density is everywhere \ufb01nite, and the special case of a = 1 is known as the exponential distribution.\n\nThe Gaussian is the most widely used distribution for continuous variables. It is also known as the normal distribution. In the case of a single variable x \u2208 (\u2212\u221e, \u221e) it is governed by two parameters, the mean \u00b5 \u2208 (\u2212\u221e, \u221e) and the variance \u03c32 > 0. The inverse of the variance \u03c4 = 1/\u03c32 is called the precision, and the square root of the variance \u03c3 is called the standard deviation. The conjugate prior for \u00b5 is the Gaussian, and the conjugate prior for \u03c4 is the gamma distribution. If both \u00b5 and \u03c4 are unknown, their joint conjugate prior is the Gaussian-gamma distribution. For a D-dimensional vector x, the Gaussian is governed by a D-dimensional mean vector \u00b5 and a D \u00d7 D covariance matrix \u03a3 that must be symmetric and The inverse of the covariance matrix \u039b = \u03a3\u22121 is the precision matrix, which is also symmetric and positive de\ufb01nite", "6a9b8294-26f2-40da-8dff-3fdba3057124": "However, in Section 3.1.5, we consider brie\ufb02y the modi\ufb01cations needed to deal with multiple target variables. In Chapter 1, we \ufb01tted polynomial functions to data sets by minimizing a sumof-squares error function. We also showed that this error function could be motivated as the maximum likelihood solution under an assumed Gaussian noise model. Let us return to this discussion and consider the least squares approach, and its relation to maximum likelihood, in more detail. As before, we assume that the target variable t is given by a deterministic function y(x, w) with additive Gaussian noise so that where \u03f5 is a zero mean Gaussian random variable with precision (inverse variance) \u03b2. Thus we can write Recall that, if we assume a squared loss function, then the optimal prediction, for a new value of x, will be given by the conditional mean of the target variable.\n\nIn the Section 1.5.5 case of a Gaussian conditional distribution of the form (3.8), the conditional mean Note that the Gaussian noise assumption implies that the conditional distribution of t given x is unimodal, which may be inappropriate for some applications", "cb5c0c32-fe91-463e-b857-7005e6bfa89e": "Or in continuous space:  https://lilianweng.github.io/posts/2018-02-19-rl-overview/  A (Long) Peek into Reinforcement Learning | Lil'Log  8) = Y74,,(8)Vaj(8) = Y> (day(s) S> m(als,6)Q,(s,)) scS scS acA  where d,,,(s) is stationary distribution of Markov chain for 7rg. If you are unfamiliar with the definition of a \u201cstationary distribution,\u201d please check this reference. Using gradient ascent we can find the best 8 that produces the highest return. It is natural to expect policy-based methods are more useful in continuous space, because there is an infinite number of actions and/or states to estimate the values for in continuous space and hence value- based approaches are computationally much more expensive. Policy Gradient Theorem  Computing the gradient numerically can be done by perturbing 8 by a small amount \u00a2 in the k-th dimension. It works even when J(@) is not differentiable (nice! ), but unsurprisingly very slow", "2f3d00d7-2403-499f-bfb9-89f3b236a678": "The component identity is an underlying explanatory factor, y.\n\nBecause the mixture components (e.g., natural object classes in image data) are statistically salient,  just modeling p(x) in an unsupervised way with no labeled example already reveals the factor y.  observing a training set of w values alone gives us no information about p(y | x). Next, let us see a simple example of how semi-supervised learning can succeed. Consider the situation where x arises from a mixture, with one mixture component per value of y, as illustrated in figure 15.4. If the mixture components are well separated, then modeling p(x) reveals precisely where each component is, and a single labeled example of each class will then be enough to perfectly learn p(y | x). But more generally, what could tie p(y | x) and p(x) together? If y is closely associated with one of the causal factors of x, then p(x) and  on od sad 1 1 ed 1 1 vous 1", "3f6a947b-8350-445c-a8cc-d129e8fc591b": "https://lilianweng.github.io/posts/2021-05-31-contrastive/  Contrastive Representation Learning | Lil'Log   Sentence as a L X d matrix  i 1 1 1 1 1 v ~  d d (a) Token Cutoff (b) Feature Cutoff (c) Span Cutoff  Multiple augmented versions of one sample can be created. When training, Shen et al. applied an additional KL-divergence term to measure the consensus between predictions from  different augmented samples. SimCSE (Gao et al. 2021; code) learns from unsupervised data by predicting a sentence from itself with only dropout noise. In other words, they treat dropout as data augmentation for text sequences. A sample is simply fed into the encoder twice with different dropout masks and these two versions are the positive pair where the other in-batch samples are considered as negative pairs. It feels quite similar to the cutoff augmentation, but dropout is more flexible with less well- defined semantic meaning of what content can be masked off", "8f07c125-174f-40bd-898c-0251c11b434c": "The combined energy function defines a joint distribution,  Pel, HOY, RO) =F exp{ Bel, WW) }, (20.46)  and a corresponding conditional distribution over the observations given h(\u201d and h\u00a9) as a multivariate Gaussian distribution:  pae(w | A, bO) = Ne ere | Sow. hl? | come |. (20.47) j  Note that the covariance matrix zh = (x, np Dp DT +I] : is nondiagonal and that W is the weight matrix associated with the Gaussian RBM modeling the conditional means.\n\nIt is difficult to train the mcRBM via contrastive divergence or persistent contrastive divergence because of its nondiagonal conditional covariance structure. CD and PCD require sampling from the joint distribution of x, h(\u2122,h\u00a9, which, in a standard RBM, is accomplished by Gibbs sampling over the conditionals. However, in the mcRBM, sampling from pme(x | A), hn) requires computing (C\u2122)-! at every iteration of learning. This can be an impractical computational burden for larger observations", "78709a34-dcc9-4f3b-ad73-54f6477cc206": "Using the sum and product rules of probability we can evaluate which can then be used in Bayes\u2019 theorem to calculate Thus the joint distribution is now expressed in terms of p(y) and p(x|y).\n\nFrom a graphical perspective, the joint distribution p(x, y) is now represented by the graph shown in Figure 8.37(c), in which the direction of the arrow is reversed. This is the simplest example of an inference problem for a graphical model. Now consider a more complex problem involving the chain of nodes of the form shown in Figure 8.32. This example will lay the foundation for a discussion of exact inference in more general graphs later in this section. Speci\ufb01cally, we shall consider the undirected graph in Figure 8.32(b). We have already seen that the directed chain can be transformed into an equivalent undirected chain. Because the directed graph does not have any nodes with more than one parent, this does not require the addition of any extra links, and the directed and undirected versions of this graph express exactly the same set of conditional independence statements", "ccff4ee8-633d-4351-8bd9-42df087f0d83": "Hinton, G. E. and D. van Camp . Keeping neural networks simple by minimizing the description length of the weights. In Proceedings of the Sixth Annual Conference on Computational Learning Theory, pp. 5\u201313. ACM. Hinton, G. E., M. Welling, Y. W. Teh, and S. Osindero . A new view of ICA. In Proceedings Hodgson, M. E. Reducing computational requirements of the minimum-distance classi\ufb01er. Remote Sensing of Environments 25, 117\u2013128. Hoerl, A. E. and R. Kennard . Ridge regression: biased estimation for nonorthogonal problems. Technometrics 12, 55\u201367. Hofmann, T. Learning the similarity of documents: an information-geometric approach to document retrieval and classi\ufb01cation. In S. A. Solla, T. K", "00392b36-af02-4d6a-a4e8-2d82d0a3eaef": "Formally, the ssRBM model is defined via its energy function: 1 E(x, 8,h) =\u2014 \u00bb a!W.ssihy + 5st) (s +4 \u00bb wh) x (20.50)  + 7 \u00bb ays? _ \u00bb ay pisihy \u2014 \u00bb bh; ty ips hi, (20.51)  where 0; is the offset of the spike h;, and A is a diagonal precision matrix on the observations \u00ab. The parameter a; > 0 is a scalar precision parameter for the real-valued slab variable s;.\n\nThe parameter \u00ae; is a nonnegative diagonal matrix that defines an h-modulated quadratic penalty on aw. Each pi; is a mean parameter for the slab variable s;. With the joint distribution defined via the energy function, deriving the ssRBM conditional distributions is relatively straightforward", "e8bc0281-0c0a-4ab7-a574-216c0e834241": "So far in this book, we have focussed primarily on sets of data points that were assumed to be independent and identically distributed (i.i.d.).\n\nThis assumption allowed us to express the likelihood function as the product over all data points of the probability distribution evaluated at each data point. For many applications, however, the i.i.d. assumption will be a poor one. Here we consider a particularly important class of such data sets, namely those that describe sequential data. These often arise through measurement of time series, for example the rainfall measurements on successive days at a particular location, or the daily values of a currency exchange rate, or the acoustic features at successive time frames used for speech recognition. An example involving speech data is shown in Figure 13.1. Sequential data can also arise in contexts other than time series, for example the sequence of nucleotide base pairs along a strand of DNA or the sequence of characters in an English sentence. For convenience, we shall sometimes refer to \u2018past\u2019 and \u2018future\u2019 observations in a sequence. However, the models explored in this chapter are equally applicable to all forms of sequential data, not just temporal sequences", "b4665b3f-807c-4d00-a34a-0629fbe8a438": "Note that this will be a normalized density provided f(x) is correctly normalized. The parameter \u03c3 is known as a scale parameter, and the density exhibits Exercise 2.59 scale invariance because if we scale x by a constant to give \ufffdx = cx, then where we have de\ufb01ned \ufffd\u03c3 = c\u03c3. This transformation corresponds to a change of scale, for example from meters to kilometers if x is a length, and we would like to choose a prior distribution that re\ufb02ects this scale invariance. If we consider an interval A \u2a7d \u03c3 \u2a7d B, and a scaled interval A/c \u2a7d \u03c3 \u2a7d B/c, then the prior should assign equal probability mass to these two intervals.\n\nThus we have \ufffd B and because this must hold for choices of A and B, we have and hence p(\u03c3) \u221d 1/\u03c3. Note that again this is an improper prior because the integral of the distribution over 0 \u2a7d \u03c3 \u2a7d \u221e is divergent. It is sometimes also convenient to think of the prior distribution for a scale parameter in terms of the density of the log of the parameter", "0a4256f5-7b8f-4f6f-80ea-e059ef43925e": "Participants In collaboration with the Mobilize Center , an NIH-funded Big Data to Knowledge (BD2K) center, we distributed a national call for applications to attend a two-day workshop on using Snorkel for biomedical knowledge base construction. Selection criteria included a strong biomedical project proposal and little-to-no prior experience using Snorkel.\n\nIn total, 15 researchers18 were invited to attend out of 33 team applications submitted, with varying backgrounds in bioinformatics, clinical informatics, and data mining from universities, companies, and organizations around the USA. The education demographics included 6 bachelors, 4 masters, and 5 Ph.D. degrees. All participants could program in Python, with 80% rating their skill as intermediate or better; 40% of participants had little-to-no prior exposure to machine learning; and 53-60% had no prior experience with text mining or information extraction applications (Table 8). 18 One participant declined to write labeling functions, so their score is not included in our analysis. Protocol The \ufb01rst day focused entirely on labeling functions, ranging from theoretical motivations to details of the Snorkel API. Over the course of 7 hours, participants were instructed in a classroom setting on how to use and evaluate models developed using Snorkel", "57b15e93-6467-46be-8833-6151547a0726": "Ashutosh Kumar, Satwik Bhattamishra, Manik Bhandari, and Partha Talukdar. 2019. Submodular optimization-based diverse paraphrasing and its effectiveness in data augmentation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3609\u20133619, Minneapolis, Minnesota. Association for Computational Linguistics. Varun Kumar, Ashutosh Choudhary, and Eunah Cho. 2020. Data augmentation using pre-trained transformer models. In Proceedings of the 2nd Workshop on Life-long Learning for Spoken Language Systems, pages 18\u201326, Suzhou, China. Association for Computational Linguistics. Samuli Laine and Timo Aila. 2017. Temporal ensembling for semi-supervised learning. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net", "27f4b8b4-9f49-4496-8989-40c5bb1013df": "12.1.6 Specialized Hardware Implementations of Deep Networks  Since the early days of neural networks research, hardware designers have worked on specialized hardware implementations that could speed up training and/or inference of neural network algorithms. See early and more recent reviews of specialized hardware for deep networks .\n\nDifferent forms of specialized hardware (Graf and Jackel, 1989; Mead and Ismail, 2012; Kim et al., 2009; Pham et al., 2012; Chen et al., 2014a,b) have been developed over the last decades with ASICs (application-specific integrated circuits), either with digital (based on binary representations of numbers), analog  (based on physical implementations of continuous values as voltages or currents), or hybrid implementations (combining digital and analog components). In recent years more flexible FPGA (field programmable gated array) implementations (where the particulars of the circuit can be written on the chip after it has been built) have been developed. Though software implementations on general-purpose processing units (CPUs and GPUs) typically use 32 or 64 bits of precision to represent floating-point numbers, it has long been known that it was possible to use less precision, at least at inference time", "eac093d3-33dc-42da-bd4b-5470fd42102a": "Exercise 5.10 In the new coordinate system, whose basis vectors are given by the eigenvectors {ui}, the contours of constant E are ellipses centred on the origin, as illustrated Exercise 5.11 in Figure 5.6. For a one-dimensional weight space, a stationary point w\u22c6 will be a minimum if The corresponding result in D-dimensions is that the Hessian matrix, evaluated at w\u22c6, should be positive de\ufb01nite. Exercise 5.12 As we shall see in Section 5.3, it is possible to evaluate the gradient of an error function ef\ufb01ciently by means of the backpropagation procedure.\n\nThe use of this gradient information can lead to signi\ufb01cant improvements in the speed with which the minima of the error function can be located. We can see why this is so, as follows. In the quadratic approximation to the error function, given in (5.28), the error surface is speci\ufb01ed by the quantities b and H, which contain a total of W(W + 3)/2 independent elements (because the matrix H is symmetric), where W is the Exercise 5.13 dimensionality of w (i.e., the total number of adaptive parameters in the network)", "05cc0b92-83f4-4ad7-bd36-6860389d7721": "The safety of a Data Augmentation method refers to its likelihood of preserving the label post-transformation. For example, rotations and flips are gener- ally safe on ImageNet challenges such as cat versus dog, but not safe for digit recogni- tion tasks such as 6 versus 9. A non-label preserving transformation could potentially strengthen the model\u2019s ability to output a response indicating that it is not confident about its prediction. However, achieving this would require refined labels  post-aug- mentation. If the label of the image after a non-label preserving transformation is some- thing like , the model could learn more robust confidence predictions. However, constructing refined labels for every non-safe Data Augmentation is a computationally expensive process. Due to the challenge of constructing refined labels for post-augmented data, it is important to consider the \u2018safety\u2019 of an augmentation.\n\nThis is somewhat domain depend- ent, providing a challenge for developing generalizable augmentation policies, (see Auto- Augment  for further exploration into finding generalizable augmentations). There is no image processing function that cannot result in a label changing transformation at some distortion magnitude", "5b890af8-1063-4ff6-9976-c0b8059583c5": "(2.72) d  Disregarding the constraint for the moment, we can simplify the Frobenius norm portion as follows: arg min||X \u2014 Xdd' ||} (2.73) d  48  CHAPTER 2", "eccf8fe9-1c3b-4bb7-b47e-de294e852989": "Usually if you cannot train a classifier to correctly label a single example, an autoencoder to successfully reproduce a single example with high fidelity, or a generative model to consistently emit samples resembling a single example, there is a software defect preventing successful optimization on the training set. This test can be extended to a small dataset with few examples. Compare back-propagated derivatives to numerical derivatives: If you are using a software framework that requires you to implement your own gradient com- putations, or if you are adding a new operation to a differentiation library and must define its bprop method, then a common source of error is implementing this  https://www.deeplearningbook.org/contents/guidelines.html    gradient expression incorrectly", "031c63f7-1406-45fd-89d8-c6bb0581bad8": ", xN}, and similarly we denote the latent variables by Z = {z1, . , zN}. From (9.10) we can write down the conditional distribution of Z, given the mixing coef\ufb01cients \u03c0, in the form Similarly, from (9.11), we can write down the conditional distribution of the observed data vectors, given the latent variables and the component parameters where \u00b5 = {\u00b5k} and \u039b = {\u039bk}. Note that we are working in terms of precision matrices rather than covariance matrices as this somewhat simpli\ufb01es the mathematics. Next we introduce priors over the parameters \u00b5, \u039b and \u03c0. The analysis is considerably simpli\ufb01ed if we use conjugate prior distributions.\n\nWe therefore choose a Section 10.4.1 Dirichlet distribution over the mixing coef\ufb01cients \u03c0 where by symmetry we have chosen the same parameter \u03b10 for each of the components, and C(\u03b10) is the normalization constant for the Dirichlet distribution de\ufb01ned by (B.23). As we have seen, the parameter \u03b10 can be interpreted as the effective Section 2.2.1 prior number of observations associated with each component of the mixture", "20b93678-b8b3-4d81-b5bc-3c99faceebd4": "Twenty contributors graded each tweet, but due to the dif\ufb01culty of the task and lack of crowdworker \ufb01ltering, there were many con\ufb02icts in worker labels. We represented each crowdworker as a labeling function\u2014showing Snorkel\u2019s ability to subsume existing 16 https://www.nlm.nih.gov/mesh/meshhome.html 17 https://www.crowd\ufb02ower.com/data/weather-sentiment/ crowdsourcing modeling approaches\u2014and then used the resulting labels to train a text model over the tweets, for making predictions independent of the crowd workers. An important question is the signi\ufb01cance of modeling the accuracies and correlations of the labeling functions on the end predictive performance of the discriminative model (versus in Sect. 3, where we only considered the effect on the accuracy of the generative model).\n\nWe compare Snorkel with a simpler pipeline that skips the generative modeling stage and trains the discriminative model on an unweighted average of the labeling functions\u2019 outputs. Table 6 shows that the discriminative model trained on Snorkel\u2019s probabilistic labels consistently predicts better, improving 5.81% on average", "2c811f24-d200-41a3-bf61-878a5dc26f88": "24 Adversarial autoencoder framework used in DOPING   GAN samples can be used as an oversampling technique to solve problems with class imbalance. Lim et al. show how GAN samples can be used for unsupervised anom- aly detection. By oversampling rare normal samples, which are samples that occur with small probability, GANs are able to reduce the false positive rate of anomaly detection. They do this using the Adversarial Autoencoder framework proposed by Makhzani et al. (Fig. 24). As exciting as the potential of GANs is, it is very difficult to get high-resolution out- puts from the current cutting-edge architectures. Increasing the output size of the images produced by the generator will likely cause training instability and non-conver- gence. Another drawback of GANs is that they require a substantial amount of data to train. Thus, depending on how limited the initial dataset is, GANs may not be a practical solution. Salimans et al. provide a more complete description of the problems with training GANs", "bd433915-74db-4ff1-8f91-d6ede954e5aa": "In collaboration with researchers and clinicians at the US Department ofVeteransAffairs,StanfordHospitalandClinics(SHC),and the Stanford Center for Biomedical Informatics Research, we used Snorkel to develop a system to extract structured data from unstructured EHR notes.\n\nSpeci\ufb01cally, the system\u2019s task was to extract mentions of pain levels at precise anatomical locations from clinician notes, with the goal of using these features to automatically assess patient well-being and detect complications after medical interventions like surgery. To this end, our collaborators created a cohort of 5800 patients from SHC EHR data, with visit dates between 1995 and 2015, resulting in 500 K unstructured clinical documents. Since distant supervision from a knowledge base is not applicable, we compared against regular expression-based labeling previously developed for this task. Chemical\u2013Disease Relations (CDR) We used the 2015 BioCreative chemical\u2013disease relation dataset , where the task is to identify mentions of causal links between chemicals and diseases in PubMed abstracts. We used all pairs of chemical and disease mentions co-occurring in a sentence as our candidate set", "837614db-26c1-4e81-9771-371a4332543d": "We \ufb01rst of all use the Gaussian mixture distribution to motivate the EM algorithm in a fairly informal way, and then we give a more careful treatment based on the latent variable viewpoint.\n\nWe shall Section 9.3 see that the K-means algorithm corresponds to a particular nonprobabilistic limit of EM applied to mixtures of Gaussians. Finally, we discuss EM in some generality. Section 9.4 Gaussian mixture models are widely used in data mining, pattern recognition, machine learning, and statistical analysis. In many applications, their parameters are determined by maximum likelihood, typically using the EM algorithm. However, as we shall see there are some signi\ufb01cant limitations to the maximum likelihood approach, and in Chapter 10 we shall show that an elegant Bayesian treatment can be given using the framework of variational inference. This requires little additional computation compared with EM, and it resolves the principal dif\ufb01culties of maximum likelihood while also allowing the number of components in the mixture to be inferred automatically from the data. We begin by considering the problem of identifying groups, or clusters, of data points in a multidimensional space. Suppose we have a data set {x1,", "a6f8fdd8-1c03-48f7-8e93-253deeea1c71": "In the same sense that traditional dropout is analogous to bagging, this approach is analogous to boosting. As intended, experiments with dropout boosting show almost no regularization effect compared to training the entire network as a single model. This demonstrates that the interpretation of dropout as bagging has value beyond the interpretation of dropout as robustness to noise. The regularization effect of the bagged ensemble is only achieved when the stochastically sampled ensemble members are trained to perform well independently of each other. Dropout has inspired other stochastic approaches to training exponentially large ensembles of models that share weights. DropConnect is a special case of dropout where each product between a single scalar weight and a single hidden unit state is considered a unit that can be dropped . Stochastic pooling is a form of randomized pooling (see section 9.3) for building ensembles of convolutional networks, with each convolutional network attending to different spatial locations of each feature map.\n\nSo far, dropout remains the most widely used implicit ensemble method. One of the key insights of dropout is that training a network with stochastic behavior and making predictions by averaging over multiple stochastic decisions implements a form of bagging with parameter sharing", "d2253bbe-4509-4318-817b-a7125e3771c6": "As mentioned previously, auto-regressive networks may be extended to process continuous-valued data. A particularly powerful and generic way of parametrizing a continuous density is as a Gaussian mixture (introduced in section 3.9.6) with mixture weights a; (the coefficient or prior probability for component i), per- component conditional mean jy; and per-component conditional variance o?. A model called RNADE  uses this parametrization to extend NADE  https://www.deeplearningbook.org/contents/generative_models.html    to real values. As with other mixture density networks, the parameters of this distribution are outputs of the network, with the mixture weight probabilities produced by a softmax unit, and the variances parametrized so that they are positive. Stochastic gradient descent can be numerically ill-behaved due to the interactions between the conditional means ju; and the conditional variances o?. To reduce this difficulty, Uria et al. use a pseudogradient that replaces the gradient on the mean, in the back-propagation phase.\n\nAnother very interesting extension of the neural auto-regressive architectures gets rid of the need to choose an arbitrary order for the observed variables", "a0dbb6d5-73b6-4f7d-b209-b4d47fc4a438": "The one-step return (target) is the same as that of Expected Sarsa, for t < T \u2212 1, and the two-step tree-backup return is \u21e1(a|St+1)Qt+n\u22121(St+1, a) + \u03b3\u21e1(At+1|St+1)Gt+1:t+n, (7.16) for t < T \u2212 1, n \u2265 2, with the n = 1 case handled by (7.15) except for GT \u22121:t+n .= RT . This target is then used with the usual action-value update rule from n-step Sarsa: Qt+n(St, At) .= Qt+n\u22121(St, At) + \u21b5  , for 0 \uf8ff t < T, while the values of all other state\u2013action pairs remain unchanged: Qt+n(s, a) = Qt+n\u22121(s, a), for all s, a such that s 6= St or a 6= At", "e934a26d-6ccb-4d4c-a3c7-c8ceeeb57acc": "Here Ex denotes the expectation of x under the conditional distribution p(x|y), with a similar notation for the conditional variance. 2.9 (\u22c6 \u22c6 \u22c6) www . In this exercise, we prove the normalization of the Dirichlet distribution (2.38) using induction. We have already shown in Exercise 2.5 that the beta distribution, which is a special case of the Dirichlet for M = 2, is normalized. We now assume that the Dirichlet distribution is normalized for M \u2212 1 variables and prove that it is normalized for M variables. To do this, consider the Dirichlet distribution over M variables, and take account of the constraint \ufffdM k=1 \u00b5k = 1 by eliminating \u00b5M, so that the Dirichlet is written and our goal is to \ufb01nd an expression for CM.\n\nTo do this, integrate over \u00b5M\u22121, taking care over the limits of integration, and then make a change of variable so that this integral has limits 0 and 1. By assuming the correct result for CM\u22121 and making use of (2.265), derive the expression for CM. Verify that this distribution is normalized, and \ufb01nd expressions for its mean and variance", "256b702a-5403-473c-ad86-964e991306a7": "Here the digit images have been turned into binary vectors by setting all elements whose values exceed 0.5 to 1 and setting the remaining elements to 0. We now \ufb01t a data set of N = 600 such digits, comprising the digits \u20182\u2019, \u20183\u2019, and \u20184\u2019, with a mixture of K = 3 Bernoulli distributions by running 10 iterations of the EM algorithm. The mixing coef\ufb01cients were initialized to \u03c0k = 1/K, and the parameters \u00b5kj were set to random values chosen uniformly in the range (0.25, 0.75) and then normalized to satisfy the constraint that \ufffd j \u00b5kj = 1.\n\nWe see that a mixture of 3 Bernoulli distributions is able to \ufb01nd the three clusters in the data set corresponding to the different digits. The conjugate prior for the parameters of a Bernoulli distribution is given by the beta distribution, and we have seen that a beta prior is equivalent to introducing additional effective observations of x", "4be8f101-0706-47de-814f-9dae4cab7eca": "It takes as input gradients of f\u2019s embedding loss for verification task.\n\nG,,: a neural network parameterized by v learning fast weights @* for the base learner g from its loss gradients. In MetaNet, the learner's loss gradients are viewed as the meta information of the task. Ok, now let's see how meta networks are trained. The training data contains multiple pairs of datasets: a support set S = {x/, y/}* | anda test set U = {x;, y;}4_,. Recall that we have four  networks and four sets of model parameters to learn, (9, od, W, v)", "b84cc958-9da8-4ff0-83f1-94fe290b95a9": "This probably explains why MP- DBMs may be trained jointly while DBMs require a greedy layer-wise pretraining. The disadvantage of back-propagating through the approximate inference graph is hat it does not provide a way to optimize the log-likelihood, but rather gives a heuristic approximation of the generalized pseudolikelihood. The MP-DBM inspired the NADE-k  extension to the NADE framework, which is described in section 20.10.10. The MP-DBM has some connections to dropout. Dropout shares the same pa- rameters among many different computational graphs, with the difference between  671  CHAPTER 20.\n\nDEEP GENERATIVE MODELS  https://www.deeplearningbook.org/contents/generative_models.html    Figure 20.5: An illustration of the multiprediction training process for a deep Boltzmann machine. Each row indicates a different example within a minibatch for the same training step. Each column represents a time step within the mean field inference process. For each example, we sample a subset of the data variables to serve as inputs to the inference process. These variables are shaded black to indicate conditioning", "b6590253-0258-475a-beba-331137d64584": "In a frequentist setting, w is considered to be a \ufb01xed parameter, whose value is determined by some form of \u2018estimator\u2019, and error bars on this estimate are obtained by considering the distribution of possible data sets D. By contrast, from the Bayesian viewpoint there is only a single data set D (namely the one that is actually observed), and the uncertainty in the parameters is expressed through a probability distribution over w. A widely used frequentist estimator is maximum likelihood, in which w is set to the value that maximizes the likelihood function p(D|w). This corresponds to choosing the value of w for which the probability of the observed data set is maximized. In the machine learning literature, the negative log of the likelihood function is called an error function. Because the negative logarithm is a monotonically decreasing function, maximizing the likelihood is equivalent to minimizing the error. One approach to determining frequentist error bars is the bootstrap , in which multiple data sets are created as follows. Suppose our original data set consists of N data points X = {x1,", "2e1c6d32-8c3f-4076-8197-f57c90727654": "To update the distribution (i.e., normalized weight vector) over the K experts at each time \u03c4, we treat the distribution as the target model p\u03b8 to be learned in the SE.\n\nIn other words, p\u03b8 is directly parameterized as the normalized weight vector p\u03b8 := \u03b8 = {\u03b8t}K t=1, where \u03b8t \u2265 0 is the probability of expert t, with \ufffdK t=1 \u03b8t = 1. Starting from the SE in Equation 3.2, and assuming D to be the cross entropy, H the Shannon entropy, and \u03b1 = \u03b2 > 0, we obtain the update rule of the target model at each time \u03c4 following the teacher-student mechanism in Equation 3.3. Speci\ufb01cally, the teacher step has: The subsequent student step is to minimize the cross entropy between the target model p\u03b8 and q(\u03c4+1) (see Equation 3.3). Given the de\ufb01nition of p\u03b8 as the vector of probabilities, the student step is equivalent to directly setting p\u03b8 to the vector of q(\u03c4+1)(t) values. Therefore: which is precisely the multiplicative weight update rule for the expert distribution/weights", "8bab147f-4cb4-4c77-b9fe-0aa84ad9473d": "Recall from Figure 2.5 that for \u03b10 < 1 the prior favours solutions in which some of the mixing coef\ufb01cients are zero. Figure 10.6 was obtained using \u03b10 = 10\u22123, and resulted in two components having nonzero mixing coef\ufb01cients. If instead we choose \u03b10 = 1 we obtain three components with nonzero mixing coef\ufb01cients, and for \u03b1 = 10 all six components have nonzero mixing coef\ufb01cients. As we have seen there is a close similarity between the variational solution for the Bayesian mixture of Gaussians and the EM algorithm for maximum likelihood. In fact if we consider the limit N \u2192 \u221e then the Bayesian treatment converges to the maximum likelihood EM algorithm. For anything other than very small data sets, the dominant computational cost of the variational algorithm for Gaussian mixtures arises from the evaluation of the responsibilities, together with the evaluation and inversion of the weighted data covariance matrices. These computations mirror precisely those that arise in the maximum likelihood EM algorithm, and so there is little computational overhead in using this Bayesian approach as compared to the traditional maximum likelihood one", "e742b0ad-79d5-41d5-8a68-3029c1e22ed8": "a distribution satisfying the conditional independence properties A \u22a5\u22a5 B | \u2205 and A \u0338\u22a5\u22a5 B | C. There is no corresponding undirected graph over the same three variables that is a perfect map. Conversely, consider the undirected graph over four variables shown in Figure 8.36. This graph exhibits the properties A \u0338\u22a5\u22a5 B | \u2205, C \u22a5\u22a5 D | A \u222a B and A \u22a5\u22a5 B | C \u222aD. There is no directed graph over four variables that implies the same set of conditional independence properties. The graphical framework can be extended in a consistent way to graphs that include both directed and undirected links. These are called chain graphs , and contain the directed and undirected graphs considered so far as special cases. Although such graphs can represent a broader class of distributions than either directed or undirected alone, there remain distributions for which even a chain graph cannot provide a perfect map. Chain graphs are not discussed further in this book.\n\nWe turn now to the problem of inference in graphical models, in which some of the nodes in a graph are clamped to observed values, and we wish to compute the posterior distributions of one or more subsets of other nodes", "d3286a7a-c572-4fec-bd93-96197a67ea18": "We compare our approach with a broad range of baselines, including (1) the standard MLE training (MLE); (2) MLE+reward, where we use the reward function to \ufb01lter examples; (3) joint MLE and PG training with MLE initialization (MLE+PG), where we initialize the model with MLE training, then train it with combined MLE and PG losses; previous text-generation RL algorithms including (4) MIXER , (5) Self-critic , and (6) one of the latest methods GOLD-s  which is a pure off-policy method based on importance-sampling PG.\n\nTo ablate the effect of multi-step training (\u00a73.2), we additionally compare with a simpli\ufb01ed variant of our approach that uses only vanilla single-step PCL training (SQL(single)). We include more baselines such as MLE weighted by rewards in \u00a7A.1.1. We evaluate generation results in terms of entailment rate, language quality (perplexity), and diversity which is measured by the Shannon entropy over unigrams and bigrams (H1, H2)", "3ebd6cb8-60d5-4f70-a6e3-0bd9c4f14750": "CONFRONTING THE PARTITION FUNCTION  The AIS sampling strategy is then to generate samples from po and use the transition operators to sequentially generate samples from the intermediate distributions until we arrive at samples from the target distribution p1:  e fork=1... K(k)  https://www.deeplearningbook.org/contents/partition.html    \u2014 Sample x7", "249046cc-691b-4c47-83b8-b791ae9cd2fe": "Can you imagine a scenario in which a TD update would be better on average than a Monte Carlo update? Give an example scenario\u2014a description of past experience and a current state\u2014in which you would expect the TD update to be better. Here\u2019s a hint: Suppose you have lots of experience driving home from work. Then you move to a new building and a new parking lot (but you still enter the highway at the same place).\n\nNow you are starting to learn predictions for the new building. Can you see why TD updates are likely to be much better, at least initially, in this case? Might the same sort of thing happen in the original scenario? \u21e4 TD methods update their estimates based in part on other estimates. They learn a guess from a guess\u2014they bootstrap. Is this a good thing to do? What advantages do TD methods have over Monte Carlo and DP methods? Developing and answering such questions will take the rest of this book and more. In this section we brie\ufb02y anticipate some of the answers. Obviously, TD methods have an advantage over DP methods in that they do not require a model of the environment, of its reward and next-state probability distributions", "1758da5c-40cd-4b7b-b8ed-1675eb74f03b": "Results Our key \ufb01nding is that labeling functions written in Snorkel, even by SME users, can match or exceed a traditional hand-labeling approach. The majority (8) of subjects matched or outperformed these hand-labeled data models. The average Snorkel user\u2019s score was 30.4 F1, and the average hand-supervision score was 20.9 F1. The best performing user model scored 48.7 F1, 19.2 points higher than the best supervised model using hand-labeled data. The worst participant scored 12.0 F1, 0.3 points higher that the lowest hand-labeled model. The full distribution of scores by participant, and broken down by participant background, compared against the baseline models trained with hand-labeled data are shown in Figs. 12, 13 and 14 respectively.\n\nAdditional Details We note that participants only needed to create a fairly small set of labeling functions to achieve the reported performances, writing a median of 10 labeling functions (with a minimum of 2, and a maximum of 15). In general, these labeling functions had simple form; for example, two from our user study: Fig. 12 Predictive performance attained by our 14 user study participants using Snorkel", "0146154b-3c63-479d-8682-6628ffe4df74": "In the related case of domain adaptation, the task (and the optimal input-to- output mapping) remains the same between each setting, but the input distribution is slightly different. For example, consider the task of sentiment analysis, which consists of determining whether a comment expresses positive or negative sentiment. Comments posted on the web come from many categories. A domain adaptation scenario can arise when a sentiment predictor trained on customer reviews of media content, such as books, videos and music, is later used to analyze comments about consumer electronics, such as televisions or smartphones.\n\nOne can imagine that there is an underlying function that tells whether any statement is positive, neutral, or negative, but of course the vocabulary and style may vary from one domain to another, making it more difficult to generalize across domains. Simple unsupervised pretraining (with denoising autoencoders) has been found to be very successful for sentiment analysis with domain adaptation . A related problem is that of concept drift, which we can view as a form of transfer learning due to gradual changes in the data distribution over time. 535  https://www.deeplearningbook.org/contents/representation.html    CHAPTER 15 REPRESENTATION LEARNING  Both concept drift and transfer learning can be viewed as particular forms of multitask learning", "a95382b2-1447-4d6b-8144-abe87cc804d4": "GAN-based synthetic medical image augmentation for increased CNN performance in liver lesion classification. arXiv preprint. 2018. 140. Changhee H, Hideaki H, Leonardo R, Ryosuke A, Wataru S, Shinichi M, Yujiro F, Giancarlo M, Hideki N. GAN-based synthetic brain mr image generation. In: 2018 IEEE 15th International Symposium on biomedical imaging (ISBI  18). IEEE, 2011. P. 734-8. Publisher\u2019s Note  Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Submit your manuscript to a SpringerOpen\u00ae journal and benefit from:  > Convenient online submission  > Rigorous peer review  > Open access: articles freely available online > High visibility within the field  > Retaining the copyright to your article  Submit your next manuscript at > springeropen.com", "8a5f99c4-d9a6-48c3-88bf-4cd980ece40e": ", this is prevented by tying the weights of f and g. Both f and g are standard neural network layers consisting of an affine transformation followed by  an element-wise nonlinearity, so it is straightforward to set the weight matrix of g to be the transpose of the weight matrix of f.  14.8 Predictive Sparse Decomposition  Predictive sparse decomposition (PSD) is a model that is a hybrid of sparse coding and parametric autoencoders . A parametric encoder is trained to predict the output of iterative inference.\n\nPSD has been applied to unsupervised feature learning for object recognition in images and video , as well as for audio . The model consists of an encoder f(a) and a decoder g(h) that are both parametric. During training, h is controlled by the optimization algorithm. Training proceeds by minimizing  Iz \u2014 g(h)|/? + Alli + 7\\|k \u2014 f(a). (14.19)  As in sparse coding, the training algorithm alternates between minimization with respect to A and minimization with respect to the model parameters", "8e61478f-afac-4d70-8218-8de9d6b69376": "Additionally, this model was pre-trained without the NSP task. This is directly comparable to OpenAI GPT, but using our larger training dataset, our input representation, and our \ufb01ne-tuning scheme. We \ufb01rst examine the impact brought by the NSP task. In Table 5, we show that removing NSP hurts performance signi\ufb01cantly on QNLI, MNLI, and SQuAD 1.1. Next, we evaluate the impact of training bidirectional representations by comparing \u201cNo NSP\u201d to \u201cLTR & No NSP\u201d. The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD. For SQuAD it is intuitively clear that a LTR model will perform poorly at token predictions, since the token-level hidden states have no rightside context. In order to make a good faith attempt at strengthening the LTR system, we added a randomly initialized BiLSTM on top. This does signi\ufb01cantly improve results on SQuAD, but the results are still far worse than those of the pretrained bidirectional models. The BiLSTM hurts performance on the GLUE tasks", "70a4e489-f5df-40d5-b2b0-0dfa9541e28e": "This approach dates back at least as far as the neocognitron .\n\nThe deep learning renaissance of 2006 began with the discovery that this greedy learning procedure could be used to find a good initialization for a joint learning procedure over all the layers, and that this approach could be used to successfully train even fully connected architectures (Hinton et al., 2006; Hinton  https://www.deeplearningbook.org/contents/representation.html    and Salakhutdinov, 2006; Hinton, 2006; Bengio et al., 2007; Ranzato et al., 2007a). Prior to this discovery, only convolutional deep networks or networks whose dept resulted from recurrence were regarded as feasible to train. Today, we now know  that greedy layer-wise pretraining is not required to train fully connected deep architectures, but the unsupervised pretraining approach was the first method to succeed. Greedy layer-wise pretraining is called greedy because it is a greedy algo-  526  CHAPTER 15. REPRESENTATION LEARNING  rithm, meaning that it optimizes each piece of the solution independently, one piece at a time, rather than jointly optimizing all pieces", "43b5f419-34b9-406d-ac8f-85d68cf7bfc1": "Because this richer model suffers a greater complexity penalty, the evidence actually falls in going from M = 1 to M = 2. When we go to M = 3 we obtain a signi\ufb01cant further improvement in data \ufb01t, as seen in Figure 1.4, and so the evidence is increased again, giving the highest overall evidence for any of the polynomials. Further increases in the value of M produce only small improvements in the \ufb01t to the data but suffer increasing complexity penalty, leading overall to a decrease in the evidence values. Looking again at Figure 1.5, we see that the generalization error is roughly constant between M = 3 and M = 8, and it would be dif\ufb01cult to choose between these models on the basis of this plot alone. The evidence values, however, show a clear preference for M = 3, since this is the simplest model which gives a good explanation for the observed data. Let us \ufb01rst consider the maximization of p(t|\u03b1, \u03b2) with respect to \u03b1. This can be done by \ufb01rst de\ufb01ning the following eigenvector equation From (3.81), it then follows that A has eigenvalues \u03b1+\u03bbi", "d7e56170-1e4c-4b17-8dbb-15e59ddd1409": "Experiments by Rifai et al. (2011la,b) show that training the CAE results in most singular values of J dropping below 1 in  519  CHAPTER 14. AUTOENCODERS  Input | Tangent vectors point  Contractive autoencoder  https://www.deeplearningbook.org/contents/autoencoders.html    Figure 14.10: Illustration of tangent vectors of the manifold estimated by local PCA and by a contractive autoencoder, The location on the manifold is defined \u2018by the input image of a dog drawn from the CIFAR-10 dataset. The tangent vectors are estimated  by the leading singular vectors of the Jacobian matrix oh of the input-to-code mapping. Although both local PCA and the CAE can capture local tangents, the CAE is able to form more accurate estimates from limited training data because it exploits parameter sharing across different locations that share a subset of active hidden units.\n\nThe CAE tangent directions typically correspond to moving or changing parts of the object (such as the head or legs). Images reproduced with permission from Rifai ef al. magnitude and therefore becoming contractive", "36ef8c71-35c6-49d8-839a-12b16c093240": "2.56 (\u22c6 \u22c6) www Express the beta distribution (2.13), the gamma distribution (2.146), and the von Mises distribution (2.179) as members of the exponential family (2.194) and thereby identify their natural parameters. 2.58 (\u22c6) The result (2.226) showed that the negative gradient of ln g(\u03b7) for the exponential family is given by the expectation of u(x). By taking the second derivatives of (2.195), show that 2.60 (\u22c6 \u22c6) www Consider a histogram-like density model in which the space x is divided into \ufb01xed regions for which the density p(x) takes the constant value hi over the ith region, and that the volume of region i is denoted \u2206i.\n\nSuppose we have a set of N observations of x such that ni of these observations fall in region i. Using a Lagrange multiplier to enforce the normalization constraint on the density, derive an expression for the maximum likelihood estimator for the {hi}. 2.61 (\u22c6) Show that the K-nearest-neighbour density model de\ufb01nes an improper distribution whose integral over all space is divergent", "4bf51632-130c-48e2-b183-16266b4e0c37": "To do this, show that the integral of the probability density over a thin shell of radius r and thickness \u03f5, where \u03f5 \u226a 1, is given by p(r)\u03f5 where where SD is the surface area of a unit sphere in D dimensions. Show that the function p(r) has a single stationary point located, for large D, at \ufffdr \u2243 \u221a which shows that \ufffdr is a maximum of the radial probability density and also that p(r) decays exponentially away from its maximum at \ufffdr with length scale \u03c3. We have already seen that \u03c3 \u226a \ufffdr for large D, and so we see that most of the probability mass is concentrated in a thin shell at large radius. Finally, show that the probability density p(x) is larger at the origin than at the radius \ufffdr by a factor of exp(D/2).\n\nWe therefore see that most of the probability mass in a high-dimensional Gaussian distribution is located at a different radius from the region of high probability density. This property of distributions in spaces of high dimensionality will have important consequences when we consider Bayesian inference of model parameters in later chapters", "db17f690-4fad-472d-b608-0e21137a7e1a": "When an infant plays, waves its arms, or looks about, it has no explicit teacher, but it does have a direct sensorimotor connection to its environment.\n\nExercising this connection produces a wealth of information about cause and e\u21b5ect, about the consequences of actions, and about what to do in order to achieve goals. Throughout our lives, such interactions are undoubtedly a major source of knowledge about our environment and ourselves. Whether we are learning to drive a car or to hold a conversation, we are acutely aware of how our environment responds to what we do, and we seek to in\ufb02uence what happens through our behavior. Learning from interaction is a foundational idea underlying nearly all theories of learning and intelligence. In this book we explore a computational approach to learning from interaction. Rather than directly theorizing about how people or animals learn, we primarily explore idealized learning situations and evaluate the e\u21b5ectiveness of various learning methods.1 That is, we adopt the perspective of an arti\ufb01cial intelligence researcher or engineer. We explore designs for machines that are e\u21b5ective in solving learning problems of scienti\ufb01c or economic interest, evaluating the designs through mathematical analysis or computational experiments", "67b1e4bd-3c01-4be9-b3f5-8eab06753c95": "Gradient-Based Optimization  Most deep learning algorithms involve optimization of some sort.\n\nOptimization refers to the task of either minimizing or maximizing some function f(a) by altering x. We usually phrase most optimization problems in terms of minimizing f(z). Maximization may be accomplished via a minimization algorithm by minimizing  \u2014 flr)  https://www.deeplearningbook.org/contents/numerical.html    Js The fun\u00a2tion we want to minimize or maximize is called the ob jective func- tion, or criterion, When we are minimizing it, we may also call it the cost function, loss function, or error function. In this book, we use these terms  interchangeably, though some machine learning publications assign special meaning to some of these terms. We often denote the value that minimizes or maximizes a function with a superscript *. For example, we might say \u00ab* = arg min f(a). 80  CHAPTER 4. NUMERICAL COMPUTATION  Global minimum at x = 0. Since f\u2019(x) = 0, gradient descent halts here", "7ccb6662-7103-46cf-bf32-0fc6709dec8f": "The complete graph is not very useful because it does not imply any independences. When we represent a probability distribution with a graph, we want to choose a graph that implies as many independences as possible, without implying any independences that do not actually exist.\n\nFrom this point of view, some distributions can be represented more efficiently using directed models, while other distributions can be represented more efficiently using undirected models. In other words, directed models can encode some independences that undirected models cannot encode, and vice versa. Directed models are able to use one specific kind of substructure that undirected models cannot represent perfectly. This substructure is called an immorality. The structure occurs when two random variables a and b are both parents of a third random variable c, and there is no edge directly connecting a and b in either direction", "ba2fcc5e-9880-4b0d-be20-1cdeb4fc164a": "The original algorithm proposed by Gatys et al. has a very slow running time and is therefore not practical for Data Augmentation. The algorithm developed by Johnson et al. is much faster, but limits transfer to a  pre-trained set of styles. Meta learning Data Augmentations  The concept of meta-learning in Deep Learning research generally refers to the concept of optimizing neural networks with neural networks. This approach has become very popular since the publication of NAS  from Zoph and Le. Real et al.\n\nalso Shorten and Khoshgoftaar J Big Data  6:60   Sample architecture A with probability p  Trains a child network The controller (RNN) with architecture  Ato get accuracy R  Compute gradient of p and scale it by R to update the controller  Fig. 28 Concept behind Neural Architecture Search   show the effectiveness of evolutionary algorithms for architecture search. Salimans et al. 111] directly compare evolutionary strategies with Reinforcement Learning. Another interesting alternative to Reinforcement Learning is simple random search", "a43e8a2e-8f41-4703-a6fd-9f9f6788ddeb": "Each episode begins in a randomly selected start state and ends when the car crosses the \ufb01nish line. The rewards are \u22121 for each step until the car crosses the \ufb01nish line. If the car hits the track boundary, it is moved back to a random start state, and the episode continues. A racetrack similar to the small racetrack on the left of Figure 5.5 has 9,115 states reachable from start states by any policy, only 599 of which are relevant, meaning that they are reachable from some start state via some optimal policy. (The number of relevant states was estimated by counting the states visited while executing optimal actions for 107 episodes.)\n\nThe table below compares solving this task by conventional DP and by RTDP. These results are averages over 25 runs, each begun with a di\u21b5erent random number seed. Conventional DP in this case is value iteration using exhaustive sweeps of the state set, with values updated one state at a time in place, meaning that the update for each state uses the most recent values of the other states (This is the Gauss-Seidel version of value iteration, which was found to be approximately twice as fast as the Jacobi version on this problem", "120bec1c-dc01-4b2c-bdf3-30b7ace4c9ac": "Either we can \ufb01x K and determine the value of V from the data, which gives rise to the K-nearest-neighbour technique discussed shortly, or we can \ufb01x V and determine K from the data, giving rise to the kernel approach. It can be shown that both the K-nearest-neighbour density estimator and the kernel density estimator converge to the true probability density in the limit N \u2192 \u221e provided V shrinks suitably with N, and K grows with N .\n\nWe begin by discussing the kernel method in detail, and to start with we take the region R to be a small hypercube centred on the point x at which we wish to determine the probability density. In order to count the number K of points falling within this region, it is convenient to de\ufb01ne the following function which represents a unit cube centred on the origin. The function k(u) is an example of a kernel function, and in this context is also called a Parzen window. From (2.247), the quantity k((x \u2212 xn)/h) will be one if the data point xn lies inside a cube of side h centred on x, and zero otherwise", "64dbd814-38ba-4167-b1d4-1bf13df93975": "From this we can see that back-propagation starting from g and back-propagation starting from g + dv diverge by 6J\u2122v after n steps of back-propagation. If v is chosen to be a unit eigenvector of J with eigenvalue \\, then multiplication by the Jacobian simply scales the difference at each step. The two executions of back-propagation are separated by a distance of 6|A|\". When v corresponds to the largest value of |A|, this perturbation achieves the widest possible separation of an initial perturbation of size 6. When |A| > 1, the deviation size 6|\\|\" grows exponentially large. When |)| < 1, the deviation size becomes exponentially small. Of course, this example assumed that the Jacobian was the same at every time step, corresponding to a recurrent network with no nonlinearity. When a nonlinearity is present, the derivative of the nonlinearity will approach zero on many time steps and help prevent the explosion resulting from a large spectral radius", "8f06faed-25a6-412f-b27f-43c2e82955a8": "Given a task 7; and its associated dataset  train? *~ test following example only contains one step):  ), we can update the model parameters by one or more gradient descent steps (the  6, = 0 \u2014 aVoL (fo)  where \u00a30) is the loss computed using the mini data batch with id (0). https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Log   \u2014 meta-learning  4} ---- learning/adaptation VL3 Vl> VLi a -\u00b0 83 i\u201d OR  Well, the above formula only optimizes for one task. To achieve a good generalization across a variety of tasks, we would like to find the optimal 9* so that the task-specific fine-tuning is more efficient.\n\nNow, we sample a new data batch with id (1) for updating the meta-objective. The loss, denoted as LY), depends on the mini batch (1). The superscripts in \u00a3L and LY only indicate different data batches, and they refer to the same loss objective for the same task", "efb5f22c-af1c-4df8-83ec-d0b1d0e23e0a": "We argue that in the context of AI tasks, such as those that involve processing images, sounds, or text, the manifold assumption is  158  https://www.deeplearningbook.org/contents/ml.html    CHAPTER 5. MACHINE LEARNING BASICS  Figure 5.12: Sampling images uniformly at random (by randomly picking each pixel according to a uniform distribution) gives rise to noisy images. Although there is a nonzero probability of generating an image of a face or of any other object frequently encountered in AI applications, we never actually observe this happening in practice.\n\nThis suggests that the images encountered in AI applications occupy a negligible proportion of the volume of image space. at least approximately correct. The evidence in favor of this assumption consists of two categories of observations. https://www.deeplearningbook.org/contents/ml.html    The first observation in favor of the manifold hypothesis is that the proba- 159  CHAPTER 5. MACHINE LEARNING BASICS  bility distribution over images, text strings, and sounds that occur in real life is highly concentrated. Uniform noise essentially never resembles structured inputs from these domains", "75511bce-eb57-4cb7-b3a4-e731a1d55cda": "A complete in-place version of iterative policy evaluation is shown in pseudocode in the box below. Note how it handles termination.\n\nFormally, iterative policy evaluation converges only in the limit, but in practice it must be halted short of this. The pseudocode tests the quantity maxs2S |vk+1(s)\u2212vk(s)| after each sweep and stops when it is su\ufb03ciently small. Input \u21e1, the policy to be evaluated Algorithm parameter: a small threshold \u2713 > 0 determining accuracy of estimation Initialize V (s), for all s 2 S+, arbitrarily except that V (terminal) = 0 The nonterminal states are S = {1, 2, . , 14}. There are four actions possible in each state, A = {up, down, right, left}, which deterministically cause the corresponding state transitions, except that actions that would take the agent o\u21b5 the grid in fact leave the state unchanged. Thus, for instance, p(6, \u22121|5, right) = 1, p(7, \u22121|7, right) = 1, and p(10, r|5, right) = 0 for all r 2 R", "ba77626a-9476-4d9c-86a7-ea0ee69b0ae1": "\u21e4Exercise 12.14 How might Double Expected Sarsa be extended to eligibility traces? \u21e4 Several methods using eligibility traces have been proposed that achieve guarantees of stability under o\u21b5-policy training, and here we present four of the most important using this book\u2019s standard notation, including general bootstrapping and discounting functions. All are based on either the Gradient-TD or the Emphatic-TD ideas presented in Sections 11.7 and 11.8. All the algorithms assume linear function approximation, though extensions to nonlinear function approximation can also be found in the literature. GTD(\u03bb) is the eligibility-trace algorithm analogous to TDC, the better of the two state-value Gradient-TD prediction algorithms discussed in Section 11.7.\n\nIts goal is to learn a parameter wt such that \u02c6v(s,w) .= w> t , zt, and \u21e2t de\ufb01ned in the usual ways for state values (12.23) (12.25) (11.1), and where, as in Section 11.7, v 2 Rd is a vector of the same dimension as w, initialized to v0 = 0, and \u03b2 > 0 is a second step-size parameter", "382b18b5-2833-407c-9fdf-7a4563c740b6": "The network before the split consisted of 41 convolutional layers, each followed by batch normalization, and with skip connections added to implement residual learning by pairs of layers (see Section 9.6). Overall, move probabilities and values were computed by 43 and 44 layers respectively. Starting with random weights, the network was trained by stochastic gradient descent (with momentum, regularization, and step-size parameter decreasing as training continues) using batches of examples sampled uniformly at random from all the steps of the most recent 500,000 games of self-play with the current best policy. Extra noise was added to the network\u2019s output p to encourage exploration of all possible moves.\n\nAt periodic checkpoints during training, which Silver et al. chose to be at every 1,000 training steps, the policy output by the ANN with the latest weights was evaluated by simulating 400 games (using MCTS with 1,600 iterations to select each move) against the current best policy. If the new policy won (by a margin set to reduce noise in the outcome), then it became the best policy to be used in subsequent self-play", "ced5fd8a-631d-4de7-829c-71f6cf1bedd9": "Indeed, it can easily be shown that 2 is just a multivariate normal random variable, with  x~ N(a;b, WW! +2). (13.4) 486  CHAPTER 13. LINEAR FACTOR MODELS  To cast PCA in a probabilistic framework, we can make a slight modification to the factor analysis model, making the conditional variances o? equal to each  other. In that case the covariance of x is just WW! + 07I, where o? is now a scalar. This yields the conditional distribution x~N(x;b,WW' +071), (13.5) or equivalently x= Wh+b+0z, (13.6)  where z ~ N(z;0, I) is Gaussian noise. Then, as Tipping and Bishop  show, we can use an iterative EM algorithm for estimating the parameters W and o?. This probabilistic PCA model takes advantage of the observation that most variations in the data can be captured by the latent variables h, up to some small residual reconstruction error o?", "f19bbe30-bae6-472f-b346-db3e36ed780b": "Early work on sparse autoencoders  explored various forms of sparsity and proposed a connection between the sparsity penalty and the log Z term that arises when applying maximum likelihood to an undirected probabilistic model p(x) = 4 p(a). The idea is that minimizing log Z prevents a probabilistic model from having high probability everywhere, and imposing sparsity on an autoencoder prevents the autoencoder from having low reconstruction error everywhere. In this case, the connection is on the level of an intuitive understanding of a general mechanism rather than a mathematical correspondence. The interpretation of the sparsity penalty as corresponding to log pmodel (A) in a directed model pyodei(h)Pmodel(@ | h) is more mathematically straightforward. One way to achieve actual zeros in h for sparse (and denoising) autoencoders was introduced in Glorot ef al. .\n\nThe idea is to use rectified linear units to produce the code layer. With a prior that actually pushes the representations to zero (like the absolute value penalty), one can thus indirectly control the average number of zeros in the representation", "e4550a39-d92a-4e0d-b3b4-89e8d1ea2a3e": "Raedt and S. Wrobel (Eds. ), Proceedings of the 22nd International Conference on Machine Learning, pp. 689\u2013696. Rauch, H. E., F. Tung, and C. T. Striebel . Maximum likelihood estimates of linear dynamical systems. AIAA Journal 3, 1445\u20131450.\n\nRicotti, L. P., S. Ragazzini, and G. Martinelli . Learning of word stress in a sub-optimal second order backpropagation neural network. In Proceedings of the IEEE International Conference on Neural Networks, Volume 1, pp. 355\u2013361. IEEE. Ripley, B. D. Pattern Recognition and Neural Networks. Cambridge University Press. Robbins, H. and S. Monro . A stochastic approximation method. Annals of Mathematical Statistics 22, 400\u2013407. Robert, C. P. and G", "76671467-6c27-488f-9b29-623e83132515": "Let X be a compact metric set (such as the space of images d) and let \u03a3 denote the set of all the Borel subsets of X. Let Prob(X) denote the space of probability measures de\ufb01ned on X. We can now de\ufb01ne elementary distances and divergences between two distributions Pr, Pg \u2208 Prob(X): where both Pr and Pg are assumed to be absolutely continuous, and therefore admit densities, with respect to a same measure \u00b5 de\ufb01ned on X.2 The KL divergence is famously assymetric and possibly in\ufb01nite when there are points such that Pg(x) = 0 and Pr(x) > 0. A Pr(x)d\u00b5(x), if and only it is absolutely continuous with respect to \u00b5, that is, \u2200A \u2208 \u03a3, \u00b5(A) = 0 \u21d2 Pr(A) = 0 . where Pm is the mixture (Pr + Pg)/2", "70743529-65e9-49b6-9171-23b2dd7a1837": "be thought of as an \u201cexpert\u201d that determines whether a particular soft constraint is satisfied.\n\nEach expert may enforce only one constraint that concerns only a low-dimensional projection of the random variables, but when combined by multiplication of probabilities, the experts together enforce a complicated high- dimensional constraint. One part of the definition of an energy-based model serves no functional purpose from a machine learning point of view: the \u2014 sign in equation 16.7. This \u2014 sign could be incorporated into the definition of E. For many choices of the function EF, the learning algorithm is free to determine the sign of the energy anyway. The \u2014 sign is present primarily to preserve compatibility between the machine learning literature and the physics literature. Many advances in probabilistic modeling were originally developed by statistical physicists, for whom F refers to actual physical energy and does not have arbitrary sign. Terminology such as \u201cenergy\u201d and \u201cpartition function\u201d remains associated with these techniques, even though their mathematical applicability is broader than the physics context in which they were developed. Some machine learning researchers (e.g., Smolensky , who referred to negative energy as harmony) have chosen to omit the negation, but this is not the standard convention", "20bca609-7e70-4fe7-b8b3-c4925a978879": "Locally weighted regression is similar, but it \ufb01ts a surface to the values of a set of nearest states by means of a parametric approximation method that minimizes a weighted error measure like (9.1), where the weights depend on distances from the query state.\n\nThe value returned is the evaluation of the locally-\ufb01tted surface at the query state, after which the local approximation surface is discarded. Being nonparametric, memory-based methods have the advantage over parametric methods of not limiting approximations to pre-speci\ufb01ed functional forms. This allows accuracy to improve as more data accumulates. Memory-based local approximation methods have other properties that make them well suited for reinforcement learning. Because trajectory sampling is of such importance in reinforcement learning, as discussed in Section 8.6, memory-based local methods can focus function approximation on local neighborhoods of states (or state\u2013action pairs) visited in real or simulated trajectories. There may be no need for global approximation because many areas of the state space will never (or almost never) be reached", "73d72e5a-cda2-4177-a7c4-696260e3e317": "The units in a network\u2019s input layer are somewhat di\u21b5erent in having their activations set to externally-supplied values that are the inputs to the function the network is approximating. The activation of each output unit of a feedforward ANN is a nonlinear function of the activation patterns over the network\u2019s input units. The functions are parameterized by the network\u2019s connection weights. An ANN with no hidden layers can represent only a very small fraction of the possible input-output functions. However an ANN with a single hidden layer containing a large enough \ufb01nite number of sigmoid units can approximate any continuous function on a compact region of the network\u2019s input space to any degree of accuracy", "c7f11143-ef50-4ea6-9dba-e1f6f1d95175": "4.5 (\u22c6) By making use of (4.20), (4.23), and (4.24), show that the Fisher criterion (4.25) can be written in the form (4.26).\n\n4.6 (\u22c6) Using the de\ufb01nitions of the between-class and within-class covariance matrices given by (4.27) and (4.28), respectively, together with (4.34) and (4.36) and the choice of target values described in Section 4.1.5, show that the expression (4.33) that minimizes the sum-of-squares error function can be written in the form (4.37). 4.7 (\u22c6) www Show that the logistic sigmoid function (4.59) satis\ufb01es the property \u03c3(\u2212a) = 1 \u2212 \u03c3(a) and that its inverse is given by \u03c3\u22121(y) = ln {y/(1 \u2212 y)}. 4.8 (\u22c6) Using (4.57) and (4.58), derive the result (4.65) for the posterior class probability in the two-class generative model with Gaussian densities, and verify the results (4.66) and (4.67) for the parameters w and w0", "845338f9-5d11-49a6-b2d4-bf4d670744e4": "Hull, C. L. Principles of Behavior. Appleton-Century, New York. Hull, C. L. A Behavior System. Wiley, New York. Io\u21b5e, S., Szegedy, C. Batch normalization: Accelerating deep network training by \u02d9Ipek, E., Mutlu, O., Mart\u00b4\u0131nez, J. F., Caruana, R. Self-optimizing memory controllers: A reinforcement learning approach. In ISCA\u201908:Proceedings of the 35th Annual International Symposium on Computer Architecture, pp. 39\u201350. IEEE Computer Society Washington, DC. dopamine signaling. Cerebral Cortex, 17(10):2443\u20132452. dynamic programming algorithms. Neural Computation, 6:1185\u20131201. Jaakkola, T., Singh, S. P., Jordan, M. I", "126ce6e1-22a8-4b41-91c5-d72f76d1b38a": "MONTE CARLO METHODS  and A to describe how the entire distribution over all the different Markov chains (running in parallel) shifts as we apply an update:  v) = Av\u2019), (17.20)  Applying the Markov chain update repeatedly corresponds to multiplying by the matrix A repeatedly. In other words, we can think of the process as exponentiating the matrix A:  vO) = Aly, (17.21)  The matrix A has special structure because each of its columns represents a probability distribution. Such matrices are called stochastic matrices. If there is a nonzero probability of transitioning from any state x to any other state 2\u2019 for some power t, then the Perron-Frobenius theorem  guarantees that the largest eigenvalue is real and equal to 1. Over time, we can  https://www.deeplearningbook.org/contents/monte_carlo.html    see that all the eigenvalues are exponentiated: v = (Vdiag(a)V_')' \u00a5 = Vaiag(a)'V\u2018o (17.22) This process causes all the eigenvalues that are not equal to 1 to decay to zero", "bc3a1c6f-0429-49a4-9d94-3b11e4134a78": "Given a (NV + 1)-tuplet of training samples, {x,x*,x7,---,X_,}, including one positive and NN \u2014 1 negative ones, N-pair loss is defined as:  N-1  Lrepair(x5*s {x }2G\") = log (1+ $0 exp( f(x) \"F(x; ) \u2014 f(x)\" F(x*)))  : exp( f(x) f(x\")  =-\u2014lo * exp( f(x) Fe\") + 8s! exp( fl)\" Fx; ))  If we only sample one negative sample per class, it is equivalent to the softmax loss for multi-class classification.\n\nNCE  Noise Contrastive Estimation, short for NCE, is a method for estimating parameters of a statistical model, proposed by Gutmann & Hyvarinen in 2010. The idea is to run logistic regression to tell apart the target data from noise. Read more on how NCE is used for learning word embedding here", "c58928b4-566c-4d65-849a-320bffbb7daf": "It is possible to considerably reduce the variance of that estimator by using variance reduction methods . The idea is to modify the estimator so that its expected value remains unchanged but its variance gets reduced. In the context of REINFORCE, the proposed variance reduction methods involve the computation of a baseline that is used to offset J(y). Note that any offset b(w) that does not depend on y would not change the expectation of the estimated gradient because  Olo Alo _ yw) (20.64) y a a", "9a368eb3-867d-458a-9362-6e3cdda4781c": "Shalev-Shwartz, S. et al. Online learning and online convex optimization. Foundations and Trends\u00ae in Machine Learning, 4(2), 107\u2013194. Singh, S., Lewis, R. L., Barto, A. G., & Sorg, J. Intrinsically motivated reinforcement learning: An evolutionary perspective. IEEE Transactions on Autonomous Mental Development. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., & Poole, B. Score-based generative modeling through stochastic di\ufb00erential equations. International Conference on Learning Representations. Sutton, R. S., McAllester, D. A., Singh, S. P., & Mansour, Y. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 1057\u20131063", "ab5943f7-b8be-4474-9d3a-cc931c9e655a": "When we make the update, unexpected results can happen because many functions composed  313  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  together are changed simultaneously, using updates that were computed under the assumption that the other functions remain constant. As a simple example, suppose we have a deep neural network that has only one unit per layer and does not use an activation function at each hidden layer: 7 = rwyuyw3...w);. Here, w; provides the weight used by layer 7 The output of layer i is hi = h-1w;. The output 7 is a linear function of the input x but a nonlinear function of the weights w;. Suppose our cost function has put a gradient of 1 on y, so we wish to decrease y slightly. The back-propagation algorithm can then compute a gradient g = Vwi. Consider what happens when we make an update w < w \u2014eg", "58890ea3-8541-4cc2-89a1-0eb272b5028f": "The predictive distribution for class C1, given a new feature vector \u03c6(x), is obtained by marginalizing with respect to the posterior distribution p(w|t), which is itself approximated by a Gaussian distribution q(w) so that with the corresponding probability for class C2 given by p(C2|\u03c6, t) = 1\u2212p(C1|\u03c6, t).\n\nTo evaluate the predictive distribution, we \ufb01rst note that the function \u03c3(wT\u03c6) depends on w only through its projection onto \u03c6. Denoting a = wT\u03c6, we have We can evaluate p(a) by noting that the delta function imposes a linear constraint on w and so forms a marginal distribution from the joint distribution q(w) by integrating out all directions orthogonal to \u03c6. Because q(w) is Gaussian, we know from Section 2.3.2 that the marginal distribution will also be Gaussian. We can evaluate the mean and covariance of this distribution by taking moments, and interchanging the order of integration over a and w, so that where we have used the result (4.144) for the variational posterior distribution q(w)", "6df9f866-38c9-4295-9239-8a83997e266a": "It is also possible to exploit discriminative models such as neural networks. These can be used to model the Exercise 13.4 emission density p(x|z) directly, or to provide a representation for p(z|x) that can be converted into the required emission density p(x|z) using Bayes\u2019 theorem . We can gain a better understanding of the hidden Markov model by considering it from a generative point of view. Recall that to generate samples from a mixture of Gaussian emission model p(x|z) where x is 2-dimensional. (a) Contours of constant probability density for the emission distributions corresponding to each of the three states of the latent variable. (b) A sample of 50 points drawn from the hidden Markov model, colour coded according to the component that generated them and with lines connecting the successive observations. Here the transition matrix was \ufb01xed so that in any state there is a 5% probability of making a transition to each of the other states, and consequently a 90% probability of remaining in the same state", "101de00c-48fc-402a-a1b5-17341d99e295": "AAAI Press. Connell, J. A colony architecture for an arti\ufb01cial creature. Technical Report AI-TR-1151. MIT Arti\ufb01cial Intelligence Laboratory, Cambridge, MA. Connell, M. E., Utgo\u21b5, P. E. Learning to control a dynamic physical system. ComputaContreras-Vidal, J. L., Schultz, W. .\n\nA predictive reinforcement model of dopamine neurons for learning approach behavior. Journal of Computational Neuroscience, 6(3):191\u2013214. Coulom, R. E\ufb03cient selectivity and backup operators in Monte-Carlo tree search. In Proceedings of the 5th International Conference on Computers and Games (CG\u201906), pp. 72\u201383. Springer-Verlag Berlin, Heidelberg. Courville, A. C., Daw, N. D., Touretzky, D. S. Bayesian theories of conditioning in a changing world. Trends in Cognitive Science, 10(7):294\u2013300", "96ae1d25-e9f0-4b61-b0f8-c9466ac5ab73": "These ideas were proposed by Mozer  and by El Hihi and Bengio . Leaky units were also found to be useful in the context of echo state networks . There are two basic strategies for setting the time constants used by leaky units. One strategy is to manually fix them to values that remain constant, for example, by sampling their values from some distribution once at initialization time. Another strategy is to make the time constants free parameters and learn them. Having such leaky units at different time scales appears to help with long-term dependencies . 10.9.3 Removing Connections  Another approach to handling long-term dependencies is the idea of organizing the state of the RNN at multiple time scales , with information flowing more easily through long distances at the slower time scales.\n\nThis idea differs from the skip connections through time discussed earlier because it involves actively removing length-one connections and replacing them with longer connections. Units modified in such a way are forced to operate on a long time scale. Skip connections through time add edges. Units receiving such new connections may learn to operate on a long time scale but may also choose to focus on their other, short-term connections", "35449f20-1793-4433-98b2-1084e183c794": "Where two edges leave a state, both possibilities are probability.\n\nThe numbers on the edges indicate the reward em The MDP on the left has two states that are represented di weight so that they can take on any value. The MDP on th of which, B and B\u2032, are represented identically and must be value. We can imagine that the value of state A is given by the value of B and B\u2032 is given by the second. Notice that th for the two MDPs. In both cases the agent will see single oc 0, then some number of Bs each followed by a \u22121, except th 1, then we start all over again with a single A and a 0, etc. as well; in both MDPs, the probability of a string of k Bs is function v\u03b8 = \u20d70. In the \ufb01rst MDP, this is an exact solution, a the second MDP, this solution produces an error in both B a of which generate the same data, have di\u21b5erent BEs", "3cb4ced1-7019-4e09-b763-57c7ff455212": "The former makes sense because it causes the parameter to move most in the directions that favor actions that yield the highest return. The latter makes sense because otherwise actions that are selected frequently are at an advantage (the updates will be more often in their direction) and might win out even if they do not yield the highest return. Note that REINFORCE uses the complete return from time t, which includes all future rewards up until the end of the episode. In this sense REINFORCE is a Monte Carlo algorithm and is well de\ufb01ned only for the episodic case with all updates made in retrospect after the episode is completed (like the Monte Carlo algorithms in Chapter 5). This is shown explicitly in the boxed algorithm on the next page. Notice that the update in the last line of pseudocode appears rather di\u21b5erent from the REINFORCE update rule (13.8)", "9002c690-e18c-4c62-a2a3-f33c17816784": "He owes a particular debt to Rupam Mahmood for essential contributions to the treatment of o\u21b5-policy Monte Carlo methods in Chapter 5, to Hamid Maei for helping develop the perspective on o\u21b5-policy learning presented in Chapter 11, to Eric Graves for conducting the experiments in Chapter 13, to Shangtong Zhang for replicating and thus verifying almost all the experimental results, to Kris De Asis for improving the new technical content of Chapters 7 and 12, and to Harm van Seijen for insights that led to the separation of n-step methods from eligibility traces and (along with Hado van Hasselt) for the ideas involving exact equivalence of forward and backward views of eligibility traces presented in Chapter 12. Sutton also gratefully acknowledges the support and freedom he was granted by the Government of Alberta and the National Science and Engineering Research Council of Canada throughout the period during which the second edition was conceived and written.\n\nIn particular, he would like to thank Randy Goebel for creating a supportive and far-sighted environment for research in Alberta. He would also like to thank DeepMind their support in the last six months of writing the book", "8772621c-1079-4f5b-8ac2-06681168d459": "Whereas in Monte Carlo updates the target is the return, in one-step updates the target is the \ufb01rst reward plus the discounted estimated value of the next state, which we call the one-step return: where Vt : S ! R here is the estimate at time t of v\u21e1.\n\nThe subscripts on Gt:t+1 indicate that it is a truncated return for time t using rewards up until time t+1, with the discounted estimate \u03b3Vt(St+1) taking the place of the other terms \u03b3Rt+2 + \u03b32Rt+3 + \u00b7 \u00b7 \u00b7 + \u03b3T \u2212t\u22121RT of the full return, as discussed in the previous chapter. Our point now is that this idea makes just as much sense after two steps as it does after one. The target for a two-step update is the two-step return: where now \u03b32Vt+1(St+2) corrects for the absence of the terms \u03b32Rt+3 + \u03b33Rt+4 + \u00b7 \u00b7 \u00b7 + \u03b3T \u2212t\u22121RT", "3d95c33b-c433-415d-b485-bf95a1a6e478": "When learning does converge, the initial point can determine how quickly learning converges and whether it converges to a point with high or low cost. Also, points of comparable cost can have wildly varying generalization error, and the initial point can affect the generalization as well. Modern initialization strategies are simple and heuristic. Designing improved initialization strategies is a difficult task because neural network optimization is not yet well understood. Most initialization strategies are based on achieving some nice properties when the network is initialized. However, we do not have a good understanding of which of these properties are preserved under which circumstances after learning begins to proceed. A further difficulty is that some initial points may be beneficial from the viewpoint of optimization but detrimental from the viewpoint of generalization. Our understanding of how the initial point affects generalization is especially primitive, offering little to no guidance for how to select the initial point.\n\nPerhaps the only property known with complete certainty is that the initial parameters need to \u201cbreak symmetry\u201d between different units", "1b22a839-959c-45ea-82b2-46304553f025": "This means replacing the true distribution p(a, y) with the empirical distribution p(a, y) defined by the training set. We now minimize the empirical risk  Yo (F(a; 9), 9), (8.3)  i=1  e.y~paata (ay) LL (F(a; 8), y)] =  where m is the number of training examples. The training process based on minimizing this average training error is known as empirical risk minimization. In this setting, machine learning is still very similar to straightforward optimization. Rather than optimizing the risk directly, we optimize the empirical risk and hope that the risk decreases significantly as well. A variety of theoretical results establish conditions under which the true risk can be expected to decrease by various amounts. Nonetheless, empirical risk minimization is prone to overfitting. Models with high capacity can simply memorize the training set. In many cases, empirical risk minimization is not really feasible. The most effective modern optimization algorithms are based on gradient descent, but many useful loss functions, such as 0-1 loss, have no useful derivatives (the derivative is either zero or undefined everywhere)", "026ee463-7925-4e50-b582-74e4ebedea8f": "(2.50)  The trace of a square matrix composed of many factors is also invariant to moving the last factor into the first position, if the shapes of the corresponding matrices allow the resulting product to be defined:  Tr(ABC) = Tr(CAB) = Tr(BCA) (2.51) or more generally, n n-1 Tr([[ FO) = Tr(F\u2122 [J FO).\n\n(2.52) i=l i=1  This invariance to cyclic permutation holds even if the resulting product has a different shape. For example, for A \u20ac R\u2122*\u201d and B \u20ac R\"*\u2122, we have  Tr(AB) = Tr(BA) (2.53)  even though AB \u20ac R\u2122*\u2122 and BA \u20ac R\u201d*\u201d. Another useful fact to keep in mind is that a scalar is its own trace: a = Tr(a). 2.11 The Determinant The determinant of a square matrix, denoted det(A), is a function that maps  https://www.deeplearningbook.org/contents/linear_algebra.html    matrices to real scalars. The determinant is equal to the product", "affaa339-07c5-448a-ae5f-b185bd231b67": "Following this principle, in the supervised setting, we thus have the speci\ufb01c constrained optimization problem: The problem can be solved with the Lagrangian method.\n\nSpeci\ufb01cally, we write the Lagrangian: where \u03b8 and \u00b5 are Lagrangian multipliers. Setting the derivative w.r.t. p and \u00b5 to equal zero implies that p must have the same form as in Equation 2.2: where we see the parameters \u03b8 in the exponential family parameterization are the Lagrangian multipliers that enforce the constraints. Plugging the solution back into the Lagrangian, we obtain: which is simply the negative of the MLE objective in Equation 2.1. Thus maximum entropy is dual to maximum likelihood. It provides an alternative view of the problem of \ufb01tting a model into data, where the data instances in the training set are treated as constraints, and the learning problem is treated as a constrained optimization problem. This optimization-theoretic view of learning will be revisited repeatedly in the sequel to allow extending machine learning under all experience of which data instances is just a special case. 2.1.2. Unsupervised MLE", "ca6d4a51-1af6-4188-940b-9c99c2258e52": "At the core of the connection is that the discriminator will play the role of f maximizing equation (4) while its only restriction is being between 0 and m for some constant m. This will yeald the same behaviour as being restricted to be between \u22121 and 1 up to a constant scaling factor irrelevant to optimization. Thus, when the discriminator approaches optimality the cost for the generator will aproximate the total variation distance \u03b4(Pr, P\u03b8). Since the total variation distance displays the same regularity as the JS, it can be seen that EBGANs will su\ufb00er from the same problems of classical GANs regarding not being able to train the discriminator till optimality and thus limiting itself to very imperfect gradients. \u2022 Maximum Mean Discrepancy (MMD)  is a speci\ufb01c case of integral probability metrics when F = {f \u2208 H : \u2225f\u2225\u221e \u2264 1} for H some Reproducing Kernel Hilbert Space (RKHS) associated with a given kernel k : X \u00d7 X \u2192 R", "3fc7bbbc-11ec-47d7-baac-8fb0964c2d5a": "Use the calculus of variations to minimize this error function with respect to the function y(x), and hence show that the optimal solution is given by an expansion of the form (6.40) in which the basis functions are given by (6.41). 6.18 (\u22c6) Consider a Nadaraya-Watson model with one input variable x and one target variable t having Gaussian components with isotropic covariances, so that the covariance matrix is given by \u03c32I where I is the unit matrix. Write down expressions for the conditional density p(t|x) and for the conditional mean E and variance var, in terms of the kernel function k(x, xn).\n\n6.19 (\u22c6 \u22c6) Another viewpoint on kernel regression comes from a consideration of regression problems in which the input variables as well as the target variables are corrupted with additive noise. Suppose each target value tn is generated as usual by taking a function y(zn) evaluated at a point zn, and adding Gaussian noise", "58fb06b0-c0f0-4227-9c6d-45d8fe4a1151": "), convex duality (e.g., facilitating dual sparsity of support vectors via the complementary slackness in the KKT conditions), and kernel methods as used in . It is intriguing that, in the dual point of view on the problem of (supervised) MLE, data instances are encoded as constraints (Equation 2.4), much like the structured constraints in posterior regularization.\n\nIn the following sections, we present the standardized formalism of machine learning algorithms and show that indeed a myriad types of experience besides data instances and constraints can all be encoded in the same generic form and be used in learning. Generalizing from Equation 2.16, we present the following general formulation for learning a target model via a constrained loss minimization program. We would refer to the formulation as the \u2018Standard Equation\u2019 because it presents a general space of learning objectives that encompasses many speci\ufb01c formalisms used in di\ufb00erent machine learning paradigms. Without loss of generality, let t \u2208 T be the variable of interest, for example, the input-output pair t = (x, y) in a prediction task, or the target variable t = x in generative modeling", "b7c225c7-6804-4aed-9429-d7919b0d0b54": "A linear value function could not represent this if its features coded separately for the angle and the angular velocity. It needs instead, or in addition, features for combinations of these two underlying state dimensions. In the following subsections we consider a variety of general ways of doing this. The states of many problems are initially expressed as numbers, such as positions and velocities in the pole-balancing task (Example 3.4), the number of cars in each lot in the Jack\u2019s car rental problem (Example 4.2), or the gambler\u2019s capital in the gambler problem (Example 4.3). In these types of problems, function approximation for reinforcement learning has much in common with the familiar tasks of interpolation and regression. Various families of features commonly used for interpolation and regression can also be used in reinforcement learning. Polynomials make up one of the simplest families of features used for interpolation and regression.\n\nWhile the basic polynomial features we discuss here do not work as well as other types of features in reinforcement learning, they serve as a good introduction because they are simple and familiar. As an example, suppose a reinforcement learning problem has states with two numerical dimensions", "962067ac-f6fa-4d72-a8d6-42d6d56bff02": "The following is an ablation study to evaluate the effect of different masking strategies.\n\nPre-training Steps (Thousands) Note that the purpose of the masking strategies is to reduce the mismatch between pre-training and \ufb01ne-tuning, as the  symbol never appears during the \ufb01ne-tuning stage. We report the Dev results for both MNLI and NER. For NER, we report both \ufb01ne-tuning and feature-based approaches, as we expect the mismatch will be ampli\ufb01ed for the feature-based approach as the model will not have the chance to adjust the representations. The results are presented in Table 8. In the table, MASK means that we replace the target token with the  symbol for MLM; SAME means that we keep the target token as is; RND means that we replace the target token with another random token. The numbers in the left part of the table represent the probabilities of the speci\ufb01c strategies used during MLM pre-training (BERT uses 80%, 10%, 10%). The right part of the paper represents the Dev set results. For the feature-based approach, we concatenate the last 4 layers of BERT as the features, which was shown to be the best approach in Section 5.3", "5c46167f-eb6d-46ef-acc3-23c77ea1d97d": "The general expression (7.77) then takes the SVM-like form where b is a bias parameter. The number of parameters in this case is M = N + 1, and y(x) has the same form as the predictive model (7.64) for the SVM, except that the coef\ufb01cients an are here denoted wn. It should be emphasized that the subsequent analysis is valid for arbitrary choices of basis function, and for generality we shall work with the form (7.77). In contrast to the SVM, there is no restriction to positivede\ufb01nite kernels, nor are the basis functions tied in either number or location to the training data points.\n\nSuppose we are given a set of N observations of the input vector x, which we denote collectively by a data matrix X whose nth row is xT n with n = 1, . , N. The corresponding target values are given by t = (t1, . , tN)T. Thus, the likelihood function is given by Next we introduce a prior distribution over the parameter vector w and as in Chapter 3, we shall consider a zero-mean Gaussian prior", "0b9cd86a-96a7-4c36-a6cc-55140ef35857": "Here we follow Tipping  and use the Laplace approximation, which was applied to the closely related problem of Bayesian logistic Section 4.4 regression in Section 4.5.1. We begin by initializing the hyperparameter vector \u03b1. For this given value of \u03b1, we then build a Gaussian approximation to the posterior distribution and thereby obtain an approximation to the marginal likelihood. Maximization of this approximate marginal likelihood then leads to a re-estimated value for \u03b1, and the process is repeated until convergence. Let us consider the Laplace approximation for this model in more detail. For a \ufb01xed value of \u03b1, the mode of the posterior distribution over w is obtained by maximizing ln p(w|t, \u03b1) = ln {p(t|w)p(w|\u03b1)} \u2212 ln p(t|\u03b1) where A = diag(\u03b1i). This can be done using iterative reweighted least squares (IRLS) as discussed in Section 4.3.3", "f7059e20-d047-4f85-95d4-f5226f55d78e": "The model can then use word class IDs rather than individual word IDs to represent the context on the right side of the conditioning bar. Composite models combining word-based and class-based models via mixing or back-off are also possible. Although word classes provide a way to generalize between sequences in which some word is replaced by another of the same class, much information is lost in this representation.\n\n12.4.2 Neural Language Models  Neural language models, or NLMs, are a class of language model designed to overcome the curse of dimensionality problem for modeling natural language sequences by using a distributed representation of words . Unlike class-based n-gram models, neural language models are able to recognize  458  CHAPTER 12", "5ae8200c-6f8f-49de-8faa-15564d53763d": ", these approaches learn representations by contrasting positive pairs against negative pairs. Along these lines, Dosovitskiy et al. proposes to treat each instance as a class represented by a feature vector (in a parametric form). Wu et al. proposes to use a memory bank to store the instance class representation vector, an approach adopted and extended in several recent papers . Other work explores the use of in-batch samples for negative sampling instead of a memory bank .\n\nRecent literature has attempted to relate the success of their methods to maximization of mutual information between latent representations . However, it is not clear if the success of contrastive approaches is determined by the mutual information, or by the speci\ufb01c form of the contrastive loss . A Simple Framework for Contrastive Learning of Visual Representations We note that almost all individual components of our framework have appeared in previous work, although the speci\ufb01c instantiations may be different. The superiority of our framework relative to previous work is not explained by any single design choice, but by their composition. We provide a comprehensive comparison of our design choices with those of previous work in Appendix C", "1df82f85-57bf-45c4-a270-f115d3d074fc": "(Additional conditions and a schedule for reducing \u21b5 over time are needed to prove convergence with probability one.) At the TD \ufb01xed point, it has also been proven (in the continuing case) that the VE is within a bounded expansion of the lowest possible error: possible error, that attained in the limit by the Monte Carlo method. Because \u03b3 is often near one, this expansion factor can be quite large, so there is substantial potential loss in asymptotic performance with the TD method. On the other hand, recall that the TD methods are often of vastly reduced variance compared to Monte Carlo methods, and thus faster, as we saw in Chapters 6 and 7. Which method will be best depends on the nature of the approximation and problem, and on how long learning continues. A bound analogous to (9.14) applies to other on-policy bootstrapping methods as well. For example, linear semi-gradient DP (Eq", "e0698a8e-c00e-4de6-b5ad-331b642600e4": "The effect of momentum is illustrated in figure 8.5. Formally, the momentum algorithm introduces a variable v that plays the role of velocity\u2014it is the direction and speed at which the parameters move through parameter space. The velocity is set to an exponentially decaying average of the negative gradient.\n\nThe name momentum derives from a physical analogy, in which the negative gradient is a force moving a particle through parameter space, according to Newton\u2019s laws of motion. Momentum in physics is mass times velocity. In the momentum learning algorithm, we assume unit mass, so the velocity vector v may also be regarded as the momentum of the particle. A hyperparameter a \u20ac [0, 1) determines how quickly the contributions of previous gradients exponentially decay. The update rule is given by  1 2 . _ + . 9). y@ 5 v cav\u2014\u20acVe (2S 140 :0),y \\), (8.15) 0+ O+v. (8.16)  293  CHAPTER 8", "425b3852-3b06-4a3d-bda4-cfc7b9ba14d7": "We shall therefore begin in this section with a \u2018conventional\u2019 derivation of the forward-backward equations, making use of the sum and product rules of probability, and exploiting conditional independence properties which we shall obtain from the corresponding graphical model using d-separation. Then in Section 13.2.3, we shall see how the forward-backward algorithm can be obtained very simply as a speci\ufb01c example of the sum-product algorithm introduced in Section 8.4.4. It is worth emphasizing that evaluation of the posterior distributions of the latent variables is independent of the form of the emission density p(x|z) or indeed of whether the observed variables are continuous or discrete. All we require is the values of the quantities p(xn|zn) for each value of zn for every n. Also, in this section and the next we shall omit the explicit dependence on the model parameters \u03b8old because these \ufb01xed throughout. We therefore begin by writing down the following conditional independence properties  where X = {x1, . , xN}. These relations are most easily proved using d-separation", "0a7e9dbc-19ca-481f-acee-f1338b2ed916": "Earlier, we described  Anan aeek 2 Le et 2 nen ee Le Af 2 AAT. fn 2 Lee te ee dt ne nee lee A eet  https://www.deeplearningbook.org/contents/regularization.html    ULUPUUL AS VAYERUIP All CLSCLUIVIE UL LWIOUCIS LOLILICU Dy LUCLUUIUY OL CACLUUILILY ULLLS. Yet this model averaging strategy does not need to be based on inclusion and exclusion. In principle, any kind of random modification is admissible. In practice, we must choose modification families that neural networks are able to learn to  resist. Ideally, we should also use model families that allow a fast approximate  263  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  inference rule. We can think of any form of modification parametrized by a vector ps as training an ensemble consisting of p(y | #, 2) for all possible values of y. There is no requirement that yz have a finite number of values", "275730b1-6a04-4687-a1a8-9ac3d00f9b70": "Consider applying to this problem a bandit algorithm using \"-greedy action selection, sample-average action-value estimates, and initial estimates of Q1(a) = 0, for all a. Suppose the initial sequence of actions and rewards is A1 = 1, R1 = \u22121, A2 = 2, R2 = 1, A3 = 2, R3 = \u22122, A4 = 2, R4 = 2, A5 = 3, R5 = 0. On some of these time steps the \" case may have occurred, causing an action to be selected at random. On which time steps did this de\ufb01nitely occur? On which time steps could this possibly have occurred? \u21e4 Exercise 2.3 In the comparison shown in Figure 2.2, which method will perform best in the long run in terms of cumulative reward and probability of selecting the best action? How much better will it be? Express your answer quantitatively. \u21e4 The action-value methods we have discussed so far all estimate action values as sample averages of observed rewards", "8edff517-d67d-47eb-9a34-d5a6dad91fda": "These backpropagation rules could allow one to learn the conditional variance of the generator, which we treated as a hyperparameter in this work. Kingma and Welling  and Rezende et al. use stochastic backpropagation to train variational autoencoders (VAEs). Like generative adversarial networks, variational autoencoders pair a differentiable generator network with a second neural network. Unlike generative adversarial networks, the second network in a VAE is a recognition model that performs approximate inference. GANs require differentiation through the visible units, and thus cannot model discrete data, while VAEs require differentiation through the hidden units, and thus cannot have discrete latent variables.\n\nOther VAElike approaches exist  but are less closely related to our method. Previous work has also taken the approach of using a discriminative criterion to train a generative model . These approaches use criteria that are intractable for deep generative models. These methods are dif\ufb01cult even to approximate for deep models because they involve ratios of probabilities which cannot be approximated using variational approximations that lower bound the probability", "bc664a60-fc5e-44ce-b011-9e98f5f0ce03": "Three generalizations of rectified linear units are based on using a nonzero slope a; when z; < 0: hi = g(z,a); = max(0, z;) + a; min (0, z;). Absolute value  https://www.deeplearningbook.org/contents/mlp.html    189  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  rectification fixes a; = \u20141 to obtain g({ z) = |z|. It is used for object recognition from images , where it makes sense to seek features that are invariant under a polarity reversal of the input illumination. Other generalizations of rectified linear units are more broadly applicable. A leaky ReLU  fixes a; to a small value like 0.01, while a parametric ReLU, or PReLU, treats a; as a learnable parameter . Maxout units  generalize rectified linear units further", "ca2bfbeb-922f-499f-9ed8-71116f7da88f": "At the end of each game is a \u201cFinal Jeopardy\u201d (FJ) round in which each contestant writes down a sealed bet and then writes an answer after the clue is read. The contestant with the highest score after three rounds of play (where a round consists of revealing all 30 clues) is the winner. The game has many other details, but these are enough to appreciate Whenever Watson selected a DD square, it chose its bet by comparing action values, \u02c6q(s, bet), that estimated the probability of a win from the current game state, s, for each round-dollar legal bet. Except for some risk-abatement measures described below, Watson selected the bet with the maximum action value. Action values were computed whenever a betting decision was needed by using two types of estimates that were learned before any live game play took place. The \ufb01rst were estimated values of the afterstates (Section 6.8) that would result from selecting each legal bet.\n\nThese estimates were obtained from a state-value function, \u02c6v(\u00b7,w), de\ufb01ned by parameters w, that gave estimates of the probability of a win for Watson from any game state", "13a0d725-4684-48d3-9b95-1e0ae9376351": "For the learning experiments, air \ufb02ow in a three-dimensional box with one kilometer sides, one of which was at ground level, was modeled by a sophisticated physics-based set of partial di\u21b5erential equations involving air velocity, temperature, and pressure. Introducing small random perturbations into the numerical simulation caused the model to produce analogs of thermal updrafts and accompanying turbulence (Figure 16.9 Left) Glider \ufb02ight was modeled by aerodynamic equations involving velocity, lift, drag, and other factors governing powerless \ufb02ight of a \ufb01xed-wing aircraft.\n\nManeuvering the glider involved changing its angle of attack (the angle between the glider\u2019s wing and the direction of air \ufb02ow) and its bank angle (Figure 16.9 Right). hots of the vertical velocity (A) and the temperature fields (B) in our numerical simulations of 3D Rayleigh\u2013B\u00e9nard c the red and blue colors indicate regions of large upward and downward flow, respectively. For the temperature fie The SARSA algorithm finds the optimal policy by estimating for every state\u2013action pair its Q function defined as the expected sum of future rewards given the current state s and the action a", "9d791d9f-fb83-4777-aea3-2462da122f1f": "One of the benefits of the EM algorithm for PCA is computational efficiency for large-scale applications . Unlike conventional PCA based on an eigenvector decomposition of the sample covariance matrix, the EM approach is iterative and so might appear to be less attractive. However, each cycle of the EM algorithm can be computationally much more efficient than conventional PCA in spaces of high dimensionality.\n\nTo see this, we note that the eigendecomposition of the covariance matrix requires O(D3 ) computation. Often we are interested only in the first M eigenvectors and their corresponding eigenvalues, in which case we can use algorithms that are 0 (MD 2 ). However, the evaluation of the covariance matrix itself takes 0 (ND 2 ) computations, where N is the number of data points. Algorithms such as the snapshot method , which assume that the eigenvectors are linear combinations of the data vectors, avoid direct evaluation of the covariance matrix but are O(N3 ) and hence unsuited to large data sets. The EM algorithm described here also does not construct the covariance matrix explicitly", "4a15a8a2-d080-4216-bb0f-7771baf5d661": "The n-step return immediately generalizes from its tabular form (7.4) to a function approximation form: with Gt:t+n .= Gt if t + n \u2265 T, as usual. The n-step update equation is Episodic semi-gradient n-step Sarsa for estimating \u02c6q \u21e1 q\u21e4 or q\u21e1 Input: a di\u21b5erentiable action-value function parameterization \u02c6q : S \u21e5 A \u21e5 Rd ! R Input: a policy \u21e1 (if estimating q\u21e1) Algorithm parameters: step size \u21b5 > 0, small \" > 0, a positive integer n Initialize value-function weights w 2 Rd arbitrarily (e.g., w = 0) All store and access operations (St, At, and Rt) can take their index mod n + 1 As we have seen before, performance is best if an intermediate level of bootstrapping is used, corresponding to an n larger than 1. Figure 10.3 shows how this algorithm tends to learn faster and obtain a better asymptotic performance at n=8 than at n=1 on the Mountain Car task", "6cab5dde-7816-4e08-a985-84b251053c65": "2.2 Ag, Ago | (2.2)  Sometimes we may need to index matrix-valued expressions that are not just a single letter. In this case, we use subscripts after the expression but do not convert anything to lowercase.\n\nFor example, f(A);,; gives element (i,j) of the matrix computed by applying the function f to A.  e Tensors: In some cases we will need an array with more than two axes. In the general case, an array of numbers arranged on a regular grid with a variable number of axes is known as a tensor. We denote a tensor named \u201cA\u201d with this typeface: A. We identify the element of A at coordinates (i, j, k) by writing A; ;  One important operation on matrices is the transpose. The transpose of a matrix is the mirror image of the matrix across a diagonal line, called the main diagonal, running down and to the right, starting from its upper left corner. See figure 2.1 for a graphical depiction of this operation", "2649f08b-bcf4-4cb7-897f-740d12f5bfa8": "If all examples in the batch are to be processed in parallel (as is typically the case), then the amount of memory scales with the batch size. For many hardware setups this is the limiting factor in batch size. Some kinds of hardware achieve better runtime with specific sizes of arrays. Especially when using GPUs, it is common for power of 2 batch sizes to offer better runtime. Typical power of 2 batch sizes range from 32 to 256, with 16 sometimes being attempted for large models. https://www.deeplearningbook.org/contents/optimization.html    e Small batches can offer a regularizing effect , perhaps due to the noise they add to the learning process. Generalization error 1s often best for a batch size of 1. Training with such a small batch  size might require a small learning rate to maintain stability because of the high variance in the estimate of the gradient. The total runtime can be very high as a result of the need to make more steps, both because of the reduced learning rate and because it takes more steps to observe the entire training set", "3bcc906c-3bb7-400c-b7fb-118b3fa9229b": "For a wide range of regression and classi\ufb01cation tasks, the RVM is found to give models that are typically an order of magnitude more compact than the corresponding support vector machine, resulting in a signi\ufb01cant improvement in the speed of processing on test data. Remarkably, this greater sparsity is achieved with little or no reduction in generalization error compared with the corresponding SVM. The principal disadvantage of the RVM compared to the SVM is that training involves optimizing a nonconvex function, and training times can be longer than for a comparable SVM. For a model with M basis functions, the RVM requires inversion of a matrix of size M \u00d7 M, which in general requires O(M 3) computation. In the speci\ufb01c case of the SVM-like model (7.78), we have M = N +1. As we have noted, there are techniques for training SVMs whose cost is roughly quadratic in N", "0f0bb195-411e-417d-a4c0-5bafbaa36ed0": "But some problems involve goals that are di\ufb03cult to translate into reward signals. This is especially true when the problem requires the agent to skillfully perform a complex task or set of tasks, such as would be required of a useful household robotic assistant. Further, reinforcement learning agents can discover unexpected ways to make their environments deliver reward, some of which might be undesirable, or even dangerous. This is a longstanding and critical challenge for any method, like reinforcement learning, that is based on optimization. We discuss this issue more in Section 17.6, the \ufb01nal section of this book. Even when there is a simple and easily identi\ufb01able goal, the problem of sparse reward often arises.\n\nDelivering non-zero reward frequently enough to allow the agent to achieve the goal once, let alone to learn to achieve it e\ufb03ciently from multiple initial conditions, can be a daunting challenge. State\u2013action pairs that clearly deserve to trigger reward may be few and far between, and rewards that mark progress toward a goal can be infrequent because progress is di\ufb03cult or even impossible to detect. The agent may wander aimlessly for long periods of time (what Minsky, 1961, called the \u201cplateau problem\u201d). In practice, designing a reward signal is often left to an informal trial-and-error search for a signal that produces acceptable results", "73ba7d5a-c1c1-469b-a8bd-cf5631f927b1": "For a conventional n-step method, the learning rule to use in conjunction with (7.13) is the n-step TD update (7.2), which has no explicit importance sampling ratios other than those embedded in the return. For action values, the o\u21b5-policy de\ufb01nition of the n-step return is a little di\u21b5erent because the \ufb01rst action does not play a role in the importance sampling. That \ufb01rst action is the one being learned; it does not matter if it was unlikely or even impossible under the target policy\u2014it has been taken and now full unit weight must be given to the reward and state that follows it. Importance sampling will apply only to the actions that follow it. First note that for action values the n-step on-policy return ending at horizon h, expectation form (7.7), can be written recursively just as in (7.12), except that for action values the recursion ends with Gh:h .= \u00afVh\u22121(Sh) as in (7.8). An o\u21b5-policy form with control variates is the recursion ends with and GT \u22121:h .= RT", "716869d5-2cb1-4f0e-b836-cd080ed464d8": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3612\u20133621. Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. 2008. Maximum entropy inverse reinforcement learning. In Aaai, volume 8, pages 1433\u20131438. Chicago, IL, USA. Please see Table A.3 for beam search results, Figure A.1 for additional results for MLE+reward, and Table A.5 for examples. Please see Table A.4 for detailed results breakdown, and Table A.6-A.9 for examples. Examples are in the format: topic:  input sentence generated text. Finally, we conduct experiment on standard generation tasks where clean supervised data is available. The study is to examine the capabilities of the proposed RL method to train a text generation model from scratch, which has been considered as exceedingly challenging for previous RL algorithms. Setup", "5cc652f9-2c37-4490-8585-14f023195935": "CONFRONTING THE PARTITION FUNCTION  Algorithm 18.1 A naive MCMC algorithm for maximizing the log-likelihood with an intractable partition function using gradient ascent  Set \u20ac, the step size, to a small positive number. Set &, the number of Gibbs steps, high enough to allow burn in. Perhaps 100 to train an RBM on a small image patch. while not converged do  Sample a minibatch of m examples {xQ), wee xl} from the training set BHF, Links Vo log p(x\"; 8). Initialize a set of m samples {x(, Leey xO) to random values (e.g., from  a uniform or normal distribution, or possibly a distribution with marginals matched to the model\u2019s marginals). for i=1 tok do  for 7 = 1 tom do  x) < gibbs_update(x/)),  end for end for BH gE \u2014 57, Ly Velog p(x\"; 8). 0+ 0+ eg", "04441420-a1cf-464f-90fd-a8b1cdadf7cd": "We shall assume that the kernel function k(x, x\u2032) is governed by a vector \u03b8 of parameters, and we shall later discuss how \u03b8 may be learned from the training data. For two-class problems, it is suf\ufb01cient to predict p(tN+1 = 1|tN) because the value of p(tN+1 = 0|tN) is then given by 1 \u2212 p(tN+1 = 1|tN). The required where p(tN+1 = 1|aN+1) = \u03c3(aN+1). This integral is analytically intractable, and so may be approximated using sampling methods . Alternatively, we can consider techniques based on an analytical approximation. In Section 4.5.2, we derived the approximate formula (4.153) for the convolution of a logistic sigmoid with a Gaussian distribution. We can use this result to evaluate the integral in (6.76) provided we have a Gaussian approximation to the posterior distribution p(aN+1|tN)", "8c39a468-8905-41c7-83ef-f9b876524891": "MACHINE LEARNING BASICS  -\u2014_wiwsew'w (5.94)  m\u2014-1 = 1s?\n\n(5.95) m\u2014-1 where this time we use the fact that W'W = I, again from the definition of the SVD. The above analysis shows that when we project the data x to z, via the linear transformation W,, the resulting representation has a diagonal covariance matrix (as given by \u00bb? ), which immediately implies that the individual elements of z are mutually uncorrelated. This ability of PCA to transform data into a representation where the elements are mutually uncorrelated is a very important property of PCA. It is a simple example of a representation that attempts to disentangle the unknown factors of variation underlying the data. In the case of PCA, this disentangling takes the form of finding a rotation of the input space (described by W) that aligns the principal axes of variance with the basis of the new representation space associated with z. While correlation is an important category of dependency between elements of the data, we are also interested in learning representations that disentangle more complicated forms of feature dependencies", "edd8bb56-e251-45e4-960a-920632071419": "2.32 (\u22c6 \u22c6 \u22c6) www This exercise and the next provide practice at manipulating the quadratic forms that arise in linear-Gaussian models, as well as giving an independent check of results derived in the main text. Consider a joint distribution p(x, y) de\ufb01ned by the marginal and conditional distributions given by (2.99) and (2.100). By examining the quadratic form in the exponent of the joint distribution, and using the technique of \u2018completing the square\u2019 discussed in Section 2.3, \ufb01nd expressions for the mean and covariance of the marginal distribution p(y) in which the variable x has been integrated out. To do this, make use of the Woodbury matrix inversion formula (2.289).\n\nVerify that these results agree with (2.109) and (2.110) obtained using the results of Chapter 2. 2.33 (\u22c6 \u22c6 \u22c6) Consider the same joint distribution as in Exercise 2.32, but now use the technique of completing the square to \ufb01nd expressions for the mean and covariance of the conditional distribution p(x|y). Again, verify that these agree with the corresponding expressions (2.111) and (2.112)", "cda38966-f87e-4a53-bbba-29c2ce27e856": "Speci\ufb01cally, setting the gradient of (3.27) with respect to w to zero, and solving for w as before, we obtain This represents a simple extension of the least-squares solution (3.15). A more general regularizer is sometimes used, for which the regularized error where q = 2 corresponds to the quadratic regularizer (3.27). Figure 3.3 shows contours of the regularization function for different values of q. The case of q = 1 is know as the lasso in the statistics literature (Tibshirani, 1996). It has the property that if \u03bb is suf\ufb01ciently large, some of the coef\ufb01cients wj are driven to zero, leading to a sparse model in which the corresponding basis functions play no role.\n\nTo see this, we \ufb01rst note that minimizing (3.29) is equivalent to minimizing the unregularized sum-of-squares error (3.12) subject to the constraint Exercise 3.5 for an appropriate value of the parameter \u03b7, where the two approaches can be related using Lagrange multipliers. The origin of the sparsity can be seen from Figure 3.4, Appendix E which shows that the minimum of the error function, subject to the constraint (3.30)", "50051cde-e509-47b0-88b2-b5fd08eb3dbf": "Carlstr\u00a8om, J., Nordstr\u00a8om, E. Control of self-similar ATM call tra\ufb03c by reinforcement learning. In Proceedings of the International Workshop on Applications of Neural Networks to Telecommunications 3, pp. 54\u201362. Erlbaum, Hillsdale, NJ. Chapman, D., Kaelbling, L. P. Input generalization in delayed reinforcement learning: An algorithm and performance comparisons. In Proceedings of the Twelfth International Conference on Arti\ufb01cial Intelligence (IJCAI-91), pp. 726\u2013731. Morgan Kaufmann, San Mateo, CA. Chaslot, G., Bakkes, S., Szita, I., Spronck, P. Monte-Carlo tree search: A new framework for game AI. In Proceedings of the Fourth AAAI Conference on Arti\ufb01cial Intelligence and Interactive Digital Entertainment (AIDE-08), pp. 216\u2013217. AAAI Press, Menlo Park, CA. Chow, C.-S., Tsitsiklis, J. N", "2dbd76fa-7b13-49c7-8905-416668403aac": "Likewise, if a = \u20141, the probability of assigning b to be \u20141 is close to 1. According to Pyode) (a, b), both signs of both variables are equally likely. According to Pyode) (a | b), both variables should have the same sign. This means  that Gibbs sampling will only very rarely flip the signs of these variables. In more practical scenarios, the challenge is even greater because we care about making transitions not only between two modes but more generally between all the many modes that a real model might contain. If several such transitions are difficult because of the difficulty of mixing between modes, then it becomes very expensive to obtain a reliable set of samples covering most of the modes, and convergence of the chain to its stationary distribution is very slow.\n\nSometimes this problem can be resolved by finding groups of highly dependent units and updating all of them simultaneously in a block. Unfortunately, when the dependencies are complicated, it can be computationally intractable to draw a sample from the group. After all, the problem that the Markov chain was originally introduced to solve is this problem of sampling from a large group of variables", "afa1e6c2-428f-440a-b556-6440dd84c457": "Fortunately, minimizing the expected value of L(a,@) is equivalent to minimizing the expected value of  ~ n oO 1 fa) 2 L(z, 0) = > (se log Pmodel (x; 9) + 2 (= log Pmodet (%; 0)) ) \u2019 (18.25) j=l j  where n is the dimensionality of x. Because score matching requires taking derivatives with respect to x, it is not applicable to models of discrete data but the latent variables in the model may be discrete. Like pseudolikelihood, score matching only works when we are able to evaluate log p(x) and its derivatives directly.\n\nIt is not compatible with methods that provide only a lower bound on log p(x), because score matching requires the derivatives and second derivatives of log (x), and a lower bound conveys no information about its derivatives. This means that score matching cannot be applied to estimating models with complicated interactions between the hidden units, such as sparse coding models or deep Boltzmann machines", "1fa993e9-e15c-4b74-a3df-345ea898d661": "The unit does not constitute a cavalry unit or for use on troops.\\n\\n The army is not under the command of a brigade from the front. For legal: there liable injunction This essay discusses the potential legal consequences of a stay in the United States for an inde\ufb01nite period of time if the government continues to delay the process of de-instituting it. To apply such a request, all applicable laws shall apply either the same terms as the existing statutes. In politics: the primary referendum was This essay discusses the electoral strategy against a candidate for governor of the Commonwealth.\\n\\n The survey of British voters in this survey provides an overview of what the candidates for the United Kingdom will be seeking in the next Parliament. In the general election a few seats will lead up to a computers: the macintoshintosh This essay discusses the various problems of the Macintosh, the \ufb01rst two-year running environment", "37801143-03a9-4f8c-afe7-a3e530a626f2": "One easy way to add capacity is thus to combine both approaches in an ensemble consisting of a neural language model and an n-gram language model . As with any ensemble, this technique can reduce test error if he ensemble members make independent mistakes.\n\nThe field of ensemble learning provides many ways of combining the ensemble members\u2019 predictions, including uniform weighting and weights chosen on a validation set. Mikolov et al. (201 1a) extended the ensemble to include not just two models but a large array of models. It is also possible to pair a neural network with a maximum entropy model and rain both jointly . This approach can be viewed as training a neural network with an extra set of inputs that are connected directly to the output and not connected to any other part of the model. The extra inputs are indicators for the presence of particular n-grams in the input context, so these variables are very high dimensional and very sparse. The increase in model capacity  467  CHAPTER 12. APPLICATIONS  is huge\u2014the new portion of the architecture contains up to |sV |\" parameters\u2014but the amount of added computation needed to process an input is minimal because the extra inputs are very sparse", "d8ac97ef-2e0e-41f0-aca1-3681d5833795": "Potentially we can set f = g.  Full Context Embeddings  The embedding vectors are critical inputs for building a good classifier. Taking a single data point as input might not be enough to efficiently gauge the entire feature space. Therefore, the Matching Network model further proposed to enhance the embedding functions by taking as input the whole support set S'in addition to the original input, so that the learned embedding can be adjusted based on the relationship with other support samples.\n\n90(X;, 5) uses a bidirectional LSTM to encode x; in the context of the entire support set S. f(x, S) encodes the test sample x visa an LSTM with read attention over the support set S.  First the test sample goes through a simple neural network, such as a CNN, to extract basic features, f\u2019(x)", "287dc4b7-efaf-4f54-a046-8b60ff9a87b5": "If the value of \u03b10 is small, then the posterior distribution will be in\ufb02uenced primarily by the data rather than by the prior. Similarly, we introduce an independent Gaussian-Wishart prior governing the mean and precision of each Gaussian component, given by because this represents the conjugate prior distribution when both the mean and precision are unknown. Typically we would choose m0 = 0 by symmetry. Section 2.3.6 The resulting model can be represented as a directed graph as shown in Figure 10.5. Note that there is a link from \u039b to \u00b5 since the variance of the distribution over \u00b5 in (10.40) is a function of \u039b. This example provides a nice illustration of the distinction between latent variables and parameters. Variables such as zn that appear inside the plate are regarded as latent variables because the number of such variables grows with the size of the data set. By contrast, variables such as \u00b5 that are outside the plate are \ufb01xed in number independently of the size of the data set, and so are regarded as parameters.\n\nFrom the perspective of graphical models, however, there is really no fundamental difference between them", "d7bf5e71-d80b-4449-b99e-1cb19b7115f4": "d  This optimization problem may be solved using eigendecomposition. Specifically, the optimal d is given by the eigenvector of X ' X corresponding to the larges  eigenvalue. (2.79)  2.80  2.81  2.82  2.83  2.84  This derivation is specific to the case of 1 = 1 and recovers only the first principal component. More generally, when we wish to recover a basis of principal  49  CHAPTER 2. LINEAR ALGEBRA  components, the matrix D is given by the | eigenvectors corresponding to the largest eigenvalues. This may be shown using proof by induction. We recommend  writing this proof as an exercise. Linear algebra is one of the fundamental mathematical disciplines necessary to understanding deep learning. Another key area of mathematics that is ubiquitous  in machine learning is probability theory, presented next. https://www.deeplearningbook.org/contents/linear_algebra.html       50  https://www.deeplearningbook.org/contents/linear_algebra.html", "890d4c86-f02e-4813-bb05-66e2a7d9b808": "E,12  = A-\u2014aB+ O(a?) E,12 = A \u20142aB + O(a?) E,1,2(gReptite] =2A-aB+ O(a?) It is not clear to me whether the ignored term O(a?) might play a big impact on the parameter learning. But given that FOMAML is able to obtain a similar performance as the full version of MAML, it might be safe to say higher-level derivatives would not be critical during gradient descent update. Cited as:  @article{weng2018metalearning,  title = \"Meta-Learning: Learning to Learn Fast\", author = \"Weng, Lilian\",  journal = \"lilianweng.github. io\",  year = \"2018\",  https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Log   url = \"https://lilianweng.github. io/posts/2018-11-30-meta-learning/\" +  Reference   Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B", "fac7f81e-6243-46ed-9aa6-788f59f26e6d": "*  https://www.deeplearningbook.org/contents/optimization.html       Apply update: @6*+1 = Ot +eE pt pee ey  end while  Nonlinear Conjugate Gradients: So far we have discussed the method of conjugate gradients as it is applied to quadratic objective functions. Of course, our primary interest in this chapter is to explore optimization methods for training neural networks and other related deep learning models where the corresponding  311  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  objective function is far from quadratic.\n\nPerhaps surprisingly, the method of conjugate gradients is still applicable in this setting, though with some modification. Without any assurance that the objective is quadratic, the conjugate directions are no longer assured to remain at the minimum of the objective for previous directions. As a result, the nonlinear conjugate gradients algorithm includes occasional resets where the method of conjugate gradients is restarted with line search along the unaltered gradient. Practitioners report reasonable results in applications of the nonlinear conjugate gradients algorithm to training neural networks, though it is often beneficial to initialize the optimization with a few iterations of stochastic gradient descent before commencing nonlinear conjugate gradients", "9799d069-32da-4f67-8728-d0de2f27d6e1": "The idea of using such skip connections dates back to Lin et al.\n\nand follows from the idea of incorporating delays in feedforward neural networks . In an ordinary recurrent network, a recurrent connection goes from a unit at time \u00a2 to a unit at time t+ 1. It is possible to construct recurrent networks with longer delays . As we have seen in section 8.2.5, gradients may vanish or explode exponentially with respect to the number of time steps. Lin ef al. introduced recurrent connections with a time delay of d to mitigate this problem. Gradients now diminish exponentially as a function of 7 rather than r. Since there are both delayed and single step connections, gradients may still explode exponentially in r. This allows the learning algorithm to capture longer dependencies, although not all long-term dependencies may be represented well in this way. 10.9.2 Leaky Units and a Spectrum of Different Time Scales  Another way to obtain paths on which the product of derivatives is close to one is to have units with linear self-connections and a weight near one on these connections", "0def9794-5c35-42b1-8231-919fdcac646c": "This is an example of a recurrent network that maps an input sequence to an output sequence of the same length. The total loss for a given sequence of x values paired with a sequence of y values would then be just the sum of the losses over all the time steps. For example, if L\u00ae is the negative log-likelihood of y\u00ae given #@,...,2, then  L({a, Le}, {y),...,y}) (10.12) => (10.13) t == Slogpmoae (y | fe,...,2}), (10.14) t  where Pmodel (y {a ... a }) is given by reading the entry for y from the model\u2019s output vector gy. Computing the gradient of this loss function with respect to the parameters is an expensive operation", "56a065dd-d2ff-40dc-81ca-21d391f63cdf": "Designing a problem solution boils down to choosing what experience to use depending on the problem structure and available resources, without worrying too much about how to use the experience in the training.\n\nBesides, the standardized ML perspective also highlights that many learning problems in di\ufb00erent research areas are essentially the same and just correspond to di\ufb00erent speci\ufb01cations of the SE components. This enables us to systematically repurpose successful techniques in one area to solve problems in another. The remainder of the article is organized as follows. Section 2 gives an overview of relevant learning and inference techniques as a prelude of the standardized framework. Section 3 presents the standard equation as a general formulation of the objective function in learning algorithms. The subsequent two sections discuss di\ufb00erent choices of two of the key components in the standard equation, respectively, illustrating that many existing methods are special cases of the formulation: Section 4 is devoted to discussion of the experience function and Section 5 focuses on the divergence function. Section 6 discusses an extended view of the standard equation in dynamic environments. Section 7 focuses on the optimization algorithms for solving the standard equation objective. Section 8 discusses the diverse types of target models. Section 9 discusses the utility of the standardized formalism for mechanical design of panoramic learning approaches", "a61077b7-fda1-4e85-bc21-2f783aa71b51": "p(h() | nik} x(k) tells how to update the latent state variable, given the previous latent state and visible variable. Denoising autoencoders and GSNs differ from classical probabilistic models (directed or undirected) in that they parametrize the generative process itself rather than the mathematical specification of the joint distribution of visible and latent variables. Instead, the latter is defined implicitly, if it exists, as the stationary  710  CHAPTER 20. DEEP GENERATIVE MODELS  distribution of the generative Markov chain. The conditions for existence of the stationary distribution are mild and are the same conditions required by standard MCMC methods (see section 17.3). These conditions are necessary to guarantee that the chain mixes, but they can be violated by some choices of the transition  woot oas ve 1 sed ts soe aN  https://www.deeplearningbook.org/contents/generative_models.html    GISULIDULIONS (LOF exallple, U wey are Geverustic). One could imagine different training criteria for GSNs. The one proposed and evaluated by Bengio ef al", "92366f8e-07cd-4702-ad73-48065aa075da": "The Bellman operator is for all s 2 S and v : S ! R. The Bellman error vector for v can be written \u00af\u03b4w = B\u21e1vw\u2212vw. If the Bellman operator is applied to a value function in the representable subspace, then, in general, it will produce a new value function that is outside the subspace, as suggested in the \ufb01gure. In dynamic programming (without function approximation), this operator is applied repeatedly to the points outside the representable space, as suggested by the gray arrows in the top of Figure 11.3. Eventually that process converges to the true value function v\u21e1, the only \ufb01xed point for the Bellman operator, the only value function for which which is just another way of writing the Bellman equation for \u21e1 (11.13). With function approximation, however, the intermediate value functions lying outside the subspace cannot be represented", "aa938e13-1b88-4f46-8900-4e8932b2634e": "Springer, 1998.\n\nR. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, and C. Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In EMNLP, pages 1631\u20131642, 2013. B. Tan, Z. Hu, Z. Yang, R. Salakhutdinov, and E. Xing. Connecting the dots between MLE and RL for sequence generation. arXiv preprint arXiv:1811.09740, 2018. T. Tran, T. Pham, G. Carneiro, L. Palmer, and I. Reid. A Bayesian data augmentation approach for learning deep models. In NeurIPS, pages 2797\u20132806, 2017. J. W. Wei and K. Zou", "4d7988c6-12c0-4396-a360-d481be3a1ede": "Expressing the log-prior as an absolute value penalty, we obtain  https://www.deeplearningbook.org/contents/autoencoders.html    O(h) = AS Ih, (14.6)  \u2014 log Pmodel(h) = S- (ain \u2014 log *) = Q(h) + const, (14.7) u  where the constant term depends only on \\ and not h. We typically treat \\ as a hyperparameter and discard the constant term since it does not affect the parameter learning. Other priors, such as the Student t\u00a2 prior, can also induce sparsity.\n\nFrom this point of view of sparsity as resulting from the effect of pmodel(h) on approximate maximum likelihood learning, the sparsity penalty is not a regularization term at all. It is just a consequence of the model\u2019s distribution over its latent variables. This view provides a different motivation for training an autoencoder: it is a way of approximately training a generative model. It also provides a different reason for  503  CHAPTER 14. AUTOENCODERS  why the features learned by the autoencoder are useful: they describe the latent variables that explain the input", "2c465598-e630-456b-9c5b-85eb53d3b637": "Other features were based on the rules of Go, such as the number of adjacent points that were empty, the number of opponent stones that would be captured by placing a stone there, the number of turns since a stone was placed there, and other features that the design team considered to be important. Training the SL policy network took approximately 3 weeks using a distributed implementation of stochastic gradient ascent on 50 processors. The network achieved 57% accuracy, where the best accuracy achieved by other groups at the time of publication was 44.4%. Training the RL policy network was done by policy gradient reinforcement learning over simulated games between the RL policy network\u2019s current policy and opponents using policies randomly selected from policies produced by earlier iterations of the learning algorithm.\n\nPlaying against a randomly selected collection of opponents prevented over\ufb01tting to the current policy. The reward signal was +1 if the current policy won, \u22121 if it lost, and zero otherwise. These games directly pitted the two policies against one another without involving MCTS. By simulating many games in parallel on 50 processors, the DeepMind team trained the RL policy network on a million games in a single day", "0f1b92ad-c930-4f7d-86eb-595a3d18e0ca": "Finally, it is worth noting that the lower bound provides an alternative approach for deriving the variational re-estimation equations obtained in Section 10.2.1. To do this we use the fact that, since the model has conjugate priors, the functional form of the factors in the variational posterior distribution is known, namely discrete for Z, Dirichlet for \u03c0, and Gaussian-Wishart for (\u00b5k, \u039bk). By taking general parametric forms for these distributions we can derive the form of the lower bound as a function of the parameters of the distributions.\n\nMaximizing the bound with respect to these parameters then gives the required re-estimation equations. Exercise 10.18 In applications of the Bayesian mixture of Gaussians model we will often be interested in the predictive density for a new value \ufffdx of the observed variable. Associated with this observation will be a corresponding latent variable \ufffdz, and the predictive density is then given by where p(\u03c0, \u00b5, \u039b|X) is the (unknown) true posterior distribution of the parameters", "892da279-eefa-49ae-985f-2c79c87e658d": "For the purposes of this example, we have made a linear re-scaling of the data, known as standardizing, such that each of the variables has zero mean and unit standard deviation. For this example, we have chosen K = 2, and so in this denote the data set in a two-dimensional Euclidean space. The initial choices for centres \u00b51 and \u00b52 are shown by the red and blue crosses, respectively. (b) In the initial E step, each data point is assigned either to the red cluster or to the blue cluster, according to which cluster centre is nearer. This is equivalent to classifying the points according to which side of the perpendicular bisector of the two cluster centres, shown by the magenta line, they lie on. (c) In the subsequent M step, each cluster centre is re-computed to be the mean of the points assigned to the corresponding cluster.\n\n(d)\u2013(i) show successive E and M steps through to \ufb01nal convergence of the algorithm. case, the assignment of each data point to the nearest cluster centre is equivalent to a classi\ufb01cation of the data points according to which side they lie of the perpendicular bisector of the two cluster centres", "741a2af0-0862-4540-9363-3b145603bd00": "This recurrent network just processes information from the input x by incorporating it into the stateh that is passed forward through time. (Left) Circuit diagram. The black square indicates a delay of a single time step. (Right) The same network seen as an unfolded computational graph, where each node is now associated with one particular time instance. signal a4),  sO = f(s) 2:6), (10.4) where we see that the state now contains information about the whole past sequence. Recurrent neural networks can be built in many different ways. Much as almost any function can be considered a feedforward neural network, essentially any function involving recurrence can be considered a recurrent neural network.\n\nMany recurrent neural networks use equation 10.5 or a similar equation to define the values of their hidden units. To indicate that the state is the hidden units of the network, we now rewrite equation 10.4 using the variable h to represent the state,  AO = fin), 2: 6), (10.5) illustrated in figure 10.2; typical RNNs will add extra architectural features such as output layers that read information out of the state h to make predictions", "8327508f-38c7-482c-9e6e-59b646668d90": "6) _ ap, P(R) Dv(h | ) Hh) (19.26) =Epw~p(p|v) io log p(h). (19.27)  Ob;  This requires computing expectations with respect to p(h | v). Unfortunately, p(h | v) is a complicated distribution. See figure 19.2 for the graph structure of p(h,v) and p(h | v). The posterior distribution corresponds to the complete graph over the hidden units, so variable elimination algorithms do not help us to compute the required expectations any faster than brute force. We can resolve this difficulty by using variational inference and variational learning instead. We can make a mean field approximation: q(h |v) = IL |v). (19.28)  The latent variables of the binary sparse coding model are binary, so to represent a factorial g we simply need to model m Bernoulli distributions g(h; | v)", "bcac4bba-05d1-41df-bff7-ea2c5d5ee92c": "Different algorithms involve different choices for the weight vector update \u2206w(\u03c4). Many algorithms make use of gradient information and therefore require that, after each update, the value of \u2207E(w) is evaluated at the new weight vector w(\u03c4+1).\n\nIn order to understand the importance of gradient information, it is useful to consider a local approximation to the error function based on a Taylor expansion. Insight into the optimization problem, and into the various techniques for solving it, can be obtained by considering a local quadratic approximation to the error function. Consider the Taylor expansion of E(w) around some point \ufffdw in weight space and the Hessian matrix H = \u2207\u2207E has elements From (5.28), the corresponding local approximation to the gradient is given by For points w that are suf\ufb01ciently close to \ufffdw, these expressions will give reasonable approximations for the error and its gradient. Consider the particular case of a local quadratic approximation around a point w\u22c6 that is a minimum of the error function", "8144e335-25aa-4b32-9d4a-4d33ea96ff0b": "Train the MLP to approximately maximize log P(y | v) using stochastic gradient descent and dropout. Figure reprinted from Goodfellow et al. 669  CHAPTER 20. DEEP GENERATIVE MODELS  Boltzmann machine.\n\nThe first is the centered deep Boltzmann machine Montavon and Muller, 2012), which reparametrizes the model in order to make he Hessian of the cost function better conditioned at the beginning of the learning process. This yields a model that can be trained without a greedy layer-wise pretraining stage. The resulting model obtains excellent test set log-likelihood and produces high-quality samples. Unfortunately, it remains unable to compete with appropriately regularized MLPs as a classifier. The second way to jointly rain a deep Boltzmann machine is to use a multi-prediction deep Boltzmann machine . This model uses an alternative training criterion that allows the use of the back-propagation algorithm to avoid the problems with MCMC estimates of the gradient", "9ef59acb-0e20-4ee3-b48b-e73a23a4256b": "Translating through intermediate languages with different vocabulary and linguistic structures can generate useful paraphrases. To ensure the diversity of augmented data, sampling and noisy beam search can also be adopted during the decoding stage . Other work focuses on directly training end-to-end models to generate paraphrases , and further augments the decoding phase with syntactic information , latent variables , and sub-modular objectives . Conditional Generation.\n\nConditional generation methods generate additional text from a language model, conditioned on the label. After training the model to generate the original text given the label, the model can generate new text . An extra \ufb01ltering process is often used to ensure high-quality augmented data. For example, in text classi\ufb01cation, Anaby-Tavor et al. \ufb01rst \ufb01ne-tune GPT-2  with the original examples prepended with their labels, and then generate augmented examples by feeding the \ufb01netuned model certain labels. Only con\ufb01dent examples as judged by a baseline classi\ufb01er trained on the original data are kept. Similarly, new answers are generated on the basis of given questions in question answering and are \ufb01ltered by customized metrics like question answering probability  and n-gram diversity", "8305676c-f786-4c0b-8c2e-24f0270306a0": "Guoyang Zeng, Fanchao Qi, Qianrui Zhou, Tingji Zhang, Bairu Hou, Yuan Zang, Zhiyuan Liu, and Maosong Sun. 2020. Openattack: An open-source textual adversarial attack toolkit. Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. 2018. mixup: Beyond empirical risk minimization. In International Conference on Learning Representations. Jiajun Zhang and Chengqing Zong. 2016. Exploiting source-side monolingual data in neural machine translation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1535\u20131545, Austin, Texas. Association for Computational Linguistics. Rongzhi Zhang, Yue Yu, and Chao Zhang. 2020a. SeqMix: Augmenting active sequence labeling via sequence mixup. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8566\u20138579, Online", "a027cb01-e4a9-40ce-82fe-324b03a3ae53": "Vincze (Eds. ), Progress in Statistics, pp. 241\u2013266. North-Holland, Amsterdam\u2013London. Glimcher, P. W. Understanding dopamine and reinforcement learning: The dopamine Glimcher, P. W. Decisions, Uncertainty, and the Brain: The science of Neuroeconomics. Glimcher, P. W., Fehr, E. (Eds.) . Neuroeconomics: Decision Making and the Brain, Goethe, J. W. V. The Sorcerer\u2019s Apprentice. In The Permanent Goethe, p. 349. The Goldstein, H. Classical Mechanics. Addison-Wesley, Reading, MA. Goodfellow, I., Bengio, Y., Courville, A. Deep Learning. MIT Press, Cambridge, MA. Goodwin, G. C., Sin, K. S", "2e0e8d8f-ad08-48b8-b543-0000bbbb6b5f": "Reinforcement learning\u2019s distinction between model-free and model-based algorithms is proving to be useful for thinking about animal learning and decision processes. Section 14.6 discusses how this distinction aligns with that between habitual and goal-directed animal behavior.\n\nThe hypothesis discussed above about how the brain might implement an actor\u2013critic algorithm is relevant only to an animal\u2019s habitual mode of behavior because the basic actor\u2013critic method is model-free. What neural mechanisms are responsible for producing goal-directed behavior, and how do they interact with those underlying habitual behavior? One way to investigate questions about the brain structures involved in these modes of behavior is to inactivate an area of a rat\u2019s brain and then observe what the rat does in an outcome-devaluation experiment (Section 14.6). Results from experiments like these indicate that the actor\u2013critic hypothesis described above is too simple in placing the actor in the dorsal striatum. Inactivating one part of the dorsal striatum, the dorsolateral striatum (DLS), impairs habit learning, causing the animal to rely more on goal-directed processes", "fb2cff3e-6e4a-4f8b-a892-53b2fd608af9": "We use IN to denote the N \u00d7 N identity matrix (also called the unit matrix), and where there is no ambiguity over dimensionality we simply use I.\n\nThe transpose matrix AT has elements (AT)ij = Aji. From the de\ufb01nition of transpose, we have which is easily proven by taking the transpose of (C.2) and applying (C.1). A useful identity involving matrix inverses is the following which is easily veri\ufb01ed by right multiplying both sides by (BPBT + R). Suppose that P has dimensionality N \u00d7 N while R has dimensionality M \u00d7 M, so that B is M \u00d7 N. Then if M \u226a N, it will be much cheaper to evaluate the right-hand side of (C.5) than the left-hand side. A special case that sometimes arises is Another useful identity involving inverses is the following: which is known as the Woodbury identity and which can be veri\ufb01ed by multiplying both sides by (A + BD\u22121C)", "88858517-b5c5-410d-bab3-e6d288bd85a4": "In their pure forms, these two kinds of feedback are quite distinct: evaluative feedback depends entirely on the action taken, whereas instructive feedback is independent of the action taken.\n\nIn this chapter we study the evaluative aspect of reinforcement learning in a simpli\ufb01ed setting, one that does not involve learning to act in more than one situation. This nonassociative setting is the one in which most prior work involving evaluative feedback has been done, and it avoids much of the complexity of the full reinforcement learning problem. Studying this case enables us to see most clearly how evaluative feedback di\u21b5ers from, and yet can be combined with, instructive feedback. The particular nonassociative, evaluative feedback problem that we explore is a simple version of the k-armed bandit problem. We use this problem to introduce a number of basic learning methods which we extend in later chapters to apply to the full reinforcement learning problem. At the end of this chapter, we take a step closer to the full reinforcement learning problem by discussing what happens when the bandit problem becomes associative, that is, when actions are taken in more than one situation. Consider the following learning problem. You are faced repeatedly with a choice among k di\u21b5erent options, or actions", "014e2c5d-5661-403b-b625-36f1267027ef": "With overtraining one would expect that even a formerly-unpredicted predictor state would become predicted by stimuli associated with earlier states: the animal\u2019s interaction with its environment both inside and outside of an experimental task would become commonplace.\n\nUpon breaking this routine with the introduction of a new task, however, one would see TD errors reappear, as indeed is observed in dopamine neuron activity. The example described above explains why the TD error shares key features with the phasic activity of dopamine neurons when the animal is learning in a task similar to the idealized task of our example. But not every property of the phasic activity of dopamine neurons coincides so neatly with properties of \u03b4. One of the most troubling discrepancies involves what happens when a reward occurs earlier than expected. We have seen that the omission of an expected reward produces a negative prediction error at the reward\u2019s expected time, which corresponds to the activity of dopamine neurons decreasing below baseline when this happens. If the reward arrives later than expected, it is then an unexpected reward and generates a positive prediction error. This happens with both TD errors and dopamine neuron responses", "ae6253be-ae5d-4c8e-88e2-7d6cf15928e3": "Contour and surface plots for a Gaussian mixture having 3 components are shown in Figure 2.23. In this section we shall consider Gaussian components to illustrate the framework of mixture models. More generally, mixture models can comprise linear combinations of other distributions. For instance, in Section 9.3.3 we shall consider mixtures of Bernoulli distributions as an example of a mixture model for discrete variables. Section 9.3.3 The parameters \u03c0k in (2.188) are called mixing coef\ufb01cients.\n\nIf we integrate both sides of (2.188) with respect to x, and note that both p(x) and the individual Gaussian components are normalized, we obtain Also, the requirement that p(x) \u2a7e 0, together with N(x|\u00b5k, \u03a3k) \u2a7e 0, implies \u03c0k \u2a7e 0 for all k. Combining this with the condition (2.189) we obtain density for each of the mixture components, in which the 3 components are denoted red, blue and green, and the values of the mixing coef\ufb01cients are shown below each component. (b) Contours of the marginal probability density p(x) of the mixture distribution. (c) A surface plot of the distribution p(x)", "324103a6-3b44-4c99-b7a0-2a384f1e7ee5": "It is worth emphasizing that because the linear dynamical system is a linearGaussian model, the joint distribution over all latent and observed variables is simply a Gaussian, and so in principle we could solve inference problems by using the standard results derived in previous chapters for the marginals and conditionals of a multivariate Gaussian.\n\nThe role of the sum-product algorithm is to provide a more ef\ufb01cient way to perform such computations. Linear dynamical systems have the identical factorization, given by (13.6), to hidden Markov models, and are again described by the factor graphs in Figures 13.14 and 13.15. Inference algorithms therefore take precisely the same form except that summations over latent variables are replaced by integrations. We begin by considering the forward equations in which we treat zN as the root node, and propagate messages from the leaf node h(z1) to the root. From (13.77), the initial message will be Gaussian, and because each of the factors is Gaussian, all subsequent messages will also be Gaussian. By convention, we shall propagate messages that are normalized marginal distributions corresponding to p(zn|x1,", "15b105d7-b724-40a7-a6ea-bbb3afb1b05b": "Note also the characteristic inverted-U shapes of each algorithm\u2019s performance; all the algorithms perform best at an intermediate value of their parameter, neither too large nor too small.\n\nIn assessing a method, we should attend not just to how well it does at its best parameter setting, but also to how sensitive it is to its parameter value. All of these algorithms are fairly insensitive, performing well over a range of parameter values varying by about an order of magnitude. Overall, on this problem, UCB seems to perform best. Despite their simplicity, in our opinion the methods presented in this chapter can fairly be considered the state of the art. There are more sophisticated methods, but their complexity and assumptions make them impractical for the full reinforcement learning problem that is our real focus. Starting in Chapter 5 we present learning methods for solving the full reinforcement learning problem that use in part the simple methods explored in this chapter. Although the simple methods explored in this chapter may be the best we can do at present, they are far from a fully satisfactory solution to the problem of balancing exploration and exploitation. One well-studied approach to balancing exploration and exploitation in k-armed bandit problems is to compute a special kind of action value called a Gittins index", "9b3299d8-7b84-44c9-9684-97e3e6d419c3": "Each example is a description of a situation together with a speci\ufb01cation\u2014the label\u2014of the correct action the system should take to that situation, which is often to identify a category to which the situation belongs. The object of this kind of learning is for the system to extrapolate, or generalize, its responses so that it acts correctly in situations not present in the training set. This is an important kind of learning, but alone it is not adequate for learning from interaction.\n\nIn interactive problems it is often impractical to obtain examples of desired behavior that are both correct and representative of all the situations in which the agent has to act. In uncharted territory\u2014where one would expect learning to be most bene\ufb01cial\u2014an agent must be able to learn from its own experience. Reinforcement learning is also di\u21b5erent from what machine learning researchers call unsupervised learning, which is typically about \ufb01nding structure hidden in collections of unlabeled data. The terms supervised learning and unsupervised learning would seem to exhaustively classify machine learning paradigms, but they do not. Although one might be tempted to think of reinforcement learning as a kind of unsupervised learning because it does not rely on examples of correct behavior, reinforcement learning is trying to maximize a reward signal instead of trying to \ufb01nd hidden structure", "bd7a95e0-8ccf-47f8-980d-02f388efd891": "Rewards are basically given directly by the environment, but values must be estimated and re-estimated from the sequences of observations an agent makes over its entire lifetime. In fact, the most important component of almost all reinforcement learning algorithms we consider is a method for e\ufb03ciently estimating values. The central role of value estimation is arguably the most important thing that has been learned about reinforcement learning over the last six decades. The fourth and \ufb01nal element of some reinforcement learning systems is a model of the environment.\n\nThis is something that mimics the behavior of the environment, or more generally, that allows inferences to be made about how the environment will behave. For example, given a state and action, the model might predict the resultant next state and next reward. Models are used for planning, by which we mean any way of deciding on a course of action by considering possible future situations before they are actually experienced. Methods for solving reinforcement learning problems that use models and planning are called model-based methods, as opposed to simpler model-free methods that are explicitly trial-and-error learners\u2014viewed as almost the opposite of planning. In Chapter 8 we explore reinforcement learning systems that simultaneously learn by trial and error, learn a model of the environment, and use the model for planning", "d4f9aa72-8e04-4e97-ab37-66377af3a536": "(9.1)  This operation is called convolution. The convolution operation is typically denoted with an asterisk:  s(t) = (x* w)(t). (9.2)  In our example, w needs to be a valid probability density function, or the output will not be a weighted average. Also, w needs to be 0 for all negative arguments, or it will look into the future, which is presumably beyond our capabilities. These limitations are particular to our example, though. In general, convolution is defined for any functions for which the above integral is defined and may be used for other  https://www.deeplearningbook.org/contents/convnets.html    purposes besides taking weighted averages.\n\nIn convolutional network terminology, the first argument (in this example, the function 2) to the convolution is often \u2018referred to as the input, and the second  327  CHAPTER 9. CONVOLUTIONAL NETWORKS  argument (in this example, the function w) as the kernel. The output is sometimes referred to as the feature map. In our example, the idea of a laser sensor that can provide measurements at every instant is not realistic", "21d8d0bc-2c20-449c-b35e-d5a30c92c712": "The choice is informed both by planning\u2014 anticipating possible replies and counterreplies\u2014and by immediate, intuitive judgments of the desirability of particular positions and moves. \u2022 An adaptive controller adjusts parameters of a petroleum re\ufb01nery\u2019s operation in real time. The controller optimizes the yield/cost/quality trade-o\u21b5 on the basis of speci\ufb01ed marginal costs without sticking strictly to the set points originally suggested by engineers.\n\nA gazelle calf struggles to its feet minutes after being born. Half an hour later it is \u2022 A mobile robot decides whether it should enter a new room in search of more trash to collect or start trying to \ufb01nd its way back to its battery recharging station. It makes its decision based on the current charge level of its battery and how quickly and easily it has been able to \ufb01nd the recharger in the past. \u2022 Phil prepares his breakfast. Closely examined, even this apparently mundane activity reveals a complex web of conditional behavior and interlocking goal\u2013subgoal relationships: walking to the cupboard, opening it, selecting a cereal box, then reaching for, grasping, and retrieving the box. Other complex, tuned, interactive sequences of behavior are required to obtain a bowl, spoon, and milk carton", "750e8a7f-834f-4525-8396-1f27577e73e8": "Discriminative approaches based on contrastive learning in the latent space have recently shown great promise, achieving state-of-theart results . In this work, we introduce a simple framework for contrastive learning of visual representations, which we call SimCLR. Not only does SimCLR outperform previous work (Figure 1), but it is also simpler, requiring neither specialized architectures  nor a memory bank .\n\nIn order to understand what enables good contrastive representation learning, we systematically study the major components of our framework and show that: arXiv:2002.05709v3    1 Jul 2020 A Simple Framework for Contrastive Learning of Visual Representations \u2022 Composition of multiple data augmentation operations is crucial in de\ufb01ning the contrastive prediction tasks that yield effective representations. In addition, unsupervised contrastive learning bene\ufb01ts from stronger data augmentation than supervised learning. \u2022 Introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations. \u2022 Representation learning with contrastive cross entropy loss bene\ufb01ts from normalized embeddings and an appropriately adjusted temperature parameter. \u2022 Contrastive learning bene\ufb01ts from larger batch sizes and longer training compared to its supervised counterpart", "d1ecc5fd-442c-4416-a0e2-12d4eb3d6a11": "Applications in which the training data comprises examples of the input vectors along with their corresponding target vectors are known as supervised learning problems. Cases such as the digit recognition example, in which the aim is to assign each input vector to one of a \ufb01nite number of discrete categories, are called classi\ufb01cation problems.\n\nIf the desired output consists of one or more continuous variables, then the task is called regression. An example of a regression problem would be the prediction of the yield in a chemical manufacturing process in which the inputs consist of the concentrations of reactants, the temperature, and the pressure. In other pattern recognition problems, the training data consists of a set of input vectors x without any corresponding target values. The goal in such unsupervised learning problems may be to discover groups of similar examples within the data, where it is called clustering, or to determine the distribution of data within the input space, known as density estimation, or to project the data from a high-dimensional space down to two or three dimensions for the purpose of visualization. Finally, the technique of reinforcement learning  is concerned with the problem of \ufb01nding suitable actions to take in a given situation in order to maximize a reward", "e3db1ada-ac7d-4ff2-b9c7-6cef11e4c837": "Most of the augmentations surveyed operate in the input layer. However, some are derived from hidden layer representations, and one method, DisturbLabel , is even manifested in the output layer. The space of intermediate representations and the label space are under-explored areas of Data Augmentation with interesting results. This survey focuses on applications for image data, although many of these techniques and concepts can be expanded to other data domains.\n\nData Augmentation cannot overcome all biases present in a small dataset. For exam-  ple, in a dog breed classification task, if there are only bulldogs and no instances of Shorten and Khoshgoftaar J Big Data  6:60   golden retrievers, no augmentation method discussed, from SamplePairing to AutoAug- ment to GANs, will create a golden retriever. However, several forms of biases such as lighting, occlusion, scale, background, and many more are preventable or at least dra- matically lessened with Data Augmentation. Overfitting is generally not as much of an issue with access to big data", "1a347cf8-4cfb-4b89-9b95-c9a80c81888d": "The algorithms presented earlier in this chapter that do not take into account interest and emphasis (in (9.7) and the box on page 202) will converge (for decreasing step sizes) to the parameter vector w1 = (3.5, 1.5), which gives the \ufb01rst state\u2014the only one we are interested in\u2014a value of 3.5 (i.e., intermediate between the true values of the \ufb01rst and second states).\n\nThe methods presented in this section that do use interest and emphasis, on the other hand, will learn the value of the \ufb01rst state exactly correctly; w1 will converge to 4 while w2 will never be updated because the emphasis is zero in all states save the leftmost. Now consider applying two-step semi-gradient TD methods. The methods from earlier in this chapter without interest and emphasis (in (9.15) and (9.16) and the box on page 209) will again converge to w1 = (3.5, 1.5), while the methods with interest and emphasis converge to w1 = (4, 2). The latter produces the exactly correct values for the \ufb01rst state and for the third state (which the \ufb01rst state bootstraps from) while never making any updates corresponding to the second or fourth states. Reinforcement learning systems must be capable of generalization if they are to be applicable to arti\ufb01cial intelligence or to large engineering applications", "2daff312-441d-4ece-8b2a-be7325cd7d36": "It can be regarded as a generalization of the notion of matrix inverse to nonsquare matrices. Indeed, if \u03a6 is square and invertible, then using the property (AB)\u22121 = B\u22121A\u22121 we see that \u03a6\u2020 \u2261 \u03a6\u22121. At this point, we can gain some insight into the role of the bias parameter w0. If we make the bias parameter explicit, then the error function (3.12) becomes Setting the derivative with respect to w0 equal to zero, and solving for w0, we obtain Thus the bias w0 compensates for the difference between the averages (over the training set) of the target values and the weighted sum of the averages of the basis function values.\n\nWe can also maximize the log likelihood function (3.11) with respect to the noise precision parameter \u03b2, giving and so we see that the inverse of the noise precision is given by the residual variance of the target values around the regression function. At this point, it is instructive to consider the geometrical interpretation of the least-squares solution. To do this we consider an N-dimensional space whose axes are given by the tn, so that t = (t1, . , tN)T is a vector in this space", "85081e58-4e7e-4ddc-b852-2d371270109c": "The prepended augmentation net maps them into a new image through a CNN with 5 layers, each with 16 channels, 3 x 3 filters, and ReLU activation functions. The image outputted from the augmentation is then transformed with another random image via Neural Style Transfer. This style transfer is carried out via the CycleGAN  exten- sion of the GAN  framework. These images are then fed into a classification model and the error from the classification model is backpropagated to update the Neural Augmentation net. The Neural Augmentation network uses this error to learn the opti- mal weighting for content and style images between different images as well as the mapping between images in the CNN (Fig. 29). Perez and Wang tested their algorithm on the MNIST and Tiny-imagenet-200 data- sets on binary classification tasks such as cat versus dog. The Tiny-imagenet-200 dataset is used to simulate limited data", "47cbe1c9-8558-42fe-8a73-223dde8b8aed": "D., Niv, Y., Dayan, P. Uncertainty based competition between prefrontal and dorsolateral striatal systems for behavioral control. Nature Neuroscience, 8(12):1704\u20131711. Daw, N. D., Shohamy, D. The cognitive neuroscience of motivation and learning. Social Dayan, P. Reinforcement comparison. In D. S. Touretzky, J. L. Elman, T. J. Sejnowski, and G. E. Hinton (Eds. ), Connectionist Models: Proceedings of the 1990 Summer School, pp. 45\u201351. Morgan Kaufmann. Dayan, P. The convergence of TD(\u03bb) for general \u03bb. Machine Learning, 8(3):341\u2013362. Dayan, P. Matters temporal. Trends in Cognitive Sciences, 6(3):105\u2013106. Dayan, P., Abbott, L. F", "ab623b25-9512-42c4-97d8-9da08fd7ef8f": "Thus the solution to the regression problem decouples between the different target variables, and we need only compute a single pseudo-inverse matrix \u03a6\u2020, which is shared by all of the vectors wk. The extension to general Gaussian noise distributions having arbitrary covariance matrices is straightforward. Again, this leads to a decoupling into K indeExercise 3.6 pendent regression problems.\n\nThis result is unsurprising because the parameters W de\ufb01ne only the mean of the Gaussian noise distribution, and we know from Section 2.3.4 that the maximum likelihood solution for the mean of a multivariate Gaussian is independent of the covariance. From now on, we shall therefore consider a single target variable t for simplicity. So far in our discussion of linear models for regression, we have assumed that the form and number of basis functions are both \ufb01xed. As we have seen in Chapter 1, the use of maximum likelihood, or equivalently least squares, can lead to severe over-\ufb01tting if complex models are trained using data sets of limited size. However, limiting the number of basis functions in order to avoid over-\ufb01tting has the side effect of limiting the \ufb02exibility of the model to capture interesting and important trends in the data", "c006b2f9-2d9f-4ec6-8bd4-bf5295428eef": "12.7 (* *) By making use of the results (2.270) and (2.271) for the mean and covariance of a general distribution, derive the result (12.35) for the marginal distribution p(x) in the probabilistic PCA model. 12.8 (* *)Imm By making use of the result (2.116), show that the posterior distribution p(zlx) for the probabilistic PCA model is given by (12.42). 12.10 (**) By evaluating the second derivatives of the log likelihood function (12.43) for the probabilistic PCA model with respect to the parameter JL, show that the stationary point JLML = x represents the unique maximum. 12.11 (* *)Imm Show that in the limit (Y2 -. 0, the posterior mean for the probabilistic PCA model becomes an orthogonal projection onto the principal subspace, as in conventional PCA.\n\n12.12 (* *) For (Y2 > 0 show that the posterior mean in the probabilistic PCA model is shifted towards the origin relative to the orthogonal projection", "03fb1d8d-552b-4f9c-9786-77961ffa7b3e": "The popular expectation maximization (EM) algorithm for unsupervised learning via MLE can be interpreted as minimizing the variational free energy .\n\nIn fact, as we discuss subsequently, popular heuristics such as the variational EM and the wake-sleep algorithms, are approximations to the EM algorithm by introducing approximating realizations to either the free energy objective function L or to the solution space of the variational distribution q. Expectation Maximization (EM). The most common approach to learning with unlabeled data or partially observed multivariate models is perhaps the EM algorithm . With the use of the variational free energy as a surrogate objective to the original marginal likelihood as in Equation 2.9, EM can be also understood as an alternating minimization algorithm, where L(q, \u03b8) is minimized with regard to q and \u03b8 in two stages, respectively. At each iteration n, the expectation (E) step maximizes L(q, \u03b8(n)) w.r.t. q. From Equation 2.9, this is achieved by setting q to the current true posterior: so that the KL divergence vanishes and the upper bound is tight", "17dd6e32-92e8-4889-b27a-3bdf2ac02a42": "If m is the maximum number of variables appearing (on either side of the conditioning bar) in a single conditional probability distribution, then the cost of the tables for the directed model scales like O(k). As long as we can design a model such that m << n, we get very dramatic savings. In other words, as long as each variable has few parents in the graph, the distribution can be represented with very few parameters. Some restrictions on the graph structure, such as requiring it to be a tree, can also guarantee that operations like computing marginal or conditional distributions over subsets of variables are efficient. It is important to realize what kinds of information can and cannot be encoded in the graph.\n\nThe graph encodes only simplifying assumptions about which variables  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    are conditionally independent from each other. It is also possible to make other kinds of simplifying assumptions. For example, suppose we assume Bob always runs the same regardless of how Alice performs", "63ae323e-6ffb-4bb0-9e10-32f715ef2936": "The concept of learnability that we introduce in this section is di\u21b5erent from that commonly used in machine learning. There, a hypothesis is said to be \u201clearnable\u201d if it is e\ufb03ciently learnable, meaning that it can be learned within a polynomial rather than an exponential number of examples. Here we use the term in a more basic way, to mean learnable at all, with any amount of experience.\n\nIt turns out many quantities of apparent interest in reinforcement learning cannot be learned even from an in\ufb01nite amount of experiential data. These quantities are well de\ufb01ned and can be computed given knowledge of the internal structure of the environment, but cannot be computed or estimated from the observed sequence of feature vectors, actions, and rewards.2 We say that they are not learnable. It will turn out that the Bellman error objective (BE) introduced in the last two sections is not learnable in this sense. That the Bellman error objective cannot be learned from the observable data is probably the strongest reason not to seek it. To make the concept of learnability clear, let\u2019s start with some simple examples", "619578c8-73a5-452a-8477-251626010fd5": "Some heuristics are available for choosing the initial scale of the weights.\n\nOne heuristic is to initialize the weights of a fully connected layer with m inputs and 1 1  n outputs by sampling each weight from U (Tm Ta ), while Glorot and Bengio   suggest using the normalized initialization  6 6  m+n m+n  Wij ~U (8.23)  This latter heuristic is designed (: apmpromitL ben the goal of initializing  https://www.deeplearningbook.org/contents/optimization.html    all layers LO Lave we sale activation varlauce and wwe goal OL WUaZig al layers to have the same gradient variance. The formula is derived using the assumption that the network consists only of a chain of matrix multiplications, with no nonlinearities. Real neural networks obviously violate this assumption,  but many strategies designed for the linear model perform reasonably well on its nonlinear counterparts. Saxe et al. recommend initializing to random orthogonal matrices, with a carefully chosen scaling or gain factor g that accounts for the nonlinearity applied at each layer", "08ca1083-23f8-445d-bec0-170e72cda531": "It is also interesting to note that the evidence is precisely the normalizing term that appears in the denominator in Bayes\u2019 theorem when evaluating the posterior distribution over parameters because We can obtain some insight into the model evidence by making a simple approximation to the integral over parameters. Consider \ufb01rst the case of a model having a single parameter w. The posterior distribution over parameters is proportional to p(D|w)p(w), where we omit the dependence on the model Mi to keep the notation uncluttered.\n\nIf we assume that the posterior distribution is sharply peaked around the most probable value wMAP, with width \u2206wposterior, then we can approximate the integral by the value of the integrand at its maximum times the width of the peak. If we further assume that the prior is \ufb02at with width \u2206wprior so that p(w) = 1/\u2206wprior, then we have This approximation is illustrated in Figure 3.12. The \ufb01rst term represents the \ufb01t to the data given by the most probable parameter values, and for a \ufb02at prior this would correspond to the log likelihood. The second term penalizes the model according to its complexity", "428abb93-585c-487e-872e-6d5f15ea7591": "At this time not even one episode had been completed, but the car has oscillated back and forth in the valley, following circular trajectories in state space. All the states visited frequently are valued worse than unexplored states, because the actual rewards have been worse than what was (unrealistically) expected. This continually drives the agent away from wherever it has been, to explore new states, until a solution is found. 1In particular, we used the tile-coding software, available at http://incompleteideas.net/tiles/ tiles3.html, with iht=IHT and tiles(iht,8,,A) to get the indices of the ones in the feature vector for state (x, xdot) and action A.\n\n2This data is actually from the \u201csemi-gradient Sarsa(\u03bb)\u201d algorithm that we will not meet until Chapter 12, but semi-gradient Sarsa would behave similarly. We can obtain an n-step version of episodic semi-gradient Sarsa by using an n-step return as the update target in the semi-gradient Sarsa update equation (10.1)", "f10ac914-9e07-4ada-992d-b84a4cab67c9": "Py COMLPOLULLS, LOW LUC OL LUE PECLLEL allve CILLeLrlOll Is 1uCLuded in the total criterion, one can find a better trade-off than with a purely generative or a purely discriminative training criterion . Salakhutdinov and Hinton  describe a method for learning the kernel function of a kernel machine used for regression, in which the usage of unlabeled examples for modeling P(x) improves P(y | x) quite significantly. See Chapelle et al. for more information about semi-supervised learning. 240  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  7.7 Multitask Learning  Multitask learning  is a way to improve generalization by pooling the examples (which can be seen as soft constraints imposed on the parameters) arising out of several tasks.\n\nIn the same way that additional training examples put more pressure on the parameters of the model toward values that generalize well, when part of a model is shared across tasks, that part of the model is more constrained toward good values (assuming the sharing is justified), often yielding better generalization", "e7c02691-891d-4e0f-9337-e9afcc357ea6": "This conditional probability can be computed with the formula  Ply =y,x=2)  https://www.deeplearningbook.org/contents/prob.html    Ply =y|x=2)= P(x=2) . (3.9)  The conditional probability is only defined when P(x ) > 0. We cannot compute the conditional probability conditioned on an event that never happens. It is important not to confuse conditional probability with computing what would happen if some action were undertaken. The conditional probability that a person is from Germany given that they speak German is quite high, but if a randomly selected person is taught to speak German, their country of origin does not change. Computing the consequences of an action is called making an intervention query.\n\nIntervention queries are the domain of causal modeling, which we do not explore in this book. 3.6 The Chain Rule of Conditional Probabilities  Any joint probability distribution over many random variables may be decomposed into conditional distributions over only one variable:  P(x, 2.x) = Pex), P(xO | x, .0.,x-Y), (3.6)  This observation is known as the chain rule, or product rule, of probability. It follows immediately from the definition of conditional probability in equation 3.5", "d228d56b-2d7e-4a4c-a4cc-b4a9fa362657": "Since the 1980s and until about 2009-2012, state-of-the-art speech recognition systems primarily combined hidden Markov models (HMMs) and Gaussian mixture  https://www.deeplearningbook.org/contents/applications.html    models (GMMs). GMMs modeled the association between acoustic features and  honemes , while HMMs modeled the sequence of phonemes. he GMM-HMM model family treats acoustic waveforms as being generated by the following process: first an HMM generates a sequence of phonemes and discrete subphonemic states (such as the beginning, middle, and end of each phoneme), then a GMM transforms each discrete symbol into a brief segment of audio waveform. Although GMM-HMM systems dominated ASR until recently, speech recognition was actually one of the first areas where neural networks were applied, and numerous ASR systems from the late 1980s and early 1990s used  453  CHAPTER 12. APPLICATIONS  neural nets", "8fe7f2d4-4d54-4165-8c7a-752f644fa10c": "We can provide an important interpretation of Bayes\u2019 theorem as follows.\n\nIf we had been asked which box had been chosen before being told the identity of the selected item of fruit, then the most complete information we have available is provided by the probability p(B). We call this the prior probability because it is the probability available before we observe the identity of the fruit. Once we are told that the fruit is an orange, we can then use Bayes\u2019 theorem to compute the probability p(B|F), which we shall call the posterior probability because it is the probability obtained after we have observed F. Note that in this example, the prior probability of selecting the red box was 4/10, so that we were more likely to select the blue box than the red one. However, once we have observed that the piece of selected fruit is an orange, we \ufb01nd that the posterior probability of the red box is now 2/3, so that it is now more likely that the box we selected was in fact the red one. This result accords with our intuition, as the proportion of oranges is much higher in the red box than it is in the blue box, and so the observation that the fruit was an orange provides signi\ufb01cant evidence favouring the red box", "6e87e58d-b04e-4755-8687-6cddeda37574": "With this prior alone, the only information that an example tells us is the color of its square, and the only way to get the colors of the entire checkerboard right is to cover each of its cells with at  155  CHAPTER 5. MACHINE LEARNING BASICS  least one example. The smoothness assumption and the associated nonparametric learning algo- rithms work extremely well as long as there are enough examples for the learning algorithm to observe high points on most peaks and low points on most valleys  https://www.deeplearningbook.org/contents/ml.html    otf the true underlying function to be learned. \u2018l\u2019his is generally true when the function to be learned is smooth enough and varies in few enough dimensions.\n\nIn high dimensions, even a very smooth function can change smoothly but in a different way along each dimension. If the function additionally behaves differently in various regions, it can become extremely complicated to describe with a set of training examples", "b6a2d2ac-6f41-4b34-9610-a4ead3d801c7": "arXiv. https://arxiv.org/abs/1904.02063 Langley, P. Toward a uni\ufb01ed science of machine learning. Machine Learning, 3(4), 253\u2013259. Lecun, Y., & Misra, I. Self-supervised learning: The dark matter of intelligence. Blog. https: //ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence. LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., & Jackel, L. D. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4), 541\u2013551. Lester, B., Al-Rfou, R., & Constant, N. The power of scale for parameter-e\ufb03cient prompt tuning. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 3045\u20133059. Levine, S", "723a6cd3-2a07-460b-8f4b-a3883dbb3a4d": "This is because the augmentation LM is \ufb01rst \ufb01t to the imbalanced data, which makes label preservation inaccurate and introduces lots of noise during augmentation.\n\nThough a more carefully designed augmentation mechanism can potentially help with imbalanced classi\ufb01cation (e.g., augmenting only the rare classes), the above observation further shows that the varying data manipulation schemes have different applicable scopes. Our approach is thus favorable as the single algorithm can be instantiated to learn different schemes. Conclusions. We have developed a new method of learning different data manipulation schemes with the same single algorithm. Different manipulation schemes reduce to just different parameterization of the data reward function. The manipulation parameters are trained jointly with the target model parameters. We instantiate the algorithm for data augmentation and weighting, and show improved performance over strong base models and previous manipulation methods. We are excited to explore more types of manipulations such as data synthesis, and in particular study the combination of different manipulation schemes. The proposed method builds upon the connections between supervised learning and reinforcement learning (RL)  through which we extrapolate an off-the-shelf reward learning algorithm in the RL literature to the supervised setting", "b42a0f98-e447-4077-8d70-f3d121c24ff3": "Data augmentation by pairing samples for images classification. ArXiv e-prints. 2018. Cecilia S, Michael JD. Improved mixed-example data augmentation. ArXiv preprint. 2018. Daojun L, Feng Y, Tian Z, Peter Y.\n\nUnderstanding mixup training methods. In: IEEE access. 2018. p. 1. Ryo T, Takashi M. Data augmentation using random image cropping and patches for deep CNNs. arXiv preprints. 2018. Yoshua B, Jerome L, Ronan C, Jason W. Curriculum learning. In: Proceedings of the 26th annual international conference on machine learning, ACM. 2009, p. 41-8. Zhun Z, Liang Z, Guoliang K, Shaozi L, Yi Y. Random erasing data augmentation. ArXiv e-prints. 2017. Terrance V, Graham WT", "da550b69-e635-493f-8e90-0c08b7c8a8df": "Do not have enough data? deep learning to the rescue! In The Thirty-Fourth AAAI Conference on Arti\ufb01cial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Arti\ufb01cial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Arti\ufb01cial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 7383\u20137390. AAAI Press. Jacob Andreas. 2020. Good-enough compositional data augmentation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7556\u20137566, Online. Association for Computational Linguistics. Isabelle Augenstein, Sebastian Ruder, and Anders S\u00f8gaard. 2018. Multi-task learning of pairwise sequence classi\ufb01cation tasks over disparate label spaces. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). vances in Neural Information Processing Systems, volume 27. Curran Associates, Inc", "66e77a5e-9eea-4469-a591-0e1b3ab26114": "688  https://www.deeplearningbook.org/contents/generative_models.html    CHAPTER 20. DEEP GENERATIVE MODELS  In this section we review some of the standard directed graphical models that have traditionally been associated with the deep learning community. We have already described deep belief networks, which are a partially directed model.\n\nWe have also already described sparse coding models, which can be thought of as shallow directed generative models. They are often used as feature learners in the context of deep learning, though they tend to perform poorly at sample generation and density estimation. We now describe a variety of deep, fully directed models. 20.10.1 Sigmoid Belief Networks  Sigmoid belief networks  are a simple form of directed graphical model with a specific kind of conditional probability distribution", "dbd3db84-42b8-4822-afa9-35915d22df5e": "We have seen that variational message passing and expectation propagation optimize two different forms of the Kullback-Leibler divergence. Minka  has shown that a broad range of message passing algorithms can be derived from a common framework involving minimization of members of the alpha family of divergences, given by (10.19). These include variational message passing, loopy belief propagation, and expectation propagation, as well as a range of other algorithms, which we do not have space to discuss here, such as tree-reweighted message passing , fractional belief propagation , and power EP .\n\n10.2 (\u22c6) Use the properties E = m1 and E = m2 to solve the simultaneous equations (10.13) and (10.15), and hence show that, provided the original distribution p(z) is nonsingular, the unique solution for the means of the factors in the approximation distribution is given by E = \u00b51 and E = \u00b52. 10.3 (\u22c6 \u22c6) www Consider a factorized variational distribution q(Z) of the form (10.5)", "690e9c8a-08b5-4fdf-a490-8a88b49f6e0d": "Exhibit several trajectories following the optimal policy (but turn the noise o\u21b5 for these trajectories).\n\n\u21e4 The o\u21b5-policy methods that we have considered so far are based on forming importancesampling weights for returns considered as unitary wholes, without taking into account the returns\u2019 internal structures as sums of discounted rewards. We now brie\ufb02y consider cutting-edge research ideas for using this structure to signi\ufb01cantly reduce the variance of o\u21b5-policy estimators. For example, consider the case where episodes are long and \u03b3 is signi\ufb01cantly less than 1. For concreteness, say that episodes last 100 steps and that \u03b3 = 0. The return from time 0 will then be just G0 = R1, but its importance sampling ratio will be a product of b(A99|S99) . In ordinary importance sampling, the return will be scaled by the entire product, but it is really only necessary to scale by the \ufb01rst after the \ufb01rst reward the return has already been determined. These later factors are all independent of the return and of expected value 1; they do not change the expected update, but they add enormously to its variance. In some cases they could even make the variance in\ufb01nite", "90562f08-1079-48d6-a229-ba8d91f3d005": "Encode the support sample into a task-specific input representation using both slow and fast weights: ri = fo9+(x\u2019)  Then store r\\ into 7-th location of the \u201ckey\u201d memory R.  Finally it is the time to construct the training loss using the test set U = {x;, y; }4_,. Starts with Liain = 0: forj=1,...,L:  a. Encode the test sample into a task-specific input representation: r; = fo,o+ (x;)  b. The fast weights are computed by attending to representations of support set samples in memory R. The attention function is of your choice. Here MetaNet uses cosine similarity:  reel; Tythy a; = cosine(R,r;) = | Lee ] \u2019 eT Mes Weel lal  \u00a2! = softmax(a;)'M c", "d9940a54-6d79-4a60-9afd-feef480561a0": "In these situations, we want to set the bias for h so that h \u00bb 1 most of the time at initialization. Otherwise u does not have a chance to learn. For example, Jozefowicz et al. advocate setting the bias to 1 for the forget gate of the LSTM model, described in section 10.10. Another common type of parameter is a variance or precision parameter. For example, we can perform linear regression with a conditional variance estimate using the model  p(y | @) =N(y | w*a + b,1/8), (8.24)  where \u00a3 is a precision parameter.\n\nWe can usually initialize variance or precision parameters to 1 safely. Another approach is to assume the initial weights are close  https://www.deeplearningbook.org/contents/optimization.html       enough to zero that the biases may be set while ignoring the effect of the weights, then set the biases to produce the correct marginal mean of the output, ani set the variance parameters to the marginal variance of the output in the training set. Besides these simple constant or random methods of initializing model parame- ters, it is possible to initialize model parameters using machine learning", "a417c0fd-dd29-4b57-9211-ff348593b33f": "It expresses a relationship between the value of a state and the values of its successor states. Think of looking ahead from a state to its possible successor states, as suggested by the diagram to the right.\n\nEach open circle represents a state and each solid circle represents a state\u2013action pair. Starting from state s, the root node at the top, the agent could take any of some set of actions\u2014 three are shown in the diagram\u2014based on its policy \u21e1. From each of these, the environment could respond with one of several next states, s0 (two are shown in the \ufb01gure), along with a reward, r, depending on its dynamics given by the function p. The Bellman equation (3.14) averages over all the possibilities, weighting each by its probability of occurring. It states that the value of the start state must equal the (discounted) value of the expected next state, plus the reward expected along the way. The value function v\u21e1 is the unique solution to its Bellman equation. We show in subsequent chapters how this Bellman equation forms the basis of a number of ways to compute, approximate, and learn v\u21e1", "9c59b79e-cef4-4cba-95b7-6eee762ffc1a": "We have already seen one example of a mixture distribution: the empirical  https://www.deeplearningbook.org/contents/prob.html    distribution over real-valued variables is a mixture distribution with one Dirac component for each training example. 64  CHAPTER 3.\n\nPROBABILITY AND INFORMATION THEORY  The mixture model is one simple strategy for combining probability distributions o create a richer distribution. In chapter 16, we explore the art of building complex probability distributions from simple ones in more detail. The mixture model allows us to briefly glimpse a concept that will be of paramount importance later\u2014the latent variable. A latent variable is a random variable that we cannot observe directly. The component identity variable c of the mixture model provides an example. Latent variables may be related to x through he joint distribution, in this case, P(x,c) = P(x |c)P(c). The distribution P(c) over the latent variable and the distribution P(x | c) relating the latent variables o the visible variables determines the shape of the distribution P(x), even though it is possible to describe P(x) without reference to the latent variable. Latent variables are discussed further in section 16.5", "e9378553-f233-4ef6-9d3a-23952da8b29f": "The next most obvious advantage of TD methods over Monte Carlo methods is that they are naturally implemented in an online, fully incremental fashion. With Monte Carlo methods one must wait until the end of an episode, because only then is the return known, whereas with TD methods one need wait only one time step. Surprisingly often this turns out to be a critical consideration. Some applications have very long episodes, so that delaying all learning until the end of the episode is too slow. Other applications are continuing tasks and have no episodes at all. Finally, as we noted in the previous chapter, some Monte Carlo methods must ignore or discount episodes on which experimental actions are taken, which can greatly slow learning.\n\nTD methods are much less susceptible to these problems because they learn from each transition regardless of what subsequent actions are taken. But are TD methods sound? Certainly it is convenient to learn one guess from the next, without waiting for an actual outcome, but can we still guarantee convergence to the correct answer? Happily, the answer is yes", "5309f2a6-dc05-4341-9d8a-54003b9736a3": "Given one positive pair, other 2(N \u2014 1) data points are treated as negative samples. The representation is produced by a base encoder f(. ):  h; = f(x:), bh; = f(x;)  The contrastive learning loss is defined using cosine similarity sim(. ye ). Note that the loss operates on an extra projection layer of the representation ce ) rather than on the representation space directly. But only the representation h is used for downstream tasks. 2; = g(h;), Zi = g(h;) 6) tog SPC (Hi) /7) SimCLR \u2014 : an 1.44 exp(sim(z;, zx)/T)  where 114.24] is an indicator function: 1 if k 4 7 0 otherwise.\n\nSimCLR needs a large batch size to incorporate enough negative samples to achieve good  performance. https://lilianweng.github.io/posts/2021-05-31-contrastive/  Contrastive Representation Learning | Lil'Log  Algorithm 1 SimCLR\u2019s main learning algorithm", "e8f5bdf7-6557-4ffe-b807-d9372c0260a0": "Generate a new episode with the new policy 7 (i.e.\n\nusing algorithms like e-greedy helps us balance between exploitation and exploration.) ral (i vet oy Reskst)  Estimate Q using the new episode: q,,(s,a) = ST i[S;os. Ay nal t= ht hi  Temporal-Difference Learning  Similar to Monte-Carlo methods, Temporal-Difference (TD) Learning is model-free and learns from episodes of experience. However, TD learning can learn from incomplete episodes and hence we  don't need to track the episode up to termination. TD learning is so important that Sutton & Barto   in their RL book describes it as \u201cone idea ... central and novel to reinforcement learning\". Bootstrapping  https://lilianweng.github.io/posts/2018-02-19-rl-overview/  A (Long) Peek into Reinforcement Learning | Lil'Log   TD learning methods update targets with regard to existing estimates rather than exclusively relying on actual rewards and complete returns as in MC methods. This approach is known as bootstrapping", "8fb456c6-0ba2-4245-bb11-f327641c6656": "Leaky units did this with connection weights that were either manually chosen constants or were parameters. Gated RNNs generalize this to connection weights that may change at each time step. Leaky units allow the network to accumulate information (such as evidence for a particular feature or category) over a long duration. Once that information has been used, however, it might be useful for the neural network to forget the old state. For example, if a sequence is made of subsequences and we want a leaky unit to accumulate evidence inside each sub-subsequence, we need a mechanism to forget the old state by setting it to zero. Instead of manually deciding when to clear the state, we want the neural network to learn to decide when to do it. This is what gated RNNs do. 10.10.1 LSTM  The clever idea of introducing self-loops to produce paths where the gradient can flow for long durations is a core contribution of the initial long short-term memory (LSTM) model . A crucial addition has been to make the weight on this self-loop conditioned on the context, rather than fixed", "605c8361-96b3-4db4-b47e-e4be322543ab": "In practice a mode will typically be found by running some form of numerical optimization algorithm (Bishop and Nabney, 2008).\n\nMany of the distributions encountered in practice will be multimodal and so there will be different Laplace approximations according to which mode is being considered. Note that the normalization constant Z of the true distribution does not need to be known in order to apply the Laplace method. As a result of the central limit theorem, the posterior distribution for a model is expected to become increasingly better approximated by a Gaussian as the number of observed data points is increased, and so we would expect the Laplace approximation to be most useful in situations where the number of data points is relatively large. One major weakness of the Laplace approximation is that, since it is based on a Gaussian distribution, it is only directly applicable to real variables. In other cases it may be possible to apply the Laplace approximation to a transformation of the variable. For instance if 0 \u2a7d \u03c4 < \u221e then we can consider a Laplace approximation of ln \u03c4", "906c7f49-716c-4d3f-8b7d-0d7c5eabb37c": "To get the detailed equations, with the right indices on the general bootstrapping and discounting parameters, it is best to start with a recursive form (12.20) for the \u03bb-return using action values, and then expand the bootstrapping case of the target after the model of (7.16): As per the usual pattern, it can also be written approximately (ignoring changes in the approximate value function) as a sum of TD errors, using the expectation form of the action-based TD error (12.28).\n\nFollowing the same steps as in the previous section, we arrive at a special eligibility trace update involving the target-policy probabilities of the selected actions, This, together with the usual parameter-update rule (12.7), de\ufb01nes the TB(\u03bb) algorithm. Like all semi-gradient algorithms, TB(\u03bb) is not guaranteed to be stable when used with o\u21b5-policy data and with a powerful function approximator. To obtain those assurances, TB(\u03bb) would have to be combined with one of the methods presented in the next section", "1934302c-db52-49bf-a868-6bf72c3e0ade": "Smart augmentation learning an optimal data augmentation strategy. IEEE Access, 5:5858\u20135869, 2017. S. Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv preprint arXiv:1805.00909, 2018. X. Li and D. Roth. Learning question classi\ufb01ers. In Proceedings of the 19th international conference on Computational linguistics, pages 1\u20137, 2002. A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies-volume 1, pages 142\u2013150. Association for Computational Linguistics, 2011. T. Malisiewicz, A. Gupta, A. A", "9684de24-c67e-47b1-bf32-6390ebf798ea": "In the case of probability distributions de\ufb01ned by an undirected graph, there is no one-pass sampling strategy that will sample even from the prior distribution with no observed variables. Instead, computationally more expensive techniques must be employed, such as Gibbs sampling, which is discussed in Section 11.3. As well as sampling from conditional distributions, we may also require samples from a marginal distribution. If we already have a strategy for sampling from a joint distribution p(u, v), then it is straightforward to obtain samples from the marginal distribution p(u) simply by ignoring the values for v in each sample. There are numerous texts dealing with Monte Carlo methods. Those of particular interest from the statistical inference perspective include Chen et al. , Gamerman , Gilks et al. , Liu , Neal , and Robert and Casella .\n\nAlso there are review articles by Besag et al. , Brooks , Diaconis and Saloff-Coste , Jerrum and Sinclair , Neal , Tierney , and Andrieu et al. that provide additional information on sampling methods for statistical inference", "d6765384-8923-4f42-87f9-3f05bef13f89": "We can use regularization to encourage models to be invariant to transformations of the input through the technique of tangent propagation .\n\nConsider the effect of a transformation on a particular input vector xn. Provided the transformation is continuous (such as translation or rotation, but not mirror re\ufb02ection for instance), then the transformed pattern will sweep out a manifold M within the D-dimensional input space. This is illustrated in Figure 5.15, for the case of D = 2 for simplicity. Suppose the transformation is governed by a single parameter \u03be (which might be rotation angle for instance). Then the subspace M swept out by xn dimensional transformation, parameterized by the continuous variable \u03be, applied to xn causes it to sweep out a one-dimensional manifold M. Locally, the effect of the transformation can be approximated by the tangent vector \u03c4 n. will be one-dimensional, and will be parameterized by \u03be. Let the vector that results from acting on xn by this transformation be denoted by s(xn, \u03be), which is de\ufb01ned so that s(x, 0) = x", "73e8123e-20a6-4b26-ac80-9626590e3abd": "This problem involves evaluative feedback, as in bandits, but also an associative aspect\u2014choosing di\u21b5erent actions in di\u21b5erent situations. MDPs are a classical formalization of sequential decision making, where actions in\ufb02uence not just immediate rewards, but also subsequent situations, or states, and through those future rewards. Thus MDPs involve delayed reward and the need to tradeo\u21b5 immediate and delayed reward. Whereas in bandit problems we estimated the value q\u21e4(a) of each action a, in MDPs we estimate the value q\u21e4(s, a) of each action a in each state s, or we estimate the value v\u21e4(s) of each state given optimal action selections. These state-dependent quantities are essential to accurately assigning credit for long-term consequences to individual action selections. MDPs are a mathematically idealized form of the reinforcement learning problem for which precise theoretical statements can be made. We introduce key elements of the problem\u2019s mathematical structure, such as returns, value functions, and Bellman equations. We try to convey the wide range of applications that can be formulated as \ufb01nite MDPs", "d1a0836a-c62d-4ed4-8e6d-25fd4bb586c6": "For example, the incremental update rule (2.3) for updating an average Qn of the n \u2212 1 past rewards is modi\ufb01ed to be where the step-size parameter \u21b5 2 (0, 1] is constant. This results in Qn+1 being a weighted average of past rewards and the initial estimate Q1: We call this a weighted average because the sum of the weights is (1 \u2212 \u21b5)n + Pn \u21b5)n\u2212i = 1, as you can check for yourself. Note that the weight, \u21b5(1 \u2212 \u21b5)n\u2212i, given to the reward Ri depends on how many rewards ago, n \u2212 i, it was observed. The quantity 1 \u2212 \u21b5 is less than 1, and thus the weight given to Ri decreases as the number of intervening rewards increases. In fact, the weight decays exponentially according to the exponent on 1 \u2212 \u21b5.\n\n(If 1 \u2212 \u21b5 = 0, then all the weight goes on the very last reward, Rn, because of the convention that 00 = 1.) Accordingly, this is sometimes called an exponential recency-weighted average. Sometimes it is convenient to vary the step-size parameter from step to step", "ed10d747-e6c6-4fca-a27e-6f2e0c574e98": "Morgan and Claypool Publishers  16. Eadicicco, L.:  Baidu\u2019s Andrew Ng on the future of arti\ufb01cial intelligence. Time  17. Fries, J.A., Varma, P., Chen, V.S., Xiao, K., Tejeda, H., Saha, P., Dunnmon, J., Chubb, H., Maskatia, S., Fiterau, M., Delp, S., Ashley, E., R\u00e9, C., Priest, J.: Weakly supervised classi\ufb01cation of rare aortic valve malformations using unlabeled cardiac mri sequences. bioRxiv  18. Graves, A., Schmidhuber, J.: Framewise phoneme classi\ufb01cation with bidirectional LSTM and other neural network architectures. Neural Netw. 18(5), 602\u2013610  19. Gupta, S., Manning, C.D. : Improved pattern learning for bootstrapped entity extraction. In: CoNLL  20", "e91fd1b1-b226-4ac7-9ba3-7fbc8caedba0": "Using the de\ufb01nition of \u03be(zn\u22121, zn), and applying Bayes\u2019 theorem, we have where we have made use of the conditional independence property (13.29) together with the de\ufb01nitions of \u03b1(zn) and \u03b2(zn) given by (13.34) and (13.35). Thus we can calculate the \u03be(zn\u22121, zn) directly by using the results of the \u03b1 and \u03b2 recursions. Let us summarize the steps required to train a hidden Markov model using the EM algorithm. We \ufb01rst make an initial selection of the parameters \u03b8old where \u03b8 \u2261 (\u03c0, A, \u03c6). The A and \u03c0 parameters are often initialized either uniformly or randomly from a uniform distribution (respecting their non-negativity and summation constraints). Initialization of the parameters \u03c6 will depend on the form of the distribution.\n\nFor instance in the case of Gaussians, the parameters \u00b5k might be initialized by applying the K-means algorithm to the data, and \u03a3k might be initialized to the covariance matrix of the corresponding K means cluster", "fe6a3587-a0ac-4403-85c0-5a8b9f521567": "The vector  g(f (&)) \u2014 & points approximately toward the nearest point on the manifold, since g(f(Z))  estimates the center of mass of the clean points a that could have given rise to &. The  autoencoder thus learns a vector field g( f (a)) \u2014 x indicated by the green arrows.\n\nThis  vector field estimates the score Vz log paata(x) up to a multiplicative factor that is the  average root mean square reconstruction error. https://www.deeplearningbook.org/contents/autoencoders.html    CHAPTER 14 AUTOENCODERS  by training with the squared error criterion  Ilg(f(#)) \u2014 ||? (14.16)  and corruption  C(x = #|2) = N (4; = &, D = 071) (14.17) with noise variance o?. See figure 14.5 for an illustration of how this works. In general, there is no guarantee that the reconstruction g(f(x)) minus the input x corresponds to the gradient of any function, let alone to the score", "ff5b3e60-3012-418b-8f61-86129094dd5d": "Our presentations are intended to illustrate some of the trade-o\u21b5s and issues that arise in real applications. For example, we emphasize how domain knowledge is incorporated into the formulation and solution of the problem. We also highlight the representation issues that are so often critical to successful applications. The algorithms used in some of these case studies are substantially more complex than those we have presented in the rest of the book. Applications of reinforcement learning are still far from routine and typically require as much art as science. Making applications easier and more straightforward is one of the goals of current research in reinforcement learning. One of the most impressive applications of reinforcement learning to date is that by Gerald Tesauro to the game of backgammon . Tesauro\u2019s program, TD-Gammon, required little backgammon knowledge, yet learned to play extremely well, near the level of the world\u2019s strongest grandmasters.\n\nThe learning algorithm in TD-Gammon was a straightforward combination of the TD(\u03bb) algorithm and nonlinear function approximation using a multilayer arti\ufb01cial neural network (ANN) trained by backpropagating TD errors. Backgammon is a major game in the sense that it is played throughout the world, with numerous tournaments and regular world championship matches", "1846e806-d552-4999-800a-04417618eecd": "We brie\ufb02y discussed in Section 9 how new algorithms can mechanically be created by composing experience and/or other algorithmic components together.\n\nIt would be signi\ufb01cant to have an automated engine that further streamlines the process. For example, once a new advance is made to reinforcement learning, the engine would automatically amplify the progress and deliver enhanced functionalities of learning data manipulation and adapting knowledge constraints. Similarly, domain experts can simply input a variety of experiences available into their own problems, and expect an algorithm to be automatically composed to learn the target model they want. The sophisticated algorithm manipulation and creation would greatly simplify machine learning work\ufb02ow in practice and boost the accessibility of ML to much broader users. From Maxwell\u2019s equations to General Relativity, and to quantum mechanics and Standard Model, \u201cPhysics is the study of symmetry,\u201d remarked physicist Phil Anderson (Anderson, 1972, p.394). The end goal of physics research seems to be clear\u2014a \u2018theory of everything\u2019 that fully explains and links together all physical aspects. The \u2018end goal\u2019 of ML/AI is surely much more elusive", "81ebb8db-08e4-4da0-b5d0-f8f3569fe85e": "In the basic Metropolis algorithm , we assume that the proposal distribution is symmetric, that is q(zA|zB) = q(zB|zA) for all values of zA and zB.\n\nThe candidate sample is then accepted with probability This can be achieved by choosing a random number u with uniform distribution over the unit interval (0, 1) and then accepting the sample if A(z\u22c6, z(\u03c4)) > u. Note that if the step from z(\u03c4) to z\u22c6 causes an increase in the value of p(z), then the candidate point is certain to be kept. If the candidate sample is accepted, then z(\u03c4+1) = z\u22c6, otherwise the candidate point z\u22c6 is discarded, z(\u03c4+1) is set to z(\u03c4) and another candidate sample is drawn from the distribution q(z|z(\u03c4+1)). This is in contrast to rejection sampling, where rejected samples are simply discarded. In the Metropolis algorithm when a candidate point is rejected, the previous sample is included instead in the \ufb01nal list of samples, leading to multiple copies of samples", "78a568ef-9fdc-4b2b-9d3d-6a50ffb85bb0": "All Boltzmann machines have an intractable partition function, so the maximum likelihood gradient must be ap- proximated using the techniques described in chapter 18.  https://www.deeplearningbook.org/contents/generative_models.html    One interesting property of Boltzmann machines when trained with learning rules based on maximum likelihood is that the update for a particular weight connecting two units depends only on the statistics of those two units, collected  under different distributions: Pmoaei(v) and Piata(v) Pmoae(h |v). The rest of the  652  CHAPTER 20. DEEP GENERATIVE MODELS  network participates in shaping those statistics, but the weight can be updated without knowing anything about the rest of the network or how those statistics were produced.\n\nThis means that the learning rule is \u201clocal,\u201d which makes Boltzmann machine learning somewhat biologically plausible. It is conceivable that if each neuron were a random variable in a Boltzmann machine, then the axons and dendrites connecting two random variables could learn only by observing the firing pattern of the cells that they actually physically touch. In particular, in the positive phase, two units that frequently activate together have their connection strengthened", "42ae08e8-ca71-47c1-930d-a3a8dcc01f44": "As long as at least one feasible point exists and f(a) is not permitted to have value oo, then  minmax max L(x2,A,a) (4.15) Ba A a,a>0  has the same optimal objective function value and set of optimal points a as  min f(a). (4.16)  zeS This follows because any time the constraints are satisfied,  max max L(x,A,a) = f(x), (4.17)  A a,a>0  while any time a constraint is violated,  https://www.deeplearningbook.org/contents/numerical.html    MX MAX L(x, A, a) = oo.\n\n(4.18)  These properties guarantee that no infeasible point can be optimal, and that the optimum within the feasible points is unchanged. 1The KKT approach generalizes the method of Lagrange multipliers which allows equality constraints but not inequality constraints. 92  CHAPTER 4. NUMERICAL COMPUTATION  To perform constrained maximization, we can construct the generalized La- grange function of \u2014 f(a), which leads to this optimization problem:  min max max \u2014 y+ Dig (a) + \u00bb ajh (a)", "79e0c850-f3a0-43f0-b272-1c40557bc84f": "Any component of nh) that is not aligned with the largest eigenvector will eventually be discarded. This problem is particular to recurrent networks. In the scalar case, imagine multiplying a weight w by itself many times.\n\nThe product w! will either vanish  https://www.deeplearningbook.org/contents/rnn.html    or explode depending on the agyagnitude of w. If we make a nonrecurrent network that has a di! erent weight w*\u201d at each time step, the situation is di qyent. If the initial state is given by 1, then the state at time tis given by I\u2019. Suppose that the w values are generated randomly, independently from one another, with zero mean and variance y. The variance of the product is O(v\"). To obtain some desired variance v* we may choose the individual weights with variance v = V/v*. Very deep feedforward networks with carefully chosen scaling can thus avoid the vanishing and exploding gradient problem, as argued by Sussillo", "d675421b-e7d7-44de-b3b7-be64368851d8": "When a task involves incorporating information from very distant locations in the input, then the prior imposed by convolution may be inappropriate. Another key insight from this view is that we should only compare convolu- tional models to other convolutional models in benchmarks of statistical learning performance.\n\nModels that do not use convolution would be able to learn even if we  https://www.deeplearningbook.org/contents/convnets.html    permuted all the pixels In the image. For many image datasets, there are separate enchmarks for models that are permutation invariant and must discover the concept of topology via learning and for models that have the knowledge of spatial relationships hard coded into them by their designer. CHAPTER 9. CONVOLUTIONAL NETWORKS  9.5 Variants of the Basic Convolution Function  When discussing convolution in the context of neural networks, we usually do not refer exactly to the standard discrete convolution operation as it is usually understood in the mathematical literature. The functions used in practice differ slightly. Here we describe these differences in detail and highlight some useful properties of the functions used in neural networks", "bb368ad0-d2bb-4282-8392-55dd0519afff": "As a result of these complications, much of the e\u21b5ort in developing Watson\u2019s DDwagering strategy was devoted to creating good models of human opponents. The models did not address the natural language aspect of the game, but were instead stochastic process models of events that can occur during play. Statistics were extracted from an extensive fan-created archive of game information from the beginning of the show to the present day. The archive includes information such as the ordering of the clues, right and wrong contestant answers, DD locations, and DD and FJ bets for nearly 300,000 clues.\n\nThree models were constructed: an Average Contestant model (based on all the data), a Champion model (based on statistics from games with the 100 best players), and a Grand Champion model (based on statistics from games with the 10 best players). In addition to serving as opponents during learning, the models were used to assess the bene\ufb01ts produced by the learned DD-wagering strategy. Watson\u2019s win rate in simulation when it used a baseline heuristic DD-wagering strategy was 61%; when it used the learned values and a default con\ufb01dence value, its win rate increased to 64%; and with live in-category con\ufb01dence, it was 67%", "382b5d7b-5a77-41d8-9d09-6f48295f410b": "However, we can take advantage of the nonuniform distribution by using shorter codes for the more probable events, at the expense of longer codes for the less probable events, in the hope of getting a shorter average code length.\n\nThis can be done by representing the states {a, b, c, d, e, f, g, h} using, for instance, the following set of code strings: 0, 10, 110, 1110, 111100, 111101, 111110, 111111. The average length of the code that has to be transmitted is then which again is the same as the entropy of the random variable. Note that shorter code strings cannot be used because it must be possible to disambiguate a concatenation of such strings into its component parts. For instance, 11001110 decodes uniquely into the state sequence c, a, d. This relation between entropy and shortest coding length is a general one. The noiseless coding theorem  states that the entropy is a lower bound on the number of bits needed to transmit the state of a random variable", "ee39ee68-d222-4c87-aed8-d5d50de02295": "Analogous results are obtained for discrete variables each of which can take M > 2 states. Exercise 4.11 As we have seen, for both Gaussian distributed and discrete inputs, the posterior class probabilities are given by generalized linear models with logistic sigmoid (K = 2 classes) or softmax (K \u2a7e 2 classes) activation functions. These are particular cases of a more general result obtained by assuming that the class-conditional densities p(x|Ck) are members of the exponential family of distributions. Using the form (2.194) for members of the exponential family, we see that the distribution of x can be written in the form We now restrict attention to the subclass of such distributions for which u(x) = x.\n\nThen we make use of (2.236) to introduce a scaling parameter s, so that we obtain the restricted set of exponential family class-conditional densities of the form Note that we are allowing each class to have its own parameter vector \u03bbk but we are assuming that the classes share the same scale parameter s", "1e4c990e-2c10-41d8-8a06-fefc4aad4e3a": "The gradient computation involves performing a forward propagation pass moving left to right through our illustration  374  CHAPTER 10.\n\nSEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  :  https://www.deeplearningbook.org/contents/rnn.html     1 \u2018S ), Ww N w a- t aN Ta oY Qt \\ rn) ) v  Figure 10.4: An RNN whose only recurrence is the feedback connection from the output to the hidden layer. At ach time step t, the input is a,, the hidden layer activations are h\\), the outputs are o), the targets are y), and the loss is LL\u201d. (Left) Circuit diagram. (Right) Unfolded 2 es graph. Such an RNN is less powerful (can express a smaller set of functions) than those in the family represented by figure 10.3. The RNN in figure 10.3 can choose to put any information it wants about the past into its hidden representation h and transmit h to the future", "3633d607-7932-4989-ab9d-48bfa5bf9b87": "The dashed circles indicate the contours of the L?\n\npenalty, which causes the minimum of the total cost to lie nearer the origin than the minimum of the unregularized cost. is the actual mechanism by which early stopping regularizes the model? Bishop 1995a) and Sj\u00e9berg and Ljung  argued that early stopping has the effect of restricting the optimization procedure to a relatively small volume of parameter space in the neighborhood of the initial parameter value 95, as illustrated in figure 7.4. More specifically, imagine taking 7 optimization steps (corresponding oT training iterations) and with learning rate \u00ab. We can view the product er as a measure of effective capacity. Assuming the gradient is bounded, restricting both the number of iterations and the learning rate limits the volume of parameter space reachable from 6@,. In this sense, er behaves as if it were the reciprocal of che coefficient used for weight decay. Indeed, we can show how\u2014in the case of a simple linear model with a quadratic error function and simple gradient descent\u2014early stopping is equivalent to L? regularization. To compare with classical L? regularization, we examine a simple setting where the only parameters are linear weights (9 = w)", "30197d14-df29-4fe6-a9ab-ae7723332805": "For generality we will here assume that each of these expectations is intractable.\n\nUnder certain mild conditions outlined in section (see paper) for chosen approximate posteriors q\u03c6(\u03b8) and q\u03c6(z|x) we can reparameterize conditional samples \ufffdz \u223c q\u03c6(z|x) as where we choose a prior p(\u03f5) and a function g\u03c6(\u03f5, x) such that the following holds: The same can be done for the approximate posterior q\u03c6(\u03b8): where we, similarly as above, choose a prior p(\u03b6) and a function h\u03c6(\u03b6) such that the following holds: For notational conciseness we introduce a shorthand notation f\u03c6(x, z, \u03b8): f\u03c6(x, z, \u03b8) = N \u00b7 (log p\u03b8(x|z) + log p\u03b8(z) \u2212 log q\u03c6(z|x)) + log p\u03b1(\u03b8) \u2212 log q\u03c6(\u03b8) (21) Using equations (20) and (18), the Monte Carlo estimate of the variational lower bound, given datapoint x(i), is: where \u03f5(l) \u223c p(\u03f5) and \u03b6(l) \u223c p(\u03b6)", "9519e825-38aa-4ada-9dab-7222f331e75d": "In order to minimize the total error function, it is necessary to be able to evaluate its derivatives with respect to the various adjustable parameters.\n\nTo do this it is convenient to regard the {\u03c0j} as prior probabilities and to introduce the corresponding posterior probabilities which, following (2.192), are given by Bayes\u2019 theorem in the form The effect of the regularization term is therefore to pull each weight towards the centre of the jth Gaussian, with a force proportional to the posterior probability of that Gaussian for the given weight. This is precisely the kind of effect that we are seeking. Derivatives of the error with respect to the centres of the Gaussians are also easily computed to give Exercise 5.30 which has a simple intuitive interpretation, because it pushes \u00b5j towards an average of the weight values, weighted by the posterior probabilities that the respective weight parameters were generated by component j. Similarly, the derivatives with respect to the variances are given by Exercise 5.31 which drives \u03c3j towards the weighted average of the squared deviations of the weights around the corresponding centre \u00b5j, where the weighting coef\ufb01cients are again given by the posterior probability that each weight is generated by component j", "ac5a7622-ad5c-42a1-ae56-7afd8c0f696e": "Finding the optimal sequence of operations to compute the gradient is NP-complete , in the sense that it may require simplifying algebraic expressions into their least expensive form. For example, suppose we have variables p, , po, ... , Pn, representing probabilities, and variables 21, 29,..., 2%, representing unnormalized log probabilities. Suppose we define  __ _exp(2i)  oS Lex) where we build the softmax function out of exponentiation, summation and division operations, and construct a cross-entropy loss J = \u2014)\u00b0,pilogq; A human mathematician can observe that the derivative of J with respect to z takes a very simple form: q \u2014 p;. The back-propagation algorithm is not capable of simplifying the gradient this way and will instead explicitly propagate gradients through all the logarithm and exponentiation operations in the original graph", "d8c35469-94dd-45e8-a216-3d1a4839b077": "Our goal is to determine the predictive distribution p(tN+1|t), where we have left the conditioning on the input variables implicit. To do this we introduce a Gaussian process prior over the vector aN+1, which has components a(x1), . , a(xN+1). This in turn de\ufb01nes a non-Gaussian process over tN+1, and by conditioning on the training data tN we obtain the required predictive distribution. The Gaussian process prior for aN+1 takes the form Unlike the regression case, the covariance matrix no longer includes a noise term because we assume that all of the training data points are correctly labelled. However, for numerical reasons it is convenient to introduce a noise-like term governed by a parameter \u03bd that ensures that the covariance matrix is positive de\ufb01nite.\n\nThus the covariance matrix CN+1 has elements given by where k(xn, xm) is any positive semide\ufb01nite kernel function of the kind considered in Section 6.2, and the value of \u03bd is typically \ufb01xed in advance", "78d0968d-5a05-4c4d-b45a-457c5b265a2f": "Prn\u20141 (a1) met nce mw) IT Dyi(\u00a9nixr) mem \u201d  (18.59)  We now have means of generating samples from the joint proposal distribution q over the extended sample via a sampling scheme given above, with the joint distribution given by  YULn1; seey hn as \u00a31) = Po(&m Tn (fm | Ln;) ee Trp, (\u00ae1 | Ly): (18.60)  We have a joint distribution on the extended space given by equation 18.59. Taking Q(@m ;--+;Lyn\u20141, 1) as the proposal distribution on the extended state space from which we will draw samples, it remains to determine the importance weights:  ~ ~ k ~ k)y\\ ~ k py) = Bl msn 1) __ Pr(ai?) Plein) Polen) (18.61)  Gm r-+++ E121) Py, (ae) .) pawl?) po (ah) \u00a9  These weights are the same as proposed for AIS. Thus we can interpret AIS as simple importance sampling applied to an extended state, and its validity follows immediately from the validity of importance sampling", "9c809156-73d4-4509-8cce-67546775df5e": "These participants learned to write labeling functions to extract relations from news articles as part of a two-day workshop on learning to use Snorkel, and matched or outperformed models trained on hand-labeled training data, showing the ef\ufb01ciency of Snorkel\u2019s process even for \ufb01rst-time users. We now describe our results in detail. First, we describe the six applications that validate our claims. We then show that Snorkel\u2019s generative modeling stage helps to improve the predictive performance of the discriminative model, demonstrating that it is 5.81% more accurate when trained on Snorkel\u2019s probabilistic labels versus labels produced by an unweighted average of labeling functions. We also validate that the ability to incorporate many different types of weak supervision incrementally improves results with an ablation study. Finally, we describe the protocol and results of our user study.\n\nTo evaluate the effectiveness of Snorkel, we consider several real-world deployments and tasks on open-source datasets that are representative of other deployments in information extraction, medical image classi\ufb01cation, and crowdsourced sentiment analysis. Summary statistics of the tasks are provided in Tables 4 and 2", "ea8b016e-6bcb-45d4-b0ee-be6f011d817c": "To do this, we create variable nodes corresponding to the nodes in the original undirected graph, and then create additional factor nodes corresponding to the maximal cliques xs. The factors fs(xs) are then set equal to the clique potentials. Note that there may be several different factor graphs that correspond to the same undirected graph. These concepts are illustrated in Figure 8.41. Similarly, to convert a directed graph to a factor graph, we simply create variable nodes in the factor graph corresponding to the nodes of the directed graph, and then create factor nodes corresponding to the conditional distributions, and then \ufb01nally add the appropriate links. Again, there can be multiple factor graphs all of which correspond to the same directed graph.\n\nThe conversion of a directed graph to a factor graph is illustrated in Figure 8.42. We have already noted the importance of tree-structured graphs for performing ef\ufb01cient inference. If we take a directed or undirected tree and convert it into a factor graph, then the result will again be a tree (in other words, the factor graph will have no loops, and there will be one and only one path connecting any two nodes)", "01e2f2fe-faa7-4a0c-8c09-de1c29bddf60": "Their method was based on a linear encoder, however, and could not learn function families as powerful as the modern DAE can. 511  https://www.deeplearningbook.org/contents/autoencoders.html    CHAPTER 14. AUTOENCODERS  a  A a ee  ne ee 2 2 \u201cTVA NNRPEER PRR RRR RS \u201cTF VNNNNVV DEE RR RR ER RR  PUAN NNNAA ETE ORR RRR PRR  . x = wee -. WSS D002 2525222 ae qu: a a a 74a , Sr rr 744 \u00abaS : >> beh Le may uae , a a TRAN RD DSSS", "082b6a0f-6d47-4409-877b-7372236219a8": "We then use this posterior distribution to \ufb01nd the expectation of the complete-data log likelihood evaluated for some general parameter value \u03b8.\n\nThis expectation, denoted Q(\u03b8, \u03b8old), is given by Note that in the de\ufb01nition of Q(\u03b8, \u03b8old), the logarithm acts directly on the joint distribution p(X, Z|\u03b8), and so the corresponding M-step maximization will, by supposition, be tractable. The general EM algorithm is summarized below. It has the property, as we shall show later, that each cycle of EM will increase the incomplete-data log likelihood (unless it is already at a local maximum). Section 9.4 Given a joint distribution p(X, Z|\u03b8) over observed variables X and latent variables Z, governed by parameters \u03b8, the goal is to maximize the likelihood function p(X|\u03b8) with respect to \u03b8. 1. Choose an initial setting for the parameters \u03b8old. 4. Check for convergence of either the log likelihood or the parameter values", "f34dbbb4-a6db-492d-bcf9-7214a783ff2c": "To resolve this difficulty, Weston et al. introduced memory networks that include a set of memory cells that can be accessed via an addressing mechanism. Memory networks originally required a supervision signal instructing them how to use their memory cells. Graves et al. introduced the neural Turing ma- chine, which is able to learn to read from and write arbitrary content to memory cells without explicit supervision about which actions to undertake, and allowed end-to-end training without this supervision signal, via the use of a content-based soft attention mechanism (see Bahdanau e# al. and section 12.4.5.1). This soft addressing mechanism has become standard with other related architectures, emulating algorithmic mechanisms in a way that still allows gradient-based opti- mization . Each memory cell can be thought of as an extension of the memory cells in LSTMs and GRUs. The difference is that the network outputs an internal state that chooses which cell to read from or write to, just as memory accesses in a  412  CHAPTER 10", "0405d57c-0491-4a5f-9f6f-484fc940c763": "The probability that X will take the value xi and Y will take the value yj is written p(X = xi, Y = yj) and is called the joint probability of X = xi and Y = yj.\n\nIt is given by the number of points falling in the cell i,j as a fraction of the total number of points, and hence Here we are implicitly considering the limit N \u2192 \u221e. Similarly, the probability that X takes the value xi irrespective of the value of Y is written as p(X = xi) and is given by the fraction of the total number of points that fall in column i, so that Because the number of instances in column i in Figure 1.10 is just the sum of the number of instances in each cell of that column, we have ci = \ufffd which is the sum rule of probability. Note that p(X = xi) is sometimes called the marginal probability, because it is obtained by marginalizing, or summing out, the other variables (in this case Y )", "41ff4363-c1ef-4cf1-904d-ce5b603234b5": "The framework is abstract and \ufb02exible and can be applied in many di\u21b5erent ways. Stretch its limits in some way in at least one of your examples. \u21e4 Exercise 3.3 Consider the problem of driving. You could de\ufb01ne the actions in terms of the accelerator, steering wheel, and brake, that is, where your body meets the machine. Or you could de\ufb01ne them farther out\u2014say, where the rubber meets the road, considering your actions to be tire torques. Or you could de\ufb01ne them farther in\u2014say, where your brain meets your body, the actions being muscle twitches to control your limbs. Or you could go to a really high level and say that your actions are your choices of where to drive.\n\nWhat is the right level, the right place to draw the line between agent and environment? On what basis is one location of the line to be preferred over another? Is there any fundamental reason for preferring one location over another, or is it a free choice? \u21e4 A mobile robot has the job of collecting empty soda cans in an o\ufb03ce environment. It has sensors for detecting cans, and an arm and gripper that can pick them up and place them in an onboard bin; it runs on a rechargeable battery", "d41d40c8-d036-46d7-bcea-42853bc011b3": "Exercise 10.32 Note that the bound given by (10.149) applies only to the two-class problem and so this approach does not directly generalize to classi\ufb01cation problems with K > 2 classes. An alternative bound for the multiclass case has been explored by Gibbs . We now have a normalized Gaussian approximation to the posterior distribution, which we shall use shortly to evaluate the predictive distribution for new data points.\n\nFirst, however, we need to determine the variational parameters {\u03ben} by maximizing the lower bound on the marginal likelihood. To do this, we substitute the inequality (10.152) back into the marginal likelihood to give As with the optimization of the hyperparameter \u03b1 in the linear regression model of Section 3.5, there are two approaches to determining the \u03ben. In the \ufb01rst approach, we recognize that the function L(\u03be) is de\ufb01ned by an integration over w and so we can view w as a latent variable and invoke the EM algorithm. In the second approach, we integrate over w analytically and then perform a direct maximization over \u03be. Let us begin by considering the EM approach. The EM algorithm starts by choosing some initial values for the parameters {\u03ben}, which we denote collectively by \u03beold", "30466989-9430-41a2-b39b-501f1eaa9ac5": "Rao and Sejnowski  suggested how STDP could be the result of a TD-like mechanism at synapses with non-contingent eligibility traces lasting about 10 milliseconds. Dayan  commented that this would require an error as in Sutton and Barto\u2019s  early model of classical conditioning and not a true TD error. Representative publications from the extensive literature on reward-modulated STDP are Wickens , Reynolds and Wickens , and Calabresi, Picconi, Tozzi and Di Filippo .\n\nPawlak and Kerr  showed that dopamine is necessary to induce STDP at the corticostriatal synapses of medium spiny neurons. See also Pawlak, Wickens, Kirkwood, and Kerr . Yagishita, Hayashi-Takagi, Ellis-Davies, Urakubo, Ishii, and Kasai  found that dopamine promotes spine enlargement of the medium spiny neurons of mice only during a time window of from 0.3 to 2 seconds after STDP stimulation. Izhikevich  proposed and explored the idea of using STDP timing conditions to trigger contingent eligibility traces", "14b190a2-2ecf-440c-8e72-212767b1c999": "The other data set from another bank\u2019s campaign contained 4,000,000 interactions involving 12 possible o\u21b5ers.\n\nAll interactions included customer features such as the time since the customer\u2019s last visit to the website, the number of their visits so far, the last time the customer clicked, geographic location, one of a collection of interests, and features giving demographic information. Greedy optimization was based on a mapping estimating the probability of a click as a function of user features. The mapping was learned via supervised learning from one of the data sets by means of a random forest (RF) algorithm . RF algorithms have been widely used for large-scale applications in industry because they are e\u21b5ective predictive tools that tend not to over\ufb01t and are relatively insensitive to outliers and noise. Theocharous et al. then used the mapping to de\ufb01ne an \"-greedy policy that selected with probability 1-\" the o\u21b5er predicted by the RF algorithm to have the highest probability of producing a click, and otherwise selected from the other o\u21b5ers uniformly at random. LTV optimization used a batch-mode reinforcement learning algorithm called \ufb01tted Q iteration (FQI). It is a variant of \ufb01tted value iteration  adapted to Q-learning", "621e9419-4161-4769-ad95-4860af367d24": "However, the key difference in the RVM is that we introduce a separate hyperparameter \u03b1i for each of the weight parameters wi instead of a single shared hyperparameter. Thus the weight prior takes the form where \u03b1i represents the precision of the corresponding parameter wi, and \u03b1 denotes (\u03b11, . , \u03b1M)T. We shall see that, when we maximize the evidence with respect to these hyperparameters, a signi\ufb01cant proportion of them go to in\ufb01nity, and the corresponding weight parameters have posterior distributions that are concentrated at zero. The basis functions associated with these parameters therefore play no role in the predictions made by the model and so are effectively pruned out, resulting in a sparse model", "b05e4fa6-069c-47fa-9d04-25d6142c6eae": "Some operations have undefined gradients, and it is important to track these cases and determine whether the gradient requested by the user is undefined. Various other technicalities make real-world differentiation more complicated. These technicalities are not insurmountable, and this chapter has described the key intellectual tools needed to compute derivatives, but it is important to be aware  https://www.deeplearningbook.org/contents/mlp.html    that many more subtleties exist. 6.5.9 Differentiation outside the Deep Learning Community  The deep learning community has been somewhat isolated from the broader computer science community and has largely developed its own cultural attitudes concerning how to perform differentiation. More generally, the field of automatic differentiation is concerned with how to compute derivatives algorithmically.\n\nThe back-propagation algorithm described here is only one approach to automatic differentiation. It is a special case of a broader class of techniques called reverse  217  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  mode accumulation. Other approaches evaluate the subexpressions of the chain rule in different orders. In general, determining the order of evaluation that results in the lowest computational cost is a difficult problem", "36f132cd-3824-4a54-a55d-0b34255f2b50": "SMOTE: synthetic minority over-sampling technique. J Artif Intellig Res. 2002;16:321-57. Hui H, Wen-Yuan W, Bing-Huan M. Borderline-SMOTE: a new over-sampling method in imbalanced data sets learn- ing. In: Proceedings of ICIC, vol. 3644, Lecture Notes in Computer Science, New York. 2005, p. 878-87.  lan JG, Jean PA, Mehdi M, Bing X, David WF, Sherjil O, Aaron C, Yoshua B. Generative adversarial nets. NIPS. 2014. Leon AG, Alexander SE, Matthias B. A neural algorithm of artistic style. ArXiv. 2015. Barret Z, Quoc VL. Neural architecture search with reinforcement learning. In: International conference on learning representatoins, 2017. Tero K, Timo A, Samuli L, Jaakko L", "04f19c28-c445-42b7-9203-d7bc38d4cfa1": "Unsupervised pretraining may offer some regularization relative to supervised training, or it may simply allow us to train much larger architectures because of the reduced computational cost of the learning rule. ain MhaA NaAwwnnnntnanti: Ga Danita Fan OnnserAlastinnal  https://www.deeplearningbook.org/contents/convnets.html    g-1U Elle LNTULUDSUITLILITLIU DADIS LULL VUVILIVULULLULIAL  Networks  Convolutional networks are perhaps the greatest success story of biologically inspired artificial intelligence. Though convolutional networks have been guided by many other fields, some of the key design principles of neural networks were drawn from neuroscience. The history of convolutional networks begins with neuroscientific experiments long before the relevant computational models were developed. Neurophysiologists David Hubel and Torsten Wiesel collaborated for several years to determine many of the most basic facts about how the mammalian vision system works . Their accomplishments were eventually recognized with a Nobel prize.\n\nTheir findings that have had the greatest influence on contemporary deep learning models were based on recording the activity of individual neurons in cats", "60abeeb3-eff4-486d-b595-1a96def93896": ", this can be mitigated by introducing skip connections in the hidden-to-hidden path, as illustrated in figure 10.13c.\n\n10.6 Recursive Neural Networks  Recursive neural networks\u201d represent yet another generalization of recurrent net- works, with a different kind of computational graph, which is structured as a deep tree, rather than the chain-like structure of RNNs. The typical computational graph for a recursive network is illustrated in figure 10.14. Recursive neural net- works were introduced by Pollack , and their potential use for learning to reason was described by Bottou . Recursive networks have been successfully applied to processing data structures as input to neural nets , in natural language processing , as well as in computer vision (Socher et al., 201 1b). https://www.deeplearningbook.org/contents/rnn.html    One clear advantage of recursive nets over recurrent nets is that for a sequence of the same length 7, the depth (measured as the number of compositions of nonlinear operations) can be drastically reduced from T to O(log), which might help deal with long-term dependencies. An open question is how to best structure  the tree", "7ef28af2-4bff-4d14-9341-179184245415": "Thus the exponential error function will be much less robust to outliers or misclassi\ufb01ed data points.\n\nAnother important difference between cross-entropy and the exponential error function is that the latter cannot be interpreted as the log likelihood function of any well-de\ufb01ned probabilistic model. Furthermore, the exponential error does not Exercise 14.8 generalize to classi\ufb01cation problems having K > 2 classes, again in contrast to the cross-entropy for a probabilistic model, which is easily generalized to give (4.108). Section 4.3.4 The interpretation of boosting as the sequential optimization of an additive model under an exponential error  opens the door to a wide range of boosting-like algorithms, including multiclass extensions, by altering the choice of error function. It also motivates the extension to regression problems . If we consider a sum-of-squares error function for regression, then sequential minimization of an additive model of the form (14.21) simply involves \ufb01tting each new base classi\ufb01er to the residual errors tn\u2212fm\u22121(xn) from the previous model. As Exercise 14.9 we have noted, however, the sum-of-squares error is not robust to outliers, and this can be addressed by basing the boosting algorithm on the absolute deviation |y \u2212 t| instead", "38fd2fd3-aaf3-47c9-9aef-6d1160d0044d": "Students will read, present, and discuss papers, and complete course projects.\n\ne Lectures o Time: Mon/Wed/Fri 3:00pm-3:50pm o Location: SOLIS 105 o Recordings: Podcast (Please login with your UCSD ID)  e Discussion forum: Piazza  e HW and write-up submission: Gradescope (Entry Code: BBX582)  e Contact: Students should ask all course-related questions on Piazza, where you will also find announcements. For personal matters or in emergencies, you can email the instructor. http://zhiting.ucsd.edu/teaching/dsc291winter2023/index.html  DSC 291 Machine Learning with Few Labels  Instructor: Zhiting Hu  Email: zhh019 @ucsd.edu Office hours: Wed 4pm-5pm Location: SDSC E249  Announcements  Machine Learning with Few Labels  \u00a9 Copyright 2023 UC San Diego. Powered by Jeky!! with al-folio theme. http://zhiting.ucsd.edu/teaching/dsc291winter2023/index.html", "bf810c41-944b-4ec7-80ad-b8fe0c298608": "In many applications one wants to be able to update the action very fast to take into account anything that has changed, but bootstrapping works best if it is over a length of time in which a signi\ufb01cant and recognizable state change has occurred. With one-step TD methods, these time intervals are the same, and so a compromise must be made. n-step methods enable bootstrapping to occur over multiple steps, freeing us from the tyranny of the single time step.\n\nThe idea of n-step methods is usually used as an introduction to the algorithmic idea of eligibility traces (Chapter 12), which enable bootstrapping over multiple time intervals simultaneously. Here we instead consider the n-step bootstrapping idea on its own, postponing the treatment of eligibility-trace mechanisms until later. This allows us to separate the issues better, dealing with as many of them as possible in the simpler n-step setting. As usual, we \ufb01rst consider the prediction problem and then the control problem. That is, we \ufb01rst consider how n-step methods can help in predicting returns as a function of state for a \ufb01xed policy (i.e., in estimating v\u21e1). Then we extend the ideas to action values and control methods", "e39eb08f-eb9f-4644-8250-1d27e3596cf2": "This graph corresponds to probability distributions that can be factored as  plasb,e,d,e) = 56M (a,b,\u20ac)6(b, dd (c,0), (3.56)  This graphical model enables us to quickly see some properties of the distribution.\n\nFor example, a and c interact directly, but a and e interact only indirectly via c.  merely as a language to describe which direct probabilistic relationships different machine learning algorithms choose to represent. No further understanding of structured probabilistic models is needed until the discussion of research topics, in part III, where we explore structured probabilistic models in much greater detail. This chapter has reviewed the basic concepts of probability theory that are most relevant to deep learning. One more set of fundamental mathematical tools remains: numerical methods. https://www.deeplearningbook.org/contents/prob.html    77  https://www.deeplearningbook.org/contents/prob.html", "ddbdf2a2-12b0-4a48-9432-f218dc32539d": "The second part of the challenge of o\u21b5-policy learning emerges as the instability of semi-gradient TD methods that involve bootstrapping. We seek powerful function approximation, o\u21b5-policy learning, and the e\ufb03ciency and \ufb02exibility of bootstrapping TD methods, but it is challenging to combine all three aspects of this deadly triad in one algorithm without introducing the potential for instability. There have been several attempts. The most popular has been to seek to perform true stochastic gradient descent (SGD) in the Bellman error (a.k.a. the Bellman residual).\n\nHowever, our analysis concludes that this is not an appealing goal in many cases, and that anyway it is impossible to achieve with a learning algorithm\u2014the gradient of the BE is not learnable from experience that reveals only feature vectors and not underlying states. Another approach, GradientTD methods, performs SGD in the projected Bellman error. The gradient of the PBE is learnable with O(d) complexity, but at the cost of a second parameter vector with a second step size. The newest family of methods, Emphatic-TD methods, re\ufb01ne an old idea for reweighting updates, emphasizing some and de-emphasizing others", "8a852705-913d-47ce-a256-ad4881ff7057": "Today machine learning is conducted at a far larger scale than it has been in the past, and the potential bene\ufb01ts of a good representation learning method have become much more apparent. We note that a new annual conference\u2014the International Conference on Learning Representations\u2014has been exploring this and related topics every year since 2013. It is also less common to explore representation learning within a reinforcement learning context. Reinforcement learning brings some new possibilities to this old issue, such as the auxiliary tasks discussed in Section 17.1. In reinforcement learning, the problem of representation learning can be identi\ufb01ed with the problem of learning the state-update function discussed in Section 17.3. Third, we still need scalable methods for planning with learned environment models. Planning methods have proven extremely e\u21b5ective in applications such as AlphaGo Zero and computer chess in which the model of the environment is known from the rules of the game or can otherwise be supplied by human designers. But cases of full model-based reinforcement learning, in which the environment model is learned from data and then used for planning, are rare.\n\nThe Dyna system described in Chapter 8 is one example, but as described there and in most subsequent work it uses a tabular model without function approximation, which greatly limits its applicability", "64c58604-64b3-4d4e-b29c-b5511d99fa88": "For the discrete distribution q\u22c6(Z) we have the standard result from which we see that the quantities rnk are playing the role of responsibilities. Note that the optimal solution for q\u22c6(Z) depends on moments evaluated with respect to the distributions of other variables, and so again the variational update equations are coupled and must be solved iteratively.\n\nAt this point, we shall \ufb01nd it convenient to de\ufb01ne three statistics of the observed data set evaluated with respect to the responsibilities, given by Note that these are analogous to quantities evaluated in the maximum likelihood EM algorithm for the Gaussian mixture model. Now let us consider the factor q(\u03c0, \u00b5, \u039b) in the variational posterior distribution. Again using the general result (10.9) we have We observe that the right-hand side of this expression decomposes into a sum of terms involving only \u03c0 together with terms only involving \u00b5 and \u039b, which implies that the variational posterior q(\u03c0, \u00b5, \u039b) factorizes to give q(\u03c0)q(\u00b5, \u039b)", "09155b2e-8e68-4a3b-834e-4a22ee645bd8": "x = tf.image.random_brightness(x, max_delta=0.8*s) x = tf.image.random_contrast(x, lower=1-0.8*s, upper=1+0.8*s) x = tf.image.random_saturation(x, lower=1-0.8*s, upper=1+0.8*s) x = tf.image.random_hue(x, max_delta=0.2*s) x = tf.clip_by_value(x, 0, 1) return x # randomly apply transformation with probability p. image = random_apply(color_jitter, image, p=0.8) image = random_apply(color_drop, image, p=0.2) return image A pseudo-code for color distortion using Pytorch is as follows 12", "c66c5751-7af4-48f2-abe3-6158ef009afd": "For instance, in a two-layer network of the kind shown in Figure 5.1, with M hidden units, each point in weight space is a member of a family of M!2M equivalent points.\n\nSection 5.1.1 Furthermore, there will typically be multiple inequivalent stationary points and in particular multiple inequivalent minima. A minimum that corresponds to the smallest value of the error function for any weight vector is said to be a global minimum. Any other minima corresponding to higher values of the error function are said to be local minima. For a successful application of neural networks, it may not be necessary to \ufb01nd the global minimum (and in general it will not be known whether the global minimum has been found) but it may be necessary to compare several local minima in order to \ufb01nd a suf\ufb01ciently good solution. Because there is clearly no hope of \ufb01nding an analytical solution to the equation \u2207E(w) = 0 we resort to iterative numerical procedures. The optimization of continuous nonlinear functions is a widely studied problem and there exists an extensive literature on how to solve it ef\ufb01ciently. Most techniques involve choosing some initial value w(0) for the weight vector and then moving through weight space in a succession of steps of the form where \u03c4 labels the iteration step", "fdb917ea-5853-4ee1-8ecc-f123dc07a733": "But for us, along with most of today\u2019s psychologists, reinforcement learning is much broader than this, including in addition to S-R learning, methods involving value functions, environment models, planning, and other processes that are commonly thought to belong to the more cognitive side of mental functioning. Neuroscience is the multidisciplinary study of nervous systems: how they regulate bodily functions; control behavior; change over time as a result of development, learning, and aging; and how cellular and molecular mechanisms make these functions possible. One of the most exciting aspects of reinforcement learning is the mounting evidence from neuroscience that the nervous systems of humans and many other animals implement algorithms that correspond in striking ways to reinforcement learning algorithms. The main objective of this chapter is to explain these parallels and what they suggest about the neural basis of reward-related learning in animals. The most remarkable point of contact between reinforcement learning and neuroscience involves dopamine, a chemical deeply involved in reward processing in the brains of mammals.\n\nDopamine appears to convey temporal-di\u21b5erence (TD) errors to brain structures where learning and decision making take place. This parallel is expressed by the reward prediction error hypothesis of dopamine neuron activity, a hypothesis that resulted from the convergence of computational reinforcement learning and results of neuroscience experiments", "8e70140d-6da6-4fbe-abe7-0da5eb88bb2d": "While the log-likelihood of a DBN is intractable, it may be approximated with AIS . This permits evaluating its quality as a generative model. The term \u201cdeep belief network\u201d is commonly used incorrectly to refer to any kind of deep neural network, even networks without latent variable semantics. The term should refer specifically to models with undirected connections in the deepest layer and directed connections pointing downward between all other pairs of consecutive layers. The term may also cause some confusion because \u201cbelief network\u201d is sometimes used to refer to purely directed models, while deep belief networks contain an undirected layer. Deep belief networks also share the acronym DBN with dynamic Bayesian networks , which are Bayesian networks for representing Markov chains. 659  CHAPTER 20. DEEP GENERATIVE MODELS  20.4 Deep Boltzmann Machines  A deep Boltzmann machine, or DBM  is another kind of deep generative model. Unlike the deep belief network (DBN), it is an entirely undirected model. Unlike the RBM, the DBM has several layers of latent variables (RBMs have just one)", "53ccf08b-ec0f-4596-b4d2-62647052b250": "Using (C.7), we can write Exercise 7.17 where \u03a6 and \u03a3 involve only those basis vectors that correspond to \ufb01nite hyperparameters \u03b1i. At each stage the required computations therefore scale like O(M 3), where M is the number of active basis vectors in the model and is typically much smaller than the number N of training patterns. We can extend the relevance vector machine framework to classi\ufb01cation problems by applying the ARD prior over weights to a probabilistic linear classi\ufb01cation model of the kind studied in Chapter 4. To start with, we consider two-class problems with a binary target variable t \u2208 {0, 1}. The model now takes the form of a linear combination of basis functions transformed by a logistic sigmoid function where \u03c3(\u00b7) is the logistic sigmoid function de\ufb01ned by (4.59). If we introduce a Gaussian prior over the weight vector w, then we obtain the model that has been considered already in Chapter 4. The difference here is that in the RVM, this model uses the ARD prior (7.80) in which there is a separate precision hyperparameter associated with each weight parameter.\n\nIn contrast to the regression model, we can no longer integrate analytically over the parameter vector w", "81b0ccf9-eb2b-4f60-a45c-3120b84d3eb4": "Equation 10.5 can be drawn in two different ways. One way to draw the RNN is with a diagram containing one node for every component that might exist in a physical implementation of the model, such as a biological neural network. In this view, the network defines a circuit that operates in real time, with physical parts whose current state can influence their future state, as in the left of figure 10.2. Throughout this chapter, we use a black square in a circuit diagram to indicate that an interaction takes place with a delay of a single time step, from the state at time t to the state at time t+ 1. The other way to draw the RNN is as an unfolded computational graph, in which each component is represented by many different variables, with one variable per time step, representing the state of the component at that point in time. Each variable for each time step is drawn as a separate node of the computational graph, as in the right of figure 10.2. What we call unfolding is the operation that maps a circuit, as in the left side of the figure, to a computational graph with repeated pieces, as in the right side. The unfolded graph now has a size that depends on the sequence length", "b79836e6-6268-4b02-82df-5bcd1c4845d7": "Imagine learning a model of images of faces, using unstructured Gaussian noise as poise: If Dmodel learns about eyes, it can reject almost all unstructured noise samples without having learned anything about other facial features, such as mouths. The constraint that pyoise must be easy to evaluate and easy to sample from can be overly restrictive. When pnoise is simple, most samples are likely to be too  ale--ta---1e- Ainai.. n+ L042. 4 Ante be \u00a3222, . model i ~ t-.~-0--. 242. 1--  https://www.deeplearningbook.org/contents/partition.html    UVVLOUSLY UILDSLIUICL LLOLL LIL Udabla LUO LULCe py LUO HILpLOVve LOLICCAaVLy", "0c8a750b-1bc2-4d96-8c43-8d357ca86a34": "An important concept for probability distributions over multiple variables is that of conditional independence .\n\nConsider three variables a, b, and c, and suppose that the conditional distribution of a, given b and c, is such that it does not depend on the value of b, so that We say that a is conditionally independent of b given c. This can be expressed in a slightly different way if we consider the joint distribution of a and b conditioned on c, which we can write in the form where we have used the product rule of probability together with (8.20). Thus we see that, conditioned on c, the joint distribution of a and b factorizes into the product of the marginal distribution of a and the marginal distribution of b (again both conditioned on c). This says that the variables a and b are statistically independent, given c. Note that our de\ufb01nition of conditional independence will require that (8.20), or equivalently (8.21), must hold for every possible value of c, and not just for some values. We shall sometimes use a shorthand notation for conditional independence  in which denotes that a is conditionally independent of b given c and is equivalent to (8.20)", "ddbcd5a9-6007-422c-a63b-efb101b91ac4": "A reinforcement learning algorithm\u2019s need to search means that it has to explore in some way.\n\nAnimals clearly explore as well, and early animal learning researchers disagreed about the degree of guidance an animal uses in selecting its actions in situations like Thorndike\u2019s puzzle boxes. Are actions the result of \u201cabsolutely random, blind groping\u201d (Woodworth, 1938, p. 777), or is there some degree of guidance, either from prior learning, reasoning, or other means? Although some thinkers, including Thorndike, seem to have taken the former position, others favored more deliberate exploration. Reinforcement learning algorithms allow wide latitude for how much guidance an agent can employ in selecting actions. The forms of exploration we have used in the algorithms presented in this book, such as \"-greedy and upper-con\ufb01dence-bound action selection, are merely among the simplest. More sophisticated methods are possible, with the only stipulation being that there has to be some form of exploration for the algorithms to work e\u21b5ectively. The feature of our treatment of reinforcement learning allowing the set of actions available at any time to depend on the environment\u2019s current state echoes something Thorndike observed in his cats\u2019 puzzle-box behaviors", "aadb7fe1-4b62-47f9-8d05-3f387f1f993c": "It remains possible that pretraining initializes the model in a location that would otherwise be inaccessible\u2014for example, a region that is surrounded by areas where the cost function varies so much from one example to another that minibatches give only a very noisy estimate of the gradient, or a region surrounded by areas where the Hessian matrix is so poorly conditioned that gradient descent methods must use very small steps.\n\nHowever, our ability to characterize exactly what aspects of the pretrained parameters are retained during the supervised training stage is limited. https://www.deeplearningbook.org/contents/representation.html    This is one reason that modern approaches typically use simultaneous unsupervised learning and supervised learning rather than two sequential stages. One may also avoid struggling with these complicated ideas about how optimization in the  supervised learning stage preserves information from the unsupervised learning stage by simply freezing the parameters for the feature extractors and using supervised learning only to add a classifier on top of the learned features. The other idea, that a learning algorithm can use information learned in the unsupervised phase to perform better in the supervised learning stage, is better understood. The basic idea is that some features that are useful for the unsupervised \u2018ask may also be useful for the supervised learning task", "40e8101a-ad71-4830-9217-b37b30234b6a": "These other points may include blurry images.\n\nPart of the reason that the model would choose to put probability mass on blurry images rather than some other part of the space is that the variational autoencoders used in practice usually have a Gaussian distribution for Pmode(@;g(z)). Maximizing a lower bound on the likelihood of such a distribution is similar to training a traditional autoencoder with mean squared error, in the sense that it has a tendency to ignore features of the input that occupy few pixels or that cause only a small change in the brightness of the pixels that they occupy. This issue is not specific to VAEs and  https://www.deeplearningbook.org/contents/generative_models.html    is shared Wp ers models that optimize a log-likelihood, or e uivalently, DKL(p datal| pmodel) as argued by Theis e\u00a2 al. Pols and by Huszar . Another troubling issue with contemporary VAE models is that they tend to a only a small  subset of the dimensions of z, as if the encoder were not able to transform enough of the local directions in input space to a space where the marginal distribution matches the factorized prior", "e610e915-ecb8-4b0e-b597-048dde3bd13d": "Maximum likelihood methods can also be cast into a sequential framework.\n\nSection 2.3.5 If our goal is to predict, as best we can, the outcome of the next trial, then we must evaluate the predictive distribution of x, given the observed data set D. From the sum and product rules of probability, this takes the form Using the result (2.18) for the posterior distribution p(\u00b5|D), together with the result (2.15) for the mean of the beta distribution, we obtain which has a simple interpretation as the total fraction of observations (both real observations and \ufb01ctitious prior observations) that correspond to x = 1. Note that in the limit of an in\ufb01nitely large data set m, l \u2192 \u221e the result (2.20) reduces to the maximum likelihood result (2.8). As we shall see, it is a very general property that the Bayesian and maximum likelihood results will agree in the limit of an in\ufb01nitely large data set. For a \ufb01nite data set, the posterior mean for \u00b5 always lies between the prior mean and the maximum likelihood estimate for \u00b5 corresponding to the relative frequencies of events given by (2.7). Exercise 2.7 From Figure 2.2, we see that as the number of observations increases, so the posterior distribution becomes more sharply peaked", "2a18f233-9020-40b5-851e-bfda270a1b09": "In each pass over that episode you can use a slightly longer horizon and obtain slightly better results. Recall that the truncated \u03bb-return is de\ufb01ned in (12.9) as Let us step through how this target could ideally be used if computational complexity was not an issue. The episode begins with an estimate at time 0 using the weights w0 from the end of the previous episode. Learning begins when the data horizon is extended to time step 1. The target for the estimate at step 0, given the data up to horizon 1, could only be the one-step return G0:1, which includes R1 and bootstraps from the estimate equation degenerating to zero. Using this update target, we construct w1. Then, after advancing the data horizon to step 2, what do we do? We have new data in the form of R2 and S2, as well as the new w1, so now we can construct a better update target G\u03bb for the \ufb01rst update from S0 as well as a better update target G\u03bb from S1. Using these improved targets, we redo the updates at S1 and S2, starting again from w0, to produce w2", "504d99d4-f263-4d70-a5c3-c1fc41b7992a": "First we choose some initial values for the parameters \u00b5old, \u03a3old and \u03c0old, and use these to evaluate the responsibilities (the E step). We then keep the responsibilities \ufb01xed and maximize (9.40) with respect to \u00b5k, \u03a3k and \u03c0k (the M step). This leads to closed form solutions for \u00b5new, \u03a3new and \u03c0new given by (9.17), (9.19), and (9.22) as before. This is precisely the EM algorithm for Exercise 9.8 Gaussian mixtures as derived earlier.\n\nWe shall gain more insight into the role of the expected complete-data log likelihood function when we give a proof of convergence of the EM algorithm in Section 9.4. Comparison of the K-means algorithm with the EM algorithm for Gaussian mixtures shows that there is a close similarity. Whereas the K-means algorithm performs a hard assignment of data points to clusters, in which each data point is associated uniquely with one cluster, the EM algorithm makes a soft assignment based on the posterior probabilities. In fact, we can derive the K-means algorithm as a particular limit of EM for Gaussian mixtures as follows", "c5b8e850-f558-43b0-bddb-d1fb00fd525d": "In other words, if f(x) = sup\u03b1\u2208A f\u03b1(x) and f\u03b1(x) is convex in x for every \u03b1, then \u2202f\u03b2(x) \u2208 \u2202f if \u03b2 = arg sup\u03b1\u2208A f\u03b1(x). This is equivalent to computing a gradient descent update for pg at the optimal D given the corresponding G. supD U(pg, D) is convex in pg with a unique global optima as proven in Thm 1, therefore with suf\ufb01ciently small updates of pg, pg converges to px, concluding the proof. In practice, adversarial nets represent a limited family of pg distributions via the function G(z; \u03b8g), and we optimize \u03b8g rather than pg itself, so the proofs do not apply. However, the excellent performance of multilayer perceptrons in practice suggests that they are a reasonable model to use despite their lack of theoretical guarantees.\n\nWe trained adversarial nets an a range of datasets including MNIST, the Toronto Face Database (TFD) , and CIFAR-10", "50399be6-117b-4797-aae8-e562d1444352": "Neural networks are incredibly powerful at mapping high-dimensional inputs into lower-dimensional representations. These networks can map images to binary classes or to n x 1 vectors in flattened layers. The sequential processing of neural networks can be manipulated such that the intermediate representations can be separated from the network as a whole. The lower-dimensional representations of image data in fully-con- nected layers can be extracted and isolated. Konno and Iwazume  find a performance boost on CIFAR-100 from 66 to 73% accuracy by manipulating the modularity of neural networks to isolate and refine individual layers after training. Lower-dimensional repre- sentations found in high-level layers of a CNN are known as the feature space. DeVries and Taylor  presented an interesting paper discussing augmentation in this feature space. This opens up opportunities for many vector operations for Data Augmentation. SMOTE is a popular augmentation used to alleviate problems with class imbalance.\n\nThis technique is applied to the feature space by joining the k nearest neighbors to form new instances. DeVries and Taylor discuss adding noise, interpolating, and extrapolating  as common forms of feature space augmentation (Figs. 13, 14)", "130ecefc-f874-4da6-86a0-c1bc1385a86f": "For now, we simply note that the entire issue of balancing exploration and exploitation does not even arise in supervised and unsupervised learning, at least in the purest forms of these paradigms. Another key feature of reinforcement learning is that it explicitly considers the whole problem of a goal-directed agent interacting with an uncertain environment. This is in contrast to many approaches that consider subproblems without addressing how they might \ufb01t into a larger picture.\n\nFor example, we have mentioned that much of machine learning research is concerned with supervised learning without explicitly specifying how such an ability would \ufb01nally be useful. Other researchers have developed theories of planning with general goals, but without considering planning\u2019s role in real-time decision making, or the question of where the predictive models necessary for planning would come from. Although these approaches have yielded many useful results, their focus on isolated subproblems is a signi\ufb01cant limitation. Reinforcement learning takes the opposite tack, starting with a complete, interactive, goal-seeking agent. All reinforcement learning agents have explicit goals, can sense aspects of their environments, and can choose actions to in\ufb02uence their environments. Moreover, it is usually assumed from the beginning that the agent has to operate despite signi\ufb01cant uncertainty about the environment it faces", "1ede47b9-378b-4a0d-b706-7316312f08f8": "Over time, the bucket brigade came to be more like TD learning in passing credit back to any temporally preceding rule, not just to the ones that triggered the current rule. The modern form of the bucket brigade, when simpli\ufb01ed in various natural ways, is nearly identical to one-step Sarsa, as detailed by Wilson . 6.5 Q-learning was introduced by Watkins , whose outline of a convergence proof was made rigorous by Watkins and Dayan . More general convergence results were proved by Jaakkola, Jordan, and Singh  and Tsitsiklis . Sarsa in the \ufb01rst edition of this book as an exercise, or to van Seijen, van Hasselt, Whiteson, and Weiring  when they established Expected Sarsa\u2019s convergence properties and conditions under which it will outperform regular Sarsa and Q-learning. Our Figure 6.3 is adapted from their results. Van Seijen et al", "b4faf955-6531-459e-bc5e-16a3af1fe855": "The approach of Lee e\u00a2 al. of making each of the detector units in the same pooling region mutually exclusive solves the computational problems but still does not allow variably sized pooling regions. For example, suppose we learn a model with 2 x 2 probabilistic max pooling over detector units that learn edge detectors. This enforces the constraint that only one of these edges may appear in each 2 x 2 region.\n\nIf we then increase the size of the input image by 50 percent in each direction, we would expect the number of edges to increase correspondingly. Instead, if we increase the size of the pooling regions by 50 percent in each direction to 3 x 3, then the mutual exclusivity constraint now specifies that each of these edges may appear only once in a 3 x 3 region. As we grow a model\u2019s input image in this way, the model generates edges with less density. Of course, these issues only arise when the model must use variable amounts of pooling in order to emit a fixed-size output vector", "e27a3d69-73fd-444b-b4ea-4d7bda645442": "It has new sections on arti\ufb01cial neural networks, the fourier basis, LSTD, kernel-based methods, Gradient-TD and Emphatic-TD methods, average-reward methods, true online TD(\u03bb), and policygradient methods.\n\nThe second edition signi\ufb01cantly expands the treatment of o\u21b5-policy learning, \ufb01rst for the tabular case in Chapters 5\u20137, then with function approximation in Chapters 11 and 12. Another change is that the second edition separates the forward-view idea of n-step bootstrapping (now treated more fully in Chapter 7) from the backwardview idea of eligibility traces (now treated independently in Chapter 12). The third part of the book has large new chapters on reinforcement learning\u2019s relationships to psychology (Chapter 14) and neuroscience (Chapter 15), as well as an updated case-studies chapter including Atari game playing, Watson\u2019s wagering strategy, and the Go playing programs AlphaGo and AlphaGo Zero (Chapter 16). Still, out of necessity we have included only a small subset of all that has been done in the \ufb01eld. Our choices re\ufb02ect our long-standing interests in inexpensive model-free methods that should scale well to large applications. The \ufb01nal chapter now includes a discussion of the future societal impacts of reinforcement learning", "363815ab-a702-40d5-9ea7-66f96f774c70": "Any point \ufffdx that lies on the line connecting xA and xB can be expressed in the form and xB both lie inside the same decision region Rk, then any point bx that lies on the line connecting these two points must also lie in Rk, and hence the decision region must be singly connected and convex. where 0 \u2a7d \u03bb \u2a7d 1. From the linearity of the discriminant functions, it follows that Because both xA and xB lie inside Rk, it follows that yk(xA) > yj(xA), and yk(xB) > yj(xB), for all j \u0338= k, and hence yk(\ufffdx) > yj(\ufffdx), and so \ufffdx also lies inside Rk. Thus Rk is singly connected and convex.\n\nNote that for two classes, we can either employ the formalism discussed here, based on two discriminant functions y1(x) and y2(x), or else use the simpler but equivalent formulation described in Section 4.1.1 based on a single discriminant function y(x)", "aca8380a-f7f2-466f-8440-bb2ae73b1396": "The gray arrows in the upper part of Figure 11.3 cannot be followed because after the \ufb01rst update (dark line) the value function must be projected back into something representable.\n\nThe next iteration then begins within the subspace; the value function is again taken outside of the subspace by the Bellman operator and then mapped back by the projection operator, as suggested by the lower gray arrow and line. Following these arrows is a DP-like process with approximation. In this case we are interested in the projection of the Bellman error vector back into the representable space. This is the projected Bellman error vector \u21e7\u00af\u03b4vw, shown in Figure 11.3 as PBE. The size of this vector, in the norm, is another measure of error in the approximate value function. For any approximate value function v, we de\ufb01ne the Mean Square Projected Bellman Error, denoted PBE, as With linear function approximation there always exists an approximate value function (within the subspace) with zero PBE; this is the TD \ufb01xed point, wTD, introduced in Section 9.4. As we have seen, this point is not always stable under semi-gradient TD methods and o\u21b5-policy training", "465e2335-f5f0-4265-b7ff-0bc6f84c6af8": "Also, the practical applicability of Bayesian methods has been greatly enhanced through the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation. Similarly, new models based on kernels have had signi\ufb01cant impact on both algorithms and applications. This new textbook re\ufb02ects these recent developments while providing a comprehensive introduction to the \ufb01elds of pattern recognition and machine learning. It is aimed at advanced undergraduates or \ufb01rst year PhD students, as well as researchers and practitioners, and assumes no previous knowledge of pattern recognition or machine learning concepts.\n\nKnowledge of multivariate calculus and basic linear algebra is required, and some familiarity with probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. Because this book has broad scope, it is impossible to provide a complete list of references, and in particular no attempt has been made to provide accurate historical attribution of ideas. Instead, the aim has been to give references that offer greater detail than is possible here and that hopefully provide entry points into what, in some cases, is a very extensive literature. For this reason, the references are often to more recent textbooks and review articles rather than to original sources", "5883a3d5-fd9e-4cc9-980f-640468467608": "1.34 (\u22c6 \u22c6) www Use the calculus of variations to show that the stationary point of the functional (1.108) is given by (1.108). Then use the constraints (1.105), (1.106), and (1.107) to eliminate the Lagrange multipliers and hence show that the maximum entropy solution is given by the Gaussian (1.109). 1.36 (\u22c6) A strictly convex function is de\ufb01ned as one for which every chord lies above the function.\n\nShow that this is equivalent to the condition that the second derivative of the function be positive. Draw a diagram to show the relationship between these various quantities. 1.40 (\u22c6) By applying Jensen\u2019s inequality (1.115) with f(x) = ln x, show that the arithmetic mean of a set of real numbers is never less than their geometrical mean. In Chapter 1, we emphasized the central role played by probability theory in the solution of pattern recognition problems. We turn now to an exploration of some particular examples of probability distributions and their properties. As well as being of great interest in their own right, these distributions can form building blocks for more complex models and will be used extensively throughout the book", "4f2e8607-bb12-4fac-9f77-ffa955bdc20f": "This divergence is symmetrical and always de\ufb01ned because we can choose \u00b5 = Pm.\n\nThe Earth-Mover (EM) distance or Wasserstein-1 where \u03a0(Pr, Pg) denotes the set of all joint distributions \u03b3(x, y) whose marginals are respectively Pr and Pg. Intuitively, \u03b3(x, y) indicates how much \u201cmass\u201d must be transported from x to y in order to transform the distributions Pr into the distribution Pg. The EM distance then is the \u201ccost\u201d of the optimal transport plan. The following example illustrates how apparently simple sequences of probability distributions converge under the EM distance but do not converge under the other distances and divergences de\ufb01ned above. Example 1 (Learning parallel lines). Let Z \u223c U the uniform distribution on the unit interval. Let P0 be the distribution of (0, Z) \u2208 R2 (a 0 on the x-axis and the random variable Z on the y-axis), uniform on a straight vertical line passing through the origin. Now let g\u03b8(z) = (\u03b8, z) with \u03b8 a single real parameter", "42d1b7b6-27e2-49d5-b013-8d677cf33170": "Suppose that the marginal distributions over the mean and precision are given by N(\u00b5|\u00b50, s0) and Gam(\u03c4|a, b), where Gam(\u00b7|\u00b7, \u00b7) denotes a gamma distribution. Write down expressions for the conditional distributions p(\u00b5|x, \u03c4) and p(\u03c4|x, \u00b5) that would be required in order to apply Gibbs sampling to the posterior distribution p(\u00b5, \u03c4|x). variance \u03c3i, and where \u03bd has zero mean and unit variance, gives a value z\u2032 is equivalent to (11.53). Similarly, using (11.57) show that (11.59) is equivalent to (11.55). 11.17 (\u22c6) www Verify that the two probabilities (11.68) and (11.69) are equal, and hence that detailed balance holds for the hybrid Monte Carlo algorithm. In Chapter 9, we discussed probabilistic models having discrete latent variables, such as the mixture of Gaussians. We now explore models in which some, or all, of the latent variables are continuous", "fb8fc1f5-7aa2-4838-b237-195b47829718": "The space between the data points corresponds to the region between the manifolds, where the reconstruction function must have a large derivative to map corrupted points back onto the manifold. CHAPTER 14. AUTOENCODERS  Figure 14.8: Nonparametric manifold learning procedures build a nearest neighbor graph in which nodes represent training examples and directed edges indicate nearest neighbor relationships. Various procedures can thus obtain the tangent plane associated with a neighborhood of the graph as well as a coordinate system that associates each training example with a real-valued vector position, or embedding. It is possible to generalize such a representation to new examples by a form of interpolation. As long as the number of examples is large enough to cover the curvature and twists of the manifold, these  https://www.deeplearningbook.org/contents/autoencoders.html    aRpae acne work well. Images from the QMUL Multiview Face Dataset  associate each node with a tangent plane that spans the directions of variations associated with the difference vectors between the example and its neighbors, as illustrated in figure 14.8. A global coordinate system can then be obtained through an optimization or by solving a linear system", "6ba82782-9a2f-4f80-8d00-a3a2f84986bb": "Alleviating class imbalance with Data Augmentation  Class imbalance is a common problem in which a dataset is primarily composed of examples from one class. This could manifest itself in a binary classification problem such that there is a clear majority-minority class distinction, or in multi-class classifi- cation in which there is one or multiple majority classes and one or multiple minority classes. Imbalanced datasets are harmful because they bias models towards majority class predictions. Imbalanced datasets also render accuracy as a deceitful performance metric. Buda et al. provide a systematic study specifically investigating the impact of imbalanced data in CNNs processing image data. Leevy et al. cover many Data- level and Algorithm-level solutions to class imbalance in big data in general.\n\nData Augmentation falls under a Data-level solution to class imbalance and there are many different strategies for implementation. A naive solution to oversampling with Data Augmentation would be a simple random oversampling with small geometric transformations such as a 30\u00b0 rotation", "9445bb2d-d418-4b5c-8703-15713468617c": "A (Long) Peek into Reinforcement Learning | Lil'Log   Posts Archit] 'L\u00a2gech Tags FAQ emojisearch.app  A (Long) Peek into Reinforcement Learning  . A couple of exciting news in Artificial Intelligence (Al) has just happened in recent years. AlphaGo defeated the best professional human player in the game of Go. Very soon the extended algorithm AlphaGo Zero beat AlphaGo by 100-0 without supervised learning on human knowledge. Top professional game players lost to the bot developed by OpenAl on DOTA2 1v1 competition. After knowing these, it is pretty hard not to be curious about the magic behind these algorithms \u2014 Reinforcement Learning (RL). I'm writing this post to briefly go over the field. We will first introduce several fundamental concepts and then dive into classic approaches to solving RL problems. Hopefully, this post could be a good starting point for newbies, bridging the future study on the cutting-edge research. What is Reinforcement Learning? Say, we have an agent in an unknown environment and this agent can obtain some rewards by interacting with the environment. The agent ought to take actions so as to maximize cumulative rewards", "edbad2a1-7090-4eb4-ba1a-d90ff30c1ba1": "The Bandit Gradient Algorithm as Stochastic Gradient Ascent One can gain a deeper insight into the gradient bandit algorithm by understanding it as a stochastic approximation to gradient ascent. In exact gradient ascent, each action preference Ht(a) would be incremented proportional to the increment\u2019s e\u21b5ect on performance: where the measure of performance here is the expected reward: and the measure of the increment\u2019s e\u21b5ect is the partial derivative of this performance measure with respect to the action preference. Of course, it is not possible to implement gradient ascent exactly in our case because by assumption we do not know the q\u21e4(x), but in fact the updates of our algorithm (2.12) are equal to (2.13) in expected value, making the algorithm an instance of stochastic gradient ascent. The calculations showing this require only beginning calculus, but take several steps", "b1ffc7f4-07d5-47f8-a379-6231894cba1b": "Morgan Kaufmann. Tanner, B. Temporal-Di\u21b5erence Networks. MSc thesis, University of Alberta. Taylor, G., Parr, R. Kernelized value function approximation for reinforcement learning. In Proceedings of the 26th International Conference on Machine Learning , pp. 1017\u20131024. ACM, New York. Taylor, M. E., Stone, P. Transfer learning for reinforcement learning domains: A survey. Tesauro, G. TD-Gammon, a self-teaching backgammon program, achieves master-level Tesauro, G. Temporal di\u21b5erence learning and TD-Gammon. Communications of the Tesauro, G., Galperin, G. R. On-line policy improvement using Monte-Carlo search. In Tesauro, G., Gondek, D. C., Lechner, J., Fan, J., Prager, J. M", "c6ec0777-6b63-4196-8591-552bbbd75696": "This has computational complexity O(NK2M) that is exponential in the number M of latent chains and so will be intractable for anything other than small values of M. One solution would be to use sampling methods (discussed in Chapter 11). As an elegant deterministic alternative, Ghahramani and Jordan  exploited variational inference techniques Section 10.1 to obtain a tractable algorithm for approximate inference. This can be done using a simple variational posterior distribution that is fully factorized with respect to the latent variables, or alternatively by using a more powerful approach in which the variational distribution is described by independent Markov chains corresponding to the chains of latent variables in the original model.\n\nIn the latter case, the variational inference algorithms involves running independent forward and backward recursions along each chain, which is computationally ef\ufb01cient and yet is also able to capture correlations between variables within the same chain. Clearly, there are many possible probabilistic structures that can be constructed according to the needs of particular applications. Graphical models provide a general technique for motivating, describing, and analysing such structures, and variational methods provide a powerful framework for performing inference in those models for which exact solution is intractable", "84deac3f-7fe9-4ac4-ba7a-cc17a67698f1": "The bottom row shows outputs of the nonlinearity.\n\nThe top row shows the outputs of max pooling, with a stride of one pixel between pooling regions and a pooling region width of three pixels. (Bottom)A view of the same network, after the input has been shifted to the right by one pixel. Every value in the bottom row has changed, but only half of the values in the top row have changed, because the max pooling units are sensitive only to the maximum value in the neighborhood, not its exact location. by two edges meeting at a specific orientation, we need to preserve the location of the edges well enough to test whether they meet. The use of pooling can be viewed as adding an infinitely strong prior that the function the layer learns must be invariant to small translations. When this assumption is correct, it can greatly improve the statistical efficiency of the network. Pooling over spatial regions produces invariance to translation, but if we pool over the outputs of separately parametrized convolutions, the features can learn which transformations to become invariant to (see figure 9.9)", "082cd12a-9d10-4664-916d-199c855c0a3c": "ular methods for computing the (6; are  310  OPTIMIZATION FOR TRAINING DEEP MODELS  1. Fletcher-Reeves:  2. Polak-  VoJ(0:)' Vol(O2) Be= VoJ(0:-1)' Vol(O:-1) (8.30) Ribi\u00e9re: a, \u2014 Wot () = VeJ(@:-1)) | Vos() (8.31)  VoJ (01-1)! VoJ (01-1)  For a quadratic surface, the conjugate directions ensure that the gradient along the previous direction does not increase in magnitude. We therefore stay at the minimum along the previous directions.\n\nAs a consequence, in a k-dimensional  parameter space, the conjugate gradient method requires at most k line searches to achieve the minimum. The conjugate gradient algorithm is given in algorithm 8.9", "cc9af0c8-d68c-4933-857f-13340b9e3ce0": "\u2022 Distant supervision Distant supervision generates training labels by heuristically aligning data points with an external knowledge base and is one of the most popular forms of weak supervision . \u2022 Weak classi\ufb01ers Classi\ufb01ers that are insuf\ufb01cient for our task\u2014e.g., limited coverage, noisy, biased, and/or trained on a different dataset\u2014can be used as labeling functions. \u2022 Labeling function generators One higher-level abstraction that we can build on top of labeling functions in Snorkel is labeling function generators, which generate multiple labeling functions from a single resource, such as crowdsourced labels and distant supervision from structured knowledge bases (Example 2.4).\n\nExample 2.4 A challenge in traditional distant supervision is that different subsets of knowledge bases have different levels of accuracy and coverage. In our running example, we can use the Comparative Toxicogenomics Database (CTD)4 as distant supervision, separately modeling different subsets of it with separate labeling functions. For example, we might write one labeling function to label a candidate True if it occurs in the \u201cCauses\u201d subset, and another to label it False if it occurs in the \u201cTreats\u201d subset", "862089b6-3f9f-4d29-8f19-2fbd10c8eed9": ", N and the kernel function k(x, xn) is given by The result (6.45) is known as the Nadaraya-Watson model, or kernel regression .\n\nFor a localized kernel function, it has the property of giving more weight to the data points xn that are close to x. Note that the kernel (6.46) satis\ufb01es the summation constraint regression model using isotropic Gaussian kernels, for the sinusoidal data set. The original sine function is shown by the green curve, the data points are shown in blue, and each is the centre of an isotropic Gaussian kernel. The resulting regression function, given by the conditional mean, is shown by the red line, along with the twostandard-deviation region for the conditional distribution p(t|x) shown by the red shading. The blue ellipse around each data point shows one standard deviation contour for the corresponding kernel. These appear noncircular due to the different scales on the horizontal and vertical axes. In fact, this model de\ufb01nes not only a conditional expectation but also a full from which other expectations can be evaluated", "b43610bb-eb20-473f-9aac-3db8a8def883": "When the Hessian has a poor condition number, gradient descent performs poorly. This is because in one direction, the derivative increases rapidly, while in another direction, it increases slowly. Gradient descent is unaware of this change in the derivative, so it does not know that it needs to explore preferentially in the direction where the derivative remains negative for longer. Poor condition number also makes choosing a good step size difficult. The step size must be small enough to avoid overshooting the minimum and going uphill in directions with strong positive curvature. This usually means that the step size is too small to make significant progress in other directions with less curvature. See figure 4.6 for an example. 87  CHAPTER 4. NUMERICAL COMPUTATION  https://www.deeplearningbook.org/contents/numerical.html    Figure 4.5: A saddle point containing both positive and negative curvature. The function in this example is f(a) = 2? \u2014 73. Along the axis corresponding to x1, the function  curves upward. This axis is an eigenvector of the Hessian and has a positive eigenvalue", "be7ddd2d-6c3f-4725-8023-06eca0d8c979": "Convex optimization  https://www.deeplearningbook.org/contents/mlp.html    converges starting trom any initial paramegers (in theory\u2014in practice it is robust but can encounter numerical problems). Stochastic gradient descent applied to nonconvex loss functions has no such convergence guarantee and is sensitive to the values of the initial parameters. For feedforward neural networks, it is important to  initialize all weights to small random values.\n\nThe biases may be initialized to zero or to small positive values. The iterative gradient-based optimization algorithms used to train feedforward networks and almost all other deep models are described in detail in chapter 8, with parameter initialization in particular discussed in section 8.4. For the moment, it suffices to understand that the training algorithm is almost always based on using the gradient to descend the cost function in one way or another. The specific algorithms are improvements and refinements on the ideas of gradient descent, introduced in section 4.3, and, more specifically, are most often improvements of the stochastic gradient descent algorithm, introduced in section 5.9. We can of course train models such as linear regression and support vector machines with gradient descent too, and in fact this is common when the training set is extremely large", "143fea4c-d4c1-4c34-9610-8f8a42a5c390": "Adversarial attacks can help to illustrate weak decision boundaries better than standard classifica- tion metrics can. In addition to serving as an evaluation metric, defense to adversarial attacks, adver- sarial training can be an effective method for searching for augmentations. By constraining the set of augmentations and distortions available to an adversarial network, it can learn to produce augmentations that result in misclassifications, thus forming an effective search algorithm. These augmentations are valuable for strength- ening weak spots in the classification model. Therefore, adversarial training can be an effective search technique for Data Augmentation. This is in heavy contrast to the traditional augmentation techniques described previously. Adversarial augmentations may not represent examples likely to occur in the test set, but they can improve weak spots in the learned decision boundary. Engstrom et al. showed that simple transformations such as rotations and translations can easily cause misclassifications by deep CNN models. The worst out of the random transformations reduced the accuracy of MNIST by 26%, CIFAR10 by 72% and ImageNet (Top 1) by 28%. Goodfellow et al", "3d18125f-0e35-488f-a4af-24834352721f": "This implies that the Bellman optimality equation (4.1) holds, and thus that the policy and the value function are optimal. The evaluation and improvement processes in GPI can be viewed as both competing and cooperating. They compete in the sense that they pull in opposing directions. Making the policy greedy with respect to the value function typically makes the value function incorrect for the changed policy, and making the value function consistent with the policy typically causes that policy no longer to be greedy. In the long run, however, these two processes interact to \ufb01nd a single joint solution: the optimal value function and an optimal policy. the evaluation and improvement processes in GPI in terms of two constraints or goals\u2014for example, as two lines in two-dimensional space as suggested by the diagram to the right. Although the real geometry is much more complicated than this, the diagram suggests what happens in the real case. Each process drives the value function or policy toward one of the lines representing a solution to one of the two goals. The goals interact because the two lines are not orthogonal. Driving directly toward one goal causes some movement away from the other goal", "7fc46e9f-1a5e-431f-a0cc-68742a4dca3c": "The network predicted expert moves on a held out test set with an accuracy of  57.0% using all input features, and 55.7% using only raw board position and move history as inputs, compared to the state-of-the-art from Rollout Policy         SL policy network                       RL policy Network          Value Network Here is some more detail about AlphaGo\u2019s ANNs and their training.\n\nThe identicallystructured SL and RL policy networks were similar to DQN\u2019s deep convolutional network described in Section 16.5 for playing Atari games, except that they had 13 convolutional layers with the \ufb01nal layer consisting of a soft-max unit for each point on the 19 \u21e5 19 Go board. The networks\u2019 input was a 19 \u21e5 19 \u21e5 48 image stack in which each point on the Go board was represented by the values of 48 binary or integer-valued features. For example, for each point, one feature indicated if the point was occupied by one of AlphaGo\u2019s stones, one of its opponent\u2019s stones, or was unoccupied, thus providing the \u201craw\u201d representation of the board con\ufb01guration", "9c6caaac-c3f7-42bc-a0a1-7b7e7f16229e": "It is sometimes called the theory of multistage decision processes, or sequential decision processes, and has roots in the statistical literature on sequential sampling beginning with the papers by Thompson  and Robbins  that we cited in Chapter 2 in connection with bandit problems (which are prototypical MDPs if formulated as multiple-situation problems).\n\nThe earliest instance of which we are aware in which reinforcement learning was discussed using the MDP formalism is Andreae\u2019s  description of a uni\ufb01ed view of learning machines. Witten and Corbin  experimented with a reinforcement learning system later analyzed by Witten  using the MDP formalism. Although he did not explicitly mention MDPs, Werbos  suggested approximate solution methods for stochastic optimal control problems that are related to modern reinforcement learning methods . Although Werbos\u2019s ideas were not widely recognized at the time, they were prescient in emphasizing the importance of approximately solving optimal control problems in a variety of domains, including arti\ufb01cial intelligence. The most in\ufb02uential integration of reinforcement learning and MDPs is due to Watkins . 3.1 Our characterization of the dynamics of an MDP in terms of p(s0, r|s, a) is slightly unusual", "3216f756-e46e-4bc1-8ffa-a6c0ca67fafd": "Let us use the word history, and the notation Ht, for an initial portion of the trajectory up to an observation: Ht .= A0, O1, . , At\u22121, Ot.\n\nThe history represents the most that we can know about the past without looking outside of the data stream (because the history is the whole past data stream). Of course, the history grows with t and can become large and unwieldy. The idea of state is that of some compact summary of the history that is as useful as the actual history for predicting the future. Let us be clear about exactly what this means. To be a summary of the history, the state must be a function of history, St = f(Ht), and to be as useful for predicting the future as the whole history, it must have what is known as the Markov property. Formally, this is a property of the function f. A function f has the Markov property if and only if any two histories h and h0 that are mapped by f to the same state (f(h)=f(h0)) also have the same probabilities for their next observation, for all o 2 O and a 2 A", "975cb3c0-6c3a-4224-ab34-0760cb0fdf07": "It is called layer-wise because these independent pieces are the layers of the network. Specifically, greedy layer-wise pretraining proceeds one layer at a time, training the k-th layer while keeping the previous ones fixed. In particular, the lower layers (which are trained first) are not adapted after the upper layers are introduced.\n\nIt is called unsuper- vised because each layer is trained with an unsupervised representation learning algorithm. However, it is also called pretraining because it is supposed to be only a first step before a joint training algorithm is applied to fine-tune all the layers together. In the context of a supervised learning task, it can be viewed as a regularizer (in some experiments, pretraining decreases test error without decreasing training error) and a form of parameter initialization. It is common to use the word \u201cpretraining\u201d to refer not only to the pretraining stage itself but to the entire two-phase protocol that combines the pretraining phase and a supervised learning phase. The supervised learning phase may involve training a simple classifier on top of the features learned in the pretraining phase, or it may involve supervised fine-tuning of the entire network learned in the pretraining phase", "15edf233-3c0c-4119-8bca-1cc7060107c5": "typically have in\ufb01nite variance, and thus unsatisfactory convergence properties, whenever the scaled returns have in\ufb01nite variance\u2014and this can easily happen in o\u21b5-policy learning when trajectories contain loops. A simple example is shown inset in Figure 5.4. There is only one nonterminal state s and two actions, right and left. The right action causes a deterministic transition to termination, whereas the left action transitions, with probability 0.9, back to s or, with probability 0.1, on to termination. The rewards are +1 on the latter transition and otherwise zero. Consider the target policy that always selects left. All episodes under this policy consist of some number (possibly zero) of transitions back to s followed by termination with a reward and return of +1. Thus the value of s under the target policy is 1 (\u03b3 = 1). Suppose we are estimating this value from o\u21b5-policy data using the behavior policy that selects right and left with equal probability. The lower part of Figure 5.4 shows ten independent runs of the \ufb01rst-visit MC algorithm using ordinary importance sampling. Even after millions of episodes, the estimates fail to converge to the correct value of 1", "b0cc17cd-389a-400c-80b4-c5bfbdf29638": "To enforce this constraint, we introduce a Lagrange multiplier that we shall denote by AI, and then make an unconstrained maximization of By setting the derivative with respect to Ul equal to zero, we see that this quantity will have a stationary point when which says that Ul must be an eigenvector of S. If we left-multiply by uf and make use of ufUl = 1, we see that the variance is given by and so the variance will be a maximum when we set Ul equal to the eigenvector having the largest eigenvalue AI. This eigenvector is known as the first principal component.\n\nWe can define additional principal components in an incremental fashion by choosing each new direction to be that which maximizes the projected variance amongst all possible directions orthogonal to those already considered. If we consider the general case of an M -dimensional projection space, the optimal linear projection for which the variance of the projected data is maximized is now defined by the M eigenvectors U 1, ... , U M of the data covariance matrix S corresponding to the M largest eigenvalues >'1, ... ,AM. This is easily shown using proof by induction", "3fbdb38d-a382-44e0-9d68-38ed03ccdb46": "The problem can be resolved by modelling state duration directly in which the diagonal coef\ufb01cients Akk are all set to zero, and each state k is explicitly associated with a probability distribution p(T|k) of possible duration times.\n\nFrom a generative point of view, when a state k is entered, a value T representing the number of time steps that the system will remain in state k is then drawn from p(T|k). The model then emits T values of the observed variable xt, which are generally assumed to be independent so that the corresponding emission density is simply \ufffdT t=1 p(xt|k). This approach requires some straightforward modi\ufb01cations to the EM optimization procedure . Another limitation of the standard HMM is that it is poor at capturing longrange correlations between the observed variables (i.e., between variables that are separated by many time steps) because these must be mediated via the \ufb01rst-order Markov chain of hidden states. Longer-range effects could in principle be included by adding extra links to the graphical model of Figure 13.5. One way to address this is to generalize the HMM to give the autoregressive hidden Markov model , an example of which is shown in Figure 13.17", "b9114f8d-36fa-469b-a5e7-aa0ef0d2ea2b": "Moreover, they o\u21b5er important advantages that make them often clearly preferred. One reason for this is that they typically enable signi\ufb01cantly faster learning, as we have seen in Chapters 6 and 7. Another is that they enable learning to be continual and online, without waiting for the end of an episode. This enables them to be used on continuing problems and provides computational advantages. A prototypical semi-gradient method is semi-gradient TD(0), which uses Ut .= Rt+1 + \u03b3\u02c6v(St+1,w) as its target. Complete pseudocode for this method is given in the box below. Input: the policy \u21e1 to be evaluated Input: a di\u21b5erentiable function \u02c6v : S+ \u21e5 Rd !\n\nR such that \u02c6v(terminal,\u00b7) = 0 Algorithm parameter: step size \u21b5 > 0 Initialize value-function weights w 2 Rd arbitrarily (e.g., w = 0) State aggregation is a simple form of generalizing function approximation in which states are grouped together, with one estimated value (one component of the weight vector w) for each group", "2970f08f-d597-4113-813e-df3940a8b6d9": "We can use this freedom to set This is known as the canonical representation of the decision hyperplane. In the case of data points for which the equality holds, the constraints are said to be active, whereas for the remainder they are said to be inactive. By de\ufb01nition, there will always be at least one active constraint, because there will always be a closest point, and once the margin has been maximized there will be at least two active constraints. The optimization problem then simply requires that we maximize \u2225w\u2225\u22121, which is equivalent to minimizing \u2225w\u22252, and so we have to solve the optimization problem subject to the constraints given by (7.5). The factor of 1/2 in (7.6) is included for later convenience. This is an example of a quadratic programming problem in which we are trying to minimize a quadratic function subject to a set of linear inequality constraints. It appears that the bias parameter b has disappeared from the optimization.\n\nHowever, it is determined implicitly via the constraints, because these require that changes to \u2225w\u2225 be compensated by changes to b. We shall see how this works shortly", "278abfe6-cf24-4ebf-b580-c11472ba2ee3": "Many algorithms that operate on probabilistic models need to compute not Pmodel (@) but only log Pmodel(#).\n\nFor energy-based models with latent variables h, these algorithms are sometimes phrased in terms of the negative of this quantity, called the free energy:  F(a) = \u2014 log Srexp (\u2014E(a,h)). (16.8)  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    In this book, we usually prefer the more general log p\u2122\u00b04el(a) formulation. 568  CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  O-O-L) (a) (b)  Figure 16.6: (a) The path between random variablea and random variable b through s is active, because s is not observed. This means that a and b are not separated. (b) Heres is shaded in, to indicate that it is observed. Because the only path betweena and b is through s, and that path is inactive, we can conclude that a and b are separated given s", "9cbd3515-6686-46d9-a8b0-bbba0e228e86": "At each point in time, the  pose of the character is described by a specification of the angles of each of the joints in the charac- ter\u2019s skeleton. Each channel in the data we feed to the convolu- tional model represents the angle about one axis of one joint.\n\n2-D | Audio data that has been prepro- | Color image data: One channel  cessed with a Fourier transform: We can transform the audio wave- form into a 2-D tensor with dif- ferent rows corresponding to dif- ferent frequencies and different columns corresponding to differ- ent points in time. Using convolu- tion in the time makes the model equivariant to shifts in time. Us- ing convolution across the fre- quency axis makes the model  https://www.deeplearningbook.org/contents/convnets.html  contains the red pixels, one the green pixels, and one the blue pixels. moves over both the horizontal  The convolution kernel  and the vertical axes of the im- age, conferring translation equiv- ariance in both directions", "8c8487de-3e65-4824-bb51-37b8acac3553": "At large internet companies with millions or billions of users, it is feasible to gather large datasets, and the expense of doing so can be considerably less than that of the alternatives, so the answer is almost always to gather more training data. For example, the development of large labeled datasets was one of the most important factors in solving object recognition. In other contexts, such as medical applications, it may be costly or infeasible to gather more data. A simple alternative to gathering more data is to reduce the size of the model or improve regularization, by adjusting hyperparameters such as weight decay coefficients, or by adding regularization strategies such as dropout. If you find that the gap between train and test performance is still unacceptable even after tuning the regularization hyperparameters, then gathering more data is advisable. VAT AR Anat AL.\n\nmw Athan 4A wnthanwn w.nnn Antena 14 tn AlAR HAnnnnnwe-e 4A ARABS AR  https://www.deeplearningbook.org/contents/guidelines.html    VV LICiL UsUlULLLY WHEEL Lu ALCL LLUOLS uaa, iu is albU HEUTSSal", "c6d57896-b26c-4bb4-8df0-814c38b93ec8": "Note that new test data must be pre-processed using the same steps as the training data. Pre-processing might also be performed in order to speed up computation.\n\nFor example, if the goal is real-time face detection in a high-resolution video stream, the computer must handle huge numbers of pixels per second, and presenting these directly to a complex pattern recognition algorithm may be computationally infeasible. Instead, the aim is to \ufb01nd useful features that are fast to compute, and yet that also preserve useful discriminatory information enabling faces to be distinguished from non-faces. These features are then used as the inputs to the pattern recognition algorithm. For instance, the average value of the image intensity over a rectangular subregion can be evaluated extremely ef\ufb01ciently , and a set of such features can prove very effective in fast face detection. Because the number of such features is smaller than the number of pixels, this kind of pre-processing represents a form of dimensionality reduction. Care must be taken during pre-processing because often information is discarded, and if this information is important to the solution of the problem then the overall accuracy of the system can suffer", "f0d20a28-8b3f-4bc9-ab38-bcc9c742783c": "We can represent the emission probabilities in the form We shall focuss attention on homogeneous models for which all of the conditional distributions governing the latent variables share the same parameters A, and similarly all of the emission distributions share the same parameters \u03c6 (the extension to more general cases is straightforward). Note that a mixture model for an i.i.d. data set corresponds to the special case in which the parameters Ajk are the same for all values of j, so that the conditional distribution p(zn|zn\u22121) is independent of zn\u22121. This corresponds to deleting the horizontal links in the graphical model shown in Figure 13.5. The joint probability distribution over both latent and observed variables is then where X = {x1, . , xN}, Z = {z1, . , zN}, and \u03b8 = {\u03c0, A, \u03c6} denotes the set of parameters governing the model.\n\nMost of our discussion of the hidden Markov model will be independent of the particular choice of the emission probabilities. Indeed, the model is tractable for a wide range of emission distributions including discrete tables, Gaussians, and mixtures of Gaussians", "640c83b4-ee2b-4157-b760-f0f73fc8e9a5": "599  https://www.deeplearningbook.org/contents/monte_carlo.html    CHAPTER 17.\n\nMONTE CARLO METHODS  17.5.1 Tempering to Mix between Modes  When a distribution has sharp peaks of high probability surrounded by regions of low probability, it is difficult to mix between the different modes of the distribution. Several techniques for faster mixing are based on constructing alternative versions of the target distribution in which the peaks are not as high and the surrounding valleys are not as low. Energy-based models provide a particularly simple way to do so. So far, we have described an energy-based model as defining a probability distribution  p(x) \u00ab exp (\u2014E(a)). (17.25)  Energy-based models may be augmented with an extra parameter \u00a7 controlling how sharply peaked the distribution is:  Pa (@) x exp (\u2014BE(@)). (17.26)  The ( parameter is often described as being the reciprocal of the temperature, reflecting the origin of energy-based models in statistical physics", "9bb81c59-351b-4abd-bcc0-9509db90d9f8": "We can often use En~p(h|v)  as a set of features to describe v.  585  CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  Overall, the RBM demonstrates the typical deep learning approach to graph- ical models: representation learning accomplished via layers of latent variables, combined with efficient interactions between layers parametrized by matrices. The language of graphical models provides an elegant, flexible and clear language for describing probabilistic models. In the chapters ahead, we use this language, among other perspectives, to describe a wide variety of deep probabilistic models.\n\nfile:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    586  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html", "9c83b2e4-1aa3-40c8-b65c-ac87196cd124": "In the case of i.i.d. data, we implicitly circumvented this problem with the evaluation of likelihood functions by taking logarithms. Unfortunately, this will not help here because we are forming sums of products of small numbers (we are in fact implicitly summing over all possible paths through the lattice diagram of Figure 13.7). We therefore work with re-scaled versions of \u03b1(zn) and \u03b2(zn) whose values remain of order unity. As we shall see, the corresponding scaling factors cancel out when we use these re-scaled quantities in the EM algorithm. In (13.34), we de\ufb01ned \u03b1(zn) = p(x1, . , xn, zn) representing the joint distribution of all the observations up to xn and the latent variable zn.\n\nNow we de\ufb01ne a normalized version of \u03b1 given by which we expect to be well behaved numerically because it is a probability distribution over K variables for any value of n. In order to relate the scaled and original alpha variables, we introduce scaling factors de\ufb01ned by conditional distributions over the observed variables cn = p(xn|x1,", "6bcc2b6f-bc13-4d77-bf4c-1db5bba84e7d": "Although the identity (9.22), known as the Sherman-Morrison formula, is super\ufb01cially complicated, it involves only vector-matrix and vector-vector multiplications and thus is only O(d2). Thus we can store the inverse matrix bA\u22121 maintain it with (9.22), and then use it in (9.21), all with only O(d2) memory and per-step computation. The complete algorithm is given in the box on the next page. Of course, O(d2) is still signi\ufb01cantly more expensive than the O(d) of semi-gradient TD.\n\nWhether the greater data e\ufb03ciency of LSTD is worth this computational expense depends on how large d is, how important it is to learn quickly, and the expense of other parts of the system. The fact that LSTD requires no step-size parameter is sometimes also touted, but the advantage of this is probably overstated. LSTD does not require a step size, but it does requires \"; if \" is chosen too small the sequence of inverses can vary wildly, and if \" is chosen too large then learning is slowed. In addition, LSTD\u2019s lack of a step-size parameter means that it never forgets", "82a1fd02-6148-4f00-8077-b468c08f304e": "Gr\u00a8unwald Introduction to Statistical Relational Learning, Lise Getoor and Ben Taskar, Eds.\n\nIntroduction to Machine Learning, second edition, Ethem Alpaydin Machine Learning in Non-Stationary Environments: Introduction to Covariate Shift Adaptation, Masashi Sugiyama and Motoaki Kawanabe Boosting: Foundations and Algorithms, Robert E. Schapire and Yoav Freund Machine Learning: A Probabilistic Perspective, Kevin P. Murphy Foundations of Machine Learning, Mehryar Mohri, Afshin Rostami, and Ameet Talwalker Introduction to Machine Learning, third edition, Ethem Alpaydin Deep Learning, Ian Goodfellow, Yoshua Bengio, and Aaron Courville Elements of Causal Inference, Jonas Peters, Dominik Janzing, and Bernhard Sch\u00a8olkopf Machine Learning for Data Streams, with Practical Examples in MOA, Albert Bifet, Ricard Gavald`a, Geo\u21b5rey Holmes, Bernhard Pfahringer", "9a6a6e97-ca70-49b4-beb1-db4cd5d7b39d": "We are free to choose the {Ui}, the {Zni}, and the {bd so as to minimize the distortion introduced by the reduction in dimensionality.\n\nAs our distortion measure, we shall use the squared distance between the original data point X n and its approximation Xn , averaged over the data set, so that our goal is to minimize N Consider first of all the minimization with respect to the quantities {Zni}. Substituting for Xn , setting the derivative with respect to Znj to zero, and making use of the orthonormality conditions, we obtain where j = 1, ... ,M. Similarly, setting the derivative of J with respect to bi to zero, and again making use of the orthonormality relations, gives where j = M +1, ... ,D. If we substitute for Zni and bi , and make use of the general expansion (12.9), we obtain from which we see that the displacement vector from X n to xn lies in the space orthogonal to the principal subspace, because it is a linear combination of {ud for i = M + 1, ..", "ee0304cb-d23b-4ee7-8fd9-661568f0f389": "The left panel of Figure 9.2 shows the \ufb01nal value function learned by the semi-gradient TD(0) algorithm (page 203) using the same state aggregation as in Example 9.1. We see that the nearasymptotic TD approximation is indeed farther from the true values than the Monte Carlo approximation shown in Figure 9.1. Nevertheless, TD methods retain large potential advantages in learning rate, and generalize Monte Carlo methods, as we investigated fully with n-step TD methods in Chapter 7.\n\nThe right panel of Figure 9.2 shows results with an n-step semi-gradient TD method using state aggregation on the 1000-state random walk that are strikingly similar to those we obtained earlier with tabular methods and the 19-state random walk (Figure 7.2). To obtain such quantitatively similar results we switched the state aggregation to 20 groups of 50 states each. The 20 groups were then quantitatively close to the 19 states of the tabular problem. In particular, recall that state transitions were up to 100 states to the left or right. A typical transition would then be of 50 states to the right or left, which is quantitatively analogous to the single-state state transitions of the 19-state tabular system", "9695c4ae-5d18-4620-896c-d261044d3302": "Fiorillo, Yun, and Song , Lammel, Lim, and Malenka , and Saddoris, Cacciapaglia, Wightmman, and Carelli  are among studies showing that the signaling properties of dopamine neurons are specialized for di\u21b5erent target regions. RPEsignaling neurons may belong to one among multiple populations of dopamine neurons having di\u21b5erent targets and subserving di\u21b5erent functions.\n\nEshel, Tian, Bukwich, and Uchida  found homogeneity of reward prediction error responses of dopamine neurons in the lateral VTA during classical conditioning in mice, though their results do not rule out response diversity across wider areas. Gershman, Pesaran, and Daw  studied reinforcement learning tasks that can be decomposed into independent components with separate reward signals, \ufb01nding evidence in human neuroimaging data suggesting that the brain exploits this kind of structure. 15.5 Schultz\u2019s 1998 survey article is a good entr\u00b4ee into the very extensive literature on reward predicting signaling of dopamine neurons", "07dd7e2c-0343-46ab-863c-af84d50465bb": "After estimating the parameters, it is also possible to sample from such a distribution very efficiently. Importance sampling is not only useful for speeding up models with large softmax outputs.\n\nMore generally, it is useful for accelerating training with large sparse output layers, where the output is a sparse vector rather than a 1-of-n choice. An example is a bag of words. A bag of words is a sparse vector v where v; indicates the presence or absence of word i from the vocabulary in the document. Alternately, v; can indicate the number of times that word i appears. Machine learning models that emit such sparse vectors can be expensive to train for a variety of reasons. Early in learning, the model may not actually choose to make the output truly sparse. Moreover, the loss function we use for training might most naturally be described in terms of comparing every element of the output to every element of the target", "85ac3bf5-a974-47ef-9825-39856cb6d183": "The latter is similar, except that it uses spiking neurons (with binary outputs) instead of the continuous- valued hidden units used for ESNs. Both ESNs and liquid state machines are ermed reservoir computing  to denote the fact hat the hidden units form a reservoir of temporal features that may capture different aspects of the history of inputs. One way to think about these reservoir computing recurrent networks is that hey are similar to kernel machines: they map an arbitrary length sequence (the history of inputs up to time t) into a fixed-length vector (the recurrent state h\u00ae), on which a linear predictor (typically a linear regression) can be applied to solve che problem of interest. The training criterion may then be easily designed to be convex as a function of the output weights. For example, if the output consists  of linear regression from the hidden units to the output targets, and the training criterion is mean squared error, then it is convex and may be solved reliably with simple learning algorithms", "6c4c6b4c-bce7-46da-83eb-1d5c036da8f9": "Another way to achieve the same result is to multiply the states of the units by 2 during training. Either way, the goal is to make sure that the expected total input to a unit at test time is roughly the same as the expected total input to that unit at train time, even though half the units at train time are missing on average. For many classes of models that do not have nonlinear hidden units, the weight scaling inference rule is exact. For a simple example, consider a softmax regression classifier with n input variables represented by the vector v:  P(y =y |v) = softmax (wiv + b)", "da34b324-0f48-4bad-bbec-7e804fbd31e8": "Learner M with parameters 0, Meta-Learner R with parameters O.\n\n1: Og < random initialization 2: 3: ford = 1,ndo  4: Drain; Diest <\u2014 random dataset from Ypeta\u2014train 5: 49 <\u2014 co > Intialize learner parameters 6: 7: fort = 1,7 do 8: X+, Y; < random batch from D\u00a2;ain 9: Ly \u2014 L(M (Xi; 61-1), Ye) > Get loss of learner on train batch 10: cr \u2014 R((Vo,_, \u00a34, \u00a31); Oa-1) > Get output of meta-learner using Equation 2 11: 0: \u2014 Ct > Update learner parameters 12: end for 13: 14: X,Y \u00a9 Deest 15: Liest <\u2014 L(M(X; Or), Y) > Get loss of learner on test batch 16: Update O04 using Vo,_, Ltest > Update meta-learner parameters 17: 18: end for MAML  MAML, short for Model-Agnostic Meta-Learning  is a fairly general optimization algorithm, compatible with any model that learns through gradient descent. Let's say our model is fg with parameters 0", "ffaa1953-48da-4354-b9d0-53e732e6d2c5": "If a doctor analyzes a patient and says that the patient has a 40 percent chance of having the flu, this means something very different\u2014we cannot make infinitely many replicas of the patient, nor is there any reason to believe that different replicas of the patient would present with the same symptoms yet have varying underlying conditions. In the case of the doctor diagnosing the patient, we use probability to represent a degree of belief, with 1 indicating absolute certainty that the patient has the flu and 0 indicating absolute certainty that the patient does not have the flu", "471bc360-0e70-47db-afec-46356cdb70f0": "We can therefore make use of the EM algorithm to find maximum likelihood estimates of the model parameters.\n\nThis may seem rather pointless because we have already obtained an exact closed-form solution for the maximum likelihood parameter values. However, in spaces of high dimensionality, there may be computational advantages in using an iterative EM procedure rather than working directly with the sample covariance matrix. This EM procedure can also be extended to the factor analysis model, for which there is no closed-form solution. Finally, it allows missing data to be handled in a principled way. We can derive the EM algorithm for probabilistic PCA by following the general framework for EM. Thus we write down the complete-data log likelihood and take its expectation with respect to the posterior distribution of the latent distribution evaluated using 'old' parameter values. Maximization of this expected completedata log likelihood then yields the 'new' parameter values. Because the data points are assumed independent, the complete-data log likelihood function takes the form where the nth row of the matrix Z is given by Zn", "2b051375-b072-4c17-8d1f-ff2564514a2a": "Set the step size (e) and tolerance (6) to small, positive numbers. while ||A\u2019 Ax \u2014 A'b||2 > 6 do  xeca\u2014e A\u2019Ar\u2014Alb end while  One can alsd solve this problem using Newton\u2019s method. In this case, because the true function is quadratic, the quadratic approximation employed by Newton\u2019s method is exact, and the algorithm converges to the global minimum in a single  https://www.deeplearningbook.org/contents/numerical.html   SLEp. Now suppose we wish to minimize the same function, but subject to the constraint \u00ab \u00abx <1. To do so, we introduce the Lagrangian  L(a,) = f(a) +2 (22 - 1) (4.23) We can now solve the problem  L(2x,2). 4.24 min max (a, A) (4.24)  The smallest-norm solution to the unconstrained least-squares problem may be found using the Moore-Penrose pseudoinverse: x = A*b.\n\nIf this point is feasible, then it is the solution to the constrained problem. Otherwise, we must find a  94  CHAPTER 4", "cd1eea9f-4a0c-4cba-a943-236d2aadc9bf": "However, the Cram\u00e9r-Rao bound  states that generalization error cannot decrease faster than Of ).\n\nBottou and Bousquet  argue that it therefore may not be worthwhile to pursue an optimization algorithm that converges faster than O(Z) for machine learning tasks\u2014faster convergence presumably corresponds to overfitting. Moreover, the asymptotic analysis obscures many advantages that stochastic gradient descent has after a small number of steps. With large datasets, the ability of SGD to make rapid initial progress while evaluating the gradient for very few examples outweighs its slow asymptotic convergence. Most of the algorithms described in the remainder of this chapter achieve benefits that matter in practice but are lost in the constant factors obscured by the OF) asymptotic analysis. One can also trade off the benefits of both batch and stochastic gradient descent by gradually increasing the minibatch size during the course of learning. For more information on SGD, see Bottou . 8.3.2 Momentum While stochastic gradient descent remains a popular optimization strategy, learning with it can sometimes be slow. The method of momentum  is designed to accelerate learning, especially in the face of high curvature, small but  consistent gradients, or noisy gradients", "47fdf3d6-bbcf-4b04-b0f6-b39c6211c406": "In order to turn this basic idea into an algorithm we can implement, the first  https://www.deeplearningbook.org/contents/linear_algebra.html  thing we need to do is figure out how to generate the optimal code point \u00a2\u201d for each input point Z. One way to do this is to minimize the distance between the input point \u00a3 and its reconstruction, gee ). We can measure this distance using a norm.\n\nIn the principal components algorithm, we use the L? norm:  c* = arg min||x \u2014 g(c)||2. (2.54) c  We can switch to the squared L? norm instead of using the L? norm itself because both are minimized by the same value of c. Both are minimized by the same value of c because the L? norm is non-negative and the squaring operation is monotonically increasing for non-negative arguments. c= arg min || \u2014 g(c)||3- (2.55) The function being minimized simplifies to (a \u2014 g(c))'(@\u2014 g(e)) (2.56) 46 CHAPTER 2", "f56f4d7d-3ad3-4c8f-a471-9469a163c088": "We have enforced this via rewards and by discounting or giving a \ufb01xed value to the terminal state.\n\nBut Samuel\u2019s method included no rewards and no special treatment of the terminal positions of games. As Samuel himself pointed out, his value function could have become consistent merely by giving a constant value to all positions. He hoped to discourage such solutions by giving his piece-advantage term a large, nonmodi\ufb01able weight. But although this may decrease the likelihood of \ufb01nding useless evaluation functions, it does not prohibit them. For example, a constant function could still be attained by setting the modi\ufb01able weights so as to cancel the e\u21b5ect of the nonmodi\ufb01able one. Because Samuel\u2019s learning procedure was not constrained to \ufb01nd useful evaluation functions, it should have been possible for it to become worse with experience. In fact, Samuel reported observing this during extensive self-play training sessions. To get the program improving again, Samuel had to intervene and set the weight with the largest absolute value back to zero. His interpretation was that this drastic intervention jarred the program out of local optima, but another possibility is that it jarred the program out of evaluation functions that were consistent but had little to do with winning or losing the game", "5e7f6cdb-f55d-4248-9d8f-011ef16aad14": "But what happens as a grows? Because H is real and symmetric, we can decompose it into a diagonal matrix A and an orthonormal basis of eigenvectors, Q, such that  mo Mant \u00ab2 a. ood se . i man 1  https://www.deeplearningbook.org/contents/regularization.html    4h \u2014 YAW . Applying the decomposition to equation /.1U, we Obtain  w = (QAQ' +al) 'QAQ'w* (7.11) -1  = [Qa + a1Q\" | QAQ' w* (7.12)  =Q(A+al) AQ! wu. (7.13)  We see that the effect of weight decay is to rescale w* along the axes defined by the eigenvectors of H. Specifically, the component of w* that is aligned with the i-th eigenvector of H is rescaled by a factor of sh", "1137e872-07c5-41c2-bef6-189470ef87fa": "A single attenuation measurement alone is not suf\ufb01cient because there are two degrees of freedom corresponding to the fraction of oil and the fraction of water (the fraction of gas is redundant because the three fractions must add to one). To address this, two gamma beams of different energies (in other words different frequencies or wavelengths) are passed through the pipe along the same path, and the attenuation of each is measured. Because the absorbtion properties of different materials vary differently as a function of energy, measurement of the attenuations at the two energies provides two independent pieces of information. Given the known absorbtion properties of oil, water, and gas at the two energies, it is then a simple matter to calculate the average fractions of oil and water (and hence of gas) measured along the path of the gamma beams. There is a further complication, however, associated with the motion of the materials along the pipe. If the \ufb02ow velocity is small, then the oil \ufb02oats on top of the water with the gas sitting above the oil. This is known as a laminar or strati\ufb01ed \ufb02ow con\ufb01guration and is illustrated in Figure A.2.\n\nAs the \ufb02ow velocity is increased, more complex geometrical con\ufb01gurations of the oil, water, and gas can arise", "ded2785b-22fd-410f-823e-bc0b9420e9dd": "Specifically, it means that the probability of one unit being on is given by a linear model (logistic regression) from the values of che other units. The Boltzmann machine becomes more powerful when not all the variables are observed. In this case, the latent variables can act similarly to hidden units in a multilayer perceptron and model higher-order interactions among the visible units.\n\nJust as the addition of hidden units to convert logistic regression into an MLP results in the MLP being a universal approximator of functions, a Boltzmann machine with hidden units is no longer limited to modeling linear relationships between variables. Instead, the Boltzmann machine becomes a universal approximator of probability mass functions over discrete variables . Formally, we decompose the units a into two subsets: the visible units v and the latent (or hidden) units h. The energy function becomes  E(u, h) v' Rv\u2014v'Wh\u2014-h'Sh\u2014b'v\u2014c'h. (20.3)  Boltzmann Machine Learning Learning algorithms for Boltzmann machines are usually based on maximum likelihood", "7c82ce67-3f39-4d64-b639-805c810a1a51": "Using Bayes\u2019 theorem, we have where {z(l) n } is a set of samples drawn from p(zn|Xn\u22121) and we have made use of the conditional independence property p(xn|zn, Xn\u22121) = p(xn|zn), which follows from the graph in Figure 13.5. The sampling weights {w(l) n } are de\ufb01ned by l w(l) n = 1. Because we wish to \ufb01nd a sequential sampling scheme, we shall suppose that a set of samples and weights have been obtained at time step n, and that we have subsequently observed the value of xn+1, and we wish to \ufb01nd the weights and samples at time step n + 1. We \ufb01rst sample from the distribution p(zn+1|Xn).\n\nThis is straightforward since, again using Bayes\u2019 theorem where we have made use of the conditional independence properties which follow from the application of the d-separation criterion to the graph in Figure 13.5", "0f721ca9-2417-4613-bde6-5ec18b6e30f6": "Prior to the advent of deep learning, the main way to learn nonlinear models  https://www.deeplearningbook.org/contents/ml.html    was to use the kernel trick in combination with a linear mq el; Mapy. kernel learning algorithms require constructing an \u2122 x 7 matrix Gij = k(v@\u2019\u2019, 2\"). Constructing this matrix has computational cost O(m ), which is clearly undesirable for datasets with billions of examples. In academia, starting in 2006, deep learning was initially interesting because it was able to generalize to new examples better than competing algorithms when trained on medium-sized datasets with tens of thousands of examples. Soon after, deep learning garnered additional interest in industry because it provided a scalable way of training nonlinear models on large datasets. Stochastic gradient descent and many enhancements to it are described further in chapter 8. 150  CHAPTER 5", "f77feae7-b1f9-4766-bf9a-ae1a0c525ed5": "In 2018 IEEE Symposium Series on Computational Intelligence (SSCI), pages 1542\u20131547.\n\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353\u2013355, Brussels, Belgium. Association for Computational Linguistics. Yaqing Wang, Quanming Yao, James T. Kwok, and Lionel M. Ni. 2020. Generalizing from a few examples. ACM Computing Surveys, 53(3):1\u201334. Yicheng Wang and Mohit Bansal. 2018. Robust machine comprehension models via adversarial training. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 575\u2013581, New Orleans, Louisiana. Association for Computational Linguistics", "7f7e1908-0707-400c-a397-f6679ea5232a": "Models with high capacity can overfit by memorizing properties of the training set that do not serve them well on the test set. One way to control the capacity of a learning algorithm is by choosing its hypothesis space, the set of functions that the learning algorithm is allowed to select as being the solution. For example, the linear regression algorithm has the set of all linear functions of its input as its hypothesis space. We can generalize linear regression to include polynomials, rather than just linear functions, in its hypothesis space. Doing so increases the model\u2019s capacity. A polynomial of degree 1 gives us the linear regression model with which we are already familiar, with the prediction  =b+wz. (5.15)  &  By introducing 2? as another feature provided to the linear regression model, we can learn a model that is quadratic as a function of z:  g = b+ wy t+ wor.\n\n(5.16)  Though this model implements a quadratic function of its input, the output is still a linear function of the parameters, so we can still use the normal equations to train the model in closed form", "e666e034-c4ce-48ea-9efa-9f43f095882a": "The use of probability to represent uncertainty, however, is not an ad-hoc choice, but is inevitable if we are to respect common sense while making rational coherent inferences.\n\nFor instance, Cox  showed that if numerical values are used to represent degrees of belief, then a simple set of axioms encoding common sense properties of such beliefs leads uniquely to a set of rules for manipulating degrees of belief that are equivalent to the sum and product rules of probability. This provided the \ufb01rst rigorous proof that probability theory could be regarded as an extension of Boolean logic to situations involving uncertainty . Numerous other authors have proposed different sets of properties or axioms that such measures of uncertainty should satisfy . In each case, the resulting numerical quantities behave precisely according to the rules of probability. It is therefore natural to refer to these quantities as (Bayesian) probabilities. In the \ufb01eld of pattern recognition, too, it is helpful to have a more general noThomas Bayes was born in Tunbridge Wells and was a clergyman as well as an amateur scientist and a mathematician. He studied logic and theology at Edinburgh University and was elected Fellow of the Royal Society in 1742", "05466c2d-2b73-4928-a7a4-eb20236be30f": "The probability assigned to the most likely class thus gives an estimate of the confidence the model has in its classification decision. Typically, maximum likelihood training results in these values being overestimates rather than accurate probabilities of correct prediction, but they are somewhat useful in the sense that examples that are actually less likely  https://www.deeplearningbook.org/contents/guidelines.html    to be correctly labeled receive smaller probabilities under the model. By viewing the training set examples that are the hardest to model correctly, one can often discover problems with the way the data have been preprocessed or labeled. For  example, the Street View transcription system originally had a problem where the address number detection system would crop the image too tightly and omit some digits. The transcription network then assigned very low probability to the correct answer on these images.\n\nSorting the images to identify the most confident mistakes showed that there was a systematic problem with the cropping. Modifying the detection system to crop much wider images resulted in much better performance of the overall system, even though the transcription network needed to be able to process greater variation in the position and scale of the address numbers. Reason about software using training and test error: It is often difficult to  432  CHAPTER 11", "997fc84d-6d4b-45bf-82ab-8c6e69614599": "For example, we could take a neural network and modify layer 1 by swapping the incoming weight vector for unit 7 with the incoming weight vector for unit 7, then do the same for the outgoing weight vectors. If we have m layers with n units each, then there are n! \"\u201d ways of arranging the hidden units. This kind of nonidentifiability is known as weight space symmetry. In addition to weight space symmetry, many kinds of neural networks have additional causes of nonidentifiability. For example, in any rectified linear or maxout network, we can scale all the incoming weights and biases of a unit by a if we also scale all its outgoing weights by 4.\n\nThis means that\u2014if the cost function does not include terms such as weight decay that depend directly on the weights rather than the models\u2019 outputs\u2014every local minimum of a rectified linear or maxout network lies on an (m x n)-dimensional hyperbola of equivalent local minima. These model identifiability issues mean that a neural network cost function can have an extremely large or even uncountably infinite amount of local minima", "bfe1fdf0-c72d-4300-9dd1-e87bdd8dbb74": "Advances in Neural Information Processing Systems, 29. Wilson, A. G., Hu, Z., Salakhutdinov, R., & Xing, E. P. Deep kernel learning. Arti\ufb01cial intelligence and statistics, 370\u2013378. Wu, Y. N., Gao, R., Han, T., & Zhu, S.-C. A tale of three probabilistic families: Discriminative, descriptive, and generative models. Quarterly of Applied Mathematics, 77(2), 423\u2013 465. Wu, Y., Zhou, P., Wilson, A. G., Xing, E., & Hu, Z. .\n\nImproving GAN training with probability ratio clipping and sample reweighting. Advances in Neural Information Processing Systems, 33, 5729\u20135740. Xing, E. P., Jordan, M. I., & Russell, S. A generalized mean \ufb01eld algorithm for variational inference in exponential families", "26ef057a-25f6-4467-b10c-8ca2f8e83841": "If we included these terms, we would have a linear factor model instead of a restricted Boltzmann machine. When designing our Boltzmann machine, we simply omit these h;h; cross terms. Omitting them does not change the conditional p(v | h), so equation 20.39 is still respected. We still have a choice, however, about whether to include the terms involving only a single h;. If we assume a diagonal precision matrix, we find that for each hidden unit h;, we have a term  1 ait S> 8; W7i- (20.41) J  In the above, we used the fact that h? = h; because h; \u20ac {0,1}. If we include this term (with its sign flipped) in the energy function, then it will naturally bias h; to be turned off when the weights for that unit are large and connected to visible units with high precision.\n\nThe choice of whether to include this bias term does not affect the family of distributions that the model can represent (assuming that we include bias parameters for the hidden units), but it does affect the learning dynamics of the model", "fc4cd6a5-7c51-4d2d-8e9b-1528ec0c5ad2": "To do this we assume that the variational distribution has the factorization de\ufb01ned by (10.42) and (10.55) with factors given by (10.48), (10.57), and (10.59). Substitute these into (10.70) and hence obtain the lower bound as a function of the parameters of the variational distribution. Then, by maximizing the bound with respect to these parameters, derive the re-estimation equations for the factors in the variational distribution, and show that these are the same as those obtained in Section 10.2.1. 10.19 (\u22c6 \u22c6) Derive the result (10.81) for the predictive distribution in the variational treatment of the Bayesian mixture of Gaussians model. 10.20 (\u22c6 \u22c6) www This exercise explores the variational Bayes solution for the mixture of Gaussians model when the size N of the data set is large and shows that it reduces (as we would expect) to the maximum likelihood solution based on EM derived in Chapter 9. Note that results from Appendix B may be used to help answer this exercise. First show that the posterior distribution q\u22c6(\u039bk) of the precisions becomes sharply peaked around the maximum likelihood solution", "0930e2e6-78e9-4530-af48-b63c03c53cf2": "At a saddle point, the Hessian matrix has both positive and negative eigenvalues. Points lying along eigenvectors associated with positive eigenvalues have greater cost than the saddle point, while points lying along negative eigenvalues have lower value.\n\nWe can think of a saddle point as being a local minimum along one cross-section of the cost function and a local maximum along another cross-section. See figure 4.5 for an illustration. Many classes of random functions exhibit the following behavior: in low- dimensional spaces, local minima are common. In higher-dimensional spaces, local minima are rare, and saddle points are more common. For a function f : R\u201d > R of this type, the expected ratio of the number of saddle points to local minima grows exponentially with n. To understand the intuition behind this behavior, observe that the Hessian matrix at a local minimum has only positive eigenvalues. The  282  https://www.deeplearningbook.org/contents/optimization.html    Hessian matrix at a saddle point has a mixture of positive and negative eigenvalues. Imagine that the sign of each eigenvalue is generated by flipping a coin", "05c9042a-1110-48e2-bd28-2dd257c8213f": "This time we are interested in computing the variance of the estimator 0) = +07\", a),  Var (An ) = Var (2 s 2) (5.48) i=1  126  CHAPTER 5. MACHINE LEARNING BASICS  i=l = +> 0-9) (5.50) i=l = m0(1~ 0) (5.51) - =9(1 _ 6) (5.52)  The variance of the estimator decreases as a function of m, the number of examples in the dataset. This is a common property of popular estimators that we will return to when we discuss consistency (see section 5.4.5). 5.4.4 Trading off Bias and Variance to Minimize Mean Squared Error  Bias and variance measure two different sources of error in an estimator. Bias measures the expected deviation from the true value of the function or parameter. https://www.deeplearningbook.org/contents/ml.html    Variance on the other hand, provides a measure of the deviation from the expected estimator value that any particular sampling of the data is likely to cause", "4a26bc5f-0123-46e2-8dd5-421a67a2da76": "Specifically, we will describe how to compute the gradient Va f(a, y) for an arbitrary function f, where zx is a set of variables whose derivatives are desired, and y is an additional set of variables that are inputs to the function but whose derivatives are not required. In learning algorithms, the gradient we most often require is the gradient of the cost function with respect to the parameters, VoJ(@). Many machine learning tasks involve computing other derivatives, either as part of the learning process, or to analyze the learned model. The back- propagation algorithm can be applied to these tasks as well and is not restricted to computing the gradient of the cost function with respect to the parameters.\n\nThe idea of computing derivatives by propagating information through a network is very general and can be used to compute values such as the Jacobian of a function  200  https://www.deeplearningbook.org/contents/mlp.html    CHAPFER-6\u2014_DEEP FEEPFORWARD NEFWORKS  f with multiple outputs. We restrict our description here to the most commonly used case, where f has a single output. 6.5.1 Computational Graphs  So far we have discussed neural networks with a relatively informal graph language", "6facaa8d-1ad3-4420-bf98-8bc69b5e1e16": "Now consider the evaluation of the derivative of En with respect to a weight wji. The outputs of the various units will depend on the particular input pattern n. However, in order to keep the notation uncluttered, we shall omit the subscript n from the network variables.\n\nFirst we note that En depends on the weight wji only via the summed input aj to unit j. We can therefore apply the chain rule for partial derivatives to give \u2202En \u2202wji = \u2202En Substituting (5.51) and (5.52) into (5.50), we then obtain Equation (5.53) tells us that the required derivative is obtained simply by multiplying the value of \u03b4 for the unit at the output end of the weight by the value of z for the unit at the input end of the weight (where z = 1 in the case of a bias). Note that this takes the same form as for the simple linear model considered at the start of this section. Thus, in order to evaluate the derivatives, we need only to calculate the value of \u03b4j for each hidden and output unit in the network, and then apply (5.53)", "465faac9-de9f-4c44-a5d9-a9d99b38b340": "demonstrated that a generative model can learn a representation of images of faces, with separate directions in representation space capturing different underlying factors of variation. Figure 15.9 demonstrates that one direction in representation space corresponds  https://www.deeplearningbook.org/contents/representation.html    549  CHAPTER 15. REPRESENTATION LEARNING  Figure 15.9: A generative model has learned a distributed representation that disentangles the concept of gender from the concept of wearing glasses. If we begin with the repre- sentation of the concept of a man with glasses, then subtract the vector representing the concept of a man without glasses, and finally add the vector representing the concept of a woman without glasses, we obtain the vector representing the concept of a woman with glasses.\n\nThe generative model correctly decodes all these representation vectors to images that may be recognized as belonging to the correct class. Images reproduced with permission from Radford et al. to whether the person is male or female, while another corresponds to whether the person is wearing glasses. These features were discovered automatically, not fixed a priori", "44f79ea3-d3c8-48b8-95e8-456ee9984ce7": "The estimator only depends on samples from p(\u03f5) and p(\u03b6) which are obviously not in\ufb02uenced by \u03c6, therefore the estimator can be differentiated w.r.t. \u03c6. The resulting stochastic gradients can be used in conjunction with stochastic optimization methods such as SGD or Adagrad . See algorithm 1 for a basic approach to computing stochastic gradients. Let the prior over the parameters and latent variables be the centered isotropic Gaussian p\u03b1(\u03b8) = N(z; 0, I) and p\u03b8(z) = N(z; 0, I). Note that in this case, the prior lacks parameters. Let\u2019s also assume that the true posteriors are approximatily Gaussian with an approximately diagonal covariance.\n\nIn this case, we can let the variational approximate posteriors be multivariate Gaussians with a diagonal covariance structure: Algorithm 2 Pseudocode for computing a stochastic gradient using our estimator. See text for meaning of the functions f\u03c6, g\u03c6 and h\u03c6", "d3eada56-fa89-4fa5-b74e-d8563c4ebd15": "Choosing the hyperparameters  422  CHAPTER 11. PRACTICAL METHODOLOGY  manually requires understanding what the hyperparameters do and how machine learning models achieve good generalization. Automatic hyperparameter selection algorithms greatly reduce the need to understand these ideas, but they are often much more computationally costly. 11.4.1 Manual Hyperparameter Tuning  To set hyperparameters manually, one must understand the relationship between hyperparameters, training error, generalization error and computational resources (memory and runtime). This means establishing a solid foundation on the funda- mental ideas concerning the effective capacity of a learning algorithm, as described in chapter 5. The goal of manual hyperparameter search is usually to find the lowest general- ization error subject to some runtime and memory budget. We do not discuss how  https://www.deeplearningbook.org/contents/guidelines.html    to determine the runtime and memory impact of various hyperparameters here because this is highly platform dependent", "09829740-92d8-442e-8c9e-cbaf77ba75eb": "Whenever the robot is searching, the possibility exists that its battery will become depleted. In this case the robot must shut down and wait to be rescued (producing a low reward). If the energy level is high, then a period of active search can always be completed without risk of depleting the battery. A period of searching that begins with a high energy level leaves the energy level high with probability \u21b5 and reduces it to low with probability 1 \u2212 \u21b5. On the other hand, a period of searching undertaken when the energy level is low leaves it low with probability \u03b2 and depletes the battery with probability 1 \u2212 \u03b2. In the latter case, the robot must be rescued, and the battery is then recharged back to high. Each can collected by the robot counts as a unit reward, whereas a reward of \u22123 results whenever the robot has to be rescued. Let rsearch and rwait, with rsearch > rwait, respectively denote the expected number of cans the robot will collect (and hence the expected reward) while searching and while waiting", "2193cda7-c424-49bb-9589-d733838cb455": "That parametric mean squared error decreases as m increases, and for m large, the Cram\u00e9r-Rao lower bound  shows that no consistent estimator has a lower MSE than the maximum likelihood estimator. For these reasons (consistency and efficiency), maximum likelihood is often considered the preferred estimator to use for machine learning.\n\nWhen the number of examples is small enough to yield overfitting behavior, regularization strategies such as weight decay may be used to obtain a biased version of maximum likelihood that has less variance when training data is limited. 5.6 Bayesian Statistics  So far we have discussed frequentist statistics and approaches based on estimat- ing a single value of 0, then making all predictions thereafter based on that one estimate. Another approach is to consider all possible values of @ when making a prediction. The latter is the domain of Bayesian statistics. As discussed in section 5.4.1, the frequentist perspective is that the true parameter value @ is fixed but unknown, while the point estimate 6 isa random variable on account of it being a function of the dataset (which is seen as random). The Bayesian perspective on statistics is quite different. The Bayesian uses probability to reflect degrees of certainty in states of knowledge", "84ff36ac-1467-408a-8b8d-74bc1e6016c4": "By convention, in the context of information theory, we treat these expressions as lim,_,9 x logx = 0.\n\n3.14 Structured Probabilistic Models  Machine learning algorithms often involve probability distributions over a very large number of random variables. Often, these probability distributions involve direct interactions between relatively few variables. Using a single function to describe the entire joint probability distribution can be very inefficient (both  computationally and statistically). 73  CHAPTER 3. PROBABILITY AND INFORMATION THEORY  https://www.deeplearningbook.org/contents/prob.html    g* = argmin,Dscx (Pl) g* = argmin, Dict (alle)  \u2014 (x) r \u2014 pz) E <= \u00a2*(z) 2 1\\ == g(x) B Z ! 4 g 1} fal a 1 \\ > 2 1} = im 1 \\ i Ha) \\ 3 g I al -~ 3 ! \\ 3 - ~ 2 1 & a} f \\  f \\ - y x x  Figure 3.6: The KL divergence is asymmetric", "a8b8cf40-6695-460f-9a93-34ff84dd41fb": "If two hidden  tadoad atocat c at a oda ad + 4 a1  https://www.deeplearningbook.org/contents/optimization.html    ULLLLS Witt LUE Salle aclulVallOll LULICLIOLL ale COLLLECLEU LO LILE Sallie i1puls, UeLL these units must have different initial parameters. If they have the same initial parameters, then a deterministic learning algorithm applied to a deterministic cost and model will constantly update both of these units in the same way. Even if the  297  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  model or training algorithm is capable of using stochasticity to compute different updates for different units (for example, if one trains with dropout), it is usually best to initialize each unit to compute a different function from all the other units. This may help to make sure that no input patterns are lost in the null space of forward propagation and that no gradient patterns are lost in the null space of back-propagation. The goal of having each unit compute a different function motivates random initialization of the parameters", "f4802f03-3349-44a6-8b4e-623bf5ac5655": "The e\u21b5ectiveness of this technique relies on the fact that sparse reward problems are not just problems with the reward signal; they are also problems with an agent\u2019s policy in preventing the agent from frequently encountering rewarding states.\n\nShaping involves changing the reward signal as learning proceeds, starting from a reward signal that is not sparse given the agent\u2019s initial behavior, and gradually modifying it toward the reward signal suited to problem of original interest. Shaping might also involve modifying the dynamics of the task as learning proceeds. Each modi\ufb01cation is made so that the agent is frequently rewarded given its current behavior. The agent faces a sequence of increasingly-di\ufb03cult reinforcement learning problems, where what is learned at each stage makes the next-harder problem relatively easy because the agent now encounters reward more frequently than it would if it did not have prior experience with easier problems. This kind of shaping is an essential technique in training animals, and it is e\u21b5ective in computational reinforcement learning as well", "69c23d2e-6aa4-4650-ba3d-43a7201be83a": "Fortunately, for typical values of \u03bb and \u03b3 the eligibility traces of almost all states are almost always nearly zero; only those states that have recently been visited will have traces signi\ufb01cantly greater than zero and only these few states need to be updated to closely approximate these algorithms. In practice, then, implementations on conventional computers may keep track of and update only the few traces that are signi\ufb01cantly greater than zero. Using this trick, the computational expense of using traces in tabular methods is typically just a few times that of a one-step method. The exact multiple of course depends on \u03bb and \u03b3 and on the expense of the other computations. Note that the tabular case is in some sense the worst case for the computational complexity of eligibility traces. When function approximation is used, the computational advantages of not using traces generally decrease. For example, if ANNs and backpropagation are used, then eligibility traces generally cause only a doubling of the required memory and computation per step.\n\nTruncated \u03bb-return methods (Section 12.3) can be computationally e\ufb03cient on conventional computers though they always require some additional memory. Eligibility traces in conjunction with TD errors provide an e\ufb03cient, incremental way of shifting and choosing between Monte Carlo and TD methods", "bae27bbb-022e-4529-8b11-1182f13dac88": "Show that a Gaussian approximation to the posterior distribution can be maintained through the use of the lower bound (10.151), in which the distribution is initialized using the prior, and as each data point is absorbed its corresponding variational parameter \u03ben is optimized. 10.33 (\u22c6) By differentiating the quantity Q(\u03be, \u03beold) de\ufb01ned by (10.161) with respect to the variational parameter \u03ben show that the update equation for \u03ben for the Bayesian logistic regression model is given by (10.163).\n\n10.34 (\u22c6 \u22c6) In this exercise we derive re-estimation equations for the variational parameters \u03be in the Bayesian logistic regression model of Section 4.5 by direct maximization of the lower bound given by (10.164). To do this set the derivative of L(\u03be) with respect to \u03ben equal to zero, making use of the result (3.117) for the derivative of the log of a determinant, together with the expressions (10.157) and (10.158) which de\ufb01ne the mean and covariance of the variational posterior distribution q(w). 10.35 (\u22c6 \u22c6) Derive the result (10.164) for the lower bound L(\u03be) in the variational logistic regression model", "75cef3f2-cfc5-4635-8839-0ff8a418a7e6": "CNNs take this property into account by sharing parameters across waee-Tbinl a tee en Tenn atnn. Mh 2A. fend (2 Ltd An, eee eta tL 2 ent)  https://www.deeplearningbook.org/contents/regularization.html  IHULLIPLe Layee LOCALLULLDS. dile salle leavuLe (a WUUCL ULL WILL LUE Salle WELEILLS )  is computed over different locations in the input. This means that we can find a cat with the same cat detector whether the cat appears at column 7?\n\nor column  i+ 1 in the image. 250  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  Parameter sharing has enabled CNNs to dramatically lower the number of unique model parameters and to significantly increase network sizes without  requiring a corresponding increase in training data. It remains one of the best  examples of how to effectively incorporate domain knowledge into the network  architecture. CNNs are discussed in more detail in chapter 9. 7.10 Sparse Representations  Weight decay acts by placing a penalty directly on the model parameters", "a1a15074-c5e0-4865-9c45-f4f1cd968ae5": "In fact, the minimization with respect to W is just a linear regression problem. Minimization of J with respect to both arguments, however, is usually not a convex problem. Minimization with respect to H requires specialized algorithms such as the feature-sign search algorithm . 635  CHAPTER 19. APPROXIMATE INFERENCE  19.4 Variational Inference and Learning  We have seen how the evidence lower bound L(v,0,q) is a lower bound on og p(v;@), how inference can be viewed as maximizing \u00a3 with respect to q, and how learning can be viewed as maximizing \u00a3 with respect to 0.\n\nWe have seen hat the EM algorithm enables us to make large learning steps with a fixed q and hat learning algorithms based on MAP inference enable us to learn using a point estimate of p(h | v) rather than inferring the entire distribution. Now we develop che more general approach to variational learning. The core idea behind variational learning is that we can maximize L over a restricted family of distributions q. This family should be chosen so that it is easy o compute E, log p(h,v)", "deed97b6-e25c-49f6-91b6-d260e3a1888e": "Jarrett et al.\n\nobserved that \u201cusing a rectifying nonlinearity is the single most important factor in improving the performance of a recognition system,\u201d among several different factors of neural network architecture design. For small datasets, Jarrett et al. observed that using rectifying non- inearities is even more important than learning the weights of the hidden layers. Random weights are sufficient to propagate useful information through a rectified inear network, enabling the classifier layer at the top to learn how to map different feature vectors to class identities. When more data is available, learning begins to extract enough useful knowledge o exceed the performance of randomly chosen parameters. Glorot et al. (201 1a) showed that learning is far easier in deep rectified linear networks than in deep networks that have curvature or two-sided saturation in their activation functions. Rectified linear units are also of historical interest because they show that neuroscience has continued to have an influence on the development of deep learning algorithms. Glorot et al. motivated rectified linear units from biological considerations", "91583999-da03-4ff8-9341-b712214df2d2": "In simulated trials with these assumptions about background \ufb01ring rate and input representation, TD errors of the TD model are remarkably similar to dopamine neuron phasic activity.\n\nPreviewing our description of details about these similarities in Section 15.4 below, the TD errors parallel the following features of dopamine neuron activity: 1) the phasic response of a dopamine neuron only occurs when a rewarding event is unpredicted; 2) early in learning, neutral cues that precede a reward do not cause substantial phasic dopamine responses, but with continued learning these cues gain predictive value and come to elicit phasic dopamine responses; 3) if an even earlier cue reliably precedes a cue that has already acquired predictive value, the phasic dopamine response shifts to the earlier cue, ceasing for the later cue; and 4) if after learning, the predicted rewarding event is omitted, a dopamine neuron\u2019s response decreases below its baseline level shortly after the expected time of the rewarding event", "d314e7c8-6b97-49b5-92f3-5458df6201a6": "To analyze how many solutions the equation has, think of the columns of A as specifying different directions we can travel in from the origin (the point specified by the vector of all zeros), then determine how many ways there are of reaching b.\n\nIn this view, each element of x specifies how far we should travel in each of these directions, with x; specifying how far to move in the direction of column 1:  Ag = 5\u00b0 2;A,;. (2.27)  In general, this kind of operation is called a linear combination. Formally, a linear combination of some set of vectors {o, ee vir)} is given by multiplying each vector v by a corresponding scalar coefficient and adding the results:  So cv. (2.28) i The span of a set of vectors is the set of all points obtainable by linear combination of the original vectors. 35  CHAPTER 2. LINEAR ALGEBRA  Determining whether Aw = b has a solution thus amounts to testing whether b is in the span of the columns of A. This particular span is known as the column space, or the range, of A", "e6f6f8e7-d7b1-4ff4-93dd-19cc8c6521d6": "CONFRONTING THE PARTITION FUNCTION  Pp model(X)  =o (log Pmodei(X) _ log Dnoise(X)) : (18.37)  _s ( log Pnoise(X) ) (18.36)  NCE is thus simple to apply as long as log Pyodel is easy to back-propagate through, and, as specified above, pyoise is easy to evaluate (in order to evaluate Pjoint) and sample from (to generate the training data).\n\nNCE is most successful when applied to problems with few random variables, but it can work well even if those random variables can take on a high number of values. For example, it has been successfully applied to modeling the conditional distribution over a word given the context of the word . Though the word may be drawn from a large vocabulary, there is only one word. When NCE is applied to problems with many random variables, it becomes less efficient. The logistic regression classifier can reject a noise sample by identifying any one variable whose value is unlikely. This means that learning slows down greatly after pmode| has learned the basic marginal statistics", "5f6bfc7a-8b57-481d-ad86-4d8df1a4de93": "6) that displays candidates, and also supports annotation, e.g., for constructing a small held-out test set for end evaluation. Execution Model Since labeling functions operate on discrete candidates, their execution is embarrassingly parallel. If Snorkel is connected to a relational database that supports 6 Note that all code is open source and available\u2014with tutorials, blog posts,workshoplectures,andothermaterial\u2014athttp://snorkel.stanford. edu/. simultaneous connections, e.g., PostgreSQL, then the master process (usually the notebook kernel) distributes the primary keys of the candidates to be labeled to Python worker processes. The workers independently read from the database to materialize the candidates via the ORM layer, then execute the labeling functions over them. The labels are returned to the master process which persists them via the ORM layer. Collecting the labels at the master is more ef\ufb01cient than having workers write directly to the database, due to table-level locking", "bcaa88c5-ab04-4493-9400-c8aed04af41c": "We do not formally define this concept in this  \u2018extbook. For our purposes, it is sufficient to understand the intuition that a set of measure zero occupies no volume in the space we are measuring. For example, within R?, a line has measure zero, while a filled polygon has positive measure. Likewise, an individual point has measure zero. Any union of countably many sets that each have measure zero also has measure zero (so the set of all the rational numbers has measure zero, for instance). Another useful term from measure theory is almost everywhere.\n\nA property that holds almost everywhere holds throughout all space except for on a set of  ?The Banach-Tarski theorem provides a fun example of such sets. 69  CHAPTER 3. PROBABILITY AND INFORMATION THEORY  measure zero. Because the exceptions occupy a negligible amount of space, they can be safely ignored for many applications. Some important results in probability theory hold for all discrete values but hold \u201calmost everywhere\u201d only for continuous values. Another technical detail of continuous variables relates to handling continuous random variables that are deterministic functions of one another", "c4684d7e-d964-4b26-bc49-a549d947acaf": "It should be emphasized that the value of \u03b1 has been determined purely by looking at the training data. In contrast to maximum likelihood methods, no independent data set is required in order to optimize the model complexity. We can similarly maximize the log marginal likelihood (3.86) with respect to \u03b2.\n\nTo do this, we note that the eigenvalues \u03bbi de\ufb01ned by (3.87) are proportional to \u03b2, and hence d\u03bbi/d\u03b2 = \u03bbi/\u03b2 giving The stationary point of the marginal likelihood therefore satis\ufb01es Again, this is an implicit solution for \u03b2 and can be solved by choosing an initial value for \u03b2 and then using this to calculate mN and \u03b3 and then re-estimate \u03b2 using (3.95), repeating until convergence. If both \u03b1 and \u03b2 are to be determined from the data, then their values can be re-estimated together after each update of \u03b3. and the prior (green) in which the axes in parameter space have been rotated to align with the eigenvectors ui of the Hessian. For \u03b1 = 0, the mode of the posterior is given by the maximum likelihood solution wML, whereas for nonzero \u03b1 the mode is at wMAP = mN", "e27d5b08-9c5b-415f-9134-02d218f114ee": "We shall also use cov as a shorthand notation for cov. The concepts of expectations and covariances are introduced in Section 1.2.2. If we have N values x1, . .\n\n, xN of a D-dimensional vector x = (x1, . , xD)T, we can combine the observations into a data matrix X in which the nth row of X corresponds to the row vector xT n. Thus the n, i element of X corresponds to the ith element of the nth observation xn. For the case of one-dimensional variables we shall denote such a matrix by x, which is a column vector whose nth element is xn. Note that x (which has dimensionality N) uses a different typeface to distinguish it from x (which has dimensionality D). The problem of searching for patterns in data is a fundamental one and has a long and successful history", "a35c8540-3bca-4b2d-8af0-5b91e9a6de26": "Joel, D., Niv, Y., Ruppin, E. Actor\u2013critic models of the basal ganglia: New anatomical and computational perspectives. Neural Networks, 15(4):535\u2013547. Johnson, A., Redish, A. D. Neural ensembles in CA3 transiently encode paths forward of the animal at a decision point. The Journal of Neuroscience, 27(45):12176\u201312189. In Proceedings of the 10th International Conference on Machine Learning , pp. 167\u2013173. Morgan Kaufmann. Kaelbling, L. P. Learning in Embedded Systems. MIT Press, Cambridge, MA. Kaelbling, L. P. (Ed.) . Special triple issue on reinforcement learning, Machine Learning, Journal of Arti\ufb01cial Intelligence Research, 4:237\u2013285. Kakade, S. M. A natural policy gradient.\n\nIn Advances in Neural Information Processing Systems 14 , pp. 1531\u20131538", "741e03d1-c626-4507-a0c0-0b8714419576": "We can find P(x) with the sum rule:  Va \u20ac x, P(x =2) = 2 Pa=ny =a) (3.3)  56  CHAPTER 3.\n\nPROBABILITY AND INFORMATION THEORY  The name \u201cmarginal probability\u201d comes from the process of computing marginal probabilities on paper. When the values of P(x, y) are written in a grid with different values of x in rows and different values of y in columns, it is natural to sum across a row of the grid, then write P(x) in the margin of the paper just to the right of the row. For continuous variables, we need to use integration instead of summation:  p(x) = [ve y)dy. (3.4)  3.5 Conditional Probability  In many cases, we are interested in the probability of some event, given that some other event has happened. This is called a conditional probability. We denote the conditional probability that y = y given x =a as P(y =y|x =~2)", "805cc799-4568-468e-a01d-f252033b5dd1": "BERT embeddings! | 78.66 86.25 94.37 88.66 84.40 92.8 69.54 | 84.94 BERT CLS-vector! 78.68 84.85 94.21 88.23 84.13 91.4 71.13 | 84.66 Ours: IS-BERT-task 81.09 87.18 94.96 88.75 85.96 88.64 74.24 | 85.91  Using labeled NLI data (supervised methods)  InferSent - GloVe! 81.57 86.54 92.50 90.38 84.18 882 75.77 | 85.59 USE! 80.09 85.19 93.98 86.70 86.38 93.2 70.14 | 85.10 SBERT-NLI? 83.64 89.43 94.39 89.86 88.96 89.6 76.00 | 87.41 Citation Cited as:  Weng, Lilian. .\n\nContrastive representation learning. Lil'Log. https://lilianweng.github.io/posts/2021-05-31-contrastive/", "d9d23a19-2974-4819-ac2e-7b5f8b61a03a": "What, then, is v\u21e1(15) for the equiprobable random policy? Now suppose the dynamics of state 13 are also changed, such that action down from state 13 takes the agent to the new state 15. What is v\u21e1(15) for the equiprobable random policy in this case? \u21e4 Exercise 4.3 What are the equations analogous to (4.3), (4.4), and (4.5) for the actionvalue function q\u21e1 and its successive approximation by a sequence of functions q0, q1, q2, . ? \u21e4 Our reason for computing the value function for a policy is to help \ufb01nd better policies. Suppose we have determined the value function v\u21e1 for an arbitrary deterministic policy \u21e1. For some state s we would like to know whether or not we should change the policy to deterministically choose an action a 6= \u21e1(s). We know how good it is to follow the current policy from s\u2014that is v\u21e1(s)\u2014but would it be better or worse to change to the new policy? One way to answer this question is to consider selecting a in s and thereafter following the existing policy, \u21e1", "b5696ceb-68d3-47dd-8485-1922fa733da9": "The contextual representation of each token is the concatenation of the left-to-right and right-to-left representations. When integrating contextual word embeddings with existing task-speci\ufb01c architectures, ELMo advances the state of the art for several major NLP benchmarks  including question answering , sentiment analysis , and named entity recognition . Melamud et al. proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs. Similar to ELMo, their model is feature-based and not deeply bidirectional. Fedus et al. shows that the cloze task can be used to improve the robustness of text generation models. As with the feature-based approaches, the \ufb01rst works in this direction only pre-trained word embedding parameters from unlabeled text . More recently, sentence or document encoders which produce contextual token representations have been pre-trained from unlabeled text and \ufb01ne-tuned for a supervised downstream task . The advantage of these approaches is that few parameters need to be learned from scratch", "9df563bc-cc22-4980-8108-1b160e19f267": "For example, the loss function \u2014log p(y | x; 8) can lack a global minimum point and instead asymptoticall approach some value as the model becomes more confident.\n\nFor a classifier wit  discrete y and p(y | x) provided by a softmax, the negative log-likelihood can become arbitrarily close to zero if the model is able to correctly classify every example in the training set, but it is impossible to actually reach the value of zero. Likewise, a model of real values p(y | x) = V(y;f(@),8~1) can have negative log-likelihood that asymptotes to negative infinity\u2014if f(@) is able to correctly predict the value of all training set y targets, the learning algorithm will increase 8 without bound. See figure 8.4 for an example of a failure of local optimization to find a good cost function value even in the absence of any local minima or saddle points. Future research will need to develop further understanding of the factors that influence the length of the learning trajectory and better characterize the outcome of the process. 288  CHAPTER 8", "a2591451-7f2b-4129-ad8c-5da7929501ec": "10.7 (\u22c6 \u22c6) Consider the problem of inferring the mean and precision of a univariate Gaussian using a factorized variational approximation, as considered in Section 10.1.3. Show that the factor q\u00b5(\u00b5) is a Gaussian of the form N(\u00b5|\u00b5N, \u03bb\u22121 N ) with mean and precision given by (10.26) and (10.27), respectively. Similarly show that the factor q\u03c4(\u03c4) is a gamma distribution of the form Gam(\u03c4|aN, bN) with parameters given by (10.29) and (10.30). 10.8 (\u22c6) Consider the variational posterior distribution for the precision of a univariate Gaussian whose parameters are given by (10.29) and (10.30)", "288a04bb-7327-4750-9d49-136a28444d8e": "Actor\u2013Critic with Eligibility Traces (continuing), for estimating \u21e1\u2713 \u21e1 \u21e1\u21e4 Naturally, in the continuing case, we de\ufb01ne values, v\u21e1(s) .= E\u21e1 and q\u21e1(s, a) .= E\u21e1, with respect to the di\u21b5erential return: With these alternate de\ufb01nitions, the policy gradient theorem as given for the episodic case (13.5) remains true for the continuing case. A proof is given in the box on the next page. The forward and backward view equations also remain the same. Proof of the Policy Gradient Theorem (continuing case) The proof of the policy gradient theorem for the continuing case begins similarly to the episodic case. Again we leave it implicit in all cases that \u21e1 is a function of \u2713 and that the gradients are with respect to \u2713", "ee22e70b-7d83-45d6-8660-0016e0b78d5e": "I would also like to thank Asela Gunawardana for plotting the spectrogram in Figure 13.1, and Bernhard Sch\u00a8olkopf for permission to use his kernel PCA code to plot Figure 12.17.\n\nMany people have helped by proofreading draft material and providing comments and suggestions, including Shivani Agarwal, C\u00b4edric Archambeau, Arik Azran, Andrew Blake, Hakan Cevikalp, Michael Fourman, Brendan Frey, Zoubin Ghahramani, Thore Graepel, Katherine Heller, Ralf Herbrich, Geoffrey Hinton, Adam Johansen, Matthew Johnson, Michael Jordan, Eva Kalyvianaki, Anitha Kannan, Julia Lasserre, David Liu, Tom Minka, Ian Nabney, Tonatiuh Pena, Yuan Qi, Sam Roweis, Balaji Sanjiya, Toby Sharp, Ana Costa e Silva, David Spiegelhalter, Jay Stokes, Tara Symeonides, Martin Szummer, Marshall Tappen, Ilkay Ulusoy, Chris Williams, John Winn, and Andrew Zisserman. Finally, I would like to thank my wife Jenna who has been hugely supportive throughout the several years it has taken to write this book", "b0e67f80-0201-4745-a9ca-f3a270c70ac5": ", xn), which we denote by \ufffd\u03b1(zn) = N(zn|\u00b5n, Vn)", "7bf83de7-8e12-413f-91fa-d7f484db249d": "It is basically the algorithm that produced the on-policy results shown in Figure 8.8. The close connection between RTDP and conventional DP makes it possible to derive some theoretical results by adapting existing theory. RTDP is an example of an asynchronous DP algorithm as described in Section 4.5.\n\nAsynchronous DP algorithms are not organized in terms of systematic sweeps of the state set; they update state values in any order whatsoever, using whatever values of other states happen to be available. In RTDP, the update order is dictated by the order states are visited in real or simulated trajectories. reached by any optimal policy from any of the start states, and there is no need to specify optimal actions for these irrelevant states. What is needed is an optimal partial policy, meaning a policy that is optimal for the relevant states but can specify arbitrary actions, or even be unde\ufb01ned, for the irrelevant states. But \ufb01nding such an optimal partial policy with an on-policy trajectory-sampling control method, such as Sarsa (Section 6.4), in general requires visiting all state\u2013action pairs\u2014even those that will turn out to be irrelevant\u2014an in\ufb01nite number of times. This can be done, for example, by using exploring starts (Section 5.3)", "986f611b-c616-43d7-803a-b96e45e192df": "The general principles apply to continuous-time problems as well, although the theory gets more complicated and we omit it from this introductory treatment. Tic-tac-toe has a relatively small, \ufb01nite state set, whereas reinforcement learning can be used when the state set is very large, or even in\ufb01nite. For example, Gerry Tesauro  combined the algorithm described above with an arti\ufb01cial neural network to learn to play backgammon, which has approximately 1020 states. With this many states it is impossible ever to experience more than a small fraction of them. Tesauro\u2019s program learned to play far better than any previous program and eventually better than the world\u2019s best human players (Section 16.1).\n\nThe arti\ufb01cial neural network provides the program with the ability to generalize from its experience, so that in new states it selects moves based on information saved from similar states faced in the past, as determined by its network. How well a reinforcement learning system can work in problems with such large state sets is intimately tied to how appropriately it can generalize from past experience. It is in this role that we have the greatest need for supervised learning methods with reinforcement learning", "3f60c394-b359-437d-b865-f444c723eabe": "It is difficult to scale learning algorithms to billions of examples if we must remember a dynamically updated vector associated with each example. Second, we would like to be able to extract the features h very quickly, in order to recognize the content of v. In a realistic deployed setting, we would need to be  640  https://www.deeplearningbook.org/contents/inference.html    CHAPTER 19. APPROXIMATE INFERENCE  able to compute h in real time. For both these reasons, we typically do not use gradient descent to compute the mean field parameters h. Instead, we rapidly estimate them with fixed-point equations. The idea behind fixed-point equations is that we are seeking a local maximum  with respect to h, where VpLl (v, 0, h) = 0. We cannot efficiently solve this equation with respect to all of h simultaneously. However, we can solve for a single  variable: A \u2014L(v,0,h) =0", "810cee48-2ebe-42fe-a27c-df0600cbb15f": "One merely chooses one GVF\u2019s cumulant to be the reward (Ct = Rt), its policy to be the the option\u2019s policy (\u21e1=\u21e1! ), and its termination function to be the discount rate times the option\u2019s termination function (\u03b3(s) = \u03b3 \u00b7 \u03b3!(s)). The true GVF then equals the reward part of the option model, v\u21e1,\u03b3,C(s) = r(s, ! ), and the learning methods described in this book can be used to approximate it. The state-transition part of the option model is only a little more complicated.\n\nOne needs to allocate one GVF for each state that the option might terminate in. We don\u2019t want these GVFs to accumulate anything except when the option terminates, and then only when the termination is in the appropriate state. This can be achieved by choosing the cumulant of the GVF that predicts transition to state s0 to be Ct = (1 \u2212 \u03b3! (St)) are chosen the same as for the reward part of the option model", "5faea18b-3ccc-4096-9078-11063e077ff3": "There we assessed e\u21b5ectiveness on a 19-state random walk task (Example 7.1, page 144). Figure 12.3 shows the performance of the o\u270fine \u03bb-return algorithm on this task alongside that of the n-step methods (repeated from Figure 7.2).\n\nThe experiment was just as described earlier except that for the \u03bb-return algorithm we varied \u03bb instead of n. The performance measure used is the estimated root-meansquared error between the correct and estimated values of each state measured at the end of the episode, averaged over the \ufb01rst 10 episodes and the 19 states. Note that overall performance of the o\u270fine \u03bb-return algorithms is comparable to that of the n-step algorithms. In both cases we get best performance with an intermediate value of the bootstrapping parameter, n for n-step methods and \u03bb for the o\u270fine \u03bb-return algorithm. The approach that we have been taking so far is what we call the theoretical, or forward, view of a learning algorithm. For each state visited, we look forward in time to all the future rewards and decide how best to combine them", "3d54bd34-a29d-41f9-bd6b-1340ea26941f": "Because this network lacks hidden-to-hidden recurrence, it requires that the output units capture all the information about the past that the network will use to predict the future. Because the output units are  376  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  https://www.deeplearningbook.org/contents/rnn.html     1 (t\u20141) r(t)  @ : ()  4 4  U U Train time Test time  Figure 10.6: Illustration of teacher forcing. Teacher forcing is a training technique that is applicable to RNNs that have connections from their output to their hidden states at the next time step. (Left) At train time, we feed the correct outputy drawn from the train set as input to A+), (Right) When the model is deployed, the true output is generally not known. In this case, we approximate the correct output yl? with the model\u2019s output o), and feed the output back into the model. explicitly trained to match the training set targets, they are unlikely to capture the necessary information about the past history of the input, unless the user knows how to describe the full state of the system and provides it as part of the training set targets", "2c22defa-d3fa-4a03-87c6-e0a1c968f5b1": "Calculus of variations is the origin of the names \u201cvariational learning\u201d and \u201cvariational inference,\u201d though these names apply even when the latent variables are discrete and calculus of variations is not needed. With continuous latent variables, calculus of variations is a powerful technique that removes much of the responsibility from the human designer of the model, who now must specify only how q factorizes, rather than needing to guess how to design a specific g that can accurately approximate the posterior. Because L(v, 8,q) is defined to be log pv, 8) \u2014 Dx q(h | v)||p(h | v;9)), we can think of maximizing \u00a3 with respect tog as minimizing Dgy(q(h | v)||p(h | v)). 636  CHAPTER 19. APPROXIMATE INFERENCE  In this sense, we are fitting g to p", "1d1b9cc1-ba2d-414d-a05a-c2f2912abfaa": "One is the extension to other classic machine learning settings, such as structured prediction, regression,andanomalydetectionsettings.Anotherdirection is extending the possible output signature of labeling functions to include continuous values, probability distributions, or other more complex outputs. The extension of the core modeling techniques\u2014for example, learning labeling function accuracies that are conditioned on speci\ufb01c subsets of the data, or jointly learning the generative and discriminative models\u2014also provide exciting avenues for future research. Another practical and interesting direction is exploring integrations with other complementary techniques for dealing with the lack of hand-labeled training data (see Other Forms of Supervision in Sect. 6).\n\nOne example is active learning , in which the goal is to intelligently sample data points to be labeled; in our setting, we could intelligently select sets of data points to show to the user when writing labeling functions\u2014e.g., data points not labeled by existing labeling functions\u2014and potentially with interesting visualizations and graphical interfaces to aid and direct this development process. Another interesting direction is formalizing the connection between labeling functions and transfer learning , and making more formal and practical connections to semi-supervised learning", "eaf119a4-169d-4726-ae97-a24076ce0613": "COURSERA: Neural networks for Tolman, E. C. Purposive Behavior in Animals and Men. Century, New York. Tolman, E. C. Cognitive maps in rats and men. Psychological Review, 55(4):189\u2013208. Tsai, H.-S., Zhang, F., Adamantidis, A., Stuber, G. D., Bonci, A., de Lecea, L., Deisseroth, K. Phasic \ufb01ring in dopaminergic neurons is su\ufb03cient for behavioral conditioning. Science, 324:1080\u20131084. Tsetlin, M. L. Automaton Theory and Modeling of Biological Systems. Academic Press, Tsitsiklis, J. N. On the convergence of optimistic policy iteration. Journal of Machine Tsitsiklis, J. N., Van Roy, B. .\n\nFeature-based methods for large scale dynamic programming. Tsitsiklis, J. N., Van Roy, B", "e35ce251-72cf-4962-8075-854efef4f997": "A common kernel is the Gaussian radial basis function (RBF) used in RBF function approximation as described in Section 9.5.5. In the method described there, RBFs are features whose centers and widths are either \ufb01xed from the start, with centers presumably concentrated in areas where many examples are expected to fall, or are adjusted in some way during learning.\n\nBarring methods that adjust centers and widths, this is a linear parametric method whose parameters are the weights of each RBF, which are typically learned by stochastic gradient, or semi-gradient, descent. The form of the approximation is a linear combination of the pre-determined RBFs. Kernel regression with an RBF kernel di\u21b5ers from this in two ways. First, it is memory-based: the RBFs are centered on the states of the stored examples. Second, it is nonparametric: there are no parameters to learn; the response to a query is given by (9.23). Of course, many issues have to be addressed for practical implementation of kernel regression, issues that are beyond the scope of our brief discussion", "2e983ab4-4883-4bbb-83df-4de9d87491a4": "A suf\ufb01cient (but not necessary) condition for ensuring that the required distribution p(z) is invariant is to choose the transition probabilities to satisfy the property of detailed balance, de\ufb01ned by for the particular distribution p\u22c6(z).\n\nIt is easily seen that a transition probability that satis\ufb01es detailed balance with respect to a particular distribution will leave that distribution invariant, because \ufffd A Markov chain that respects detailed balance is said to be reversible. Our goal is to use Markov chains to sample from a given distribution. We can achieve this if we set up a Markov chain such that the desired distribution is invariant. However, we must also require that for m \u2192 \u221e, the distribution p(z(m)) converges to the required invariant distribution p\u22c6(z), irrespective of the choice of initial distribution p(z(0)). This property is called ergodicity, and the invariant distribution is then called the equilibrium distribution. Clearly, an ergodic Markov chain can have only one equilibrium distribution. It can be shown that a homogeneous Markov chain will be ergodic, subject only to weak restrictions on the invariant distribution and the transition probabilities", "9f84fb1a-9068-4413-9291-6a6947d07982": "It is easy to see that in this case, When \u03b8t \u2192 0, the sequence (P\u03b8t)t\u2208N converges to P0 under the EM distance, but does not converge at all under either the JS, KL, reverse KL, or TV divergences. Figure 1 illustrates this for the case of the EM and JS distances. Example 1 gives us a case where we can learn a probability distribution over a low dimensional manifold by doing gradient descent on the EM distance. This cannot be done with the other distances and divergences because the resulting loss function is not even continuous.\n\nAlthough this simple example features distributions with disjoint supports, the same conclusion holds when the supports have a non empty intersection contained in a set of measure zero. This happens to be the case when two low dimensional manifolds intersect in general position . Since the Wasserstein distance is much weaker than the JS distance3, we can now ask whether W(Pr, P\u03b8) is a continuous loss function on \u03b8 under mild assumptions. This, and more, is true, as we now state and prove. Theorem 1. Let Pr be a \ufb01xed distribution over X", "c223a380-a3ff-480f-80e8-f170b7fd5ac0": "Let g be a function mapping one image function to another image function, such that I\u2019 = g(J) is the image function with I\u2019 (x,y) = I(x \u2014 1,y). This shifts every pixel of I one unit to he right. If we apply this transformation to J, then apply convolution, the result will be the same as if we applied convolution to J\u2019, then applied the transformation  334  CHAPTER 9. CONVOLUTIONAL NETWORKS  g to the output. When processing time-series data, this means that convolution produces a sort of timeline that shows when different features appear in the input. If we move an event later in time in the input, the exact same representation of it will appear in the output, just later. Similarly with images, convolution creates a 2-D map of where certain features appear in the input. If we move the object in the input, its representation will move the same amount in the output.\n\nThis is useful for when we know that some function of a small number of neighboring pixels is  https://www.deeplearningbook.org/contents/convnets.html    usetul when applied to multiple input locations", "ec44d108-5632-4f31-9cb2-7f0f822573df": "Reinforcement learning methods specify how the agent\u2019s policy is changed as a result of its experience. The value function of a state s under a policy \u21e1, denoted v\u21e1(s), is the expected return when starting in s and following \u21e1 thereafter.\n\nFor MDPs, we can de\ufb01ne v\u21e1 formally by where E\u21e1 denotes the expected value of a random variable given that the agent follows policy \u21e1, and t is any time step. Note that the value of the terminal state, if any, is always zero. We call the function v\u21e1 the state-value function for policy \u21e1. Similarly, we de\ufb01ne the value of taking action a in state s under a policy \u21e1, denoted The value functions v\u21e1 and q\u21e1 can be estimated from experience. For example, if an agent follows policy \u21e1 and maintains an average, for each state encountered, of the actual returns that have followed that state, then the average will converge to the state\u2019s value, v\u21e1(s), as the number of times that state is encountered approaches in\ufb01nity", "cb491e23-7204-498a-ace8-ecc334e7edb2": "Consider a second set of points {yn} together with their corresponding convex hull.\n\nBy de\ufb01nition, the two sets of points will be linearly separable if there exists a vector \ufffdw and a scalar w0 such that \ufffdwTxn + w0 > 0 for all xn, and \ufffdwTyn +w0 < 0 for all yn. Show that if their convex hulls intersect, the two sets of points cannot be linearly separable, and conversely that if they are linearly separable, their convex hulls do not intersect. 4.2 (\u22c6 \u22c6) www Consider the minimization of a sum-of-squares error function (4.15), and suppose that all of the target vectors in the training set satisfy a linear constraint where tn corresponds to the nth row of the matrix T in (4.15)", "884cec70-0bcb-4831-9c11-c2a005ddccff": "Now let us examine what happens when we try to use rejection sampling in spaces of high dimensionality. Consider, for the sake of illustration, a somewhat arti\ufb01cial problem in which we wish to sample from a zero-mean multivariate Gaussian distribution with covariance \u03c32 rejection sampling from a proposal distribution that is itself a zero-mean Gaussian distribution having covariance \u03c32 there exists a k such that kq(z) \u2a7e p(z). In D-dimensions the optimum value of k is given by k = (\u03c3q/\u03c3p)D, as illustrated for D = 1 in Figure 11.7. The acceptance rate will be the ratio of volumes under p(z) and kq(z), which, because both distributions are normalized, is just 1/k. Thus the acceptance rate diminishes exponentially with dimensionality.\n\nEven if \u03c3q exceeds \u03c3p by just one percent, for D = 1, 000 the acceptance ratio will be approximately 1/20, 000. In this illustrative example the comparison function is close to the required distribution", "ae131ac4-4ed2-432c-a4b1-355cc59a09a2": "Many specialized application areas have application-specific criteria as well. What is important is to determine which performance metric to improve ahead of time, then concentrate on improving this metric. Without clearly defined goals, it can be difficult to tell whether changes to a machine learning system make progress or not. 419  CHAPTER 11.\n\nPRACTICAL METHODOLOGY  11.2 Default Baseline Models  After choosing performance metrics and goals, the next step in any practical application is to establish a reasonable end-to-end system as soon as possible. In this section, we provide recommendations for which algorithms to use as the first baseline approach in various situations. Keep in mind that deep learning research progresses quickly, so better default algorithms are likely to become available soon after this writing. Depending on the complexity of your problem, you may even want to begin without using deep learning. If your problem has a chance of being solved by just choosing a few linear weights correctly, you may want to begin with a simple statistical model like logistic regression. If you know that your problem falls into an \u201cAl-complete\u201d category like object recognition, speech recognition, machine translation, and so on, then you are likely to do well by beginning with an appropriate deep learning model", "b89ab64b-674d-4eb5-b994-61fed345bcad": "Improved regularization of convolutional neural networks with cutout. arXiv preprint. 2017. Agnieszka M, Michal G. Data augmentation for improving deep learning in image classification problem. In: IEEE 2018 international interdisciplinary Ph.D. Workshop, 2018. Jonathan K, Michael S, Jia D, Li F-F. 3D object representations for fine-grained categorization. In: 4th IEEE Work- shop on 3D Representation and Recognition, at ICCV 2013 (3dRR-13). Sydney, Australia. Dec. 8, 2013. Tomohiko K, Michiaki |. Icing on the cake: an easy and quick post-learning method you can try after deep learning arXiv preprints. 2018.  . Terrance V, Graham WT. Dataset augmentation in feature space. In: Proceedings of the international conference on  machine learning (ICML), workshop track, 2017. Sebastien CW, Adam G, Victor S, Mark DM", "27066e9b-af92-41c6-8afc-216e666a3efd": "We can thus think of the normal distribution as being the one that inserts the least amount of prior knowledge into a model. Fully developing and justifying this idea requires more mathematical tools and is postponed to section 19.4.2. The normal distribution generalizes to R\u201d, in which case it is known as the multivariate normal distribution.\n\nIt may be parametrized with a positive definite symmetric matrix B:  N (a; pb, ) =  (Qn) \"det() exp ( 5 (@ p) Da \u2014 H)) . (3.23)  https://www.deeplearningbook.org/contents/prob.html    CHAPTER 3. PROBABILITY AND INFORMATION THEORY  The parameter yp still gives the mean of the distribution, though now it is vector valued. The parameter & gives the covariance matrix of the distribution. As in the univariate case, when we wish to evaluate the PDF several times for many different values of the parameters, the covariance is not a computationally efficient way to parametrize the distribution, since we need to invert & to evaluate the PDF. We can instead use a precision matrix 3:  det (3) (2m)\u201d  We often fix the covariance matrix to be a diagonal matrix", "a12f8ad3-e4f1-4e6a-8b6f-e386ff040bb2": "CONVOLUTIONAL NETWORKS  dian 2nd tn 2424t da. Lee ene be Le Linda ntanlee tela. thle TW at tL 2  https://www.deeplearningbook.org/contents/convnets.html    LLOLL ALU 1S CULISLUCLEU VY SULIC LU VE VIVIURICALLY LI PlaUsLVIC. DULLOWIIULE LIC SUCCESS of back-propagation-based training of TDNNs, LeCun et al. developed the modern convolutional network by applying the same training algorithm to 2-D convolution applied to images. So far we have described how simple cells are roughly linear and selective for certain features, complex cells are more nonlinear and become invariant to some transformations of these simple cell features, and stacks of layers that alternate between selectivity and invariance can yield grandmother cells for specific phenomena.\n\nWe have not yet described precisely what these individual cells detect. In a deep nonlinear network, it can be difficult to understand the function of individual cells. Simple cells in the first layer are easier to analyze, because their responses are driven by a linear function", "99ae69a5-26f4-45a4-b834-34effbbce8ac": "Consider the following two MRPs: Where two edges leave a state, both transitions are assumed to occur with equal probability, and the numbers indicate the reward received. The MRP on the left has two states that are represented distinctly. The MRP on the right has three states, two of which, B and B0, appear the same and must be given the same approximate value.\n\nSpeci\ufb01cally, w has two components and the value of state A is given by the \ufb01rst component and the value of B and B0 is given by the second. The second MRP has been designed so that equal time is spent in all three states, so we can take \u00b5(s) = 1 Note that the observable data distribution is identical for the two MRPs. In both cases the agent will see single occurrences of A followed by a 0, then some number of apparent Bs, each followed by a \u22121 except the last, which is followed by a 1, then we start all over again with a single A and a 0, etc. All the statistical details are the same as well; in both MRPs, the probability of a string of k Bs is 2\u2212k. Now suppose w = 0", "0c3b32ac-f3b8-4b7a-9c93-e3ac5a4b83c4": "By convention we de\ufb01ne new parameters given by \u03bd = 2a and \u03bb = a/b, in terms of which the distribution p(x|\u00b5, a, b) takes the form which is known as Student\u2019s t-distribution.\n\nThe parameter \u03bb is sometimes called the precision of the t-distribution, even though it is not in general equal to the inverse of the variance. The parameter \u03bd is called the degrees of freedom, and its effect is illustrated in Figure 2.15. For the particular case of \u03bd = 1, the t-distribution reduces to the Cauchy distribution, while in the limit \u03bd \u2192 \u221e the t-distribution St(x|\u00b5, \u03bb, \u03bd) becomes a Gaussian N(x|\u00b5, \u03bb\u22121) with mean \u00b5 and precision \u03bb. Exercise 2.47 From (2.158), we see that Student\u2019s t-distribution is obtained by adding up an in\ufb01nite number of Gaussian distributions having the same mean but different precisions. This can be interpreted as an in\ufb01nite mixture of Gaussians (Gaussian mixtures will be discussed in detail in Section 2.3.9", "e8d2e134-b4e8-47d8-be72-9606dd32581f": "Fast forward selection to speed up sparse Gaussian processes. In C. M. Bishop and B. Frey (Eds. ), Proceedings Ninth International Workshop on Arti\ufb01cial Intelligence and Statistics, Key West, Florida. Shachter, R. D. and M. Peot . Simulation approaches to general probabilistic inference on belief networks. In P. P. Bonissone, M. Henrion, L. N. Kanal, and J. F. Lemmer (Eds. ), Uncertainty in Arti\ufb01cial Intelligence, Volume 5. Elsevier. Shannon, C. E. A mathematical theory of communication. The Bell System Technical Journal 27(3), 379\u2013423 and 623\u2013656. Sietsma, J. and R. J. F. Dow .\n\nCreating arti\ufb01cial neural networks that generalize. Neural Networks 4(1), 67\u201379", "c37a6ef0-4b2c-4ea5-8af3-bfcd2c73f6eb": "Multireward reinforced summarization with saliency and entailment.\n\nIn Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 646\u2013 653. Romain Paulus, Caiming Xiong, and Richard Socher. 2018. A deep reinforced model for abstractive summarization. In International Conference on Learning Representations. Lianhui Qin, Vered Shwartz, Peter West, Chandra Bhagavatula, Jena D Hwang, Ronan Le Bras, Antoine Bosselut, and Yejin Choi. 2020. Backpropagationbased decoding for unsupervised counterfactual and abductive reasoning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 794\u2013805. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9", "63934eb1-3df9-4172-9289-fac15f9be27e": "This provides a technique for generating samples from a general multivariate Gaussian using samples from a univariate Gaussian having zero mean and unit variance.\n\n11.6 (\u22c6 \u22c6) www In this exercise, we show more carefully that rejection sampling does indeed draw samples from the desired distribution p(z). Suppose the proposal distribution is q(z) and show that the probability of a sample value z being accepted is given by \ufffdp(z)/kq(z) where \ufffdp is any unnormalized distribution that is proportional to p(z), and the constant k is set to the smallest value that ensures kq(z) \u2a7e \ufffdp(z) for all values of z. Note that the probability of drawing a value z is given by the probability of drawing that value from q(z) times the probability of accepting that value given that it has been drawn. Make use of this, along with the sum and product rules of probability, to write down the normalized form for the distribution over z, and show that it equals p(z). 11.7 (\u22c6) Suppose that z has a uniform distribution over the interval", "bb7be9a9-9547-46b3-b570-3b77c1592735": "If it decreases in such a way as to satisfy the standard stochastic approximation conditions (2.7), then the SGD method (9.5) is guaranteed to converge to a local optimum. We turn now to the case in which the target output, here denoted Ut 2 R, of the tth training example, St 7! Ut, is not the true value, v\u21e1(St), but some, possibly random, approximation to it. For example, Ut might be a noise-corrupted version of v\u21e1(St), or it might be one of the bootstrapping targets using \u02c6v mentioned in the previous section.\n\nIn these cases we cannot perform the exact update (9.5) because v\u21e1(St) is unknown, but we can approximate it by substituting Ut in place of v\u21e1(St). This yields the following general SGD method for state-value prediction: If Ut is an unbiased estimate, that is, if E = v\u21e1(St), for each t, then wt is guaranteed to converge to a local optimum under the usual stochastic approximation conditions (2.7) for decreasing \u21b5", "77285af6-a655-4165-8a7c-3cfa7b00070b": "Specifically, as illustrated in figure 20.3, the DBM layers can be organized into a bipartite graph, with odd layers on one side and even layers on the other. This immediately implies that when we condition on the variables in the even layer, the variables in the odd layers become conditionally independent. Of course, when we condition on the variables in the odd layers, the variables in the even layers also become conditionally independent. The bipartite structure of the DBM means that we can apply the same equa- tions we have previously used for the conditional distributions of an RBM to determine the conditional distributions in a DBM. The units within a layer are conditionally independent from each other given the values of the neighboring layers, so the distributions over binary variables can be fully described by the Bernoulli parameters, giving the probability of each unit being active.\n\nIn our example with two hidden layers, the activation probabilities are given by  P(v; =1| hn) =o (Wr) (20.26)  Figure 20.3: A deep Boltzmann machine, rearranged to reveal its bipartite graph structure. 661  CHAPTER 20", "7f38aade-aa87-476e-b323-aeb53039049f": "The true GVF then equals the s0 portion of the option\u2019s state-transition model, v\u21e1,\u03b3,C(s) = p(s0|s, ! ), and again this book\u2019s methods could be employed to learn it. Although each of these steps is seemingly natural, putting them all together (including function approximation and other essential components) is quite challenging and beyond the current state of the art. Exercise 17.1 This section has presented options for the discounted case, but discounting is arguably inappropriate for control when using function approximation (Section 10.4). What is the natural Bellman equation for a hierarchical policy, analogous to (17.4), but for the average reward setting (Section 10.3)? What are the two parts of the option model, analogous to (17.2) and (17.3), for the average reward setting? \u21e4 Throughout this book we have written the learned approximate value functions (and the policies in Chapter 13) as functions of the environment\u2019s state.\n\nThis is a signi\ufb01cant limitation of the methods presented in Part I, in which the learned value function was implemented as a table such that any value function could be exactly approximated; that case is tantamount to assuming that the state of the environment is completely observed by the agent", "126190e2-ced6-46c5-b3c2-bf6d2ce15950": "Surprisingly, \u00a3 can be considerably easier to compute for some distributions q. Simple algebra shows that we can rearrange \u00a3 into a much more convenient form:  L(v, 8, q) =log p(v; 8) \u2014 Di (a(h | v)||p(h | \u00a5; 4)) (19.2) qh | v) = log p(v; 8) \u2014 Enng log \u2014\u2014\u2014~ 19.3 qh | v) = log p(v; 8) \u2014 Eq", "dc17d236-2a58-4ce3-9dfa-e1d05007d8c5": "Replacing the indicator function metric in Equation 4.2 with the new at\u2217(t) \u2265 0 yields the experience function for data augmentation (Figure 2): The resulting student-step updates of \u03b8, keeping (\u03b1 = 1, \u03b2 = \u03f5) of supervised MLE, is thus: The metric at\u2217(t) can be de\ufb01ned in various ways, leading to di\ufb00erent augmentation strategies. For example, setting at\u2217(t) \u221d exp{R(t, t\u2217)}, where R(t, t\u2217) is a task-speci\ufb01c evaluation metric such as BLEU for machine translation, results in the reward-augmented maximum likelihood (RAML) algorithm . Besides the manually designed strategies, we can also specify at\u2217(t) as a parameterized transformation process and learn any free parameters thereof automatically (Section 6). Notice the same form of the augmentation experience fdata-aug and the reweighting experience fdata-w, where the similarity metrics both include learnable components (i.e., at\u2217(t) and w(t\u2217), respectively). Thus the same approach to automated data reweighting can also be applied for automated data augmentation, as discussed more in Section 9.2. 4.1.5", "2abea93e-ce5f-45e7-bd19-1cf14f897cbf": "The final state of the learner parameter @r is used to train the meta-learner on the test data Dest. Two implementation details to pay extra attention to:  How to compress the parameter space in LSTM meta-learner? As the meta-learner is modeling parameters of another neural network, it would have hundreds of thousands of variables to learn. https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Log   Following the idea of sharing parameters across coordinates,  To simplify the training process, the meta-learner assumes that the loss \u00a3; and the gradient Vo,_,\u00a3+ are independent. Algorithm 1 Train Meta-Learner  Input: Meta-training set Aneta\u2014train", "922a5750-73d3-415f-b237-04c0904a567f": "The expected update for a state\u2013action pair, s, a, is: The corresponding sample update for s, a, given a sample next state and reward, S0 and R (from the model), is the Q-learning-like update: The di\u21b5erence between these expected and sample updates is signi\ufb01cant to the extent that the environment is stochastic, speci\ufb01cally, to the extent that, given a state and action, many possible next states may occur with various probabilities. If only one next state is possible, then the expected and sample updates given above are identical (taking \u21b5 = 1). If there are many possible next states, then there may be signi\ufb01cant di\u21b5erences. In favor of the expected update is that it is an exact computation, resulting in a new Q(s, a) whose correctness is limited only by the correctness of the Q(s0, a0) at successor states. The sample update is in addition a\u21b5ected by sampling error. On the other hand, the sample update is cheaper computationally because it considers only one next state, not all possible next states.\n\nIn practice, the computation required by update operations is usually dominated by the number of state\u2013action pairs at which Q is evaluated", "ab92b5fe-9f65-4940-bbee-d14fb8efc7e0": "In these settings, the task is to estimate the reliability of data sources that provide assertions of facts and determine which facts are likely true. Many approaches to these problems use probabilistic graphical models that are related to Snorkel\u2019s generative model in that they represent the unobserved truth as a latent variable, e.g., the latent truth model . Our setting differs in that labeling functions assign labels to user-provided data, and they may provide any label or abstain, which we must model. Work on data fusion has also explored how to model user-speci\ufb01ed correlations among data sources . Snorkel automatically identi\ufb01es which correlations among labeling functions to model. Snorkel provides a new paradigm for soliciting and managing weak supervision to create training data sets. In Snorkel, users provide higher-level supervision in the form of labeling functions that capture domain knowledge and resources, without having to carefully manage the noise and con\ufb02icts inherent in combining weak supervision sources.\n\nOur evaluations demonstrate that Snorkel signi\ufb01cantly reduces the cost and dif\ufb01culty of training powerful machine learning models while exceeding prior weak supervision methods and approaching the quality of large, hand-labeled training sets", "3a27bb9a-54a1-453a-8eff-3198fbce819b": "Early stopping therefore has the advantage over weight decay in that it automatically determines the correct amount of regularization while weight decay requires many training experiments with different values of its hyperparameter. 7.9 Parameter Tying and Parameter Sharing  Thus far, in this chapter, when we have discussed adding constraints or penalties to the parameters, we have always done so with respect to a fixed region or point. For example, L? regularization (or weight decay) penalizes model parameters for deviating from the fixed value of zero. Sometimes, however, we may need other ways to express our prior knowledge about suitable values of the model parameters. Sometimes we might not know precisely what values the parameters should take, but we know, from knowledge of the domain and model architecture, that there should be some dependencies between the model parameters. A common type of dependency that we often want to express is that certain parameters should be close to one another.\n\nConsider the following scenario:  https://www.deeplearningbook.org/contents/regularization.html    249  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  we have two models performing the same classification task (with the same set of classes) but with somewhat different input distributions", "bb4a0296-f12a-4b1a-8e17-348f1d29f184": "This meta-algorithm is a general strategy that works  https://www.deeplearningbook.org/contents/regularization.html    well with a variety of training algorithms and ways ot quantitying error on the idation set. Let n be the number of steps between evaluations. Let p be the \u201cpatience,\u201d the number of times to observe worsening validation set  error before giving up. Let 0, be the initial parameters. 0+ 86, i 0 j<0 V+ oO oO on) while j < p do Update @ by running the training algorithm for n steps. iHtitn vu\u2019 + ValidationSetError(@) if v\u2019 <v then j<0 re wot ved else jogjtl end if end while Best parameters are 6*, best number of training steps is 7*. 244  CHAPTER 7.\n\nREGULARIZATION FOR DEEP LEARNING  small compared to the training set or by evaluating the validation set error less frequently and obtaining a lower-resolution estimate of the optimal training time. An additional cost to early stopping is the need to maintain a copy of the best parameters", "cfbb79a0-4c49-4fd4-9456-45150ebad90b": "The concept of state dependence is broad enough to allow for many types of modulating in\ufb02uences on the generation of reward signals.\n\nValue functions provide a further link to psychologists\u2019 concept of motivation. If the most basic motive for selecting an action is to obtain as much reward as possible, for a reinforcement learning agent that selects actions using a value function, a more proximal motive is to ascend the gradient of its value function, that is, to select actions expected to lead to the most highly-valued next states (or what is essentially the same thing, to select actions with the greatest action-values). For these agents, value functions are the main driving force determining the direction of their behavior. Another dimension of motivation is that an animal\u2019s motivational state not only in\ufb02uences learning, but also in\ufb02uences the strength, or vigor, of the animal\u2019s behavior after learning. For example, after learning to \ufb01nd food in the goal box of a maze, a hungry rat will run faster to the goal box than one that is not hungry. This aspect of motivation does not link so cleanly to the reinforcement learning framework we present here, but in the Bibliographical and Historical Remarks section at the end of this chapter we cite several publications that propose theories of behavioral vigor based on reinforcement learning", "ab347657-5b4c-4d01-a0e5-2cb1ac141ae2": "Critics of the Bayesian approach identify the prior as a source of subjective human judgment affecting the predictions. Bayesian methods typically generalize much better when limited training data is available but typically suffer from high computational cost when the number of  dentate scene nlan f6 Tanne  https://www.deeplearningbook.org/contents/ml.html    LIAL CAGHIPLES 1b lalye. 134  CHAPTER 5. MACHINE LEARNING BASICS  Example: Bayesian Linear Regression Here we consider the Bayesian esti- mation approach to learning the linear regression parameters. In linear regression, we learn a linear mapping from an input vector \u00ab \u20ac R\u201d to predict the value of a scalar y \u20ac R. The prediction is parametrized by the vector w \u20ac R\u201d:  j=w's. (5.69)  Given a set of m training samples (X (i) y train) ), we can express the prediction of y over the entire training set as  g train) _ =X (train) ay", "5c75d769-01ce-43d8-a016-6623ab76b085": "Malah, and B. H. Juang . On the application of hidden Markov models for enhancing noisy speech. IEEE Transactions on Acoustics, Speech and Signal Processing 37(12), 1846\u20131856. Everitt, B. S. An Introduction to Latent Variable Models. Chapman and Hall. Faul, A. C. and M. E. Tipping . Analysis of sparse Bayesian learning. In T. G. Dietterich, S. Becker, and Z. Ghahramani (Eds. ), Advances in Neural Information Processing Systems, Volume 14, pp. 383\u2013389. MIT Press. Feynman, R. P., R. B. Leighton, and M. Sands . The Feynman Lectures of Physics, Volume Two. Addison-Wesley. Chapter 19. Fletcher, R", "0766ebe6-4918-461b-b70c-c680726ba545": "This could be done by introducing a different set of basis functions for each component of t, leading to multiple, independent regression problems.\n\nHowever, a more interesting, and more common, approach is to use the same set of basis functions to model all of the components of the target vector so that where y is a K-dimensional column vector, W is an M \u00d7 K matrix of parameters, and \u03c6(x) is an M-dimensional column vector with elements \u03c6j(x), with \u03c60(x) = 1 as before. Suppose we take the conditional distribution of the target vector to be an isotropic Gaussian of the form If we have a set of observations t1, . , tN, we can combine these into a matrix T of size N \u00d7 K such that the nth row is given by tT input vectors x1, . , xN into a matrix X. The log likelihood function is then given by As before, we can maximize this function with respect to W, giving If we examine this result for each target variable tk, we have where tk is an N-dimensional column vector with components tnk for n = 1, . N", "790f0a5f-c15b-474e-8301-ba242a93b142": "Baras, D., Meir, R. .\n\nReinforcement learning, spike-time-dependent plasticity, and the Barnard, E. Temporal-di\u21b5erence methods and Markov models. IEEE Transactions on stochastic factorization. In Advances in Neural Information Processing Systems 24 , pp. 720\u2013728. Curran Associates, Inc. learn. Technical report, Research School of Information Sciences and Engineering, Australian National University. Bartlett, P. L., Baxter, J. A biologically plausible and locally optimal learning algorithm for spiking neurons. Rapport technique, Australian National University. Barto, A. G. Learning by statistical cooperation of self-interested neuron-like computing Barto, A. G. From chemotaxis to cooperativity: Abstract exercises in neuronal learning strategies. In R. Durbin, R. Maill and G. Mitchison (Eds. ), The Computing Neuron, pp. 73\u201398", "e98eaad1-dc16-4b80-9df8-951472787fa0": "Here, we focus on the use of these units as outputs of the model, but in principle they can be used internally as well. We revisit these units with additional detail about their use as hidden units in section 6.3. Throughout this section, we suppose that the feedforward network provides a set of hidden features defined by h = f(x;@). The role of the output layer is then  https://www.deeplearningbook.org/contents/mlp.html    to provide some additional transformation from the features to complete the task that the network must perform. 177  CHAPTER 6.\n\nDEEP FEEDFORWARD NETWORKS  6.2.2.1 Linear Units for Gaussian Output Distributions  One simple kind of output unit is based on an affine transformation with no nonlinearity. These are often just called linear units. Given features h, a layer of linear output units produces a vector y = W'hh+ob. Linear output layers are often used to produce the mean of a conditional Gaussian distribution:  Ply |e) =N(y; 9, D). (6.17) Maximizing the log-likelihood is then equivalent to minimizing the mean squared error", "c16f8750-0c06-4e8f-81f5-b43188168cc3": "We therefore need a way to modify the support vector machine so as to allow some of the training points to be misclassi\ufb01ed. From (7.19) we see that in the case of separable classes, we implicitly used an error function that gave in\ufb01nite error if a data point was misclassi\ufb01ed and zero error if it was classi\ufb01ed correctly, and then optimized the model parameters to maximize the margin. We now modify this approach so that data points are allowed to be on the \u2018wrong side\u2019 of the margin boundary, but with a penalty that increases with the distance from that boundary. For the subsequent optimization problem, it is convenient to make this penalty a linear function of this distance. To do this, we introduce slack variables, \u03ben \u2a7e 0 where n = 1, . , N, with one slack variable for each training data point . These are de\ufb01ned by \u03ben = 0 for data points that are on or inside the correct margin boundary and \u03ben = |tn \u2212 y(xn)| for other points", "0cedcffc-d3bf-42ab-8dde-3e83f2e3f1ac": "Projected equation methods for approximate solution of large linear systems. Journal of Computational and Applied Mathematics, 227(1):27\u201350. Bhat, N., Farias, V., Moallemi, C. C. Non-parametric approximate dynamic programming via the kernel method. In Advances in Neural Information Processing Systems 25 , pp. 386\u2013394.\n\nCurran Associates, Inc. Bhatnagar, S., Sutton, R., Ghavamzadeh, M., Lee, M. Natural actor\u2013critic algorithms. Biermann, A. W., Fair\ufb01eld, J. R. C., Beres, T. R. Signature table systems and learning. IEEE Transactions on Systems, Man, and Cybernetics, 12(5):635\u2013648. Bishop, C. M. Neural Networks for Pattern Recognition. Clarendon, Oxford. Bishop, C. M. Pattern Recognition and Machine Learning. Springer Science + Business Blodgett, H. C", "b5ca562e-7120-4ce7-95a1-b773491b0f7a": "It can be very difficult (o establish that models are being compared fairly.\n\nFor example, suppose we use AIS to estimate log Z in order to compute log p(x) \u2014 log Z for a new model we have just invented. A computationally economical implementation of AIS may fail o find several modes of the model distribution and underestimate Z, which will result in us overestimating log p(x). It can thus be difficult to tell whether a high ikelihood estimate is the result of a good model or a bad AIS implementation. Other fields of machine learning usually allow for some variation in the pre-  processing of the data. For example, when comparing the accuracy of object  713  https://www.deeplearningbook.org/contents/generative_models.html    CHAPTER 20. DEEP GENERATIVE MODELS  recognition algorithms, it is usually acceptable to preprocess the input images slightly differently for each algorithm based on what kind of input requirements it has. Generative modeling is different because changes in preprocessing, even very small and subtle ones, are completely unacceptable. Any change to the input data changes the distribution to be captured and fundamentally alters the task", "082d0938-b86f-4db9-8a70-27695a30081f": "For the inference, we resize the given image to 256x256, and take a single center crop of 224x224. For linear evaluation, we follow similar procedure as \ufb01ne-tuning (described in Appendix B.5), except that a larger learning rate of 1.6 (following LearningRate = 0.1 \u00d7 BatchSize/256) and longer training of 90 epochs. Alternatively, using LARS optimizer with the pretraining hyper-parameters also yield similar results.\n\nFurthermore, we \ufb01nd that attaching the linear classi\ufb01er on top of the base encoder (with a stop_gradient on the input to linear classi\ufb01er to prevent the label information from in\ufb02uencing the encoder) and train them simultaneously during the pretraining achieves similar performance. B.7. Correlation Between Linear Evaluation and Fine-Tuning Here we study the correlation between linear evaluation and \ufb01ne-tuning under different settings of training step and network architecture. A Simple Framework for Contrastive Learning of Visual Representations of labels bene\ufb01ts more from training longer", "f6b9deab-db92-4937-a2a5-e24afa0215c9": "(Saying \u201cany function\u201d is not exactly correct because the function has to be mathematically well-behaved, but we skip this technicality here.) Alternatively, it is possible to use just sine features, linear combinations of which are always odd functions, that is functions that are anti-symmetric about the origin. But it is generally better to keep just the cosine features because \u201chalf-even\u201d functions tend to be easier to approximate than \u201chalf-odd\u201d functions because the latter are often discontinuous at the origin. Of course, this does not rule out using both sine and cosine features to approximate over the interval , which might be advantageous in some circumstances. Following this logic and letting \u2327 = 2 so that the features are de\ufb01ned over the half-\u2327 for i = 0, . , n. Figure 9.3 shows one-dimensional Fourier cosine features xi, for i = 1, 2, 3, 4; x0 is a constant function. This same reasoning applies to the Fourier cosine series approximation in the multiSuppose each state s corresponds to a vector of k numbers, s = (s1, s2, ..., sk)>, with each si 2", "edaca6cc-5337-422e-8359-eaa0b1fa24f4": "If we want to sample only every s pixels in each direction in the output, then we can define a downsampled convolution function c such that  Zijk = (KV, S)i jk = S-  - (9.8)  lym,n  We refer to s as the stride of this downsampled convolution. It is also possible o define a separate stride for each direction of motion. See figure 9.12 for an illustration. One essential feature of any convolutional network implementation is the ability o implicitly zero pad the input V to make it wider.\n\nWithout this feature, the width of the representation shrinks by one pixel less than the kernel width at each layer. Zero padding the input allows us to control the kernel width and he size of the output independently. Without zero padding, we are forced to choose between shrinking the spatial extent of the network rapidly and using small  kernels\u2014both scenarios that significantly limit the expressive power of the network. See figure 9.13 for an example. Three special cases of the zero-padding setting are worth mentioning", "7ae1ccfd-3744-4f6f-b1d5-6985734486b0": "The committee prediction is given by This procedure is known as bootstrap aggregation or bagging . Suppose the true regression function that we are trying to predict is given by h(x), so that the output of each of the models can be written as the true value plus an error in the form ym(x) = h(x) + \u03f5m(x). (14.8) The average sum-of-squares error then takes the form where Ex denotes a frequentist expectation with respect to the distribution of the input vector x. The average error made by the models acting individually is therefore Similarly, the expected error from the committee (14.7) is given by If we assume that the errors have zero mean and are uncorrelated, so that This apparently dramatic result suggests that the average error of a model can be reduced by a factor of M simply by averaging M versions of the model. Unfortunately, it depends on the key assumption that the errors due to the individual models are uncorrelated. In practice, the errors are typically highly correlated, and the reduction in overall error is generally small", "5183f3f4-6dda-46d2-8ca1-d682e926a1be": "However, numerical differentiation plays an important role in practice, because a comparison of the derivatives calculated by backpropagation with those obtained using central differences provides a powerful check on the correctness of any software implementation of the backpropagation algorithm. When training networks in practice, derivatives should be evaluated using backpropagation, because this gives the greatest accuracy and numerical ef\ufb01ciency. However, the results should be compared with numerical differentiation using (5.69) for some test cases in order to check the correctness of the implementation.\n\nWe have seen how the derivatives of an error function with respect to the weights can be obtained by the propagation of errors backwards through the network. The technique of backpropagation can also be applied to the calculation of other derivatives. Here we consider the evaluation of the Jacobian matrix, whose elements are given by the derivatives of the network outputs with respect to the inputs where each such derivative is evaluated with all other inputs held \ufb01xed. Jacobian matrices play a useful role in systems built from a number of distinct modules, as illustrated in Figure 5.8. Each module can comprise a \ufb01xed or adaptive function, which can be linear or nonlinear, so long as it is differentiable", "c4009cc3-0e33-49e2-a00d-af90503d51f3": "We have seen in earlier chapters that the maximum likelihood approach is most effective when the number of data points is large in relation to the number of parameters.\n\nHere we note that a hidden Markov model can be trained effectively, using maximum likelihood, provided the training sequence is suf\ufb01ciently long. Alternatively, we can make use of multiple shorter sequences, which requires a straightforward modi\ufb01cation of the hidden Markov model EM algorithm. In the case of left-to-right Exercise 13.12 models, this is particularly important because, in a given observation sequence, a given state transition corresponding to a nondiagonal element of A will seen at most once. Another quantity of interest is the predictive distribution, in which the observed data is X = {x1, . , xN} and we wish to predict xN+1, which would be important for real-time applications such as \ufb01nancial forecasting. Again we make use of the sum and product rules together with the conditional independence properties (13.29) and (13.31) giving which can be evaluated by \ufb01rst running a forward \u03b1 recursion and then computing the \ufb01nal summations over zN and zN+1", "8892da8c-cd4d-4899-a17e-59b34764cad6": "Algorithm  8.9 The conjugate gradient method  Require: Initial parameters 05 Require: Training set of m examples  Initialize  po=0  Initialize go = 0 Initialize t = 1 while stopping criterion not met do  Initializ Compu Compu (Nonlin  e the gradient g; = 0  e gradient: gy \u2014 \u00a3Ve dO; L(f(e; 6), y)  e Bi = (a geaa (Polak-Ribi\u00e9re)  ear conjugate gradient: optionally reset % to zero, for example if \u00a2 is  a multiple of some constant k, such as k = 5)  Compu  fe search direction: p\u00a2 = \u2014gi+ Sip t\u20141  Perform line search to find: \u00ab* = argmin, 4 ey L( f(a; 0, + ep,), y) (On a truly quadratic cost function, analytically solve for e* rather than  explicit  y searching for it)  nee nw", "0eddf24d-73ee-4b35-bcca-532aa100075e": "Another proposal by Daw, Courville, and Touretzky  is that the brain\u2019s TD system uses representations produced by statistical modeling carried out in sensory cortex rather than simpler representations based on raw sensory input. Ludvig, Sutton, and Kehoe  found that TD learning with a microstimulus (MS) representation (Figure 14.1) \ufb01ts the activity of dopamine neurons in the early-reward and other situations better than when a CSC representation is used. Pan, Schmidt, Wickens, and Hyland  found that even with the CSC representation, prolonged eligibility traces improve the \ufb01t of the TD error to some aspects of dopamine neuron activity.\n\nIn general, many \ufb01ne details of TD-error behavior depend on subtle interactions between eligibility traces, discounting, and stimulus representations. Findings like these elaborate and re\ufb01ne the reward prediction error hypothesis without refuting its core claim that the phasic activity of dopamine neurons is well characterized as signaling TD errors", "5a1f25df-b3ec-4ac6-8ab5-a869a1db6cfa": "If we are lucky, there may be some regularity in the target function, besides being smooth. For example, a convolutional network with max pooling can recognize an object regardless of its location in the image, even though spatial translation of the object may not correspond to smooth transformations in the input space.\n\nLet us examine a special case of a distributed representation learning algorithm, which extracts binary features by thresholding linear functions of the input. Each binary feature in this representation divides R\u00a2 into a pair of half-spaces, as illustrated in figure 15.7. The exponentially large number of intersections of n of the corresponding half-spaces determines how many regions this distributed representation learner can distinguish. How many regions are generated by an arrangement of n hyperplanes in R\u00a2? By applying a general result concerning the intersection of hyperplanes , one can show  that the number of regions this binary feature representation can distinguish is  Ss (\") = O(n?). (15.4)  j=0  Therefore, we see a growth that is exponential in the input size and polynomial in the number of hidden units", "5f997a38-4421-478d-af49-599e8c910575": "Pattern Recogn Lett. 2008;30(2):88-97. Marius C, Mohamed O, Sebastian R, Timo R, Markus E, Rodrigo B, Uwe F, Stefan R, Bernt S. The cityscape dataset for semantic urban scene understanding. In: CVPR; 2016.\n\nEsteban R, Sherry M, Andrew S, Saurabh S, Yutaka LS, Jie T, Quoc VL, Alexey K. Large-scale evolution of image clas- sifiers. In: Proceedings of the 34th international conference on machine learning (ICML'17). 2017  Esteban R, Alok A, Yanping H, Quoc VL. Regularized evolution for image classifier architecture search. arXiv pre- print. 2018. Tim S, Jonathan H, Xi C, Szymon \u00a7, Ilya S. Evolution strategies as a scalable alternative to reinforcement learning. arXiv e-prints. 2017. Horia M, Aurelia G, Benjamin R", "488a2aa3-0b59-4c5b-8f7d-9e65bb07fa10": "APPLICATIONS  database is intended to convey commonsense knowledge about everyday life or expert knowledge about an application area to an artificial intelligence system, we call the database a knowledge base.\n\nKnowledge bases range from general ones like Freebase, OpenCyc, WordNet, Wikibase, and so forth, to more specialized knowledge bases like GeneOntology.\u201d Representations for entities and relations can be learned by considering each triplet in a knowledge base as a training example and maximizing a training objective that captures their joint distribution . In addition to training data, we also need to define a model family to train. A common approach is to extend neural language models to model entities and relations. Neural language models learn a vector that provides a distributed representation of each word. They also learn about interactions between words, such as which word is likely to come after a sequence of words, by learning functions of these vectors. We can extend this approach to entities and relations by learning an embedding vector for each relation. In fact, the parallel between modeling language and modeling knowledge encoded as relations is so close that researchers have trained representations of such entities by using both knowledge bases and natural language sentences , or by combining data from multiple relational databases", "c7f997d6-a19e-46dc-aa7c-f13f81f680d2": "For the purpose of visualization, this figure uses a somewhat simplified concept of distance\u2014the spurious mode is far from the correct mode along the  number line in R. This corresponds to a Markov chain based on making local moves with a single x variable in R. For most deep probabilistic models, the Markov chains are based on Gibbs sampling and can make nonlocal moves of individual variables but cannot move all the variables simultaneously. For these problems, it is usually better to consider the edit distance between modes, rather than the Euclidean distance.\n\nHowever, edit distance in a high-dimensional space is difficult to depict in a 2-D plot. distribution that are far from the data distribution will not be visited by Markov chains initialized at training points, unless k is very large. Carreira-Perpinan and Hinton  showed experimentally that the CD estimator is biased for RBMs and fully visible Boltzmann machines, in that it converges to different points than the maximum likelihood estimator. They argue that because the bias is small, CD could be used as an inexpensive way to initialize a model that could later be fine-tuned via more expensive MCMC methods", "2acf82c6-60bb-46b9-b1b2-5f36636381b1": "That is, each time step will have a di\u21b5erent \u03bb and \u03b3, denoted \u03bbt and \u03b3t.\n\nWe change notation now so that \u03bb : S\u21e5 A ! is now a function from states and actions to the unit interval such that \u03bbt .= \u03bb(St, At), and similarly, \u03b3 : S ! is a function from states to the unit interval such that \u03b3t Introducing the function \u03b3, the termination function, is particularly signi\ufb01cant because it changes the return, the fundamental random variable whose expectation we seek to = Rt+1 + \u03b3t+1Rt+2 + \u03b3t+1\u03b3t+2Rt+3 + \u03b3t+1\u03b3t+2\u03b3t+3Rt+4 + \u00b7 \u00b7 \u00b7 where, to assure the sums are \ufb01nite, we require that Q1 all t. One convenient aspect of this de\ufb01nition is that it enables the episodic setting and its algorithms to be presented in terms of a single stream of experience, without special terminal states, start distributions, or termination times. An erstwhile terminal state becomes a state at which \u03b3(s)=0 and which transitions to the start distribution", "acd920f8-2ad2-406e-86c7-7ffdb8aef920": "If the environment\u2019s dynamics are completely known, then (4.4) is a system of |S| simultaneous linear equations in |S| unknowns (the v\u21e1(s), s 2 S). In principle, its solution is a straightforward, if tedious, computation. For our purposes, iterative solution methods are most suitable. Consider a sequence of approximate value functions v0, v1, v2, . ., each mapping S+ to R (the real numbers).\n\nThe initial approximation, v0, is chosen arbitrarily (except that the terminal state, if any, must be given value 0), and each successive approximation is obtained by using the Bellman equation for v\u21e1 (4.4) as an update rule: for all s 2 S. Clearly, vk = v\u21e1 is a \ufb01xed point for this update rule because the Bellman equation for v\u21e1 assures us of equality in this case. Indeed, the sequence {vk} can be shown in general to converge to v\u21e1 as k ! 1 under the same conditions that guarantee the existence of v\u21e1. This algorithm is called iterative policy evaluation", "866f46f5-a562-40af-82f4-5a3858b1b9a7": "This result is easily generalized to the multiclass case where again the maximum likelihood estimate of the prior probability associated with class Ck is given by the fraction of the training set points assigned to that class. Exercise 4.9 Now consider the maximization with respect to \u00b51. Again we can pick out of the log likelihood function those terms that depend on \u00b51 giving Setting the derivative with respect to \u00b51 to zero and rearranging, we obtain which is simply the mean of all the input vectors xn assigned to class C1. By a similar argument, the corresponding result for \u00b52 is given by which again is the mean of all the input vectors xn assigned to class C2.\n\nFinally, consider the maximum likelihood solution for the shared covariance matrix \u03a3. Picking out the terms in the log likelihood function that depend on \u03a3, we have Using the standard result for the maximum likelihood solution for a Gaussian distribution, we see that \u03a3 = S, which represents a weighted average of the covariance matrices associated with each of the two classes separately. This result is easily extended to the K class problem to obtain the corresponding maximum likelihood solutions for the parameters in which each class-conditional density is Gaussian with a shared covariance matrix", "669234b0-02fa-44c2-8fef-f3cf29b2e3e6": "Likewise, an event that is guaranteed to happen has probability 1, and no state can have a greater chance of occurring. xex P(x) = 1. We refer to this property as being normalized.\n\nWithout this property, we could obtain probabilities greater than one by computing probability of one of many events occurring. https://www.deeplearningbook.org/contents/prob.html    For example, consider a single discrete random variable x with k different states. We can place a uniform distribution on x\u2014that is, make each of its  states equally likely\u2014by setting its PMF to P(x =2;) == (3.1)  for all 7. We can see that this fits the requirements for a probability mass function. The value k is positive because k is a positive integer. We also see that  MPesny= p= f=1 (3.2)  so the distribution is properly normalized. 55  CHAPTER 3. PROBABILITY AND INFORMATION THEORY  3.3.2 Continuous Variables and Probability Density Functions  When working with continuous random variables, we describe probability distri- butions using a probability density function (PDF) rather than a probability mass function", "99fadead-6b1a-4b05-ab31-8a740bd85bbd": "Adams\u2019 question was whether devaluation would e\u21b5ect the rate of lever-pressing for the overtrained rats less than it would for the non-overtrained rats, which would be evidence that extended training reduces sensitivity to outcome devaluation.\n\nIt turned out that devaluation strongly decreased the lever-pressing rate of the non-overtrained rats. For the overtrained rats, in contrast, devaluation had little e\u21b5ect on their lever-pressing; in fact, if anything, it made it more vigorous. (The full experiment included control groups showing that the di\u21b5erent amounts of training did not by themselves signi\ufb01cantly e\u21b5ect lever-pressing rates after learning.) This result suggested that while the non-overtrained rats were acting in a goal-directed manner sensitive to their knowledge of the outcome of their actions, the overtrained rats had developed a lever-pressing habit. Viewing this and other results like it from a computational perspective provides insight as to why one might expect animals to behave habitually in some circumstances, in a goal-directed way in others, and why they shift from one mode of control to another as they continue to learn", "33459576-8a33-4e0f-9983-8ff0e9160c33": "The global sentence representation Eo(x) is computed by applying a mean-over-time pooling layer on the token representations F(x) = {FP (x) \u20ac R7}L_,  Since the mutual information estimation is generally intractable for continuous and high- dimensional random variables, IS-BERT relies on the Jensen-Shannon estimator  to maximize the mutual information between E(x) and FLO (x). 13S? (Ff?\n\n(x); \u20aco(x)) = Ex p \u2014 E,_ pp plsp(Tu(Fy\u201d &\"); Eo()))]  https://lilianweng.github.io/posts/2021-05-31-contrastive/  Contrastive Representation Learning | Lil'Log   where T,, : F x E \u2014\u00bb Risa learnable network with parameters w, generating discriminator scores. The negative sample x\u2019 is sampled from the distribution P=P.And sp(x) = log(1 + e\u201d) is the softplus activation function", "4ff1f7c9-5047-467a-9113-2549afe7b5dd": "The average value of some function f(x) under a probability distribution p(x) is called the expectation of f(x) and will be denoted by E. For a discrete distribution, it is given by so that the average is weighted by the relative probabilities of the different values of x. In the case of continuous variables, expectations are expressed in terms of an integration with respect to the corresponding probability density In either case, if we are given a \ufb01nite number N of points drawn from the probability distribution or probability density, then the expectation can be approximated as a We shall make extensive use of this result when we discuss sampling methods in Chapter 11. The approximation in (1.35) becomes exact in the limit N \u2192 \u221e.\n\nSometimes we will be considering expectations of functions of several variables, in which case we can use a subscript to indicate which variable is being averaged over, so that for instance Ex (1.36) denotes the average of the function f(x, y) with respect to the distribution of x. Note that Ex will be a function of y. We can also consider a conditional expectation with respect to a conditional distribution, so that Ex = \ufffd with an analogous de\ufb01nition for continuous variables", "dd52cff8-843b-4d63-86f0-f6f9cfbb8d33": "Sampling provides a flexible way to approximate many sums and integrals at  https://www.deeplearningbook.org/contents/monte_carlo.html    CHAPTER 17.\n\nMONTE CARLO METHODS  reduced cost. Sometimes we use this to provide a significant speedup to a costly but tractable sum, as in the case when we subsample the full training cost with minibatches. In other cases, our learning algorithm requires us to approximate an intractable sum or integral, such as the gradient of the log partition function of an undirected model. In many other cases, sampling is actually our goal, in the sense that we want to train a model that can sample from the training distribution. 17.1.2. Basics of Monte Carlo Sampling  When a sum or an integral cannot be computed exactly (for example, the sum has an exponential number of terms, and no exact simplification is known), it is often possible to approximate it using Monte Carlo sampling. The idea is to view the sum or integral as if it were an expectation under some distribution and to approximate the expectation by a corresponding average", "5b46ce83-dc96-4a7b-9430-62bf4f02c233": "Similarly, the full generation model p\u03b8(a|s) in Eq. (1) that applies softmax to f\u03b8 now precisely corresponds to the policy \u03c0\u03b8 induced from Q\u03b8(s, a). That is, We could further gain even more intuitive interpretation of the above generation policy \u03c0\u2217 from the lens of advantage function .\n\nSpeci\ufb01cally, in SQL, the optimal statevalue function is the log-normalizer of the optimal Q-values . This allows a more concise form of Eq. (3): where A\u2217 is the optimal advantage function. The equation says that, in the proposed text generation SQL formulation, the optimal policy generates token a in state s according to the token\u2019s advantage. Vanilla training based on the Bellman temporal consistency can suffer from the instability and inef\ufb01ciency issues similar to the conventional Qlearning (\u00a72), as we discuss more in the appendix (\u00a7A.3.2). Fortunately, our SQL formulation allows us to import latest advances of RL techniques to overcome the dif\ufb01culties. Speci\ufb01cally, we adapt the uni\ufb01ed path consistency learning (PCL) that has excelled in game control", "9c4277ab-4be9-4074-87fc-52228132bde8": "In some contexts, more theoretically pleasing justifications are available, but  188  https://www.deeplearningbook.org/contents/mlp.html    CHAPTER 6. DEEP FEEDFORWARD NETWORKS  these usually do not apply to neural network training. The important point is that in practice one can safely disregard the nondifferentiability of the hidden unit activation functions described below.\n\nUnless indicated otherwise, most hidden units can be described as accepting a vector of inputs 2, computing an affine transformation z = W's + b, and then applying an element-wise nonlinear function g(z). Most hidden units are distinguished from each other only by the choice of the form of the activation function g(z). 6.3.1 Rectified Linear Units and Their Generalizations  Rectified linear units use the activation function g(z) = max{0, z}. These units are easy to optimize because they are so similar to linear units. The only difference between a linear unit and a rectified linear unit is that a rectified linear unit outputs zero across half its domain. This makes the derivatives through a rectified linear unit remain large whenever the unit is active", "96815e52-0b75-4db2-b7e1-bee1f936d715": "\u2022 We can derive an EM algorithm for PCA that is computationally efficient in situations where only a few leading eigenvectors are required and that avoids having to evaluate the data covariance matrix as an intermediate step. \u2022 The combination of a probabilistic model and EM allows us to deal with missing values in the data set. \u2022 Mixtures of probabilistic PCA models can be formulated in a principled way and trained using the EM algorithm. \u2022 Probabilistic PCA forms the basis for a Bayesian treatment of PCA in which the dimensionality of the principal subspace can be found automatically from the data. \u2022 The existence of a likelihood function allows direct comparison with other probabilistic density models. By contrast, conventional PCA will assign a low reconstruction cost to data points that are close to the principal subspace even if they lie arbitrarily far from the training data. \u2022 Probabilistic PCA can be used to model class-conditional densities and hence be applied to classification problems.\n\nThe probabilistic PCA model can be run generatively to provide samples from the distribution. This formulation of PCA as a probabilistic model was proposed independently by Tipping and Bishop  and by Roweis", "ea570ab6-4456-426d-863d-db9018bab585": ", N, together with corresponding target values {tn}, the goal is to predict the value of t for a new value of x. In the simplest approach, this can be done by directly constructing an appropriate function y(x) whose values for new inputs x constitute the predictions for the corresponding values of t. More generally, from a probabilistic perspective, we aim to model the predictive distribution p(t|x) because this expresses our uncertainty about the value of t for each value of x. From this conditional distribution we can make predictions of t, for any new value of x, in such a way as to minimize the expected value of a suitably chosen loss function. As discussed in Section 1.5.5, a common choice of loss function for real-valued variables is the squared loss, for which the optimal solution is given by the conditional expectation of t. Although linear models have signi\ufb01cant limitations as practical techniques for pattern recognition, particularly for problems involving input spaces of high dimensionality, they have nice analytical properties and form the foundation for more sophisticated models to be discussed in later chapters.\n\nThe simplest linear model for regression is one that involves a linear combination of the input variables where x = (x1,", "f4d0bcd0-1028-4b87-95e9-0130bf2caf56": "Likewise, given the odd layers, the even layers can be sampled simultaneously and independently as a block. Efficient sampling is especially important for training with the stochastic maximum likelihood algorithm.\n\n20.4.1 Interesting Properties  Deep Boltzmann machines have many interesting properties. DBMs were developed after DBNs. Compared to DBNs, the posterior distribu- tion P(h | v) is simpler for DBMs. Somewhat counterintuitively, the simplicity of this posterior distribution allows richer approximations of the posterior. In the case of the DBN, we perform classification using a heuristically motivated approximate inference procedure, in which we guess that a reasonable value for the mean field expectation of the hidden units can be provided by an upward pass through the network in an MLP that uses sigmoid activation functions and the same weights as the original DBN. Any distribution Q(h) can be used to obtain a variational lower bound on the log-likelihood. This heuristic procedure therefore enables us to obtain such a bound. Yet the bound is not explicitly optimized in any way, so it may be far from tight", "a5a72c70-6321-46b6-887b-748b1defeac1": "The zt vector is in fact a dutch-style eligibility trace. It is initialized to z0 = x0 and then updated according to which is the dutch trace for the case of \u03b3\u03bb=1 (cf. Eq. 12.11). The at auxilary vector is initialized to a0 = w0 and then updated according to The auxiliary vectors, at and zt, are updated on each time step t < T and then, at time T when G is observed, they are used in (12.14) to compute wT . In this way we achieve exactly the same \ufb01nal result as the MC/LMS algorithm that has poor computational properties (12.13), but now with an incremental algorithm whose time and memory complexity per step is O(d). This is surprising and intriguing because the notion of an eligibility trace (and the dutch trace in particular) has arisen in a setting without temporal-di\u21b5erence (TD) learning . It seems eligibility traces are not speci\ufb01c to TD learning at all; they are more fundamental than that. The need for eligibility traces seems to arise whenever one tries to learn long-term predictions in an e\ufb03cient manner", "72a6fbc3-7d53-4ef2-806c-88d1c95a0ff0": "Although linear learning rate scaling is popular with SGD/Momentum optimizer, we \ufb01nd a square root learning rate scaling is more desirable with LARS optimizer. With square root learning rate scaling, we have LearningRate = 0.075 \u00d7 \u221a BatchSize, instead of LearningRate = 0.3 \u00d7 BatchSize/256 in the linear scaling case, but the learning rate is the same under both scaling methods when batch size of 4096 (our default batch size). A comparison is presented in Table B.1, where we observe that square root learning rate scaling improves the performance for models trained with small batch sizes and in smaller number of epochs.\n\nWe also train with larger batch size (up to 32K) and longer (up to 3200 epochs), with the square root learning rate scaling. A shown in Figure B.2, the performance seems to saturate with a batch size of 8192, while training longer can still signi\ufb01cantly improve the performance. A Simple Framework for Contrastive Learning of Visual Representations B.2", "ebe156c6-2884-4e03-8bd2-bebf3626fdcb": "+ Batch normalization  is another regularization technique that normalizes the set of activations in a layer. Normalization works by subtracting the batch mean from each activation and dividing by the batch standard deviation. This normalization technique, along with standardization, is a standard technique in the preprocessing of pixel values. + Transfer Learning  is another interesting paradigm to prevent overfitting.\n\nTransfer Learning works by training a network on a big dataset such as ImageNet {12] and then using those weights as the initial weights in a new classification task. Typically, just the weights in convolutional layers are copied, rather than the entire network including fully-connected layers. This is very effective since many image datasets share low-level spatial characteristics that are better learned with big data. Understanding the relationship between transferred data domains is an ongoing  research task . Yosinski et al. find that transferability is negatively affected  primarily by the specialization of higher layer neurons and difficulties with splitting co-adapted neurons. + Pretraining  is conceptually very similar to transfer learning. In Pretraining, the  network architecture is defined and then trained on a big dataset such as ImageNet {12]", "d263342a-ca99-4db0-8794-f7ce19a25caa": "We have seen that both the E and the M steps of the EM algorithm are increasing the value of a well-de\ufb01ned bound on the log likelihood function and that the complete EM cycle will change the model parameters in such a way as to cause the log likelihood to increase (unless it is already at a maximum, in which case the parameters remain unchanged). We can also use the EM algorithm to maximize the posterior distribution p(\u03b8|X) for models in which we have introduced a prior p(\u03b8) over the parameters. To see this, we note that as a function of \u03b8, we have p(\u03b8|X) = p(\u03b8, X)/p(X) and so Making use of the decomposition (9.70), we have where ln p(X) is a constant. We can again optimize the right-hand side alternately with respect to q and \u03b8. The optimization with respect to q gives rise to the same Estep equations as for the standard EM algorithm, because q only appears in L(q, \u03b8).\n\nThe M-step equations are modi\ufb01ed through the introduction of the prior term ln p(\u03b8), which typically requires only a small modi\ufb01cation to the standard maximum likelihood M-step equations", "7513111d-7b87-419e-8646-f771fedb1e9f": "We shall see in later chapters how to construct models for combining data that do not require the conditional independence assumption (1.84). So far, we have discussed decision theory in the context of classi\ufb01cation problems. We now turn to the case of regression problems, such as the curve \ufb01tting example discussed earlier. The decision stage consists of choosing a speci\ufb01c estiSection 1.1 mate y(x) of the value of t for each input x. Suppose that in doing so, we incur a loss L(t, y(x)). The average, or expected, loss is then given by A common choice of loss function in regression problems is the squared loss given by L(t, y(x)) = {y(x) \u2212 t}2.\n\nIn this case, the expected loss can be written Our goal is to choose y(x) so as to minimize E. If we assume a completely \ufb02exible function y(x), we can do this formally using the calculus of variations to Appendix D Solving for y(x), and using the sum and product rules of probability, we obtain which is the conditional average of t conditioned on x and is known as the regression function. This result is illustrated in Figure 1.28", "11a4ed17-b733-41d3-89cb-99d75ddd757d": "The mirror axis of symmetry is given by the hyperplane defined by the weights and bias of the unit. A function computed on top of that unit (the green decision surface) will be a mirror image of a simpler pattern across that axis of symmetry. (Center) The function can be obtained by folding the space around the axis of symmetry. (Right) Another repeating pattern can be folded on top of the first (by another downstream unit) to obtain another symmetry (which is now repeated four times, with two hidden layers). Figure reproduced with  https://www.deeplearningbook.org/contents/mlp.html   permission from Montufar et al. were first proved for models that do not resemble the continuous, differentiable neural networks used for machine learning but have since been extended to these models. The first results were for circuits of logic gates .\n\nLater work extended these results to linear threshold units with nonnegative weights , and then to networks with continuous-valued activations . Many modern neural networks use rectified linear units. Leshno ef al", "e46fefd4-c696-42e3-a31f-43bcec58a997": "Continuation methods have been extremely successful in recent years. See Mobahi and Fisher  for an overview of recent literature, especially for AI applications. Continuation methods traditionally were mostly designed with the goal of overcoming the challenge of local minima. Specifically, they were designed to  https://www.deeplearningbook.org/contents/optimization.html    reach a global minimum despite the presence of many local minima. To do so, these continuation methods would construct easier cost functions by \u201cblurring\u201d the original cost function. This blurring operation can be done by approximating  J (0) = Ey ivor.o(0)J(8\u2019) (8.40)  via sampling. The intuition for this approach is that some nonconvex functions become approximately convex when blurred. In many cases, this blurring preserves  323  CHAPTER 8.\n\nOPTIMIZATION FOR TRAINING DEEP MODELS  enough information about the location of a global minimum that we can find the global minimum by solving progressively less-blurred versions of the problem. This approach can break down in three different ways", "fa0759ee-8e85-436c-9296-6fd8115f3851": "An important motivation for such models is that many data sets have the property that the data points all lie close to a manifold of much lower dimensionality than that of the original data space.\n\nTo see why this might arise, consider an artificial data set constructed by taking one of the off-line digits, represented by a 64 x 64 pixel grey-level image, and embedding it in a larger image of size 100 x 100 by padding with pixels having the value zero (corresponding to white pixels) in which the location and orientation of the digit is varied at random, as illustrated in Figure 12.1. Each of the resulting images is represented by a point in the 100 x 100 = 10, OOO-dimensional data space. However, across a data set of such images, there are only three degrees offreedom of variability, corresponding to the vertical and horizontal translations and the rotations. The data points will therefore live on a subspace of the data space whose intrinsic dimensionality is three. Note that the manifold will be nonlinear because. for instance. if we translate the digit past a particular pixel, that pixel value will go from zero (white) 10 one (black) and back to zero again", "56f21784-b562-431c-a0df-6ae72648fbd1": "Because there is a unique path coming into that state we can trace the path back to step N \u2212 1 to see what state it occupied at that time, and so on back through the lattice to the state n = 1. The basic hidden Markov model, along with the standard training algorithm based on maximum likelihood, has been extended in numerous ways to meet the requirements of particular applications. Here we discuss a few of the more important examples. We see from the digits example in Figure 13.11 that hidden Markov models can be quite poor generative models for the data, because many of the synthetic digits look quite unrepresentative of the training data. If the goal is sequence classi\ufb01cation, there can be signi\ufb01cant bene\ufb01t in determining the parameters of hidden Markov models using discriminative rather than maximum likelihood techniques. Suppose we have a training set of R observation sequences Xr, where r = 1, . , R, each of which is labelled according to its class m, where m = 1, . .\n\n, M", "f84eb227-43b1-4f9a-9008-37e21f93c914": "e All constraints on both # and the KKT multipliers are satisfied. e The inequality constraints exhibit \u201ccomplementary slackness\u201d: a \u00a9 h(a) = 0. For more information about the KKT approach, see Nocedal and Wright . 93  CHAPTER 4.\n\nNUMERICAL COMPUTATION  4.5 Example: Linear Least Squares Suppose we want to find the value of x that minimizes 1 2 i (a) = 5\\|Aw \u2014 O)/3 (4.21) Specialized linear algebra algorithms can solve this problem efficiently; however, we can also explore how to solve it using gradient-based optimization as a simple  example of how these techniques work. First, we need to obtain the gradient: Val(x) = A\u2019 (Aw \u2014b) = A\u2019 Aw \u2014 Ald. (4.22)  We can then follow this gradient downhill, taking small steps. See algorithm 4.1 for details. Algorithm 4.1 An algorithm to minimize f(a) = 3 || Ax \u2014 b||5 with respect to x using gradient descent, starting from an arbitrary value of a", "cee074bc-9e14-4f5e-862b-fc4e92b6d79f": "Generative adversarial nets.\n\nProceedings of the 27th International Conference on Neural Information Processing Systems-Volume 2, 2672\u20132680. Gori, M. Machine learning: A constraint-based approach. Morgan Kaufmann. Gutmann, M. U., Dutta, R., Kaski, S., & Corander, J. Likelihood-free inference via classi\ufb01cation. Statistics and Computing, 28(2), 411\u2013425. Hao, S., Tan, B., Tang, K., Zhang, H., Xing, E. P., & Hu, Z. BertNet: Harvesting knowledge graphs from pretrained language models. arXiv. https://arxiv.org/abs/2206.14268 Hinton, G., Vinyals, O., & Dean, J. Distilling the knowledge in a neural network. arXiv. Hinton, G. E., Dayan, P., Frey, B. J., & Neal, R. M", "2c6261c2-cd49-408b-98e7-0043e184cdf8": "All these different methods are designed to encourage the learning process to learn a function f* that satisfies the condition  P(e) ~ fete) (5.103)  for most configurations x and small change \u00a2e. In other words, if we know a good answer for an input x (for example, if x is a labeled training example), then that answer is probably good in the neighborhood of a. If we have several good answers in some neighborhood, we would combine them (by some form of averaging or interpolation) to produce an answer that agrees with as many of them as much as possible. An extreme example of the local constancy approach is the k-nearest neighbors family of learning algorithms. These predictors are literally constant over each region containing all the points x that have the same set of k nearest neighbors in  154  CHAPTER 5. MACHINE LEARNING BASICS  the training set. For k = 1, the number of distinguishable regions cannot be more than the number of training examples. While the k-nearest neighbors algorithm copies the output from nearby training examples, most kernel machines interpolate between training set outputs associated with nearby training examples", "f9a81a0a-a8e3-4bbb-92c6-f69fe0a68a09": "This is also true for other nonlinear activation functions that satisfy mild conditions, but nonlinearity is essential: if all the units in a multi-layer feedforward ANN have linear activation functions, the entire network is equivalent to a network with no hidden layers (because linear functions of linear functions are themselves linear).\n\nDespite this \u201cuniversal approximation\u201d property of one-hidden-layer ANNs, both experience and theory show that approximating the complex functions needed for many arti\ufb01cial intelligence tasks is made easier\u2014indeed may require\u2014abstractions that are hierarchical compositions of many layers of lower-level abstractions, that is, abstractions produced by deep architectures such as ANNs with many hidden layers. (See Bengio, 2009, for a thorough review.) The successive layers of a deep ANN compute increasingly abstract representations of the network\u2019s \u201craw\u201d input, with each unit providing a feature contributing to a hierarchical representation of the overall input-output function of the network. Training the hidden layers of an ANN is therefore a way to automatically create features appropriate for a given problem so that hierarchical representations can be produced without relying exclusively on hand-crafted features", "e0e98e93-b2c7-4697-b9a2-4b334f524c45": "Consider the two Markov reward processes3 (MRPs) diagrammed below: Where two edges leave a state, both transitions are assumed to occur with equal probability, and the numbers indicate the reward received. All the states appear the same; they all produce the same single-component feature vector x = 1 and have approximated value w. Thus, the only varying part of the data trajectory is the reward sequence. The left MRP stays in the same state and emits an endless stream of 0s and 2s at random, each with 0.5 probability.\n\nThe right MRP, on every step, either stays in its current state or 2They would of course be estimated if the state sequence were observed rather than only the 3All MRPs can be considered MDPs with a single action in all states; what we conclude about MRPs switches to the other, with equal probability. The reward is deterministic in this MRP, always a 0 from one state and always a 2 from the other, but because the each state is equally likely on each step, the observable data is again an endless stream of 0s and 2s at random, identical to that produced by the left MRP", "e12ac75d-a71b-4784-a169-fcfee8f5be3b": "For example, in speech recognition, the correct interpretation of the current sound as a phoneme may depend on the next few phonemes because of co-articulation and may even depend on the next few words because of the linguistic dependencies between nearby words: if there are wo interpretations of the current word that are both acoustically plausible, we may have to look far into the future (and the past) to disambiguate them. This is also true of handwriting recognition and many other sequence-to-sequence learning casks, described in the next section. Bidirectional recurrent neural networks (or bidirectional RNNs) were invented o address that need .\n\nThey have been extremely suc- cessful  in applications where that need arises, such as handwriting recognition , speech recogni- ion , and bioinformatics . As the name suggests, bidirectional RNNs combine an RNN that moves forward hrough time, beginning from the start of the sequence, with another RNN that moves backward through time, beginning from the end of the sequence. Figure 10.11 illustrates the typical bidirectional RNN, with h\u00ae standing for the state of the  388  CHAPTER 10", "797c0758-d578-4ed0-b221-997cc042cc30": "In particular, our principal focus is on the learning objective that drives the model training given the experience and thus often lies at the core for designing new algorithms, understanding learning properties, and validating outcomes. We investigate the underlying connections between a range of seemingly distinct ML paradigms. Each of these paradigms has made particular assumptions on the form of experience available.\n\nFor example, the present most popular supervised learning relies on collections of data instances, often applying a maximum likelihood objective solved with simple gradient descent. Maximum likelihood based unsupervised learning instead can invoke di\ufb00erent solvers, such as expectation-maximization (EM), variational inference, and wake-sleep in training for varied degree of approximation to the problem. Active learning  manages data instances which, instead of being given all at once, are adaptively selected. Reinforcement learning  makes use of feedback obtained via interaction with the environment. Knowledge-constrained learning like posterior regularization  incorporates structures, knowledge, and rules expressed as constraints. Generative adversarial learning  leverages a companion model called discriminator to guide training of the model of interest. In light of these results, we present a standard equation (SE) of the objective function. The SE formulates a rather broad design space of learning algorithms", "ef8ec938-d239-431a-911a-da5ca5e8fddb": "For example, in our hypothetical medical illustration, it may be appropriate to use an automatic system to classify those X-ray images for which there is little doubt as to the correct class, while leaving a human expert to classify the more ambiguous cases. We can achieve this by introducing a threshold \u03b8 and rejecting those inputs x for which the largest of the posterior probabilities p(Ck|x) is less than or equal to \u03b8. This is illustrated for the case of two classes, and a single continuous input variable x, in Figure 1.26.\n\nNote that setting \u03b8 = 1 will ensure that all examples are rejected, whereas if there are K classes then setting \u03b8 < 1/K will ensure that no examples are rejected. Thus the fraction of examples that get rejected is controlled by the value of \u03b8. We can easily extend the reject criterion to minimize the expected loss, when We have broken the classi\ufb01cation problem down into two separate stages, the inference stage in which we use training data to learn a model for p(Ck|x), and the subsequent decision stage in which we use these posterior probabilities to make optimal class assignments. An alternative possibility would be to solve both problems together and simply learn a function that maps inputs x directly into decisions", "9bcff01d-a888-44b1-aa48-b64819434a80": "If we group the potentials and summations together in this way, we can express The reader is encouraged to study this re-ordering carefully as the underlying idea forms the basis for the later discussion of the general sum-product algorithm. Here the key concept that we are exploiting is that multiplication is distributive over addition, so that in which the left-hand side involves three arithmetic operations whereas the righthand side reduces this to two operations. Let us work out the computational cost of evaluating the required marginal using this re-ordered expression. We have to perform N \u2212 1 summations each of which is over K states and each of which involves a function of two variables. For instance, the summation over x1 involves only the function \u03c81,2(x1, x2), which is a table of K \u00d7 K numbers. We have to sum this table over x1 for each value of x2 and so this has O(K2) cost. The resulting vector of K numbers is multiplied by the matrix of numbers \u03c82,3(x2, x3) and so is again O(K2)", "de2e89f9-bdca-4add-a260-14d1662a8914": "The great aspect of MMD is that via the kernel trick there is no need to train a separate network to maximize equation (4) for the ball of a RKHS. However, this has the disadvantage that evaluating the MMD distance has computational cost that grows quadratically with the amount of samples used to estimate the expectations in (4). This last point makes MMD have limited scalability, and is sometimes inapplicable to many real life applications because of it. There are estimates with linear computational cost for the MMD  which in a lot of cases makes MMD very useful, but they also have worse sample complexity.\n\nGenerative Moment Matching Networks (GMMNs)  are the generative counterpart of MMD. By backproping through the kernelized formula for equation (4), they directly optimize dMMD(Pr, P\u03b8) (the IPM when F is as in the previous item). As mentioned, this has the advantage of not requiring a separate network to approximately maximize equation (4). However, GMMNs have enjoyed limited applicability. Partial explanations for their unsuccess are the quadratic cost as a function of the number of samples and vanishing gradients for low-bandwidth kernels", "d24631b7-90ab-4d58-bdf8-b7ae9588abfa": "Previous approaches had involved adding penalties to the cost function to encourage units to have normalized activation statistics or  involved intervening to renormalize unit statistics after each gradient descent step. The former approach usually resulted in imperfect normalization and the latter usually resulted in significant wasted time, as the learning algorithm repeatedly proposed changing the mean and variance, and the normalization step repeatedly undid this change. Batch normalization reparametrizes the model to make some units always be standardized by definition, deftly sidestepping both problems. At test time, 2 and o may be replaced by running averages that were collected during training time. This allows the model to be evaluated on a single example, without needing to use definitions of 2 and o that depend on an entire minibatch. Revisiting the 7 = rw ,w2...w; example, we see that we can mostly resolve the difficulties in learning this model by normalizing h)_;. Suppose that x is drawn from a unit Gaussian. Then /j_; will also come from a Gaussian, because the transformation from x to h; is linear. However, hj; will no longer have zero mean and unit variance", "2d3e8505-2acd-44f6-b3d7-31a4b15815af": "The task seems to break down naturally into episodes\u2014the successive runs through the maze\u2014so you decide to treat it as an episodic task, where the goal is to maximize expected total reward (3.7).\n\nAfter running the learning agent for a while, you \ufb01nd that it is showing no improvement in escaping from the maze. What is going wrong? Have you e\u21b5ectively communicated to the agent what you want it to achieve? \u21e4 In the preceding section we described two kinds of reinforcement learning tasks, one in which the agent\u2013environment interaction naturally breaks down into a sequence of separate episodes (episodic tasks), and one in which it does not (continuing tasks). The former case is mathematically easier because each action a\u21b5ects only the \ufb01nite number of rewards subsequently received during the episode. In this book we consider sometimes one kind of problem and sometimes the other, but often both. It is therefore useful to establish one notation that enables us to talk precisely about both cases simultaneously. To be precise about episodic tasks requires some additional notation. Rather than one long sequence of time steps, we need to consider a series of episodes, each of which consists of a \ufb01nite sequence of time steps", "88e377ae-951c-4f09-b4af-e704b4cd2a84": "See also Oudeyer and Kaplan , Oudeyer, Kaplan, and Hafner , and Barto . Abbeel, P., Ng, A. Y. .\n\nApprenticeship learning via inverse reinforcement learning. In Proceedings of the 21st International Conference on Machine Learning. ACM, New York. Abramson, B. Expected-outcome: A general model of static evaluation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 12(2):182\u2013193. devaluation. The Quarterly Journal of Experimental Psychology, 34(2):77\u201398. Adams, C. D., Dickinson, A. Instrumental responding following reinforcer devaluation. The Quarterly Journal of Experimental Psychology, 33(2):109\u2013121. mathematically informed understanding of mental illness. Journal of Neurology, Neurosurgery & Psychiatry. doi:10.1136/jnnp-2015-310737 Agrawal, R. Sample mean based index policies with O(logn) regret for the multi-armed bandit problem", "e5d55931-da78-441c-8df8-2159237604f5": "2021b. It\u2019s not just size that matters: Small language models are also few-shot learners. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2339\u20132352, Online. Association for Computational Linguistics. Sebastian Schuster, Sonal Gupta, Rushin Shah, and Mike Lewis. 2019. Cross-lingual transfer learning for multilingual task oriented dialog. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3795\u20133805, Minneapolis, Minnesota. Association for Computational Linguistics. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2015. Improving neural machine translation models with monolingual data. Computer Science. Peter Shaw, Ming-Wei Chang, Panupong Pasupat, and Kristina Toutanova. 2020", "9f35d620-fadd-48ac-94c2-8106e9176e0c": "First, it might successfully define a series of cost functions where the first is convex and the optimum tracks from one function to the next, arriving at the global minimum, but it might require so many incremental cost functions that the cost of the entire procedure remains high. NP-hard optimization problems remain NP-hard, even when continuation methods are applicable. The other two ways continuation methods fail both correspond to the method not being applicable. First, the function might not become convex, no matter how much it is blurred. Consider, for example, the function J(@) = \u20146'86. Second, the function may become convex as a result of blurring, but the minimum of this blurred function may track to a local rather than a global minimum of the original cost function. Though continuation methods were mostly originally designed to deal with the problem of local minima, local minima are no longer believed to be the primary problem for neural network optimization. Fortunately, continuation methods can still help.\n\nThe easier objective functions introduced by the continuation method can eliminate flat regions, decrease variance in gradient estimates, improve conditioning of the Hessian matrix, or do anything else that will either make local updates easier to compute or improve the correspondence between local update directions and progress toward a global solution", "970a731d-4482-48f5-8b70-b73d8eb69620": "If we treat the data as i.i.d., then the only information we can glean from the data is the relative frequency of rainy days. However, we know in practice that the weather often exhibits trends that may last for several days. Observing whether or not it rains today is therefore of signi\ufb01cant help in predicting if it will rain tomorrow. To express such effects in a probabilistic model, we need to relax the i.i.d. assumption, and one of the simplest ways to do this is to consider a Markov model.\n\nFirst of all we note that, without loss of generality, we can use the product rule to express the joint distribution for a sequence of observations in the form If we now assume that each of the conditional distributions on the right-hand side is independent of all previous observations except the most recent, we obtain the \ufb01rst-order Markov chain, which is depicted as a graphical model in Figure 13.3", "402f3a4f-b284-4059-bd25-9fa4b0ca4d79": "Given an approximate value function, V , the increments speci\ufb01ed by (6.1) or (6.2) are computed for every time step t at which a nonterminal state is visited, but the value function is changed only once, by the sum of all the increments. Then all the available experience is processed again with the new value function to produce a new overall increment, and so on, until the value function converges. We call this batch updating because updates are made only after processing each complete batch of training data. Under batch updating, TD(0) converges deterministically to a single answer independent of the step-size parameter, \u21b5, as long as \u21b5 is chosen to be su\ufb03ciently small. The constant\u21b5 MC method also converges deterministically under the same conditions, but to a di\u21b5erent answer. Understanding these two answers will help us understand the di\u21b5erence between the two methods.\n\nUnder normal updating the methods do not move all the way to their respective batch answers, but in some sense they take steps in these directions. Before trying to understand the two answers in general, for all possible tasks, we \ufb01rst look at a few examples", "ade07f62-0a92-40d4-a70a-f0d71154fad0": "When this constraint is too limiting, a popular approximate inference algorithm is called loopy belief propagation. Both approaches often work well with sparsely connected graphs. By comparison, models used in deep learning tend to connect each visible unit v; to many hidden units h;, so that h can provide a distributed representation of vj (and probably several other observed variables too).\n\nDistributed representations have many advantages, but from the point of view of graphical models and computational complexity, distributed representations have the disadvantage of usually yielding graphs that are not sparse enough for the traditional techniques of exact inference and loopy belief propagation to be relevant. As a consequence, one of the most striking differences between the larger graphical models community and the deep graphical models community is that loopy belief propagation is almost never used for deep learning. Most deep models are instead designed to make Gibbs sampling or variational inference algorithms efficient. Another consideration is that deep learning models contain a very large number of latent variables, making efficient numerical code essential. This provides an additional motivation, besides the choice of high-level inference algorithm, for grouping the units into layers with a matrix describing the interaction between two layers", "9900ba92-eb4b-42da-891c-6fdf5f8cad84": "Note that the result of averaging many solutions for the complex model with M = 25 is a very good \ufb01t to the regression function, which suggests that averaging may be a bene\ufb01cial procedure. Indeed, a weighted averaging of multiple solutions lies at the heart of a Bayesian approach, although the averaging is with respect to the posterior distribution of parameters, not with respect to multiple data sets. We can also examine the bias-variance trade-off quantitatively for this example. The average prediction is estimated from and the integrated squared bias and integrated variance are then given by where the integral over x weighted by the distribution p(x) is approximated by a \ufb01nite sum over data points drawn from that distribution. These quantities, along with their sum, are plotted as a function of ln \u03bb in Figure 3.6.\n\nWe see that small values of \u03bb allow the model to become \ufb01nely tuned to the noise on each individual data set leading to large variance. Conversely, a large value of \u03bb pulls the weight parameters towards zero leading to large bias", "1f1066dc-2288-40fd-8b3c-81b9e61ab8df": "Joshi (Ed. ), Proceedings of the Ninth International Joint Conference on Arti\ufb01cial Intelligence, pp. 670\u2013672. Morgan Kaufmann. Seo, H., Barraclough, D., Lee, D. Dynamic signals related to choices and outcomes in the dorsolateral prefrontal cortex. Cerebral Cortex, 17(suppl 1):110\u2013117. Seung, H. S. Learning in spiking neural networks by reinforcement of stochastic synaptic Shah, A. Psychological and neuroscienti\ufb01c connections with reinforcement learning. In M. Wiering and M. van Otterlo (Eds. ), Reinforcement Learning: State-of-the-Art, pp. 507\u2013537. Springer-Verlag Berlin Heidelberg. Shannon, C. E. Programming a computer for playing chess. Philosophical Magazine and Shannon, C. E. Presentation of a maze-solving machine. In H. V", "9fbfdf7b-0f23-44dc-9980-9715627b2e10": "It is natural, then, to go through the three and see if there is any one that can be given up. Of the three, function approximation most clearly cannot be given up. We need methods that scale to large problems and to great expressive power. We need at least linear function approximation with many features and parameters. State aggregation or nonparametric methods whose complexity grows with data are too weak or too expensive. Least-squares methods such as LSTD are of quadratic complexity and are therefore too expensive for large problems. Doing without bootstrapping is possible, at the cost of computational and data e\ufb03ciency. Perhaps most important are the losses in computational e\ufb03ciency.\n\nMonte Carlo (nonbootstrapping) methods require memory to save everything that happens between making each prediction and obtaining the \ufb01nal return, and all their computation is done once the \ufb01nal return is obtained. The cost of these computational issues is not apparent on serial von Neumann computers, but would be on specialized hardware. With bootstrapping and eligibility traces (Chapter 12), data can be dealt with when and where it is generated, then need never be used again. The savings in communication and memory made possible by bootstrapping are great", "edf62912-d3aa-42d6-86d1-090dfe086abc": "Let p\u03b8(t) be the target model with parameters \u03b8 to be learned. Generally, the SE is agnostic to the speci\ufb01c forms of the target model, meaning that the target model can take an arbitrary form as desired by the problem at hand (e.g., classi\ufb01cation, regression, generation, control) and can be of arbitrary types ranging from deep neural networks of arbitrary architectures, prompts for pretrained models, symbolic systems (e.g., knowledge graph), probabilistic graphical models of arbitrary dependence structures, and so on. We discuss more details of the di\ufb00erent choices of the target model in Section 8.\n\nThe SE contains three major terms that constitute a learning formalism: the uncertainty function H (\u00b7) that controls the compactness of the output model (e.g., as measured by the amount of allowed randomness while trying to \ufb01t experience); the divergence function D (\u00b7, \u00b7) that measures the distance between the target model to be trained and the auxiliary model that facilitates a teacher\u2013student mechanism as shown below; and the experience function, which is introduced by a penalty term U(\u03be) that draws in the set of \u2018experience functions\u2019 f (\u03b8) k that represent external experience of various kinds for training the target model", "0d88adfd-03cc-4369-ba70-bf20384222a9": "Eligibility traces are the \ufb01rst line of defense against both long-delayed rewards and non-Markov tasks. By adjusting \u03bb, we can place eligibility trace methods anywhere along a continuum from Monte Carlo to one-step TD methods. Where shall we place them? We do not yet have a good theoretical answer to this question, but a clear empirical answer appears to be emerging. On tasks with many steps per episode, or many steps within the half-life of discounting, it appears signi\ufb01cantly better to use eligibility traces than not to (e.g., see Figure 12.14). On the other hand, if the traces are so long as to produce a pure Monte Carlo method, or nearly so, then performance degrades sharply. An intermediate mixture appears to be the best choice. Eligibility traces should be used to bring us toward Monte Carlo methods, but not all the way there", "9efa3ee1-cb59-4f75-b68d-de8a00062f9a": "A prominent hypothesis is that the brain implements something like an actor\u2013critic striatum), both of which play critical roles in reward-based learning, may function respectively like an actor and a critic.\n\nThat the TD error is the reinforcement signal for both the actor and the critic \ufb01ts well with the facts that dopamine neuron axons target both the dorsal and ventral subdivisions of the striatum; that dopamine appears to be critical for modulating synaptic plasticity in both structures; and that the e\u21b5ect on a target structure of a neuromodulator such as dopamine depends on properties of the target structure and not just on properties of the neuromodulator. The actor and the critic can be implemented by ANNs consisting of neuron-like units having learning rules based on the policy-gradient actor\u2013critic method described in Section 13.5. Each connection in these networks is like a synapse between neurons in the brain, and the learning rules correspond to rules governing how synaptic e\ufb03cacies change as functions of the activities of the presynaptic and the postsynaptic neurons, together with neuromodulatory input corresponding to input from dopamine neurons", "0d246d77-495f-4b98-8914-95f39ad1d250": "For more practical examples, where the desired distribution may be multimodal and sharply peaked, it will be extremely dif\ufb01cult to \ufb01nd a good proposal distribution and comparison function. Furthermore, the exponential decrease of acceptance rate with dimensionality is a generic feature of rejection sampling. Although rejection can be a useful technique in one or two dimensions it is unsuited to problems of high dimensionality. It can, however, play a role as a subroutine in more sophisticated algorithms for sampling in high dimensional spaces. One of the principal reasons for wishing to sample from complicated probability distributions is to be able to evaluate expectations of the form (11.1). The technique of importance sampling provides a framework for approximating expectations directly but does not itself provide a mechanism for drawing samples from distribution p(z). The \ufb01nite sum approximation to the expectation, given by (11.2), depends on being able to draw samples from the distribution p(z)", "70187880-b4fd-4566-b147-c552a50d42ac": "Using GANs Shorten and Khoshgoftaar J Big Data  6:60   to oversample data could be another effective way to increase the minority class size while preserving the extrinsic distribution.\n\nOversampling with GANs can be done using the entire minority class as \u201creal\u201d examples, or by using subsets of the minority class as inputs to GANs. The use of evolutionary sampling  to find these subsets to input to  GANs for class sampling is a promising area for future work. Discussion  The interesting ways to augment image data fall into two general categories: data warp- ing and oversampling. Many of these augmentations elucidate how an image classifier can be improved, while others do not. It is easy to explain the benefit of horizontal flip- ping or random cropping. However, it is not clear why mixing pixels or entire images together such as in PatchShuffle regularization or SamplePairing is so effective. Addi- tionally, it is difficult to interpret the representations learned by neural networks for GAN-based augmentation, variational auto-encoders, and meta-learning. CNN visuali- zation has been led by Yosinski et al", "829ea666-7293-4e4d-84fd-bfca0c615d5a": "By analogy with (3.92), we obtain where \u03b3 represents the effective number of parameters and is de\ufb01ned by Section 3.5.3 Note that this result was exact for the linear regression case. For the nonlinear neural network, however, it ignores the fact that changes in \u03b1 will cause changes in the Hessian H, which in turn will change the eigenvalues. We have therefore implicitly ignored terms involving the derivatives of \u03bbi with respect to \u03b1. Similarly, from (3.95) we see that maximizing the evidence with respect to \u03b2 gives the re-estimation formula As with the linear model, we need to alternate between re-estimation of the hyperparameters \u03b1 and \u03b2 and updating of the posterior distribution. The situation with a neural network model is more complex, however, due to the multimodality of the posterior distribution. As a consequence, the solution for wMAP found by maximizing the log posterior will depend on the initialization of w. Solutions that differ only as a consequence of the interchange and sign reversal symmetries in the hidden units Section 5.1.1 are identical so far as predictions are concerned, and it is irrelevant which of the equivalent solutions is found", "e9f9020f-3114-4173-8688-da0eb63d6c99": "Activation time is especially important for a more realistic actor unit because it in\ufb02uences how contingent eligibility traces have to work in order to properly apportion credit for reinforcement to the appropriate synapses. The expression de\ufb01ning contingent eligibility traces for the actor unit\u2019s learning rule given above includes the postsynaptic factor because by ignoring activation time, the presynaptic activity x(St) participates in causing the postsynaptic activity appearing in ment correctly, the presynaptic factor de\ufb01ning the eligibility trace must be a cause of the postsynaptic factor that also de\ufb01nes the trace.\n\nContingent eligibility traces for a more realistic actor unit would have to take activation time into account. (Activation time should not be confused with the time required for a neuron to receive a reinforcement signal in\ufb02uenced by that neuron\u2019s activity. The function of eligibility traces is to span this time interval which is generally much longer than the activation time. We discuss this further in the following section.) There are hints from neuroscience for how this process might work in the brain", "650edf53-b827-441f-9f16-7f39ba0b792b": "We can correct this bias by de\ufb01ning a different estimator \ufffd\u03a3 given by Clearly from (2.122) and (2.124), the expectation of \ufffd\u03a3 is equal to \u03a3.\n\nOur discussion of the maximum likelihood solution for the parameters of a Gaussian distribution provides a convenient opportunity to give a more general discussion of the topic of sequential estimation for maximum likelihood. Sequential methods allow data points to be processed one at a time and then discarded and are important for on-line applications, and also where large data sets are involved so that batch processing of all data points at once is infeasible. Consider the result (2.121) for the maximum likelihood estimator of the mean \u00b5ML, which we will denote by \u00b5(N) ML when it is based on N observations. If we dissect out the contribution from the \ufb01nal data point xN, we obtain This result has a nice interpretation, as follows. After observing N \u2212 1 data points we have estimated \u00b5 by \u00b5(N\u22121) ML by moving the old estimate a small amount, proportional to 1/N, in the direction of the \u2018error signal\u2019 (xN \u2212\u00b5(N\u22121) so the contribution from successive data points gets smaller", "724f4082-af7e-44b2-83b0-01261a5d10fc": "The logistic sigmoid is commonly used to produce the \u00a2 parameter of a Bernoulli distribution because its range is (0,1), which lies within the valid range of values for the @ parameter. See figure 3.3 for a graph of the sigmoid function. The sigmoid function saturates when its argument is very positive or very negative, meaning that the function becomes very flat and insensitive to small changes in its input. Another commonly encountered function is the softplus function : G(x) = log (1 + exp(z)) . (3.31)  The softplus function can be useful for producing the \u00a7 or o parameter of a normal  https://www.deeplearningbook.org/contents/prob.html    distribution because its range is (0,00). It also arises commonly when manipulating expressions involving sigmoids. The name of the softplus function comes from the fact that it is a smoothed, or \u201csoftened,\u201d version of  xt = max(0, 2). (3.32)  See figure 3.4 for a graph of the softplus function. The following properties are all useful enough that you may wish to memorize  66  CHAPTER 3", "3c216def-b277-4051-9648-4be07e8b5403": "We now de\ufb01ne the Gram matrix K = \u03a6\u03a6T, which is an N \u00d7 N symmetric matrix with elements where we have introduced the kernel function k(x, x\u2032) de\ufb01ned by (6.1).\n\nIn terms of the Gram matrix, the sum-of-squares error function can be written as If we substitute this back into the linear regression model, we obtain the following prediction for a new input x where we have de\ufb01ned the vector k(x) with elements kn(x) = k(xn, x). Thus we see that the dual formulation allows the solution to the least-squares problem to be expressed entirely in terms of the kernel function k(x, x\u2032). This is known as a dual formulation because, by noting that the solution for a can be expressed as a linear combination of the elements of \u03c6(x), we recover the original formulation in terms of the parameter vector w. Note that the prediction at x is given by a linear combination Exercise 6.1 of the target values from the training set. In fact, we have already obtained this result, using a slightly different notation, in Section 3.3.3", "25870906-d682-437d-9e84-48bd17dd10c8": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 119\u2013126, Online. Association for Computational Linguistics. Phoebe Mulcaire, Jungo Kasai, and Noah A. Smith. 2019. Low-resource parsing with crosslingual contextualized representations. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 304\u2013315, Hong Kong, China. Association for Computational Linguistics. Tong Niu and Mohit Bansal. 2018. Adversarial oversensitivity and over-stability strategies for dialogue models.\n\nIn The SIGNLL Conference on Computational Natural Language Learning (CoNLL). Tong Niu and Mohit Bansal. 2019. Automatically learning data augmentation policies for dialogue tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1317\u20131323, Hong Kong, China. Association for Computational Linguistics", "165eac46-8189-4623-8b98-a88b2af87e3d": "Since learning in Snorkel is done with a Gibbs sampler, the overhead of modeling additional correlations is linear in the number of correlations. The dashed lines in Fig. 9 show the number of correlations included in each model versus \u03f5. For example, on the Spouses task, \ufb01tting the parameters of the generative model at \u03f5 = 0.5 takes 4 min, and \ufb01tting its parameters with \u03f5 = 0.02 takes 57 min. Further, parameter estimation is often run repeatedly during development for two reasons: (i) \ufb01tting generative model hyperparameters using a development set requires repeated runs, and (ii) as users iterate on their labeling functions, they must re-estimate the generative model to evaluate them. Based on our observations, we seek to automatically choose a value of \u03f5 that trades-off between predictive performance and computational cost using the labeling functions\u2019 outputs \ufffd alone.\n\nIncluding \u03f5 as a hyperparameter in a grid search over a development set is generally not feasible because of its large effect on running time. We therefore want to choose \u03f5 before other hyperparameters, without performing any parameter estimation. We propose using the number of correlations selected at each value of \u03f5 as an inexpensive indicator", "a3b43d64-ad85-44fb-93d4-ebe445cee4c8": "This form of dynamic structure inside neural networks is sometimes called conditional computation . Since many components of the architecture may be relevant only for a small amount of possible inputs, the  443  CHAPTER 12. APPLICATIONS  system can run faster by computing these features only when they are needed. Dynamic structure of computations is a basic computer science principle applied generally throughout the software engineering discipline.\n\nThe simplest versions of dynamic structure applied to neural networks are based on determining which subset of some group of neural networks (or other machine learning models) should be applied to a particular input. A venerable strategy for accelerating inference in a classifier is to use a cascade of classifiers. The cascade strategy may be applied when the goal is to detect the presence of a rare object (or event). To know for sure that the object is present, we must use a sophisticated classifier with high capacity, which is expensive to run. Because the object is rare, however, we can usually use much less computation to reject inputs as not containing the object. In these situations, we can train a sequence of classifiers. The first classifiers in the sequence have low capacity and are trained to have high recall", "97cc46b2-a71e-4262-9244-af324b8c254d": "Thus the policy \u03c0\u03b8(at|st) exactly corresponds to the above generation model p\u03b8(yt|y<t).\n\nPolicy gradient (PG) is one of the most widely used algorithms for text generation . It optimizes the cumulative reward with the policy gradient using the estimated Q\u03c0\u03b8 value based on sample \u03c4. PG is an on-policy algorithm, meaning that the sample \u03c4 needs to come from the the current policy \u03c0\u03b8 itself. In practice, however, optimizing this objective alone from scratch is unlikely going to work because most samples \u03c4 \u223c \u03c0\u03b8 are just gibberish with zero reward, failing to provide meaningful training signals for updating the policy. Previous literature either initializes the policy \u03c0\u03b8 with MLE training, and/or use a combination of MLE and PG updates, which often leads to marginal gains in practice . Value-based RL techniques, such as Q-learning, implicitly learn the policy \u03c0 by approximating the value Q\u03c0(s, a) directly", "0acc1828-840a-45d9-8b47-9602e48e6f08": "It is worth emphasizing once again that maximum likelihood would lead to values of the likelihood function that increase monotonically with K (assuming the singular solutions have been avoided, and discounting the effects of local maxima) and so cannot be used to determine an appropriate model complexity. By contrast, Bayesian inference automatically makes the trade-off between model complexity and \ufb01tting the data. Section 3.4 This approach to the determination of K requires that a range of models having different K values be trained and compared. An alternative approach to determining a suitable value for K is to treat the mixing coef\ufb01cients \u03c0 as parameters and make point estimates of their values by maximizing the lower bound  with respect to \u03c0 instead of maintaining a probability distribution over them as in the fully Bayesian approach.\n\nThis leads to the re-estimation equation Exercise 10.23 and this maximization is interleaved with the variational updates for the q distribution over the remaining parameters. Components that provide insuf\ufb01cient contribution to explaining the data will have their mixing coef\ufb01cients driven to zero during the optimization, and so they are effectively removed from the model through automatic relevance determination. This allows us to make a single training run in which we start with a relatively large initial value of K, and allow surplus components to be pruned out of the model", "ec271ce0-a0c2-41e0-9a8e-288191ce3cd9": "This is illustrated in Figure 5.16. The regularization function depends on the network weights through the Jacobian J. A backpropagation formalism for computing the derivatives of the regularizer with respect to the network weights is easily obtained by extension of the Exercise 5.26 techniques introduced in Section 5.3. If the transformation is governed by L parameters (e.g., L = 3 for the case of translations combined with in-plane rotations in a two-dimensional image), then the manifold M will have dimensionality L, and the corresponding regularizer is given by the sum of terms of the form (5.128), one for each transformation.\n\nIf several transformations are considered at the same time, and the network mapping is made invariant to each separately, then it will be (locally) invariant to combinations of the transformations . A related technique, called tangent distance, can be used to build invariance properties into distance-based methods such as nearest-neighbour classi\ufb01ers . We have seen that one way to encourage invariance of a model to a set of transformations is to expand the training set using transformed versions of the original input patterns. Here we show that this approach is closely related to the technique of tangent propagation", "6ae952bc-7cd8-4a9f-b772-e5cf7877823f": "While pushing the \ufb01eld forward rapidly, the bewildering and ever-growing variety of paradigms and algorithms also makes it extremely di\ufb03cult to master existing ML techniques and to develop universal, repeatable, and reusable computer programs that can simultaneously learn from diverse experience in the real world.\n\nIt is a constant desire and aspiration to search for a standardized ML formalism that uni\ufb01es the distinct learning principles, much like the Standard Model in physics, to gain a more holistic understanding of the diverse paradigms and algorithms, lay out a blueprint permitting fuller and more systematic exploration in the design and analysis of new algorithms, and eventually serves as a vehicle toward panoramic machine learning capable of integrating all available information (data, knowledge, constraints, reward, adversary, etc.) in learning and thus applicable to all problems. This work presents an attempt toward this end. In particular, we establish a standard equation of the learning objective, which subsumes many of the known algorithms as special cases and o\ufb00ers guiding principles of designing new more powerful learning algorithms in a mechanical and composable way. Human learning has the hallmark of learning concepts from diverse sources of information. Take the example of learning a language", "de5b5e9c-37cc-4fee-b135-6646a755786f": "Only a few studies have included learned linear models, and even fewer have also explored including temporally-abstract models using options as discussed in Section 17.2. More work is needed before planning with learned models can be e\u21b5ective. For example, the learning of the model needs to be selective because the scope of a model strongly a\u21b5ects planning e\ufb03ciency. If a model focuses on the key consequences of the most important options, then planning can be e\ufb03cient and rapid, but if a model includes details of unimportant consequences of options that are unlikely to be selected, then planning may be almost useless. Environment models should be constructed judiciously with regard to both their states and dynamics with the goal of optimizing the planning process. The various parts of the model should be continually monitored as to the degree to which they contribute to, or detract from, planning e\ufb03ciency. The \ufb01eld has not yet addressed this complex of issues or designed model-learning methods that take into 1Some would claim that deep learning solves this problem, for example, that DQN as described in Section 16.5 illustrates a solution, but we are unconvinced.\n\nThere is as yet little evidence that deep learning alone solves the representation learning problem in a general and e\ufb03cient way", "122bd3db-e60f-4e71-aa9d-68b728885264": "In fact, we may never have techniques to represent suitable probability distributions over high- dimensional continuous spaces, such as the set of all possible video frames. This seems like an intractable problem. A unified view of self- supervised methods  There is a way to think about SSL within the unified framework of an energy-based model (EBM). An EBM is a trainable system that, given two inputs, x and y, tells us how incompatible they are with each other. For example, x could be a short video  https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/  Self-supervised learning: The dark matter of intelligence   clip, and y another proposed video clip. The machine would tell us to what extent y is a good continuation for x. To indicate the incompatibility between x and y, the machine produces a single number, called an energy.\n\nIf the energy is low, x and y are deemed compatible; if itis high, they are deemed incompatible. Energy Function  An energy-based model (EBM) measures the compatibility between an observation x and a proposed prediction y", "823c9b8e-da7d-4291-8146-1d5b4b5b4aa5": "In particular, Hinton et al. recommend a strategy introduced by Srebro and Shraibman : constraining the norm of each column of the weight matrix of a neural net layer, rather than constraining the Frobenius norm of the entire weight matrix. Constraining the norm of each column separately prevents any one hidden unit from having very large weights. If we converted this constraint into a penalty in a Lagrange function, it would be similar to L?\n\nweight decay but with a separate KKT multiplier for the weights of each hidden unit. Each of these KKT multipliers would be dynamically updated separately to make each hidden unit obey the constraint. In practice, column norm limitation is always implemented as an explicit constraint with reprojection. 7.3 Regularization and Under-Constrained Problems  In some cases, regularization is necessary for machine learning problems to be properly defined. Many linear models in machine learning, including linear re- gression and PCA, depend on inverting the matrix X'X. This is not possible when X! X is singular", "9d761262-3e13-43de-be8e-128c8395fbd9": "Ciosek, K., Whiteson, S. Expected policy gradients for reinforcement learning. ArXiv: Claridge-Chang, A., Roorda, R. D., Vrontou, E., Sjulson, L., Li, H., Hirsh, J., Miesenb\u00a8ock, G. Clark, R. E., Squire, L. R. Classical conditioning and brain systems: the role of awareness. Clark, W. A., Farley, B. G. Generalization of pattern recognition in a self-organizing system. In Proceedings of the 1955 Western Joint Computer Conference, pp. 86\u201391. Clouse, J. On Integrating Apprentice Learning and Reinforcement Learning TITLE2. Cobo, L. C., Zang, P., Isbell, C. L., Thomaz, A. L. Automatic state abstraction from Arti\ufb01cial Intelligence (IJCAI-11), pp. 1243-1248", "570fa349-4856-491d-905e-c988da1cc70a": "I' and ,,' uSIng maximum likelihuo<l, To write \"\"\"\"n lhe likeliltood function, we need an \"\"pression for tl>o marginal distributioo p{\") of tl>o ~,,'ed ...riahle_ This is exprt__sed. fmn' the sum aod p,oduct rules \"fprobability, in the form E,e,,-ise 12,7 ll\"\"auS(: this corresponds to a linear\u00b7Gau\"i,n lT1(llIcL thi< marginal di,tribulion is again Gaussian. atld is given by where the D x D covariance matrix C is defined by This result can also be derived more directly by noting that the predictive distribution will be Gaussian and then evaluating its mean and covariance using (12.33). This gives where we have used the fact that z and E are independent random variables and hence are uncorrelated", "371f3b98-1746-4d1b-bdf4-dda45d5ac740": "However, the original and most popular method for overcoming the joint training problem of DBMs is greedy layer-wise pretraining. In this method, each layer of the DBM is trained in isolation as an RBM. The first layer is trained to model the input data. Each subsequent RBM is trained to model samples from the previous RBM\u2019s posterior distribution. After all the  666  CHAPTER 20. DEEP GENERATIVE MODELS  Algorithm 20.1 The variational stochastic maximum likelihood algorithm for training a DBM with two hidden layers  Set \u20ac, the step size, to a small positive number Set k, the number of Gibbs steps, high enough to allow a Markov chain of pv, h\u00ae, nh); @+ cAg) to burn in, starting from samples from p(v, h, nh): 4)", "a8d9acb4-2c71-4b8d-81de-05b58a12f055": "In both cases the agent will see single occurrence 0, then some number of Bs each followed by a \u22121, except the last w 1, then we start all over again with a single A and a 0, etc. All the as well; in both MDPs, the probability of a string of k Bs is 2\u2212k. No function v\u03b8 = \u20d70. In the \ufb01rst MDP, this is an exact solution, and the o the second MDP, this solution produces an error in both B and B\u2032 of of which generate the same data, have di\u21b5erent BEs.\n\nThus, the BE cann data alone; knowledge of the MDP beyond what is revealed in the da Moreover, the two MDPs have di\u21b5erent minimal-BE value functions the minimal-BE value function is the exact solution v\u03b8 = \u20d70 for any \u03b3. 2. This is a critical observation, as it is possible for an error function to be unob perfectly satisfactory for use in learning settings because the value that minimi from data. For example, this is what happens with the VE", "2c90de0c-8105-4ad7-99f3-13ed98c0f790": "From (13.57), we see that the likelihood function can be found using Similarly, using (13.33) and (13.43), together with (13.63), we see that the required marginals are given by Exercise 13.15 Finally, we note that there is an alternative formulation of the forward-backward algorithm  in which the backward pass is de\ufb01ned by a recursion based the quantities \u03b3(zn) = \ufffd\u03b1(zn)\ufffd\u03b2(zn) instead of using \ufffd\u03b2(zn).\n\nThis \u03b1\u2013\u03b3 recursion requires that the forward pass be completed \ufb01rst so that all the quantities \ufffd\u03b1(zn) are available for the backward pass, whereas the forward and backward passes of the \u03b1\u2013\u03b2 algorithm can be done independently. Although these two algorithms have comparable computational cost, the \u03b1\u2013\u03b2 version is the most commonly encountered one in the case of hidden Markov models, whereas for linear dynamical systems a Section 13.3 recursion analogous to the \u03b1\u2013\u03b3 form is more usual. In many applications of hidden Markov models, the latent variables have some meaningful interpretation, and so it is often of interest to \ufb01nd the most probable sequence of hidden states for a given observation sequence", "5e69b8d2-f30d-422c-bded-b8b6b6acef15": "Suppose we wish to minimize an error function E with respect to the parameter w in Figure 5.8. The derivative of the error function is given by in which the Jacobian matrix for the red module in Figure 5.8 appears in the middle term. Because the Jacobian matrix provides a measure of the local sensitivity of the outputs to changes in each of the input variables, it also allows any known errors \u2206xi associated with the inputs to be propagated through the trained network in order to estimate their contribution \u2206yk to the errors at the outputs, through the relation which is valid provided the |\u2206xi| are small.\n\nIn general, the network mapping represented by a trained neural network will be nonlinear, and so the elements of the Jacobian matrix will not be constants but will depend on the particular input vector used. Thus (5.72) is valid only for small perturbations of the inputs, and the Jacobian itself must be re-evaluated for each new input vector. The Jacobian matrix can be evaluated using a backpropagation procedure that is similar to the one derived earlier for evaluating the derivatives of an error function with respect to the weights. We start by writing the element Jki in the form where we have made use of (5.48)", "74c0e0b7-2c4d-4f28-83df-3827ea69eb59": "The classifier may then be trained to assign the same label to x and x\u2019.\n\nThis encourages the classifier to learn a function that is robust to small changes anywhere along the manifold where the unlabeled data lie. The assumption motivating this approach is that different classes usually lie on disconnected manifolds, and a small perturbation should not be able to jump from one class manifold to another class manifold. 266  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  7.14 Tangent Distance, Tangent Prop and Manifold Tangent Classifier  Many machine learning algorithms aim to overcome the curse of dimensionality by assuming that the data lies near a low-dimensional manifold, as described in section 5.11.3. One of the early attempts to take advantage of the manifold hypothesis is the tangent distance algorithm . It is a nonparametric nearest neighbor algorithm in which the metric used is not the generic Euclidean distance but one that is derived from knowledge of the manifolds near which probability concentrates. It is assumed that we are trying to classify examples, and that examples on the same manifold share the same category", "bd1a142a-9e55-42be-9d91-469fd34c5ecc": "Snorkel\u2019s work\ufb02ow is designed around data programming , a fundamentally new paradigm for training machine learning models using weak supervision, and proceeds in three main stages (Fig. 3): 1.\n\nWriting Labeling Functions Rather than hand-labeling training data, users of Snorkel write labeling functions, which allow them to express various weak supervision sources such as patterns, heuristics, external knowledge bases, and more. This was the component most informed by early interactions (and mistakes) with users over the last year of deployment, and we present a \ufb02exible interface and supporting data model. 2. Modeling Accuracies and Correlations Next, Snorkel automatically learns a generative model over the labeling functions, which allows it to estimate their accuracies and correlations. This step uses no ground-truth data, learning instead from the agreements and disagreements of the labeling functions. We observe that this step improves end predictive performance 5.81% over Snorkel with unweighted label combination, and anecdotally that it streamlines the user development experience by providing actionable feedback about labeling function quality. 3", "7ec84263-a349-4661-a170-208ff8a0092b": "F. The Behavior of Organisms: An Experimental Analysis. Appleton-Century, Skinner, B. F. Reinforcement today. American Psychologist, 13(3):94\u201399. Skinner, B. F. Operant behavior. American Psychologist, 18(8):503\u2013515. Sofge, D. A., White, D. A. Applied learning: Optimal control for manufacturing. In D. A. White and D. A. Sofge (Eds.\n\n), Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches, pp. 259\u2013281. Van Nostrand Reinhold, New York. Agents. Ph.D. thesis, University of Michigan, Ann Arbor. Sorg, J., Lewis, R. L., Singh, S. P. Reward design via online gradient ascent. In Advances Sorg, J., Singh, S", "961da8b1-e0b6-4be5-b29c-55a0341f6358": "In order to derive the rules of probability, consider the slightly more general example shown in Figure 1.10 involving two random variables X and Y (which could for instance be the Box and Fruit variables considered above). We shall suppose that X can take any of the values xi where i = 1, . , M, and Y can take the values yj where j = 1, . , L. Consider a total of N trials in which we sample both of the variables X and Y , and let the number of such trials in which X = xi and Y = yj be nij. Also, let the number of trials in which X takes the value xi (irrespective of the value that Y takes) be denoted by ci, and similarly let the number of trials in which Y takes the value yj be denoted by rj", "5fa8a1f9-eaf7-472f-bf57-cba6736cc744": "In his hedonistic neuron hypothesis, Klopf  conjectured that individual neurons seek to maximize the di\u21b5erence between synaptic input treated as rewarding and synaptic input treated as punishing by adjusting the e\ufb03cacies of their synapses on the basis of rewarding or punishing consequences of their own action potentials. In other words, individual neurons can be trained with response-contingent reinforcement like an animal can be trained in an instrumental conditioning task. His hypothesis included the idea that rewards and punishments are conveyed to a neuron via the same synaptic input that excites or inhibits the neuron\u2019s spike-generating activity. (Had Klopf known what we know today about neuromodulatory systems, he might have assigned the reinforcing role to neuromodulatory input, but he wanted to avoid any centralized source of training information.)\n\nSynaptically-local traces of past pre- and postsynaptic activity had the key function in Klopf\u2019s hypothesis of making synapses eligible\u2014the term he introduced\u2014for modi\ufb01cation by later reward or punishment", "8f2329bd-b8e4-4ab9-bf2e-03ea92faf044": "After normalization the resulting values for q(m) can be used for model selection or model averaging in the usual way. 10.2. Illustration: Variational Mixture of Gaussians We now return to our discussion of the Gaussian mixture model and apply the variational inference machinery developed in the previous section. This will provide a good illustration of the application of variational methods and will also demonstrate how a Bayesian treatment elegantly resolves many of the dif\ufb01culties associated with the maximum likelihood approach .\n\nThe reader is encouraged to work through this example in detail as it provides many insights into the practical application of variational methods. Many Bayesian models, corresponding to much more sophisticated distributions, can be solved by straightforward extensions and generalizations of this analysis. Our starting point is the likelihood function for the Gaussian mixture model, illustrated by the graphical model in Figure 9.6. For each observation xn we have a corresponding latent variable zn comprising a 1-of-K binary vector with elements znk for k = 1, . , K. As before we denote the observed data set by X = {x1,", "4ef43b00-9b3a-4397-af42-c508efcf2b8b": "In just ten trials, the dog began to salivate merely upon seeing the black square, despite the fact that the sight of it had never been followed by food. The sound of the metronome itself acted as a US in conditioning a salivation CR to the black square CS. This was second-order conditioning. If the black square had been used as a US to establish salivation CRs to another otherwise neutral CS, it would have been third-order conditioning, and so on.\n\nHigher-order conditioning is di\ufb03cult to demonstrate, especially above the second order, in part because a higher-order reinforcer loses its reinforcing value due to not being repeatedly followed by the original US during higher-order conditioning trials. But under the right conditions, such as intermixing \ufb01rst-order trials with higher-order trials or by providing a general energizing stimulus, higher-order conditioning beyond the second order can be demonstrated. As we describe below, the TD model of classical conditioning uses the bootstrapping idea that is central to our approach to extend the Rescorla\u2013Wagner model\u2019s account of blocking to include both the anticipatory nature of CRs and higher-order conditioning. Higher-order instrumental conditioning occurs as well", "b71b1fff-fb7d-47d5-9b21-4031c8a34af8": "A recurring problem in machine learning is that large training sets are necessary for good generalization, but large training sets are also more computationally expensive.\n\nThe cost function used by a machine learning algorithm often decomposes as a sum over training examples of some per-example loss function. For example, the negative conditional log-likelihood of the training data can be written as  1 2 ; . J(8) = Ex ynpasal (a, 8) = \u2014S 7 Lay, 8), (5.96) i=l  where L is the per-example loss L(x, y,@) = \u2014 log p(y | x; 8). For these additive cost functions, gradient descent requires computing 1 ) 6 VoJ(@) = ~~ VoL(2,y, 8). 5.97 018) = oL (x,y, 8) (5.97)  The computational cost of this operation is O(m). As the training set size grows to billions of examples, the time to take a single gradient step becomes prohibitively long. The insight of SGD is that the gradient is an expectation. The expectation may be approximately estimated using a small set of samples", "9825f0a3-13b1-472d-805e-3edc9a472dda": "Now consider the corresponding results for the linear regression model. The mean of the target distribution is now given by the function wT\u03c6(x), which contains M parameters. However, not all of these parameters are tuned to the data. The effective number of parameters that are determined by the data is \u03b3, with the remaining M \u2212\u03b3 parameters set to small values by the prior. This is re\ufb02ected in the Bayesian result for the variance that has a factor N \u2212 \u03b3 in the denominator, thereby correcting for the bias of the maximum likelihood result.\n\nWe can illustrate the evidence framework for setting hyperparameters using the sinusoidal synthetic data set from Section 1.1, together with the Gaussian basis function model comprising 9 basis functions, so that the total number of parameters in the model is given by M = 10 including the bias. Here, for simplicity of illustration, we have set \u03b2 to its true value of 11.1 and then used the evidence framework to determine \u03b1, as shown in Figure 3.16. We can also see how the parameter \u03b1 controls the magnitude of the parameters {wi}, by plotting the individual parameters versus the effective number \u03b3 of parameters, as shown in Figure 3.17", "8e1ea40f-b21a-445c-b1c0-981d3a3dfad6": "In testing the \ufb01nal RL policy, they found that it won more than 80% of games played against the SL policy, and it won 85% of games played against a Go program using MCTS that simulated 100,000 games per move. The value network, whose structure was similar to that of the SL and RL policy networks except for its single output unit, received the same input as the SL and RL policy networks with the exception that there was an additional binary feature giving the current color to play. Monte Carlo policy evaluation was used to train the network from data obtained from a large number of self-play games played using the RL policy. To avoid over\ufb01tting and instability due to the strong correlations between positions encountered in self-play, the DeepMind team constructed a data set of 30 million positions each chosen randomly from a unique self-play game. Then training was done using 50 million mini-batches each of 32 positions drawn from this data set. Training took one week on 50 GPUs.\n\nThe rollout policy was learned prior to play by a simple linear network trained by supervised learning from a corpus of 8 million human moves. The rollout policy network had to output actions quickly while still being reasonably accurate", "4fe5e982-b7af-415c-b60b-51aaa3820228": "Maintaining su\ufb03cient exploration is an issue in Monte Carlo control methods.\n\nIt is not enough just to select the actions currently estimated to be best, because then no returns will be obtained for alternative actions, and it may never be learned that they are actually better. One approach is to ignore this problem by assuming that episodes begin with state\u2013action pairs randomly selected to cover all possibilities. Such exploring starts can sometimes be arranged in applications with simulated episodes, but are unlikely in learning from real experience. In on-policy methods, the agent commits to always exploring and tries to \ufb01nd the best policy that still explores. In o\u21b5-policy methods, the agent also explores, but learns a deterministic optimal policy that may be unrelated to the policy followed. O\u21b5-policy prediction refers to learning the value function of a target policy from data generated by a di\u21b5erent behavior policy. Such learning methods are based on some form of importance sampling, that is, on weighting returns by the ratio of the probabilities of taking the observed actions under the two policies, thereby transforming their expectations from the behavior policy to the target policy. Ordinary importance sampling uses a simple average of the weighted returns, whereas weighted importance sampling uses a weighted average", "531e1593-0c57-41e6-9cbf-6b15120a0c05": "We then recover the original eigenvector equation with W as the eigenvector basis:  . x'x= (u=w\") U=SWw! =w>?w'. (5.87)  The SVD is helpful to show that PCA results in a diagonal Var. Using the SVD of X, we can express the variance of X as:  Var = \u2014*_x'x (5.88) = \u2014_(u=w')'uEw\" (5.89) = \u2014_ws'u'u=w\" (5.90) - \u2014_wsw!, (5.91)  where we use the fact that U'U = I because the U matrix of the singular value decomposition is defined to be orthogonal. This shows that the covariance of z is diagonal as required:  1 Var = \u2014\u2014Z 'Z (5.92)  1 =\u2014_W X xXw (5.93) TOOT  https://www.deeplearningbook.org/contents/ml.html       \u201c146  CHAPTER 5", "586bc8d7-bbbd-4eb7-aa3e-c634f7fdb18b": "This allows us to make a Taylor series expansion of the network function around wMAP and retain only the linear terms With this approximation, we now have a linear-Gaussian model with a Gaussian distribution for p(w) and a Gaussian for p(t|w) whose mean is a linear function of w of the form We can therefore make use of the general result (2.115) for the marginal p(t) to give Exercise 5.38 where the input-dependent variance is given by We see that the predictive distribution p(t|x, D) is a Gaussian whose mean is given by the network function y(x, wMAP) with the parameter set to their MAP value.\n\nThe variance has two terms, the \ufb01rst of which arises from the intrinsic noise on the target variable, whereas the second is an x-dependent term that expresses the uncertainty in the interpolant due to the uncertainty in the model parameters w. This should be compared with the corresponding predictive distribution for the linear regression model, given by (3.58) and (3.59). So far, we have assumed that the hyperparameters \u03b1 and \u03b2 are \ufb01xed and known", "f04fdcc6-e3ce-4ec0-8363-4a83d92ed41a": "Thus, when f\u2019(z) = 0 and f\u201d(x) > 0, we can conclude that x is a local minimum. Similarly, when f\u2019(x) = 0 and f\u201d(x) <0, we can conclude that x is a local maximum. This is known as the second derivative test. Unfortunately, when f (x) = 0, the test is inconclusive. In this case \u00ab may \"\u201d  https://www.deeplearningbook.org/contents/numerical.html    be a saddle point or a part ot a flat region.\n\nIn multiple dimensions, we need to examine all the second derivatives of the function. Using the eigendecomposition of the Hessian matrix, we can generalize  the second derivative test to multiple dimensions. At a critical point, where Vaf (x) =0, we can examine the eigenvalues of the Hessian to determine whether the critical point is a local maximum, local minimum, or saddle point. When the Hessian is positive definite (all its eigenvalues are positive), the point is a local minimum", "08fd4562-2bdd-4041-b03a-0ac33461cc8c": "Journal of Experimental Psychology, 32(2):150\u2013155. Estes, W. K. Discriminative conditioning. II. E\u21b5ects of a Pavlovian conditioned stimulus Estes, W. K. Toward a statistical theory of learning. Psychololgical Review, 57(2):94\u2013107. Farley, B. G., Clark, W. A. Simulation of self-organizing systems by digital computer. Farries, M. A., Fairhall, A. L. .\n\nReinforcement learning with modulated spike timingdependent synaptic plasticity. Journal of Neurophysiology, 98(6):3648\u20133665. Feldbaum, A. A. Optimal Control Systems. Academic Press, New York. Finch, G., Culler, E. Higher order conditioning with constant motivation. The American Finnsson, H., Bj\u00a8ornsson, Y. Simulation-based approach to general game playing", "c67eef28-fc56-4f1a-a81b-011e3882158c": "815\u2013823, 2015. Sohn, K. Improved deep metric learning with multi-class n-pair loss objective. In Advances in neural information processing systems, pp. 1857\u20131865, 2016. Sohn, K., Berthelot, D., Li, C.-L., Zhang, Z., Carlini, N., Cubuk, E. D., Kurakin, A., Zhang, H., and Raffel, C. Fixmatch: Simplifying semi-supervised learning with consistency and con\ufb01dence.\n\narXiv preprint arXiv:2001.07685, 2020. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1\u20139, 2015. Tian, Y., Krishnan, D., and Isola, P. Contrastive multiview coding", "782faf16-30ab-492a-958b-ecad7c52c771": "For discrete observations, this corresponds to expanded tables of conditional probabilities for the emission distributions. In the case of a Gaussian emission density, we can use the linearGaussian framework in which the conditional distribution for xn given the values of the previous observations, and the value of zn, is a Gaussian whose mean is a linear combination of the values of the conditioning variables.\n\nClearly the number of additional links in the graph must be limited to avoid an excessive the number of free parameters. In the example shown in Figure 13.17, each observation depends on emission probabilities and the transition probabilities depend on the values of a sequence of observations u1, . , uN. the two preceding observed variables as well as on the hidden state. Although this graph looks messy, we can again appeal to d-separation to see that in fact it still has a simple probabilistic structure. In particular, if we imagine conditioning on zn we see that, as with the standard HMM, the values of zn\u22121 and zn+1 are independent, corresponding to the conditional independence property (13.5)", "736a6eb0-350a-47d1-abd1-bcf2441d2dc5": "GANs o\ufb00er much more \ufb02exibility in the de\ufb01nition of the objective function, including Jensen-Shannon , and all f-divergences  as well as some exotic combinations . On the other hand, training GANs is well known for being delicate and unstable, for reasons theoretically investigated in . In this paper, we direct our attention on the various ways to measure how close the model distribution and the real distribution are, or equivalently, on the various ways to de\ufb01ne a distance or divergence \u03c1(P\u03b8, Pr). The most fundamental di\ufb00erence between such distances is their impact on the convergence of sequences of probability distributions. A sequence of distributions (Pt)t\u2208N converges if and only if there is a distribution P\u221e such that \u03c1(Pt, P\u221e) tends to zero, something that depends on how exactly the distance \u03c1 is de\ufb01ned. Informally, a distance \u03c1 induces a weaker topology when it makes it easier for a sequence of distribution to converge.1 Section 2 clari\ufb01es how popular probability distances di\ufb00er in that respect.\n\nIn order to optimize the parameter \u03b8, it is of course desirable to de\ufb01ne our model distribution P\u03b8 in a manner that makes the mapping \u03b8 \ufffd\u2192 P\u03b8 continuous", "38402d87-2403-412b-863f-9850cafdddbb": "This type of distribution is what we expect in many classification problems, and it would make MCMC methods converge very slowly because of poor mixing between modes. 598  https://www.deeplearningbook.org/contents/monte_carlo.html    Figure 17.1: Paths followed by Gibbs sampling for three distributions, with the Markov chain initialized at the mode in both cases. (Left)A multivariate normal distribution with two independent variables. Gibbs sampling mixes well because the variables are independent. (Center)A multivariate normal distribution with highly correlated variables. The correlation between variables makes it difficult for the Markov chain to mix. Because the update for each variable must be conditioned on the other variable, the correlation reduces the rate at which the Markov chain can move away from the starting point. (Right)A mixture of Gaussians with widely separated modes that are not axis aligned. Gibbs sampling mixes very slowly because it is difficult to change modes while altering  only one variable at a time", "6abbc9b4-ae74-4d12-b0d4-ee499c6a7a48": "However, there remain at least three ways in which the convergence of the residualgradient method is unsatisfactory. The \ufb01rst of these is that empirically it is slow, much slower that semi-gradient methods. Indeed, proponents of this method have proposed increasing its speed by combining it with faster semi-gradient methods initially, then gradually switching over to residual gradient for the convergence guarantee . The second way in which the residual-gradient algorithm is unsatisfactory is that it still seems to converge to the wrong values. It does get the right values in all tabular cases, such as the A-split example, as for those an exact solution to the Bellman Example 11.3: A-presplit example, a counterexample for the BE Consider the three-state episodic MRP shown to the right: Episodes start in either A1 or A2, with equal probability.\n\nThese two states look exactly the same to the function approximator, like a single state A whose feature representation is distinct from and unrelated to the feature representation of the other two states, B and C, which are also distinct from each other", "beb697eb-e025-48c3-8300-1269c9ba8867": "We \ufb01rst consider the most common type of experience, namely, data instances, which are assumed to be independent and identically distributed (i.i.d.). Such data instance experience can appear in a wide range of contexts, including supervised, self-supervised, unsupervised, actively supervised, and other scenarios with data augmentation and manipulation. Figure 2 illustrates the experience functions based on the data instances. 4.1.1. Supervised data instances. Without loss of generality and for consistency of notations with the rest of the section, we consider data instances to consist of a pair of input-output variables, namely t = (x, y). For example, in image classi\ufb01cation, x represents the input image and y is the object label. In the supervised setting, we observe the full data drawn i.i.d", "de221e46-4b5e-4dd2-8e38-5ecdffb2b08d": "The node c is said to be tail-to-tail with respect to this path because the node is connected to the tails of the two arrows, and the presence of such a path connecting nodes a and b causes these nodes to be dependent. However, when we condition on node c, as in Figure 8.16, the conditioned node \u2018blocks\u2019 the path from a to b and causes a and b to become (conditionally) independent. We can similarly consider the graph shown in Figure 8.17.\n\nThe joint distribution corresponding to this graph is again obtained from our general formula (8.5) to give First of all, suppose that none of the variables are observed. Again, we can test to see if a and b are independent by marginalizing over c to give which in general does not factorize into p(a)p(b), and so Now suppose we condition on node c, as shown in Figure 8.18. Using Bayes\u2019 theorem, together with (8.26), we obtain and so again we obtain the conditional independence property As before, we can interpret these results graphically. The node c is said to be head-to-tail with respect to the path from node a to node b", "5fd31b67-505d-4f8f-8f1c-acaee434fcd5": "Because of the uniform distribution over the literals, the marginal distribution over the root of the reduction tree specifies what fraction of assignments satisfy the problem. While this is a contrived worst-case example, NP hard graphs commonly arise in practical real-world scenarios. This motivates the use of approximate inference. In the context of deep learning, this usually refers to variational inference, in which we approximate the true distribution p(h | v) by seeking an approximate distribution q(h|v) that is as close to the true one as possible. This and other techniques are described in depth in chapter 19. 16.7 The Deep Learning Approach to Structured Probabilistic Models  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html   Deep learning practitioners generally use the same basic computational tools as  other machine learning practitioners who work with structured probabilistic models. 581  CHAPTER 16.\n\nSTRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  In the context of deep learning, however, we usually make different design decisions about how to combine these tools, resulting in overall algorithms and models that have a very different flavor from more traditional graphical models", "6cd79383-d1f9-48d4-8e65-d9a63edc106c": "In the E step, a sample of Z is taken from the posterior distribution p(Z|X, \u03b8old) where X is the data set. This effectively makes a hard assignment of each data point to one of the components in the mixture. In the M step, this sampled approximation to the posterior distribution is used to update the model parameters in the usual way. Now suppose we move from a maximum likelihood approach to a full Bayesian treatment in which we wish to sample from the posterior distribution over the parameter vector \u03b8.\n\nIn principle, we would like to draw samples from the joint posterior p(\u03b8, Z|X), but we shall suppose that this is computationally dif\ufb01cult. Suppose further that it is relatively straightforward to sample from the complete-data parameter posterior p(\u03b8|Z, X). This inspires the data augmentation algorithm, which alternates between two steps known as the I-step (imputation step, analogous to an E step) and the P-step (posterior step, analogous to an M step). I-step. We wish to sample from p(Z|X) but we cannot do this directly. We therefore note the relation and hence for l = 1,", "8c5bf395-1147-42d3-926a-2ecbc9a8316b": "Despite these potential problems, Samuel\u2019s checkers player using the generalization learning method approached \u201cbetter-than-average\u201d play. Fairly good amateur opponents characterized it as \u201ctricky but beatable\u201d .\n\nIn contrast to the rote-learning version, this version was able to develop a good middle game but remained weak in opening and endgame play. This program also included an ability to search through sets of features to \ufb01nd those that were most useful in forming the value function. A later version  included re\ufb01nements in its search procedure, such as alpha-beta pruning, extensive use of a supervised learning mode called \u201cbook learning,\u201d and hierarchical lookup tables called signature tables  to represent the value function instead of linear function approximation. This version learned to play much better than the 1959 program, though still not at a master level. Samuel\u2019s checkers-playing program was widely recognized as a signi\ufb01cant achievement in arti\ufb01cial intelligence and machine learning. IBM Watson1 is the system developed by a team of IBM researchers to play the popular TV quiz show Jeopardy!.2 It gained fame in 2011 by winning \ufb01rst prize in an exhibition match against human champions", "57382daa-617e-4ff6-bde2-d7170b41934f": "For a TD method, for instance, the reinforcement signal at time t is the TD error \u03b4t\u22121 = Rt + \u03b3V (St) \u2212 V (St\u22121).1 The 1 As we mentioned in Section 6.1, \u03b4t in our notation is de\ufb01ned to be Rt+1 + \u03b3V (St+1) \u2212 V (St), so \u03b4t reinforcement signal for some algorithms could be just the reward signal, but for most of the algorithms we consider the reinforcement signal is the reward signal adjusted by other information, such as the value estimates in TD errors. Estimates of state values or of action values, that is, V or Q, specify what is good or bad for the agent over the long run. They are predictions of the total reward an agent can expect to accumulate over the future. Agents make good decisions by selecting actions leading to states with the largest estimated state values, or by selecting actions with the largest estimated action values. Prediction errors measure discrepancies between expected and actual signals or sensations.\n\nReward prediction errors (RPEs) speci\ufb01cally measure discrepancies between the expected and the received reward signal, being positive when the reward signal is greater than expected, and negative otherwise", "fbf76ce0-8261-4000-be51-6709804e8365": "To see that this is indeed conjugate, let us multiply the prior (2.229) by the likelihood function (2.227) to obtain the posterior distribution, up to a normalization coef\ufb01cient, in the form This again takes the same functional form as the prior (2.229), con\ufb01rming conjugacy. Furthermore, we see that the parameter \u03bd can be interpreted as a effective number of pseudo-observations in the prior, each of which has a value for the suf\ufb01cient statistic u(x) given by \u03c7. In some applications of probabilistic inference, we may have prior knowledge that can be conveniently expressed through the prior distribution.\n\nFor example, if the prior assigns zero probability to some value of variable, then the posterior distribution will necessarily also assign zero probability to that value, irrespective of any subsequent observations of data. In many cases, however, we may have little idea of what form the distribution should take. We may then seek a form of prior distribution, called a noninformative prior, which is intended to have as little in\ufb02uence on the posterior distribution as possible . This is sometimes referred to as \u2018letting the data speak for themselves\u2019", "6dd3ffcc-650f-4d78-97fe-748b9e5cb4ef": "OPTIMIZATION FOR TRAINING DEEP MODELS  RMSProp is shown in its standard form in algorithm 8.5 and combined with Nesterov momentum in algorithm 8.6. Compared to AdaGrad, the use of the moving average introduces a new hyperparameter, p, that controls the length scale of the moving average. Empirically, RMSProp has been shown to be an effective and practical op- timization algorithm for deep neural networks. It is currently one of the go-to optimization methods being employed routinely by deep learning practitioners. https://www.deeplearningbook.org/contents/optimization.html       8.5.3 Adam  Adam  is yet another adaptive learning rate optimization algorithm and is presented in algorithm 8.7.\n\nThe name \u201cAdam\u201d derives from he phrase \u201cadaptive moments.\u201d In the context of the earlier algorithms, it is perhaps best seen as a variant on the combination of RMSProp and momentum with a few important distinctions. First, in Adam, momentum is incorporated directly as an estimate of the first-order moment (with exponential weighting) of he gradient. The most straightforward way to add momentum to RMSProp is to apply momentum to the rescaled gradients", "3b612df6-781f-46eb-a685-c591ff633bc9": "the groups: q(y) = \ufffdM i=1 qi(yi). The variational principle summarized in  gives a more principled interpretation of the mean-\ufb01eld and other approximation methods. In particular, in the case where p\u03b8(x, y) is an exponential family distribution with su\ufb03cient statistics T(x, y), the exact E-step (Equation 2.10) can be interpreted as seeking the optimal valid mean parameters (i.e., expected su\ufb03cient statistics) for which the free energy is minimized. For discrete latent variables y, the set of all valid mean parameters constitutes a marginal polytope M. In this perspective, the mean-\ufb01eld methods (Equation 2.12) correspond to replacing M with an inner approximation M\u2032 \u2286 M. With the restricted set M\u2032 of mean parameters, the E-step generally no longer tightens the bound of the negative marginal log-likelihood, and the algorithm does not necessarily decrease the negative marginal log-likelihood monotonically.\n\nHowever, the algorithm preserves the property that it minimizes the upper bound of the negative marginal log-likelihood. Besides the mean\ufb01eld methods, there are other approaches for approximation such as belief propagation", "63e3b91f-4957-4a16-ba02-190318633272": "The log-likelihood can be decomposed as  log Pmodel (#) = log S> Pinodel (h, x). (14.3) h  We can think of the autoencoder as approximating this sum with a point estimate for just one highly likely value for h. This is similar to the sparse coding generative model (section 13.4), but with h being the output of the parametric encoder rather than the result of an optimization that infers the most likely h. From this point of view, with this chosen h, we are maximizing  log Pmodel (A, x) = log Pmodel(h) + log Pmodel (x | h). (14.4)  The log Pmodel(h) term can be sparsity inducing. For example, the Laplace prior, XX _yin. Pmodel (hi) = 9\u00b0 Alhal (14.5)  corresponds to an absolute value sparsity penalty", "c6b7fc37-dd66-4d8a-a261-c97878a1d08b": "The quantity p(D|w) on the right-hand side of Bayes\u2019 theorem is evaluated for the observed data set D and can be viewed as a function of the parameter vector w, in which case it is called the likelihood function. It expresses how probable the observed data set is for different settings of the parameter vector w. Note that the likelihood is not a probability distribution over w, and its integral with respect to w does not (necessarily) equal one. Given this de\ufb01nition of likelihood, we can state Bayes\u2019 theorem in words where all of these quantities are viewed as functions of w. The denominator in (1.43) is the normalization constant, which ensures that the posterior distribution on the left-hand side is a valid probability density and integrates to one.\n\nIndeed, integrating both sides of (1.43) with respect to w, we can express the denominator in Bayes\u2019 theorem in terms of the prior distribution and the likelihood function In both the Bayesian and frequentist paradigms, the likelihood function p(D|w) plays a central role. However, the manner in which it is used is fundamentally different in the two approaches", "d9dbd406-4e0a-45e8-a35d-4d18ecdbe4a6": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ronan Le Bras, Ji-Ping Wang, Chandra Bhagavatula, Yejin Choi, and Doug Downey. 2020. Generative data augmentation for commonsense reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1008\u20131025, Online.\n\nAssociation for Computational Linguistics. Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and Taylor Berg-Kirkpatrick. 2017. Improved variational autoencoders for text modeling using dilated convolutions. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML\u201917, page 3881\u20133890. JMLR.org. David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In 33rd annual meeting of the association for computational linguistics, pages 189\u2013196", "6e4120ae-73b2-4bf6-b0ff-4921b9177960": "In one dimension, the Wishart reduces to the gamma distribution Gam(\u03bb|a, b) given by (B.26) with parameters a = \u03bd/2 and b = 1/2W. In this appendix, we gather together some useful properties and identities involving matrices and determinants. This is not intended to be an introductory tutorial, and it is assumed that the reader is already familiar with basic linear algebra. For some results, we indicate how to prove them, whereas in more complex cases we leave the interested reader to refer to standard textbooks on the subject. In all cases, we assume that inverses exist and that matrix dimensions are such that the formulae are correctly de\ufb01ned. A comprehensive discussion of linear algebra can be found in Golub and Van Loan , and an extensive collection of matrix properties is given by L\u00a8utkepohl . Matrix derivatives are discussed in Magnus and Neudecker . A matrix A has elements Aij where i indexes the rows, and j indexes the columns", "9f7841f2-c178-4300-a190-abfb70af4bab": "MACHINE LEARNING BASICS  length of vector. In Section 9.7 and chapter 10, we describe how to handle different types of such heterogeneous data. In cases like these, rather than describing the dataset as a matrix with m rows, we describe it as a set containing m elements: fa, we)... al}, This notation does not imply that any two example vectors x and a) have the same size.\n\nIn the case of supervised learning, the example contains a label or target as well as a collection of features. For example, if we want to use a learning algorithm  https://www.deeplearningbook.org/contents/ml.html    to perform object recognition from photographs, we need to specify which object appears in each of the photos. We might do this with a numeric code, with 0 signifying a person, 1 signifying a car, 2 signifying a cat, and so forth. Often when working with a dataset containing a design matrix of feature observations X, we also provide a vector of labels y, with y; providing the label for example 7. Of course, sometimes the label may be more than just a single number", "97c892a2-8055-46dd-8ebe-b7fd67b51a7f": "At each step of the iteration, one of the hyperparameters has a small value and the remaining two have large values, so that two of the three latent variables are suppressed. During the course of the Gibbs sampling, the solution makes sharp transitions between the three modes. The model described here involves a prior only over the matrix W. A fully Bayesian treatment of PCA, including priors over 1-\", a 2 , and n, and solved using variational methods, is described in Bishop . For a discussion of various Bayesian approaches to detennining the appropriate dimensionality for a PCA model, see Minka .\n\nFactor analysis is a linear-Gaussian latent variable model that is closely related to probabilistic PCA. Its definition differs from that of probabilistic PCA only in that the conditional distribution of the observed variable x given the latent variable z is taken to have a diagonal rather than an isotropic covariance so that where ill is a D x D diagonal matrix. Note that the factor analysis model, in common with probabilistic PCA. assumes that the observed variables Xl, ... ,Xo are independent", "f472e727-3283-42be-a2ef-cfae8f5bbc35": "MAP Bayesian inference with a Gaussian prior on the weights thus corresponds to weight decay. As with full Bayesian inference, MAP Bayesian inference has the advantage of leveraging information that is brought by the prior and cannot be found in the training data. This additional information helps to reduce the variance in the MAP point estimate (in comparison to the ML estimate). However, it does so at the price of increased bias. Many regularized estimation strategies, such as maximum likelihood learning regularized with weight decay, can be interpreted as making the MAP approxima- tion to Bayesian inference. This view applies when the regularization consists of adding an extra term to the objective function that corresponds to log p(@).\n\nNot all regularization penalties correspond to MAP Bayesian inference. For example, some regularizer terms may not be the logarithm of a probability distribution. Other regularization terms depend on the data, which of course a prior probability distribution is not allowed to do. https://www.deeplearningbook.org/contents/ml.html    MAP Bayesian inference provides a straightforward way to design complicated yet interpretable regularization terms", "5fb3438e-0462-4f49-aa6b-0a483a15c956": "Psychological Review, 97(2):285\u2013308.\n\nReddy, G., Celani, A., Sejnowski, T. J., Vergassola, M. Learning to soar in turbulent environments. Proceedings of the National Academy of Sciences, 113(33):E4877\u2013E4884. Redish, D. A. Addiction as a computational process gone awry. Science, 306:1944\u2013 Reetz, D. Approximate solutions of a discounted Markovian decision process. Bonner Rescorla, R. A., Wagner, A. R. A theory of Pavlovian conditioning: Variations in the e\u21b5ectiveness of reinforcement and nonreinforcement. In A. H. Black and W. F. Prokasy (Eds. ), Classical Conditioning II, pp. 64\u201399. Appleton-Century-Crofts, New York. Revusky, S., Garcia, J", "7e32121f-c167-4aa6-a94e-52336835abab": "We observe that the cross is surrounded by numerous red points, and so we might suppose that it belongs to the red class. However, there are also plenty of green points nearby, so we might think that it could instead belong to the green class. It seems unlikely that it belongs to the blue class. The intuition here is that the identity of the cross should be determined more strongly by nearby points from the training set and less strongly by more distant points. In fact, this intuition turns out to be reasonable and will be discussed more fully in later chapters. How can we turn this intuition into a learning algorithm? One very simple approach would be to divide the input space into regular cells, as indicated in Figure 1.20. When we are given a test point and we wish to predict its class, we \ufb01rst decide which cell it belongs to, and we then \ufb01nd all of the training data points that fall in the same cell.\n\nThe identity of the test point is predicted as being the same as the class having the largest number of training points in the same cell as the test point (with ties being broken at random). There are numerous problems with this naive approach, but one of the most severe becomes apparent when we consider its extension to problems having larger numbers of input variables, corresponding to input spaces of higher dimensionality", "a6dd1578-2b35-4678-ab9d-0b0f22d3a3f2": "Monte Carlo matrix inversion and reinforcement learning. In Advances in Neural Information Processing Systems 6 , pp. 687\u2013694. Morgan Kaufmann, San Francisco. Barto, A. G., Jordan, M. I. .\n\nGradient following without back-propagation in layered Conference on Neural Networks, pp. II629\u2013II636. SOS Printing, San Diego. Barto, A. G., Mahadevan, S. Recent advances in hierarchical reinforcement learning. Barto, A. G., Singh, S. P. On the computational economics of reinforcement learning. In Connectionist Models: Proceedings of the 1990 Summer School. Morgan Kaufmann. Barto, A. G., Sutton, R. S. Goal seeking components for adaptive intelligence: An initial assessment. Technical Report AFWAL-TR-81-1070. Air Force Wright Aeronautical Laboratories/Avionics Laboratory, Wright-Patterson AFB, OH", "9e2dad4a-230a-426c-8412-35cffd503667": "To provide a concrete example, we tested a feedforward network using h = cos(Wa-+ 6) on the MNIST dataset and obtained an error rate of less than 1 percent, which is competitive with results obtained using more conventional activation functions. During research and development of new techniques, it is common to test many different activation functions and find that several variations on standard practice perform comparably. This means that usually new hidden unit types are published only if they are clearly demonstrated to provide a significant improvement. New hidden unit types that perform roughly comparably to known types are so common as to be uninteresting.\n\nIt would be impractical to list all the hidden unit types that have appeared in the literature. We highlight a few especially useful and distinctive ones. One possibility is to not have an activation g(z) at all. One can also think of this as using the identity function as the activation function. We have already seen that a linear unit can be useful as the output of a neural network. It may also be used as a hidden unit", "a2b92dc0-a5e1-44de-974f-663b970c81b6": "where <1, is the ,'anaoce of :c,.\n\nThis i< known as the (\",,,el,,,;,,,, matri.' of the original dota and ha' the propeny thai if t\"\"o rompooent, X; and x, of the data are perfee1ly correl.ted. then Ai _ I.\u2022nd if they a.-e uocorrelated. then Ai _ O. 11\",,'1\"\"', using PeA we can make a It>Of'e subst.mial nonnalizat;oo of the data to gi\\'C it zero mean and unit co'\u00b7ariance. so that different \"anables become derorrelate<l To do this. we first \"\"rile the ei8Cn\"cclor equation (12, 17) in the form su= UL (12.23) \u00b1A~/2. The plot on the right shows the result of whitening of the data to give it zero mean and unit covariance", "9ccb50c2-da7d-46dd-bee0-384ffb109112": "There are many potential sources of bias that could separate the Shorten and Khoshgoftaar J Big Data  6:60   Contrast +20% \u2014_ Hist.equalization  Fig. 3. Examples of Color Augmentations provided by Mikolajczyk and Grochowski  in the domain of melanoma classification  distribution of the training data from the testing data. If positional biases are pre- sent, such as in a facial recognition dataset where every face is perfectly centered in the frame, geometric transformations are a great solution. In addition to their pow- erful ability to overcome positional biases, geometric transformations are also use- ful because they are easily implemented.\n\nThere are many imaging processing libraries that make operations such as horizontal flipping and rotation painless to get started with. Some of the disadvantages of geometric transformations include additional memory, transformation compute costs, and additional training time. Some geo- metric transformations such as translation or random cropping must be manually observed to make sure they have not altered the label of the image", "f91615c1-a138-44bf-ab09-a9e9496951c0": "For example, in Chapter 7 we shall introduce the relevance vector machine, which is a Bayesian model having one complexity parameter for every training data point. The Bayesian view of model comparison simply involves the use of probabilities to represent uncertainty in the choice of model, along with a consistent application of the sum and product rules of probability. Suppose we wish to compare a set of L models {Mi} where i = 1, . , L. Here a model refers to a probability distribution over the observed data D. In the case of the polynomial curve-\ufb01tting problem, the distribution is de\ufb01ned over the set of target values t, while the set of input values X is assumed to be known.\n\nOther types of model de\ufb01ne a joint distributions over X and t. We shall suppose that the data is generated from one of these models but we Section 1.5.4 are uncertain which one. Our uncertainty is expressed through a prior probability distribution p(Mi). Given a training set D, we then wish to evaluate the posterior distribution p(Mi|D) \u221d p(Mi)p(D|Mi). (3.66) The prior allows us to express a preference for different models", "d878cba7-7fe9-4755-9aac-630ef63febf4": "Require: \u03c6 (Current value of variational parameters) where \u00b5z and \u03c3z are yet unspeci\ufb01ed functions of x. Since they are Gaussian, we can parameterize the variational approximate posteriors: With \u2299 we signify an element-wise product. These can be plugged into the lower bound de\ufb01ned above (eqs (21) and (22)). In this case it is possible to construct an alternative estimator with a lower variance, since in this model p\u03b1(\u03b8), p\u03b8(z), q\u03c6(\u03b8) and q\u03c6(z|x) are Gaussian, and therefore four terms of f\u03c6 can be solved analytically. The resulting estimator is:", "aff3dc44-9b78-4002-99e0-c2118432e65c": "Di\u21b5erential value functions also have Bellman equations, just slightly di\u21b5erent from those we have seen earlier.\n\nWe simply remove all \u03b3s and replace all rewards by the di\u21b5erence between the reward and the true average reward: There is also a di\u21b5erential form of the two TD errors: where \u00afRt is an estimate at time t of the average reward r(\u21e1). With these alternate de\ufb01nitions, most of our algorithms and many theoretical results carry through to the average-reward setting without change. For example, the average reward version of semi-gradient Sarsa is de\ufb01ned just as in (10.2) except with the di\u21b5erential version of the TD error. That is, by Exercise 10.4 Give pseudocode for a di\u21b5erential version of semi-gradient Q-learning", "e091c5f5-048b-43f9-a03e-17dc1195ab39": "Typical applications involved learning to discriminate simple shapes or characters.\n\nAt the same time that the perceptron was being developed, a closely related system called the adaline, which is short for \u2018adaptive linear element\u2019, was being explored by Widrow and co-workers. The functional form of the model was the same as for the perceptron, but a different approach to training was adopted . We turn next to a probabilistic view of classi\ufb01cation and show how models with linear decision boundaries arise from simple assumptions about the distribution of the data. In Section 1.5.4, we discussed the distinction between the discriminative and the generative approaches to classi\ufb01cation. Here we shall adopt a generative approach in which we model the class-conditional densities p(x|Ck), as well as the class priors p(Ck), and then use these to compute posterior probabilities p(Ck|x) through Bayes\u2019 theorem. Consider \ufb01rst of all the case of two classes. The posterior probability for class and \u03c3(a) is the logistic sigmoid function de\ufb01ned by which is plotted in Figure 4.9. The term \u2018sigmoid\u2019 means S-shaped", "4add089f-7c1d-40a3-9929-7d515ad56961": "Statistical learning theory. John Wiley and Sons. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., & Polosukhin, I. Attention is all you need. Proceedings of the 31st International Conference on Neural Information Processing Systems, 6000\u20136010. Wainwright, M. J., & Jordan, M. I. (n.d.). A variational principle for graphical models. New Directions in Statistical Signal Processing, 155. Wainwright, M. J., & Jordan, M. I. Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning, 1\u2013305. Wilson, A. G., Hu, Z., Salakhutdinov, R. R., & Xing, E. P. Stochastic variational deep kernel learning", "6dd26673-5d18-4048-9b65-1796a4c8e829": "Including more positive samples into the set NV; leads to improved results. According to their experiments, supervised contrastive loss:  does outperform the base cross entropy, but only by a small amount. https://lilianweng.github.io/posts/2021-05-31-contrastive/  Contrastive Representation Learning | Lil'Log   outperforms the cross entropy on robustness benchmark (ImageNet-C, which applies common naturally occuring perturbations such as noise, blur and contrast changes to the ImageNet dataset). is less sensitive to hyperparameter changes. Language: Sentence Embedding  In this section, we focus on how to learn sentence embedding. Text Augmentation  Most contrastive methods in vision applications depend on creating an augmented version of each image.\n\nHowever, it is more challenging to construct text augmentation which does not alter the semantics of a sentence. In this section we look into three approaches for augmenting text sequences, including lexical edits, back-translation and applying cutoff or dropout. Lexical Edits  EDA  defines a set of simple but powerful operations for text augmentation", "6d80d0d0-493a-4cad-bbe8-92d89a57d1e2": "12.4.5 Neural Machine Translation  Machine translation is the task of reading a sentence in one natural language and emitting a sentence with the equivalent meaning in another language. Machine translation systems often involve many components. At a high level, there is often one component that proposes many candidate translations. Many of these translations will not be grammatical due to differences between the languages.\n\nFor example, many languages put adjectives after nouns, so when translated to English directly they yield phrases such as \u201capple red.\u201d The proposal mechanism suggests many variants of the suggested translation, ideally including \u201cred apple.\u201d A second component of the translation system, a language model, evaluates the proposed translations and can score \u201cred apple\u201d as better than \u201capple red.\u201d  The earliest exploration of neural networks for machine translation already incorporated the idea of encoder and decoder , while the first large-scale competitive use of neural networks in translation was to upgrade the language model of a translation system by using a neural language model . Previously, most  https://www.deeplearningbook.org/contents/applications.html  machine translation systems had used an 7-gram model for this component", "a090ff39-fc9d-4169-aeb2-f0b93dd855f8": "For example, Monte Carlo methods (Chapter 5) update a state based on all the future rewards, and n-step TD methods (Chapter 7) update based on the next n rewards and state n steps in the future. Such formulations, based on looking forward from the updated state, are called forward views. Forward views are always somewhat complex to implement because the update depends on later things that are not available at the time. However, as we show in this chapter it is often possible to achieve nearly the same updates\u2014and sometimes exactly the same updates\u2014with an algorithm that uses the current TD error, looking backward to recently visited states using an eligibility trace. These alternate ways of looking at and implementing learning algorithms are called backward views. Backward views, transformations between forward views and backward views, and equivalences between them, date back to the introduction of temporal di\u21b5erence learning but have become much more powerful and sophisticated since 2014. Here we present the basics of the modern view. As usual, \ufb01rst we fully develop the ideas for state values and prediction, then extend them to action values and control", "99f94950-4e01-41f8-a84b-076769f7c20c": "We thus can see the regression target in the above objective as an approximation to the advantage function: \u02dcA\u00af\u03b8 (st, at) := \u2212V\u00af\u03b8 (st) + \u03b3V\u00af\u03b8 (st+1) + rt. Therefore, by optimizing the regression objective, log \u03c0\u03b8(at|st), which is the log probability of generating token at given preceding tokens st, is encouraged to match the approximate advantage value \u02dcA\u00af\u03b8 (st, at), no more and no less. This is different from the objective of MLE where the model is trained to (blindly) increase the probability of the observed token at given st and decrease the probability of the rest. (11) Recall that \u03c0\u2032 is an arbitrary behavior policy (e.g., data distribution), and Q\u00af\u03b8 is the target Q-network which is a slow copy of the Q\u03b8 to be learned and is held \ufb01xed during the gradient updates.\n\nHowever, the above objective is inef\ufb01cient due to exact the same reasons as in standard Q-learning discussed earlier, namely the unstable per-step bootstrappingstyle training with sparse reward signals, plus the slow updates w.r.t only one token at out of the large vocabulary (action space)", "53171e78-71d3-494e-b461-c4af8187fae0": "For example, if we want to train a speech recognition system to transcribe entire sentences, then the label for each example sentence is a sequence of words. Just as there is no formal definition of supervised and unsupervised learning, there is no rigid taxonomy of datasets or experiences. The structures described here cover most cases, but it is always possible to design new ones for new applications. 5.1.4 Example: Linear Regression  Our definition of a machine learning algorithm as an algorithm that is capable of improving a computer program\u2019s performance at some task via experience is somewhat abstract. To make this more concrete, we present an example of a simple machine learning algorithm: linear regression.\n\nWe will return to this example repeatedly as we introduce more machine learning concepts that help to understand the algorithm\u2019s behavior. As the name implies, linear regression solves a regression problem. In other words, the goal is to build a system that can take a vector x \u20ac R\u201d as input and predict the value of a scalar y \u20ac R as its output. The output of linear regression is a linear function of the input. Let \u00a2 be the value that our model predicts y should take on", "fd868b2b-bc26-43c8-97db-81962750ce83": "Most bootstrapping approaches rely on pseudo-labels or cluster indices, but BYOL directly boostrapps the latent representation. It is quite interesting and surprising that without negative samples, BYOL still works well.\n\nLater | ran into this post by Abe Fetterman & Josh Albrecht, they highlighted two surprising findings while they were trying to reproduce BYOL:  BYOL generally performs no better than random when batch normalization is removed. The presence of batch normalization implicitly causes a form of contrastive learning. They believe that using negative samples is important for avoiding model collapse (i.e. what if you use all-zeros representation for every data point?). Batch normalization injects dependency on negative samples inexplicitly because no matter how similar a batch of inputs are, the values are re-distributed (spread out ~ N(0, 1) and therefore batch normalization prevents model collapse. Strongly recommend you to read the full article if you are working in this area", "d1e15f03-1d2e-46a6-93a9-dbf237599975": "riting these as (2) and (h2), we obtain  1 G(hy | v) = exp (-3 [hi + (hd) + v? + hiw7 + (h3)w3 (19.66) \u20142vhw1 \u2014 2u(h2)we + 2h (hau) : (19.67)  From this, we can see that q has the functional form of a Gaussian. We can thus conclude q(h | v) =.N(h; uw, 8-1) where p and diagonal are variational parameters, which we can optimize using any technique we choose.\n\nIt is important to recall that we did not ever assume that gq would be Gaussian; its Gaussian form was derived automatically by using calculus of variations to maximize q with respect to \u00a3. Using the same approach on a different model could yield a different functional form of q. This was, of course, just a small case constructed for demonstration purposes. For examples of real applications of variational learning with continuous variables in the context of deep learning, see Goodfellow et al. 647  CHAPTER 19", "fd7cfda5-071a-468b-b500-858af288c04d": "Consider a cubic (M = 3) polynomial in D dimensions, and evaluate numerically the total number of independent parameters for (i) D = 10 and (ii) D = 100, which correspond to typical small-scale and medium-scale machine learning applications. Using integration by parts, prove the relation \u0393(x + 1) = x\u0393(x). Show also that \u0393(1) = 1 and hence that \u0393(x + 1) = x! when x is an integer. 1.18 (\u22c6 \u22c6) www We can use the result (1.126) to derive an expression for the surface area SD, and the volume VD, of a sphere of unit radius in D dimensions. To do this, consider the following result, which is obtained by transforming from Cartesian to polar coordinates Using the de\ufb01nition (1.141) of the Gamma function, together with (1.126), evaluate both sides of this equation, and hence show that Next, by integrating with respect to radius from 0 to 1, show that the volume of the unit sphere in D dimensions is given by Finally, use the results \u0393(1) = 1 and \u0393(3/2) = \u221a\u03c0/2 to show that (1.143) and (1.144) reduce to the usual expressions for D = 2 and D = 3", "9c01dbc7-df25-456b-a835-32e0595387fe": "7: 8: Update 8 \u2014 8 \u2014 BYo oz zy Lr: (for) 9: end while  First-Order MAML  https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Log   The meta-optimization step above relies on second derivatives. To make the computation less expensive, a modified version of MAML omits second derivatives, resulting in a simplified and cheaper implementation, known as First-Order MAML (FOMAML). Let's consider the case of performing k inner gradient steps, k > 1. Starting with the initial model parameter 0, neta!\n\n= Omneta 91 = 0) \u2014 aVeL\u201d (4) 62 = 0; \u2014 aVgl (6;) 0, = On-1 \u2014 AV pL (0,4) Then in the outer loop, we sample a new data batch for updating the meta-objective. Ometa \u2014 Oneta ~~ PBomamu ; update for meta-objective where guamt = VoL?) (x) = Vo\u00a3 D (Ox)", "dadd191e-a870-4b69-b91d-4f624fce63c3": "(19.48)  For continuous values, the expectation is an integral: H{p| =\u2014-  with respect to the function p(x), because the result might not be a probability distribution. Instead, we need to use Lagrange multipliers to add a constraint that p(x) integrate to 1. Also, the entropy should increase without bound as the variance increases. This makes the question of which distribution has the greatest entropy uninteresting. Instead, we ask which distribution has maximal entropy for fixed variance o?. Finally, the problem  wee Dae dnb act nd Lanneene 2 Ataaeth cata ane. Le ALi nd 2 2Ltaentlee -eta nest  https://www.deeplearningbook.org/contents/inference.html    iS ULUCLUCLELININCU VECAUSE LUE UISLLIVULIOLL Call VE SUULCU aALVUILLaLIy WILLOUL changing the entropy. To impose a unique solution, we add a constraint that the mean of the distribution be 4", "260fe5e7-7d00-4c12-b4d9-4dda98f4bf10": "To compare alternatives we need to estimate the value of all the actions from each state, not just the one we currently favor. This is the general problem of maintaining exploration, as discussed in the context of the k-armed bandit problem in Chapter 2. For policy evaluation to work for action values, we must assure continual exploration. One way to do this is by specifying that the episodes start in a state\u2013action pair, and that every pair has a nonzero probability of being selected as the start.\n\nThis guarantees that all state\u2013action pairs will be visited an in\ufb01nite number of times in the limit of an in\ufb01nite number of episodes. We call this the assumption of exploring starts. The assumption of exploring starts is sometimes useful, but of course it cannot be relied upon in general, particularly when learning directly from actual interaction with an environment. In that case the starting conditions are unlikely to be so helpful. The most common alternative approach to assuring that all state\u2013action pairs are encountered is to consider only policies that are stochastic with a nonzero probability of selecting all actions in each state. We discuss two important variants of this approach in later sections. For now, we retain the assumption of exploring starts and complete the presentation of a full Monte Carlo control method", "8a1ce9e2-d221-49c3-a8a2-98cdc86f4b3d": "Suppose that h is the top hidden layer used to predict the output probabilities y. If we parametrize the transformation from h to y with learned weights W  460  CHAPTER 12. APPLICATIONS  and learned biases b, then the affine-softmax output layer performs the following computations:  https://www.deeplearningbook.org/contents/applications.html    ai = bi+ So Wits Vie {1,...,|V]}, (12.8) i = \u2014<\u2014_. (12.9) View  If h contains nm, elements, then the above operation is O(|V|np).\n\nWith nj; in the thousands and |V| in the hundreds of thousands, this operation dominates the computation of most neural language models. 12.4.3.1 Use of a Short List  The first neural language models  dealt with the high cost of using a softmax over a large number of output words by limiting the vocabulary size to 10,000 or 20,000 words", "95d20862-5e64-48fc-b174-b6cf77e195b5": "To see a concrete example of how the divergence function may in\ufb02uence the learning, consider the experience to be data instances with the data distribution pd(t), and, following the con\ufb01gurations of supervised MLE (Section 4.1.1), set f = fdata, \u03b1 = 1, \u03b2 = \u03f5, and the uncertainty measure H to be the Shannon entropy. As a result, the solution of q in Equation 3.2 reduces to the data distribution q(t) = pd(t). The learning of the model thus reduces to minimizing the divergence between the model and data distributions: which is a common objective shared by many ML algorithms depending on how the divergence function is specialized. Thus, in this setting, the divergence function directly determines the learning objective. In the following sections, we will see other richer in\ufb02uences of D in combination with other SE components. We next discuss some of the common choices for the divergence function, which opens up the door to recover and generalize more learning algorithms besides those discussed earlier, such as the generative adversarial learning  that is widely used to simulate complex distributions (e.g., natural image distributions). 5.1", "8299d8d9-656f-45d3-a1d9-55d5d744b029": "We now prove that equality can hold only when both \u21e10 and \u21e1 are optimal among the \"-soft policies, that is, when they are better than or equal to all other \"-soft policies. Consider a new environment that is just like the original environment, except with the requirement that policies be \"-soft \u201cmoved inside\u201d the environment.\n\nThe new environment has the same action and state set as the original and behaves as follows. If in state s and taking action a, then with probability 1 \u2212 \" the new environment behaves exactly like the old environment. With probability \" it repicks the action at random, with equal probabilities, and then behaves like the old environment with the new, random action. The best one can do in this new environment with general policies is the same as the best one could do in the original environment with \"-soft policies. Let ev\u21e4 and eq\u21e4 denote the optimal value functions for the new environment. Then a policy \u21e1 is optimal among \"-soft policies if and only if v\u21e1 = ev\u21e4", "57abd193-7f3d-42ed-81ec-d45de80132ea": "Some neuroscience models developed at this time are well interpreted in terms of temporal-di\u21b5erence learning (Hawkins and Kandel, 1984; Byrne, Gingrich, and Baxter, 1990; Gelperin, Hop\ufb01eld, and Tank, 1985; Tesauro, 1986; Friston et al., 1994), although in most cases there was no historical connection. Our early work on temporal-di\u21b5erence learning was strongly in\ufb02uenced by animal learning theories and by Klopf\u2019s work. Relationships to Minsky\u2019s \u201cSteps\u201d paper and to Samuel\u2019s checkers players were recognized only afterward. By 1981, however, we were fully aware of all the prior work mentioned above as part of the temporal-di\u21b5erence and trial-and-error threads.\n\nAt this time we developed a method for using temporal-di\u21b5erence learning combined with trial-and-error learning, known as the actor\u2013critic architecture, and applied this method to Michie and Chambers\u2019s pole-balancing problem . This method was extensively studied in Sutton\u2019s  Ph.D. dissertation and extended to use backpropagation neural networks in Anderson\u2019s  Ph.D. dissertation", "cf68e0c2-35f5-4d07-bea3-aedc565165dc": "557  CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html  Figure 16.1: Probabilistic modeling of natural images. (Pep )) Example 32% 32 pixel color images from the CIFAR-10 dataset . (Bottom) Samples drawn from a structured probabilistic model trained on this dataset. Each sample appears  at the same position in the grid as the training example that is closest to it in Euclidean  space. This comparison allows us to see that the model is truly synthesizing new images,  rather than memorizing the training data. Contrast of both sets of images has been adjusted for display. Figure reproduced with permission from Courville ef al. 558  CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  e Statistical efficiency: As the number of parameters in a model increases, so does the amount of training data needed to choose the values of those parameters using a statistical estimator", "30d80984-1c6d-408b-9b04-20ad8a26e954": "10.2.4 Modeling Sequences Conditioned on Context with RNNs  In the previous section we described how an RNN could correspond to a directed graphical model over a sequence of random variables y with no inputs #. Of course, our development of RNNs as in equation 10.8 included a sequence of inputs ae), a, a), In general, RNNs allow the extension of the graphical model view to represent not only a joint distribution over the y variables but also a conditional distribution over y given a. As discussed in the context of feedforward networks in section 6.2.1.1, any model representing a variable P(y; 0) can be reinterpreted as a model representing a conditional distribution P(y|w) with w = 0. We can extend such a model to represent a distribution P(y | x) by using the same P(y | w) as before, but making w a function of x. In the case of an RNN, this can be achieved in different ways. We review here the most common and obvious choices. Previously, we have discussed RNNs that take a sequence of vectors x) for t =1,...,7 as input", "49d8752e-f205-49c1-86d8-7bb4492424e2": "This allows us to expand the model function to give Substituting into the mean error function (5.130) and expanding, we then have Because the distribution of transformations has zero mean we have E = 0. Also, we shall denote E by \u03bb. Omitting terms of O(\u03be3), the average error function then becomes \ufffdE = E + \u03bb\u2126 (5.131) where E is the original sum-of-squares error, and the regularization term \u2126 takes the form in which we have performed the integration over t. We can further simplify this regularization term as follows. In Section 1.5.5 we saw that the function that minimizes the sum-of-squares error is given by the conditional average E of the target values t. From (5.131) we see that the regularized error will equal the unregularized sum-of-squares plus terms which are O(\u03be), and so the network function that minimizes the total error will have the form Thus, to leading order in \u03be, the \ufb01rst term in the regularizer vanishes and we are left with which is equivalent to the tangent propagation regularizer (5.128)", "29a77faf-d331-422a-9457-fb473f09f8b4": "Second, we need to pass on this critical lineage information to the end model being trained ingly turning to some form of weak supervision: cheaper sources of labels that are noisier or heuristic. The most popular form is distant supervision, in which the records of an external knowledge base are heuristically aligned with data points to produce noisy labels . Other forms include crowdsourced labels , rules and heuristics for labeling data , and others . While these sources are inexpensive, they often have limited accuracy and coverage.\n\nIdeally, we would combine the labels from many weak supervision sources to increase the accuracy and coverage of our training set. However, two key challenges arise in doing so effectively. First, sources will overlap and con\ufb02ict, and to resolve their con\ufb02icts we need to estimate their accuracies and correlation structure, without access to ground truth. Second, we need to pass on critical lineage information about label quality to the end model being trained. Example 1.1 In Fig. 1, we obtain labels from a high-accuracy, low-coverage Source 1, and from a low-accuracy, highcoverage Source 2, which overlap and disagree (split-color points)", "91dc4ca4-476e-4b1f-a402-b5bd6cb294c1": "In IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). Shumin Deng, Ningyu Zhang, Zhanlin Sun, Jiaoyan Chen, and Huajun Chen. 2019. When low resource nlp meets unsupervised language model: Meta-pretraining then meta-learning for few-shot text classi\ufb01cation. In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics. Terrance DeVries and Graham W. Taylor. 2017. Improved regularization of convolutional neural networks with cutout. Quynh Do and Judith Gaspers. 2019. Cross-lingual transfer learning with data selection for large-scale spoken language understanding", "78e798fe-7f87-41bb-b1e1-63196ac52a0f": "There is a second approach to avoiding the in\ufb01nite number of episodes nominally required for policy evaluation, in which we give up trying to complete policy evaluation before returning to policy improvement. On each evaluation step we move the value function toward q\u21e1k, but we do not expect to actually get close except over many steps. We used this idea when we \ufb01rst introduced the idea of GPI in Section 4.6. One extreme form of the idea is value iteration, in which only one iteration of iterative policy evaluation is performed between each step of policy improvement. The in-place version of value iteration is even more extreme; there we alternate between improvement and evaluation steps for single states. For Monte Carlo policy iteration it is natural to alternate between evaluation and improvement on an episode-by-episode basis. After each episode, the observed returns are used for policy evaluation, and then the policy is improved at all the states visited in the episode.\n\nA complete simple algorithm along these lines, which we call Monte Carlo ES, for Monte Carlo with Exploring Starts, is given in pseudocode in the box on the next page", "df7509e5-2ab2-43ec-b5f5-4c013ee4bbb3": "In the case of a directed polytree, conversion to an undirected graph results in loops due to the moralization step, whereas conversion to a factor graph again results in a tree, as illustrated in Figure 8.43. In fact, local cycles in a directed graph due to links connecting parents of a node can be removed on conversion to a factor graph by de\ufb01ning the appropriate factor function, as shown in Figure 8.44. We have seen that multiple different factor graphs can represent the same directed or undirected graph. This allows factor graphs to be more speci\ufb01c about the the creation of loops. (c) The result of converting the polytree into a factor graph, which retains the tree structure. precise form of the factorization. Figure 8.45 shows an example of a fully connected undirected graph along with two different factor graphs", "6fa6d697-0825-4495-87da-90159748a61b": "Broader composition of data augmentations further improves performance Our best results in the main text (Table 6 and 7) can be further improved when expanding the default augmentation policy to include the following: (1) Sobel \ufb01ltering, (2) additional color distortion (equalize, solarize), and (3) motion blur. For linear evaluation protocol, the ResNet-50 models (1\u00d7, 2\u00d7, 4\u00d7) trained with broader data augmentations achieve 70.0 (+0.7), 74.4 (+0.2), 76.8 (+0.3), respectively. B.3. Effects of Longer Training for Supervised Models Here we perform experiments to see how training steps and stronger data augmentation affect supervised training. We test ResNet-50 and ResNet-50 (4\u00d7) under the same set of data augmentations (random crops, color distortion, 50% Gaussian blur) as used in our unsupervised models. Figure B.3 shows the top-1 accuracy. We observe that there is no signi\ufb01cant bene\ufb01t from training supervised models longer on ImageNet. Stronger data augmentation slightly improves the accuracy of ResNet-50 (4\u00d7) but does not help on ResNet-50.\n\nWhen stronger data augmentation is applied, ResNet-50 generally requires longer training (e.g", "a186c228-6d89-4f5d-b2d3-78fc8978f5bf": "midbrain dopamine system. Neuropharmacology, 76:353\u2013359. Lane, S. H., Handelman, D. A., Gelfand, J. J. Theory and development of higher-order CMAC neural networks. IEEE Control Systems, 12(2):23\u201330. LeCun, Y. Une procdure d\u2019apprentissage pour rseau a seuil asymmetrique (a learning scheme for asymmetric threshold networks). In Proceedings of Cognitiva 85, Paris, France.\n\ndocument recognition. Proceedings of the IEEE, 86(11):2278\u20132324. Legenstein, R. W., Maass, D. P. A learning theory for reward-modulated spike-timingdependent plasticity with application to biofeedback. PLoS Computational Biology, 4(10). Levy, W. B., Steward, D. Temporal contiguity requirements for long-term associative potentiation/depression in the hippocampus", "bf26ac2f-6187-4284-8c44-5eada0e44457": "A complete list of books published in the Adaptive Computation and Machine Learning The cover design is based on the trajectories of a simulated bicycle controlled by a reinforcement learning system developed by Jette Randl\u00f8v. This work is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 2.0 Generic License. To view a copy of this license, visit http://creativecommons. org/licenses/by-nc-nd/2.0/ or send a letter to Creative Commons, PO Box 1866, This book was set in 10/12, CMR by Westchester Publishing Services. Printed and bound Names: Sutton, Richard S., author. | Barto, Andrew G., author. Title: Reinforcement learning: an introduction / Richard S. Sutton and Andrew G. Barto. Description: Second edition. | Cambridge, MA : The MIT Press,  | Series: Adaptive computation and machine learning series | Includes bibliographical references and index. Identi\ufb01ers: LCCN 2018023826 | ISBN 9780262039246 (hardcover : alk", "63d6c556-2062-4c24-8f51-93f8214bc066": "In fact it often remains very close to log 2 \u2248 0.69 which is the highest value taken by the JS distance. In other words, the JS distance saturates, the discriminator has zero loss, and the generated samples are in some cases meaningful (DCGAN generator, top right plot) and in other cases collapse to a single nonsensical image . This last phenomenon has been theoretically explained in  and highlighted in . When using the \u2212 log D trick , the discriminator loss and the generator loss are di\ufb00erent. Figure 8 in Appendix E reports the same plots for GAN training, but using the generator loss instead of the discriminator loss.\n\nThis does not change the conclusions. Finally, as a negative result, we report that WGAN training becomes unstable at times when one uses a momentum based optimizer such as Adam  (with \u03b21 > 0) on the critic, or when one uses high learning rates. Since the loss for the critic is nonstationary, momentum based methods seemed to perform worse. We identi\ufb01ed momentum as a potential cause because, as the loss blew up and samples got worse, the cosine between the Adam step and the gradient usually turned negative", "2e786a8d-4390-4b1a-8002-7931147946e0": "Let's label the estimated return following n steps as Ga\u201d, n=1,...,00, then:  n Gi Notes  n=1 Gq = Ri + WV (Si41) TD learning n=2 Ge) = Ris + Rig + VV (St42)  n=n Gi\u201d) = Rip t+ YR +--+ 4 yt Rien +9\" V(Sisn)  n= 0o G\\~) = Rip tyRigg t+ +97 Rp +: 77 V (Sr) MC estimation  The generalized n-step TD learning still has the same form for updating the value function:  V(S;) \u2014 V(S;) + (GE \u2014 V(S;))  At time step t, TD(A)  O\u2014e\u2014O 1-) O\u2014-e\u2014O\u2014@\u2014O (1-A)A O\u2014e\u2014O\u2014e\u2014O\u2014-@\u00a9 (1 -)\u201d  O-e\u2014-O-e\u2014-O-@-\u00a9 ---@-Ol-awr  TERMINAL We are free to pick any n in TD learning as we like", "01c201fb-8410-429c-8200-9b551e6f4df1": "The arrived at the answer in a matter of moments by noting that the sum can be represented as 50 pairs (1 + 100, 2+99, etc.) each of which added to 101, giving the answer 5,050.\n\nIt is now believed that the problem which was actually set was of the same form but somewhat harder in that the sequence had a larger starting value and a larger increment. Gauss was a German mathematician and scientist with a reputation for being a hard-working perfectionist. One of his many contributions was to show that least squares can be derived under the assumption of normally distributed errors. He also created an early formulation of non-Euclidean geometry (a self-consistent geometrical theory that violates the axioms of Euclid) but was reluctant to discuss it openly for fear that his reputation might suffer if it were seen that he believed in such a geometry. At one point, Gauss was asked to conduct a geodetic survey of the state of Hanover, which led to his formulation of the normal distribution, now also known as the Gaussian. After his death, a study of his diaries revealed that he had discovered several important mathematical results years or even decades before they were published by others", "95fd7a32-7f6c-4e31-be04-2fcebf4e95f9": "The prior p(aN) is given by a zero-mean Gaussian process with covariance matrix CN, and the data term (assuming independence of the data points) is given by We then obtain the Laplace approximation by Taylor expanding the logarithm of p(aN|tN), which up to an additive normalization constant is given by the quantity First we need to \ufb01nd the mode of the posterior distribution, and this requires that we evaluate the gradient of \u03a8(aN), which is given by where \u03c3N is a vector with elements \u03c3(an). We cannot simply \ufb01nd the mode by setting this gradient to zero, because \u03c3N depends nonlinearly on aN, and so we resort to an iterative scheme based on the Newton-Raphson method, which gives rise to an iterative reweighted least squares (IRLS) algorithm. This requires the second Section 4.3.3 derivatives of \u03a8(aN), which we also require for the Laplace approximation anyway, and which are given by where WN is a diagonal matrix with elements \u03c3(an)(1 \u2212 \u03c3(an)), and we have used the result (4.88) for the derivative of the logistic sigmoid function", "52d5b062-262c-456c-9173-517167f104e9": "With noisier rewards it takes more exploration to \ufb01nd the optimal action, and \"-greedy methods should fare even better relative to the greedy method. On the other hand, if the reward variances were zero, then the greedy method would know the true value of each action after trying it once. In this case the greedy method might actually perform best because it would soon \ufb01nd the optimal action and then never explore. But even in the deterministic case there is a large advantage to exploring if we weaken some of the other assumptions. For example, suppose the bandit task were nonstationary, that is, the true values of the actions changed over time. In this case exploration is needed even in the deterministic case to make sure one of the nongreedy actions has not changed to become better than the greedy one. As we shall see in the next few chapters, nonstationarity is the case most commonly encountered in reinforcement learning.\n\nEven if the underlying task is stationary and deterministic, the learner faces a set of banditlike decision tasks each of which changes over time as learning proceeds and the agent\u2019s decision-making policy changes. Reinforcement learning requires a balance between exploration and exploitation. denoted 1, 2, 3, and 4", "bee4d073-917d-4e98-b20e-77fbc3718a1f": "GloVe embeddings di25 | 18:30 | Old: | 87:85 |280:18 | 83:0 72.87 81.52 Avg. fast-text embeddings 77.96 | 79.23 | 91.68 | 87.81 | 82.15 | 83.6 T4.49 82.42 Avg. BERT embeddings 78.66 | 86.25 | 94.37 | 88.66 | 8440 | 92.8 69.45 84.94 BERT CLS-vector 78.68 | 84.85 | 94.21 88.23", "344666cf-fa7d-46eb-9515-7aa24c8f57aa": "One can always predictably obtain less regularization by decreasing this coefficient.\n\nIn unsupervised pretraining, there is not a way of flexibly adapting  https://www.deeplearningbook.org/contents/representation.html    the strength ot the regularization\u2014either the supervised model is initialized to pretrained parameters, or it is not. Another disadvantage of having two separate training phases is that each phase has its own hyperparameters. The performance of the second phase usually cannot be predicted during the first phase, so there is a long delay between proposing hyperparameters for the first phase and being able to update them using feedback from the second phase. The most principled approach is to use validation set error in the supervised phase to select the hyperparameters of the pretraining phase, as discussed in Larochelle et al. In practice, some hyperparameters, like the number of pretraining iterations, are more conveniently set during the pretraining phase, using early stopping on the unsupervised objective, which is not ideal but is computationally much cheaper than using the supervised objective", "d4e8c926-4ba4-4119-92df-b3a468a0214b": "Each leaf requires at least one  training example to define, so it is not possible for the decision tree to learn a function that has more local maxima than the number of training examples. 143  CHAPTER 5. MACHINE LEARNING BASICS  examples. The term is usually associated with density estimation, learning to draw samples from a distribution, learning to denoise data from some distribution,  eo 7 i re  https://www.deeplearningbook.org/contents/ml.html       relate? @ IMNalo1d tual LUE Gala les Weal, OF CLUSLELINY LILLE Gala WILO BrOUPS OL related examples. A classic unsupervised learning task is to find the \u201cbest\u201d representation of the data.\n\nBy \u201cbest\u201d we can mean different things, but generally speaking we are looking for a representation that preserves as much information about 2x as possible while obeying some penalty or constraint aimed at keeping the representation simpler or more accessible than 2 itself. There are multiple ways of defining a simpler representation. Three of the most common include lower-dimensional representations, sparse representations, and independent representations. Low-dimensional representations attempt to compress as much information about x as possible in a smaller representation", "58eac762-759b-418c-98c4-6cd275651055": "simpler approach introducoo by b.ased on the rddmu \"pproximation, which is appropriate when the number of data points is relatively large and the corresponding posterior distribution is tightly peaked . It involves a specific choice of prior over W that allows surplus dimensions in the principal subspace to be pruned out of the model.\n\nThis corresponds to an example of automatic relevance determination, or ARD, discussed in Section 7.2.2. Specifically, we define an independent Gaussian prior over each column of W, which represent the vectors defining the principal subspace. Each such Gaussian has an independent variance governed by a precision hyperparameter O:i so that where Wi is the i th column of W. The resulting model can be represented using the directed graph shown in Figure 12.13. The values for O:i will be found iteratively by maximizing the marginallikelihood function in which W has been integrated out. As a result of this optimization, some of the O:i may be driven to infinity, with the corresponding parameters vector Wi being driven to zero (the posterior distribution becomes a delta function at the origin) giving a sparse solution", "4ee46755-1f08-4b4f-a9d2-fe32dd40dacf": "Our focus in this chapter has been on models for which the observed variables are continuous. We can also consider models having continuous latent variables together with discrete observed variables, giving rise to latent trait models . In this case, the marginalization over the continuous latent variables, even for a linear relationship between latent and observed variables, cannot be performed analytically, and so more sophisticated techniques are required. Tipping  uses variational inference in a model with a two-dimensional latent space, allowing a binary data set to be visualized analogously to the use of PCA to visualize continuous data.\n\nNote that this model is the dual of the Bayesian logistic regression problem discussed in Section 4.5. In the case of logistic regression we have N observations of the feature vector <l>n which are parameterized by a single parameter vector w, whereas in the latent space visualization model there is a single latent space variable x . We have already noted that an arbitrary distribution can be formed by taking a Gaussian random variable and transforming it through a suitable nonlinearity. This is exploited in a general latent variable model called a density network  in which the nonlinear function is governed by a multilayered neural network", "d0e21509-7e4c-4747-83e3-8800152b5e24": "Currently, two of the main strategies  for dealing with a large number of underlying causes are to use a supervised learning signal at the same time as the unsupervised learning signal so that the model will choose to capture the most relevant factors of variation, or to use much larger representations if using purely unsupervised learning. An emerging strategy for unsupervised learning is to modify the definition of which underlying causes are most salient. Historically, autoencoders and generative models have been trained to optimize a fixed criterion, often similar to mean squared error. These fixed criteria determine which causes are considered salient.\n\nFor example, mean squared error applied to the pixels of an image implicitly specifies that an underlying cause is only salient if it significantly changes the brightness of a large number of pixels. This can be problematic if the task we wish to solve involves interacting with small objects. See figure 15.5 for an example  541  CHAPTER 15. REPRESENTATION LEARNING  Input Reconstruction  ;  Figure 15.5: An autoencoder trained with mean squared error for a robotics task has failed to reconstruct a ping pong ball", "af58b3fe-f47e-4405-bdac-984ee238778a": "Deep learning methods have achieved strong performance on a wide range of supervised learning tasks .\n\nTraditionally, these results were attained through the use of large, welllabeled datasets. This make them challenging to apply in settings where collecting a large amount of high-quality labeled data for training is expensive. Moreover, given the fast-changing nature of real-world applications, it is infeasible to relabel every example whenever new data comes in. This highlights a need for learning algorithms that can be trained with a limited amount of labeled data. There has been a substantial amount of research towards learning with limited labeled data for various tasks in the NLP community. One common approach for mitigating the need for labeled data is data augmentation. Data augmentation  generates new data by modifying existing data points through transformations that are designed based on prior knowledge about the problem\u2019s structure . This augmented data can be generated from labeled data, and then directly used in supervised learning , or in semi-supervised learning for unlabeled data through consistency regularization  (\u201cconsistency training\u201d)", "1c72a80f-83c3-464a-b863-5fb9c414ad79": "(12.22)  We can also define an attribute, a concept analogous to a relation, but taking only one argument: (entity,, attribute,). (12.23)  For example, we could define the has_fur attribute, and apply it to entities like  https://www.deeplearningbook.org/contents/applications.html    dog. \u00b0 Many applications require representing relations and reasoning about them. How should we best do this within the context of neural networks? Machine learning models of course require training data. We can infer relations between entities from training datasets consisting of unstructured natural language. There are also structured databases that identify relations explicitly. A common structure for these databases is the relational database, which stores this same kind of information, albeit not formatted as three token sentences. When a  478  CHAPTER 12", "eb848ff9-e213-47b1-b936-69ac8fa46795": "Typically there are O(k) parameters, with O(1) parameters associated with each of the O(k) regions. The nearest neighbor scenario, in which each training example can be used to define at most one region, is illustrated in figure 5.10. Is there a way to represent a complex function that has many more regions to be distinguished than the number of training examples?\n\nClearly, assuming only smoothness of the underlying function will not allow a learner to do that. For example, imagine that the target function is a kind of checkerboard. A checkerboard contains many variations, but there is a simple structure to them. Imagine what happens when the number of training examples is substantially smaller than the number of black and white squares on the checkerboard. Based on only local generalization and the smoothness or local constancy prior, the learner would be guaranteed to correctly guess the color of a new point if it lay within the same checkerboard square as a training example. There is no guarantee, however, that the learner could correctly extend the checkerboard pattern to points lying in squares that do not contain training examples", "e3df7efc-109c-47e1-9f03-11fb57efc8ad": "For practical applications of probabilistic models, it will typically be the highernumbered variables corresponding to terminal nodes of the graph that represent the observations, with lower-numbered nodes corresponding to latent variables. The primary role of the latent variables is to allow a complicated distribution over the observed variables to be represented in terms of a model constructed from simpler (typically exponential family) conditional distributions. We can interpret such models as expressing the processes by which the observed data arose. For instance, consider an object recognition task in which each observed data point corresponds to an image (comprising a vector of pixel intensities) of one of the objects. In this case, the latent variables might have an interpretation as the position and orientation of the object. Given a particular observed image, our goal is to \ufb01nd the posterior distribution over objects, in which we integrate over all possible positions and orientations. We can represent this problem using a graphical model of the form show in Figure 8.8.\n\nThe graphical model captures the causal process  by which the observed data was generated. For this reason, such models are often called generative models", "61e59700-eb65-4c71-924a-7b6cc2ed0455": "Progressive growing of GANs for improved quality, stability, and variation. CORR, abs/1710.10196, 2017.  . Justin J, Alexandre A, Li FF.\n\nPerceptual losses for real-time style transfer and super-resolution. ECCV. 2016;2016:694-711. Luis P, Jason W. The effectiveness of data augmentation in image classification using deep learning. In: Stanford University research report, 2017. Lemley J, Barzrafkan S, Corcoran P. Smart augmentation learning an optimal data augmentation strategy. In: IEEE Access. 2017. Ekin DC, Barret Z, Dandelion M, Vijay V, Quoc VL. AutoAugment: learning augmentation policies from data. ArXiv preprint. 2018. Xin Y, Paul SB, Ekta W. Generative adversarial network in medical imaging: a review. arXiv preprint. 2018. )", "9f62e7e5-4d2d-4e7c-8e25-cc906e67560b": "Our discussion of value iteration as a form of truncated policy iteration is based on the approach of Puterman and Shin , who presented a class of algorithms called modi\ufb01ed policy iteration, which includes policy iteration and value iteration as special cases. An analysis showing how value iteration can be made to \ufb01nd an optimal policy in \ufb01nite time is given by Bertsekas .\n\nIterative policy evaluation is an example of a classical successive approximation algorithm for solving a system of linear equations. The version of the algorithm that uses two arrays, one holding the old values while the other is updated, is often called a Jacobi-style algorithm, after Jacobi\u2019s classical use of this method. It is also sometimes called a synchronous algorithm because the e\u21b5ect is as if all the values are updated at the same time. The second array is needed to simulate this parallel computation sequentially. The in-place version of the algorithm is often called a Gauss\u2013Seidel-style algorithm after the classical Gauss\u2013Seidel algorithm for solving systems of linear equations. In addition to iterative policy evaluation, other DP algorithms can be implemented in these di\u21b5erent versions", "c08b9324-9bc8-4808-80d4-a351f43baa28": "(2.62)  We can solve this optimization problem using vector calculus (see section 4.3 if you do not know how to do this):  Ve(\u20142a'De+c'c) =0 (2.63) \u20142D'a+2c=0 (2.64) c=D'z. (2.65)  This makes the algorithm efficient: we can optimally encode x using just a matrix-vector operation.\n\nTo encode a vector, we apply the encoder function  f(a) =D'e. (2.66) Using a further matrix multiplication, we can also define the PCA reconstruction operation: r(x) = 9(f ())=DD'e. (2.67) 47  CHAPTER 2. LINEAR ALGEBRA  Next, we need to choose the encoding matrix D. To do so, we revisit the idea of minimizing the L? distance between inputs and reconstructions. Since we will use the same matrix D to decode all the points, we can no longer consider the points in isolation", "81a52206-6be4-4d4a-b037-47914c6f1dc7": "The resultant prediction algorithm (after combining with (7.5)) is analogous to Expected Sarsa. \u21e4Exercise 7.7 Write the pseudocode for the o\u21b5-policy action-value prediction algorithm described immediately above.\n\nPay particular attention to the termination conditions for the recursion upon hitting the horizon or the end of episode. \u21e4 Exercise 7.8 Show that the general (o\u21b5-policy) version of the n-step return (7.13) can still be written exactly and compactly as the sum of state-based TD errors (6.5) if the approximate state value function does not change. \u21e4 Exercise 7.9 Repeat the above exercise for the action version of the o\u21b5-policy n-step return (7.14) and the Expected Sarsa TD error (the quantity in brackets in Equation 6.9). \u21e4 Exercise 7.10 (programming) Devise a small o\u21b5-policy prediction problem and use it to show that the o\u21b5-policy learning algorithm using (7.13) and (7.2) is more data e\ufb03cient than the simpler algorithm using (7.1) and (7.9)", "2bf5c6d0-159e-4ac5-8648-eb748b40d647": "However, it turns out that any linear parametric regression method like those we described in Section 9.4, with states represented by feature vectors x(s) = (x1(s), x2(s), . , xd(s))>, can be recast as kernel regression where k(s, s0) is the inner product of the feature vector representations of s and s0; that is Kernel regression with this kernel function produces the same approximation that a linear parametric method would if it used these feature vectors and learned with the same training data. We skip the mathematical justi\ufb01cation for this, which can be found in any modern machine learning text, such as Bishop , and simply point out an important implication.\n\nInstead of constructing features for linear parametric function approximators, one can instead construct kernel functions directly without referring at all to feature vectors. Not all kernel functions can be expressed as inner products of feature vectors as in (9.24), but a kernel function that can be expressed like this can o\u21b5er signi\ufb01cant advantages over the equivalent parametric method. For many sets of feature vectors, (9.24) has a compact functional form that can be evaluated without any computation taking place in the d-dimensional feature space", "9cd371ba-f88d-4bc0-ba9b-b6fdf35ccec4": "Efficient object localization using convolutional networks. In: CVPR'15. 2015. 9. Sergey |, Christan S. Batch normalization: accelerating deep network training by reducing internal covariate shift. In: ICML; 2015. 0. Karl W, Taghi MK, DingDing W. A survey of transfer learning. J Big Data. 2016;3:9. 1. Shao L. Transfer learning for visual categorization: a survey. IEEE Trans Neural Netw Learn Syst. 2015;26(5):1019-34. 2. Jia D, Wei D, Richard S, Li-Jia L, Kai L, Li F-F. ImageNet: a large-scale hierarchical image database. In: CVPRO9, 2009. 3. Amir Z, Alexander S, William S, Leonidas G, Jitendra M, Silvio S. Taskonomy: disentangling task transfer learning. In: CVPR'18", "9b04a93d-9110-45fb-bd92-aabe8c5738eb": "13.27 (\u22c6) www Consider a linear dynamical system of the form discussed in Section 13.3 in which the amplitude of the observation noise goes to zero, so that \u03a3 = 0. Show that the posterior distribution for zn has mean xn and zero variance. This accords with our intuition that if there is no noise, we should just use the current observation xn to estimate the state variable zn and ignore all previous observations. 13.28 (\u22c6 \u22c6 \u22c6) Consider a special case of the linear dynamical system of Section 13.3 in which the state variable zn is constrained to be equal to the previous state variable, which corresponds to A = I and \u0393 = 0. For simplicity, assume also that V0 \u2192 \u221e so that the initial conditions for z are unimportant, and the predictions are determined purely by the data. Use proof by induction to show that the posterior mean for state zn is determined by the average of x1, . , xn.\n\nThis corresponds to the intuitive result that if the state variable is constant, our best estimate is obtained by averaging the observations", "86072135-e805-435c-83df-99b9c2a65f83": "10.27 (\u22c6 \u22c6) By making use of the formulae given in Appendix B show that the variational lower bound for the linear basis function regression model, de\ufb01ned by (10.107), can be written in the form (10.107) with the various terms de\ufb01ned by (10.108)\u2013(10.112). 10.28 (\u22c6 \u22c6 \u22c6) Rewrite the model for the Bayesian mixture of Gaussians, introduced in Section 10.2, as a conjugate model from the exponential family, as discussed in Section 10.4. Hence use the general results (10.115) and (10.119) to derive the speci\ufb01c results (10.48), (10.57), and (10.59). 10.29 (\u22c6) www Show that the function f(x) = ln(x) is concave for 0 < x < \u221e by computing its second derivative. Determine the form of the dual function g(\u03bb) de\ufb01ned by (10.133), and verify that minimization of \u03bbx \u2212 g(\u03bb) with respect to \u03bb according to (10.132) indeed recovers the function ln(x)", "82338590-27f0-44da-9a4c-bf48d85b174f": "We then obtain the discriminant function in the form An interesting property of least-squares solutions with multiple target variables is that if every target vector in the training set satis\ufb01es some linear constraint for some constants a and b, then the model prediction for any value of x will satisfy the same constraint so that Exercise 4.2 Thus if we use a 1-of-K coding scheme for K classes, then the predictions made by the model will have the property that the elements of y(x) will sum to 1 for any value of x.\n\nHowever, this summation constraint alone is not suf\ufb01cient to allow the model outputs to be interpreted as probabilities because they are not constrained to lie within the interval (0, 1). The least-squares approach gives an exact closed-form solution for the discriminant function parameters. However, even as a discriminant function (where we use it to make decisions directly and dispense with any probabilistic interpretation) it suffers from some severe problems. We have already seen that least-squares solutions Section 2.3.7 lack robustness to outliers, and this applies equally to the classi\ufb01cation application, as illustrated in Figure 4.4", "f7157e87-fc1f-4c86-b198-06d88b164e91": "Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc\u2019Aurelio Ranzato. 2018. Unsupervised machine translation using monolingual corpora only. In International Conference on Learning Representations. Bill Yuchen Lin, Dong-Ho Lee, Ming Shen, Ryan Moreno, Xiao Huang, Prashant Shiralkar, and Xiang Ren. 2020.\n\nTriggerNER: Learning with entity triggers as explanations for named entity recognition. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8503\u2013 8511, Online. Association for Computational Linguistics. Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2017. Adversarial multi-task learning for text classi\ufb01cation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1\u201310, Vancouver, Canada. Association for Computational Linguistics. Adyasha Maharana and Mohit Bansal. 2020", "fcba50ad-ad79-48a3-93e8-4716f22fc2db": "Scalar coefficients are conventionally written on the left of vector they operate on.\n\nWe therefore usually write such a  formula as  d= arg min ) > |x \u2014 dle d||2 subject to ||d||2 = 1, (2.70) d -  or, exploiting the fact that a scalar is its own transpose, as  d= arg min ) > 2 \u2014 2 dad |} subject to ||d||2 = 1. (2.71) d -  The reader should aim to become familiar with such cosmetic rearrangements. At this point, it can be helpful to rewrite the problem in terms of a single design matrix of examples, rather than as a sum over separate example vectors. This will enable us to use more compact notation. Let X \u20ac R\u2019\u201d*\u201d be the matrix defined by stacking all the vectors describing the points, such that X;, = gl)\u201d We can now rewrite the problem as  d* = argmin ||X \u2014 Xdd\"||% subject to d'd=1", "29f96760-9039-44d7-996e-f85a270cb532": "Many structured probabilistic models with a single hidden layer of latent variables, including restricted Boltzmann machines and deep belief networks, are universal approximators of probability distributions . In section 6.4.1, we saw that a sufficiently deep feedforward network can have  AK Aw RK Aetinl 2 Aen ta wn Avene 2 Mn ntewrnnle that fn tan ahallawe sinh wnneeltn nan alan  https://www.deeplearningbook.org/contents/representation.html    all CAPVUCIUAL AU VALLLARS UVEL @ LSLUWULKA Lllal 1D LUU SLGALUW. OULLL LODSULLD Call aAldU be obtained for other models such as probabilistic models.\n\nOne such probabilistic model is the Sum-product network, or SPN . These models use polynomial circuits to compute the probability distribution over a  set of random variables. Delalleau and Bengio  showed that there exist probability distributions for which a minimum depth of SPN is required to avoid needing an exponentially large model", "0fce451d-e4d3-4d4e-ba9f-a4cdc3a49671": "The popular precision at 10 percent metric counts how many times the model ranks a \u201ccorrect\u201d fact among che top 10 percent of all corrupted versions of that fact. Another application of knowledge bases and distributed representations for hem is word-sense disambiguation , which is the task of deciding which sense of a word is the appropriate one in some context. Eventually, knowledge of relations combined with a reasoning process and an understanding of natural language could allow us to build a general question- answering system. A general question-answering system must be able to process input information and remember important facts, organized in a way that enables it to retrieve and reason about them later. This remains a difficult open problem that can only be solved in restricted \u201ctoy\u201d environments. Currently, the best approach to remembering and retrieving specific declarative facts is to use an  https://www.deeplearningbook.org/contents/applications.html    explicit memory mechanism, as described in section 10.12.\n\nMemory networks were first proposed to solve a toy question-answering task . Kumar et al. have proposed an extension that uses GRU recurrent nets to read the input into the memory and to produce the answer given the contents of the  memory", "6cf81258-122d-4f28-93c2-500bcc24570c": "That is, for each possible action a available in the current state St, we can compute \u02c6q(St, a, wt) and then \ufb01nd the greedy action A\u21e4 t = argmaxa \u02c6q(St, a, wt\u22121). Policy improvement is then done (in the on-policy case treated in this chapter) by changing the estimation policy to a soft approximation of the greedy policy such as the \"-greedy policy. Actions are selected according to this same policy. Pseudocode for the complete algorithm is given in the box. Input: a di\u21b5erentiable action-value function parameterization \u02c6q : S \u21e5 A \u21e5 Rd ! R Algorithm parameters: step size \u21b5 > 0, small \" > 0 Initialize value-function weights w 2 Rd arbitrarily (e.g., w = 0) car up a steep mountain road, as suggested by the diagram in the upper left of Figure 10.1", "a2235029-9c57-4fe3-be64-736b6240ad72": "This can be resolved by dividing the input space up into regions and \ufb01t a different polynomial in each region, leading to spline functions . There are many other possible choices for the basis functions, for example where the \u00b5j govern the locations of the basis functions in input space, and the parameter s governs their spatial scale. These are usually referred to as \u2018Gaussian\u2019 basis functions, although it should be noted that they are not required to have a probabilistic interpretation, and in particular the normalization coef\ufb01cient is unimportant because these basis functions will be multiplied by adaptive parameters wj.\n\nAnother possibility is the sigmoidal basis function of the form where \u03c3(a) is the logistic sigmoid function de\ufb01ned by Equivalently, we can use the \u2018tanh\u2019 function because this is related to the logistic sigmoid by tanh(a) = 2\u03c3(a) \u2212 1, and so a general linear combination of logistic sigmoid functions is equivalent to a general linear combination of \u2018tanh\u2019 functions. These various choices of basis function are illustrated in Figure 3.1. Yet another possible choice of basis function is the Fourier basis, which leads to an expansion in sinusoidal functions. Each basis function represents a speci\ufb01c frequency and has in\ufb01nite spatial extent", "6dd230b5-4796-4e17-be60-c33a7c690cfd": "From B0 the expected penalty (negative reward) for possibly running into an edge is more than compensated for by the expected gain for possibly stumbling onto A or B.\n\nExercise 3.14 The Bellman equation (3.14) must hold for each state for the value function v\u21e1 shown in Figure 3.2 (right) of Example 3.5. Show numerically that this equation holds for the center state, valued at +0.7, with respect to its four neighboring states, valued at +2.3, +0.4, \u22120.4, and +0.7. (These numbers are accurate only to one decimal place.) \u21e4 Exercise 3.15 In the gridworld example, rewards are positive for goals, negative for running into the edge of the world, and zero the rest of the time. Are the signs of these rewards important, or only the intervals between them? Prove, using (3.8), that adding a constant c to all the rewards adds a constant, vc, to the values of all states, and thus does not a\u21b5ect the relative values of any states under any policies. What is vc in terms of c and \u03b3? \u21e4 Exercise 3.16 Now consider adding a constant c to all the rewards in an episodic task, such as maze running", "9c4f11f8-f99a-4d9a-905b-127a12f0963d": "(13.11)  This specifies that the learned features must be linearly decorrelated from each other. Without this constraint, all the learned features would simply capture the  one slowest signal. One could imagine using other mechanisms, such as minimizing reconstruction error, to force the features to diversify, but this decorrelation mechanism admits a simple solution due to the linearity of SFA features. The SFA problem may be solved in closed form by a linear algebra package. SFA is typically used to learn nonlinear features by applying a nonlinear basis expansion to a before running SFA. For example, it is common to replace x with the quadratic basis expansion, a vector containing elements 2,x; for all i and j", "bf7f21b1-f30b-4988-9518-556d717b1198": "MACHINE LEARNING BASICS  fs  ns ee es |  https://www.deeplearningbook.org/contents/ml.html   010  oll 110 111  Figure 5.7: Diagrams descri  on the right (1).\n\nInternal n  bing chooses to send the input example to the child node on the left (0) or to the child node  odes  how a decision tree works. (Top)Each node of the tree  are drawn as circles and leaf nodes as squares. Each  node is displayed with a binary string identifier corresponding to its position in the tree,  obtained by appending a bi  to i  right or bottom). (Bottom)The  how a decision tree might  s parent identifier (0 = choose left or top, 1 = choose tree divides space into regions. The 2-D plane shows  ivide R?. The nodes of the tree are plotted in this plane,  with each internal node drawn along the dividing line it uses to categorize examples, and leaf nodes drawn in the cen  is a piecewise-constant func  ion,  er of the region of examples they receive. The result with one piece per leaf", "f5bed9e6-cb62-44f2-8829-059c5cb323af": "An important characterization of a manifold is the set of its tangent planes. At a point x on a ddimensional manifold, the tangent plane is given by d basis vectors that span the local directions of variation allowed on the manifold. As illustrated in figure 14.6, these local directions specify how one can change x infinitesimally while staying on the manifold. All autoencoder training procedures involve a compromise between two forces:  1. Learning a representation h of a training example x such that x can be approximately recovered from h through a decoder. The fact that a is drawn from the training data is crucial, because it means the autoencoder need not successfully reconstruct inputs that are not probable under the data-generating distribution. 2. Satisfying the constraint or regularization penalty.\n\nThis can be an architec- tural constraint that limits the capacity of the autoencoder, or it can be a regularization term added to the reconstruction cost. These techniques generally prefer solutions that are less sensitive to the input", "045b11ef-b725-4578-b65f-a5f4a59635a7": "Let X \u2286 Rd be a compact set (such as d the space of images). We de\ufb01ne Prob(X) to be the space of probability measures over X. We note Cb(X) = {f : X \u2192 R, f is continuous and bounded} Note that if f \u2208 Cb(X), we can de\ufb01ne \u2225f\u2225\u221e = maxx\u2208X |f(x)|, since f is bounded. With this norm, the space (Cb(X), \u2225 \u00b7 \u2225\u221e) is a normed vector space. As for any normed vector space, we can de\ufb01ne its dual Cb(X)\u2217 = {\u03c6 : Cb(X) \u2192 R, \u03c6 is linear and continuous} and give it the dual norm \u2225\u03c6\u2225 = supf\u2208Cb(X),\u2225f\u2225\u221e\u22641 |\u03c6(f)|. With this de\ufb01nitions, (Cb(X)\u2217, \u2225 \u00b7 \u2225) is another normed space.\n\nNow let \u00b5 be a signed measure over X, and let us de\ufb01ne the total variation distance where the supremum is taken all Borel sets in X", "96e14b7c-5581-455b-a117-88e78db10b16": "In the special case of \u03b1 = \u03b2, the teacher model reduces to q(n+1)(t) \u221d p\u03b8(n)(t) exp{f(t)/\u03b1}.\n\nThis special form allows us to use a simple importance sampling method , with p\u03b8(n) (i.e., the current model distribution) as the proposal distribution: Here the experience function f(t) serves as a sample reweighting mechanism  that highlights the \u2018high-quality\u2019 samples (i.e., those receiving high goodness scores in the light of the experience) for updating the parameters \u03b8. For other choices of the divergence function D other than the cross entropy or KL divergence, sampling from q(n+1) may similarly be su\ufb03cient in order to estimate and optimize the divergence Equation 7.7. The probability functional descent (PFD) can also be used to optimize with a certain class of divergences, following similar derivations discussed in Sections 5 and 7.2.1. Besides the objective function (Sections 3-6) and the optimization (Section 7) above, we now brie\ufb02y discuss the third and last core ingredient of an ML approach, namely, the target model p\u03b8", "e70fa90f-d3cc-4d6f-98dd-75550e8eccad": "Slice sampling can be applied to multivariate distributions by repeatedly sampling each variable in turn, in the manner of Gibbs sampling. This requires that we are able to compute, for each component zi, a function that is proportional to p(zi|z\\i). 11.5. The Hybrid Monte Carlo Algorithm As we have already noted, one of the major limitations of the Metropolis algorithm is that it can exhibit random walk behaviour whereby the distance traversed through the state space grows only as the square root of the number of steps. The problem cannot be resolved simply by taking bigger steps as this leads to a high rejection rate.\n\nIn this section, we introduce a more sophisticated class of transitions based on an analogy with physical systems and that has the property of being able to make large changes to the system state while keeping the rejection probability small. It is applicable to distributions over continuous variables for which we can readily evaluate the gradient of the log probability with respect to the state variables. We will discuss the dynamical systems framework in Section 11.5.1, and then in Section 11.5.2 we explain how this may be combined with the Metropolis algorithm to yield the powerful hybrid Monte Carlo algorithm", "a1e35fdc-a351-4c4e-9ba5-2ba4c904d786": "For each task, we selected the best \ufb01ne-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set. Additionally, for BERTLARGE we found that \ufb01netuning was sometimes unstable on small datasets, so we ran several random restarts and selected the best model on the Dev set. With random restarts, we use the same pre-trained checkpoint but perform different \ufb01ne-tuning data shuf\ufb02ing and classi\ufb01er layer initialization.9 Results are presented in Table 1.\n\nBoth BERTBASE and BERTLARGE outperform all systems on all tasks by a substantial margin, obtaining 4.5% and 7.0% respective average accuracy improvement over the prior state of the art. Note that BERTBASE and OpenAI GPT are nearly identical in terms of model architecture apart from the attention masking. For the largest and most widely reported GLUE task, MNLI, BERT obtains a 4.6% absolute accuracy improvement. On the of\ufb01cial GLUE leaderboard10, BERTLARGE obtains a score of 80.5, compared to OpenAI GPT, which obtains 72.8 as of the date of writing", "969ec3dd-ff3f-4a1e-9b41-07eb50f7c008": "Gershman, Moustafa, and Ludvig  focused on how time is represented in reinforcement learning models of the basal ganglia, discussing evidence for, and implications of, various computational approaches to time representation. The hypothetical neural implementation of the actor\u2013critic architecture described in this section includes very little detail about known basal ganglia anatomy and physiology. In addition to the more detailed hypothesis of Houk, Adams, and Barto , a number of other hypotheses include more speci\ufb01c connections to anatomy and physiology and are claimed to explain additional data. These include hypotheses proposed by Suri and Schultz , Brown, Bullock, and Grossberg , Contreras-Vidal and Schultz , Suri, Bargas, and Arbib , O\u2019Reilly and Frank , and O\u2019Reilly, Frank, Hazy, and Watz . Joel, Niv, and Ruppin  critically evaluated the anatomical plausibility of several of these models and present an alternative intended to accommodate some neglected features of basal ganglionic circuitry. 15.8 The actor learning rule discussed here is more complicated than the one in the early actor\u2013critic network of Barto et al", "6d4111e4-dfe6-418a-b349-1643c956151e": "In the convolutional layer the units are organized into planes, each of which is called a feature map. Units in a feature map each take inputs only from a small subregion of the image, and all of the units in a feature map are constrained to share the same weight values. For instance, a feature map might consist of 100 units arranged in a 10 \u00d7 10 grid, with each unit taking inputs from a 5\u00d75 pixel patch of the image. The whole feature map therefore has 25 adjustable weight parameters plus one adjustable bias parameter.\n\nInput values from a patch are linearly combined using the weights and the bias, and the result transformed by a sigmoidal nonlinearity using (5.1). If we think of the units as feature detectors, then all of the units in a feature map detect the same pattern but at different locations in the input image. Due to the weight sharing, the evaluation of the activations of these units is equivalent to a convolution of the image pixel intensities with a \u2018kernel\u2019 comprising the weight parameters. If the input image is shifted, the activations of the feature map will be shifted by the same amount but will otherwise be unchanged", "70de0834-0855-43db-9a65-9cc9f90ec54a": "), Reinforcement Learning: State-of-the-Art, pp. 441\u2013467. Springer-Verlag Berlin Heidelberg. Nutt, D. J., Lingford-Hughes, A., Erritzoe, D., Stokes, P. R. A. .\n\nThe dopamine theory of addiction: 40 years of highs and lows. Nature Reviews Neuroscience, 16(5):305\u2013312. O\u2019Doherty, J. P., Dayan, P., Friston, K., Critchley, H., Dolan, R. J. Temporal di\u21b5erence models and reward-related learning in the human brain. Neuron, 38(2):329\u2013337. O\u2019Doherty, J. P., Dayan, P., Schultz, J., Deichmann, R., Friston, K., Dolan, R. J. \u00b4Olafsd\u00b4ottir, H. F., Barry, C., Saleem, A", "dfb6ea96-6536-4e5c-8fb6-6da9064b1924": "In the case of continuous variables, the required integrations may not have closed-form analytical solutions, while the dimensionality of the space and the complexity of the integrand may prohibit numerical integration. For discrete variables, the marginalizations involve summing over all possible con\ufb01gurations of the hidden variables, and though this is always possible in principle, we often \ufb01nd in practice that there may be exponentially many hidden states so that exact calculation is prohibitively expensive. In such situations, we need to resort to approximation schemes, and these fall broadly into two classes, according to whether they rely on stochastic or deterministic approximations. Stochastic techniques such as Markov chain Monte Carlo, described in Chapter 11, have enabled the widespread use of Bayesian methods across many domains.\n\nThey generally have the property that given in\ufb01nite computational resource, they can generate exact results, and the approximation arises from the use of a \ufb01nite amount of processor time. In practice, sampling methods can be computationally demanding, often limiting their use to small-scale problems. Also, it can be dif\ufb01cult to know whether a sampling scheme is generating independent samples from the required distribution. In this chapter, we introduce a range of deterministic approximation schemes, some of which scale well to large applications", "3d2a41dc-33e9-4899-9483-99b0bec6b8bd": "Here we provide an example of how this can occur, even if there are no saddle points or local minima. This example cost function contains only asymptotes toward low values, not minima. The main cause of difficulty in this case is being initialized on the wrong side of the \u201cmountain\u201d and not being able to traverse it. In higher-dimensional space, learning algorithms can often circumnavigate such mountains, but the trajectory associated with doing so may be long and result in excessive training time, as illustrated in figure 8.2.  training arrives at a global minimum, a local minimum, or a saddle point, but in practice, neural networks do not arrive at a critical point of any kind. Figure 8.1  al awen AL ab 2 een beeen den Afb A nk net 2b 8 tn Af AM we tnt TAA  https://www.deeplearningbook.org/contents/optimization.html    SLLOWS Ullal UCULAL LEUWULKS OLLELL UU LLOL ALLIVE Gb @ LORIOL UI Sillall BIGUILELLL. LuUeTU, such critical points do not even necessarily exist", "6e04cfab-d217-4081-8676-ed933f45d4ec": "Can this same recipe for success transfer to generative modeling?\n\nGenerative modeling seems to be more difficult than classification or regression because the learning process requires optimizing intractable criteria. In the context of differentiable generator nets, the criteria are intractable because the data does not specify both the inputs z and the outputs z of the generator net. In the case of supervised learning, both the inputs x and the outputs y were given, and the optimization procedure needs only to learn how to produce the specified mapping. In the case of generative modeling, the learning procedure needs to determine how to arrange z space in a useful way and additionally how to map from z to a.  https://www.deeplearningbook.org/contents/generative_models.html    Dosovitskly, et al. studied a simplified problem, where the correspondence between % and 7 is piven. Specifically, the training data is computer-rendered imagery of chairs. The latent variables 2 are parameters given to the rendering engine describing the choice of which chair model to use, the position of the chair,  and other configuration details that affect the rendering of the image", "e68cd55e-132d-4073-9fdd-931973b2aa8d": "First, when we refer to convolution in the context of neural networks, we usually actually mean an operation that consists of many applications of convolution in parallel. This is because convolution with a single kernel can extract only one kind of feature, albeit at many spatial locations. Usually we want each layer of our network to extract many kinds of features, at many locations. Additionally, the input is usually not just a grid of real values. Rather, it is a grid of vector-valued observations. For example, a color image has a red, green and blue intensity at each pixel.\n\nIn a multilayer convolutional network, the input to the second layer is the output of the first layer, which usually has the output of many different convolutions at each position. When working with images, we usually think of the input and output of the convolution as being 3-D tensors, with one index into the different channels and two indices into the spatial coordinates of each channel. Software implementations usually work in batch mode, so they will actually use 4-D tensors, with the fourth axis indexing different examples in the batch, but we will omit the batch axis in our description here for simplicity", "d7f595a5-fca1-4e24-baa2-3cb5a233efb5": "However, the gradient of the lower bound w.r.t. \u03c6 is a bit problematic. The usual (na\u00a8\u0131ve) Monte Carlo gradient estimator for this type of problem is: \u2207\u03c6Eq\u03c6(z)  = Eq\u03c6(z) \ufffd f(z)\u2207q\u03c6(z) log q\u03c6(z) \ufffd \u2243 1 In this section we introduce a practical estimator of the lower bound and its derivatives w.r.t. the parameters. We assume an approximate posterior in the form q\u03c6(z|x), but please note that the technique can be applied to the case q\u03c6(z), i.e. where we do not condition on x, as well", "bebca987-eb39-4410-a856-c95d58d667d9": "However, we have assumed that the contribution to the predictive variance arising from the additive noise, governed by the parameter \u03b2, is a constant. For some problems, known as heteroscedastic, the noise variance itself will also depend on x. To model this, we can extend the Gaussian process framework by introducing a second Gaussian process to represent the dependence of \u03b2 on the input x . Because \u03b2 is a variance, and hence nonnegative, we use the Gaussian process to model ln \u03b2(x). In the previous section, we saw how maximum likelihood could be used to determine a value for the correlation length-scale parameter in a Gaussian process. This technique can usefully be extended by incorporating a separate parameter for each input variable .\n\nThe result, as we shall see, is that the optimization of these parameters by maximum likelihood allows the relative importance of different inputs to be inferred from the data. This represents an example in the Gaussian process context of automatic relevance determination, or ARD, which was originally formulated in the framework of neural networks . The mechanism by which appropriate inputs are preferred is discussed in Section 7.2.2", "592e64e5-196d-4f9d-aa57-d7c50a9862bd": "which we can evaluate by \ufb01rst writing its square in the form Now make the transformation from Cartesian coordinates (x, y) to polar coordinates (r, \u03b8) and then substitute u = r2. Show that, by performing the integrals over \u03b8 and u, and then taking the square root of both sides, we obtain Finally, use this result to show that the Gaussian distribution N(x|\u00b5, \u03c32) is normalized. with respect to \u03c32, verify that the Gaussian satis\ufb01es (1.50). Finally, show that (1.51) holds. 1.11 (\u22c6) By setting the derivatives of the log likelihood function (1.54) with respect to \u00b5 and \u03c32 equal to zero, verify the results (1.55) and (1.56). where xn and xm denote data points sampled from a Gaussian distribution with mean \u00b5 and variance \u03c32, and Inm satis\ufb01es Inm = 1 if n = m and Inm = 0 otherwise", "e2fd4f78-4df1-4967-ad2a-fc05dc297512": "Here we illustrate the case of t= 2.\n\nOne of these kernels has edges labeled \u201ca\u201d and \u201cb,\u201d while the other has edges labeled \u201cc\u201d and \u201cd.\u201d Each time we move one pixel to the right in the output, we move on to using a different kernel. This means that, like the locally connected layer, neighboring units in the output have different parameters. Unlike the locally connected layer, after we have gone through allt available kernels, we cycle back to the first kernel. If two output units are separated by a multiple of t steps, then they share parameters. (Bottom) Traditional convolution is equivalent to tiled convolution with t = 1. There is only one kernel, and it is applied everywhere, as indicated in the diagram by using the kernel with weights labeled \u201ca\u201d and \u201cb\u201d everywhere. Locally connected layers and tiled convolutional layers both have an interesting interaction with max pooling: the detector units of these layers are driven by different filters. If these filters learn to detect different transformed versions of  349  CHAPTER 9", "bafd466a-37d9-4bf1-b4a4-6185faac3122": "This situation typically arises when one of the factors is the prior p(\u03b8), and so we see that the prior factor can be incorporated once exactly and does not need to be re\ufb01ned. 10.38 (\u22c6 \u22c6 \u22c6) In this exercise and the next, we shall verify the results (10.214)\u2013(10.224) for the expectation propagation algorithm applied to the clutter problem. Begin by using the division formula (10.205) to derive the expressions (10.214) and (10.215) by completing the square inside the exponential to identify the mean and variance. Also, show that the normalization constant Zn, de\ufb01ned by (10.206), is given for the clutter problem by (10.216). This can be done by making use of the general result (2.115). 10.39 (\u22c6 \u22c6 \u22c6) Show that the mean and variance of qnew(\u03b8) for EP applied to the clutter problem are given by (10.217) and (10.218). To do this, \ufb01rst prove the following results for the expectations of \u03b8 and \u03b8\u03b8T under qnew(\u03b8) and then make use of the result (10.216) for Zn", "5f3afb78-5e79-46bc-9179-ef51fdfbb55a": "Such \ufb01xed basis function models have important limitations, and these will be Section 3.6 resolved in later chapters by allowing the basis functions themselves to adapt to the data. Notwithstanding these limitations, models with \ufb01xed nonlinear basis functions play an important role in applications, and a discussion of such models will introduce many of the key concepts needed for an understanding of their more complex counterparts. with p(C2|\u03c6) = 1 \u2212 p(C1|\u03c6). Here \u03c3(\u00b7) is the logistic sigmoid function de\ufb01ned by (4.59). In the terminology of statistics, this model is known as logistic regression, although it should be emphasized that this is a model for classi\ufb01cation rather than regression. For an M-dimensional feature space \u03c6, this model has M adjustable parameters.\n\nBy contrast, if we had \ufb01tted Gaussian class conditional densities using maximum likelihood, we would have used 2M parameters for the means and M(M + 1)/2 parameters for the (shared) covariance matrix. Together with the class prior p(C1), this gives a total of M(M +5)/2+1 parameters, which grows quadratically with M, in contrast to the linear dependence on M of the number of parameters in logistic regression", "edc28d6d-6fe7-4c63-be84-dabc1554f45f": "Here we have introduced the two quantities Here si is called the sparsity and qi is known as the quality of \u03d5i, and as we shall see, a large value of si relative to the value of qi means that the basis function \u03d5i is more likely to be pruned from the model. The \u2018sparsity\u2019 measures the extent to which basis function \u03d5i overlaps with the other basis vectors in the model, and the \u2018quality\u2019 represents a measure of the alignment of the basis vector \u03d5n with the error between the training set values t = (t1, . , tN)T and the vector y\u2212i of predictions that would result from the model with the vector \u03d5i excluded . The stationary points of the marginal likelihood with respect to \u03b1i occur when is equal to zero. There are two possible forms for the solution. Recalling that \u03b1i \u2a7e 0, we see that if q2 i < si, then \u03b1i \u2192 \u221e provides a solution. Conversely, if q2 These two solutions are illustrated in Figure 7.11", "fb9bb10e-f774-492c-b2dd-8f30b3fc3a09": "12.13 (* *) Show that the optimal reconstruction of a data point under probabilistic PCA, according to the least squares projection cost of conventional PCA, is given by 12.14 (*) The number of independent parameters in the covariance matrix for the probabilistic PCA model with an M -dimensional latent space and a D-dimensional data space is given by (12.51). Verify that in the case of M = D - 1, the number of independent parameters is the same as in a general covariance Gaussian, whereas for M =\u00b0it is the same as for a Gaussian with an isotropic covariance. 12.15 (**) IIiI!I Derive the M-step equations (12.56) and (12.57) for the probabilistic PCA model by maximization of the expected complete-data log likelihood function given by (12.53). 12.16 (* * *) In Figure 12.11, we showed an application of probabilistic PCA to a data set in which some of the data values were missing at random. Derive the EM algorithm for maximizing the likelihood function for the probabilistic PCA model in this situation", "05fcceac-e920-443f-bc80-cd7c6d831634": "Here we see that the additional data points in the righthand \ufb01gure produce a signi\ufb01cant change in the location of the decision boundary, even though these point would be correctly classi\ufb01ed by the original decision boundary in the left-hand \ufb01gure. The sum-of-squares error function penalizes predictions that are \u2018too correct\u2019 in that they lie a long way on the correct side of the decision the decision boundary found by least squares (magenta curve) and also by the logistic regression model (green curve), which is discussed later in Section 4.3.2. The right-hand plot shows the corresponding results obtained when extra data points are added at the bottom left of the diagram, showing that least squares is highly sensitive to outliers, unlike logistic regression. boundary.\n\nIn Section 7.1.2, we shall consider several alternative error functions for classi\ufb01cation and we shall see that they do not suffer from this dif\ufb01culty. However, problems with least squares can be more severe than simply lack of robustness, as illustrated in Figure 4.5. This shows a synthetic data set drawn from three classes in a two-dimensional input space (x1, x2), having the property that linear decision boundaries can give excellent separation between the classes. Indeed, the technique of logistic regression, described later in this chapter, gives a satisfactory solution as seen in the right-hand plot", "c1905c4a-e258-49b6-bdaa-42db62bbbd94": "SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  @) a  https://www.deeplearningbook.org/contents/rnn.html    Figure 10.10: A conditional recurrent neural network mapping a variable-length sequence of x values into a distribution over sequences of y values of the same length. Compared to figure 10.3, this RNN contains connections from the previous output to the current state. These connections allow this RNN to model an arbitrary distribution over sequences ofy  given sequences of x of the same length. The RNN of figure 10.3 is only able to represent distributions in which the y values are conditionally independent from each other given the a values. 387  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  sponds to a conditional distribution P(y\u2122,...,y | a\u2122,...,a(7)) that makes a conditional independence assumption that this distribution factorizes as  Ply | a, 22,0)", "d9276db8-62ad-47c5-b9b8-f5281c5e1a20": "MCTS does not have to retain approximate value functions or policies from one action selection to the next, though in many implementations it retains selected action values likely to be useful for its next execution. For the most part, the actions in the simulated trajectories are generated using a simple policy, usually called a rollout policy as it is for simpler rollout algorithms. When both the rollout policy and the model do not require a lot of computation, many simulated trajectories can be generated in a short period of time. As in any tabular Monte Carlo method, the value of a state\u2013action pair is estimated as the average of the (simulated) returns from that pair.\n\nMonte Carlo value estimates are maintained only for the subset of state\u2013action pairs that are most likely to be reached in a few steps, which form a tree rooted at the current state, as illustrated in Figure 8.10. MCTS incrementally extends the tree by adding nodes representing states that look promising based on the results of the simulated trajectories. Any simulated trajectory will pass through the tree and then exit it at some leaf node", "47d7605d-86c3-4a47-8100-364bc13581e2": "APPLICATIONS  natural language processing applications, such as parsing , part-of-speech tagging, semantic role labeling, chunking, and so on, sometimes using a single multitask learning architecture  in which the word embeddings are shared  https://www.deeplearningbook.org/contents/applications.html    across tasks. Two-dimensional visualizations of embeddings became a popular tool for an- alyzing language models following the development of the Lenk dimensionality reduction algorithm  and its high-profile appli- cation to visualization word embeddings by Joseph Turian in 2009. 12.5 Other Applications  In this section we cover a few other types of applications of deep learning that are different from the standard object recognition, speech recognition, and natural language processing tasks discussed above.\n\nPart III of this book will expand that scope even further to tasks that remain primarily research areas. 12.5.1 Recommender Systems  One of the major families of applications of machine learning in the information technology sector is the ability to make recommendations of items to potential users or customers. Two major types of applications can be distinguished: online advertising and item recommendations (often these recommendations are still for the purpose of selling a product)", "16106058-dce3-4aa3-8bae-1998af16208d": "This leads Exercise 10.31 to a lower bound on f(x), which is a linear function of x2 whose conjugate function is given by If we denote this value of x, corresponding to the contact point of the tangent line for this particular value of \u03bb, by \u03be, then we have Instead of thinking of \u03bb as the variational parameter, we can let \u03be play this role as this leads to simpler expressions for the conjugate function, which is then given by Hence the bound on f(x) can be written as where \u03bb(\u03be) is de\ufb01ned by (10.141).\n\nThis bound is illustrated in the right-hand plot of Figure 10.12. We see that the bound has the form of the exponential of a quadratic function of x, which will prove useful when we seek Gaussian representations of posterior distributions de\ufb01ned through logistic sigmoid functions. Section 4.5 The logistic sigmoid arises frequently in probabilistic models over binary variables because it is the function that transforms a log odds ratio into a posterior probability. The corresponding transformation for a multiclass distribution is given by the softmax function. Unfortunately, the lower bound derived here for the logistic Section 4.3 sigmoid does not directly extend to the softmax", "3299e9ba-ec4c-4925-90b3-ca66e0761871": "(2.20)  The structure of the identity matrix is simple: all the entries along the main diagonal are 1, while all the other entries are zero. See figure 2.2 for an example. https://www.deeplearningbook.org/contents/linear_algebra.html    The matrix inverse of A is denoted as Att and it is defined as the matrix  such that 1 AVA=I,. (2.21)  We can now solve equation 2.11 using the following steps:  Ax=b (2.22) A\u201d Ax = Aq'b (2.23) I,2 = A~'b (2.24) | 10 0 ] 0 1 0  Figure 2.2: Example identity matrix: This is 8. 34  CHAPTER 2. LINEAR ALGEBRA  x= A'b.\n\n(2.25)  Of course, this process depends on it being possible to find A~!. We discuss the conditions for the existence of A~! in the following section. When A~! exists, several different algorithms can find it in closed form. In theory, the same inverse matrix can then be used to solve the equation many times for different values of b", "0194dee8-db0c-444c-98d5-954b7b998435": "The integration over w can be performed analytically by noting that p(w) is Gaussian and h(w, \u03be) is the exponential of a quadratic function of w. Thus, by completing the square and making use of the standard result for the normalization coef\ufb01cient of a Gaussian distribution, we can obtain a closed form solution which takes the form Exercise 10.35 set. The plot on the left shows the predictive distribution obtained using variational inference. We see that the decision boundary lies roughly mid way between the clusters of data points, and that the contours of the predictive distribution splay out away from the data re\ufb02ecting the greater uncertainty in the classi\ufb01cation of such regions. The plot on the right shows the decision boundaries corresponding to \ufb01ve samples of the parameter vector w drawn from the posterior distribution p(w|t). This variational framework can also be applied to situations in which the data is arriving sequentially . In this case we maintain a Gaussian posterior distribution over w, which is initialized using the prior p(w)", "ecb0a112-b7bf-475b-92d3-526b14604a7b": "15.3. Semi-Supervised Disentangling of Causal Factors  An important question about representation learning is: what makes one repre- sentation better than another? One hypothesis is that an ideal representation is one in which the features within the representation correspond to the underly- ing causes of the observed data, with separate features or directions in feature space corresponding to different causes, so that the representation disentangles the causes from one another. This hypothesis motivates approaches in which we first seek a good representation for p(a). Such a representation may also be a good representation for computing p(y | a) if y is among the most salient causes of a. This idea has guided a large amount of deep learning research since at least the 1990s  in more detail.\n\nFor other arguments about when semi-supervised learning can outperform pure supervised learning, we refer the reader to section 1.2 of Chapelle et al. In other approaches to representation learning, we have often been concerned with a representation that is easy to model\u2014for example, one whose entries are sparse or independent from each other. A representation that cleanly separates the underlying causal factors may not necessarily be one that is easy to model", "0ce03cdd-f82d-41c2-ba1e-46e0fffac178": "Note that in the neural networks literature, it is usual to consider the minimization of an error function rather than the maximization of the (log) likelihood, and so here we shall follow this convention. Consider \ufb01rst the determination of w. Maximizing the likelihood function is equivalent to minimizing the sum-of-squares error function given by where we have discarded additive and multiplicative constants. The value of w found by minimizing E(w) will be denoted wML because it corresponds to the maximum likelihood solution. In practice, the nonlinearity of the network function y(xn, w) causes the error E(w) to be nonconvex, and so in practice local maxima of the likelihood may be found, corresponding to local minima of the error function, as discussed in Section 5.2.1.\n\nHaving found wML, the value of \u03b2 can be found by minimizing the negative log likelihood to give Note that this can be evaluated once the iterative optimization required to \ufb01nd wML is completed", "554e1866-f385-4936-a802-2c729c200173": "Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. 2019. Universal adversarial triggers for attacking and analyzing nlp. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2153\u20132162. Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference.\n\nIn Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112\u20131122. Ronald J Williams. 1992. Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229\u2013256. Lijun Wu, Fei Tian, Tao Qin, Jianhuang Lai, and TieYan Liu. 2018. A study of reinforcement learning for neural machine translation", "f82f8021-d9cb-44d5-8e31-1cbdf67df5da": "One unusual capability of the GAN training procedure is that it can fit proba- bility distributions that assign zero probability to the training points. Rather than maximizing the log-probability of specific points, the generator net learns to trace out a manifold whose points resemble training points in some way. Somewhat para- doxically, this means that the model may assign a log-likelihood of negative infinity to the test set, while still representing a manifold that a human observer judges to capture the essence of the generation task. This is not clearly an advantage or  698  CHAPTER 20.\n\nDEEP GENERATIVE MODELS  https://www.deeplearningbook.org/contents/generative_models.html    Figure 20.7: Images generated by GANs trained on the LSUN dataset. (Left)Images of bedrooms generated by a DCGAN model, reproduced with permission from Radford et al. (Right)Images of churches generated by a LAPGAN model, reproduced with permission from Denton et al", "7f8d0ff0-c1bb-494b-b07a-0a4b6b04a264": "Redo the derivation above to determine the additional amount that must be added to the sum of TD errors in order to equal the Monte Carlo error. \u21e4 Example 6.1: Driving Home Each day as you drive home from work, you try to predict how long it will take to get home. When you leave your o\ufb03ce, you note the time, the day of week, the weather, and anything else that might be relevant. Say on this Friday you are leaving at exactly 6 o\u2019clock, and you estimate that it will take 30 minutes to get home. As you reach your car it is 6:05, and you notice it is starting to rain. Tra\ufb03c is often slower in the rain, so you reestimate that it will take 35 minutes from then, or a total of 40 minutes.\n\nFifteen minutes later you have completed the highway portion of your journey in good time. As you exit onto a secondary road you cut your estimate of total travel time to 35 minutes. Unfortunately, at this point you get stuck behind a slow truck, and the road is too narrow to pass. You end up having to follow the truck until you turn onto the side street where you live at 6:40. Three minutes later you are home", "c9afbec8-a59e-4fe9-bf9b-9fabe46f3446": "In the SE Equation 3.2 (with cross entropy and Shannon entropy), we set \u03b1 = 1, and \u03b2 to a very small positive value \u03f5. As a result, the auxiliary distribution q(x, y) is determined directly by the full data instances (not the model p\u03b8). That is, the solution of q in the teacher-step (Equation 3.3) is: which reduces to the empirical distribution. The subsequent student-step that maximizes the loglikelihood of samples from q then leads to the supervised MLE updates w.r.t. \u03b8. 4.1.2. Self-supervised data instances. Given an observed data instance t\u2217 \u2208 D in general, one could potentially derive various supervision signals based on the structures of the data and the target model.\n\nIn particular, one could apply a \u201csplit\u201d function that arti\ufb01cially partitions t\u2217 into two parts (x\u2217, y\u2217) = split(t\u2217) in di\ufb00erent, sometimes stochastic ways", "2057456b-c976-48fb-9f75-7ba77a9bce78": "Instead of St+1 becoming the new St for the next update as it would in the usual form of Q-learning, a new unconnected experience was drawn from the replay memory to supply data for the next update.\n\nBecause Q-learning is an o\u21b5-policy algorithm, it does not need to be applied along connected trajectories. Q-learning with experience replay provided several advantages over the usual form of Q-learning. The ability to use each stored experience for many updates allowed DQN to learn more e\ufb03ciently from its experiences. Experience replay reduced the variance of the updates because successive updates were not correlated with one another as they would be with standard Q-learning. And by removing the dependence of successive experiences on the current weights, experience replay eliminated one source of instability. Mnih et al. modi\ufb01ed standard Q-learning in a second way to improve its stability. As in other methods that bootstrap, the target for a Q-learning update depends on the current action-value function estimate. When a parameterized function approximation method is used to represent action values, the target is a function of the same parameters that are being updated", "39e897ca-4055-4722-b556-a5d9203b7d20": "Of course, the particular states and actions vary greatly from task to task, and how they are represented can strongly a\u21b5ect performance. In reinforcement learning, as in other kinds of learning, such representational choices are at present more art than science.\n\nIn this book we o\u21b5er some advice and examples regarding good ways of representing states and actions, but our primary focus is on general principles for learning how to behave once the representations have been selected. Example 3.1: Bioreactor Suppose reinforcement learning is being applied to determine moment-by-moment temperatures and stirring rates for a bioreactor (a large vat of nutrients and bacteria used to produce useful chemicals). The actions in such an application might be target temperatures and target stirring rates that are passed to lower-level control systems that, in turn, directly activate heating elements and motors to attain the targets. The states are likely to be thermocouple and other sensory readings, perhaps \ufb01ltered and delayed, plus symbolic inputs representing the ingredients in the vat and the target chemical. The rewards might be moment-by-moment measures of the rate at which the useful chemical is produced by the bioreactor", "18d23386-9e2b-4da1-a085-4227f61ada61": "However, it becomes tricky to do hard negative mining when we want to remain unsupervised. Increasing training batch size or memory bank size implicitly introduces more hard negative samples, but it leads to a heavy burden of large memory usage as a side effect. Chuang et al. studied the sampling bias in contrastive learning and proposed debiased loss. In the unsupervised setting, since we do not know the ground truth labels, we may accidentally sample false negative samples. Sampling bias can lead to significant performance drop.\n\n95 + > ~ : 3g Pe false negative g \u201d sample 3 a8 ~p & \u2014\u2014 Biased \u2014<e\u2014 Unbiased 80 3062 126 254 510  Negative Sample Size (N)  Let us assume the probability of anchor class c is uniform p(c) = 1\u00b0 and the probability of  https://lilianweng.github.io/posts/2021-05-31-contrastive/  Contrastive Representation Learning | Lil'Log   observing a different class is 7~ = 1 \u2014 n*", "bef3cf90-fdc0-4920-b71c-4a50e8407e0e": "The reward signal is zero throughout each trial except when the agent reaches the rewarding state, shown near the right end of the time line, when the reward signal becomes some positive number, say R?. The goal of TD learning is to predict the return for each state visited in a trial, which in this undiscounted case and given our assumption that predictions are con\ufb01ned to individual trials, is simply R? for each state. Preceding the rewarding state is a sequence of reward-predicting states, with the earliest reward-predicting state shown near the left end of the time line. This is like the state near the start of a trial, for example like the state marked by the instruction cue in a trial of the monkey experiment of Schultz et al. described above. It is the \ufb01rst state in a trial that reliably predicts that trial\u2019s reward.\n\n(Of course, in reality states visited on preceding trials are even earlier reward-predicting states, but because we are con\ufb01ning predictions to individual trials, these do not qualify as predictors of this trial\u2019s reward", "2627358a-d16b-49ff-910e-832276a3027b": "For example, the data instances may follow changing distributions or come from evolving tasks (e.g., a sequence of tasks that are increasingly complex); the experience in a strategic game context can involve complex interactions with the target model through co-training or adversarial dynamics; and the success criteria of the model w.r.t. the experience can be adapting. In this section, we discuss an extended view of the SE in dealing with the learning in such dynamic contexts. Instead of serving as the static overall objective function, now the SE is a core part of an outer loop in the learning procedure. More speci\ufb01cally, each of the SE components (e.g., experience function, divergence function, balancing weights) can change over time.\n\nFor example, consider a dynamic experience function f\u03c4, which is indexed by \u03c4 indicating its evolution over time, iterations, or tasks. This di\ufb00ers from the experience discussed earlier in Section 4 that is de\ufb01ned a priori (e.g., a static set of data instances) and encoded as a \ufb01xed experience function f", "2e79177e-7161-47a6-96ba-17c2adf0c02a": "where KL is the Kullback\u2013Leibler divergence.\n\nWe recognize in the previous expression the Jensen\u2013 Shannon divergence between the model\u2019s distribution and the data generating process: C(G) = \u2212 log(4) + 2 \u00b7 JSD (pdata \u2225pg ) (6) Since the Jensen\u2013Shannon divergence between two distributions is always non-negative, and zero iff they are equal, we have shown that C\u2217 = \u2212 log(4) is the global minimum of C(G) and that the only solution is pg = pdata, i.e., the generative model perfectly replicating the data distribution. Proof. Consider V (G, D) = U(pg, D) as a function of pg as done in the above criterion. Note that U(pg, D) is convex in pg. The subderivatives of a supremum of convex functions include the derivative of the function at the point where the maximum is attained", "883afe8f-b97a-4ee4-8afd-87101aa04d86": "Another way of controlling the exponential growth in the number of parameters in models of discrete variables is to use parameterized models for the conditional distributions instead of complete tables of conditional probability values. To illustrate this idea, consider the graph in Figure 8.13 in which all of the nodes represent binary variables. Each of the parent variables xi is governed by a single parameter \u00b5i representing the probability p(xi = 1), giving M parameters in total for the parent nodes. The conditional distribution p(y|x1, . .\n\n, xM), however, would require 2M parameters representing the probability p(y = 1) for each of the 2M possible settings of the parent variables. Thus in general the number of parameters required to specify this conditional distribution will grow exponentially with M. We can obtain a more parsimonious form for the conditional distribution by using a logistic sigmoid function acting on a linear combination of the parent variables, giving Section 2.4 where \u03c3(a) = (1+exp(\u2212a))\u22121 is the logistic sigmoid, x = (x0, x1,", "0e8cf1a7-ab11-4eb4-9e71-851dbd3efb1d": "As we shall see, we can exploit the graphical structure both to \ufb01nd ef\ufb01cient algorithms for inference, and to make the structure of those algorithms transparent. Speci\ufb01cally, we shall see that many algorithms can be expressed in terms of the propagation of local messages around the graph. In this section, we shall focus primarily on techniques for exact inference, and in Chapter 10 we shall consider a number of approximate inference algorithms. To start with, let us consider the graphical interpretation of Bayes\u2019 theorem. Suppose we decompose the joint distribution p(x, y) over two variables x and y into a product of factors in the form p(x, y) = p(x)p(y|x). This can be represented by the directed graph shown in Figure 8.37(a). Now suppose we observe the value of y, as indicated by the shaded node in Figure 8.37(b). We can view the marginal distribution p(x) as a prior over the latent variable x, and our goal is to infer the corresponding posterior distribution over x", "7e490574-d387-4364-8f83-1c66c403ad39": "The approximation J is given by  F(0) = Jw) + 5(w ~ w\")\"H(w ~ w*), (7.6)  where H is the Hessian matrix of J with respect to w evaluated at w*.\n\nThere is no first-order term in this quadratic approximation, because w* is defined to be a minimum, where the gradient vanishes. Likewise, because w%* is the location of a minimum of J, we can conclude that H is positive semidefinite. The minimum of J occurs where its gradient Vwd(w) = H(w \u2014w*) (7.7) is equal to 0. To study the effect of weight decay, we modify equation 7.7 by adding the weight decay gradient. We can now solve for the minimum of the regularized version of J. We use the variable W to represent the location of the minimum. aw + H(w \u2014 w*) =0 (7.8) (H +al)w = Hw* (7.9) w =(H+al)!Hw* (7.10)  As qa approaches 0, the regularized solution wW approaches w*", "74a89872-ec39-4a19-afd1-702d7816ddbb": "The usual justi\ufb01cation for a Gaussian approximation to a posterior distribution is that the true posterior will tend to a Gaussian as the number of data points increases as a consequence of the central limit theorem. In the case of Gaussian processes, the number of variables grows with Section 2.3 the number of data points, and so this argument does not apply directly.\n\nHowever, if we consider increasing the number of data points falling in a \ufb01xed region of x space, then the corresponding uncertainty in the function a(x) will decrease, again leading asymptotically to a Gaussian . Three different approaches to obtaining a Gaussian approximation have been considered. One technique is based on variational inference (Gibbs and MacKay, Section 10.1 2000) and makes use of the local variational bound (10.144) on the logistic sigmoid. This allows the product of sigmoid functions to be approximated by a product of Gaussians thereby allowing the marginalization over aN to be performed analytically. The approach also yields a lower bound on the likelihood function p(tN|\u03b8)", "7ceb0607-4eef-4425-a24e-292aeabe6033": "If \u201ccauses\u201d does not appear, it outputs None, indicating abstention: if de < cs and \"causes\" in x.parent.words: We could also write this with Snorkel\u2019s declarative interface: Declarative Labeling Functions Snorkel includes a library of declarative operators that encode the most common weak supervision function types, based on our experience with users over the last year.\n\nThe semantics and syntax of these operatorsissimpleandeasilycustomizable,consistingoftwo main types: (i) labeling function templates, which are simply functions that take one or more arguments and output a single labeling function; and (ii) labeling function generators, which take one or more arguments and output a set of labeling functions (described below). These functions capture a range of common forms of weak supervision, for example: \u2022 Pattern-based Pattern-based heuristics embody the motivation of soliciting higher information density input from SMEs. For example, pattern-based heuristics encompass feature annotations  and pattern-bootstrapping approaches  (Example 2.3)", "99b5216d-f1f8-4838-b5f5-a064e8cddac4": "State transitions were considered to be stochastic because the next state of the system not only depends on the scheduler\u2019s command, but also on aspects of the system\u2019s behavior that the scheduler cannot control, such as the workloads of the processor cores accessing the DRAM system. permission, from J. F. Mart\u00b4\u0131nez and E. \u02d9Ipek, Dynamic multicore resource management: A machine learning approach, Micro, IEEE, 29(5), p. 12. Critical to this MDP are constraints on the actions available in each state. Recall from Chapter 3 that the set of available actions can depend on the state: At 2 A(St), where At is the action at time step t and A(St) is the set of actions available in state St. In this application, the integrity of the DRAM system was assured by not allowing actions that would violate timing or resource constraints. Although \u02d9Ipek et al. did not make it explicit, they e\u21b5ectively accomplished this by pre-de\ufb01ning the sets A(St) for all possible states St", "77f06359-2759-48c2-b53c-235702384e26": "If the function is complicated (we want to distinguish a huge number of regions compared to the number of examples), is there any hope to generalize well? The answer to both of these questions\u2014whether it is possible to represent a complicated function efficiently, and whether it is possible for the estimated function to generalize well to new inputs\u2014is yes. The key insight is that a very large number of regions, such as O(2*), can be defined with O(k) examples, so long as we  O  Figure 5.10: Illustration of how the nearest neighbor algorithm breaks up the input space into regions. An example (represented here by a circle) within each region defines the region boundary (represented here by the lines). They value associated with each example defines what the output should be for all points within the corresponding region. The regions defined by nearest neighbor matching form a geometric pattern called a Voronoi diagram. The number of these contiguous regions cannot grow faster than the number of training examples.\n\nWhile this figure illustrates the behavior of the nearest neighbor algorithm specifically, other machine learning algorithms that rely exclusively on the local smoothness prior for generalization exhibit similar behaviors: each training example only informs the learner about how to generalize in some neighborhood immediately surrounding that example. 156  CHAPTER 5", "5280073c-8384-41af-a579-7aa534c49ce0": "Andreae, J. H. STELLA, A scheme for a learning machine. In Proceedings of the 2nd IFAC Congress, Basle, pp. 497\u2013502. Butterworths, London. Andreae, J. H. Learning machines\u2014a uni\ufb01ed view. In A. R. Meetham and R. A. Hudson Andreae, J. H. Thinking with the Teachable Machine. Academic Press, London. Andreae, J. H. A model of how the brain learns: A short introduction to multiple context associative learning (MCAL) and the PP system. Unpublished report. Andreae, J. H. Working memory for the associative learning of language. Unpublished Arthur, W. B. Designing economic agents that act like human agents: A behavioral approach to bounded rationality", "6819bea3-f507-45e7-bf9e-f82a8a6bd7e5": "The temporal-di\u21b5erence and optimal control threads were fully brought together integrated prior work in all three threads of reinforcement learning research. Paul Werbos  contributed to this integration by arguing for the convergence of trial-and-error learning and dynamic programming since 1977. By the time of Watkins\u2019s work there had been tremendous growth in reinforcement learning research, primarily in the machine learning sub\ufb01eld of arti\ufb01cial intelligence, but also in arti\ufb01cial neural networks and arti\ufb01cial intelligence more broadly. In 1992, the remarkable success of Gerry Tesauro\u2019s backgammon playing program, TD-Gammon, brought additional attention to the \ufb01eld. In the time since publication of the \ufb01rst edition of this book, a \ufb02ourishing sub\ufb01eld of neuroscience developed that focuses on the relationship between reinforcement learning algorithms and reinforcement learning in the nervous system. Most responsible for this is an uncanny similarity between the behavior of temporal-di\u21b5erence algorithms and the activity of dopamine producing neurons in the brain, as pointed out by a number of researchers . Chapter 15 provides an introduction to this exciting aspect of reinforcement learning.\n\nOther important contributions made in the recent history of reinforcement learning are too numerous to mention in this brief account; we cite many of these at the end of the individual chapters in which they arise", "d40cf219-8c8c-4f21-acaf-6ff495e0da34": "In order to \ufb01nd a generalization of the between-class covariance matrix, we follow Duda and Hart  and consider \ufb01rst the total covariance matrix where m is the mean of the total data set k Nk is the total number of data points. The total covariance matrix can be decomposed into the sum of the within-class covariance matrix, given by (4.40) and (4.41), plus an additional matrix SB, which we identify as a measure of the between-class covariance ST = SW + SB (4.45) These covariance matrices have been de\ufb01ned in the original x-space. We can now de\ufb01ne similar matrices in the projected D\u2032-dimensional y-space Again we wish to construct a scalar that is large when the between-class covariance is large and when the within-class covariance is small. There are now many possible choices of criterion . One example is given by Maximization of such criteria is straightforward, though somewhat involved, and is discussed at length in Fukunaga . The weight values are determined by those eigenvectors of S\u22121 W SB that correspond to the D\u2032 largest eigenvalues", "b9f12644-b7cf-4b32-aed8-4c1683e03194": "Chapter 2  Linear Algebra  Linear algebra is a branch of mathematics that is widely used throughout science and engineering. Yet because linear algebra is a form of continuous rather than discrete mathematics, many computer scientists have little experience with it. A good understanding of linear algebra is essential for understanding and working with many machine learning algorithms, especially deep learning algorithms. We therefore precede our introduction to deep learning with a focused presentation of the key linear algebra prerequisites. If you are already familiar with linear algebra, feel free to skip this chapter. If you have previous experience with these concepts but need a detailed reference sheet to review key formulas, we recommend The Matrix Cookbook . If you have had no exposure at all to linear algebra, this chapter will teach you enough to read this book, but we highly recommend that you also consult another resource focused exclusively on teaching linear algebra, such as Shilov . This chapter completely omits many important linear algebra topics that are not essential for understanding deep learning. 2.1 Scalars, Vectors, Matrices and Tensors The study of linear algebra involves several types of mathematical objects: e Scalars: A scalar is just a single number, in contrast to most of the other objects studied in linear algebra, which are usually arrays of multiple numbers", "71f28d1a-1960-41b0-b376-b544b59f8620": "For example, if using the augmentations demonstrated in Shorten and Khoshgoftaar J Big Data  6:60  image A i patch A (256 x 256) i (224 x 224)  averaging intensity of two patches for each pixel (RGB channels)  randomly picked 1) extracting a 224x224 patch from a random from training set position, 2) horizontal flipping randomly  Fig.\n\n7 SamplePairing augmentation strategy   8.5%  6.0% (non-zero select from all select oni selectonly | selectonly _  selectonly es sampleP ating enone same class from dierent from same super fram afierent  validation set error rate Ba PJ rm 2 = 2 #2 #2 i lower |s bett  with SamplePairing Fig. 8 Results on the reduced CIFAR-10 dataset. Experimental results demonstrated with respect to sampling  pools for image mixing   the AlexNet paper by Krizhevsky et al. , the 2048 x dataset increase can be further expanded to (2048 x N)?. The concept of mixing images in an unintuitive way was further investigated by  Summers and Dinneen . They looked at using non-linear methods to combine images into new training instances", "c4f162af-ac92-4da8-8b9b-4a04cecd9d65": "Pseudocode for this algorithm is shown in the box on the next page.\n\ntree-backup return (7.16) can be written as a sum of expectation-based TD errors: Initialize Q(s, a) arbitrarily, for all s 2 S, a 2 A Initialize \u21e1 to be greedy with respect to Q, or as a \ufb01xed given policy Algorithm parameters: step size \u21b5 2 (0, 1], a positive integer n All store and access operations can take their index mod n + 1 So far in this chapter we have considered three di\u21b5erent kinds of action-value algorithms, corresponding to the \ufb01rst three backup diagrams shown in Figure 7.5. n-step Sarsa has all sample transitions, the tree-backup algorithm has all state-to-action transitions fully branched without sampling, and n-step Expected Sarsa has all sample transitions except for the last state-to-action one, which is fully branched with an expected value. To what extent can these algorithms be uni\ufb01ed? One idea for uni\ufb01cation is suggested by the fourth backup diagram in Figure 7.5", "482e1c64-e9bf-4da8-81de-390a267269da": "Lifted Structured Loss  Lifted Structured Loss  utilizes all the pairwise edges within one training batch for better computational efficiency.\n\nX1 x2 X3 x4 X5 X6 (b) Triplet embedding X1 x2 X3 x4 X5 X6  (c) Lifted structured embedding  Let D;; = |f(x:) \u2014 f(x;)|2, a structured loss function is defined as  https://lilianweng.github.io/posts/2021-05-31-contrastive/     Contrastive Representation Learning | Lil'Log   1 ij Letract = SS max(0, ci)? 2|P| 4 (i,j)EP where \u00a3)) , = D,; + max ( max \u00a2\u2014 Dj, max \u00a2\u2014 Dj) (ik)EN (j1)EN  where P contains the set of positive pairs and WV is the set of negative pairs. Note that the dense pairwise squared distance matrix can be easily computed per training batch. The red part in ce is used for mining hard negatives. However, it is not smooth and may cause the convergence to a bad local optimum in practice", "3edd5279-dc27-4a77-8afe-80b234cac721": "The \ufb01xed points of o\u21b5-policy TD. In Advances in Neural Information Processing Systems 24 , pp. 2169\u20132177. Curran Associates, Inc. Konda, V. R., Tsitsiklis, J. N. Actor-critic algorithms. In Advances in Neural Information Processing Systems 12 , pp. 1008\u20131014. MIT Press, Cambridge, MA. Konda, V. R., Tsitsiklis, J. N. On actor-critic algorithms. SIAM Journal on Control reinforcement learning using the Fourier basis . In Proceedings of the Twenty-Fifth Conference of the Association for the Advancement of Arti\ufb01cial Intelligence, pp. 380\u2013385. Korf, R. E. Optimal path \ufb01nding algorithms. In L. N. Kanal and V. Kumar (Eds. ), Search in Arti\ufb01cial Intelligence, pp. 223\u2013267", "9c992a9a-d1d1-41b8-bd91-5b1d7ee524f1": "The results displayed Shorten and Khoshgoftaar J Big Data  6:60   Table 4 Test accuracies showing the impact of adversarial training, clean refers to the original testing data, FGSM refers to adversary examples derived from Fast Gradient Sign Method and PGD refers to adversarial examples derived from Projected Gradient Descent   Models MNIST CIFAR-10  Clean FGSM PGD Clean FGSM PGD Standard 0.9939 0.0922 ie) 0.9306 0.5524 0.0256 Adversarially trained 0.9932 0.9492 0.0612 0.8755 0.8526 0.1043 Our method 0.9903 0.9713 0.9171 0.8714 0.6514 0.3440  below show how anticipation of adversarial attacks in the training process can dramati- cally reduce the success of attacks. As shown in Table 4, the adversarial training in their experiment did not improve the test accuracy. However, it does significantly improve the test accuracy of adversarial examples. Adversarial defense is a very interesting subject for evaluating security and robustness of Deep Learning models.\n\nImproving on the Fast Gradient Sign Method, DeepFool, developed by Moosavi-Dezfooli et al", "c00f4d2a-534e-48bf-9bd6-5af91cd438b5": "We describe these exponential gains more precisely in sections 6.4.1, 15.4 and 15.5. The exponential advantages conferred by the use of deep distributed representations counter the exponential challenges posed by the curse of dimensionality. 5.11.3 Manifold Learning  An important concept underlying many ideas in machine learning is that of a manifold. A manifold is a connected region. Mathematically, it is a set of points associated with a neighborhood around each point. From any given point, the manifold locally appears to be a Euclidean space. In everyday life, we experience the surface of the world as a 2-D plane, but it is in fact a spherical manifold in 3-D space. The concept of a neighborhood surrounding each point implies the existence of transformations that can be applied to move on the manifold from one position to a neighboring one", "2db1782e-9123-4afe-ad0d-3bb46d8f372d": "Browne, Powley, Whitehouse, Lucas, Cowling, Rohlfshagen, Tavener, Perez, Samothrakis, and Colton  is an excellent survey of MCTS methods and their applications. David Silver contributed to the ideas and presentation in this section. In the second part of the book we extend the tabular methods presented in the \ufb01rst part to apply to problems with arbitrarily large state spaces. In many of the tasks to which we would like to apply reinforcement learning the state space is combinatorial and enormous; the number of possible camera images, for example, is much larger than the number of atoms in the universe.\n\nIn such cases we cannot expect to \ufb01nd an optimal policy or the optimal value function even in the limit of in\ufb01nite time and data; our goal instead is to \ufb01nd a good approximate solution using limited computational resources. In this part of the book we explore such approximate solution methods. The problem with large state spaces is not just the memory needed for large tables, but the time and data needed to \ufb01ll them accurately. In many of our target tasks, almost every state encountered will never have been seen before. To make sensible decisions in such states it is necessary to generalize from previous encounters with di\u21b5erent states that are in some sense similar to the current one", "4d6bb50e-cfef-4794-8884-78b93b6b7341": "It is not necessary o understand calculus of variations to understand the content of this chapter. At he moment, it is only necessary to understand that calculus of variations may be used to derive the following two results. 176  https://www.deeplearningbook.org/contents/mlp.html    CHAPTER 6. DEEP FEEDFORWARD NETWORKS  Our first result derived using calculus of variations is that solving the optimiza- tion problem  f= argmin Sx,y~PaatallY \u2014 f(a)| |? (6.14)  yields  \u00a3*(\u00ae) = Eyvpasca(yle ly) (6.15) so long as this function lies within the class we optimize over. In other words, if we could train on infinitely many samples from the true data generating distribution, minimizing the mean squared error cost function would give a function that predicts the mean of y for each value of x. Different cost functions give different statistics", "4338ea6e-4819-4584-bd49-5b9709e0e59e": "In many practical applications of pattern recognition, we will apply some form of \ufb01xed pre-processing, or feature extraction, to the original data variables. If the original variables comprise the vector x, then the features can be expressed in terms of the basis functions {\u03c6j(x)}. By using nonlinear basis functions, we allow the function y(x, w) to be a nonlinear function of the input vector x.\n\nFunctions of the form (3.2) are called linear models, however, because this function is linear in w. It is this linearity in the parameters that will greatly simplify the analysis of this class of models. However, it also leads to some signi\ufb01cant limitations, as we discuss in Section 3.6. The example of polynomial regression considered in Chapter 1 is a particular example of this model in which there is a single input variable x, and the basis functions take the form of powers of x so that \u03c6j(x) = xj. One limitation of polynomial basis functions is that they are global functions of the input variable, so that changes in one region of input space affect all other regions", "3d807191-677c-45ad-b1be-6f18a2e182d6": "Note that at this stage, we do not need to specify anything further about these variables, such as whether they are discrete or continuous. Indeed, one of the powerful aspects of graphical models is that a speci\ufb01c graph can make probabilistic statements for a broad class of distributions. By application of the product rule of probability (1.11), we can write the joint distribution in the form A second application of the product rule, this time to the second term on the righthand side of (8.1), gives Note that this decomposition holds for any choice of the joint distribution.\n\nWe now represent the right-hand side of (8.2) in terms of a simple graphical model as follows. First we introduce a node for each of the random variables a, b, and c and associate each node with the corresponding conditional distribution on the right-hand side of (8.2). Then, for each conditional distribution we add directed links (arrows) to the graph from the nodes corresponding to the variables on which the distribution is conditioned. Thus for the factor p(c|a, b), there will be links from nodes a and b to node c, whereas for the factor p(a) there will be no incoming links", "9c7cf33f-82b7-48de-b55d-4d5092ef7fbe": "When the temperature falls to zero, and ( rises to infinity, the energy-based model becomes  i  5  Ripa aE pe Wis] Py |X  eo  QO | H)-9 /-9 J col tn) 9} \u2014 sq) o~/O} eX} o /S  hW IOs 0 fs Poo ca i + og) m ivi) ilu) ) |)  OO ee eed bend tad fe od od bed ed A fame me pe fee ~/s|=)-/a]y]al+s A>] RAR RS SG rag ry rae ee ON Ee EONS See wl OH 2) we PLOIN  Ni +)~ 4) NS, | oe]  Figure 17.2: An illustration of the slow mixing problem in deep probabilistic models. Each panel should be read left to right, top to bottom.\n\n(Left)Consecutive samples from Gibbs sampling applied to a deep Boltzmann machine trained on the MNIST dataset. Consecutive samples are similar to each other", "2295138e-427e-40b0-b5cd-91b7cdd88ce2": "Ng  and Ng, Harada, and Russell  used the term shaping in a sense somewhat di\u21b5erent from Skinner\u2019s, focusing on the problem of how to alter the reward signal without altering the set of optimal policies. Dickinson and Balleine  discuss the complexity of the interaction between learning and motivation. Wise  provides an overview of reinforcement learning and its relation to motivation. Daw and Shohamy  link motivation and learning to aspects of reinforcement learning theory. See also McClure, Daw, and Montague , Niv, Joel, and Dayan , Rangel, Camerer, and Montague , and Dayan and Berridge . McClure et al. , Niv, Daw, and Dayan , and Niv, Daw, Joel, and Dayan  present theories of behavioral vigor related to the reinforcement learning framework. 14.4 Spence, Hull\u2019s student and collaborator at Yale, elaborated the role of higherorder reinforcement in addressing the problem of delayed reinforcement . Learning over very long delays, as in taste-aversion conditioning with delays up to several hours, led to interference theories as alternatives to decayingtrace theories", "b4bd6036-1ca4-4e2f-b0d4-5b72db72c1e9": "Posterior probabilities allow us to determine a rejection criterion that will minimize the misclassi\ufb01cation rate, or more generally the expected loss, for a given fraction of rejected data points. Compensating for class priors. Consider our medical X-ray problem again, and suppose that we have collected a large number of X-ray images from the general population for use as training data in order to build an automated screening system.\n\nBecause cancer is rare amongst the general population, we might \ufb01nd that, say, only 1 in every 1,000 examples corresponds to the presence of cancer. If we used such a data set to train an adaptive model, we could run into severe dif\ufb01culties due to the small proportion of the cancer class. For instance, a classi\ufb01er that assigned every point to the normal class would already achieve 99.9% accuracy and it would be dif\ufb01cult to avoid this trivial solution. Also, even a large data set will contain very few examples of X-ray images corresponding to cancer, and so the learning algorithm will not be exposed to a broad range of examples of such images and hence is not likely to generalize well. A balanced data set in which we have selected equal numbers of examples from each of the classes would allow us to \ufb01nd a more accurate model", "e4820c30-80e5-47c7-b4cf-ae7c8b9ef9f6": "The variational autoencoder 1s defined for arbitrary computational graphs, which makes it applicable to a wider  range of probabilistic model families because there is no need to restrict the choice of models to those with tractable mean field fixed-point equations. The variational autoencoder also has the advantage of increasing a bound on the log-likelihood of the model, while the criteria for the MP-DBM and related models are more heuristic and have little probabilistic interpretation beyond making the results of approximate inference accurate. One disadvantage of the variational autoencoder is that it learns an inference network for only one problem, inferring z given x. The older methods are able to perform approximate inference over any subset of variables given any other subset of variables, because the mean field fixed-point equations specify how to share parameters between the computational graphs for all these different problems. One very nice property of the variational autoencoder is that simultaneously training a parametric encoder in combination with the generator network forces the model to learn a predictable coordinate system that the encoder can capture. This makes it an excellent manifold learning algorithm.\n\nSee figure 20.6 for examples of low-dimensional manifolds learned by the variational autoencoder", "f1dcb2d5-c728-4c2b-80a1-62ab2d683ef5": "Note that the EM algorithm takes many more iterations to reach (approximate) convergence compared with the K-means algorithm, and that each cycle requires signi\ufb01cantly more computation. It is therefore common to run the K-means algorithm in order to \ufb01nd a suitable initialization for a Gaussian mixture model that is subsequently adapted using EM.\n\nThe covariance matrices can conveniently be initialized to the sample covariances of the clusters found by the K-means algorithm, and the mixing coef\ufb01cients can be set to the fractions of data points assigned to the respective clusters. As with gradient-based approaches for maximizing the log likelihood, techniques must be employed to avoid singularities of the likelihood function in which a Gaussian component collapses onto a particular data point. It should be emphasized that there will generally be multiple local maxima of the log likelihood function, and that EM is not guaranteed to \ufb01nd the largest of these maxima. Because the EM algorithm for Gaussian mixtures plays such an important role, we summarize it below. Given a Gaussian mixture model, the goal is to maximize the likelihood function with respect to the parameters (comprising the means and covariances of the components and the mixing coef\ufb01cients). 1", "fbac91d8-2221-4111-9745-e2a63c934cdc": "Reinforcement learning can be used at both high and low levels in a system.\n\nAlthough the tic-tac-toe player learned only about the basic moves of the game, nothing prevents reinforcement learning from working at higher levels where each of the \u201cactions\u201d may itself be the application of a possibly elaborate problem-solving method. In hierarchical learning systems, reinforcement learning can work simultaneously on several levels. the same because of symmetries. How might we amend the learning process described above to take advantage of this? In what ways would this change improve the learning process? Now think again. Suppose the opponent did not take advantage of symmetries. In that case, should we? Is it true, then, that symmetrically equivalent positions should necessarily have the same value? \u21e4 it always played the move that brought it to the position that it rated the best. Might it learn to play better, or worse, than a nongreedy player? What problems might occur? \u21e4 moves, including exploratory moves. If the step-size parameter is appropriately reduced over time (but not the tendency to explore), then the state values would converge to a di\u21b5erent set of probabilities", "19481b53-f638-439e-8568-d3b446791501": "The subgraph B contains exactly one edge for each edge from node uY) to node u\u201c of G. The edge from u) to u is associated with the computation of our In addition, a dot product is performed for each node, between the gradient already computed with respect to nodes wu) that are children du for the same children dud) nodes u@).\n\nTo summarize, the amount of computation required for performing the back-propagation scales linearly with the number of edges in G, where the computation for each edge corresponds to computing a partial derivative (of one node with respect to one of its parents) as well as performing one multiplication and one addition", "f4e1182b-9b8c-468c-a648-d95711a7e49e": "The MC algorithm is an o\u270fine algorithm and we do not seek to improve this aspect of it. Rather we seek merely an implementation of this algorithm with computational advantages. We will still update the weight vector only at the end of the episode, but we will do some computation during each step of the episode and less at its end. This will give a more equal distribution of computation\u2014O(d) per step\u2014and also remove the need to store the feature vectors at each step for use later at the end of each episode. Instead, we will introduce an additional vector memory, the eligibility trace, keeping in it a summary of all the feature vectors seen so far. This will be su\ufb03cient to e\ufb03ciently recreate exactly the same overall update as the sequence of MC t is a forgetting, or fading, matrix.\n\nNow, recursing, = FT \u22121FT \u22122 (FT \u22123wT \u22123 + \u21b5GxT \u22123) + \u21b5G (FT \u22121xT \u22122 + xT \u22121) where aT \u22121 and zT \u22121 are the values at time T \u2212 1 of two auxiliary memory vectors that can be updated incrementally without knowledge of G and with O(d) complexity per time step", "e7265477-9e9d-483b-9c7a-892c82db8099": "Even if we could perform the value iteration update on a million states per second, it would take over a thousand years to complete a single sweep. Asynchronous DP algorithms are in-place iterative DP algorithms that are not organized in terms of systematic sweeps of the state set. These algorithms update the values of states in any order whatsoever, using whatever values of other states happen to be available. The values of some states may be updated several times before the values of others are updated once.\n\nTo converge correctly, however, an asynchronous algorithm must continue to update the values of all the states: it can\u2019t ignore any state after some point in the computation. Asynchronous DP algorithms allow great \ufb02exibility in selecting states to update. For example, one version of asynchronous value iteration updates the value, in place, of only one state, sk, on each step, k, using the value iteration update (4.10). If 0 \uf8ff \u03b3 < 1, asymptotic convergence to v\u21e4 is guaranteed given only that all states occur in the sequence {sk} an in\ufb01nite number of times (the sequence could even be stochastic)", "fe73a7f1-0d0a-4aab-a10e-74acfe243da4": "The Least Recently Used Access (LRUA) writer is designed for  https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Log   MANN to better work in the scenario of meta-learning. A LRUA write head prefers to write new content to either the /Jeast used memory location or the most recently used memory location. Rarely used locations: so that we can preserve frequently used information (see LFU);  The last used location: the motivation is that once a piece of information is retrieved once, it probably won't be called again for a while (see MRU). There are many cache replacement algorithms and each of them could potentially replace the design here with better performance in different use cases. Furthermore, it would be a good idea to learn the memory usage pattern and addressing strategies rather than arbitrarily set it. The preference of LRUA is carried out in a way that everything is differentiable:  The usage weight w;' at time t is a sum of current read and write vectors, in addition to the decayed last usage weight, yw;'_,, where \u00a5 is a decay factor", "fceb6df1-42b2-4a9f-bd53-5fa82f3b0ab4": "The typical remedy is to add a noise term to the model distribution. This is why virtually all generative models described in the classical machine learning literature include a noise component. In the simplest case, one assumes a Gaussian noise with relatively high bandwidth in order to cover all the examples.\n\nIt is well known, for instance, that in the case of image generation models, this noise degrades the quality of the samples and makes them blurry. For example, we can see in the recent paper  that the optimal standard deviation of the noise added to the model when maximizing likelihood is around 0.1 to each pixel in a generated image, when the pixels were already normalized to be in the range . This is a very high amount of noise, so much that when papers report the samples of their models, they don\u2019t add the noise term on which they report likelihood numbers. In other words, the added noise term is clearly incorrect for the problem, but is needed to make the maximum likelihood approach work", "5c1ed269-f53d-4b49-b78e-7212fc1de613": "Actually, the notion of \u201cthe on-policy distribution\u201d is not quite right, as there are many on-policy distributions, and any one of these is su\ufb03cient to guarantee stability. Consider an undiscounted episodic problem. The way episodes terminate is fully determined by the transition probabilities, but there may be several di\u21b5erent ways the episodes might begin. However the episodes start, if all state transitions are due to the target policy, then the state distribution that results is an on-policy distribution. You might start close to the terminal state and visit only a few states with high probability before ending the episode. Or you might start far away and pass through many states before terminating. Both are on-policy distributions, and training on both with a linear semi-gradient method would be guaranteed to be stable. However the process starts, an on-policy distribution results as long as all states encountered are updated up until termination. If there is discounting, it can be treated as partial or probabilistic termination for these purposes", "94c2cd12-4ff5-43e0-bff9-739ebc946748": ", uses a neural network to find the smallest possible noise perturbation that causes misclassifications. Another interesting framework that could be used in an adversarial training context is to have an adversary change the labels of training data. Xie et al. presented Distur- bLabel, a regularization technique that randomly replaces labels at each iteration. This is a rare example of adding noise to the loss layer, whereas most of the other augmenta- tion methods discussed add noise into the input or hidden representation layers. On the MNIST dataset with LeNet  CNN architecture, DisturbLabel produced a 0.32% error rate compared to a baseline error rate of 0.39%. DisturbLabel combined with Dropout Regularization produced a 0.28% error rate compared to the 0.39% baseline. To translate this to the context of adversarial training, one network takes in the classifier\u2019s training data as input and learns which labels to flip to maximize the error rate of the classifica- tion network. The effectiveness of adversarial training in the form of noise or augmentation search is still a relatively new concept that has not been widely tested and understood", "66f3c9a1-c1cf-4a71-a832-45d60388f56e": "Chapter 14  Autoencoders  An autoencoder is a neural network that is trained to attempt to copy its input to its output. Internally, it has a hidden layer h that describes a code used to represent the input. The network may be viewed as consisting of two parts: an encoder function h = f(a) and a decoder that produces a reconstruction r = g(h). This architecture is presented in figure 14.1. If an autoencoder succeeds in simply learning to set g(f(a)) = a everywhere, then it is not especially useful. Instead, autoencoders are designed to be unable to learn to copy perfectly. Usually they are restricted in ways that allow them to copy only approximately, and to copy only input that resembles the training data. Because the model is forced to prioritize which aspects of the input should be copied, it often learns useful properties of the data. Modern autoencoders have generalized the idea of an encoder and a de- coder beyond deterministic functions to stochastic mappings Pencoder(h | #) and Pdecoder (& | h)", "5be4fa40-3b11-443d-9485-300e098d97bf": "In many ways, both AlphaGo and AlphaGo Zero are descendants of Tesauro\u2019s TDGammon (Section 16.1), itself a descendant of Samuel\u2019s checkers player (Section 16.2). All these programs included reinforcement learning over simulated games of self-play. AlphaGo and AlphaGo Zero also built upon the progress made by DeepMind on playing Atari games with the program DQN (Section 16.5) that used deep convolutional ANNs to approximate optimal value functions.\n\nnately place black and white \u2018stones\u2019 on unoccupied intersections, or \u2018points,\u2019 on a board with a grid of 19 horizontal and 19 vertical lines to produce positions like that shown to the right. The game\u2019s goal is to capture an area of the board larger than that captured by the opponent. Stones are captured according to simple rules. A player\u2019s stones are captured if they are completely surrounded by the other player\u2019s stones, meaning that there is no horizontally or vertically adjacent point that is unoccupied. For example, Figure 16.5 shows on the left three white stones with an unoccupied adjacent point (labeled X)", "727bbf78-82e4-477d-b9df-b3dd9b16947a": "In fact, there are many solutions, as there are more components to the weight vector (8) than there are nonterminal states (7). Moreover, the set of feature vectors, {x(s) : s 2 S}, is a linearly independent set. In all these ways this task seems a favorable case for linear function approximation. If we apply semi-gradient TD(0) to this problem (11.2), then the weights diverge to in\ufb01nity, as shown in Figure 11.2 (left). The instability occurs for any positive step size, no matter how small.\n\nIn fact, it even occurs if an expected update is done as in dynamic programming (DP), as shown in Figure 11.2 (right). That is, if the weight vector, wk, is updated for all states at the same time in a semi-gradient way, using the DP (expectation-based) target: In this case, there is no randomness and no asynchrony, just as in a classical DP update. The method is conventional except in its use of semi-gradient function approximation. Yet still the system is unstable", "051daeca-6f51-49e6-845d-a3d250931879": "We do not compare with previous specialized adversarial text attack methods, because they either are not applicable to the challenging universal attack setting , or were not designed to generate human-readable sentences .\n\nWe use similar settings as in \u00a74.1 to explore the diversity-quality trade-off by plotting the entailment rate and perplexity against diversity, respectively. The entailment classi\ufb01er to be attacked is used as entailment score reward functions. We also include a tokenlevel repetition penalty reward for readability. Results. Figure 3 (right) shows the results, and Table A.2 shows samples. We can see that SQL outperforms MLE+PG consistently across different diversity values. The outputs from MLE+PG are not diverse even with high p\u2019s, indicating the model collapses and can only generate a small set of unique adversarial examples. The model by SQL discovers the pattern \u201csaint-pierre-et-saint-paul\u201d (an entity name), and exploits this to generate samples with high universal entailment rate. A reward function does not just have to be a metric like the BLEU score, but also a complicated pipeline that eventually returns a score", "fec80b52-c0dd-415b-a709-f11c9ef1b1ee": "One obvious problem is that the estimated density has discontinuities that are due to the bin edges rather than any property of the underlying distribution that generated the data. Another major limitation of the histogram approach is its scaling with dimensionality. If we divide each variable in a D-dimensional space into M bins, then the total number of bins will be M D. This exponential scaling with D is an example of the curse of dimensionality. In a space of high dimensionalSection 1.4 ity, the quantity of data needed to provide meaningful estimates of local probability density would be prohibitive. The histogram approach to density estimation does, however, teach us two important lessons. First, to estimate the probability density at a particular location, we should consider the data points that lie within some local neighbourhood of that point. Note that the concept of locality requires that we assume some form of distance measure, and here we have been assuming Euclidean distance.\n\nFor histograms, this neighbourhood property was de\ufb01ned by the bins, and there is a natural \u2018smoothing\u2019 parameter describing the spatial extent of the local region, in this case the bin width. Second, the value of the smoothing parameter should be neither too large nor too small in order to obtain good results", "b6e9fcea-ac13-42e8-ae2d-bda2c9b97ec6": "To the best of our knowledge, Q-learning has never been found to diverge in this case, but there has been no theoretical analysis. In the rest of this section we present several other ideas that have been explored. Suppose that instead of taking just a step toward the expected one-step return on each iteration, as in Baird\u2019s counterexample, we actually change the value function all the way to the best, least-squares approximation. Would this solve the instability problem? Of course it would if the feature vectors, {x(s) : s 2 S}, formed a linearly independent set, as they do in Baird\u2019s counterexample, because then exact approximation is possible on each iteration and the method reduces to standard tabular DP. But of course the point here is to consider the case when an exact solution is not possible. In this case stability is not guaranteed even when forming the best approximation at each iteration, as shown in the example. Example 11.1: Tsitsiklis and Van Roy\u2019s Counterexample This example shows that linear function approximation would not work with DP even if the least-squares solution was found at each step", "0c917ca0-6623-4efe-b778-a205f12e09ff": "Note, however, that if a digit \u20182\u2019 is written in the reverse order, that is, starting at the bottom right and ending at the top left, then even though the pen tip coordinates may be identical to an example from the training set, the probability of the observations under the model will be extremely small. In the speech recognition context, warping of the time axis is associated with natural variations in the speed of speech, and again the hidden Markov model can accommodate such a distortion and not penalize it too heavily. If we have observed a data set X = {x1, . , xN}, we can determine the parameters of an HMM using maximum likelihood. The likelihood function is obtained from the joint distribution (13.10) by marginalizing over the latent variables Because the joint distribution p(X, Z|\u03b8) does not factorize over n (in contrast to the mixture distribution considered in Chapter 9), we cannot simply treat each of the summations over zn independently. Nor can we perform the summations explicitly because there are N variables to be summed over, each of which has K states, resulting in a total of KN terms", "3deec0a3-02f0-48ef-bd32-4ea737ecc91b": "For known mean and unknown precision matrix \u039b, the conjugate prior is the Wishart distribution given by Exercise 2.45 where \u03bd is called the number of degrees of freedom of the distribution, W is a D\u00d7D scale matrix, and Tr(\u00b7) denotes the trace. The normalization constant B is given by Again, it is also possible to de\ufb01ne a conjugate prior over the covariance matrix itself, rather than over the precision matrix, which leads to the inverse Wishart distribution, although we shall not discuss this further. If both the mean and the precision are unknown, then, following a similar line of reasoning to the univariate case, the conjugate prior is given by which is known as the normal-Wishart or Gaussian-Wishart distribution. We have seen that the conjugate prior for the precision of a Gaussian is given by a gamma distribution. If we have a univariate Gaussian N(x|\u00b5, \u03c4 \u22121) together Section 2.3.6 with a Gamma prior Gam(\u03c4|a, b) and we integrate out the precision, we obtain the marginal distribution of x in the form Exercise 2.46 where we have made the change of variable z = \u03c4", "bd6c66e0-069c-4c30-9248-4d06c4402d70": "A logical calculus of the ideas immanent in nervous activity. McMahan, H. B., Gordon, G. J. Fast Exact Planning in Markov Decision Processes. Melo, F. S., Meyn, S. P., Ribeiro, M. I. An analysis of reinforcement learning with function approximation. In Proceedings of the 25th International Conference on Machine Learning , pp. 664\u2013671. Mendel, J. M. A survey of learning control systems.\n\nISA Transactions, 5:297\u2013303. Mendel, J. M., McLaren, R. W. Reinforcement learning control and pattern recognition systems. In J. M. Mendel and K. S. Fu (Eds. ), Adaptive, Learning and Pattern Recognition Systems: Theory and Applications, pp. 287\u2013318. Academic Press, New York. Michie, D. Trial and error", "13a5af9b-c5d0-40af-b341-c935379a3d22": "Once we know the posterior distribution over models, the predictive distribution is given, from the sum and product rules, by This is an example of a mixture distribution in which the overall predictive distribution is obtained by averaging the predictive distributions p(t|x, Mi, D) of individual models, weighted by the posterior probabilities p(Mi|D) of those models.\n\nFor instance, if we have two models that are a-posteriori equally likely and one predicts a narrow distribution around t = a while the other predicts a narrow distribution around t = b, the overall predictive distribution will be a bimodal distribution with modes at t = a and t = b, not a single model at t = (a + b)/2. A simple approximation to model averaging is to use the single most probable model alone to make predictions. This is known as model selection. For a model governed by a set of parameters w, the model evidence is given, from the sum and product rules of probability, by From a sampling perspective, the marginal likelihood can be viewed as the probaChapter 11 bility of generating the data set D from a model whose parameters are sampled at random from the prior", "bd60a6be-361c-4cc9-b03d-7f976c2df8ab": "Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. 2019. Plug and play language models: A simple approach to controlled text generation. In International Conference on Learning Representations.\n\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. 2020. Plug and play language models: A simple approach to controlled text generation. In International Conference on Learning Representations. Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P Xing, and Zhiting Hu. 2022. RLPrompt: Optimizing discrete text prompts with reinforcement learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Benjamin Eysenbach and Sergey Levine. 2021", "904b2456-4be6-4545-a018-4b3840c83d4b": "The search tree at the start of this next execution might be just this new root node, or it might include descendants of this node left over from MCTS\u2019s previous execution. The remainder of the tree is discarded.\n\nThe main innovation that made AlphaGo such a strong player is that it selected moves by a novel version of MCTS that was guided by both a policy and a value function learned by reinforcement learning with function approximation provided by deep convolutional ANNs. Another key feature is that instead of reinforcement learning starting from random network weights, it started from weights that were the result of previous supervised learning from a large collection of human expert moves. The DeepMind team called AlphaGo\u2019s modi\ufb01cation of basic MCTS \u201casynchronous policy and value MCTS,\u201d or APV-MCTS. It selected actions via basic MCTS as described above but with some twists in how it extended its search tree and how it evaluated action edges", "91c05363-68fc-4790-8825-189a3adab54d": "Suppose that instead of setting the initial action values to zero, as we did in the 10-armed testbed, we set them all to +5. Recall that the q\u21e4(a) in this problem are selected from a normal distribution with mean 0 and variance 1. An initial estimate of +5 is thus wildly optimistic. But this optimism encourages action-value methods to explore. Whichever actions are initially selected, the reward is less than the starting estimates; the learner switches to other actions, being \u201cdisappointed\u201d with the rewards it is receiving. The result is that all actions are tried several times before the value estimates converge. The system does a fair amount of exploration even if greedy actions are selected all the time. using Q1(a) = +5, for all a. For comparison, also shown is an \"-greedy method with Q1(a) = 0.\n\nInitially, the optimistic method performs worse because it explores more, but eventually it performs better because its exploration decreases with time. We call this technique for encouraging exploration optimistic initial values. We regard it as a simple trick that can be quite e\u21b5ective on stationary problems, but it is far from being a generally useful approach to encouraging exploration", "fea6004b-2fb6-4447-9635-50ded6b9def7": "The objective function is MSE loss instead of cross-entropy, because conceptually RN focuses more on predicting relation scores which is more like regression, rather than binary  classification, \u00a3(B) = > -1  2 (xix ;yiy,)eB \"is vi=yi) :  embedding module relation module 1 fF  Feature maps concatenation  | L | Relation One-hot |, \\| score vector  (Note: There is another Relation Network for relational reasoning, proposed by DeepMind. Don't get  https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Log   confused.) Prototypical Networks  Prototypical Networks  use an embedding function fg to encode each input into a M-dimensional feature vector. A prototype feature vector is defined for every class c \u20ac C, as the mean vector of the embedded support data samples in this class. Ve = : S fo(x;)  |S", "a7b842ba-77f3-492a-8685-66b7a2fe83c6": "The Journal of Neuroscience, 29(43):13524\u201313531. Ghiassian, S., Ra\ufb01ee, B., Sutton, R. S. A \ufb01rst empirical study of emphatic temporal di\u21b5erence learning. Workshop on Continual Learning and Deep Learning at the Conference on Neural Information Processing Systems . ArXiv:1705.04185. Ghiassian, S., Patterson, A., White, M., Sutton, R. S., White, A. Online O\u21b5-policy Gibbs, C. M., Cool, V., Land, T., Kehoe, E. J., Gormezano, I. Second-order conditioning of the rabbit\u2019s nictitating membrane response.\n\nIntegrative Physiological and Behavioral Science, 26(4):282\u2013295. Gittins, J. C., Jones, D. M. A dynamic allocation index for the sequential design of experiments. In J. Gani, K. Sarkadi, and I", "a199b936-58ed-4ec1-8338-85c6f60fb339": "What (conceptually) are the two sets of probabilities computed when we do, and when we do not, learn from exploratory moves? Assuming that we do continue to make exploratory moves, which set of probabilities might be better to learn? Which would result in more wins?\n\n\u21e4 Reinforcement learning is a computational approach to understanding and automating goal-directed learning and decision making. It is distinguished from other computational approaches by its emphasis on learning by an agent from direct interaction with its environment, without requiring exemplary supervision or complete models of the environment. In our opinion, reinforcement learning is the \ufb01rst \ufb01eld to seriously address the computational issues that arise when learning from interaction with an environment in order to achieve long-term goals. Reinforcement learning uses the formal framework of Markov decision processes to de\ufb01ne the interaction between a learning agent and its environment in terms of states, actions, and rewards. This framework is intended to be a simple way of representing essential features of the arti\ufb01cial intelligence problem. These features include a sense of cause and e\u21b5ect, a sense of uncertainty and nondeterminism, and the existence of explicit goals. The concepts of value and value function are key to most of the reinforcement learning are important for e\ufb03cient search in the space of policies", "75ea2062-5afa-48f9-9183-f2f403b7256d": "9.27 (\u22c6 \u22c6) Derive M-step formulae for updating the covariance matrices and mixing coef\ufb01cients in a Gaussian mixture model when the responsibilities are updated incrementally, analogous to the result (9.78) for updating the means. A central task in the application of probabilistic models is the evaluation of the posterior distribution p(Z|X) of the latent variables Z given the observed (visible) data variables X, and the evaluation of expectations computed with respect to this distribution. The model might also contain some deterministic parameters, which we will leave implicit for the moment, or it may be a fully Bayesian model in which any unknown parameters are given prior distributions and are absorbed into the set of latent variables denoted by the vector Z.\n\nFor instance, in the EM algorithm we need to evaluate the expectation of the complete-data log likelihood with respect to the posterior distribution of the latent variables. For many models of practical interest, it will be infeasible to evaluate the posterior distribution or indeed to compute expectations with respect to this distribution. This could be because the dimensionality of the latent space is too high to work with directly or because the posterior distribution has a highly complex form for which expectations are not analytically tractable", "17585670-e624-422f-ad19-520e1a966152": "b Neural network training in AlphaGo Zero.\n\nThe neural network takes the raw board position s as its input, passes it through many convolutional layers with parameters \u03b8, and outputs both a vector p, representing a probability distribution over moves, and a scalar value v, representing the probability of the current player winning in position s. The neural network is trained on randomly sampled steps from recent games of self-play, (s,\u03c0\u03c0\u03c0, z). The parameters \u03b8 are updated so as to maximise the similarity of the policy vector p to the search probabilities \u03c0\u03c0\u03c0, and to minimise the error between the predicted winner v and the game winner z (see Equation 1). each time it executed. The result was that the policy actually followed by AlphaGo Zero was an improvement over the policy given by the network\u2019s outputs p. Silver et al. wrote that \u201cMCTS may therefore be viewed as a powerful policy improvement operator.\u201d Here is more detail about AlphaGo Zero\u2019s ANN and how it was trained. The network took as input a 19 \u21e5 19 \u21e5 17 image stack consisting of 17 binary feature planes", "069eeab8-c4c5-45d7-a821-0d7c83256661": "Note that positive de\ufb01nite is not the same as all the elements being positive. For example, the matrix \ufffd 1 2 3 4 has eigenvalues \u03bb1 \u2243 5.37 and \u03bb2 \u2243 \u22120.37. A matrix is said to be positive semidefinite if wTAw \u2a7e 0 holds for all values of w, which is denoted A \u2ab0 0, and is equivalent to \u03bbi \u2a7e 0.\n\nWe can think of a function y(x) as being an operator that, for any input value x, returns an output value y. In the same way, we can de\ufb01ne a functional F to be an operator that takes a function y(x) and returns an output value F. An example of a functional is the length of a curve drawn in a two-dimensional plane in which the path of the curve is de\ufb01ned in terms of a function. In the context of machine learning, a widely used functional is the entropy H for a continuous variable x because, for any choice of probability density function p(x), it returns a scalar value representing the entropy of x under that density. Thus the entropy of p(x) could equally well have been written as H", "01a5f4eb-ac9f-4ef3-803a-fbc8867d3469": "Of course, when we use a machine learning algorithm, we do not fix the parameters ahead of time, then sample both datasets. We sample the training set, then use it to choose the parameters to reduce training set error, then sample the test set. Under this process, the expected test error is greater than or equal to the expected value of training error. The factors determining how well a machine learning algorithm will perform are its ability to  1. Make the training error small. 2. Make the gap between training and test error small. https://www.deeplearningbook.org/contents/ml.html    These two factors correspond to the two central challenges in machine learning: underfitting and overfitting. Underfitting occurs when the model is not able to  109  CHAPTER 5.\n\nMACHINE LEARNING BASICS  obtain a sufficiently low error value on the training set. Overfitting occurs when the gap between the training error and test error is too large. We can control whether a model is more likely to overfit or underfit by altering its capacity. Informally, a model\u2019s capacity is its ability to fit a wide variety of functions. Models with low capacity may struggle to fit the training set", "254081b5-a80b-4193-bc1b-8f7c6ecffa39": "Early work extending Dyna to linear function approximation (Section 9.4) was done by Sutton, Szepesv\u00b4ari, Geramifard, and Bowling  and by Parr, Li, Taylor, Painter-Wake\ufb01eld, and Littman . 8.3 There have been several works with model-based reinforcement learning that take the idea of exploration bonuses and optimistic initialization to its logical extreme, in which all incompletely explored choices are assumed maximally rewarding and optimal paths are computed to test them. The E3 algorithm of Kearns and Singh  and the R-max algorithm of Brafman and Tennenholtz  are guaranteed to \ufb01nd a near-optimal solution in time polynomial in the number of states and actions. This is usually too slow for practical algorithms but is probably the best that can be done in the worst case. 8.4 Prioritized sweeping was developed simultaneously and independently by Moore and Atkeson  and Peng and Williams . The results in the box on page 170 are due to Peng and Williams .\n\nThe results in the box on page 171 are due to Moore and Atkeson. Key subsequent work in this area includes that by McMahan and Gordon  and by van Seijen and Sutton", "9a87bd6b-142d-4d16-95d3-104b25a65b3c": "Suppose we use a variational distribution that factorizes between latent variables and parameters so that q(z, \u03b8) = qz(z)q\u03b8(\u03b8), in which the distribution q\u03b8(\u03b8) is approximated by a point estimate of the form q\u03b8(\u03b8) = \u03b4(\u03b8 \u2212 \u03b80) where \u03b80 is a vector of free parameters.\n\nShow that variational optimization of this factorized distribution is equivalent to an EM algorithm, in which the E step optimizes qz(z), and the M step maximizes the expected complete-data log posterior distribution of \u03b8 with respect to \u03b80. 10.6 (\u22c6 \u22c6) The alpha family of divergences is de\ufb01ned by (10.19). Show that the KullbackLeibler divergence KL(p\u2225q) corresponds to \u03b1 \u2192 1. This can be done by writing p\u03f5 = exp(\u03f5 ln p) = 1 + \u03f5 ln p + O(\u03f52) and then taking \u03f5 \u2192 0. Similarly show that KL(q\u2225p) corresponds to \u03b1 \u2192 \u22121", "fc3d66a7-c8a6-4932-814f-de1963a0e9ef": "We now consider a periodic generalization of the Gaussian called the von Mises distribution. Here we shall limit our attention to univariate distributions, although periodic distributions can also be found over hyperspheres of arbitrary dimension. For an extensive discussion of periodic distributions, see Mardia and Jupp . By convention, we will consider distributions p(\u03b8) that have period 2\u03c0. Any probability density p(\u03b8) de\ufb01ned over \u03b8 must not only be nonnegative and integrate to one, but it must also be periodic. Thus p(\u03b8) must satisfy the three conditions From (2.172), it follows that p(\u03b8 + M2\u03c0) = p(\u03b8) for any integer M. We can easily obtain a Gaussian-like distribution that satis\ufb01es these three properties as follows. Consider a Gaussian distribution over two variables x = (x1, x2) having mean \u00b5 = (\u00b51, \u00b52) and a covariance matrix \u03a3 = \u03c32I where I is the 2 \u00d7 2 identity matrix, so that The contours of constant p(x) are circles, as illustrated in Figure 2.18. Now suppose we consider the value of this distribution along a circle of \ufb01xed radius", "78434c44-787e-4559-beb9-44afda76557e": "Annealed importance sampling was first discovered by Jarzynski  and then again, independently, by Neal . It is currently the most common way of estimating the partition function for undirected probabilistic models.\n\nThe reasons for this may have more to do with the publication of an influential paper  describing its application to estimating the partition function of restricted Boltzmann machines and deep belief networks than  626  CHAPTER 18. CONFRONTING THE PARTITION FUNCTION  https://www.deeplearningbook.org/contents/partition.html    with any inherent advantage the method has over the other method described elow. A discussion of the properties of the AIS estimator (e.g., its variance and efficiency) can be found in Neal . 18.7.2 Bridge Sampling  Bridge sampling  is another method that, like AIS, addresses the shortcomings of importance sampling. Rather than chaining together a series of intermediate distributions, bridge sampling relies on a single distribution p,, known as the bridge, to interpolate between a distribution with known partition function, po, and a distribution p, for which we are trying to estimate the partition function Z\\", "63c332f7-5604-4561-8c70-730f75e5ee10": "DEEP GENERATIVE MODELS  to update all the even layers simultaneously and then to update all the odd layers simultaneously, following the same schedule as Gibbs sampling. Now that we have specified our family of approximating distributions Q, it remains to specify a procedure for choosing the member of this family that best fits P. The most straightforward way to do this is to use the mean field equations specified by equation 19.56. These equations were derived by solving for where the derivatives of the variational lower bound are zero. They describe in an abstract manner how to optimize the variational lower bound for any model, simply by taking expectations with respect to Q. Applying these general equations, we obtain the update rules (again, ignoring bias terms):  A =o (Same +> wii?) , Vi, (20.33) k! a  AQ) =o 2 Wah , Vk. (20.34)  At a fixed point of this system of equations, we have a local maximum of the variational lower bound \u00a3(Q)", "8776653f-a793-4a0e-adf0-69e1f7a1cb6b": "Technical report, arXiv:1207.0580. Jarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y. What is the best multi-stage architecture for object recognition? In Proc. International Conference on Computer Vision (ICCV\u201909), pages 2146\u20132153. IEEE. Kingma, D. P. and Welling, M. Auto-encoding variational bayes. In Proceedings of the International Conference on Learning Representations (ICLR). Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. Technical report, University of Toronto. Krizhevsky, A., Sutskever, I., and Hinton, G. ImageNet classi\ufb01cation with deep convolutional neural networks. In NIPS\u20192012. LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition", "3aaa157f-88a0-4718-8d66-33f3ea48ea98": "Provided the marginals only involves a small number of variables, the evaluation of their normalization coef\ufb01cient will be feasible. So far, we have discussed the notion of conditional independence based on simple graph separation and we have proposed a factorization of the joint distribution that is intended to correspond to this conditional independence structure. However, we have not made any formal connection between conditional independence and factorization for undirected graphs.\n\nTo do so we need to restrict attention to potential functions \u03c8C(xC) that are strictly positive (i.e., never zero or negative for any choice of xC). Given this restriction, we can make a precise relationship between factorization and conditional independence. To do this we again return to the concept of a graphical model as a \ufb01lter, corresponding to Figure 8.25. Consider the set of all possible distributions de\ufb01ned over a \ufb01xed set of variables corresponding to the nodes of a particular undirected graph. We can de\ufb01ne UI to be the set of such distributions that are consistent with the set of conditional independence statements that can be read from the graph using graph separation", "51c2ee7a-cc60-4152-bd69-30823bd1710c": "In asynchronous DP methods, the evaluation and improvement processes are interleaved at an even \ufb01ner grain. In some cases a single state is updated in one process before returning to the other. As long as both processes continue to update all states, the ultimate result is typically the same\u2014convergence to the optimal value function and an optimal policy. We use the term generalized policy iteration (GPI) to refer to the general idea of letting policy-evaluation and policyimprovement processes interact, independent of the granularity and other details of the two processes. Almost all reinforcement learning methods are well described as GPI. That is, all have identi\ufb01able policies and value functions, with the policy always being improved with respect to the value function and the value function always being driven toward the value function for the policy, as suggested by the diagram to the right. If both the evaluation process and the improvement process stabilize, that is, no longer produce changes, then the value function and policy must be optimal. The value function stabilizes only when it is consistent with the current policy, and the policy stabilizes only when it is greedy with respect to the current value function.\n\nThus, both processes stabilize only when a policy has been found that is greedy with respect to its own evaluation function", "80fc9da5-4e88-4e68-8a44-eff0611bc9d0": "If the pattern is correctly classi\ufb01ed, then the weight vector remains unchanged, whereas if it is incorrectly classi\ufb01ed, then for class C1 we add the vector \u03c6(xn) onto the current estimate of weight vector w while for class C2 we subtract the vector \u03c6(xn) from w. The perceptron learning algorithm is illustrated in Figure 4.7. If we consider the effect of a single update in the perceptron learning algorithm, we see that the contribution to the error from a misclassi\ufb01ed pattern will be reduced because from (4.55) we have where we have set \u03b7 = 1, and made use of \u2225\u03c6ntn\u22252 > 0. Of course, this does not imply that the contribution to the error function from the other misclassi\ufb01ed patterns will have been reduced. Furthermore, the change in weight vector may have caused some previously correctly classi\ufb01ed patterns to become misclassi\ufb01ed.\n\nThus the perceptron learning rule is not guaranteed to reduce the total error function at each stage. However, the perceptron convergence theorem states that if there exists an exact solution (in other words, if the training data set is linearly separable), then the perceptron learning algorithm is guaranteed to \ufb01nd an exact solution in a \ufb01nite number of steps", "3ff8aeb8-8477-4362-9d2b-bbd47f8f2d7e": "2018. Adversarial example generation with syntactically controlled paraphrase networks. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1875\u20131885. Robin Jia and Percy Liang. 2016. Data recombination for neural semantic parsing. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12\u201322, Berlin, Germany. Association for Computational Linguistics. Robin Jia and Percy Liang. 2017. Adversarial examples for evaluating reading comprehension systems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2021\u20132031, Copenhagen, Denmark. Association for Computational Linguistics. Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo Zhao. 2019", "04b15fc0-410a-4a78-93cd-b4747dc67483": "These models include the mean and covariance RBM (mcRBM? '), the mean product of Student #distribution (mPoT) model, and the spike and slab RBM (ssRBM). Mean and Covariance RBM The mcRBM uses its hidden units to indepen- dently encode the conditional mean and covariance of all observed units. The mcRBM hidden layer is divided into two groups of units: mean units and covariance units.\n\nThe group that models the conditional mean is simply a Gaussian RBM. The other half is a covariance RBM , also called a cRBM, whose components model the conditional covariance structure, as described below. 'The term \u201cmcRBM\u201d is pronounced by saying the name of the letters M-C-R-B-M; the \u201cmc\u201d is not pronounced like the \u201cMc\u201d in \u201cMcDonald\u2019s.\u201d  675  https://www.deeplearningbook.org/contents/generative_models.html    CHAPTER 20", "6cb00e09-434d-4321-b5ca-ae956dd65519": "Training an EBM with a contrastive method consists in simultaneously pushing down on the energy of compatible (x,y) pairs from the training set, indicated by the blue dots, and pushing up on the energy of well chosen (x,y) pairs that are incompatible, symbolized by the  https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/  Self-supervised learning: The dark matter of intelligence   green dots.\n\nIn this simple example, x and y are both scalars, but in real situations, x and y could be an image or a video with millions of dimensions. Coming up with incompatible pairs that will shape the energy in suitable ways is challenging and expensive computationally. The method used to train NLP systems by masking or substituting some input words belongs to the category of contrastive methods. But they don\u2019t use the joint embedding architecture. Instead, they use a predictive architecture in which the model directly produces a prediction for y. One starts for a complete segment of text y, then corrupts it, e.g., by masking some words to produce the observation x", "17a9b17e-6a9d-4f03-bf1c-f742eada69e4": "Those fundamental challenges call for a standardized ML formalism that o\ufb00ers a principled framework for understanding, unifying, and generalizing current major paradigms of learning algorithms, and for mechanical design of new approaches for integrating any useful experience in learning.\n\nThe power of standardized theory is perhaps best demonstrated in physics, which has a long history of pursuing symmetry and simplicity of its principles: exempli\ufb01ed by the famed Maxwell\u2019s equations in the 1800s that reduced various principles of electricity and magnetism into a single electromagnetic theory, followed by General Relativity in the 1910s and the Standard Model in the 1970s, physicists describe the world best by unifying and reducing di\ufb00erent theories to a standardized one. Likewise, it is a constant quest in the \ufb01eld of ML to establish a \u2018Standard Model\u2019 , that gives a holistic view of the broad learning principles, lays out a blueprint permitting fuller and more systematic exploration in the design and analysis of new algorithms, and eventually serves as a vehicle toward panoramic learning that integrates all available sources of experience. This paper presents an attempt toward this end", "45d47693-851a-4d33-a3c3-739db83efb87": "If the noise on the input variable x is described by a variable \u03be having a distribution \u03bd(\u03be), then the sum-of-squares error function becomes Using the calculus of variations, we can optimize with respect to the function f(x) Appendix D where the basis functions are given by We see that there is one basis function centred on every data point. This is known as the Nadaraya-Watson model and will be derived again from a different perspective in Section 6.3.1. If the noise distribution \u03bd(\u03be) is isotropic, so that it is a function only of \u2225\u03be\u2225, then the basis functions will be radial. Note that the basis functions (6.41) are normalized, so that \ufffd n h(x \u2212 xn) = 1 for any value of x. The effect of such normalization is shown in Figure 6.2.\n\nNormalization is sometimes used in practice as it avoids having regions of input space where all of the basis functions take small values, which would necessarily lead to predictions in such regions that are either small or controlled purely by the bias parameter. Another situation in which expansions in normalized radial basis functions arise is in the application of kernel density estimation to the problem of regression, as we shall discuss in Section 6.3.1", "eaa35c66-0919-481c-8cea-60c4fe778870": "Learning algorithms based on maximizing \u00a3 can be run with prolonged periods of improving q and prolonged periods of improving 0, however. If the role of biological dreaming is to train networks for predicting gq, then this explains how animals are able to remain awake for several hours (the longer they are awake, the greater the gap between \u00a3 and log p(v), but L\u00a3 will remain a lower  649  CHAPTER 19. APPROXIMATE INFERENCE  bound) and to remain asleep for several hours (the generative model itself is not modified during sleep) without damaging their internal models. Of course, these ideas are purely speculative, and there is no hard evidence to suggest that dreaming accomplishes either of these goals. Dreaming may also serve reinforcement learning rather than probabilistic modeling, by sampling synthetic experiences from the animal\u2019s transition model, on which to train the animal\u2019s policy. Or sleep may serve some other purpose not yet anticipated by the machine learning community. 19.5.2 Other Forms of Learned Inference  https://www.deeplearningbook.org/contents/inference.html    This strategy of learned approximate inference has also been applied to other models", "72f42ca3-a61d-4053-92f1-e764f0ae04df": "The remaining sections describe approaches to overcoming the problem. Recurrent networks involve the composition of the same function multiple times, once per time step. These compositions can result in extremely nonlinear behavior, as illustrated in figure 10.15. In particular, the function composition employed by recurrent neural networks somewhat resembles matrix multiplication. We can think of the recurrence relation  AO =Wwtal-d (10.36) as a very simple recurrent neural network lacking a nonlinear activation function,  396  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  Projection of output  \u201460 \u201440 \u201420 0 20 40 60  Input coordinate  https://www.deeplearningbook.org/contents/rnn.html    Figure 10,15: Repeated function composition.\n\nWhen composing many nonlinear functions (like the lineartanh layer shown here), the result is highly nonlinear, typically with most of the values associated with a tiny derivative, some values with a large derivative, and many alternations between increasing and decreasing. Here, we plot a linear projection of  a 100-dimensional hidden state down to a single dimension, plotted on the y-axis", "60dcd580-3e78-4ca8-b690-30b50a4eb9db": "Each third-level action contributes with weight \u21e1(At+1|St+1)\u21e1(At+2|St+2)\u21e1(a00|St+3), and so on. It is as if each arrow to an action node in the diagram is weighted by the action\u2019s probability of being selected under the target policy and, if there is a tree below the action, then that weight applies to all the leaf nodes in the tree.\n\nWe can think of the 3-step tree-backup update as consisting of 6 half-steps, alternating between sample half-steps from an action to a subsequent state, and expected half-steps considering from that state all possible actions with their probabilities of occurring under the policy. Now let us develop the detailed equations for the n-step tree-backup algorithm", "ab61bc62-b71d-45a3-b65d-0f953dea93bb": "Since then, there has been considerable interest in this topic, both Chapter 7 in terms of theory and applications.\n\nOne of the most signi\ufb01cant developments has been the extension of kernels to handle symbolic objects, thereby greatly expanding the range of problems that can be addressed. The simplest example of a kernel function is obtained by considering the identity mapping for the feature space in (6.1) so that \u03c6(x) = x, in which case k(x, x\u2032) = xTx\u2032. We shall refer to this as the linear kernel. The concept of a kernel formulated as an inner product in a feature space allows us to build interesting extensions of many well-known algorithms by making use of the kernel trick, also known as kernel substitution. The general idea is that, if we have an algorithm formulated in such a way that the input vector x enters only in the form of scalar products, then we can replace that scalar product with some other choice of kernel. For instance, the technique of kernel substitution can be applied to principal component analysis in order to develop a nonlinear variant of PCA (Sch\u00a8olkopf et al., Section 12.3 1998)", "e0b6b99d-7dab-4bfe-acaf-b3fe99377566": "Speci\ufb01cally, consider the common choice of the function h as a linear function: h(x; \u03b8) = \u03b8\u22a4T(x), where T(x) is, with a slight abuse of notation, Alternating optimization for posterior regularization.\n\nHaving seen EM-style alternating minimization algorithms being applied as a general solver for a number of optimization-theoretic frameworks described above, it is not surprising that the posterior regularization framework can also be solved with an alternating minimization procedure. For example, consider the simple case of linear constraint in Equation 2.17, penalty function U(\u03be) = \u2225\u03be\u22251, and q factorizing across \u03b8 = {\u03b8c}. At each iteration n, the solution of q(\u03b8c) is given as : where \u03b8\\c denotes all components of \u03b8 except \u03b8c, and Z is the normalization factor. Intuitively, a con\ufb01guration of \u03b8c with a higher expected constraint value E\\cT(x\u2217; \u03b8) will receive a higher probability under q(n+1)(\u03b8c). The optimization procedure iterates over all components c of \u03b8. 2.4. Summary", "951d106b-94bf-4739-a331-42be2c86d85c": "a disadvantage, and one may also guarantee that the generator network assigns nonzero probability to all points simply by making the last layer of the generator network add Gaussian noise to all the generated values. Generator networks that add Gaussian noise in this manner sample from the same distribution that one obtains by using the generator network to parametrize the mean of a conditional Gaussian distribution. Dropout seems to be important in the discriminator network. In particular, units should be stochastically dropped while computing the gradient for the generator network to follow. Following the gradient of the deterministic version of the discriminator with its weights divided by two does not seem to be as effective. Likewise, never using dropout seems to yield poor results. While the GAN framework is designed for differentiable generator networks, similar principles can be used to train other kinds of models. For example, self- supervised boosting can be used to train an RBM generator to fool a logistic regression discriminator .\n\n20.10.5 Generative Moment Matching Networks  Generative moment matching networks  are another form of generative model based on differentiable generator networks", "3618724f-b478-4b4c-82b4-814ba9550761": "Most of the time we move greedily, selecting the move that leads to the state with greatest value, that is, with the highest estimated probability of winning. Occasionally, however, we select randomly from among the other moves instead. These are called exploratory moves because they cause us to experience states that we might otherwise never see. A sequence of moves made and considered during a game can be diagrammed as in Figure 1.1. While we are playing, we change the values of the states in which we \ufb01nd ourselves during the game. We attempt to make them more accurate estimates of the probabilities of winning. To do this, we \u201cback up\u201d the value of the state after each greedy move to the state before the move, as suggested by the arrows in Figure 1.1. More precisely, the current value of the earlier state is updated to be closer to the value of the later state. This can be done by moving the earlier state\u2019s value a fraction of the way toward the value of the later state", "556ea78c-11fa-4153-bcb6-be38f8ad4b0a": "Exercise 5.9 Modify the algorithm for \ufb01rst-visit MC policy evaluation (Section 5.1) to use the incremental implementation for sample averages described in Section 2.4.\n\n\u21e4 O\u21b5-policy MC prediction (policy evaluation) for estimating Q \u21e1 q\u21e1 We are now ready to present an example of the second class of learning control methods we consider in this book: o\u21b5-policy methods. Recall that the distinguishing feature of on-policy methods is that they estimate the value of a policy while using it for control. In o\u21b5-policy methods these two functions are separated. The policy used to generate behavior, called the behavior policy, may in fact be unrelated to the policy that is evaluated and improved, called the target policy. An advantage of this separation is that the target policy may be deterministic (e.g., greedy), while the behavior policy can continue to sample all possible actions. O\u21b5-policy Monte Carlo control methods use one of the techniques presented in the improving the target policy. These techniques require that the behavior policy has a nonzero probability of selecting all actions that might be selected by the target policy (coverage)", "f2618500-9c41-461d-a9d3-7f206e856a83": "These require that the regularizer should be invariant to re-scaling of the weights and to shifts of the biases. Such a regularizer is given by \u03bb1 2 where W1 denotes the set of weights in the \ufb01rst layer, W2 denotes the set of weights in the second layer, and biases are excluded from the summations. This regularizer will remain unchanged under the weight transformations provided the regularization parameters are re-scaled using \u03bb1 \u2192 a1/2\u03bb1 and \u03bb2 \u2192 c\u22121/2\u03bb2. The regularizer (5.121) corresponds to a prior of the form Note that priors of this form are improper (they cannot be normalized) because the bias parameters are unconstrained. The use of improper priors can lead to dif\ufb01culties in selecting regularization coef\ufb01cients and in model comparison within the Bayesian framework, because the corresponding evidence is zero. It is therefore common to include separate priors for the biases (which then break shift invariance) having their own hyperparameters.\n\nWe can illustrate the effect of the resulting four hyperparameters by drawing samples from the prior and plotting the corresponding network functions, as shown in Figure 5.11", "a04b413e-6059-4b2f-bf88-6b8e0dc8014d": "So do recurrent networks, described in chapter 10, which construct very deep computational graphs by repeatedly applying the same operation at each time step of a long temporal sequence. Repeated application of the same parameters gives rise to especially pronounced difficulties. For example, suppose that a computational graph contains a path that consists of repeatedly multiplying by a matrix W. After \u00a2 steps, this is equivalent to mul- tiplying by W*. Suppose that W has an eigendecomposition W = Vdiag(A)V~!. In this simple case, it is straightforward to see that  W? * = Vdiag(A)V~! * = Vdiag(A)tV~!. (8.11) Any eigenvalues \\; that are not near an absolute value of 1 will either explode if they are greater than 1 in magnitude or vanish if they are less than 1 in magnitude. The  vanishing and exploding gradient problem refers to the fact that gradients through such a graph are also scaled according to diag()\u2018", "9db3e8f8-d2c1-42cf-8a23-49a1c4011225": "The strategy in this section can be used to derive a lower bound estimator (a stochastic objective function) for a variety of directed graphical models with continuous latent variables. We will restrict ourselves here to the common case where we have an i.i.d.\n\ndataset with latent variables per datapoint, and where we like to perform maximum likelihood (ML) or maximum a posteriori (MAP) inference on the (global) parameters, and variational inference on the latent variables. It is, for example, arXiv:1312.6114v11    10 Dec 2022 straightforward to extend this scenario to the case where we also perform variational inference on the global parameters; that algorithm is put in the appendix, but experiments with that case are left to future work. Note that our method can be applied to online, non-stationary settings, e.g. streaming data, but here we assume a \ufb01xed dataset for simplicity. Let us consider some dataset X = {x(i)}N i=1 consisting of N i.i.d. samples of some continuous or discrete variable x. We assume that the data are generated by some random process, involving an unobserved continuous random variable z", "cf83ce12-c8dd-4948-b992-d21dda3c2734": "But many of these neurons lost that reward response as training continued and developed responses instead to the illumination of the light that predicted the reward (Figure 15.2, middle panel). With continued training, lever pressing became faster while the number of dopamine neurons responding to the trigger cue decreased. Following this study, the same monkeys were trained on a new task (Schultz, Apicella, and Ljungberg, 1993). Here the monkeys faced two levers, each with a light above it. Illuminating one of these lights was an \u2018instruction cue\u2019 indicating which of the two levers by monitored dopamine neurons within small time intervals, averaged over all the monitored dopamine neurons (ranging from 23 to 44 neurons for these data). Top: dopamine neurons are activated by the unpredicted delivery of drop of apple juice.\n\nMiddle: with learning, dopamine neurons developed responses to the reward-predicting trigger cue and lost responsiveness to the delivery of reward. Bottom: with the addition of an instruction cue preceding the trigger cue by 1 second, dopamine neurons shifted their responses from the trigger cue to the earlier instruction cue. From Schultz et al. , MIT Press", "a19328f1-158e-480a-81a8-1e28d9ec273c": "https://www.deeplearningbook.org/contents/generative_models.html    \u2018The spike and slab RBM has two sets of hidden units: binary SP1Ke units 1 and real-valued slab units s. yemmgen of the visible units conditioned on the hidden units is given by (\u00a9 s)W . In other words, each column W:,i defines a component that can appear in the input when h; = 1. The corresponding spike  677  CHAPTER 20. DEEP GENERATIVE MODELS  variable h; determines whether that component is present at all. The corresponding slab variable s; determines the intensity of that component, if it is present. When a spike variable is active, the corresponding slab variable adds variance to the input along the axis defined by W.,;. This allows us to model the covariance of the inputs. Fortunately, contrastive divergence and persistent contrastive divergence with Gibbs sampling are still applicable. There is no need to invert any matrix", "3a305160-bfcf-49f7-a7b3-f214dba18af1": "In Figure 2.28, we show the results of applying the K-nearest-neighbour algorithm to the oil \ufb02ow data, introduced in Chapter 1, for various values of K. As expected, we see that K controls the degree of smoothing, so that small K produces many small regions of each class, whereas large K leads to fewer larger regions. red, green, and blue points correspond to the \u2018laminar\u2019, \u2018annular\u2019, and \u2018homogeneous\u2019 classes, respectively.\n\nAlso shown are the classi\ufb01cations of the input space given by the K-nearest-neighbour algorithm for various values of K. An interesting property of the nearest-neighbour (K = 1) classi\ufb01er is that, in the limit N \u2192 \u221e, the error rate is never more than twice the minimum achievable error rate of an optimal classi\ufb01er, i.e., one that uses the true class distributions  . As discussed so far, both the K-nearest-neighbour method, and the kernel density estimator, require the entire training data set to be stored, leading to expensive computation if the data set is large", "0145fff8-b015-4215-9d52-053a4524b69a": "Thus the number of parameters scales quadratically with D and can become excessive in spaces of high dimensionality. If we restrict the covariance matrix to be diagonal, then it has only D independent parameters, and so the number of parameters now grows linearly with dimensionality. However, it now treats the variables as if they were independent and hence can no longer express any correlations between them. Probabilistic PeA provides an elegant compromise in which the M most significant correlations can be captured while still ensuring that the total number of parameters grows only linearly with D. We can see this by evaluating the number of degrees of freedom in the PPCA model as follows. The covariance matrix C depends on the parameters W, which has size D x M, and a 2 , giving a total parameter count of DM+ 1.\n\nHowever, we have seen that there is some redundancy in this parameterization associated with rotations of the coordinate system in the latent space. The orthogonal matrix R that expresses these rotations has size M x M. In the first column of this matrix there are M - 1 independent parameters, because the column vector must be normalized to unit length", "bdb2ccd0-369b-45e0-b363-30ce49e50941": "Again, comparing with (2.194) we have so that, given any M \u2212 1 of the parameters \u00b5k, the value of the remaining parameter is \ufb01xed. In some circumstances, it will be convenient to remove this constraint by expressing the distribution in terms of only M \u2212 1 parameters. This can be achieved by using the relationship (2.209) to eliminate \u00b5M by expressing it in terms of the remaining {\u00b5k} where k = 1, . , M \u2212 1, thereby leaving M \u2212 1 parameters.\n\nNote that these remaining parameters are still subject to the constraints Making use of the constraint (2.209), the multinomial distribution in this representation then becomes which we can solve for \u00b5k by \ufb01rst summing both sides over k and then rearranging and back-substituting to give This is called the softmax function, or the normalized exponential. In this representation, the multinomial distribution therefore takes the form This is the standard form of the exponential family, with parameter vector \u03b7 = (\u03b71, . , \u03b7M\u22121)T in which Finally, let us consider the Gaussian distribution", "38c11db0-3719-4338-9c33-6a4df7eaee23": "The dataset D contains as elements the abstract examples z @ (for the i-th example), which could stand for an (input,target) pair 2 = (@, y) in the case of supervised learning, or for just an input z = 2 in the case of unsupervised learning. The algorithm returns the vector of errors e for each example in ID, whose mean is the estimated generalization error. The errors on individual examples can be used to compute a confidence interval around the mean (equation 5.47).\n\nThough these confidence intervals are not well justified after the use of cross-validation, it is still common practice to use them to declare that algorithm A is better than algorithm B only if the confidence interval of the error of algorithm A lies below and does not intersect the confidence interval of algorithm B", "f2a2423d-9c2a-4b14-aa5d-8b3a309f9611": "From this point of view, the response of a simple cell to an image is given by  s(1) = LY wle ye. y).\n\n(9.15)  ceEX yey  Specifically, w(x, y) takes the form of a Gabor function:  w(x, 45 0, Be, By, f, 9,20, yo.T) = aexp (\u2014Bra!\u201d \u2014 By\u2019) cos(fa' +), (9.16)  where ax! = (x \u2014 x9) cos(T) + (y \u2014 yo) sin(T) (9.17) and y\u2019 = \u2014(@ \u2014 a) sin(r) + (y \u2014 yo) cos(r). (9.18) 362  https://www.deeplearningbook.org/contents/convnets.html    CHAPFER-9\u2014CONVOEUFIONAE NEF WORKS  4242525888 424425588  SSB  SSSS222 SSSS222eU  Figure 9.18: Gabor functions with a variety of parameter settings. White indicates large positive weight, black indicates large negative weight, and the background gray corresponds to zero weight", "a38970b4-64cb-4fd3-83ef-04d32f10b671": "Here is how the tic-tac-toe problem would be approached with a method making use of a value function. First we would set up a table of numbers, one for each possible state of the game. Each number will be the latest estimate of the probability of our winning from that state. We treat this estimate as the state\u2019s value, and the whole table is the learned value function. State A has higher value than state B, or is considered \u201cbetter\u201d than state B, if the current estimate of the probability of our winning from A is higher than it is from B. Assuming we always play Xs, then for all states with three Xs in a row the probability of winning is 1, because we have already won. Similarly, for all states with three Os in a row, or that are \ufb01lled up, the correct probability is 0, as we cannot win from them. We set the initial values of all the other states to 0.5, representing a guess that we have a 50% chance of winning.\n\nWe then play many games against the opponent. To select our moves we examine the states that would result from each of our possible moves (one for each blank space on the board) and look up their current values in the table", "020261ae-eeb9-4b42-b49d-77ce9ee1ffb2": "A nonlinear projection head improves the representation quality of the layer before it We then study the importance of including a projection head, i.e. g(h). Figure 8 shows linear evaluation results using three different architecture for the head: (1) identity mapping; (2) linear projection, as used by several previous approaches ; and (3) the default nonlinear projection with one additional hidden layer (and ReLU activation), similar to Bachman et al. We observe that a nonlinear projection is better than a linear projection (+3%), and much better than no projection (>10%). When a projection head is used, similar results are observed regardless of output dimension. Furthermore, even when nonlinear projection is used, the layer before the projection head, h, is still much better (>10%) than the layer after, z = g(h), which shows that the hidden layer before the projection head is a better representation than the layer after. We conjecture that the importance of using the representation before the nonlinear projection is due to loss of information induced by the contrastive loss.\n\nIn particular, z = g(h) is trained to be invariant to data transformation", "e643ed8c-7c3f-419c-b4e5-8f0ef17670fa": "This is the idea of double learning. Note that although we learn two estimates, only one estimate is updated on each play; double learning doubles the memory requirements, but does not increase the amount of computation per step. The idea of double learning extends naturally to algorithms for full MDPs. For example, the double learning algorithm analogous to Q-learning, called Double Q-learning, divides the time steps in two, perhaps by \ufb02ipping a coin on each step. If the coin comes up heads, If the coin comes up tails, then the same update is done with Q1 and Q2 switched, so that Q2 is updated. The two approximate value functions are treated completely symmetrically. The behavior policy can use both action-value estimates. For example, an \"-greedy policy for Double Q-learning could be based on the average (or sum) of the two action-value estimates. A complete algorithm for Double Q-learning is given in the box below. This is the algorithm used to produce the results in Figure 6.5. In that example, double learning seems to eliminate the harm caused by maximization bias", "7cf0c820-305c-46ae-85a3-f8049ab32f39": "It is necessary to draw the samples starting from a fresh Markov chain initialized from a random starting point after the model is done training. The samples present in the persistent negative chains used for training have been influenced by several recent versions of the model, and thus can make the model appear to have greater capacity than it actually does. Berglund and Raiko  performed experiments to examine the bias and  https://www.deeplearningbook.org/contents/partition.html    variance in the estimate of the gradient provided by CD and SML. CD proves to have lower variance than_the estimator based on exact sampling. SML has higher variance. The cause of CD\u2019s low variance is its use of the same training points in both the positive and negative phase. If the negative phase is initialized from different training points, the variance rises above that of the estimator based on exact sampling. All these methods based on using MCMC to draw samples from the model can in principle be used with almost any variant of MCMC.\n\nThis means that techniques such as SML can be improved by using any of the enhanced MCMC techniques described in chapter 17, such as parallel tempering", "6eba189d-476a-460c-9d0f-02cebe987f3a": "In going to the dual formulation we have turned the original optimization problem, which involved minimizing (7.6) over M variables, into the dual problem (7.10), which has N variables.\n\nFor a \ufb01xed set of basis functions whose number M is smaller than the number N of data points, the move to the dual problem appears disadvantageous. However, it allows the model to be reformulated using kernels, and so the maximum margin classi\ufb01er can be applied ef\ufb01ciently to feature spaces whose dimensionality exceeds the number of data points, including in\ufb01nite feature spaces. The kernel formulation also makes clear the role of the constraint that the kernel function k(x, x\u2032) be positive de\ufb01nite, because this ensures that the Lagrangian function \ufffdL(a) is bounded below, giving rise to a wellde\ufb01ned optimization problem. In order to classify new data points using the trained model, we evaluate the sign of y(x) de\ufb01ned by (7.1). This can be expressed in terms of the parameters {an} and the kernel function by substituting for w using (7.8) to give years, Euler worked hard to persuade Lagrange to move to Berlin, which he eventually did in 1766 where he succeeded Euler as Director of Mathematics at the Berlin Academy", "4764e0d2-b8d4-4911-b3c6-c25f59ca214d": "While animals undoubtedly use algorithms that do not exactly match those we have presented in this book, one can gain insight into animal behavior by considering the tradeo\u21b5s that various reinforcement learning algorithms imply. An idea developed by computational neuroscientists Daw, Niv, and Dayan  is that animals use both model-free and model-based processes.\n\nEach process proposes an action, and the action chosen for execution is the one proposed by the process judged to be the more trustworthy of the two as determined by measures of con\ufb01dence that are maintained throughout learning. Early in learning the planning process of a model-based system is more trustworthy because it chains together short-term predictions which can become accurate with less experience than long-term predictions of the model-free process. But with continued experience, the model-free process becomes more trustworthy because planning is prone to making mistakes due to model inaccuracies and short-cuts necessary to make planning feasible, such as various forms of \u201ctree-pruning\u201d: the removal of unpromising search tree branches. According to this idea one would expect a shift from goal-directed behavior to habitual behavior as more experience accumulates. Other ideas have been proposed for how animals arbitrate between goal-directed and habitual control, and both behavioral and neuroscience research continues to examine this and related questions", "11a5f1e4-a675-4926-b361-5fe563249217": "An optimal one-way multigrid algorithm for discrete-time stochastic control. IEEE Transactions on Automatic Control, 36(8):898\u2013914. Chrisman, L. Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. In Proceedings of the Tenth National Conference on Arti\ufb01cial Intelligence (AAAI-92), pp. 183\u2013188. AAAI/MIT Press, Menlo Park, CA. its application to learning.\n\nIn Proceedings of the Fifth National Conference on Arti\ufb01cial Intelligence, pp. 148\u2013152. Morgan Kaufmann. Cichosz, P. Truncating temporal di\u21b5erences: On the e\ufb03cient implementation of TD(\u03bb) for reinforcement learning. Journal of Arti\ufb01cial Intelligence Research, 2:287\u2013318. Ciosek, K., Whiteson, S. Expected policy gradients. ArXiv:1706.05374v1. A revised version appeared in Proceedings of the Annual Conference of the Association for the Advancement of Arti\ufb01cial Intelligence , pp. 2868\u20132875", "1d53748c-10ee-482a-8587-66c8e39e3395": "Input: the policy \u21e1 to be evaluated Input: a di\u21b5erentiable function \u02c6v : S+ \u21e5 Rd !\n\nR such that \u02c6v(terminal,\u00b7) = 0 Algorithm parameters: step size \u21b5 > 0, a positive integer n Initialize value-function weights w arbitrarily (e.g., w = 0) All store and access operations (St and Rt) can take their index mod n + 1 The key equation of this algorithm, analogous to (7.2), is Gt:t+n .= Rt+1 +\u03b3Rt+2 +\u00b7 \u00b7 \u00b7+\u03b3n\u22121Rt+n +\u03b3n\u02c6v(St+n,wt+n\u22121), 0 \uf8ff t \uf8ff T \u2212n. (9.16) Exercise 9.1 Show that tabular methods such as presented in Part I of this book are a special case of linear function approximation. What would the feature vectors be? \u21e4 Linear methods are interesting because of their convergence guarantees, but also because in practice they can be very e\ufb03cient in terms of both data and computation", "5daa8e7e-3c99-4ec7-8ca5-6ff738c97a96": "The difference between these two results can be understood by noting that there is a large positive contribution to the Kullback-Leibler divergence from regions of Z space in which p(Z) is near zero unless q(Z) is also close to zero. Thus minimizing this form of KL divergence leads to distributions q(Z) that avoid regions in which p(Z) is small. Conversely, the Kullback-Leibler divergence KL(p\u2225q) is minimized by distributions q(Z) that are nonzero in regions where p(Z) is nonzero. We can gain further insight into the different behaviour of the two KL divergences if we consider approximating a multimodal distribution by a unimodal one, as illustrated in Figure 10.3. In practical applications, the true posterior distribution will often be multimodal, with most of the posterior mass concentrated in some number of relatively small regions of parameter space. These multiple modes may arise through nonidenti\ufb01ability in the latent space or through complex nonlinear dependence on the parameters", "557a3cf9-ccfa-4906-acc3-0d1dea38d5d2": "CPU implementations will be slow as a result of the lack  445  CHAPTER 12.\n\nAPPLICATIONS  of cache coherence, and GPU implementations will be slow because of the lack of coalesced memory transactions and the need to serialize warps when members of a warp take different branches. In some cases, these issues can be mitigated by partitioning the examples into groups that all take the same branch, then processing hese groups of examples simultaneously. This can be an acceptable strategy for minimizing the time required to process a fixed amount of examples in an offline setting. In a real-time setting where examples must be processed continuously, partitioning the workload can result in load-balancing issues. For example, if we assign one machine to process the first step in a cascade and another machine to process the last step in a cascade, then the first will tend to be overloaded, and the ast will tend to be underloaded. Similar issues arise if each machine is assigned to  https://www.deeplearningbook.org/contents/applications.html    implement different nodes of a neural decision tree", "814a80b9-9dd2-4a0a-aa06-7615084d8462": "The distributed representation the model learns for each word enables this sharing by allowing the model to treat words that have features in common similarly.\n\nFor example, if the word dog and the word cat map to representations that share many attributes, then sentences that contain the word cat can inform the predictions that will be made by the model for sentences that contain the word dog, and vice versa. Because there are many such attributes, there are many ways in which generalization can happen, transferring information from each training sentence to an exponentially large number of semantically related sentences. The curse of dimensionality requires the model to generalize to a number of sentences that is exponential in the sentence length. The model counters this curse by relating each training sentence to an exponential number of similar sentences. We sometimes call these word representations word embeddings. In this interpretation, we view the raw symbols as points in a space of dimension equal to the vocabulary size. The word representations embed those points in a feature space of lower dimension. In the original space, every word is represented by a one-hot vector, so every pair of words is at Euclidean distance 2 from each other. In the embedding space, words that frequently appear in similar contexts  459  CHAPTER 12", "319b838a-589f-405c-b499-fd2f95cb62c4": "For example, node (1,0) corresponds to the prefix (bo (w,) = 1, b;(w4) = 0), and the probability of w, can be decomposed as follows:  P(y = wa) = P(bo = 1,b1 = 0, b2 = 0) (12.11) P(bo = 1)P(b1 = 0| bo = 1)P(b2 = 0 | bo = 1,b1 = 0). (12.12)  https://www.deeplearningbook.org/contents/applications.html    463  CHAPTER 12. APPLICATIONS  of bits required to identify a word, with the weighting given by the frequency of these words. In this example, the number of operations needed to compute the hidden activations grows as O(In? ), while the output computations grow as O(n, 7m ). As long as np < In_, we can reduce computation more by shrinking np, than by shrinking np. Indeed, n, is often small", "44dd8369-cfce-4291-a94b-432db464a792": "For example, consider a collection of images in which each image has a different width and height. It is unclear how to model such inputs with a weight matrix of fixed size. Convolution is straightforward to apply; the kernel is simply applied a different number of times depending on the size of the input, and the output of the convolution operation scales accordingly. Convolution may be viewed as matrix multiplication; the same convolution kernel induces a different size of doubly block circulant matrix for each size of input. Sometimes the output of the network as well as the input is allowed to have variable size, for example, if we want to assign a class label to each pixel of the input. In this case, no further design work is necessary. In other cases, the network must produce some fixed-size output, for example, if we want to assign a single class label to the entire image. In this case, we must make some additional design steps, like inserting a pooling layer whose pooling regions scale in size proportional to the size of the input, to maintain a fixed number of pooled outputs.\n\nSome examples of this kind of strategy are shown  https://www.deeplearningbook.org/contents/convnets.html  in figure 9.11", "ab294b8f-8f28-47b8-b080-99a3e5461b13": "Limitations of Fixed Basis Functions Throughout this chapter, we have focussed on models comprising a linear combination of \ufb01xed, nonlinear basis functions. We have seen that the assumption of linearity in the parameters led to a range of useful properties including closed-form solutions to the least-squares problem, as well as a tractable Bayesian treatment. Furthermore, for a suitable choice of basis functions, we can model arbitrary nonlinearities in the mapping from input variables to targets. In the next chapter, we shall study an analogous class of models for classi\ufb01cation. It might appear, therefore, that such linear models constitute a general purpose framework for solving problems in pattern recognition. Unfortunately, there are some signi\ufb01cant shortcomings with linear models, which will cause us to turn in later chapters to more complex models such as support vector machines and neural networks.\n\nThe dif\ufb01culty stems from the assumption that the basis functions \u03c6j(x) are \ufb01xed before the training data set is observed and is a manifestation of the curse of dimensionality discussed in Section 1.4. As a consequence, the number of basis functions needs to grow rapidly, often exponentially, with the dimensionality D of the input space. Fortunately, there are two properties of real data sets that we can exploit to help alleviate this problem", "dc28504d-9d51-4b52-b33f-f4f6ed80c4e7": "To derive the same linear regression algorithm we obtained before, we define p(y | x) = N (y; 9(a; w), 07). The function \u00a2(w; w) gives the prediction of he mean of the Gaussian. In this example, we assume that the variance is fixed to some constant a? chosen by the user. We will see that this choice of the functional form of p(y | x) causes the maximum likelihood estimation procedure to yield the same learning algorithm as we developed before. Since the examples are assumed (0 be i.i.d., the conditional log-likelihood (equation 5.63) is given by  Vlog p(y | 2; 6) (5.64) i=l m Ad . 2 my, 9 = || mlogo \u2014 -5 log(2n) \u00bb G2 (5.65) 131  CHAPTER 5. MACHINE LEARNING BASICS  https://www.deeplearningbook.org/contents/ml.html       WIICLE Y bb LUC Outpul UL LUC Leal LESLesslol ULL LIC eo ULL slihoc w", "851348cb-bda5-4c8f-b4e5-82eac6f52e46": "These include learning with noisy or even negative data (\u00a74.1), generating adversarial text attacks (\u00a74.2), and generating prompts to steer pretrained LMs (\u00a74.3). We also study the performance on standard supervised generation tasks (\u00a7A.1.4) and show that our approach is competitive to train text generation models from scratch. We provide detailed con\ufb01gurations in the appendix (\u00a7A.2). The popular MLE algorithm learns by (blindly) imitating training data. However, it is often expensive to curate clean quality data. It is thus highly desirable to be able to learn from data with noises, or even negative examples. With the guidance of task metrics (rewards), the model can even learn to \u201coutperform\u201d the training data and achieve desired generation behaviors.\n\nTo this end, we consider the task of entailment generation . Given a sentence (premise), the goal is to generate a new sentence (hypothesis) that logically follows the premise. Setup (more in \u00a7A.2.1). We sub-sampled 50k training examples from the SNLI dataset , a commonly used entailment classi\ufb01cation dataset", "36c99990-1bde-4c2a-ba04-0fbd1bc2cebb": "Unlike VAEs and GANs, they do not need to pair the generator network with any other network\u2014neither an inference network, as used with VAEs, nor a  699  CHAPTER 20. DEEP GENERATIVE MODELS  https://www.deeplearningbook.org/contents/generative_models.html    discriminator network, as used with GANs. Generative moment matching networks are trained with a technique called moment matching. The basic idea behind moment matching is to train the generator in such a way that many of the statistics of samples generated by the model are as similar as possible to those of the statistics of the examples in the training set. In this context, a moment is an expectation of different powers of a random variable. For example, the first moment is the mean, the second moment is the mean of the squared values, and so on. In multiple dimensions, each element of the random vector may be raised to different powers, so that a moment may be any quantity of the form  Eg Ta\", (20.82)  T is a vector of nonnegative integers. where n = [nj,na,...,Nd|  Upon first examination, this approach seems to be computationally infeasible", "e2b17912-5c58-439d-89dc-5b5a8bde69a4": "See figure 16.13 for an example of how factor graphs can resolve ambiguity in the interpretation of undirected networks. 16.3. Sampling from Graphical Models  Graphical models also facilitate the task of drawing samples from a model. One advantage of directed graphical models is that a simple and efficient proce- dure called ancestral sampling can produce a sample from the joint distribution represented by the model. The basic idea is to sort the variables x; in the graph into a topological ordering, so that for all i and j, 7 is greater than? if x; is a parent of xj. The variables can then be sampled in this order. In other words, we first sample x; ~ P(x1), then sample P(x2 | Pag(x2)), and so on, until finally we sample P(x, | Pag(x)). So long as each conditional distribution p(x; | Pag(x;)) is easy to sample from, then the whole model is easy to sample from. The topological sorting operation guarantees that we can read the conditional distributions in equation 16.1 and sample from them in order", "ed4ba921-8b0e-4619-8f49-580b3fae37b2": "SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  1 ge) be pe gt) \\ \u2019 f f f fo. / ~.7 ~~? Figure 10.1: The classical dynamical system described by equation 10.1, illustrated as an unfolded computational graph. Each node represents the state at some timet, and the function f maps the state at t to the state at t+ 1. The same parameters (the same value of 8 used to parametrize f) are used for all time steps. 10.1 Unfolding Computational Graphs  A computational graph is a way to formalize the structure of a set of computations, such as those involved in mapping inputs and parameters to outputs and loss. Please refer to section 6.5.1 for a general introduction. In this section we explain the idea of unfolding a recursive or recurrent computation into a computational graph that has a repetitive structure, typically corresponding to a chain of events. Unfolding this graph results in the sharing of parameters across a deep network structure", "7b3e173e-5610-46bd-a923-1767497a3b09": "Emily Sheng, Kai-Wei Chang, Prem Natarajan, and Nanyun Peng. 2020. Towards controllable biases in language generation. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3239\u20133254. Zhan Shi, Xinchi Chen, Xipeng Qiu, and Xuanjing Huang. 2018. Toward diverse text generation with inverse reinforcement learning. In Proceedings of the 27th International Joint Conference on Arti\ufb01cial Intelligence, pages 4361\u20134367. Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An introduction. MIT press. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in Neural Information Processing Systems, 30:5998\u20136008", "c0e13d58-d4f8-45f0-b6e2-990d7d2886a7": "This is a  of text extracted This is a piece of text extracted  a large set of  articles from a large set of news articles  A masked language model, which is an instance of denoising auto-encoder, itself an instance of contrastive self- supervised learning. Variable y is a text segment; x is a version of the text in which some words have been masked. The network is trained to reconstruct the uncorrupted text. As we pointed out earlier, a predictive architecture of this type can produce only a single prediction for a given input. Since the model must be able to predict multiple possible outcomes, the prediction is not a single set of words but a series of scores for every word in the vocabulary for each missing word location. But we cannot use this trick for images because we cannot enumerate all possible images. Is there a solution to  https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/  Self-supervised learning: The dark matter of intelligence   this problem? The short answer is no. There are interesting ideas in this direction, but they have not yet led to results that are as good as joint embedding architectures", "5036d517-21bf-4319-aa58-f2c07f71131e": "This is implemented by triggering an eligibility trace at a synapse upon a coincidence of presynaptic and postsynaptic activity (or more exactly, upon pairing of presynaptic activity with the postsynaptic activity that that presynaptic activity participates in causing)\u2014what we call a contingent eligibility trace.\n\nThis is essentially the three-factor learning rule of an actor unit described in the previous section. The shape and time course of an eligibility trace in Klopf\u2019s theory re\ufb02ects the durations of the many feedback loops in which the neuron is embedded, some of which lie entirely within the brain and body of the organism, while others extend out through the organism\u2019s external environment as mediated by its motor and sensory systems. His idea was that the shape of a synaptic eligibility trace is like a histogram of the durations of the feedback loops in which the neuron is embedded. The peak of an eligibility trace would then occur at the duration of the most prevalent feedback loops in which that neuron participates. The eligibility traces used by algorithms described in this book are simpli\ufb01ed versions of Klopf\u2019s original idea, being exponentially (or geometrically) decreasing functions controlled by the parameters \u03bb and \u03b3", "75ddb105-862a-4250-b89b-9cd4da22ff51": "This is a long known fact that W metrizes the weak* topology of (C(X), \u2225 \u00b7 \u2225\u221e) on Prob(X), and by de\ufb01nition this is the topology of convergence in distribution. A proof of this can be found (for example) in . 3. This is a straightforward application of Pinsker\u2019s inequality 4.\n\nThis is trivial by recalling the fact that \u03b4 and W give the strong and weak* topologies on the dual of (C(X), \u2225 \u00b7 \u2225\u221e) when restricted to Prob(X). where \u02dcf lies in F = { \u02dcf : X \u2192 R , \u02dcf \u2208 Cb(X), \u2225 \u02dcf\u2225L \u2264 1} and \u03b8 \u2208 Rd. Since X is compact, we know by the Kantorovich-Rubenstein duality  that there is an f \u2208 F that attains the value Let us de\ufb01ne X\u2217(\u03b8) = {f \u2208 F : V (f, \u03b8) = W(Pr, P\u03b8)}. By the above point we know then that X\u2217(\u03b8) is non-empty", "9c107d7c-b1b4-48bd-bbee-8a65ec219a90": "Data Augmentation prevents overfitting by modifying lim-  ited datasets to possess the characteristics of big data. Abbreviations  GAN: generative adversarial network; CNN: convolutional neural network; DCGAN: deep convolutional generative adversarial network; NAS: neural architecture search; SRCNN: super-resolution convolutional neural network; SRGAN: super-resolution generative adversarial network; CT: computerized tomography; MRI: magnetic resonance imaging; PET: positron emission tomography; ROS: random oversampling; SMOTE: synthetic minority oversampling technique; RGB: red-green-blue; PCA: principal components analysis; UCI: University of California Irvine; MNIST: Modified National Institute of Standards and Technology; CIFAR: Canadian Institute for Advanced Research; t-SNE: t-distributed stochastic neighbor embedding.\n\nAcknowledgements  We would like to thank the reviewers in the Data Mining and Machine Learning Laboratory at Florida Atlantic University. Additionally, we acknowledge partial support by the NSF . Opinions, findings, conclusions, or recommen- dations in this paper are solely of the authors\u2019 and do not reflect the views of the NSF", "1a26a893-9189-47c1-b3d5-7eee05054808": "This demonstrates the \ufb02exibility of Snorkel, in that the labeling functions (and by extension, the generative model) do not need to operate over the same domain as the discriminative model being trained. Predictive performance is summarized in Table 5. Abnormality Detection in Lung Radiographs (Rad) In many real-world radiology settings, there are large repositories of image data with corresponding narrative text reports, but limited or no labels that could be used for training an image classi\ufb01cation model. In this application, in collaboration with radiologists, we wrote labeling functions over the text radiology reports, and used the resulting labels to train an image classi\ufb01er to detect abnormalities in lung X-ray images.\n\nWe used a publicly available dataset from the OpenI biomedical image repository15 consisting of 3,851 distinct radiology reports\u2014composed of unstructured text and Medical Subject Headings (MeSH)16 codes\u2014and accompanying X-ray images. Crowdsourcing (Crowd) We trained a model to perform sentiment analysis using crowdsourced annotations from the weather sentiment task from Crowd\ufb02ower.17 In this task, contributors were asked to grade the sentiment of oftenambiguous tweets relating to the weather, choosing between \ufb01ve categories of sentiment", "02b85516-7bc6-4b2f-a85d-f298205ecf62": "A possible source of confusion is the terminology used by the famous psychologist B. F. Skinner and his followers. For Skinner, positive reinforcement occurs when the consequences of an animal\u2019s behavior increase the frequency of that behavior; punishment occurs when the behavior\u2019s consequences decrease that behavior\u2019s frequency. Negative reinforcement occurs when behavior leads to the removal of an aversive stimulus (that is, a stimulus the animal does not like), thereby increasing the frequency of that behavior.\n\nNegative punishment, on the other hand, occurs when behavior leads to the removal of an appetitive stimulus (that is, a stimulus the animal likes), thereby decreasing the frequency of that behavior. We \ufb01nd no critical need for these distinctions because our approach is more abstract than this, with both reward and reinforcement signals allowed to take on both positive and negative values. (But note especially that when our reinforcement signal is negative, it is not the same as Skinner\u2019s negative reinforcement.) On the other hand, it has often been pointed out that using a single number as a reward or a penalty signal, depending only on its sign, is at odds with the fact that animals\u2019 appetitive and aversive systems have qualitatively di\u21b5erent properties and involve di\u21b5erent brain mechanisms", "1002afcb-2037-4d7a-be4b-37c693f4068d": "We use WordPiece embeddings (Wu et al., 2016) with a 30,000 token vocabulary. The \ufb01rst token of every sequence is always a special classi\ufb01cation token (). The \ufb01nal hidden state corresponding to this token is used as the aggregate sequence representation for classi\ufb01cation tasks. Sentence pairs are packed together into a single sequence. We differentiate the sentences in two ways. First, we separate them with a special token (). Second, we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B. As shown in Figure 1, we denote input embedding as E, the \ufb01nal hidden vector of the special  token as C \u2208 RH, and the \ufb01nal hidden vector for the ith input token as Ti \u2208 RH. For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings. A visualization of this construction can be seen in Figure 2. Unlike Peters et al. and Radford et al. , we do not use traditional left-to-right or right-to-left language models to pre-train BERT", "1bbb8a73-cdc2-4fc3-8deb-e04636f3b186": "Consider the extreme case in which not accounting for dependencies is catastrophic: Example 3.1 Consider a set of 10 labeling functions, where 5 are perfectly correlated, i.e., they vote the same way on every data point, and 5 are conditionally independent given the true label.\n\nIf the correlated labeling functions have accuracy \u03b1 = 50% and the uncorrelated ones have accuracy \u03b2 = 99%, then the maximum likelihood estimate of their accuracies according to the independent model is \u02c6\u03b1 = 100% and \u02c6\u03b2 = 50%. Specifying a generative model to account for such dependencies by hand is impractical for three reasons. First, it is dif\ufb01cult for non-expert users to specify these dependencies. Second, as users iterate on their labeling functions, their dependency structure can change rapidly, like when a user relaxes a labeling function to label many more candidates. Third, the dependency structure can be dataset speci\ufb01c, making it impossible to specify a priori, such as when a corpus contains many strings that match multiple regular expressions used in different labeling functions. We observed users of earlier versions of Snorkel struggling for these reasons to construct accurate and ef\ufb01cient generative models with dependencies", "0d5679ff-b8a6-450d-af54-aa80feee727a": "To choose its moves, TD-Gammon considered each of the 20 or so ways it could play its dice roll and the corresponding positions that would result. The resulting positions are afterstates as discussed in Section 6.8. The network was consulted to estimate each of their values. The move was then selected that would lead to the position with the highest estimated value. Continuing in this way, with TD-Gammon making the moves for both sides, it was possible to easily generate large numbers of backgammon games. Each game was treated as an episode, with the sequence of positions acting as the states, S0, S1, S2, . .. Tesauro applied the nonlinear TD rule (15.1) fully incrementally, that is, after each individual move. The weights of the network were set initially to small random values. The initial evaluations were thus entirely arbitrary", "da6d2f68-2f7f-4ad6-ac32-daa30f71425d": "One approach to achieve this is with LSTMs and other self-loops and gating mechanisms, described in section 10.10.\n\nAnother idea is to regularize or constrain the parameters so as to encourage \u201cinformation flow.\u201d In particular, we would like the gradient vector , a) L being back-propagated to maintain its  V  1 ead 1 c at 1 a a1 4 aca oa Vora  https://www.deeplearningbook.org/contents/rnn.html    IMagUILUde, EVEL I LILE LOSS LUIICLIOL OLlLy PELllallzes LILGC OULPUL abl LUE CLLG OL LILLE sequence. Formally, we want  dh (VawL) ane) (10.50) to be as large as Vat. (10.51) With this objective, Pascanu et al. propose the following regularizer: (t) 2 1 (Vawl) sry | Q 1]. (10.52) d IV pL  Computing the gradient of this regularizer may appear difficult, but Pascanu et al", "b466011e-a43d-462a-85fc-092a02da081e": "MACHINE LEARNING BASICS  e Regression: In this type of task, the computer program is asked to predict a numerical value given some input. To solve this task, the learning algorithm is asked to output a function f : R\u201d \u2014 R. This type of task is similar to classification, except that the format of output is different. An example of a regression task is the prediction of the expected claim amount that an insured person will make (used to set insurance premiums), or the prediction of future prices of securities. These kinds of predictions are also used for algorithmic trading. Transcription: In this type of task, the machine learning system is asked to observe a relatively unstructured representation of some kind of data and transcribe the information into discrete textual form.\n\nFor example, in optical character recognition, the computer program is shown a photograph containing an image of text and is asked to return this text in the form of a sequence of characters (e.g., in ASCII or Unicode format). Google Street View uses deep learning to process address numbers in this way . Another example is speech recognition, where the computer program is provided an audio waveform and emits a sequence of characters or word ID codes describing the words that were spoken in the audio recording", "bbecb251-c82b-4f90-8e90-293955756bc2": "used the CTR metric and a metric they called the LTV metric. These metrics are similar, except that the LTV metric critically distinguishes between individual website visitors: Testing the policies produced by the greedy and LTV approaches was done using a high con\ufb01dence o\u21b5-policy evaluation method on a test data set consisting of realworld interactions with a bank website served by a random policy.\n\nAs expected, results showed that greedy optimization performed best as measured by the CTR metric, while LTV optimization performed best as measured by the LTV metric. Furthermore\u2014 although we have omitted its details\u2014the high con\ufb01dence o\u21b5-policy evaluation method provided probabilistic guarantees that the LTV optimization method would, with high probability, produce policies that improve upon policies currently deployed. Assured by these probabilistic guarantees, Adobe announced in 2016 that the new LTV algorithm would be a standard component of the Adobe Marketing Cloud so that a retailer could issue a sequence of o\u21b5ers following a policy likely to yield higher return than a policy that is insensitive to long-term results. Birds and gliders take advantage of upward air currents\u2014thermals\u2014to gain altitude in order to maintain \ufb02ight while expending little, or no, energy", "a955497b-e0df-4f6e-870c-1fd3c33f8101": "We see that as more unlabeled data is added, the performance increases tional documents\u2014we get signi\ufb01cant improvements in the end discriminative model performance, with no change in the labeling functions. For example, in the EHR experiment, where we had access to a large unlabeled corpus, we were able to achieve signi\ufb01cant gains (8.1 F1 score points) in going from 100 to 50 thousand documents. Further empirical validation of these strong unlabeled scaling results can be found in follow-up work using Snorkel in a range of application domains, including aortic valve classi\ufb01cation in MRI videos , industrial-scale content classi\ufb01cation at Google , \ufb01ne-grained named entity recognition , radiology image triage , and others. Based on both this empirical validation, and feedback from Snorkel users in practice, we see this ability to leverage available unlabeled data without any additional user labeling effort as a signi\ufb01cant advantage of the proposed weak supervision approach", "c5106f62-4a22-4877-bf72-4406e1de6562": "arXiv preprint arXiv:1906.05849, 2019. Tschannen, M., Djolonga, J., Rubenstein, P. K., Gelly, S., and Lucic, M. On mutual information maximization for representation learning. arXiv preprint arXiv:1907.13625, 2019. A Simple Framework for Contrastive Learning of Visual Representations Wu, Z., Xiong, Y., Yu, S. X., and Lin, D. Unsupervised feature learning via non-parametric instance discrimination. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3733\u20133742, 2018. Xiao, J., Hays, J., Ehinger, K. A., Oliva, A., and Torralba, A. Sun database: Large-scale scene recognition from abbey to zoo. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3485\u20133492. IEEE, 2010", "c90263fb-9bea-47e0-8b42-65896ff8d88e": "It is not a function of the current value of the model parameters or the input example. Typically, an input unit is included with probability 0.8, and a hidden unit is included with  https://www.deeplearningbook.org/contents/regularization.html    probability 0.5. We then run forward propagation, back-propagation, and the learning update as usual. Figure 7.7 illustrates how to run forward propagation with dropout. More formally, suppose that a mask vector yz specifies which units to include, and J (0, 41) defines the cost of the model defined by parameters @ and mask y. Then dropout training consists of minimizing E,, J(@, w). The expectation contains  exponentially many terms, but we can obtain an unbiased estimate of its gradient by sampling values of p.  Dropout training is not quite the same as bagging training. In the case of bagging, the models are all independent. In the case of dropout, the models share parameters, with each model inheriting a different subset of parameters from the parent neural network. This parameter sharing makes it possible to represent an exponential number of models with a tractable amount of memory", "53ccdc7c-b9dd-40a1-8a98-c573c90f600b": "The equations we wish to implement use the bprop method of section 6.5.6, which computes the contribution of a single edge in the computational graph to the gradient.\n\nThe Vw operator used in calculus, however, takes into account the contribution of W to the value of f due to all edges in the computational graph. To resolve this ambiguity, we introduce dummy variables W() that are defined to be copies of W but with each W) used only at time step t. We may then use V ww) to denote the contribution of the weights at time step t to the gradient", "05c66c3d-a536-405e-914a-692ea8621e27": "Formally, we have model A with parameters w(4) and model B with parameters w'8). The two models map the input to two different but related outputs: 4) = f (w'A), a) and g\\\u00ae) = gw\u201d), 2). Let us imagine that the tasks are similar enough (perhaps with similar input and output distributions) that we believe the model parameters should be close to each other: Vi, wi) should be close to w\\? ), We can leverage this information through regularization. Specifically, we can use a parameter norm penalty of the form 2(w\"A), w)) = |Jw\"4) \u2014 wl) ||. Here we used an L? penalty, but other choices are also possible. This kind of approach was proposed by Lasserre et al. , who regularized the parameters of one model, trained as a classifier in a supervised paradigm, to be close to the parameters of another model, trained in an unsupervised paradigm (to capture the distribution of the observed input data)", "6cfb5401-41f7-4b8f-8494-32f372e3a970": "REINFORCE with baseline is unbiased and will converge asymptotically to a local minimum, but like all Monte Carlo methods it tends to learn slowly (produce estimates of high variance) and to be inconvenient to implement online or for continuing problems.\n\nAs we have seen earlier in this book, with temporal-di\u21b5erence methods we can eliminate these inconveniences, and through multi-step methods we can \ufb02exibly choose the degree of bootstrapping. In order to gain these advantages in the case of policy gradient methods we use actor\u2013critic methods with a bootstrapping critic. First consider one-step actor\u2013critic methods, the analog of the TD methods introduced in Chapter 6 such as TD(0), Sarsa(0), and Q-learning. The main appeal of one-step methods is that they are fully online and incremental, yet avoid the complexities of eligibility traces. They are a special case of the eligibility trace methods, and not as general, but easier to understand", "4d4c696d-e1b6-497e-a441-8b3435d79e3b": "In this way they restore the special properties that make on-policy learning stable with computationally simple semi-gradient methods. The whole area of o\u21b5-policy learning is relatively new and unsettled. Which methods are best or even adequate is not yet clear. Are the complexities of the new methods introduced at the end of this chapter really necessary? Which of them can be combined e\u21b5ectively with variance reduction methods? The potential for o\u21b5-policy learning remains tantalizing, the best way to achieve it still a mystery. 11.1 The \ufb01rst semi-gradient method was linear TD(\u03bb) . The name \u201csemi-gradient\u201d is more recent .\n\nSemi-gradient o\u21b5-policy TD(0) with general importance-sampling ratio may not have been explicitly stated until Sutton, Mahmood, and White , but the action-value forms were introduced by Precup, Sutton, and Singh , who also did eligibility trace forms of these algorithms (see Chapter 12). Their continuing, undiscounted forms have not been signi\ufb01cantly explored. The n-step forms given here are new", "373a5e23-bbc5-4604-908f-fd98d2a39548": "Here, a, Pr, By, f, , Xo, yo, and 7 are parameters that control the properties of the Gabor function. Figure 9.18 shows some examples of Gabor functions with different settings of these parameters. The parameters x9, yo, and 7 define a coordinate system. We translate and rotate x and y to form a\u2019 and y\u2019. Specifically, the simple cell will respond to image features centered at the point (xo, yo), and it will respond to changes in brightness aS we move along a line rotated 7 radians from the horizontal. Viewed as a function of a and y\u2019, the function w then responds to changes in brightness as we move along the 2\u2019 axis. It has two important factors: one is a Gaussian function, and the other is a cosine function. The Gaussian factor aexp (\u20148,7\u201d \u2014 Byy\u2019) can be seen as a gating term that ensures that the simple cell will respond only to values near where z\u2019 and y\u2019 are both zero, in other words, near the center of the cell\u2019s receptive field", "2c4baccf-09a2-455f-9300-7c9c372923ff": "In this way, the response of a unit in the subsampling layer will be relatively insensitive to small shifts of the image in the corresponding regions of the input space. In a practical architecture, there may be several pairs of convolutional and subsampling layers. At each stage there is a larger degree of invariance to input transformations compared to the previous layer. There may be several feature maps in a given convolutional layer for each plane of units in the previous subsampling layer, so that the gradual reduction in spatial resolution is then compensated by an increasing number of features. The \ufb01nal layer of the network would typically be a fully connected, fully adaptive layer, with a softmax output nonlinearity in the case of multiclass classi\ufb01cation. The whole network can be trained by error minimization using backpropagation to evaluate the gradient of the error function.\n\nThis involves a slight modi\ufb01cation of the usual backpropagation algorithm to ensure that the shared-weight constraints are satis\ufb01ed. Due to the use of local receptive \ufb01elds, the number of weights in Exercise 5.28 the network is smaller than if the network were fully connected", "9d9923c1-4193-4596-be89-2794b0865066": "dimensional input vector x and project it down to one dimension using If we place a threshold on y and classify y \u2a7e \u2212w0 as class C1, and otherwise class C2, then we obtain our standard linear classi\ufb01er discussed in the previous section. In general, the projection onto one dimension leads to a considerable loss of information, and classes that are well separated in the original D-dimensional space may become strongly overlapping in one dimension. However, by adjusting the components of the weight vector w, we can select a projection that maximizes the class separation. To begin with, consider a two-class problem in which there are N1 points of class C1 and N2 points of class C2, so that the mean vectors of the two classes are given by The simplest measure of the separation of the classes, when projected onto w, is the separation of the projected class means. This suggests that we might choose w so as to maximize resulting from projection onto the line joining the class means.\n\nNote that there is considerable class overlap in the projected space. The right plot shows the corresponding projection based on the Fisher linear discriminant, showing the greatly improved class separation. is the mean of the projected data from class Ck", "185d2cfa-aad7-4454-a1fb-6dfd37a03aa9": "We can see that this is a valid kernel by expanding the square and then making use of (6.14) and (6.16), together with the validity of the linear kernel k(x, x\u2032) = xTx\u2032. Note that the feature vector that corresponds to the Gaussian kernel has in\ufb01nite dimensionality. Exercise 6.11 The Gaussian kernel is not restricted to the use of Euclidean distance. If we use kernel substitution in (6.24) to replace xTx\u2032 with a nonlinear kernel \u03ba(x, x\u2032), we obtain An important contribution to arise from the kernel viewpoint has been the extension to inputs that are symbolic, rather than simply vectors of real numbers. Kernel functions can be de\ufb01ned over objects as diverse as graphs, sets, strings, and text documents. Consider, for instance, a \ufb01xed set and de\ufb01ne a nonvectorial space consisting of all possible subsets of this set. If A1 and A2 are two such subsets then one simple choice of kernel would be where A1 \u2229 A2 denotes the intersection of sets A1 and A2, and |A| denotes the number of subsets in A", "cbac04fe-64a0-4997-a2d6-86ae417abd1a": "Perhaps the most notable feature of the TD model is that it is based on a theory\u2014the theory we have described in this book\u2014that suggests an account of what an animal\u2019s nervous system is trying to do while undergoing conditioning: it is trying to form accurate long-term predictions, consistent with the limitations imposed by the way stimuli are represented and how the nervous system works. In other words, it suggests a normative account of classical conditioning in which long-term, instead of immediate, prediction is a key feature. The development of the TD model of classical conditioning is one instance in which the explicit goal was to model some of the details of animal learning behavior. In addition to its standing as an algorithm, then, TD learning is also the basis of this model of aspects of biological learning.\n\nAs we discuss in Chapter 15, TD learning has also turned out to underlie an in\ufb02uential model of the activity of neurons that produce dopamine, a chemical in the brain of mammals that is deeply involved in reward processing. These are instances in which reinforcement learning theory makes detailed contact with animal behavioral and neural data. We now turn to considering correspondences between reinforcement learning and animal behavior in instrumental conditioning experiments, the other major type of laboratory experiment studied by animal learning psychologists", "226fbf64-a3f1-457d-a361-b160fb4ebf01": "Denoising autoencoders are, in some sense, just MLPs trained to denoise.\n\nThe name \u201cdenoising autoencoder,\u201d however, refers to a model that is intended not merely to learn to denoise its input but to learn a good internal representation as a side effect of learning to denoise. This idea came much later . The learned representation may then be used to pretrain a deeper unsupervised network or a supervised network. Like sparse autoencoders, sparse coding, contractive autoencoders, and other regularized autoencoders, the motivation for DAEs was to allow the learning of a very high-capacity encoder while preventing the encoder and decoder from learning a useless identity function. Prior to the introduction of the modern DAE, Inayoshi and Kurita  explored some of the same goals with some of the same methods. Their approach minimizes reconstruction error in addition to a supervised objective while injecting noise in the hidden layer of a supervised MLP, with the objective to improve generalization by introducing the reconstruction error and the injected noise", "a000fbf1-db39-4c89-be78-8bd5b0b04dc6": "REGULARIZATION FOR DEEP LEARNING  https://www.deeplearningbook.org/contents/regularization.html    al  Figure 7.9: Illustration of the main idea of the tangent prop algorithm  and manifold tangent classifier , which both regularize the classifier output function f(a). Each curve represents the manifold for a different class, illustrated here as a one-dimensional manifold embedded in a two-dimensional space. On one curve, we have chosen a single point and drawn a vector that is tangent to the class manifold (parallel to and touching the manifold) and a vector that is normal to the class manifold (orthogonal to the manifold). In multiple dimensions there may be many tangent directions and many normal directions. We expect the classification function to change rapidly as it moves in the direction normal to the manifold, and not to change as it moves along the class manifold", "ef049f90-426c-4f53-b913-e7623d7ea249": "The lower-right corner is the extreme case of expected updates so deep that they run all the way to terminal states (or, in a continuing task, until discounting has reduced the contribution of any further rewards to a negligible level). This is the case of exhaustive search. Intermediate methods along this dimension include heuristic search and related methods that search and update up to a limited depth, perhaps selectively. There are also methods that are intermediate along the horizontal dimension. These include methods that mix expected and sample updates, as well as the possibility of methods that mix samples and distributions within a single update. The interior of the square is \ufb01lled in to represent the space of all such intermediate methods. A third dimension that we have emphasized in this book is the binary distinction between on-policy and o\u21b5-policy methods. In the former case, the agent learns the value function for the policy it is currently following, whereas in the latter case it learns the value function for the policy for a di\u21b5erent policy, often the one that the agent currently thinks is best. The policy generating behavior is typically di\u21b5erent from what is currently thought best because of the need to explore", "b2c82d15-5f9a-442c-8ea7-7a6c4cd3da1d": "Combine the two results into one expression that depends on a and P, and then use the chain rule again, this time on \u2713>x(s), noting that the derivative of the logistic function f(x) = 1/(1 + e\u2212x) is f(x)(1 \u2212 f(x)). \u21e4 Prior to this chapter, this book focused on action-value methods\u2014meaning methods that learn action values and then use them to determine action selections. In this chapter, on the other hand, we considered methods that learn a parameterized policy that enables actions to be taken without consulting action-value estimates. In particular, we have considered policy-gradient methods\u2014meaning methods that update the policy parameter on each step in the direction of an estimate of the gradient of performance with respect to the policy parameter. Methods that learn and store a policy parameter have many advantages. They can learn speci\ufb01c probabilities for taking the actions. They can learn appropriate levels of exploration and approach deterministic policies asymptotically.\n\nThey can naturally handle continuous action spaces. All these things are easy for policy-based methods but awkward or impossible for \"-greedy methods and for action-value methods in general", "986773fb-5442-4974-b893-d0d86b8d0fb7": "A very powerful and common type of mixture model is the Gaussian mixture model, in which the components p(x | c =?) are Gaussians. Each component has a separately parametrized mean po and covariance 5\u201c), Some mixtures can have more constraints. For example, the covariances could be shared across components via the constraint \u00a9 = &, Vi. As with a single Gaussian distribution, the mixture of Gaussians might constrain the covariance matrix for each component to be diagonal or isotropic.\n\nIn addition to the means and covariances, the parameters of a Gaussian mixture specify the prior probability a; = P(c = 1) given to each component i. The word \u201cprior\u201d indicates that it expresses the model\u2019s beliefs about c before it has observed x. By comparison, P(c | x) is a posterior probability, because it is computed after observation of x. A Gaussian mixture model is a universal approximator of densities, in the sense that any smooth density can be approximated with any specific nonzero amount of error by a Gaussian mixture model with enough components", "62c60022-a87f-4459-ac18-b2bcc0c97bed": "This is the operation needed to back-propagate error derivatives through a convolutional layer, so it is needed to train convolutional networks that have more than one hidden layer. This same operation is also needed if we wish to reconstruct the visible units from the hidden units . Reconstructing the visible units is an operation commonly used in the models described in part III of this book, such as autoencoders, RBMs, and sparse coding. Transpose convolution is necessary to construct convolutional versions of those models.\n\nLike the kernel gradient operation, this input gradient operation can sometimes be implemented using a convolution but in general requires a third operation to be implemented. Care must be taken to coordinate this transpose operation with the forward propagation. The size of the output that the transpose operation should return depends on the zero-padding policy and stride of the forward propagation operation, as well as the size of the forward propagation\u2019s output map. In some cases, multiple sizes of input to forward propagation can result in the same size of output map, so the transpose operation must be explicitly told what the size of the original input was", "b49bc3d3-65f3-4b10-84e5-1fe1c6bdef44": "In other academic communities, L? regularization is also known as ridge regression or Tikhonov regularization. We can gain some insight into the behavior of weight decay regularization by studying the gradient of the regularized objective function. To simplify the presentation, we assume no bias parameter, so @ is just w. Such a model has the following total objective function:  J(w;X,y) = Fw\" wt I(w:X,y), (7.2) with the corresponding parameter gradient  VwJ(w; X,y) =aw + Vwd(w;X,y). 7.3  To take a single gradient step to update the weights, we perform this update:  we w-e(awt Vw (w;X,y)). 7A  Written another way, the update is  w<\u00a2 (l\u2014ca)w\u2014 Vw (w; X,y).\n\n7.5  We can see that the addition of the weight decay term has modified the learning rule to multiplicatively shrink the weight vector by a constant factor on each step, just before performing the usual gradient update. This describes what happens in  https://www.deeplearningbook.org/contents/regularization.html    a single step", "7c7c066f-5be1-4eaa-a075-4c0bc4a517d6": "Under some additional mild conditions, A is guaranteed to have only one eigenvector with eigenvalue 1. The process thus converges to a stationary distribution, sometimes also called the equilibrium distribution. At convergence,  (0)  v =Av=v, (17.23)  and this same condition holds for every additional step.\n\nThis is an eigenvector equation. To be a stationary point, v must be an eigenvector with corresponding eigenvalue 1. This condition guarantees that once we have reached the stationary distribution, repeated applications of the transition sampling procedure do not change the distribution over the states of all the various Markov chains (although the transition operator does change each individual state, of course). If we have chosen T correctly, then the stationary distribution q will be equal to the distribution p we wish to sample from. We describe how to choose T\u2019 in section 17.4. Most properties of Markov chains with countable states can be generalized to continuous variables. In this situation, some authors call the Markov chain a Harris chain, but we use the term Markov chain to describe both conditions", "85b33109-8399-484d-a6ae-a24eec60a952": "Read and write commands are column commands because they sequentially transfer bits into or out of columns of the row bu\u21b5er; multiple bits can be transferred without re-opening the row. Read and write commands to the currently-open row can be carried out more quickly than accessing a di\u21b5erent row, which would involve additional row commands: precharge and activate; this is sometimes referred to as \u201crow locality.\u201d A memory controller maintains a memory transaction queue that stores memory-access requests from the processors sharing the memory system.\n\nThe controller has to process requests by issuing commands to the memory system while adhering to a large number of timing constraints. A controller\u2019s policy for scheduling access requests can have a large e\u21b5ect on the performance of the memory system, such as the average latency with which requests can be satis\ufb01ed and the throughput the system is capable of achieving. The simplest scheduling strategy handles access requests in the order in which they arrive by issuing all the commands required by the request before beginning to service the next one", "2761082e-e087-4a16-a666-e005f23060c7": "As this figure makes clear, an important aspect of this model is that there are no direct interactions between any two visible units or between any two hidden units (hence \u201crestricted\u201d; a general Boltzmann machine may have arbitrary connections). The restrictions on the RBM structure yield the nice properties  p(h | v) = Ip(hi| v) (16.11) and p(v | h) = Iip(v;| h)", "3a2794b6-a9a5-4a9b-a71b-fbe80c3b2774": "Because there are N \u2212 1 summations and multiplications of this kind, the total cost of evaluating the marginal p(xn) is O(NK2).\n\nThis is linear in the length of the chain, in contrast to the exponential cost of a naive approach. We have therefore been able to exploit the many conditional independence properties of this simple graph in order to obtain an ef\ufb01cient calculation. If the graph had been fully connected, there would have been no conditional independence properties, and we would have been forced to work directly with the full joint distribution. We now give a powerful interpretation of this calculation in terms of the passing of local messages around on the graph. From (8.52) we see that the expression for the marginal p(xn) decomposes into the product of two factors times the normalization constant We shall interpret \u00b5\u03b1(xn) as a message passed forwards along the chain from node xn\u22121 to node xn. Similarly, \u00b5\u03b2(xn) can be viewed as a message passed backwards along the chain to node xn from node xn+1", "8fd9af35-74b0-4415-935d-dca7b8f4014c": "The location of the minimum of this quadratic approximation therefore depends on O(W 2) parameters, and we should not expect to be able to locate the minimum until we have gathered O(W 2) independent pieces of information. If we do not make use of gradient information, we would expect to have to perform O(W 2) function evaluations, each of which would require O(W) steps. Thus, the computational effort needed to \ufb01nd the minimum using such an approach would be O(W 3). Now compare this with an algorithm that makes use of the gradient information. Because each evaluation of \u2207E brings W items of information, we might hope to \ufb01nd the minimum of the function in O(W) gradient evaluations. As we shall see, by using error backpropagation, each such evaluation takes only O(W) steps and so the minimum can now be found in O(W 2) steps", "5b158439-d04a-47e8-98db-5470917c5711": "A model-based algorithm selects actions by using a model to predict the consequences of possible courses of action in terms of future states and the reward signals expected to arise from those states. The simplest kind of planning is to compare the predicted consequences of collections of \u201cimagined\u201d sequences of decisions. Questions about whether or not animals use environment models, and if so, what are the models like and how are they learned, have played in\ufb02uential roles in the history of animal learning research. Some researchers challenged the then-prevailing stimulus-response (S\u2013R) view of learning and behavior, which corresponds to the simplest model-free way of learning policies, by demonstrating latent learning. In the earliest latent learning experiment, two groups of rats were run in a maze.\n\nFor the experimental group, there was no reward during the \ufb01rst stage of the experiment, but food was suddenly introduced into the goal box of the maze at the start of the second stage. For the control group, food was in the goal box throughout both stages. The question was whether or not rats in the experimental group would have learned anything during the \ufb01rst stage in the absence of food reward", "4b78104a-0cf4-4763-844c-0b2f494cf674": "As decision-time planning algorithms, rollout algorithms make immediate use of these action-value estimates, then discard them. This makes rollout algorithms relatively simple to implement because there is no need to sample outcomes for every state-action pair, and there is no need to approximate a function over either the state space or the state-action space. described in Section 4.2 tells us that given any two policies \u21e1 and \u21e10 that are identical except that \u21e10(s) = a 6= \u21e1(s) for some state s, if q\u21e1(s, a) \u2265 v\u21e1(s), then policy \u21e10 is as good as, or better, than \u21e1. Moreover, if the inequality is strict, then \u21e10 is in fact better than \u21e1. This applies to rollout algorithms where s is the current state and \u21e1 is the rollout policy. Averaging the returns of the simulated trajectories produces estimates of q\u21e1(s, a0) for each action a0 2 A(s). Then the policy that selects an action in s that maximizes these estimates and thereafter follows \u21e1 is a good candidate for a policy that improves over \u21e1", "0026f819-b68a-4382-ac5f-acfc3bafd043": "The KL-divergence term can then be interpreted as regularizing \u03c6, encouraging the approximate posterior to be close to the prior p\u03b8(z).\n\nThis yields a second version of the SGVB estimator \ufffdLB(\u03b8, \u03c6; x(i)) \u2243 L(\u03b8, \u03c6; x(i)), corresponding to eq. (3), which typically has less variance than the generic estimator: \ufffdLB(\u03b8, \u03c6; x(i)) = \u2212DKL(q\u03c6(z|x(i))||p\u03b8(z)) + 1 where the minibatch XM = {x(i)}M i=1 is a randomly drawn sample of M datapoints from the full dataset X with N datapoints. In our experiments we found that the number of samples L per datapoint can be set to 1 as long as the minibatch size M was large enough, e.g. M = 100. Derivatives \u2207\u03b8,\u03c6 \ufffdL(\u03b8; XM) can be taken, and the resulting gradients can be used in conjunction with stochastic optimization methods such as SGD or Adagrad", "418cc32d-2134-4233-9496-99645bb04d79": "The VE is not obse policy together completely determine the probability distrib Assume for the moment that the state, action, and rewar for any \ufb01nite sequence \u03be = \u03c60, a0, r1, . , rk, \u03c6k, there is a w sibly zero) of it occuring as the initial portion of a traject P(\u03be) = Pr{\u03c6(S0) = \u03c60, A0 = a0, R1 = r1, . , Rk = rk, \u03c6(Sk) then is a complete characterization of a source of data trajec everything about the statistics of the data, but it is still les particular, the VE and BE objectives are readily computed fr Section 3, but these cannot be determined from P alone. The problem can be seen in very simple, POMDP-like exam data produced by two di\u21b5erent MDPs is identical in every re In such a case the BE is literally not a function of the data estimate it from data. One of the simplest examples is the pa These MDPs have only one action (or, equivalently, no actions chains", "d7638361-bb8b-440b-bb2c-ccf7f5128543": "The speci\ufb01cation of their interface de\ufb01nes a particular task: the actions are the choices made by the agent; the states are the basis for making the choices; and the rewards are the basis for evaluating the choices. Everything inside the agent is completely known and controllable by the agent; everything outside is incompletely controllable but may or may not be completely known. A policy is a stochastic rule by which the agent selects actions as a function of states. The agent\u2019s objective is to maximize the amount of reward it receives over time. When the reinforcement learning setup described above is formulated with well de\ufb01ned transition probabilities it constitutes a Markov decision process (MDP). A \ufb01nite MDP is an MDP with \ufb01nite state, action, and (as we formulate it here) reward sets. Much of the current theory of reinforcement learning is restricted to \ufb01nite MDPs, but the methods and ideas apply more generally. The return is the function of future rewards that the agent seeks to maximize (in expected value)", "0572370f-ba7c-4017-95c8-4b9c3b9b1df1": "So far we have discussed only a single example.\n\nOverall, unregularized maximum likelihood will drive the model to learn parameters that drive the softmax to predict the fraction of counts of each outcome observed in the training set:  mm a 1 YD=i,aG) =x  MU (6.31) j=l  softmax(z(a;0)); ~ aG=a  Because maximum likelihood is a consistent estimator, this is guaranteed to happen as long as the model family is capable of representing the training distribution. In practice, limited model capacity and imperfect optimization will mean that the model is only able to approximate these fractions. Many objective functions other than the log-likelihood do not work as well tadoad ivi c at a \u201cc W a at c at atoa 7 4 laws  https://www.deeplearningbook.org/contents/mlp.html    WILLL LIKE SOLLINAX LULCLION. OPCCICALLy, OVJECLIVE LUILCLLOUS LUabt GO LObt USE a 1U% LO undo the exp of the softmax fail to learn when the argument to the \u20acxp becomes very negative, causing the gradient to vanish", "c2a870a1-eea9-4940-9f9b-bc6557396406": "The write vector is an interpolation between the previous read weight (prefer \u201cthe last used location\u201d) and the previous least-used weight (prefer \u201crarely used location\u201d).\n\nThe interpolation parameter is the sigmoid of a hyperparameter a. The least-used weight w'% is scaled according to usage weights w\u2122, in which any dimension remains at 1 if smaller than the n-th smallest element in the vector and 0 otherwise. we = wy tw + we w? = softmax(cosine(k;, M,(z))) w? = o(a)wi_, + (1 -o(a))w!,  wi! = 1w\u00a5(i)<m(w'n) Where m(w?, 7) is the n-th smallest element in vector w/\u2019. Finally, after the least used memory location, indicated by wi\", is set to zero, every memory row is updated:  M, (i) = My_, (i) + w}? (i) ky, Vi  Meta Networks  Meta Networks , short for MetaNet, is a meta-learning model with architecture and training process designed for rapid generalization across tasks", "b115f11e-bcf0-4e28-a369-3cf7cab3933e": "Using d-separation, we note that there is a unique path from any xi to any other xj\u0338=i and that this path is tail-to-tail with respect to the observed node \u00b5. Every such path is blocked and so the observations D = {x1, . .\n\n, xN} are independent given \u00b5, so that However, if we integrate over \u00b5, the observations are in general no longer independent Here \u00b5 is a latent variable, because its value is not observed. Another example of a model representing i.i.d. data is the graph in Figure 8.7 corresponding to Bayesian polynomial regression. Here the stochastic nodes correspond to {tn}, w and \ufffdt. We see that the node for w is tail-to-tail with respect to the path from\ufffdt to any one of the nodes tn and so we have the following conditional independence property Thus, conditioned on the polynomial coef\ufb01cients w, the predictive distribution for \ufffdt is independent of the training data {t1, . , tN}", "a5ba9a83-058c-46bc-87cd-636eb59ed478": "Using the result (3.49) for linear regression models, we see that the posterior distribution for the weights is again Gaussian and takes the form where the mean and covariance are given by where \u03a6 is the N \u00d7 M design matrix with elements \u03a6ni = \u03c6i(xn), and A = diag(\u03b1i).\n\nNote that in the speci\ufb01c case of the model (7.78), we have \u03a6 = K, where K is the symmetric (N + 1) \u00d7 (N + 1) kernel matrix with elements k(xn, xm). The values of \u03b1 and \u03b2 are determined using type-2 maximum likelihood, also known as the evidence approximation, in which we maximize the marginal likeliSection 3.5 hood function obtained by integrating out the weight parameters Because this represents the convolution of two Gaussians, it is readily evaluated to Exercise 7.10 give the log marginal likelihood in the form where t = (t1, . , tN)T, and we have de\ufb01ned the N \u00d7 N matrix C given by Our goal is now to maximize (7.85) with respect to the hyperparameters \u03b1 and \u03b2. This requires only a small modi\ufb01cation to the results obtained in Section 3.5 for the evidence approximation in the linear regression model", "243967cd-3a8f-4247-904f-54fcc963b69d": "If player black places a stone on X, the three white stones are captured and taken o\u21b5 the board (Figure 16.5 middle). However, if player white were to place a stone on point X \ufb01rst, then the possibility of this capture would be blocked (Figure 16.5 right). Other rules are needed to prevent in\ufb01nite capturing/re-capturing loops. The game ends when neither player wishes to place another stone. These rules are simple, but they produce a very complex game that has had wide Methods that produce strong play for other games, such as chess, have not worked as well for Go. The search space for Go is signi\ufb01cantly larger than that of chess because Go has a larger number of legal moves per position than chess (\u21e1 250 versus \u21e1 35) and Go games tend to involve more moves than chess games (\u21e1 150 versus \u21e1 80).\n\nBut the size of the search space is not the major factor that makes Go so di\ufb03cult. Exhaustive search is infeasible for both chess and Go, and Go on smaller boards (e.g., 9 \u21e5 9) has proven to be exceedingly di\ufb03cult as well", "10751d66-f8f6-4bd7-85c3-4afa42de5fcb": "At each step, the Q function is updated as follows: where r is the received reward and \u03b7 is the learning rate. The update is made online and does not require any prior model of velocity field, the red and blue colors indicate regions of large upward and downward flow, res indicate regions of high and low temperature, respectively. Notice that the hot and cold regions cell, in agreement with the basic physics of convection. (C) The force-body diagram of flight with The figure also shows the bank angle \u03bc (blue), the angle of attack \u03b1 (green), and the glide angle accessible by controlling the angle of attack. At small angles of attack, the glider moves fast but a sinks more slowly. If the angle of attack is too high, at about 16\u00b0, the glider stalls, leading to a s fixed angle of attack for most of the simulations (Results, Control over the Angle of Attack).\n\nsimulated cube of air: in red (blue) is a region of large upward (downward) \ufb02ow. Right: diagram of powerless \ufb02ight showing bank angle \u00b5 and angle of attack \u21b5. Adapted with permission From PNAS vol. 113(22), p", "64d56559-499b-48b1-9561-762c1a11c482": "OPTIMIZATION FOR TRAINING DEEP MODELS  Many existing research directions are aimed at finding good initial points for problems that have difficult global structure, rather than at developing algorithms that use nonlocal moves. Gradient descent and essentially all learning algorithms that are effective for training neural networks are based on making small local moves. The previous sections have primarily focused on how the correct direction of these local moves can be difficult to compute.\n\nWe may be able to compute some properties of the objective function, such as its gradient, only approximately, with bias or variance in our estimate of the correct direction. In these cases, local descent may or may not define a reasonably short path to a valid solution, but we are not actually able to follow the local descent path. The objective function may have issues, such as poor conditioning or discontinuous gradients, causing the region where the gradient provides a good model of the objective function to be very small. In these cases, local descent with steps of size \u00ab may define a reasonably short path to the solution, but we are only able to compute the local descent direction with steps of size 6 < e. In these cases, local descent may define a path to the solution, but the path contains many steps, so following it incurs a high computational cost", "fcc838fb-6530-48e1-833c-711b2989e4f1": "We now turn to the question of how these averages can be computed in a computationally e\ufb03cient manner, in particular, with constant memory To simplify notation we concentrate on a single action.\n\nLet Ri now denote the reward received after the ith selection of this action, and let Qn denote the estimate of its action value after it has been selected n \u2212 1 times, which we can now write simply as The obvious implementation would be to maintain a record of all the rewards and then perform this computation whenever the estimated value was needed. However, if this is done, then the memory and computational requirements would grow over time as more rewards are seen. Each additional reward would require additional memory to store it and additional computation to compute the sum in the numerator. As you might suspect, this is not really necessary. It is easy to devise incremental formulas for updating averages with small, constant computation required to process each new reward. Given Qn and the nth reward, Rn, the new average of all n rewards can be computed by which holds even for n = 1, obtaining Q2 = R1 for arbitrary Q1. This implementation requires memory only for Qn and n, and only the small computation (2.3) for each new reward", "36d908bd-482a-44c6-a0f2-dd84a609a031": "Experimentally, deep autoencoders yield much better compression than corre- sponding shallow or linear autoencoders .\n\nNo naw ne Abeba nee fan bentntn en 2 Tan 4--bnnnn dn tn be we A AIL- Henn tn  https://www.deeplearningbook.org/contents/autoencoders.html    fA COLLUMOM SLLALEY, 1UL ULLAL a UCCP AULUCLICUUEL 15 LU ICCULLy PLELLaliL the deep architecture by training a stack of shallow autoencoders, so we often encounter shallow autoencoders, even when the ultimate goal is to train a deep autoencoder. 14.4 Stochastic Encoders and Decoders  Autoencoders are just feedforward networks. The same loss functions and output unit types that can be used for traditional feedforward networks are also used for autoencoders", "749a9413-e193-488a-9d77-dc499b5515fe": "The model contains a binary latent variable z that indicates which component of the mixture is responsible for generating the corresponding data point. Thus the model is speci\ufb01ed in terms of a joint distribution p(x, z) (14.2) and the corresponding density over the observed variable x is obtained by marginalizing over the latent variable In the case of our Gaussian mixture example, this leads to a distribution of the form with the usual interpretation of the symbols. This is an example of model combination.\n\nFor independent, identically distributed data, we can use (14.3) to write the marginal probability of a data set X = {x1, . , xN} in the form Thus we see that each observed data point xn has a corresponding latent variable zn. Now suppose we have several different models indexed by h = 1, . , H with prior probabilities p(h). For instance one model might be a mixture of Gaussians and another model might be a mixture of Cauchy distributions. The marginal distribution over the data set is given by This is an example of Bayesian model averaging", "04705862-9f95-41d9-b1ea-83c69b392df1": "The animal learns to increase its tendency to produce rewarded behavior and to decrease its tendency to produce penalized behavior. The reinforcing stimulus is said to be contingent on the animal\u2019s behavior, whereas in classical conditioning it is not (although it is di\ufb03cult to remove all behavior contingencies in a classical conditioning experiment). Instrumental conditioning experiments are like those that inspired Thorndike\u2019s Law of E\u21b5ect that 1What control means for us is di\u21b5erent from what it typically means in animal learning theories; there the environment controls the agent instead of the other way around.\n\nSee our comments on terminology at the end of this chapter. we brie\ufb02y discuss in Chapter 1. Control is at the core of this form of learning, which corresponds to the operation of reinforcement learning\u2019s policy-improvement algorithms. Thinking of classical conditioning in terms of prediction, and instrumental conditioning in terms of control, is a starting point for connecting our computational view of reinforcement learning to animal learning, but in reality, the situation is more complicated than this. There is more to classical conditioning than prediction; it also involves action, and so is a mode of control, sometimes called Pavlovian control. Further, classical and instrumental conditioning interact in interesting ways, with both sorts of learning likely being engaged in most experimental situations", "ac900452-07e6-4233-a269-1258b3d51292": "We can think of initializing the parameters 0 to @ as being similar to imposing a Gaussian prior p(@) with mean 0. From this point of view, it makes sense to choose 09 to be near 0. This prior says that it is more likely that units do not interact with each other than that they do interact. Units interact only if the likelihood term of the objective function expresses a strong preference for them to interact. On the other hand, if we initialize 09 to large values, then our prior specifies which units should interact with each other, and how they should interact", "6556e67c-9686-4460-bcff-2d2f471468b4": "The G\u00e2teaux derivative of J at q, if exists, is de\ufb01ned as : The above notions allow us to de\ufb01ne gradient descent applied to the functional J. Concretely, we can de\ufb01ne a linear approximation to J(q) around a given q0: Once the functional gradient is de\ufb01ned as above, the remaining problem of the optimization is then about how to obtain the in\ufb02uence function \u03c8q given the functional J(q). In some cases the in\ufb02uence function as de\ufb01ned in Equation 7.3 is not directly tractable and approximations are needed. Chu et al. developed a variational approximation method applied when J is convex (which is the case in Equation 3.2 when D is convex w.r.t q).\n\nConcretely, with the convex conjugate of J de\ufb01ned as J\u2217(\u03d5) = suph Eh \u2212J(h), it can be shown under mild conditions that the in\ufb02uence function for J at q is: where C(T ) is the space of continuous functions T \u2192 R. We thus can approximate the in\ufb02uence function by parameterizing it as a neural network and training the network to maximize the objective Eq \u2212J\u2217(\u03d5)", "5afcd738-bbb0-4b0a-bb01-e2bbbc76a6c2": "(7.59) de{0,1}\" To see that the weight scaling rule is exact, we can simplify Psysemble: Pensemble(\u00a5 =Y | v) = Qn Il Ply =y | Vv; d) (7.60) de{0,1}\u201d 260  https://www.deeplearningbook.org/contents/regularization.html       CHAPFER-7REGUEARIZAFION FOR-DEEP LEARNING  = op Il softmax (WT! (d@ v) +b), 7.61) de{0,1}\u201d W,/(d b \u2014 on exp (W,/. (d\u00a9 v) + by) 7.62) de{0,1}\" doy EXP (wi. (d Ov) + by) *\\/TMacto.y\" exp (W,l(d Ov) +by) 7.63)  gn Tlaefo.1}\" Ly exp (wy", "e0b7e1ab-6013-4613-baa2-26bb501de649": "negative ISIs); it increases to a maximum at a positive ISI where conditioning is most e\u21b5ective; and it then decreases to zero after an interval that varies widely with response systems. The precise shape of this dependency for the TD model depends on the values of its parameters and details of the stimulus representation, but these basic features of ISI-dependency are core properties of the TD model. sults, however, the TD model predicts (with the presence representation and more complex representations as well) that blocking is reversed if the blocked stimulus is moved earlier With the TD model, an earlier predictive stimulus takes precedence over a later predictive stimulus because, like all the prediction methods described in this book, the TD model is based on the backing-up or bootstrapping idea: updates to associative strengths shift the strengths at a particular state toward the strength at later states. Another consequence of bootstrapping is that the TD model provides an account of higherorder conditioning, a feature of classical conditioning that is beyond the scope of the Rescorla-Wagner and similar models.\n\nAs we described above, higher-order conditioning is the phenomenon in which a previously-conditioned CS can act as a US in conditioning another initially neutral stimulus", "e6b31690-6420-49d0-b604-be7cfb4dbd89": "The most successful way to do this for ANNs with hidden layers (provided the units have di\u21b5erentiable activation functions) is the backpropagation algorithm, which consists of alternating forward and backward passes through the network. Each forward pass computes the activation of each unit given the current activations of the network\u2019s input units. After each forward pass, a backward pass e\ufb03ciently computes a partial derivative for each weight. (As in other stochastic gradient learning algorithms, the vector of these partial derivatives is an estimate of the true gradient.) In Section 15.10 we discuss methods for training ANNs with hidden layers that use reinforcement learning principles instead of backpropagation. These methods are less e\ufb03cient than the backpropagation algorithm, but they may be closer to how real neural networks learn.\n\nThe backpropagation algorithm can produce good results for shallow networks having 1 or 2 hidden layers, but it may not work well for deeper ANNs. In fact, training a network with k + 1 hidden layers can actually result in poorer performance than training a network with k hidden layers, even though the deeper network can represent all the functions that the shallower network can . Explaining results like these is not easy, but several factors are important", "8484ac8d-372b-4476-ab9c-209189e3b565": "We see that the relative size of the quality and sparsity terms determines whether a particular basis vector will be pruned from the model or not.\n\nA more complete analysis , based on the second derivatives of the marginal likelihood, con\ufb01rms these solutions are indeed the unique maxima of \u03bb(\u03b1i). Exercise 7.16 Note that this approach has yielded a closed-form solution for \u03b1i, for given values of the other hyperparameters. As well as providing insight into the origin of sparsity in the RVM, this analysis also leads to a practical algorithm for optimizing the hyperparameters that has signi\ufb01cant speed advantages. This uses a \ufb01xed set of candidate basis vectors, and then cycles through them in turn to decide whether each vector should be included in the model or not. The resulting sequential sparse Bayesian learning algorithm is described below. Sequential Sparse Bayesian Learning Algorithm 1. If solving a regression problem, initialize \u03b2. 2", "35f0efd8-cebb-40f5-ace4-1a4b2e86fa5c": "The cats selected actions from those that they instinctively perform in their current situation, which Thorndike called their \u201cinstinctual impulses.\u201d First placed in a puzzle box, a cat instinctively scratches, claws, and bites with great energy: a cat\u2019s instinctual responses to \ufb01nding itself in a con\ufb01ned space.\n\nSuccessful actions are selected from these and not from every possible action or activity. This is like the feature of our formalism where the action selected from a state s belongs to a set of admissible actions, A(s). Specifying these sets is an important aspect of reinforcement learning because it can radically simplify learning. They are like an animal\u2019s instinctual impulses. On the other hand, Thorndike\u2019s cats might have been exploring according to an instinctual context-speci\ufb01c ordering over actions rather than by just selecting from a set of instinctual impulses. This is another way to make reinforcement learning easier. Among the most prominent animal learning researchers in\ufb02uenced by the Law of E\u21b5ect were Clark Hull  and B. F. Skinner . At the center of their research was the idea of selecting behavior on the basis of its consequences", "30db8d62-e435-4914-9472-b31cc46e8cb0": "Character-level convolutional networks for text classi\ufb01cation. Advances in Neural Information Processing Systems, 28:649\u2013657. Zhengli Zhao, Dheeru Dua, and Sameer Singh. 2017. Generating natural adversarial examples. Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. 2020. Random erasing data augmentation. In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence, volume 34, pages 13001\u201313008. Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and Jingjing Liu. 2020. Freelb: Enhanced adversarial training for natural language understanding. In ICLR.\n\nWe train our models on NVIDIA 2080ti and NVIDIA V-100 gpus. Supervised experiments take 20 minutes, and semi-supervised experiments take two hours. The BERT-base model has 100M parameters. We use the same hyperaparameter across all datasets, and so only use the validation set to \ufb01nd the best model checkpoint", "320be9b3-168b-4463-b983-77d488f5003c": "Predictability minimization is only a regularizer that encourages the hidden units of a neural network to be statistically independent while they accomplish some other task; it is not a primary training criterion. 2) The nature of the competition is different. In predictability minimization, two networks\u2019 outputs are compared, with one network trying to make the outputs similar and the other trying to make the outputs different. The output in question is a single scalar. In GANs, one network produces a rich, high dimensional vector that is used as the input to another network, and attempts to choose an input that the other network does not know how to process. 3) The speci\ufb01cation of the learning process is different. Predictability minimization is described as an optimization problem with an objective function to be minimized, and learning approaches the minimum of the objective function.\n\nGANs are based on a minimax game rather than an optimization problem, and have a value function that one agent seeks to maximize and the other seeks to minimize. The game terminates at a saddle point that is a minimum with respect to one player\u2019s strategy and a maximum with respect to the other player\u2019s strategy", "cb9843e1-1680-4c23-9a7c-6bd92713d37d": "The corresponding conjugate prior is therefore given by a Gaussian distribution of the form p(w) = N(w|m0, S0) (3.48) Next we compute the posterior distribution, which is proportional to the product of the likelihood function and the prior. Due to the choice of a conjugate Gaussian prior distribution, the posterior will also be Gaussian. We can evaluate this distribution by the usual procedure of completing the square in the exponential, and then \ufb01nding the normalization coef\ufb01cient using the standard result for a normalized Gaussian. However, we have already done the necessary work in deriving the genExercise 3.7 Note that because the posterior distribution is Gaussian, its mode coincides with its mean. Thus the maximum posterior weight vector is simply given by wMAP = mN. If we consider an in\ufb01nitely broad prior S0 = \u03b1\u22121I with \u03b1 \u2192 0, the mean mN of the posterior distribution reduces to the maximum likelihood value wML given by (3.15). Similarly, if N = 0, then the posterior distribution reverts to the prior", "6ba39605-c094-42bb-8056-a4291c7c4a9a": "An important example concerns situations in which the conditional distribution p(t|x) is multimodal, as often arises in the solution of inverse problems.\n\nHere we consider brie\ufb02y one simple Section 5.6 generalization of the squared loss, called the Minkowski loss, whose expectation is given by which reduces to the expected squared loss for q = 2. The function |y \u2212 t|q is plotted against y \u2212 t for various values of q in Figure 1.29. The minimum of E is given by the conditional mean for q = 2, the conditional median for q = 1, and the conditional mode for q \u2192 0. Exercise 1.27 In this chapter, we have discussed a variety of concepts from probability theory and decision theory that will form the foundations for much of the subsequent discussion in this book. We close this chapter by introducing some additional concepts from the \ufb01eld of information theory, which will also prove useful in our development of pattern recognition and machine learning techniques. Again, we shall focus only on the key concepts, and we refer the reader elsewhere for more detailed discussions  . We begin by considering a discrete random variable x and we ask how much information is received when we observe a speci\ufb01c value for this variable", "44584230-f7e7-46dd-9058-647f7120a03b": "Here the learning algorithm is not given examples of optimal outputs, in contrast to supervised learning, but must instead discover them by a process of trial and error. Typically there is a sequence of states and actions in which the learning algorithm is interacting with its environment. In many cases, the current action not only affects the immediate reward but also has an impact on the reward at all subsequent time steps.\n\nFor example, by using appropriate reinforcement learning techniques a neural network can learn to play the game of backgammon to a high standard . Here the network must learn to take a board position as input, along with the result of a dice throw, and produce a strong move as the output. This is done by having the network play against a copy of itself for perhaps a million games. A major challenge is that a game of backgammon can involve dozens of moves, and yet it is only at the end of the game that the reward, in the form of victory, is achieved. The reward must then be attributed appropriately to all of the moves that led to it, even though some moves will have been good ones and others less so. This is an example of a credit assignment problem", "b4448b61-beb1-4b17-a91b-8e1c78abf3bb": "Hidden units that are not differentiable are usually nondifferentiable at only a small number of points.\n\nIn general, a function g(z) has a left derivative defined by the slope of the function immediately to the left of z and a right derivative defined by the slope of the function immediately to the right of z A function is differentiable at z only if both the left derivative and the right derivative are defined and equal to each other. The functions used in the context of neural networks usually have defined left derivatives and defined right derivatives. In the case of g(z) = max{0, z}, the left derivative at z = 0 is 0, and the right derivative is 1. Software implementations of neural network training usually return one of the one-sided derivatives rather than reporting that the derivative is undefined or raising an error. This may be heuristically justified by observing that gradient- based optimization on a digital computer is subject to numerical error anyway. When a function is asked to evaluate g(0), it is very unlikely that the underlying value truly was 0. Instead, it was likely to be some small value \u00a2\u20ac that was rounded to 0", "999d8efc-f56c-404d-b268-b55559e2167e": "They observed how neurons in the cat\u2019s brain responded to images projected in precise locations on a screen in front of the cat. Their great discovery was that neurons in the early visual system responded most strongly to very specific patterns of light, such as precisely oriented bars, but responded hardly at all to other patterns. Their work helped to characterize many aspects of brain function that are beyond the scope of this book. From the point of view of deep learning, we can focus on a simplified, cartoon view of brain function. In this simplified view, we focus on a part of the brain called V1, also known as the primary visual cortex. V1 is the first area of the brain that begins to  358  CHAPTER 9. CONVOLUTIONAL NETWORKS  perform significantly advanced processing of visual input. In this cartoon view, images are formed by light arriving in the eye and stimulating the retina, the light-sensitive tissue in the back of the eye. The neurons in the retina perform some simple preprocessing of the image but do not substantially alter the way it is represented", "402e8197-198d-4a18-bfb7-283212d03019": "In International Conference on Machine Learning, pages 1645\u20131654. PMLR. Natasha Jaques, Judy Hanwen Shen, Asma Ghandeharioun, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard. 2020. Human-centric dialog training via of\ufb02ine reinforcement learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3985\u20134003. Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. 2020. Is bert really robust? a strong baseline for natural language attack on text classi\ufb01cation and entailment.\n\nIn Proceedings of the AAAI conference on arti\ufb01cial intelligence. Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Sha\ufb01q Joty, Richard Socher, and Nazneen Fatema Rajani. 2020. Gedi: Generative discriminator guided sequence generation. arXiv preprint arXiv:2009.06367", "24b40421-2f3d-44bb-ab76-e80ee2f8cd45": "Consider a model de\ufb01ned in terms of a linear combination of M \ufb01xed basis functions given by the elements of the vector \u03c6(x) so that where x is the input vector and w is the M-dimensional weight vector. Now consider a prior distribution over w given by an isotropic Gaussian of the form governed by the hyperparameter \u03b1, which represents the precision (inverse variance) of the distribution. For any given value of w, the de\ufb01nition (6.49) de\ufb01nes a particular function of x. The probability distribution over w de\ufb01ned by (6.50) therefore induces a probability distribution over functions y(x). In practice, we wish to evaluate this function at speci\ufb01c values of x, for example at the training data points x1, . , xN. We are therefore interested in the joint distribution of the function values y(x1), . , y(xN), which we denote by the vector y with elements yn = y(xn) for n = 1, . , N. From (6.49), this vector is given by where \u03a6 is the design matrix with elements \u03a6nk = \u03c6k(xn)", "4593ed9b-9135-4fc1-b970-3ecc6b9c44f9": "In addition, some papers measure the quality of GAN outputs by a visual Turing test. In these tests, the study asks two experts to distinguish between real and artificial images in medical image tasks such as skin lesion classification and liver cancer detection.\n\nTable 5 shows that the first and second experts were only able to correctly label 62.5% and 58.6% of the GAN-gener- ated liver lesion images as fake. Labeling images as fake refers to their origin coming  from the generator rather than an actual liver lesion image (Table 6; Fig. 23). Table 5 Results of \u2018Visual Turing Test\u2019 on DCGAN-generated liver lesion images presented by Frid-Adar et al. Classification accuracy Is ROI real? Real (%) Synthetic (%) Total score Total score Expert 1 78 775 235\\302 =77.8% 189\\302 = 62.5% Expert 2 69.2 69.2 209\\302 = 69.2% 177\\302 = 58.6%  Table 6 Results of \u2018Visual Turing Test\u2019 on different DCGAN- and WGAN \u2014generated brain tumor MR images presented by Han et al", "1894b3c8-1c72-4551-af44-3e92a286f44d": "We have already seen sigmoid units as output units, used to predict the probability that a binary variable is 1. Unlike piecewise linear units, sigmoidal units saturate across most of their domain\u2014they saturate to a high value when z is very positive, saturate to a low value when z is very negative, and are only strongly sensitive to their input when z is near 0. The widespread saturation of sigmoidal units can make gradient-based learning very difficult. For this reason, their use as hidden units in feedforward networks is now discouraged. Their use as output units is compatible with the use of gradient-based learning when an  https://www.deeplearningbook.org/contents/mlp.html    appropriate cost function can undo the saturation of the sigmoid in the output ayer. When a sigmoidal activation function must be used, the hyperbolic tangent  activation function typically performs better than the logistic sigmoid. It resembles  the identity function more closely, in the sense that tanh(0) = 0 while o(0) = 5", "295977d2-c4f3-4c78-a613-08c846369f12": "DEEP GENERATIVE MODELS  https://www.deeplearningbook.org/contents/generative_models.html    Figure 20.12: Illustration of clamping the right half of the image and running the Markov chain by resampling only the left half at each step. These samples come from a GSN trained to reconstruct MNIST digits at each time step using the walk-back procedure. 20.12 Generative Stochastic Networks  Generative stochastic networks, or GSNs  are generaliza- tions of denoising autoencoders that include latent variables h in the generative Markov chain, in addition to the visible variables (usually denoted x). A GSN is parametrized by two conditional probability distributions that specify one step of the Markov chain:  1. p(x(*) | h*)) tells how to generate the next visible variable given the current latent state.\n\nSuch a \u201creconstruction distribution\u201d is also found in denoising autoencoders, RBMs, DBNs and DBMs. 2", "b67314e1-8d36-45cf-8adb-516e2eab502c": "When x is continuous, we use the same definition of information by analogy, but some of the properties from the discrete case are lost. For example, an event with unit density still has zero information, despite not being an event that is guaranteed to occur. Self-information deals only with a single outcome. We can quantify the amount of uncertainty in an entire probability distribution using the Shannon entropy,  https://www.deeplearningbook.org/contents/prob.html    A(x) = Ex~P|1(x)| = \u2014Ex~P , (3.49) also denoted H(P). In other words, the Shannon entropy of a distribution is the expected amount of information in an event drawn from that distribution. It gives a lower bound on the number of bits (if the logarithm is base 2, otherwise the units are different) needed on average to encode symbols drawn from a distribution P. Distributions that are nearly deterministic (where the outcome is nearly certain) have low entropy; distributions that are closer to uniform have high entropy. See figure 3.5 for a demonstration", "a9a50d3d-afe1-4cfc-9d16-a3675fa370ae": "Werbos  argued explicitly for greater interrelation of dynamic programming and learning methods and for dynamic programming\u2019s relevance to understanding neural and cognitive mechanisms. For us the full integration of dynamic programming methods with online learning did not occur until the work of Chris Watkins in 1989, whose treatment of reinforcement learning using the MDP formalism has been widely adopted.\n\nSince then these relationships have been extensively developed by many researchers, most particularly by Dimitri Bertsekas and John Tsitsiklis , who coined the term \u201cneurodynamic programming\u201d to refer to the combination of dynamic programming and arti\ufb01cial neural networks. Another term currently in use is \u201capproximate dynamic programming.\u201d These various approaches emphasize di\u21b5erent aspects of the subject, but they all share with reinforcement learning an interest in circumventing the classical shortcomings of dynamic programming. We consider all of the work in optimal control also to be, in a sense, work in reinforcement learning. We de\ufb01ne a reinforcement learning method as any e\u21b5ective way of solving reinforcement learning problems, and it is now clear that these problems are closely related to optimal control problems, particularly stochastic optimal control problems such as those formulated as MDPs. Accordingly, we must consider the solution methods of optimal control, such as dynamic programming, also to be reinforcement learning methods", "b8e339bf-e14a-4aac-97bf-bc7d9a673c36": "Let h(s) denote the probability that an episode begins in each state s, and let \u2318(s) denote the number of time steps spent, on average, in state s in a single episode. Time is spent in a state s if episodes start in s, or if transitions are made into s from a preceding state \u00afs in which time is spent: This system of equations can be solved for the expected number of visits \u2318(s). The on-policy distribution is then the fraction of time spent in each state normalized to sum to one: This is the natural choice without discounting. If there is discounting (\u03b3 < 1) it should be treated as a form of termination, which can be done simply by including a factor of \u03b3 in the second term of (9.2). The two cases, continuing and episodic, behave similarly, but with approximation they must be treated separately in formal analyses, as we will see repeatedly in this part of the book. This completes the speci\ufb01cation of the learning objective. It is not completely clear that the VE is the right performance objective for reinforcement learning. Remember that our ultimate purpose\u2014the reason we are learning a value function\u2014is to \ufb01nd a better policy", "1857d3b3-ef92-408d-9f2a-ab7c9e2115d6": "In order to obtain a practical algorithm, we need to constrain the factors \ufffdfi(\u03b8) in some way, and in particular we shall assume that they come from the exponential family. The product of the factors will therefore also be from the exponential family and so can be described by a \ufb01nite set of suf\ufb01cient statistics. For example, if each of the \ufffdfi(\u03b8) is a Gaussian, then the overall approximation q(\u03b8) will also be Gaussian. Ideally we would like to determine the \ufffdfi(\u03b8) by minimizing the Kullback-Leibler divergence between the true posterior and the approximation given by Note that this is the reverse form of KL divergence compared with that used in variational inference. In general, this minimization will be intractable because the KL divergence involves averaging with respect to the true distribution. As a rough approximation, we could instead minimize the KL divergences between the corresponding pairs fi(\u03b8) and \ufffdfi(\u03b8) of factors. This represents a much simpler problem to solve, and has the advantage that the algorithm is noniterative.\n\nHowever, because each factor is individually approximated, the product of the factors could well give a poor approximation", "7644ba59-675f-4cb7-9406-97ee7121c191": "Are there algorithms that avoid maximization bias? To start, consider a bandit case in which we have noisy estimates of the value of each of many actions, obtained as sample averages of the rewards received on all the plays with each action. As we discussed above, there will be a positive maximization bias if we use the maximum of the estimates as an estimate of the maximum of the true values. One way to view the problem is that it is due to using the same samples (plays) both to determine the maximizing action and to estimate its value. Suppose we divided the plays in two sets and used them to learn two independent estimates, call them Q1(a) and Q2(a), each an estimate of the true value q(a), for all a 2 A. We could then use one estimate, say Q1, to determine the maximizing action A\u21e4 = argmaxa Q1(a), and the other, Q2, to provide the estimate of its value, Q2(A\u21e4) = Q2(argmaxa Q1(a)). This estimate will then be unbiased in the sense that E = q(A\u21e4).\n\nWe can also repeat the process with the role of the two estimates reversed to yield a second unbiased estimate Q1(argmaxa Q2(a))", "39fee547-107c-401e-b9a4-41d3b2e00810": "We have little chance of choosing a solution that generalizes well when so many wildly different solutions exist. In  https://www.deeplearningbook.org/contents/ml.html    his example, the quadratic model is perfectly matched to the true structure of che task, so it generalizes well to new data. So far we have described only one way of changing a model\u2019s capacity: by changing the number of input features it has, and simultaneously adding new parameters associated with those features.\n\nThere are in fact many ways to change a model\u2019s capacity. Capacity is not determined only by the choice of model. The model specifies which family of functions the learning algorithm can choose from when varying the parameters in order to reduce a training objective. This is called he representational capacity of the model. In many cases, finding the best  111  CHAPTER 5. MACHINE LEARNING BASICS  function within this family is a difficult optimization problem. In practice, the learning algorithm does not actually find the best function, but merely one that significantly reduces the training error", "eaf0dc99-6c89-457d-b7e2-7780c584c922": "One difference between the purely linear case and the nonlinear case is that the use of a squashing nonlinearity such as tanh can cause the recurrent dynamics to become bounded. Note that it is possible for back-propagation to retain unbounded dynamics even when forward propagation has bounded dynamics, for example, when a sequence of tanh units are all in the middle of their linear regime and are connected by weight matrices with spectral radius greater than 1. Nonetheless, it is rare for all the tanh units to simultaneously lie at their linear activation point. The strategy of echo state networks is simply to fix the weights to have some spectral radius such as 3, where information is carried forward through time but does not explode because of the stabilizing effect of saturating nonlinearities like tanh.\n\nMore recently, it has been shown that the techniques used to set the weights in ESNs could be used to initialize the weights in a fully trainable recurrent net- work (with the hidden-to-hidden recurrent weights trained using back-propagation through time), helping to learn long-term dependencies", "e89c40b4-c0ad-4aab-872f-4c6cbcdd6d7e": "The most standard,  maw neta ween ea etane fan MINA Lanlintae.n 2. A 1ee 2A LIA ne AL AAI  https://www.deeplearningbook.org/contents/monte_carlo.html    CLLCLIC BUALALLLCES LUL IWLUIVLY LOCLULGQUeS ale VILLy aAVPPUCAaVIE WIL LIC WlOUecL does not assign zero probability to any state. Therefore, it is most convenient to present these techniques as sampling from an energy-based model (EBM) p(x) \u00ab exp(\u2014\u00a3(a)) as described in section 16.2.4. In the EBM formulation,  every state is guaranteed to have nonzero probability. MCMC methods are in fact more broadly applicable and can be used with many probability distributions that contain zero probability states.\n\nHowever, the theoretical guarantees concerning the behavior of MCMC methods must be proved on a case-by-case basis for different families of such distributions. In the context of deep learning, it is most common to rely on the general theoretical guarantees that naturally apply to all energy-based models", "f43bb9b3-f5e6-4e96-b456-7e4fc87ccb3e": "Smart: Robust and ef\ufb01cient \ufb01ne-tuning for pretrained natural language models through principled regularized optimization. Yichen Jiang and Mohit Bansal. 2019. Avoiding reasoning shortcuts: Adversarial evaluation, training, and model development for multi-hop QA.\n\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2726\u2013 2736, Florence, Italy. Association for Computational Linguistics. Thorsten Joachims. 1997. A probabilistic analysis of the rocchio algorithm with t\ufb01df for text categorization. In Proceedings of the Fourteenth International Conference on Machine Learning, ICML \u201997, page 143\u2013151, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc. Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vi\u00b4egas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2017", "71db8e59-112c-4b01-87e2-42225316c579": "\u201cDesign considerations for image Data Augmentation\u201d discusses additional characteristics of augmentation such as test-time augmentation and the impact of image resolution. The paper concludes with a \u201cDiscussion\u201d of the pre-  sented material, areas of \u201cFuture work\u2019, and \u201cConclusion\u201d. Background  Image augmentation in the form of data warping can be found in LeNet-5 . This was one of the first applications of CNNs on handwritten digit classification. Data augmentation has also been investigated in oversampling applications.\n\nOversampling is a technique used to re-sample imbalanced class distributions such that the model is not overly biased towards labeling instances as the majority class type. Random  Oversampling (ROS) is a naive approach which duplicates images randomly from the Shorten and Khoshgoftaar J Big Data  6:60   minority class until a desired class ratio is achieved. Intelligent oversampling tech- niques date back to SMOTE (Synthetic Minority Over-sampling Technique), which was developed by Chawla et al", "dda29ea7-72ac-40d6-888a-54e0d3921a2d": "The \ufb01rst is the use of eligibility traces, and the second is the use of TD methods to learn value functions that provide nearly immediate evaluations of actions (in tasks like instrumental conditioning experiments) or that provide immediate prediction targets (in tasks like classical conditioning experiments). Both of these methods correspond to similar mechanisms proposed in theories of animal learning.\n\nPavlov  pointed out that every stimulus must leave a trace in the nervous system that persists for some time after the stimulus ends, and he proposed that stimulus traces make learning possible when there is a temporal gap between the CS o\u21b5set and the US onset. To this day, conditioning under these conditions is called trace conditioning (page 344). Assuming a trace of the CS remains when the US arrives, learning occurs through the simultaneous presence of the trace and the US. We discuss some proposals for trace mechanisms in the nervous system in Chapter 15. Stimulus traces were also proposed as a means for bridging the time interval between actions and consequent rewards or penalties in instrumental conditioning. In Hull\u2019s in\ufb02uential learning theory, for example, \u201cmolar stimulus traces\u201d accounted for what he called an animal\u2019s goal gradient, a description of how the maximum strength of an instrumentally-conditioned response decreases with increasing delay of reinforcement", "2e6e613e-04e9-4a81-be63-e9724f938a6f": "Unlike the DBN, however, the DBM posterior distribution over their hidden units\u2014while complicated\u2014is easy to approximate with a variational approximation (as discussed in section 19.4), specifically a mean field approximation. The mean field approximation is a simple form of variational inference, where we restrict the approximating distribution to fully factorial distributions. In the context of DBMs, the mean field equations capture the bidirectional interactions between layers. In this section we derive the iterative approximate inference procedure originally introduced in Salakhutdinov and Hinton . In variational approximations to inference, we approach the task of approxi- mating a particular target distribution\u2014in our case, the posterior distribution over  663  https://www.deeplearningbook.org/contents/generative_models.html    CHAPTER 20. DEEP GENERATIVE MODELS  the hidden units given the visible units\u2014by some reasonably simple family of dis- tributions. In the case of the mean field approximation, the approximating family is the set of distributions where the hidden units are conditionally independent", "45bce88f-22a6-4e46-97ee-6dc57ae498f6": "To understand the difference, note that if a single Gaussian collapses onto a data point it will contribute multiplicative factors to the likelihood function arising from the other data points and these factors will go to zero exponentially fast, giving an overall likelihood that goes to zero rather than in\ufb01nity. However, once we have (at least) two components in the mixture, one of the components can have a \ufb01nite variance and therefore assign \ufb01nite probability to all of the data points while the other component can shrink onto one speci\ufb01c data point and thereby contribute an ever increasing additive value to the log likelihood. This is illustrated in Figure 9.7. These singularities provide another example of the severe over-\ufb01tting that can occur in a maximum likelihood approach. We shall see that this dif\ufb01culty does not occur if we adopt a Bayesian approach", "8bfcfc66-11cf-4d95-a07b-c666bf0d8843": "If Y = \u2018g(X) and z = f(Y), then  Oz x xY \u2014  ad Se Ad +N fa ae  https://www.deeplearningbook.org/contents/mlp.html    Vv z= J) ay; (0.47)  203  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  6.5.3 Recursively Applying the Chain Rule to Obtain Backprop  Using the chain rule, it is straightforward to write down an algebraic expression for the gradient of a scalar with respect to any node in the computational graph that produced that scalar. Actually evaluating that expression in a computer, however, introduces some extra considerations. Specifically, many subexpressions may be repeated several times within the overall expression for the gradient.\n\nAny procedure that computes the gradient will need to choose whether to store these subexpressions or to recompute them several times. An example of how these repeated subexpressions arise is given in figure 6.9. In some cases, computing the same subexpression twice would simply be wasteful", "2863d166-564d-490d-8c88-f4403f8856dc": "The core idea is that even though J(f(z;w)) is a step function with useless derivatives, the expected cost Ey. pz) J (f(z; w)) is often a smooth function amenable to gradient descent. Although that expectation is typically not tractable when y is high-dimensional (or is the result of the composition of many discrete stochastic decisions), it can be estimated without bias using a Monte Carlo average. The stochastic estimate of the gradient can be used with SGD or other stochastic gradient-based optimization techniques. The simplest version of REINFORCE can be derived by simply differentiating the expected cost:  E- Op(y) = OPO) 20. ag = IWS (20.60) Olo = sy)p(y) ew) (20.61) y WwW 1 )\\O log p(y) ~ (i) 708 Pye) \u2014 SS Iy) ae (20.62) yO~p(y), =1 686 CHAPTER 20. DEEP GENERATIVE MODELS TR at. ON AN VAltAn Ae", "f8f4af9b-570a-4bf4-b6eb-86d32fb4a87a": "If we consider the special case in which the transformation of the inputs simply consists of the addition of random noise, so that x \u2192 x + \u03be, then the regularizer takes the form Exercise 5.27 which is known as Tikhonov regularization .\n\nDerivatives of this regularizer with respect to the network weights can be found using an extended backpropagation algorithm . We see that, for small noise amplitudes, Tikhonov regularization is related to the addition of random noise to the inputs, which has been shown to improve generalization in appropriate circumstances . Another approach to creating models that are invariant to certain transformation of the inputs is to build the invariance properties into the structure of a neural network. This is the basis for the convolutional neural network , which has been widely applied to image data. Consider the speci\ufb01c task of recognizing handwritten digits. Each input image comprises a set of pixel intensity values, and the desired output is a posterior probability distribution over the ten digit classes. We know that the identity of the digit is invariant under translations and scaling as well as (small) rotations", "682950c3-8dc4-44e3-ad8b-b71474595216": "For the mean, make use of the matrix identity (C.6), and for the variance, make use of the matrix identity (C.7).\n\n6.22 (\u22c6 \u22c6) Consider a regression problem with N training set input vectors x1, . , xN and L test set input vectors xN+1, . , xN+L, and suppose we de\ufb01ne a Gaussian process prior over functions t(x). Derive an expression for the joint predictive distribution for t(xN+1), . , t(xN+L), given the values of t(x1), . , t(xN). Show the marginal of this distribution for one of the test observations tj where N + 1 \u2a7d j \u2a7d N + L is given by the usual Gaussian process regression result (6.66) and (6.67). 6.23 (\u22c6 \u22c6) www Consider a Gaussian process regression model in which the target variable t has dimensionality D. Write down the conditional distribution of tN+1 for a test input vector xN+1, given a training set of input vectors x1,", "df8d83fa-a9e8-4edd-974b-ae66561484a1": ", k, has its own weight vector, \u2713j, but because the actor units are all identical, we describe just one of the units and omit the subscript. One way for these units to follow the actor\u2013critic algorithm given in the equations above is for each to be a Bernoulli-logistic unit. This means that the output of each actor unit at each time is a random variable, At, taking value 0 or 1. Think of value 1 as the neuron \ufb01ring, that is, emitting an action potential. The weighted sum, \u2713>x(St), of a unit\u2019s input vector determines the unit\u2019s action probabilities via the exponential soft-max distribution (13.2), which for two actions is the logistic function: The weights of each actor unit are incremented, as above, by: \u2713  \u2713 + \u21b5\u2713 \u03b4tz\u2713 \u03b4 again corresponds to the dopamine signal: the same reinforcement signal that is sent to all the critic unit\u2019s synapses. Figure 15.5a shows \u03b4t being broadcast to all the synapses of all the actor units (which makes this actor network a team of reinforcement learning agents, something we discuss in Section 15.10 below)", "d799191e-8fbc-42f8-b1eb-4491a94d5a6c": "Furthermore, if data points arrive sequentially, then the posterior distribution at any stage acts as the prior distribution for the subsequent data point, such that the new posterior distribution is again given by (3.49). Exercise 3.8 For the remainder of this chapter, we shall consider a particular form of Gaussian prior in order to simplify the treatment.\n\nSpeci\ufb01cally, we consider a zero-mean isotropic Gaussian governed by a single precision parameter \u03b1 so that and the corresponding posterior distribution over w is then given by (3.49) with The log of the posterior distribution is given by the sum of the log likelihood and the log of the prior and, as a function of w, takes the form Maximization of this posterior distribution with respect to w is therefore equivalent to the minimization of the sum-of-squares error function with the addition of a quadratic regularization term, corresponding to (3.27) with \u03bb = \u03b1/\u03b2. We can illustrate Bayesian learning in a linear basis function model, as well as the sequential update of a posterior distribution, using a simple example involving straight-line \ufb01tting. Consider a single input variable x, a single target variable t and posterior distribution would become a delta function centred on the true parameter values, shown by the white cross", "dde9de80-9c62-4672-b1eb-b062f5f31af0": "2017. Semi-supervised sequence tagging with bidirectional language models. In ACL. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018a. Deep contextualized word representations. In NAACL. Matthew Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. 2018b. Dissecting contextual word embeddings: Architecture and representation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1499\u20131509. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding with unsupervised learning. Technical report, OpenAI. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text.\n\nIn Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383\u20132392", "cb22b245-8f1d-42c6-a1fa-e5bb63078b76": "After 1000 time steps, the short path is \u201cblocked,\u201d and a longer path is opened up along the left-hand side of the barrier, as shown in upper right of the \ufb01gure. The graph shows average cumulative reward for a Dyna-Q agent and an enhanced Dyna-Q+ agent to be described shortly. The \ufb01rst part of the graph shows that both Dyna agents found the short path within 1000 steps. When the environment changed, the graphs become \ufb02at, indicating a period during which the agents obtained no reward because they were wandering around behind the barrier. After a while, however, they were able to \ufb01nd the new opening and the new optimal behavior. Greater di\ufb03culties arise when the environment changes to become better than it was before, and yet the formerly correct policy does not reveal the improvement. In these cases the modeling error may not be detected for a long time, if ever. The general problem here is another version of the con\ufb02ict between exploration and exploitation. In a planning context, exploration means trying actions that improve the model, whereas exploitation means behaving in the optimal way given the current model", "2a1b938c-0992-426d-a055-dad1592b6617": "Consider a state space z consisting of the integers, with probabilities where z(\u03c4) denotes the state at step \u03c4. If the initial state is z(1) = 0, then by symmetry the expected state at time \u03c4 will also be zero E = 0, and similarly it is easily seen that E = \u03c4/2. Thus after \u03c4 steps, the random walk has only travExercise 11.10 elled a distance that on average is proportional to the square root of \u03c4. This square root dependence is typical of random walk behaviour and shows that random walks are very inef\ufb01cient in exploring the state space. As we shall see, a central goal in designing Markov chain Monte Carlo methods is to avoid random walk behaviour. Before discussing Markov chain Monte Carlo methods in more detail, it is useful to study some general properties of Markov chains in more detail. In particular, we ask under what circumstances will a Markov chain converge to the desired distribution. A \ufb01rst-order Markov chain is de\ufb01ned to be a series of random variables z(1), . .\n\n, z(M) such that the following conditional independence property holds for m \u2208 {1,", "ca8e6d84-a0be-40e9-ae42-bafe98dc9cc1": "The objective function J is a quadratic function of \u00b5k, and it can be minimized by setting its derivative with respect to \u00b5k to zero giving which we can easily solve for \u00b5k to give The denominator in this expression is equal to the number of points assigned to cluster k, and so this result has a simple interpretation, namely set \u00b5k equal to the mean of all of the data points xn assigned to cluster k. For this reason, the procedure is known as the K-means algorithm.\n\nThe two phases of re-assigning data points to clusters and re-computing the cluster means are repeated in turn until there is no further change in the assignments (or until some maximum number of iterations is exceeded). Because each phase reduces the value of the objective function J, convergence of the algorithm is assured. HowExercise 9.1 ever, it may converge to a local rather than global minimum of J. The convergence properties of the K-means algorithm were studied by MacQueen . The K-means algorithm is illustrated using the Old Faithful data set in FigAppendix A ure 9.1", "58703c8f-5989-48ea-88a4-661dea151e9e": "Section 13.3 There is an algorithm for exact inference on directed graphs without loops known as belief propagation , and is equivalent to a special case of the sum-product algorithm. Here we shall consider only the sum-product algorithm because it is simpler to derive and to apply, as well as being more general. We shall assume that the original graph is an undirected tree or a directed tree or polytree, so that the corresponding factor graph has a tree structure.\n\nWe \ufb01rst convert the original graph into a factor graph so that we can deal with both directed and undirected models using the same framework. Our goal is to exploit the structure of the graph to achieve two things: (i) to obtain an ef\ufb01cient, exact inference algorithm for \ufb01nding marginals; (ii) in situations where several marginals are required to allow computations to be shared ef\ufb01ciently. We begin by considering the problem of \ufb01nding the marginal p(x) for particular variable node x. For the moment, we shall suppose that all of the variables are hidden. Later we shall see how to modify the algorithm to incorporate evidence corresponding to observed variables", "372183a2-ee7e-4d26-9fc5-b130e84ce636": "Similarly, if we apply the MDP framework to a person or animal, the muscles, skeleton, and sensory organs should be considered part of the environment. Rewards, too, presumably are computed inside the physical bodies of natural and arti\ufb01cial learning systems, but are considered external to the agent. The general rule we follow is that anything that cannot be changed arbitrarily by the agent is considered to be outside of it and thus part of its environment. We do not assume that everything in the environment is unknown to the agent. For example, the agent often knows quite a bit about how its rewards are computed as a function of its actions and the states in which they are taken. But we always consider the reward computation to be external to the agent because it de\ufb01nes the task facing the agent and thus must be beyond its ability to change arbitrarily. In fact, in some cases the agent may know everything about how its environment works and still face a di\ufb03cult reinforcement learning task, just as we may know exactly how a puzzle like Rubik\u2019s cube works, but still be unable to solve it. The agent\u2013environment boundary represents the limit of the agent\u2019s absolute control, not of its knowledge", "ecf9bce9-9c92-407f-b559-6a87afaa57ca": "The Dirac delta distribution is only necessary to define the empirical distribution over continuous variables.\n\nFor discrete variables, the situation is simpler: an empirical distribution can be conceptualized as a multinoulli distribution, with a probability associated with each possible input value that is simply equal to the empirical frequency of that value in the training set. We can view the empirical distribution formed from a dataset of training examples as specifying the distribution that we sample from when we train a model on this dataset. Another important perspective on the empirical distribution is that it is the probability density that maximizes the likelihood of the training data (see section 5.5). 3.9.6 Mixtures of Distributions  It is also common to define probability distributions by combining other simpler probability distributions. One common way of combining distributions is to construct a mixture distribution. A mixture distribution is made up of several component distributions. On each trial, the choice of which component distribution should generate the sample is determined by sampling a component identity from a multinoulli distribution:  P(x)= = P(c=i)P(x|c=i), (3.29) where P(c) is the multinoulli dishibution over component identities", "d89037ca-f349-48cc-9ae7-a46b27559d96": "In this chapter we have been imagining a setting in which many tasks are being learned simultaneously, using o\u21b5-policy methods, from the same stream of experience. The actions taken will of course in\ufb02uence this stream of experience, which in turn will determine how much learning occurs and which tasks are learned. When reward is not available, or not strongly in\ufb02uenced by behavior, the agent is free to choose actions that maximize in some sense the learning on the tasks, that is, to use some measure of learning progress as an internal or \u201cintrinsic\u201d reward, implementing a computational form of curiosity.\n\nIn addition to measuring learning progress, intrinsic reward can, among other possibilities, signal the receipt of unexpected, novel, or otherwise interesting input, or can assess the agent\u2019s ability to cause changes in its environment. Intrinsic reward signals generated in these ways can be used by an agent to pose tasks for itself by de\ufb01ning auxiliary tasks, GVFs, or options, as discussed above, so that skills learned in this way can contribute to the agent\u2019s ability to master future tasks. The result is a computational analog of something like play. Many preliminary studies of such uses of intrinsic reward signals have been conducted, and exciting topics for future research remain in this general area", "5f1811e5-9bc1-4a86-823d-c3dc7edbfb76": "The condition for a target point to lie inside the \u03f5-tube is that yn \u2212 \u03f5 \u2a7d tn \u2a7d yn+\u03f5, where yn = y(xn). Introducing the slack variables allows points to lie outside the tube provided the slack variables are nonzero, and the corresponding conditions are The error function for support vector regression can then be written as which must be minimized subject to the constraints \u03ben \u2a7e 0 and \ufffd\u03ben \u2a7e 0 as well as (7.53) and (7.54). This can be achieved by introducing Lagrange multipliers an \u2a7e 0, \ufffdan \u2a7e 0, \u00b5n \u2a7e 0, and \ufffd\u00b5n \u2a7e 0 and optimizing the Lagrangian We now substitute for y(x) using (7.1) and then set the derivatives of the Lagrangian with respect to w, b, \u03ben, and \ufffd\u03ben to zero, giving Using these results to eliminate the corresponding variables from the Lagrangian, we see that the dual problem involves maximizing Exercise 7.7 with respect to {an} and {\ufffdan}, where we have introduced the kernel k(x, x\u2032) = \u03c6(x)T\u03c6(x\u2032)", "94ec3447-218f-47c6-9ad2-c544d7a3ab55": "The graphical representation of this model is shown in Figure 8.24. We see that observation of z blocks the path between xi and xj for j \u0338= i (because such paths are tail-to-tail at the node z) and so xi and xj are conditionally independent given z. If, however, we marginalize out z (so that z is unobserved) the tail-to-tail path from xi to xj is no longer blocked. This tells us that in general the marginal density p(x) will not factorize with respect to the components of x. We encountered a simple application of the naive Bayes model in the context of fusing data from different sources for medical diagnosis in Section 1.5. If we are given a labelled training set, comprising inputs {x1, . , xN} together with their class labels, then we can \ufb01t the naive Bayes model to the training data using maximum likelihood assuming that the data are drawn independently from the model. The solution is obtained by \ufb01tting the model for each class separately using the correspondingly labelled data", "0a7447ec-06f7-43ef-a32c-227f932fd799": "Because pooling summarizes the responses over a whole neighborhood, it is possible to use fewer pooling units than detector units, by reporting summary statistics for pooling regions spaced k pixels apart rather than 1 pixel apart. See figure 9.10 for an example. This improves the computational efficiency of the network because the next layer has roughly k times fewer inputs to process.\n\nWhen  337  https://www.deeplearningbook.org/contents/convnets.html    CHAPTER 9. CONVOLUTIONAL NETWORKS  Large response Large response  in pooling unit, in pooling unit,  Large  response in detectoi unit 1  Figure 9.9: Example of learned invariances. A pooling unit that pools over multiple features that are learned with separate parameters can learn to be invariant to transformations of the input. Here we show how a set of three learned filters and a max pooling unit can learn to become invariant to rotation. All three filters are intended to detect a hand written 5. Each filter attempts to match a slightly different orientation of the 5. When a 5 appears in the input, the corresponding filter will match it and cause a large activation in a detector unit", "124ca3ab-b141-4c92-a14b-4bcc06e2d23f": "This is an example of a linear Gaussian model , which we shall study in greater generality in Section 8.1.4. We wish to \ufb01nd the marginal distribution p(y) and the conditional distribution p(x|y). This is a problem that will arise frequently in subsequent chapters, and it will prove convenient to derive the general results here. We shall take the marginal and conditional distributions to be where \u00b5, A, and b are parameters governing the means, and \u039b and L are precision matrices. If x has dimensionality M and y has dimensionality D, then the matrix A has size D \u00d7 M. First we \ufb01nd an expression for the joint distribution over x and y. To do this, we de\ufb01ne and then consider the log of the joint distribution where \u2018const\u2019 denotes terms independent of x and y", "37d53ca0-fd2a-413c-b783-951b8532ba14": "Common sense tells us that this is unreasonable, and in fact this is an extreme example of the over-\ufb01tting associated with maximum likelihood. We shall see shortly how to arrive at more sensible conclusions through the introduction of a prior distribution over \u00b5. We can also work out the distribution of the number m of observations of x = 1, given that the data set has size N. This is called the binomial distribution, and from (2.5) we see that it is proportional to \u00b5m(1 \u2212 \u00b5)N\u2212m.\n\nIn order to obtain the normalization coef\ufb01cient we note that out of N coin \ufb02ips, we have to add up all of the possible ways of obtaining m heads, so that the binomial distribution can be written is the number of ways of choosing m objects out of a total of N identical objects. Exercise 2.3 The mean and variance of the binomial distribution can be found by using the result of Exercise 1.10, which shows that for independent events the mean of the sum is the sum of the means, and the variance of the sum is the sum of the variances. Because m = x1 +", "e35720ce-fe1a-4495-939a-be8de029db7c": "Some examples of important design patterns for recurrent neural networks include the following:  e Recurrent networks that produce an output at each time step and have recurrent connections between hidden units, illustrated in figure 10.3  e Recurrent networks that produce an output at each time step and have recurrent connections only from the output at one time step to the hidden units at the next time step, illustrated in figure 10.4  https://www.deeplearningbook.org/contents/rnn.html    e Recurrent networks with recurrent connections between hidden units, that read an entire sequence and then produce a single output, illustrated in figure 10.5  Figure 10.3 is a reasonably representative example that we return to throughout most of the chapter. The recurrent neural network of figure 10.3 and equation 10.8 is universal in the sense that any function computable by a Turing machine can be computed by such a recurrent network of a finite size.\n\nThe output can be read from the RNN after a number of time steps that is asymptotically linear in the number of time steps used by the Turing machine and asymptotically linear in the length of the input . The functions computable by a Turing machine are discrete, so these results regard exact implementation of the function, not approximations", "ef83f212-e430-4255-b388-a7dc07955ec4": "We need a rule that assigns each value of x to one of the available classes.\n\nSuch a rule will divide the input space into regions Rk called decision regions, one for each class, such that all points in Rk are assigned to class Ck. The boundaries between decision regions are called decision boundaries or decision surfaces. Note that each decision region need not be contiguous but could comprise some number of disjoint regions. We shall encounter examples of decision boundaries and decision regions in later chapters. In order to \ufb01nd the optimal decision rule, consider \ufb01rst of all the case of two classes, as in the cancer problem for instance. A mistake occurs when an input vector belonging to class C1 is assigned to class C2 or vice versa. The probability of this occurring is given by We are free to choose the decision rule that assigns each point x to one of the two classes. Clearly to minimize p(mistake) we should arrange that each x is assigned to whichever class has the smaller value of the integrand in (1.78). Thus, if p(x, C1) > p(x, C2) for a given value of x, then we should assign that x to class C1", "054a3ab6-6605-4d13-88e9-763b93fcf273": "Also, as alluded to earlier, it may be convenient to introduce such hidden variables so that the maximization of L(\u03b8|\u03b8n) is simpli\ufb01ed given knowledge of the hidden variables. (as compared with a direct maximization of L(\u03b8)) The convergence properties of the EM algorithm are discussed in detail by McLachlan and Krishnan . In this section we discuss the general convergence of the algorithm. Recall that \u03b8n+1 is the estimate for \u03b8 which maximizes the di\ufb00erence \u2206(\u03b8|\u03b8n). Starting with the current estimate for \u03b8, that is, \u03b8n we had that \u2206(\u03b8n|\u03b8n) = 0. Since \u03b8n+1 is chosen to maximize \u2206(\u03b8|\u03b8n), we then have that \u2206(\u03b8n+1|\u03b8n) \u2265 \u2206(\u03b8n|\u03b8n) = 0, so for each iteration the likelihood L(\u03b8) is nondecreasing. When the algorithm reaches a \ufb01xed point for some \u03b8n the value \u03b8n maximizes l(\u03b8). Since L and l are equal at \u03b8n if L and l are di\ufb00erentiable at \u03b8n, then \u03b8n must be a stationary point of L", "f1342bed-58de-404c-ae82-02d00965db22": "What is clear is that more precision is required during training than at inference time, and that some forms of dynamic fixed-point representation of numbers can be used to reduce how many bits are required per number. Traditional fixed-point numbers are restricted to a fixed range (which corresponds to a given exponent in a floating-point representation). Dynamic fixed-point representations share that range among a set of numbers (such as all the weights in one layer).\n\nUsing fixed-point rather than floating-point representations and using fewer bits per number reduces the hardware surface area, power requirements, and computing time needed for performing multiplications, and multiplications are the most demanding of the operations needed to use or train a modern deep network with backprop. 12.2 Computer Vision  Computer vision has traditionally been one of the most active research areas for deep learning applications, because vision is a task that is effortless for humans and many animals but challenging for computers . Many of he most popular standard benchmark tasks for deep learning algorithms are forms of object recognition or optical character recognition. Computer vision is a very broad field encompassing a wide variety of ways to process images and an amazing diversity of applications. Applications of computer vision range from reproducing human visual abilities, such as recognizing faces,", "dbd579ca-93ef-46e9-a8f2-b68b00c2811f": "To make a machine learning algorithm, we need to design an algorithm that will improve the weights w in a way that reduces MSEijes, when the algorithm is allowed to gain experience by observing a training set (Xin), y(train)) One intuitive way of doing this (which we justify later, in section 5.5.1) is just to minimize the mean squared error on the training set, MSF\u00a2rain-  To minimize MSEtain, we can simply solve for where its gradient is 0:  VwMSErain = 0 (5.6) 1 . .\n\n= Vw\u2014\\lg _ y rain) | =0 (5.7) m 106  CHAPTER 5", "634a5892-dda7-40de-afc4-1f5477fd2d5a": "Nevertheless, even if this assumption is not precisely satis\ufb01ed, the model may still give good classi\ufb01cation performance in practice because the decision boundaries can be insensitive to some of the details in the class-conditional densities, as illustrated in Figure 1.27. We have seen that a particular directed graph represents a speci\ufb01c decomposition of a joint probability distribution into a product of conditional probabilities.\n\nThe graph also expresses a set of conditional independence statements obtained through the d-separation criterion, and the d-separation theorem is really an expression of the equivalence of these two properties. In order to make this clear, it is helpful to think of a directed graph as a \ufb01lter. Suppose we consider a particular joint probability distribution p(x) over the variables x corresponding to the (nonobserved) nodes of the graph. The \ufb01lter will allow this distribution to pass through if, and only if, it can be expressed in terms of the factorization (8.5) implied by the graph", "b84a7e63-e076-4aa1-b328-0516bd32c160": "Akaike and Kitagawa: The Practice of Time Series Analysis. Bishop:  Pattern Recognition and Machine Learning. Cowell, Dawid, Lauritzen, and Spiegelhalter: Probabilistic Networks and Doucet, de Freitas, and Gordon: Sequential Monte Carlo Methods in Practice. Fine: Feedforward Neural Network Methodology. Hawkins and Olwell: Cumulative Sum Charts and Charting for Quality Improvement. Jensen: Bayesian Networks and Decision Graphs. Marchette: Computer Intrusion Detection and Network Monitoring: Rubinstein and Kroese: The Cross-Entropy Method:  A Unified Approach to  Combinatorial Optimization, Monte Carlo Simulation, and Machine Learning. Studen\u00fd: Probabilistic Conditional Independence Structures. Vapnik: The Nature of Statistical Learning Theory, Second Edition. Wallace: Statistical and Inductive Inference by Minimum Massage Length. Library of Congress Control Number: 2006922522 \u00a9 2006 Springer Science+Business Media, LLC All rights reserved", "826a1a29-796d-44a6-b360-d1245a86a96a": "The result is the graph shown in Figure 8.1. If there is a link going from a node a to a node b, then we say that node a is the parent of node b, and we say that node b is the child of node a. Note that we shall not make any formal distinction between a node and the variable to which it corresponds but will simply use the same symbol to refer to both. An interesting point to note about (8.2) is that the left-hand side is symmetrical with respect to the three variables a, b, and c, whereas the right-hand side is not. Indeed, in making the decomposition in (8.2), we have implicitly chosen a particular ordering, namely a, b, c, and had we chosen a different ordering we would have obtained a different decomposition and hence a different graphical representation. We shall return to this point later.\n\nFor the moment let us extend the example of Figure 8.1 by considering the joint distribution over K variables given by p(x1, . , xK)", "d3c66dd2-0497-4f67-8b1a-a8930736586a": "In the context of deep learning, the approach most commonly used to model these dependencies is to introduce several latent or \u201chidden\u201d variables, h. The model can then capture dependencies between any pair of variables v; and v; indirectly, via direct dependencies between v; and h, and direct dependencies between h and vj. A good model of v which did not contain any latent variables would need to have very large numbers of parents per node in a Bayesian network or very large cliques in a Markov network. Just representing these higher-order interactions is costly\u2014both in a computational sense, because the number of parameters that must be stored in memory scales exponentially with the number of members in a clique, but also in a statistical sense, because this exponential number of parameters requires a wealth of data to estimate accurately.\n\nWhen the model is intended to capture dependencies between visible variables with direct connections, it is usually infeasible to connect all variables, so the  579  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    CHAPTER 16", "751b90d9-bbda-4b93-8479-7430f3686bba": "Fortunately, in this book, we usually need to decompose only a specific class of  Effect of eigenvectors and eigenvalues  Before multiplication After multiplication  3 1 4 OL 1 vw yO) 7 1 | q-s oF fe 4 @) nc 4 \u2014ib 4 ~2  Figure 2.3: An example of the effect of eigenvectors and eigenvalues. Here, we have a matrix A with two orthonormal eigenvectors, v\u201c) with eigenvalue \\, and y\u00ae@) with eigenvalue A\u00bb.\n\n(Left)We plot the set of all unit vectors u \u20ac R? as a unit circle. (Right)We plot the set of all points Au. By observing the way that A distorts the unit circle, we can see that it scales space in direction vw) by Ai. 41  https://www.deeplearningbook.org/contents/linear_algebra.html    CHAPTER 2. LINEAR ALGEBRA  matrices that have a simple decomposition", "9fa7a25c-d31a-4ede-bfc7-2dea28e36e54": "The method of conjugate gradients seeks to address this problem.\n\nIn the method of conjugate gradients, we seek to find a search direction that is conjugate to the previous line search direction; that is, it will not undo progress made in that direction. At training iteration t, the next search direction d; takes the form:  d; = VoJ(8) + Brdt-1, (8.29) where (3; is a coefficient whose magnitude controls how much of the direction, d:_1, we should add back to the current search direction. Two directions, d, and d,_1, are defined as conjugate if d| Hd, = 0, where H is the Hessian matrix. The straightforward way to impose conjugacy would involve calculation of the eigenvectors of H to choose 6;, which would not satisfy our goal of developing  https://www.deeplearningbook.org/contents/optimization.html  a method that is more computationally viable than Newton\u2019s method for large problems. Can we calculate the conjugate directions without resorting to these calculations? Fortunately, the answer to that is yes. Two pop  CHAPTER 8", "1d0f496a-1e0e-4f29-8e00-994ea5bba032": "The result tells us how an infinitesimal change in yz or a would change the output if we could repeat the sampling operation again with the same value of z. Being able to back-propagate through this sampling operation allows us to incorporate it into a larger graph. We can build elements of the graph on top of the  684  CHAPTER 20. DEEP GENERATIVE MODELS  output of the sampling distribution. For example, we can compute the derivatives of some loss function J(y). We can also build elements of the graph whose outputs are the inputs or the parameters of the sampling operation.\n\nFor example, we could build a larger graph with w = f(x;@) and o = g(a; 6). In this augmented graph, we can use back-propagation through these functions to derive VgJ(y). The principle used in this Gaussian sampling example is more generally appli- cable", "a5208794-d371-4682-a610-46861a3d0d79": "Then the loss function for a positive pair of examples (i, j) is de\ufb01ned as where 1 \u2208 {0, 1} is an indicator function evaluating to 1 iff k \u0338= i and \u03c4 denotes a temperature parameter. The \ufb01nal loss is computed across all positive pairs, both (i, j) and (j, i), in a mini-batch. This loss has been used in previous work ; for convenience, we term it NT-Xent (the normalized temperature-scaled cross entropy loss). A Simple Framework for Contrastive Learning of Visual Representations Algorithm 1 SimCLR\u2019s main learning algorithm. update networks f and g to minimize L Algorithm 1 summarizes the proposed method.\n\nTo keep it simple, we do not train the model with a memory bank . Instead, we vary the training batch size N from 256 to 8192. A batch size of 8192 gives us 16382 negative examples per positive pair from both augmentation views. Training with large batch size may be unstable when using standard SGD/Momentum with linear learning rate scaling . To stabilize the training, we use the LARS optimizer  for all batch sizes", "f694da60-8029-4622-a152-f604362da6bd": "The uni\ufb01ed view we present in this chapter is that all state-space planning methods share a common structure, a structure that is also present in the learning methods presented in this book. It takes the rest of the chapter to develop this view, but there are two basic ideas: (1) all state-space planning methods involve computing value functions as a key intermediate step toward improving the policy, and (2) they compute value functions by updates or backup operations applied to simulated experience.\n\nThis common structure can be diagrammed as follows: Dynamic programming methods clearly \ufb01t this structure: they make sweeps through the space of states, generating for each state the distribution of possible transitions. Each distribution is then used to compute a backed-up value (update target) and update the state\u2019s estimated value. In this chapter we argue that various other state-space planning methods also \ufb01t this structure, with individual methods di\u21b5ering only in the kinds of updates they do, the order in which they do them, and in how long the backed-up information is retained. Viewing planning methods in this way emphasizes their relationship to the learning methods that we have described in this book. The heart of both learning and planning methods is the estimation of value functions by backing-up update operations", "eaadad4a-98ea-43d6-b1e4-9e3b0710b4e9": "Hancock, B., Varma, P., Wang, S., Bringmann, M., Liang, P., R\u00e9, C.: Training classi\ufb01ers with natural language explanations  21. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. CoRR, arXiv:1512.03385  22.\n\nHearst, A.M.: Automatic acquisition of hyponyms from large text corpora. In: Meeting of the Association for Computational Linguistics (ACL)  23. Hinton, G.E. : Training products of experts by minimizing contrastive divergence. Neural Comput. 14(8), 1771\u20131800  24. Hoffmann, R., Zhang, C., Ling, X., Zettlemoyer, L., Weld, D.S. : Knowledge-based weak supervision for information extraction of overlapping relations. In: Meeting of the Association for Computational Linguistics (ACL)  25", "f074dfa2-f851-4a10-aec0-48a28d6aacf8": "Here we give an indication of the problem in the context of our solutions for the maximum likelihood parameter settings for the univariate Gaussian distribution. In particular, we shall show that the maximum likelihood approach systematically underestimates the variance of the distribution. This is an example of a phenomenon called bias and is related to the problem of over-\ufb01tting encountered in the context of polynomial curve \ufb01tting. Section 1.1 We \ufb01rst note that the maximum likelihood solutions \u00b5ML and \u03c32 ML are functions of the data set values x1, . , xN. Consider the expectations of these quantities with respect to the data set values, which themselves come from a Gaussian distribution with parameters \u00b5 and \u03c32.\n\nIt is straightforward to show that Exercise 1.12 so that on average the maximum likelihood estimate will obtain the correct mean but will underestimate the true variance by a factor (N \u2212 1)/N. The intuition behind this result is given by Figure 1.15", "2f1f14dd-5c77-403b-add9-8de355649ac7": "(Left)Ground truth.\n\nThis is the correct image, which the network should emit. (Center)/Image produced by a predictive generative network trained with mean squared error alone. Because the ears do not cause an extreme difference in brightness compared to the neighboring skin, they were not sufficiently salient for the model to learn  https://www.deeplearningbook.org/contents/representation.html    to represent them. (Right)Image produced by a model trained with a combination of mean squared error and adversarial loss. Using this learned cost function, the ears are salient because they follow a predictable pattern. Learning which underlying causes are important and relevant enough to model is an important active area of research. Figures  graciously provided by Lotter e\u00a2 al. and consistent position means that a feedforward network can easily learn to detect them, making them highly salient under the generative adversarial framework. See figure 15.6 for example images. Generative adversarial networks are only one step toward determining which factors should be represented. We expect that future research will discover better ways of determining which factors to represent and develop mechanisms for representing different factors depending on the task", "8302d060-1dad-4c71-93cf-f5f445ec2633": "Even though we are given the exact, correct values, v\u21e1(St) for each St, there is still a di\ufb03cult problem because our function approximator has limited resources and thus limited resolution. In particular, there is generally no w that gets all the states, or even all the examples, exactly correct. In addition, we must generalize to all the other states that have not appeared in examples. We assume that states appear in examples with the same distribution, \u00b5, over which we are trying to minimize the VE as given by (9.1). A good strategy in this case is to try to minimize error on the observed examples. Stochastic gradient-descent (SGD) methods do this by adjusting the weight vector after each example by a small amount in the direction that would most reduce the error on that example: where \u21b5 is a positive step-size parameter, and rf(w), for any scalar expression f(w) that is a function of a vector (here w), denotes the column vector of partial derivatives of the expression with respect to the components of the vector: This derivative vector is the gradient of f with respect to w", "8a563655-0c1f-4693-ba69-9ecc2ea74a5e": "However, because we are now maximizing rather than summing, it is possible that there may be multiple con\ufb01gurations of x all of which give rise to the maximum value for p(x). In such cases, this strategy can fail because it is possible for the individual variable values obtained by maximizing the product of messages at each node to belong to different maximizing con\ufb01gurations, giving an overall con\ufb01guration that no longer corresponds to a maximum. The problem can be resolved by adopting a rather different kind of message passing from the root node to the leaves. To see how this works, let us return once again to the simple chain example of N variables x1, . , xN each having K states, ing explicitly the K possible states (one per row of the diagram) for each of the variables xn in the chain model. In this illustration K = 3. The arrow shows the direction of message passing in the max-product algorithm", "55ceb601-2294-4f61-9b87-fb183a2f9c87": "For this parameterization, prove that the eligibility vector is The policy gradient theorem (13.5) can be generalized to include a comparison of the action value to an arbitrary baseline b(s): The baseline can be any function, even a random variable, as long as it does not vary with a; the equation remains valid because the subtracted quantity is zero: The policy gradient theorem with baseline (13.10) can be used to derive an update rule using similar steps as in the previous section. The update rule that we end up with is a new version of REINFORCE that includes a general baseline: Because the baseline could be uniformly zero, this update is a strict generalization of REINFORCE. In general, the baseline leaves the expected value of the update unchanged, but it can have a large e\u21b5ect on its variance. For example, we saw in Section 2.8 that an analogous baseline can signi\ufb01cantly reduce the variance (and thus speed the learning) of gradient bandit algorithms", "57ffb6a8-c358-4b24-96ee-798f9ea02ccf": "Foundational concepts such as parameter estimation, bias and variance are useful to formally characterize notions of generalization, underfitting and overfitting.\n\n5.4.1 Point Estimation  Point estimation is the attempt to provide the single \u201cbest\u201d prediction of some quantity of interest. In general the quantity of interest can be a single parameter or a vector of parameters in some parametric model, such as the weights in our linear regression example in section 5.1.4, but it can also be a whole function. To distinguish estimates of parameters from their true value, our convention  will be to denote a point estimate of a parameter @ by @. Let {2 ),...,20} be a set of m independent and identically distributed  120  https://www.deeplearningbook.org/contents/ml.html    CHAPTER 5. MACHINE LEARNING BASICS  Algorithm 5.1 The k-fold cross-validation algorithm. It can be used to estimate generalization error of a learning algorithm A when the given dataset D is too small for a simple train/test or train/valid split to yield accurate estimation of generalization error, because the mean of a loss LZ on a small test set may have too high a variance", "988aec82-a6cc-4595-98e2-5940bac9bd20": "So far, the only method of modifying a learning algorithm that we have discussed concretely is to increase or decrease the model\u2019s representational capacity by adding or removing functions from the hypothesis space of solutions the learning algorithm is able to choose from. We gave the specific example of increasing or decreasing the degree of a polynomial for a regression problem. The view we have described so far is oversimplified. The behavior of our algorithm is strongly affected not just by how large we make the set of functions allowed in its hypothesis space, but by the specific identity of those functions. The learning algorithm we have studied so far, linear regression, has a hypothesis space consisting of the set of linear functions of its input.\n\nThese linear functions can be useful for problems where the relationship between inputs and outputs truly is close to linear. They are less useful for problems that behave in a very nonlinear fashion. For example, linear regression would not perform well if we tried to use it to predict sin(z) from x. We can thus control the performance of our algorithms by choosing what kind of functions we allow them to draw solutions from, as well as by controlling the amount of these functions. We can also give a learning algorithm a preference for one solution over another  116  CHAPTER 5", "39d3d342-f282-4884-ab4b-9490b873042c": "From now on we omit the conditioning on Mi to keep the notation uncluttered. From Bayes\u2019 theorem the model evidence is given by ln p(D) \u2243 ln p(D|\u03b8MAP) + ln p(\u03b8MAP) + M where \u03b8MAP is the value of \u03b8 at the mode of the posterior distribution, and A is the Hessian matrix of second derivatives of the negative log posterior The \ufb01rst term on the right hand side of (4.137) represents the log likelihood evaluated using the optimized parameters, while the remaining three terms comprise the \u2018Occam factor\u2019 which penalizes model complexity. If we assume that the Gaussian prior distribution over parameters is broad, and that the Hessian has full rank, then we can approximate (4.137) very roughly using Exercise 4.23 where N is the number of data points, M is the number of parameters in \u03b8 and we have omitted additive constants.\n\nThis is known as the Bayesian Information Criterion (BIC) or the Schwarz criterion . Note that, compared to AIC given by (1.73), this penalizes model complexity more heavily. Complexity measures such as AIC and BIC have the virtue of being easy to evaluate, but can also give misleading results", "76c263fc-e59d-4b8b-9c34-c1b23199e80a": "In other words, the nonlinear features have mapped both a = ' and # = ! to a single point in feature space, h =  '. The linear model can now describe the function as increasing in h, and decreasing in ho. In this example, the motivation for learning the feature space is only to make the model capacity greater so that it can fit the training set. In more realistic applications, learned representations can also help the model to generalize. 169  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  Mc w Figure 6.2: An example of a feedforward network, drawn in two different styles. Specifically, this is the feedforward network we use to solve the XOR example.\n\nIt has a single hidden layer containing two units. (Left) In this style, we draw every unit as a node in the graph. This style is explicit and unambiguous, but for networks larger than this example, it can consume too much space. (Right) In this style, we draw a node in the graph for each entire vector representing a layer\u2019s activations. This style is much more compact", "d9750cd6-81dd-405d-8cbe-ff659f2c0f51": "For example,  log learning rate ~ u(\u20141,-\u20145), (11.2)  learning rate = 1(!0g_leaming_rate_ (11.3)  where u(a,b) indicates a sample of the uniform distribution in the interval (a, b). Similarly the log number_of_hidden_units may be sampled from u(log(50), log). Unlike in a grid search, we should not discretize or bin the values of the hy- perparameters, so that we can explore a larger set of values and avoid additional computational cost.\n\nIn fact, as illustrated in figure 11.2, a random search can be exponentially more efficient than a grid search, when there are several hyperpa- rameters that do not strongly affect the performance measure. This is studied at length in Bergstra and Bengio , who found that random search reduces the validation set error much faster than grid search, in terms of the number of trials run by each method. As with grid search, we may often want to run repeated versions of random search, to refine the search based on the results of the first run", "5350054c-eab8-461a-bc14-ae38f7dd762d": "Instead of modelling the density of data, however, these methods aim to \ufb01nd a smooth boundary enclosing a region of high density. The boundary is chosen to represent a quantile of the density, that is, the probability that a data point drawn from the distribution will land inside that region is given by a \ufb01xed number between 0 and 1 that is speci\ufb01ed in advance. This is a more restricted problem than estimating the full density but may be suf\ufb01cient in speci\ufb01c applications.\n\nTwo approaches to this problem using support vector machines have been proposed. The algorithm of Sch\u00a8olkopf et al. tries to \ufb01nd a hyperplane that separates all but a \ufb01xed fraction \u03bd of the training data from the origin while at the same time maximizing the distance (margin) of the hyperplane from the origin, while Tax and Duin  look for the smallest sphere in feature space that contains all but a fraction \u03bd of the data points. For kernels k(x, x\u2032) that are functions only of x \u2212 x\u2032, the two algorithms are equivalent. We now extend support vector machines to regression problems while at the same time preserving the property of sparseness", "6ecfcfb1-6ee3-4d69-90bb-1478d8d5dd56": "In this type of problem, each agent in a collection of reinforcement learning agents receives the same reinforcement signal, where that signal depends on the activities of all members of the collection, or team.\n\nIf each team member uses a su\ufb03ciently capable learning algorithm, the team can learn collectively to improve performance of the entire team as evaluated by the globally-broadcast reinforcement signal, even if the team members do not directly communicate with one another. This is consistent with the wide dispersion of dopamine signals in the brain and provides a neurally plausible alternative to the widely-used error-backpropagation method for training multilayer networks. The distinction between model-free and model-based reinforcement learning is helping neuroscientists investigate the neural bases of habitual and goal-directed learning and decision making. Research so far points to their being some brain regions more involved in one type of process than the other, but the picture remains unclear because model-free and model-based processes do not appear to be neatly separated in the brain. Many questions remain unanswered. Perhaps most intriguing is evidence that the hippocampus, a structure traditionally associated with spatial navigation and memory, appears to be involved in simulating possible future courses of action as part of an animal\u2019s decisionmaking process", "c1aadb12-4493-4442-ac07-c0581ad6e07b": "loss of ge\"\"raJity in assuming a zero mean. unit co\\'ariance Gau\"ian for the latent distributi\"n II{Z) because a more gcneral Gau\"i3n di\"ributi\"n would gi\"e rise to an equivalent probabili\"ic n>odel", "37a53d06-89cc-46ff-8978-22de1964301b": "This means that it is not always clear that there is a computational benefit to using sparse outputs, because the model may choose to make the majority of the output nonzero, and all these non-zero values need to be compared to the corresponding training target, even if the training target is zero. Dauphin et al. demonstrated that such models can be accelerated using importance sampling. The efficient algorithm minimizes the loss reconstruction for the \u201cpositive words\u201d (those that are nonzero in the target) and an equal number of \u2018negative words.\u201d The negative words are chosen randomly, using a heuristic to sample words that are more likely to be mistaken. The bias introduced by this heuristic oversampling can then be corrected using importance weights.\n\nIn all these cases, the computational complexity of gradient estimation for the output layer is reduced to be proportional to the number of negative samples rather than proportional to the size of the output vector. 12.4.3.4 Noise-Contrastive Estimation and Ranking Loss  https://www.deeplearningbook.org/contents/applications.html    Other approaches based on sampling have been proposed to reduce the computa- tional cost of training neural language models with large vocabularies", "0455d752-b7d5-42a7-b12b-fd1e89c735a7": "We shall suppose that the variance \u03c32 is known, and we consider the task of inferring the mean \u00b5 given a set of N observations X = {x1, . , xN}. The likelihood function, that is the probability of the observed data given \u00b5, viewed as a function of \u00b5, is given by Again we emphasize that the likelihood function p(X|\u00b5) is not a probability distribution over \u00b5 and is not normalized. We see that the likelihood function takes the form of the exponential of a quadratic form in \u00b5. Thus if we choose a prior p(\u00b5) given by a Gaussian, it will be a conjugate distribution for this likelihood function because the corresponding posterior will be a product of two exponentials of quadratic functions of \u00b5 and hence will also be Gaussian.\n\nWe therefore take our prior distribution to be and the posterior distribution is given by Simple manipulation involving completing the square in the exponent shows that the Exercise 2.38 in which \u00b5ML is the maximum likelihood solution for \u00b5 given by the sample mean It is worth spending a moment studying the form of the posterior mean and variance", "7b47a9cc-2a7a-427d-a305-de98ddf2a6e9": "One of the most exciting aspects of modern reinforcement learning is its substantive and fruitful interactions with other engineering and scienti\ufb01c disciplines. Reinforcement learning is part of a decades-long trend within arti\ufb01cial intelligence and machine learning toward greater integration with statistics, optimization, and other mathematical subjects.\n\nFor example, the ability of some reinforcement learning methods to learn with parameterized approximators addresses the classical \u201ccurse of dimensionality\u201d in operations research and control theory. More distinctively, reinforcement learning has also interacted strongly with psychology and neuroscience, with substantial bene\ufb01ts going both ways. Of all the forms of machine learning, reinforcement learning is the closest to the kind of learning that humans and other animals do, and many of the core algorithms of reinforcement learning were originally inspired by biological learning systems. Reinforcement learning has also given back, both through a psychological model of animal learning that better matches some of the empirical data, and through an in\ufb02uential model of parts of the brain\u2019s reward system. The body of this book develops the ideas of reinforcement learning that pertain to engineering and arti\ufb01cial intelligence, with connections to psychology and neuroscience summarized in Chapters 14 and 15. Finally, reinforcement learning is also part of a larger trend in arti\ufb01cial intelligence back toward simple general principles", "0c2b9fa1-4a4b-4ee9-a29c-70f0504f1585": "To show that this is true for all natural numbers, we proceed by induction.\n\nAssume the theorem is true for some n then, Since ln(x) is concave, we may apply Jensen\u2019s inequality to obtain the useful result, This allows us to lower-bound a logarithm of a sum, a result that is used in the derivation of the EM algorithm. Jensen\u2019s inequality provides a simple proof that the arithmetic mean is greater than or equal to the geometric mean. Proof: If x1, x2, . , xn \u2265 0 then, since ln(x) is concave we have The EM algorithm is an e\ufb03cient iterative procedure to compute the Maximum Likelihood (ML) estimate in the presence of missing or hidden data. In ML estimation, we wish to estimate the model parameter(s) for which the observed data are the most likely. Each iteration of the EM algorithm consists of two processes: The E-step, and the M-step. In the expectation, or E-step, the missing data are estimated given the observed data and current estimate of the model parameters", "b5461a6e-24ed-4bd0-adb4-a21eb66079ce": "One way to solve his problem is to use a model that learns a different feature space in which a  inear model is able to represent the solution. Specifically, we will introduce a simple feedforward network with one hidden ayer containing two hidden units. See figure 6.2 for an illustration of this model. This feedforward network has a vector of hidden units h that are computed by a unction fO(a; W,,c).\n\nThe values of these hidden units are then used as the input or a second layer. The second layer is the output layer of the network. The output  ayer is still just a linear regression model, but now it is applied to A rather than to aw. https://www.deeplearningbook.org/contents/mlp.html    The nes) ork now contains two functions chained to: ether, h= \u201d (aW 4) and y=fl h;w, 6), with the complete model being /(@; W,c, w,b) = f (f (2). What function should f@) compute? Linear models have served us well so far, and it may be tempting to make f(! linear as well", "960260f7-8b4c-4a0b-a12c-cb0a21290a3a": "Substituting (8.62) into (8.61) and interchanging the sums and products, we obHere we have introduced a set of functions \u00b5fs\u2192x(x), de\ufb01ned by which can be viewed as messages from the factor nodes fs to the variable node x. We see that the required marginal p(x) is given by the product of all the incoming messages arriving at node x. In order to evaluate these messages, we again turn to Figure 8.46 and note that each factor Fs(x, Xs) is described by a factor (sub-)graph and so can itself be factorized. In particular, we can write where, for convenience, we have denoted the variables associated with factor fx, in addition to x, by x1, . , xM. This factorization is illustrated in Figure 8.47. Note that the set of variables {x, x1, . , xM} is the set of variables on which the factor fs depends, and so it can also be denoted xs, using the notation of (8.59)", "5d8664dd-e940-4dd8-abc0-79ed6862f276": "Kaiming H, Xiangyu Z, Shaoqing R, Jian S. Deep residual learning for image recognition. In: CVPR, 2016. 4.\n\nChristian S, Vincent V, Sergey |, Jon S, Zbigniew W. Rethinking the inception architecture for computer vision. arXiv e-prints, 2015. 5. Gao H, Zhuang L, Laurens M, Kilian QW. Densely connected convolutional networks. arXiv preprint. 2016. Jan K, Vladimir G, Daniel C. Regularization for deep learning: a taxonomy. arXiv preprint. 2017 7. Nitish S, Geoffrey H, Alex K, Ilya S, Ruslan S. Dropout: a simple way to prevent neural networks from overfitting. J Mach Learn Res. 2014;15(1):1929-58. 8. Jonathan T, Ross G, Arjun J, Yann L, Christoph B", "03718134-f2f0-45f6-b232-1c724b04ce69": "We see that, for a value of ln \u03bb = \u221218, the over-\ufb01tting has been suppressed and we now obtain a much closer representation of the underlying function sin(2\u03c0x). If, however, we use too large a value for \u03bb then we again obtain a poor \ufb01t, as shown in Figure 1.7 for ln \u03bb = 0. The corresponding coef\ufb01cients from the \ufb01tted polynomials are given in Table 1.2, showing that regularization has the desired effect of reducing The impact of the regularization term on the generalization error can be seen by plotting the value of the RMS error (1.3) for both training and test sets against ln \u03bb, as shown in Figure 1.8. We see that in effect \u03bb now controls the effective complexity of the model and hence determines the degree of over-\ufb01tting.\n\nThe issue of model complexity is an important one and will be discussed at length in Section 1.3. Here we simply note that, if we were trying to solve a practical application using this approach of minimizing an error function, we would have to \ufb01nd a way to determine a suitable value for the model complexity", "4cac9791-1bd6-4a4f-ae1d-14f1e4ee00ad": "11.3 (\u22c6) Given a random variable z that is uniformly distributed over (0, 1), \ufb01nd a transformation y = f(z) such that y has a Cauchy distribution given by (11.8). 11.4 (\u22c6 \u22c6) Suppose that z1 and z2 are uniformly distributed over the unit circle, as shown in Figure 11.3, and that we make the change of variables given by (11.10) and (11.11). Show that (y1, y2) will be distributed according to (11.12). 11.5 (\u22c6) www Let z be a D-dimensional random variable having a Gaussian distribution with zero mean and unit covariance matrix, and suppose that the positive de\ufb01nite symmetric matrix \u03a3 has the Cholesky decomposition \u03a3 = LLT where L is a lowertriangular matrix (i.e., one with zeros above the leading diagonal). Show that the variable y = \u00b5 + Lz has a Gaussian distribution with mean \u00b5 and covariance \u03a3", "a82810d6-a07d-40e1-8cae-d255f2ab4587": "Next we sample the real-valued observable variables given the factors  \u00ab= Wh+ 6+ noise, (13.2) where the noise is typically Gaussian and diagonal (independent across dimensions). This is illustrated in figure 13.1. 13.1 Probabilistic PCA and Factor Analysis  Probabilistic PCA (principal components analysis), factor analysis and other linear factor models are special cases of the above equations (13.1 and 13.2) and only differ in the choices made for the noise distribution and the model\u2019s prior over latent variables h before observing a.\n\nIn factor analysis , the latent variable prior is just the unit variance Gaussian h~ N(h;0,J), (13.3)  while the observed variables x; are assumed to be conditionally independent, given h. Specifically, the noise is assumed to be drawn from a diagonal co-  variance Gaussian distribution, with covariance matrix w = diag(o? ), with o? = ' a vector of per-variable variances. https://www.deeplearningbook.org/contents/linear_factors.html    Lhe role ot the latent variables _is thus to capture the dependencies between the different observed variables \u00ab", "22e1671f-ea77-49fe-9e79-e50fe3daae9a": "We \ufb01rst remove the term \ufffdfj(\u03b8j) from q(\u03b8) to give and then multiply by the exact factor fj(\u03b8j). To determine the re\ufb01ned term \ufffdfjl(\u03b8l), we need only consider the functional dependence on \u03b8l, and so we simply \ufb01nd the corresponding marginal of q\\j(\u03b8)fj(\u03b8j). (10.239) Up to a multiplicative constant, this involves taking the marginal of fj(\u03b8j) multiplied by any terms from q\\j(\u03b8) that are functions of any of the variables in \u03b8j. Terms that correspond to other factors \ufffdfi(\u03b8i) for i \u0338= j will cancel between numerator and denominator when we subsequently divide by q\\j(\u03b8). We therefore obtain We recognize this as the sum-product rule in the form in which messages from variable nodes to factor nodes have been eliminated, as illustrated by the example shown in Figure 8.50", "2e0efde4-e88e-40bc-b71c-6b6933891c81": "A general feature of reinforcement learning is the trade-off between exploration, in which the system tries out new kinds of actions to see how effective they are, and exploitation, in which the system makes use of actions that are known to yield a high reward. Too strong a focus on either exploration or exploitation will yield poor results. Reinforcement learning continues to be an active area of machine learning research. However, a detailed treatment lies beyond the scope of this book. Although each of these tasks needs its own tools and techniques, many of the key ideas that underpin them are common to all such problems.\n\nOne of the main goals of this chapter is to introduce, in a relatively informal way, several of the most important of these concepts and to illustrate them using simple examples. Later in the book we shall see these same ideas re-emerge in the context of more sophisticated models that are applicable to real-world pattern recognition applications. This chapter also provides a self-contained introduction to three important tools that will be used throughout the book, namely probability theory, decision theory, and information theory. Although these might sound like daunting topics, they are in fact straightforward, and a clear understanding of them is essential if machine learning techniques are to be used to best effect in practical applications. 1.1", "f59bb9dc-cc10-43bb-8b61-94e7db58c558": "There are different ways in which a group of recurrent units can be forced to operate at different time scales. One option is to make the recurrent units leaky, but to have different groups of units associated with different fixed time scales. This was the proposal in Mozer  and has been successfully used in Pascanu et al. Another option is to have explicit and discrete updates taking place at different times, with a different frequency for different groups of units. This is the approach of E] Hihi and Bengio  and Koutnik e\u00a2 al. It worked well on a number of benchmark datasets. 403  https://www.deeplearningbook.org/contents/rnn.html    CHAPFER-+10-_SEQUENCE MODELING: RECURRENT AND-RECURSEVE NEFS-  10.10 The Long Short-Term Memory and Other Gated RNNs  As of this writing, the most effective sequence models used in practical applications are called gated RNNs. These include the long short-term memory and networks based on the gated recurrent unit.\n\nLike leaky units, gated RNNs are based on the idea of creating paths through time that have derivatives that neither vanish nor explode", "4577660b-47e9-43ae-9802-70f7fb8ceb1e": "By contrast, a complex model (such as a ninth order polynomial) can generate a great variety of different data sets, and so its distribution p(D) is spread over a large region of the space of data sets. Because the distributions p(D|Mi) are normalized, we see that the particular data set D0 can have the highest value of the evidence for the model of intermediate complexity. Essentially, the simpler model cannot \ufb01t the data well, whereas the more complex model spreads its predictive probability over too broad a range of data sets and so assigns relatively small probability to any one of them. Implicit in the Bayesian model comparison framework is the assumption that the true distribution from which the data are generated is contained within the set of models under consideration. Provided this is so, we can show that Bayesian model comparison will on average favour the correct model. To see this, consider two models M1 and M2 in which the truth corresponds to M1", "0b537db4-6a88-49b6-95f1-fc6325df3985": "(5.60) We also consider a standard sum-of-squares error function, so that for pattern n the error is given by where yk is the activation of output unit k, and tk is the corresponding target, for a particular input pattern xn.\n\nFor each pattern in the training set in turn, we \ufb01rst perform a forward propagation using Next we compute the \u03b4\u2019s for each output unit using Then we backpropagate these to obtain \u03b4s for the hidden units using One of the most important aspects of backpropagation is its computational ef\ufb01ciency. To understand this, let us examine how the number of computer operations required to evaluate the derivatives of the error function scales with the total number W of weights and biases in the network. A single evaluation of the error function (for a given input pattern) would require O(W) operations, for suf\ufb01ciently large W. This follows from the fact that, except for a network with very sparse connections, the number of weights is typically much greater than the number of units, and so the bulk of the computational effort in forward propagation is concerned with evaluating the sums in (5.48), with the evaluation of the activation functions representing a small overhead", "9a786362-382d-4c70-ae6b-ba6d8218c607": "For this, we need the gradient vector and Hessian matrix of the log posterior distribution, which from (7.109) are given by Exercise 7.18 where B is an N \u00d7 N diagonal matrix with elements bn = yn(1 \u2212 yn), the vector y = (y1, . , yN)T, and \u03a6 is the design matrix with elements \u03a6ni = \u03c6i(xn). Here we have used the property (4.88) for the derivative of the logistic sigmoid function.\n\nAt convergence of the IRLS algorithm, the negative Hessian represents the inverse covariance matrix for the Gaussian approximation to the posterior distribution. The mode of the resulting approximation to the posterior distribution, corresponding to the mean of the Gaussian approximation, is obtained setting (7.110) to zero, giving the mean and covariance of the Laplace approximation in the form We can now use this Laplace approximation to evaluate the marginal likelihood", "3b6ab032-9a46-4f10-aa32-e59492a5cf23": "p(a| b) = (18.18)  Unfortunately, in order to compute the log-likelihood, we need to marginalize out large sets of variables. If there are n variables total, we must marginalize a set of size n \u2014 1. By the chain rule of probability,  log p(x) = log p(a1) + log p(a2 | 21) + +--+ p(an | X1:n-1)- (18.19) In this case, we have made a maximally small, but c can be as large as x2.,. What  if we simply move c into b to reduce the computational cost? This yields the pseudolikelihood  objective function, based on predicting the value  613  CHAPTER 18", "234a1319-0ec1-4ab7-bc4b-c57203709882": "Both tangent propagation and the manifold tangent classifier regularize f(a) to not change very much as x moves along the manifold.\n\nTangent propagation requires the user to manually specify functions that compute the tangent directions (such as specifying that small translations of images remain in the same class manifold), while the manifold tangent classifier estimates the manifold tangent directions by training an autoencoder to fit the training data. The use of autoencoders to estimate manifolds is described in chapter 14. 268  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  Tangent prop has been used not just for supervised learning  but also in the context of reinforcement learning . Tangent propagation is closely related to dataset augmentation. In both cases, the user of the algorithm encodes his or her prior knowledge of the task by specifying a set of transformations that should not alter the output of the network. The difference is that in the case of dataset augmentation, the network is explicitly trained to correctly classify distinct inputs that were created by applying more than an infinitesimal amount of these transformations. Tangent propagation does not require explicitly visiting a new input point", "e2ea3490-c5e9-4524-814d-883abfc7a414": "If we alter just the distribution of DP updates in Baird\u2019s counterexample, from the uniform distribution to the on-policy distribution (which generally requires asynchronous updating), then convergence is guaranteed to a solution with error bounded by (9.14). This example is striking because the TD and DP methods used are arguably the simplest and best-understood bootstrapping methods, and the linear, semi-descent method used is arguably the simplest and best-understood kind of function approximation. The example shows that even the simplest combination of bootstrapping and function approximation can be unstable if the updates are not done according to the on-policy distribution. There are also counterexamples similar to Baird\u2019s showing divergence for Q-learning. This is cause for concern because otherwise Q-learning has the best convergence guarantees of all control methods. Considerable e\u21b5ort has gone into trying to \ufb01nd a remedy to this problem or to obtain some weaker, but still workable, guarantee.\n\nFor example, it may be possible to guarantee convergence of Q-learning as long as the behavior policy is su\ufb03ciently close to the target policy, for example, when it is the \"-greedy policy", "2dbf7436-b627-495d-a0fd-260f1ef4f3b2": "For each class, we have a separate hidden Markov model with its own parameters \u03b8m, and we treat the problem of determining the parameter values as a standard classi\ufb01cation problem in which we optimize the cross-entropy Using Bayes\u2019 theorem this can be expressed in terms of the sequence probabilities associated with the hidden Markov models where p(m) is the prior probability of class m. Optimization of this cost function is more complex than for maximum likelihood , and in particular requires that every training sequence be evaluated under each of the models in order to compute the denominator in (13.73). Hidden Markov models, coupled with discriminative training methods, are widely used in speech recognition . A signi\ufb01cant weakness of the hidden Markov model is the way in which it represents the distribution of times for which the system remains in a given state. To see the problem, note that the probability that a sequence sampled from a given hidden Markov model will spend precisely T steps in state k and then make a transition to a different state is given by and so is an exponentially decaying function of T. For many applications, this will be a very unrealistic model of state duration", "7f93e316-7c21-480a-bad0-66487be52c79": "If it is not in the evidence set, then it is sampled from the conditional distribution p(zi|pai) in which the conditioning variables are set to their currently sampled values.\n\nThe weighting associated with the resulting sample z is then given by This method can be further extended using self-importance sampling  in which the importance sampling distribution is continually updated to re\ufb02ect the current estimated posterior distribution. The rejection sampling method discussed in Section 11.1.2 depends in part for its success on the determination of a suitable value for the constant k. For many pairs of distributions p(z) and q(z), it will be impractical to determine a suitable value for k in that any value that is suf\ufb01ciently large to guarantee a bound on the desired distribution will lead to impractically small acceptance rates. As in the case of rejection sampling, the sampling-importance-resampling (SIR) approach also makes use of a sampling distribution q(z) but avoids having to determine the constant k. There are two stages to the scheme. In the \ufb01rst stage, L samples z(1), . , z(L) are drawn from q(z). Then in the second stage, weights w1,", "229cd1b1-d1f2-44a8-86e4-cc76e3eb3537": "If the search is of su\ufb03cient depth k such that \u03b3k is very small, then the actions will be correspondingly near optimal.\n\nOn the other hand, the deeper the search, the more computation is required, usually resulting in a slower response time. A good example is provided by Tesauro\u2019s grandmaster-level backgammon player, TD-Gammon (Section 16.1). This system used TD learning to learn an afterstate value function through many games of self-play, using a form of heuristic search to make its moves. As a model, TD-Gammon used a priori knowledge of the probabilities of dice rolls and the assumption that the opponent always selected the actions that TD-Gammon rated as best for it. Tesauro found that the deeper the heuristic search, the better the moves made by TD-Gammon, but the longer it took to make each move. Backgammon has a large branching factor, yet moves must be made within a few seconds. It was only feasible to search ahead selectively a few steps, but even so the search resulted in signi\ufb01cantly better action selections", "1ce2ccc8-23f5-4293-869f-6f94cf84d177": "To prepare for our description of the neuroscience experiments supporting the reward prediction error hypothesis, and to provide some context so that the signi\ufb01cance of the hypothesis can be appreciated, we next present some of what is known about dopamine, the brain structures it in\ufb02uences, and how it is involved in reward-based learning. Dopamine is produced as a neurotransmitter by neurons whose cell bodies lie mainly in two clusters of neurons in the midbrain of mammals: the substantia nigra pars compacta (SNpc) and the ventral tegmental area (VTA). Dopamine plays essential roles in many processes in the mammalian brain. Prominent among these are motivation, learning, action-selection, most forms of addiction, and the disorders schizophrenia and Parkinson\u2019s disease.\n\nDopamine is called a neuromodulator because it performs many functions other than direct fast excitation or inhibition of targeted neurons. Although much remains unknown about dopamine\u2019s functions and details of its cellular e\u21b5ects, it is clear that it is fundamental to reward processing in the mammalian brain. Dopamine is not the only neuromodulator involved in reward processing, and its role in aversive situations\u2014punishment\u2014remains controversial", "0b6e6d84-3cca-4fce-b0f4-8f2a127d057b": "These smaller models also have dramatically reduced computational cost in terms of storing the model, performing inference in the model, and drawing samples from the model. 16.2 Using Graphs to Describe Model Structure  Structured probabilistic models use graphs (in the graph theory sense of \u201cnodes\u201d or \u201cvertices\u201d connected by edges) to represent interactions between random variables. Each node represents a random variable. Each edge represents a direct interaction. These direct interactions imply other, indirect interactions, but only the direct interactions need to be explicitly modeled. There is more than one way to describe the interactions in a probability distribution using a graph. In the following sections, we describe some of the  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    most popular and useful approaches. Graphical models can be largely divided into two categories: models based on directed acyclic graphs, and models based on undirected graphs", "d5e1ca69-7a25-4af7-9a37-e7bdd01b7a67": "Let us study the approximate behavior of gradient descent on J by analyzing gradient descent on J:  wl) = wt) \u2014 Vy F(woY) (7.35) = wi) \u2014 eH(w?-) \u2014w*), (7.36) w \u2014 w* = (I-\u00abH)(w-) \u2014 w*). (7.37)  Let us now rewrite this expression in the space of the eigenvectors of H, exploiting the eigendecomposition of H: H = QAQ\", where A is a diagonal matrix and Q is an orthonormal basis of eigenvectors. w') \u2014 w* = (I\u2014 eQAQ')(w) \u2014 w*) (7.38) Q'(w? \u2014 w*) = (I- A)Q\u2122 (ww) \u2014 w*) (7.39) Assuming that w\u00a9) = 0 and that \u20ac is chosen to be small enough to guarantee  |1 \u2014 eX] < 1, the parameter trajectory during training after 7 parameter updates is as follows:  Qlw\u2122 = Q' wu\"", "28072d55-6f4e-4153-b45a-0501c462396c": "In this way an agent could learn to predict and control great numbers of signals, not just long-term reward. Why might it be useful to predict and control signals other than long-term reward? These are auxiliary tasks in that they are extra, in-addition-to, the main task of maximizing reward. One answer is that the ability to predict and control a diverse multitude of signals can constitute a powerful kind of environmental model.\n\nAs we saw in Chapter 8, a good model can enable the agent to get reward more e\ufb03ciently. It takes a couple of further concepts to develop this answer clearly, so we postpone it to the next section. First let\u2019s consider two simpler ways in which a multitude of diverse predictions can be helpful to a reinforcement learning agent. One simple way in which auxiliary tasks can help on the main task is that they may require some of the same representations as are needed on the main task. Some of the auxiliary tasks may be easier, with less delay and a clearer connection between actions and outcomes. If good features can be found early on easy auxiliary tasks, then those features may signi\ufb01cantly speed learning on the main task. There is no necessary reason why this has to be true, but in many cases it seems plausible", "5e1f7eef-bb90-4d6d-9127-9177e89ea6bc": "imo, the  paper is poorly written, but the idea is still interesting. So I'm presenting the idea in my own  language. Key components of MetaNet are:  https://lilianweng.github.io/posts/2018-11-30-meta-learning/ Meta-Learning: Learning to Learn Fast | Lil'Log   An embedding function f\u00bb, parameterized by 8, encodes raw inputs into feature vectors. Similar to Siamese Neural Network, these embeddings are trained to be useful for telling whether two inputs are of the same class (verification task). A base learner model gg, parameterized by weights o, completes the actual learning task. If we stop here, it looks just like Relation Network. MetaNet, in addition, explicitly models the fast weights of both functions and then aggregates them back into the model (See Fig. 8). Therefore we need additional two functions to output fast weights for f and g respectively. F,,,: a LSTM parameterized by w for learning fast weights 0+ of the embedding function f", "f2e73cde-1169-4133-8dd7-0eb77baa0eca": "In the \ufb01nal update above, the new parameter is the old parameter times a scalar constant, 1 + \u21b5(2\u03b3 \u2212 1). If this constant is greater than 1, then the system is unstable and w will go to positive or negative in\ufb01nity depending on its initial value. Here this constant is greater than 1 whenever \u03b3 > 0.5. Note that stability does not depend on the speci\ufb01c step size, as long as \u21b5 > 0. Smaller or larger step sizes would a\u21b5ect the rate at which w goes to in\ufb01nity, but not whether it goes there or not. Key to this example is that the one transition occurs repeatedly without w being updated on other transitions. This is possible under o\u21b5-policy training because the behavior policy might select actions on those other transitions which the target policy never would.\n\nFor these transitions, \u21e2t would be zero and no update would be made. Under on-policy training, however, \u21e2t is always one. Each time there is a transition from the w state to the 2w state, increasing w, there would also have to be a transition out of the 2w state", "478a12ef-36e6-441c-b66b-ae453ce0db37": "In chapter 20, we will see how RBMs can be used to build many deeper models.\n\nHere, we show how the RBM exemplifies many of the practices used in a wide variety of deep graphical models: its units are organized into large groups called layers, the connectivity between layers is described by a matrix, the connectivity is relatively dense, the model is designed to allow efficient Gibbs sampling, and the emphasis of the model design is on freeing the training algorithm to learn latent variables whose semantics were not specified by the designer. In section 20.2, we revisit the RBM in more detail. The canonical RBM is an energy-based model with binary visible and hidden units. Its energy function is  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html   E(v,h) =\u2014b'v\u2014c'h\u2014v' Wh, (16.10) where 0, \u20ac, and W are unconstrained, real valued, learnable parameters. We can see that the model is divided into two groups of units: v and h, and the interaction between them is described by a matrix W. The model is depicted graphically in figure 16.14", "f366c7fc-8f4c-43b7-871f-fee212622b71": "A common strategy discussed in part III of this book is to initialize a supervised model with the parameters learned by an unsupervised model trained on the same inputs. One can also perform supervised training on a related task. Even performing supervised training on an unrelated task can sometimes yield an initialization that offers faster convergence than a random initialization. Some of these initialization strategies may yield faster convergence and better generalization because they encode information about the distribution in the initial parameters of the model. Others apparently perform well primarily because they set the parameters to have the right scale or set different units to compute different functions from each other.\n\n8.5 Algorithms with Adaptive Learning Rates  Neural network researchers have long realized that the learning rate is reliably one of the most difficult to set hyperparameters because it significantly affects model performance. As we discuss in sections 4.3 and 8.2, the cost is often highly  302  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  sensitive to some directions in parameter space and insensitive to others. The momentum algorithm can mitigate these issues somewhat, but it does so at the expense of introducing another hyperparameter. In the face of this, it is natural o ask if there is another way", "46ccbb9d-f688-4e25-bfb9-a2875a645dd3": "Models with poor general- izability have overfitted the training data. One way to discover overfitting is to plot the training and validation accuracy at each epoch during training. The graph below depicts what overfitting might look like when visualizing these accuracies over training epochs (Fig. 1). To build useful Deep Learning models, the validation error must continue to decrease with the training error. Data Augmentation is a very powerful method of achieving this. The augmented data will represent a more comprehensive set of possible data points, thus minimizing the distance between the training and validation set, as well as any future testing sets. Data Augmentation, the focus of this survey, is not the only technique that has been developed to reduce overfitting. The following few paragraphs will introduce other solu- tions available to avoid overfitting in Deep Learning models. This listing is intended to give readers a broader understanding of the context of Data Augmentation. Many other strategies for increasing generalization performance focus on the model\u2019s architecture itself", "7aa6b6b1-bd3b-4a7b-8d95-e9c5ac264d24": "B., Deisseroth, K., Janak, P. H. A Sterling, P., Laughlin, S. Principles of Neural Design. MIT Press, Cambridge, MA. Sternberg, S. Stochastic learning theory. In: Handbook of Mathematical Psychology, Volume II, R. D. Luce, R. R. Bush, and E. Galanter (Eds.). John Wiley & Sons. Sugiyama, M., Hachiya, H., Morimura, T. Statistical Reinforcement Learning: Modern Suri, R. E., Bargas, J., Arbib, M. A. .\n\nModeling functions of striatal dopamine modulation in learning and planning. Neuroscience, 103(1):65\u201385. Suri, R. E., Schultz, W. Learning of sequential movements by neural network model with dopamine-like reinforcement signal. Experimental Brain Research, 121(3):350\u2013354", "0d1de366-2438-4e27-ae78-b6a0c65d7996": "Regularization is any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error.\n\nRegularization is one of the central concerns of the field of machine learning, rivaled in its importance only by optimization. The no free lunch theorem has made it clear that there is no best machine learning algorithm, and, in particular, no best form of regularization. Instead we must choose a form of regularization that is well suited to the particular task we want to solve. The philosophy of deep learning in general and this book in particular is that a wide range of tasks (such as all the intellectual tasks that  117  CHAPTER 5. MACHINE LEARNING BASICS  people can do) may all be solved effectively using very general-purpose forms of regularization. 5.3 Hyperparameters and Validation Sets  https://www.deeplearningbook.org/contents/ml.html    Most machine learning algorithms have hyperparameters, settings that we can use to control the algorithm\u2019s behavior. The values of hyperparameters are not adapted by the learning algorithm itself (though we can design a nested learning procedure in which one learning algorithm learns the best hyperparameters for another learning algorithm)", "4928aafa-1d8f-49fc-a81a-7804abbef495": "As noted above, we have restricted the definition of an operation to be a function that returns a single tensor. Most software implementations need to support operations that can return more than one tensor. For example, if we wish to compute both the maximum value in a tensor and the index of that value, it is best to compute both in a single pass through memory, so it is most efficient to implement this procedure as a single operation with two outputs. We have not described how to control the memory consumption of back- propagation.\n\nBack-propagation often involves summation of many tensors together. In the naive approach, each of these tensors would be computed separately, then all of them would be added in a second step. The naive approach has an overly high memory bottleneck that can be avoided by maintaining a single buffer and adding each value to that buffer as it is computed. Real-world implementations of back-propagation also need to handle various data types, such as 32-bit floating point, 64-bit floating point, and integer values. The policy for handling each of these types takes special care to design", "417372cf-80f9-4e67-9195-e652a90d7eb9": "In fact, this suf\ufb01ciency property holds also for Bayesian inference, although we shall defer discussion of this until Chapter 8 when we have equipped ourselves with the tools of graphical models and can thereby gain a deeper insight into these important concepts.\n\nWe have already encountered the concept of a conjugate prior several times, for example in the context of the Bernoulli distribution (for which the conjugate prior is the beta distribution) or the Gaussian (where the conjugate prior for the mean is a Gaussian, and the conjugate prior for the precision is the Wishart distribution). In general, for a given probability distribution p(x|\u03b7), we can seek a prior p(\u03b7) that is conjugate to the likelihood function, so that the posterior distribution has the same functional form as the prior. For any member of the exponential family (2.194), there exists a conjugate prior that can be written in the form where f(\u03c7, \u03bd) is a normalization coef\ufb01cient, and g(\u03b7) is the same function as appears in (2.194)", "c05e9dda-e582-404c-af92-c045655a4527": "Both rely on predicting the association between a user and an item, either to predict the probability of some action (the user buying the product, or some proxy for this action) or the expected gain (which may depend on the value of the product) if an ad is shown or a recommendation is made regarding that product to that user. The internet is currently financed in great part by various forms of online advertising. Major parts of the economy rely on online shopping. Companies including Amazon and eBay use machine learning, including deep learning, for their product recommendations. Sometimes, the items are not products that are actually for sale. Examples include selecting posts to display on social network news feeds, recommending movies to watch, recommending jokes, recommending advice from experts, matching players for video games, or matching people in dating services.\n\nOften, this association problem is handled like a supervised learning problem: given some information about the item and about the user, predict the proxy of interest (user clicks on ad, user enters a rating, user clicks on a \u201clike\u201d button, user  473  CHAPTER 12", "a5eb5487-6150-49bd-96ef-54ffc3c5d883": "Additional evidence for the reinforcing function of dopamine comes from optogenetic experiments with fruit \ufb02ies, except in these animals dopamine\u2019s e\u21b5ect is the opposite of its e\u21b5ect in mammals: optically triggered bursts of dopamine neuron activity act just like electric foot shock in reinforcing avoidance behavior, at least for the population of dopamine neurons activated . Although none of these optogenetic experiments showed that phasic dopamine neuron activity is speci\ufb01cally like a TD error, they convincingly demonstrated that phasic dopamine neuron activity acts just like \u03b4 acts (or perhaps like minus \u03b4 acts in fruit \ufb02ies) as the reinforcement signal in algorithms for both prediction (classical conditioning) and control (instrumental conditioning).\n\nment signal like reinforcement learning\u2019s \u03b4, then because this is a scalar signal, i.e., a single number, all dopamine neurons in both the SNpc and VTA would be expected to activate moreor-less identically so that they would act in near synchrony to send the same signal to all of the sites their axons target", "aa41bf16-e031-4a7e-b489-8de3a754e18e": "That is, where \u00b5 : S\u21e5Rd0 ! R and \u03c3 : S\u21e5Rd0 ! R+ are two parameterized function approximators. To complete the example we need only give a form for these approximators. For this we divide the policy\u2019s parameter vector into two parts, \u2713 = >, one part to be used for the approximation of the mean and one part for the approximation of the standard deviation. The mean can be approximated as a linear function.\n\nThe standard deviation must always be positive and is better approximated as the exponential of a linear function. Thus where x\u00b5(s) and x\u03c3(s) are state feature vectors perhaps constructed by one of the methods described in Section 9.5. With these de\ufb01nitions, all the algorithms described in the rest of this chapter can be applied to learn to select real-valued actions. Exercise 13.4 Show that for the gaussian policy parameterization (13.19) the eligibility vector has the following two parts: Exercise 13.5 A Bernoulli-logistic unit is a stochastic neuron-like unit used in some ANNs (Section 9.6)", "cd0cfaf0-4fef-4a4a-a8e4-936f6b967134": "They estimate action values for a given policy by averaging the returns of many simulated trajectories that start with each possible action and then follow the given policy. When the action-value estimates are considered to be accurate enough, the action (or one of the actions) having the highest estimated value is executed, after which the process is carried out anew from the resulting next state. As explained by Tesauro and Galperin , who experimented with rollout algorithms for playing backgammon, the term \u201crollout\u201d comes from estimating the value of a backgammon position by playing out, i.e., \u201crolling out,\u201d the position many times to the game\u2019s end with randomly generated sequences of dice rolls, where the moves of both players are made by some \ufb01xed policy. Unlike the Monte Carlo control algorithms described in Chapter 5, the goal of a rollout algorithm is not to estimate a complete optimal action-value function, q\u21e4, or a complete action-value function, q\u21e1, for a given policy \u21e1.\n\nInstead, they produce Monte Carlo estimates of action values only for each current state and for a given policy usually called the rollout policy", "252370a7-9946-4ec4-b706-7aa91a382072": "(3.8)  We can denote independence and conditional independence with compact notation: x Ly means that x and y are independent, while xLy | z means that x and y are conditionally independent given z.\n\n3.8 Expectation, Variance and Covariance  The expectation, or expected value, of some function f(x) with respect to a probability distribution P(x) is the average, or mean value, that f takes on when a is drawn from P. For discrete variables this can be computed with a summation:  x PLf(#)] = LPO) (3.9)  while for continuous variables, it is computed with an integral:  Bxaplf(o)] = / p(x) f(\u00ab)de. (3.10)  58  CHAPTER 3. PROBABILITY AND INFORMATION THEORY  When the identity of the distribution is clear from the context, we may simply write the name of the random variable that the expectation is over, as in Ex. If it is clear which random variable the expectation is over, we may omit the  subscript entirely, as in E. By default, we can assume that E averages over the values of all the random variables inside the brackets", "638e88c2-ba16-4840-adf3-ce4794f12b5e": "The quantity zn, which corresponds to the nth element of z, can then be given a simple interpretation as an effective target value in this space obtained by making a local linear approximation to the logistic sigmoid function around the current operating point w(old) In our discussion of generative models for multiclass classi\ufb01cation, we have Section 4.2 seen that for a large class of distributions, the posterior probabilities are given by a softmax transformation of linear functions of the feature variables, so that where the \u2018activations\u2019 ak are given by There we used maximum likelihood to determine separately the class-conditional densities and the class priors and then found the corresponding posterior probabilities using Bayes\u2019 theorem, thereby implicitly determining the parameters {wk}.\n\nHere we consider the use of maximum likelihood to determine the parameters {wk} of this model directly. To do this, we will require the derivatives of yk with respect to all of the activations aj. These are given by Exercise 4.17 where Ikj are the elements of the identity matrix. Next we write down the likelihood function", "5ab70392-41d7-4905-af07-2116a725e1af": "We thank Phil Thomas for helping us make these chapters accessible to non-psychologists and non-neuroscientists, and we thank Peter Sterling for helping us improve the exposition. We are grateful to Jim Houk for introducing us to the subject of information processing in the basal ganglia and for alerting us to other relevant aspects of neuroscience. Jos\u00b4e Mart\u00b4\u0131nez, Terry Sejnowski, David Silver, Gerry Tesauro, Georgios Theocharous, and Phil Thomas generously helped us understand details of their reinforcement learning applications for inclusion in the case-studies chapter, and they provided helpful comments on drafts of these sections. Special thanks are owed to David Silver for helping us better understand Monte Carlo Tree Search and the DeepMind Go-playing programs.\n\nWe thank George Konidaris for his help with the section on the Fourier basis. Emilio Cartoni, Thomas Cederborg, Stefan Dernbach, Clemens Rosenbaum, Patrick Taylor, Thomas Colin, and Pierre-Luc Bacon helped us in a number important ways for which we are most grateful. Sutton would also like to thank the members of the Reinforcement Learning and Arti\ufb01cial Intelligence laboratory at the University of Alberta for contributions to the second edition", "a5199fd3-a53e-42eb-952c-ca418b3f6a5e": "Waltz and Fu  provide an early example of this type of function approximation in a reinforcement learning system. 9.5.4 Tile coding, including hashing, was introduced by Albus . He described it in terms of his \u201ccerebellar model articulator controller,\u201d or CMAC, as tile coding is sometimes known in the literature. The term \u201ctile coding\u201d was new to the \ufb01rst edition of this book, though the idea of describing CMAC in these terms is taken from Watkins . Tile coding has been used in many reinforcement learning systems  as well as in other types of learning control systems .\n\nThis section draws heavily on the work of Miller and Glanz . General software for tile coding is available in several languages (e.g., see http://incompleteideas.net/tiles/tiles3.html). 9.5.5 Function approximation using radial basis functions has received wide attention ever since being related to ANNs by Broomhead and Lowe . Powell  reviewed earlier uses of RBFs, and Poggio and Girosi  extensively developed and applied this approach", "b61e8ee6-cdf5-4505-ab86-db9cc4ec66e6": "Because the weights are invariant to these transformations, we expect the same weight values to reconstruct the data points in the low-dimensional space as in the high-dimensional data space. In spite of the nonlinearity, the optimization for LLE does not exhibit local minima. In isometric feature mapping, or isomap , the goal is to project the data to a lower-dimensional space using MDS, but where the dissimilarities are defined in terms of the geodesic distances measured along the manifold.\n\nFor instance, if two points lie on a circle, then the geodesic is the arc-length distance measured around the circumference of the circle not the straight line distance measured along the chord connecting them. The algorithm first defines the neighbourhood for each data point, either by finding the K nearest neighbours or by finding all points within a sphere of radius E. A graph is then constructed by linking all neighbouring points and labelling them with their Euclidean distance. The geodesic distance between any pair of points is then approximated by the sum of the arc lengths along the shortest path connecting them (which itself is found using standard algorithms). Finally, metric MDS is applied to the geodesic distance matrix to find the low-dimensional projection", "38470462-cae3-430c-a8ad-b81ed7ea98b9": "Earlier we discussed how a held-out test set, composed of examples coming from the same distribution as the training set, can be used to estimate the generalization error of a learner, after the learning process has completed. It is important that the test examples are not used in any way to make choices about the model, including its hyperparameters. For this reason, no example from the test set can be used in the validation set. Therefore, we always construct the validation set from the training data. Specifically, we split the training data into two disjoint subsets. One of these subsets is used to learn the parameters. The other subset is our validation set, used to estimate the generalization error during or after training, allowing for the hyperparameters to be updated accordingly. The subset of data used to learn the parameters is still typically called the training set, even though this may be confused with the larger pool of data used for the entire training process.\n\nThe subset of data used to guide the selection of hyperparameters is called the validation set. Typically, one uses about 80 percent of the training data for training and 20 percent for validation", "acc93b7d-80b7-4e7c-a363-6a063e14f1ba": "Then in final optimization step, we need to update both 6 and \u00a2 to maximize:  ErcclEgtcp,ptcpl > P, (0,51) (ylX)]] (x,y)EBE  Common Approaches  There are three common approaches to meta-learning: metric-based, model-based, and optimization-based.\n\nOriol Vinyals has a nice summary in his talk at meta-learning symposium @ NIPS 2018:  Model-based Metric-based Optimization-based  Key idea RNN; memory Metric learning Gradient descent  How P,(y|x) is modeled? f(x, S) ones ko(x, x;)y; (*) P,4(0,5) (ylx)  (*) kg is a kernel function measuring the similarity between x; and x. Next we are gonna review classic models in each approach. Metric-Based  The core idea in metric-based meta-learning is similar to nearest neighbors algorithms (i.e., k-NN classificer and k-means clustering) and kernel density estimation. The predicted probability over a set of known labels y is a weighted sum of labels of support set samples", "a2ad15b9-80a4-4326-ba75-896c0f674c5b": "One-step actor\u2013critic methods replace the full return of REINFORCE (13.11) with the one-step return (and use a learned state-value function as the baseline) as follows: The natural state-value-function learning method to pair with this is semi-gradient TD(0). Pseudocode for the complete algorithm is given in the box at the top of the next page. Note that it is now a fully online, incremental algorithm, with states, actions, and rewards processed as they occur and then never revisited.\n\nOne-step Actor\u2013Critic (episodic), for estimating \u21e1\u2713 \u21e1 \u21e1\u21e4 Input: a di\u21b5erentiable policy parameterization \u21e1(a|s, \u2713) Input: a di\u21b5erentiable state-value function parameterization \u02c6v(s,w) Parameters: step sizes \u21b5\u2713 > 0, \u21b5w > 0 Initialize policy parameter \u2713 2 Rd0 and state-value weights w 2 Rd (e.g., to 0) Loop forever (for each episode): The generalizations to the forward view of n-step methods and then to a \u03bb-return t respectively", "b5321b14-6a90-45b5-8542-3d3b4e77abbf": "The curve is initialized using the first principal component, and then the algorithm alternates between a data projection step and curve re-estimation step.\n\nIn the projection step, each data point is assigned to a value of >.. corresponding to the closest point on the curve. Then in the re-estimation step, each point on the curve is given by a weighted average of those points that project to nearby points on the curve, with points closest on the curve given the greatest weight. In the case where the subspace is constrained to be linear, the procedure converges to the first principal component and is equivalent to the power method for finding the largest eigenvector of the covariance matrix. Principal curves can be generalized to multidimensional manifolds called principal surfaces although these have found limited use due to the difficulty of data smoothing in higher dimensions even for two-dimensional manifolds. PCA is often used to project a data set onto a lower-dimensional space, for example two dimensional, for the purposes of visualization. Another linear technique with a similar aim is multidimensional scaling, or MDS", "1f5cce33-9fb4-4669-b819-20cb59727bb9": "MORSE, Multi-objective recon\ufb01gurable self-optimizing memory scheduler. In IEEE 18th International Symposium on High Performance Computer Architecture (HPCA), pp. 1\u201312. M\u00a8uller, M. Computer Go. Arti\ufb01cial Intelligence, 134(1):145\u2013179. Munos, R., Stepleton, T., Harutyunyan, A., Bellemare, M. Safe and e\ufb03cient o\u21b5-policy reinforcement learning. In Advances in Neural Information Processing Systems 29 , pp. 1046\u20131054. Curran Associates, Inc. Naddaf, Y. Game-Independent AI Agents for Playing Atari 2600 Console Games. Ph.D. Narendra, K. S., Thathachar, M. A. L. Learning automata\u2014A survey. IEEE Transactions Narendra, K. S., Thathachar, M. A. L. Learning Automata: An Introduction. PrenticeNarendra, K", "6a56908a-9804-4c37-a1cd-5e69087171d6": "Positive de\ufb01niteness also ensures that the inverse A\u22121 exists.\n\nFor linear TD(0), in the continuing case with \u03b3 < 1, the A matrix (9.11) can be where \u00b5(s) is the stationary distribution under \u21e1, p(s0|s) is the probability of transition from s to s0 under policy \u21e1, P is the |S|\u21e5|S| matrix of these probabilities, D is the |S| \u21e5 |S| diagonal matrix with the \u00b5(s) on its diagonal, and X is the |S| \u21e5 d matrix with x(s) as its rows. From here it is clear that the inner matrix D(I \u2212 \u03b3P) is key to determining the positive de\ufb01niteness of A. For a key matrix of this form, positive de\ufb01niteness is assured if all of its columns sum to a nonnegative number. This was shown by Sutton (1988, p. 27) based on two previously established theorems. One theorem says that any matrix M is positive de\ufb01nite if and only if the symmetric matrix S = M + M> is positive de\ufb01nite (Sutton 1988, appendix)", "77f7f950-7a2d-4c99-9ab1-06645700ec55": "Going even further, by taking logarithms and using the series expansion for log(1 + x), we can conclude that if all A; are small (that is, eA; < 1 and A;/a < 1) then  TR, (7.44) Ea 1  an \u2014. (7.45) TE  That is, under these assumptions, the number of training iterations 7 plays a role inversely proportional to the L? regularization parameter, and the inverse of Te plays the role of the weight decay coefficient. Parameter values corresponding to directions of significant curvature (of the objective function) are regularized less than directions of less curvature. Of course, in the context of early stopping, this really means that parameters that correspond to directions of significant curvature tend to learn early relative to parameters corresponding to directions of less curvature.\n\nThe derivations in this section have shown that a trajectory of length 7 ends at a point that corresponds to a minimum of the [?-regularized objective. Early stopping is of course more than the mere restriction of the trajectory length; instead, early stopping typically involves monitoring the validation set error in order to stop the trajectory at a particularly good point in space", "a918d787-c7e6-41e9-87b0-38a35bb54e95": "Second, token-level augmentations (especially word replacement and random swapping) work well in general for supervised learning, especially when there is extremely limited labeled data.\n\nThird, round-trip translation usually works the best for semi-supervised learning, showing the most consistent gains. However, if the computation is limited, cutoff may be a better choice. This work mainly focuses on data augmentation and semi-supervised learning (consistency regularization) in NLP; however, there are other orthogonal directions for tackling the problem of learning with limited data. For completeness, we summarize this related work below. Low-Resourced Languages. Most languages lack large monolingual or parallel corpora, or suf\ufb01cient manually-crafted linguistic resources for building statistical NLP applications . Researchers have therefore developed a variety of methods for improving performance on low-resource languages, including cross-lingual transfer learning which transfers models from resource-rich to resource-poor languages , few/zero-shot learning  which uses only a few examples from the low-resource domain to adapt models trained in another domain, and polyglot learning  which combines resource-rich and resource-poor learning using an universal language representation. Other Methods for Semi-Supervised Learning", "95eaab79-6c49-4063-a9f8-0ca73c99bf57": "In all these cases, the events we would like to predict depend on our acting in a certain way. To learn them all, in parallel, requires learning from the one stream of experience. There are many target policies, and thus the one behavior policy cannot equal all of them. Yet parallel learning is conceptually possible because the behavior policy may overlap in part with many of the target policies. To take full advantage of this requires o\u21b5-policy learning. To better understand the stability challenge of o\u21b5-policy learning, it is helpful to think about value function approximation more abstractly and independently of how learning is done. We can imagine the space of all possible state-value functions\u2014all functions from states to real numbers v : S ! R. Most of these value functions do not correspond to any policy. More important for our purposes is that most are not representable by the function approximator, which by design has far fewer parameters than there are states. Given an enumeration of the state space S = {s1, s2, . , s|S|}, any value function v corresponds to a vector listing the value of each state in order >", "5787f775-4739-459b-9ace-5e59bdd1d316": "One widely used kernel function for Gaussian process regression is given by the exponential of a quadratic form, with the addition of constant and linear terms to give where CN+1 is an (N + 1) \u00d7 (N + 1) covariance matrix with elements given by (6.62). Because this joint distribution is Gaussian, we can apply the results from Section 2.3.1 to \ufb01nd the conditional Gaussian distribution.\n\nTo do this, we partition the covariance matrix as follows where CN is the N \u00d7N covariance matrix with elements given by (6.62) for n, m = 1, . , N, the vector k has elements k(xn, xN+1) for n = 1, . , N, and the scalar c = k(xN+1, xN+1) + \u03b2\u22121. Using the results (2.81) and (2.82), we see that the conditional distribution p(tN+1|t) is a Gaussian distribution with mean and covariance given by These are the key results that de\ufb01ne Gaussian process regression", "9b041755-e6d4-4668-afca-78797b3816b3": "Lo UCL1IUC how much to gather. It is helpful to plot curves showing the relationship between training set size and generalization error, as in figure 5.4. By extrapolatin such curves, one can predict how much additional training data would be needed to achieve a certain level of performance. Usually, adding a small fraction of the total number of examples will not have a noticeable effect on generalization error. It is therefore recommended to experiment with training set sizes on a logarithmic scale,  for example, doubling the number of examples between consecutive experiments. If gathering much more data is not feasible, the only other way to improve generalization error is to improve the learning algorithm itself. This becomes the domain of research and not the domain of advice for applied practitioners. 11.4 Selecting Hyperparameters  Most deep learning algorithms come with several hyperparameters that control many aspects of the algorithm\u2019s behavior. Some of these hyperparameters affect the time and memory cost of running the algorithm. Some of these hyperparameters affect the quality of the model recovered by the training process and its ability to infer correct results when deployed on new inputs.\n\nThere are two basic approaches to choosing these hyperparameters: choosing them manually and choosing them automatically", "41872c4c-1b16-4320-8150-84a265edd132": "These two approaches to determining \u03b1 should of course converge to the same result (assuming they \ufb01nd the same local maximum of the evidence function). This can be veri\ufb01ed by \ufb01rst noting that the quantity \u03b3 is de\ufb01ned by At a stationary point of the evidence function, the re-estimation equation (3.92) will be self-consistently satis\ufb01ed, and hence we can substitute for \u03b3 to give and solving for \u03b1 we obtain (9.63), which is precisely the EM re-estimation equation.\n\nAs a \ufb01nal example, we consider a closely related model, namely the relevance vector machine for regression discussed in Section 7.2.1. There we used direct maximization of the marginal likelihood to derive re-estimation equations for the hyperparameters \u03b1 and \u03b2. Here we consider an alternative approach in which we view the weight vector w as a latent variable and apply the EM algorithm. The E step involves \ufb01nding the posterior distribution over the weights, and this is given by (7.81). In the M step we maximize the expected complete-data log likelihood, which is de\ufb01ned by where the expectation is taken with respect to the posterior distribution computed using the \u2018old\u2019 parameter values", "2b1366e5-6d88-4c05-a345-a56666d7947f": "APPLICATIONS  France ChiRaian ZIV 007  2006 200  Germany Ira Ontario a  2005 12900  999  19918996  1995 2002 Europea itishy,,  https://www.deeplearningbook.org/contents/applications.html    \u201414 South 1Z \u201434 \u201432 -\u201430 -\u201428 \u201426 5.0 35.5 36.0 36.5 37.0 37.5 38.0  Figure 12.3: Two-dimensional visualizations of word embeddings obtained from a neural machine translation model , zooming in on specific areas where semantically related words have embedding vectors that are close to each other. Countries appear on the left and numbers on the right. Keep in mind that these embeddings are 2-D for the purpose of visualization. In real applications, embeddings typically have higher dimensionality and can simultaneously capture many kinds of similarity between words. that two words are similar without losing the ability to encode each word as distinct from the other. Neural language models share statistical strength between one word (and its context) and other similar words and contexts", "25951d8f-206e-47c9-95cd-028991a2f868": "If a thief steals your credit card or credit card information, the thief\u2019s purchases will often come from a different probability distribution over purchase types than your own.\n\nThe credit card company can prevent fraud by placing a hold on an account as soon as that card has been used for an uncharacteristic purchase. See Chandola et al. for a survey of anomaly detection methods. Synthesis and sampling: In this type of task, the machine learning al- gorithm is asked to generate new examples that are similar to those in the training data. Synthesis and sampling via machine learning can be useful Law wr nin anwnlinntinnn Law man nnntinn lanwn 2A leen nn AL AAR tannt Lee Land  https://www.deeplearningbook.org/contents/ml.html    1U1L L1cUla APPLUCALIULS WIC BCUCLALILY, lalLye VULULLICDS UL CULLLOLLE vy lialiu would be expensive, boring, or require too much time", "e860d0b2-824a-4524-bdbb-744bc17e2cd1": "and L. Saul (2000, December). Nonlinear dimensionality reduction by locally linear embedding. Science 290, 2323\u20132326. Rubin, D. B. Iteratively reweighted least squares. In Encyclopedia of Statistical Sciences, Volume 4, pp. 272\u2013275. Wiley. Rumelhart, D. E., G. E. Hinton, and R. J. Williams . Learning internal representations by error propagation. In D. E. Rumelhart, J. L. McClelland, and the PDP Research Group (Eds.\n\n), Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1: Foundations, pp. 318\u2013362. MIT Press. Reprinted in Anderson and Rosenfeld . Savage, L. J. The subjective basis of statistical practice. Technical report, Department of Statistics, University of Michigan, Ann Arbor", "67990ac2-be83-4c1c-8ca5-7666b2a7d0a3": "When it is not feasible to sample from p, an alternative is to use importance sampling, presented in section 17.2.\n\nA more general approach is to form a sequence of estimators that converge toward the distribution of interest. That is the approach of Monte Carlo Markov chains (section 17.3). 17.2 Importance Sampling  An important step in the decomposition of the integrand (or summand) used by the Monte Carlo method in equation 17.2 is deciding which part of the integrand should play the role of probability p(a) and which part of the integrand should play the role of the quantity f(a) whose expected value (under that probability distribution) is to be estimated", "7d4316bb-a4a6-4f74-bc0f-21946221ec16": "In this approach, the probability distribution estimated by  che model is represented explicitly as  log Pmodel (X) = log Pmodel (X; 8) + \u20ac, (18.28)  where c is explicitly introduced as an approximation of \u2014 log Z(@). Rather than estimating only 0, the noise contrastive estimation procedure treats c as just another parameter and estimates @ and c simultaneously, using the same algorithm e144 om Tame tee : a Ma  https://www.deeplearningbook.org/contents/partition.html    Tor Doth.\n\nLhe resulting 1g p4seve1A) thUS May Not correspond exactly to a vad probability distribution, but it will become closer and closer to being valid as the estimate of c improves. Such an approach would not be possible using maximum likelihood as the criterion for the estimator. The maximum likelihood criterion would choose to set c arbitrarily high, rather than setting c to create a valid probability distribution. \u2018NCE is also applicable to problems with a tractable partition function, where there is no need to introduce the extra parameter c. However, it has generated the most interest as a means of estimating models with difficult partition functions", "7cd8a043-8951-430f-a69c-485a04105904": "The image on the right was formed by taking each pixel in the original image and subtracting the value of its neighboring pixel on  https://www.deeplearningbook.org/contents/convnets.html    the left.\n\nThis shows the strength of all the vertically oriented edges in the input image, which can be a useful operation for object detection. Both images are 280 pixels tall. The input image is 320 pixels wide, while the output image is 319 pixels wide. This transformation can be described by a convolution kernel containing two elements, and  requires 319 x 280 x 3 = 267,960 floating-point operations (two multiplications and one addition per output pixel) to compute using convolution. To describe the same transformation with a matrix multiplication would take 320 x 280 x 319 x 280, or over eight billion, entries in the matrix, making convolution four billion times more efficient for representing this transformation. The straightforward matrix multiplication algorithm performs over sixteen billion floating point operations, making convolution roughly 60,000 times more efficient computationally. Of course, most of the entries of the matrix would be zero", "5ea93ad1-6efc-4df5-a69e-f7d673968804": "In the \ufb01rst MRP, this is an exact solution, and the BE is zero. In the second MRP, this solution produces a squared error in both B and B0 of 1, such that BE = \u00b5(B)1 + \u00b5(B0)1 = 2 data distribution, have di\u21b5erent BEs; the BE is not learnable. Moreover (and unlike the earlier example for the VE) the minimizing value of w is di\u21b5erent for the two MRPs. For the \ufb01rst MRP, w = 0 minimizes the BE for any \u03b3. For the second MRP, the minimizing w is a complicated function of \u03b3, but in the limit, as \u03b3 ! 1, it is (\u2212 1 2, 0)>. Thus the solution that minimizes BE cannot be estimated from data alone; knowledge of the MRP beyond what is revealed in the data is required. In this sense, it is impossible in principle to pursue the BE as an objective for learning.\n\nIt may be surprising that in the second MRP the BE-minimizing value of A is so far from zero. Recall that A has a dedicated weight and thus its value is unconstrained by function approximation", "f24af605-68d0-402a-b66d-60ccbbf9afaa": "Invariant information clustering for unsupervised image classi\ufb01cation and segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pp. 9865\u20139874, 2019. Kingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\n\nKolesnikov, A., Zhai, X., and Beyer, L. Revisiting self-supervised visual representation learning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp. 1920\u20131929, 2019. Kornblith, S., Shlens, J., and Le, Q. V. Do better ImageNet models transfer better? In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2661\u20132671, 2019. Krause, J., Deng, J., Stark, M., and Fei-Fei, L. Collecting a large-scale dataset of \ufb01ne-grained cars. In Second Workshop on Fine-Grained Visual Categorization, 2013", "075ef079-1179-4c17-bdfe-3061521c0da0": "For continuous state spaces, a common choice is a Gaussian centred on the current state, leading to an important trade-off in determining the variance parameter of this distribution. If the variance is small, then the proportion of accepted transitions will be high, but progress through the state space takes the form of a slow random walk leading to long correlation times. However, if the variance parameter is large, then the rejection rate will be high because, in the kind of complex problems we are considering, many of the proposed steps will be to states for which the probability p(z) is low. Consider a multivariate distribution p(z) having strong correlations between the components of z, as illustrated in Figure 11.10. The scale \u03c1 of the proposal distribution should be as large as possible without incurring high rejection rates. This suggests that \u03c1 should be of the same order as the smallest length scale \u03c3min", "15afe2d9-a4e8-4e42-be32-a649de40a5ff": "CONFRONTING THE PARTITION FUNCTION  Using the supervised task of classifying between training samples and generated samples (with the model energy function used in defining the classifier) to provide a gradient on the model was introduced earlier in various forms . Noise contrastive estimation is based on the idea that a good generative model should be able to distinguish data from noise.\n\nA closely related idea is that a good generative model should be able to generate samples that no classifier can distinguish from data. This idea yields generative adversarial networks (section 20.10.4). 18.7 Estimating the Partition Function  While much of this chapter is dedicated to describing methods that avoid needing to compute the intractable partition function Z(@) associated with an undirected graphical model, in this section we discuss several methods for directly estimating the partition function. Estimating the partition function can be important because we require it if  https://www.deeplearningbook.org/contents/partition.html    we wish to compute the normalized likelihood of data. \u2018l\u2019his is often important in evaluating the model, monitoring training performance, and comparing models to each other", "c13cf56f-553d-4ded-8b04-1da8bae6f1a3": "To do so, we can either add an edge connecting a and c or we can add an edge connecting b and d. In this  example, we choose to add the edge connectinga and c. (Right) To finish the conversion process, we must assign a direction to each edge. When doing so, we must not create any directed cycles. One way to avoid directed cycles is to impose an ordering over the nodes, and always point each edge from the node that comes earlier in the ordering to the node that comes later in the ordering. In this example, we use the variable names to impose alphabetical order. point each edge from the node that comes earlier in the ordering to the node that comes later in the ordering. See figure 16.12 for a demonstration. 16.2.7. Factor Graphs  Factor graphs are another way of drawing undirected models that resolve an ambiguity in the graphical representation of standard undirected model syntax. In an undirected model, the scope of every \u00a2 function must be a subset of some clique in the graph", "f0a362b8-e26f-4825-aba0-3c45208042a0": "20.9.1 Back-Propagating through Discrete Stochastic Operations  https://www.deeplearningbook.org/contents/generative_models.html    When a model emits a discrete variable Y, the reparametrization trick is not applicable. Suppose that the model takes inputs \u00a9 and parameters 9, both encapsulated in the vector w, and combines them with random noise z to produce y:  y = f(ziw). (20.58) Because y is discrete, f must be a step function. The derivatives of a step function are not useful at any point. Right at each step boundary, the derivatives are undefined, but that is a small problem.\n\nThe large problem is that the derivatives are zero almost everywhere on the regions between step boundaries. The derivatives of any cost function J(y) therefore do not give any information for how to update the model parameters 0. The REINFORCE algorithm (REward Increment = nonnegative Factor x Offset Reinforcement x Characteristic Eligibility) provides a framework defining a family of simple but powerful solutions", "ba0e3e93-df47-47f4-9565-7328841efae7": "Tesauro et al. regarded this as a signi\ufb01cant improvement, given that the DD wagering was needed only about 1.5 to 2 times in each game. Because Watson had only a few seconds to bet, as well as to select squares and decide whether or not to buzz in, the computation time needed to make these decisions was a critical factor. The ANN implementation of \u02c6v allowed DD bets to be made quickly enough to meet the time constraints of live play. However, once games could be simulated fast enough through improvements in the simulation software, near the end of a game it was feasible to estimate the value of bets by averaging over many Monte-Carlo trials in which the consequence of each bet was determined by simulating play to the game\u2019s end.\n\nSelecting endgame DD bets in live play based on Monte-Carlo trials instead of the ANN signi\ufb01cantly improved Watson\u2019s performance because errors in value estimates in endgames could seriously a\u21b5ect its chances of winning. Making all the decisions via Monte-Carlo trials might have led to better wagering decisions, but this was simply impossible given the complexity of the game and the time constraints of live play", "f136c317-deeb-4d77-9a08-b44683da9485": "We now make a further distinction between latent variables, denoted Z, and parameters, denoted \u03b8, where parameters are intensive (\ufb01xed in number independent of the size of the data set), whereas latent variables are extensive (scale in number with the size of the data set). For example, in a Gaussian mixture model, the indicator variables zkn (which specify which component k is responsible for generating data point xn) represent the latent variables, whereas the means \u00b5k, precisions \u039bk and mixing proportions \u03c0k represent the parameters. Consider the case of independent identically distributed data. We denote the data values by X = {xn}, where n = 1, . N, with corresponding latent variables Z = {zn}.\n\nNow suppose that the joint distribution of observed and latent variables is a member of the exponential family, parameterized by natural parameters \u03b7 so that We shall also use a conjugate prior for \u03b7, which can be written as Recall that the conjugate prior distribution can be interpreted as a prior number \u03bd0 of observations all having the value \u03c70 for the u vector", "228b15c7-0090-4865-a421-5d6cc583c71d": "Consider the functional dependence of (2.70) on xa in which xb is regarded as a constant. If we pick out all terms that are second order in xa, we have Now consider all of the terms in (2.70) that are linear in xa where we have used \u039bT ba = \u039bab. From our discussion of the general form (2.71), the coef\ufb01cient of xa in this expression must equal \u03a3\u22121 a|b\u00b5a|b and hence where we have made use of (2.73). The results (2.73) and (2.75) are expressed in terms of the partitioned precision matrix of the original joint distribution p(xa, xb)", "0431a0d1-f93c-435e-9ea2-f588a9460bd4": "backward() optimizer.step()  BYOL  Different from the above approaches, interestingly, BYOL  claims to achieve a new state-of-the-art results without using egative samples. It relies on two neural networks, referred to as online and target networks that interact and learn from each other. The target network (parameterized by \u00a3) has the same architecture as the online one (parameterized by 8), but with polyak averaged weights, \u20ac < T\u20ac + (1 \u20147)0. The goal is to learn a presentation y that can be used in downstream tasks. The online network parameterized by @ contains:  An encoder fo; A projector gg;  A predictor qa.\n\nhttps://lilianweng.github.io/posts/2021-05-31-contrastive/     Contrastive Representation Learning | Lil'Log  The target network has the same network architecture, but with different parameter \u20ac, updated by polyak averaging 6: \u20ac < r\u20ac + (1\u20147)0", "c2d98311-b846-469c-9f26-a24d25623527": "We partition the latent variables into three disjoint groups A, B, C and then let us suppose that we are assuming a factorization between C and the remaining latent variables, so that Using the general result (10.9), together with the product rule for probabilities, we see that the optimal solution for q(A, B) is given by We now ask whether this resulting solution will factorize between A and B, in other words whether q\u22c6(A, B) = q\u22c6(A)q\u22c6(B).\n\nThis will happen if, and only if, ln p(A, B|X, C) = ln p(A|X, C) + ln p(B|X, C), that is, if the conditional independence relation A \u22a5\u22a5 B | X, C (10.86) is satis\ufb01ed. We can test to see if this relation does hold, for any choice of A and B by making use of the d-separation criterion. To illustrate this, consider again the Bayesian mixture of Gaussians represented by the directed graph in Figure 10.5, in which we are assuming a variational factorization given by (10.42)", "c87b113d-78a5-4d57-bc79-69b1a79148a1": "(s, a)  acA Q,(s,a) = R(s, a)+y>_ Ps \u201c1V;,(s') cS Ve(s) = J a(a)s)(R(s,a) +7) | PryVa(s')) acA s'cS Q,,(s,a) = R(s,a) + yy Ps, S> m(a'|s')Q,,(s', a\u2019) s'cS alc A  Bellman Optimality Equations  If we are only interested in the optimal values, rather than computing the expectation following a policy, we could jump right into the maximum returns during the alternative updates without using a policy. RECAP: the optimal values V, and Q, are the best returns we can obtain, defined here. V.(s) = max Q. +(8,@) Q. (s,a) = R(s,a) + >> PAV", "74cf4954-9728-4a1e-92c9-d41778fa1fe9": "A regularization term is added to the error function that penalizes changes in the model output when the input is transformed. This leads to the technique of tangent propagation, discussed in Section 5.5.4. 3. Invariance is built into the pre-processing by extracting features that are invariant under the required transformations. Any subsequent regression or classi\ufb01cation system that uses such features as inputs will necessarily also respect these invariances. 4. The \ufb01nal option is to build the invariance properties into the structure of a neural network (or into the de\ufb01nition of a kernel function in the case of techniques such as the relevance vector machine). One way to achieve this is through the use of local receptive \ufb01elds and shared weights, as discussed in the context of convolutional neural networks in Section 5.5.6. Approach 1 is often relatively easy to implement and can be used to encourage complex invariances such as those illustrated in Figure 5.14.\n\nFor sequential training algorithms, this can be done by transforming each input pattern before it is presented to the model so that, if the patterns are being recycled, a different transformation (drawn from an appropriate distribution) is added each time", "8f25d304-007d-4dee-9ca8-9755ebc30dec": "Taking advantage of the central limit theorem, which tells us that the mean will be approximately distributed with a normal distribution, we can use the standard error to compute the probability that the true expectation falls in any chosen interval. For example, the 95 percent confidence interval centered  https://www.deeplearningbook.org/contents/ml.html    on the mean {i is (fm \u2014 1.96SE( fim); fim + 1-96SE(fim)), (5.47)  under the normal distribution with mean /i,,, and variance SE(/j,)?.\n\nIn machine learning experiments, it is common to say that algorithm A is better than algorithm B if the upper bound of the 95 percent confidence interval for the error of algorithm A is less than the lower bound of the 95 percent confidence interval for the error of algorithm B. Example: Bernoulli Distribution We once again consider a set of samples {2, wee ol\u2122} drawn independently and identically from a Bernoulli distribution  (recall P(x; 6) = ge \u2014 g)(i-a ))", "3b1b011c-ccb9-436a-87dd-339051c211a0": "Optimization algorithms for training deep models also typically include some specialization on the specific structure of machine learning objective functions.\n\nTypically, the cost function can be written as an average over the training set, such as  J(8) = E(ay)~Paara/ (Ff (@; 9), y), (8.1)  where L is the per-example loss function, f(a;6@) is the predicted output when the input is z, and fdata is the empirical distribution. In the supervised learning case, y is the target output. Throughout this chapter, we develop the unregularized supervised case, where the arguments to L are f(a;@) and y. It is trivial to extend this development, for example, to include @ or a as arguments, or to exclude y as arguments, to develop various forms of regularization or unsupervised learning. Equation 8.1 defines an objective function with respect to the training set", "fc74a5fe-0b25-4e36-a05c-02c6b73d6c2a": "The ith feature in the order-n Fourier cosine basis can then be written j 2 {0, . , n} for j = 1, . , k and i = 1, . .\n\n, (n+1)k. This de\ufb01nes a feature for each of the (n + 1)k possible integer vectors ci. The inner product s>ci has the e\u21b5ect of assigning an integer in {0, . , n} to each dimension of s. As in the one-dimensional case, this integer determines the feature\u2019s frequency along that dimension. The features can of course be shifted and scaled to suit the bounded state space of a particular application. As an example, consider the k = 2 case in which s = (s1, s2)>, where each ci = (ci that de\ufb01nes it (s1 is the horizontal axis and ci is shown as a row vector with the index i omitted). Any zero in c means the feature is constant along that state dimension", "e0438061-ed3f-45fd-8381-3977eae190fa": "By recording the cumulative number of lever presses as a function of time, Skinner and his followers could investigate the e\u21b5ect of di\u21b5erent reinforcement schedules on the animal\u2019s rate of lever-pressing. Modeling results from experiments likes these using the reinforcement learning principles we present in this book is not well developed, but we mention some exceptions in the Bibliographic and Historical Remarks section at the end of this chapter. Another of Skinner\u2019s contributions resulted from his recognition of the e\u21b5ectiveness of training an animal by reinforcing successive approximations of the desired behavior, a process he called shaping. Although this technique had been used by others, including Skinner himself, its signi\ufb01cance was impressed upon him when he and colleagues were attempting to train a pigeon to bowl by swiping a wooden ball with its beak.\n\nAfter waiting for a long time without seeing any swipe that they could reinforce, they ... decided to reinforce any response that had the slightest resemblance to a swipe\u2014perhaps, at \ufb01rst, merely the behavior of looking at the ball\u2014and then to select responses which more closely approximated the \ufb01nal form. The result amazed us", "8e7a210d-4261-420c-98fb-deff53863770": "Unsupervised pretraining can help tasks other than classification, however, and can act to improve optimization rather than being merely a regularizer. For example, it can improve both train and test reconstruction error for deep autoencoders . Erhan et al. performed many experiments to explain several successes of unsupervised pretraining. Improvements to training error and improvements to test error may both be explained in terms of unsupervised pretraining taking the parameters into a region that would otherwise be inaccessible. Neural network training is nondeterministic and converges to a different function every time it is run. Training may halt at a point where the gradient becomes small, a point where early stopping ends training to prevent overfitting, or at a point where the gradient is large, but it is difficult to find a downhill step because of problems such as stochasticity or poor conditioning of the Hessian.\n\nNeural networks that receive unsupervised pretraining consistently halt in the same region of function space,  https://www.deeplearningbook.org/contents/representation.html    while neural networks without pretraining consistently halt in another region. See figure 15.1 for a visualization of this phenomenon", "213f74f1-af96-4ced-8485-ec50b7d166b1": "Generative adversarial networks has been sometimes confused with the related concept of \u201cadversarial examples\u201d . Adversarial examples are examples found by using gradient-based optimization directly on the input to a classi\ufb01cation network, in order to \ufb01nd examples that are similar to the data yet misclassi\ufb01ed. This is different from the present work because adversarial examples are not a mechanism for training a generative model. Instead, adversarial examples are primarily an analysis tool for showing that neural networks behave in intriguing ways, often con\ufb01dently classifying two images differently with high con\ufb01dence even though the difference between them is imperceptible to a human observer. The existence of such adversarial examples does suggest that generative adversarial network training could be inef\ufb01cient, because they show that it is possible to make modern discriminative networks con\ufb01dently recognize a class without emulating any of the human-perceptible attributes of that class", "eeca0732-d36d-4016-89c0-4c84fcbd6cbe": "While this approach allows us to overcome many challenges, it is not without its own complications. One of the major difficulties in graphical modeling is understanding which variables need to be able to interact directly, that is, which graph structures are most suitable for a given problem.\n\nIn section 16.5, we outline two approaches to resolving this difficulty by learning about the dependencies. Finally, we close with a discussion of the unique emphasis that deep learning practitioners place on specific approaches to graphical modeling, in section 16.7. 16.1 The Challenge of Unstructured Modeling  The goal of deep learning is to scale machine learning to the kinds of challenges needed to solve artificial intelligence. This means being able to understand high- dimensional data with rich structure. For example, we would like AI algorithms to be able to understand natural images,! audio waveforms representing speech, and documents containing multiple words and punctuation characters. Classification algorithms can take an input from such a rich high-dimensional distribution and summarize it with a categorical label\u2014what object is in a photo, what word is spoken in a recording, what topic a document is about. The process of classification discards most of the information in the input and produces a single output (or a probability distribution over values of that single output)", "4086147d-9699-48db-b089-bb63a716e190": "Thus the estimate for the group is biased toward the true value of state 100, which is higher than the true value of state 1. One of the most important special cases of function approximation is that in which the approximate function, \u02c6v(\u00b7,w), is a linear function of the weight vector, w. Corresponding to every state s, there is a real-valued vector x(s) .= (x1(s), x2(s), . , xd(s))>, with the same number of components as w. Linear methods approximate the state-value function The vector x(s) is called a feature vector representing state s. Each component xi(s) of x(s) is the value of a function xi : S ! R. We think of a feature as the entirety of one of these functions, and we call its value for a state s a feature of s. For linear methods, features are basis functions because they form a linear basis for the set of approximate functions", "73a68fca-4438-4a09-8fb9-927e794860c4": "If we parametrized the output in terms of standard deviation, the log-likelihood would still involve division as well as squaring. The gradient through the squaring operation can vanish near zero, making it difficult to learn parameters that are squared. Regardless of whether we use standard deviation, variance, or precision, we must ensure that the covariance matrix of the Gaussian is positive definite.\n\nBecause the eigenvalues of the precision  https://www.deeplearningbook.org/contents/mlp.html    matrix are the reciprocals of the eigenvalues of the covariance matrix, this is equivalent to ensuring that the precision matrix is positive definite. If we use a diagonal matrix, or a scalar times the diagonal matrix, then the only condition  we need to enforce on the output of the model is positivity. If we suppose that @ is the raw activation of the model used to determine the diagonal precision, we can use the softplus function to obtain a positive precision vector: 8 = \u00a2(a). This same strategy applies equally if using variance or standard deviation rather than precision or if using a scalar times identity rather than diagonal matrix", "d86f92c9-d0bd-4f56-b975-89a6c0ba6b90": "J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel, T., Hassabis, D. Mastering the game of Go with deep neural networks and tree search. Nature, 529:484\u2013489. Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., Riedmiller, M. Deterministic policy gradient algorithms. In Proceedings of the 31st International Conference on Machine Learning , pp. 387\u2013395", "2c50e40b-c82a-475b-a81b-6180e6a0ac83": "First, an autoencoder is trained to reconstruct the training set. Next, the encoder of the autoencoder is used to transform the entire training set into code space. The generator network is then trained to generate code samples, which may be mapped to visually pleasing samples via the decoder.\n\nUnlike GANs, the cost function is defined only with respect to a batch of examples from both the training set and the generator network. It is not possible to make a training update as a function of only one training example or only  700  https://www.deeplearningbook.org/contents/generative_models.html    CHAPTER 20. DEEP GENERATIVE MODELS  one sample from the generator network. This is because the moments must be computed as an empirical average across many samples. When the batch size is too small, MMD can underestimate the true amount of variation in the distributions being sampled. No finite batch size is sufficiently large to eliminate this problem entirely, but larger batches reduce the amount of underestimation. When the batch size is too large, the training procedure becomes infeasibly slow, because many examples must be processed in order to compute a single small gradient step", "02cf53fc-52e6-4a3b-924a-b7fb3d275b1c": "Additionally, because random replacement only occurs for 1.5% of all tokens (i.e., 10% of 15%), this does not seem to harm the model\u2019s language understanding capability. In Section C.2, we evaluate the impact this procedure. Compared to standard langauge model training, the masked LM only make predictions on 15% of tokens in each batch, which suggests that more pre-training steps may be required for the model to converge.\n\nIn Section C.1 we demonstrate that MLM does converge marginally slower than a leftto-right model (which predicts every token), but the empirical improvements of the MLM model far outweigh the increased training cost. Input =  the man went to  store  he bought a gallon  milk  Input =  the man  to the store  penguin  are flight ##less birds  To generate each training input sequence, we sample two spans of text from the corpus, which we refer to as \u201csentences\u201d even though they are typically much longer than single sentences (but can be shorter also). The \ufb01rst sentence receives the A embedding and the second receives the B embedding. 50% of the time B is the actual next sentence that follows A and 50% of the time it is a random sentence, which is done for the \u201cnext sentence prediction\u201d task", "a49c9884-09f9-4cbe-b782-cf154c0fba45": "From the four-argument dynamics function, p, one can compute anything else one might want to know about the environment, such as the state-transition probabilities (which we denote, with a slight abuse of notation, as a three-argument function p : S\u21e5S\u21e5A ! ), and the expected rewards for state\u2013action\u2013next-state triples as a three-argument function r : S \u21e5 A \u21e5 S ! R, In this book, we usually use the four-argument p function (3.2), but each of these other notations are also occasionally convenient.\n\nThe MDP framework is abstract and \ufb02exible and can be applied to many di\u21b5erent problems in many di\u21b5erent ways. For example, the time steps need not refer to \ufb01xed intervals of real time; they can refer to arbitrary successive stages of decision making and acting. The actions can be low-level controls, such as the voltages applied to the motors of a robot arm, or high-level decisions, such as whether or not to have lunch or to go to graduate school. Similarly, the states can take a wide variety of forms", "952c202a-b407-44a3-932d-0f50390d5452": "If we let St denote the state before the greedy move, and St+1 the state after the move, then the update to the estimated value of St, denoted V (St), can be written as where \u21b5 is a small positive fraction called the step-size parameter, which in\ufb02uences the rate of learning.\n\nThis update rule is an example of a temporal-di\u21b5erence learning method, so called because its changes are based on a di\u21b5erence, V (St+1)\u2212V (St), between estimates at two successive times. The method described above performs quite well on this task. For example, if the step-size parameter is reduced properly over time, then this method converges, for any \ufb01xed opponent, to the true probabilities of winning from each state given optimal play by our player. Furthermore, the moves then taken (except on exploratory moves) are in fact the optimal moves against this (imperfect) opponent. In other words, the method converges to an optimal policy for playing the game against this opponent. If the step-size parameter is not reduced all the way to zero over time, then this player also plays well against opponents that slowly change their way of playing. This example illustrates the di\u21b5erences between evolutionary methods and methods that learn value functions", "dfe78b00-7798-44d7-984d-4987dc538800": "{Rig + (Rive + yRizg +...)/S: = 5]  = E  Similarly for Q-value,  Q(s,@a) = E = E  Bellman Expectation Equations  The recursive update process can be further decomposed to be equations built on both state-value and action-value functions.\n\nAs we go further in future action steps, we extend V and Q alternatively by following the policy 7. Q,(s,a) r  https://lilianweng.github.io/posts/2018-02-19-rl-overview/  A (Long) Peek into Reinforcement Learning | Lil'Log   V,(s) = > x(als)Q,", "385f82cc-cd32-4bf5-ad34-ba880e48a34f": "13.18 (\u22c6 \u22c6 \u22c6) Using the result of Exercise 13.17, derive the recursion equations, including the initial conditions, for the forward-backward algorithm for the input-output hidden Markov model shown in Figure 13.18. 13.19 (\u22c6) www The Kalman \ufb01lter and smoother equations allow the posterior distributions over individual latent variables, conditioned on all of the observed variables, to be found ef\ufb01ciently for linear dynamical systems. Show that the sequence of latent variable values obtained by maximizing each of these posterior distributions individually is the same as the most probable sequence of latent values. To do this, simply note that the joint distribution of all latent and observed variables in a linear dynamical system is Gaussian, and hence all conditionals and marginals will also be Gaussian, and then make use of the result (2.98)", "49204f5e-675c-4965-8822-fb2af29d5801": "If the posterior distribution p(\u03b1, \u03b2|t) is sharply peaked around values \ufffd\u03b1 and \ufffd\u03b2, then the predictive distribution is obtained simply by marginalizing over w in which \u03b1 and \u03b2 are \ufb01xed to the values \ufffd\u03b1 and \ufffd\u03b2, so that From Bayes\u2019 theorem, the posterior distribution for \u03b1 and \u03b2 is given by If the prior is relatively \ufb02at, then in the evidence framework the values of \ufffd\u03b1 and \ufffd\u03b2 are obtained by maximizing the marginal likelihood function p(t|\u03b1, \u03b2). We shall proceed by evaluating the marginal likelihood for the linear basis function model and then \ufb01nding its maxima. This will allow us to determine values for these hyperparameters from the training data alone, without recourse to cross-validation. Recall that the ratio \u03b1/\u03b2 is analogous to a regularization parameter. As an aside it is worth noting that, if we de\ufb01ne conjugate (Gamma) prior distributions over \u03b1 and \u03b2, then the marginalization over these hyperparameters in (3.74) can be performed analytically to give a Student\u2019s t-distribution over w (see Section 2.3.7)", "7d3e0744-dd44-4be1-9a90-da3dd55cd4b4": "The factor At \u2212\u21e1(1|St, \u2713) in (15.3) is positive when At = 1 and negative otherwise. The postsynaptic contingency in the eligibility traces of actor units is the only di\u21b5erence between the critic and actor learning rules. By keeping information about what actions were taken in what states, contingent eligibility traces allow credit for reward (positive \u03b4), or blame for punishment (negative \u03b4), to be apportioned among the policy parameters (the e\ufb03cacies of the actor units\u2019 synapses) according to the contributions these parameters made to the units\u2019 outputs that could have in\ufb02uenced later values of \u03b4. Contingent eligibility traces mark the synapses as to how they should be modi\ufb01ed to alter the units\u2019 future responses to favor positive values of \u03b4.\n\nWhat do the critic and actor learning rules suggest about how e\ufb03cacies of corticostriatal synapses change? Both learning rules are related to Donald Hebb\u2019s classic proposal that whenever a presynaptic signal participates in activating the postsynaptic neuron, the synapse\u2019s e\ufb03cacy increases", "a9f58dff-3ceb-4bea-bcf5-de812e817822": "Running inference in the trained model has the same cost per example as if dropout were not used, though we must pay the cost of dividing the weights by 2 once before beginning to run inference on examples.\n\nAnother significant advantage of dropout is that it does not significantly limit the type of model or training procedure that can be used. It works well with nearly any model that uses a distributed representation and can be trained with stochastic gradient descent. This includes feedforward neural networks, probabilistic models such as restricted Boltzmann machines , and recurrent neural networks . Many other regularization strategies of comparable power impose more severe restrictions on che architecture of the model. Though the cost per step of applying dropout to a specific model is negligible, he cost of using dropout in a complete system can be significant. Because dropout is a regularization technique, it reduces the effective capacity of a model. To offset his effect, we must increase the size of the model. Typically the optimal validation set error is much lower when using dropout, but this comes at the cost of a much arger model and many more iterations of the training algorithm. For very large datasets, regularization confers little reduction in generalization error", "7dd2e8a9-ef9f-49dc-882a-b60a70d6e6bd": "w': dw <\u2014 dw + Vy(R\u2014V(s;;w'))?. Update synchronously 8 using d@, and w using dw. https://lilianweng.github.io/posts/2018-02-19-rl-overview/  A (Long) Peek into Reinforcement Learning | Lil'Log   A3C enables the parallelism in multiple agent training. The gradient accumulation step (6.2) can be considered as a reformation of minibatch-based stochastic gradient update: the values of w or 8 get corrected by a little bit in the direction of each training thread independently. Evolution Strategies  Evolution Strategies (ES) is a type of model-agnostic optimization approach. It learns the optimal solution by imitating Darwin's theory of the evolution of species by natural selection. Two prerequisites for applying ES: (1) our solutions can freely interact with the environment and see whether they can solve the problem; (2) we are able to compute a fitness score of how good each solution is. We don't have to know the environment configuration to solve the problem.\n\nSay, we start with a population of random solutions", "d61f4837-6bfe-40b4-a5b0-6fe3b2947a99": "If each of the agent\u2019s actions happened to in\ufb02uence only the immediate reward, not future rewards as well, then a myopic agent could maximize (3.8) by separately maximizing each immediate reward. But in general, acting to maximize immediate reward can reduce access to future rewards so that the return is reduced. As \u03b3 approaches 1, the return objective takes future rewards into account more strongly; the agent becomes more farsighted. Returns at successive time steps are related to each other in a way that is important for the theory and algorithms of reinforcement learning: Note that this works for all time steps t < T, even if termination occurs at t + 1, if we de\ufb01ne GT = 0. This often makes it easy to compute returns from reward sequences. Note that although the return (3.8) is a sum of an in\ufb01nite number of terms, it is still \ufb01nite if the reward is nonzero and constant\u2014if \u03b3 < 1.\n\nFor example, if the reward is a constant +1, then the return is Exercise 3.5 The equations in Section 3.1 are for the continuing case and need to be modi\ufb01ed (very slightly) to apply to episodic tasks. Show that you know the modi\ufb01cations needed by giving the modi\ufb01ed version of (3.3). \u21e4 after each failure", "c3157e00-963d-4c11-a777-2483507f9c03": "10.1 Semi-gradient Sarsa with function approximation was \ufb01rst explored by Rummery and Niranjan . Linear semi-gradient Sarsa with \"-greedy action selection does not converge in the usual sense, but does enter a bounded region near the best solution . Precup and Perkins  showed convergence in a di\u21b5erentiable action selection setting. See also Perkins and Pendrith  and Melo, Meyn, and Ribeiro . The mountain\u2013car example is based on a similar task studied by Moore , but the exact form used here is from Sutton . 10.3 The average-reward formulation has been described for dynamic programming  and from the point of view of reinforcement learning . The algorithm described here is the on-policy analog of the \u201cR-learning\u201d algorithm introduced by Schwartz . The name R-learning was probably meant to be the alphabetic successor to Q-learning, but we prefer to think of it as a reference to the learning of di\u21b5erential or relative values. The access-control queuing example was suggested by the work of Carlstr\u00a8om and Nordstr\u00a8om", "a2a95e2a-cc67-4dc2-8612-eaaac8a690f6": "= g(hox) # projection end for for alli \u20ac {1,..., 2N} andj \u20ac {1,...,2N} do 8ij = 2 2;/(lzillllz;ll) # pairwise similarity end for  define ((i, j) a8 &(i,j)=\u2014log spe PPOs  L= DL  update networks f and g to minimize \u00a3  end for  return encoder network f(-), and throw away g(-)  Barlow Twins  Barlow Twins  feeds two distorted versions of samples into the same network to extract features and learns to make the cross-correlation matrix between these two groups of output features close to the identity. The goal is to keep the representation vectors of different distorted versions of one sample similar, while minimizing the redundancy between these vectors. Distorted images  Empirical Target cross-corr. cross-corr. c I * 4 Ler >  feature dimension  Let C be a cross-correlation matrix computed between outputs from two identical networks along the batch dimension.\n\nC is a square matrix with the size same as the feature network's output dimensionality", "73a7157c-4e19-4283-bc3e-3a94d426f36c": "The distribution of updates can be altered in similar ways to focus on the current state and its likely successors. As a limiting case we might use exactly the methods of heuristic search to construct a search tree, and then perform the individual, one-step updates from bottom up, as suggested by Figure 8.9. If the updates are ordered in this way and a tabular representation is used, then exactly the same overall update would be achieved as in depth-\ufb01rst heuristic search. Any state-space search can be viewed in this way as the piecing together of a large number of individual one-step updates. Thus, the performance improvement observed with deeper searches is not due to the use of multistep updates as such.\n\nInstead, it is due to the focus and concentration of updates on states and actions immediately downstream from the current state. By devoting a large amount of computation speci\ufb01cally relevant to the candidate actions, decision-time planning can produce better decisions than can be produced by relying on unfocused updates. Rollout algorithms are decision-time planning algorithms based on Monte Carlo control applied to simulated trajectories that all begin at the current environment state", "165f40df-1bcf-449a-926d-c0db8c191938": "Ranzato and Hinton  avoid direct sampling from the conditional pmc(a | Am, h&)) by sampling directly from the marginal p(x) using Hamiltonian (hybrid) Monte Carlo  on the mcRBM free energy. ?This version of the Gaussian-Bernoulli RBM energy function assumes the image data have zero mean per pixel. Pixel offsets can easily be added to the model to account for nonzero pixel means. https://www.deeplearningbook.org/contents/generative_models.html    676  CHAPTER 20. DEEP GENERATIVE MODELS  Mean Product of Student t-distributions The mean product of Student t- distribution (mPoT) model  extends the PoT model  in a manner similar to how the mcRBM extends the cRBM", "1a4a049c-85da-4c6f-b667-41fa4cb55238": "This task could be treated as episodic, where the natural episodes are the repeated attempts to balance the pole. The reward in this case could be +1 for every time step on which failure did not occur, so that the return at each time would be the number of steps until failure. In this case, successful balancing forever would mean a return of in\ufb01nity. Alternatively, we could treat pole-balancing as a continuing task, using discounting. In this case the reward would be \u22121 on each failure and zero at all other times. The return at each time would then be related to \u2212\u03b3K, where K is the number of time steps before failure. In either case, the return is maximized by keeping the pole balanced for as long as possible. Exercise 3.7 Imagine that you are designing a robot to run a maze. You decide to give it a reward of +1 for escaping from the maze and a reward of zero at all other times", "1ff183f5-2dbc-47ca-a941-0e2dedbc7374": "For example, imagine we have two models: model M, defining a probabil- ity distribution p4(x;04) = Zi BA (x;@4) and model Mg defining a probability distribution p p(x; Og) = z,,Pb(x;Op)- A common way to compare the models is to evaluate and compare the likelihood that both models assign to an i.i.d. test dataset. Suppose the test set consists of m examples {af! ),..., 2\u00b0}. If II; pa(x: 04) > I], pa & . @5), or equivalently if  S\u00a9 log pa (x; 04) \u2014 S> log pp (x; Ap) > 0, (18.38)  then we say that M 4 is a better model than Mz (or, at least, it is a better model of the test set), in the sense that it has a better test log-likelihood.\n\nUnfortunately, testing whether this condition holds requires knowledge of the partition function. Indeed, equation 18.38 seems to require evaluating the log-probability that the model assigns to each point, which in turn requires evaluating the partition function", "aa2dd9c0-a0b9-41ba-a4b2-6438f22b0224": "APPLICATIONS  (or any pair of words sharing some \u201cfeatures\u201d learned by the model) are close to each other. This often results in words with similar meanings being neighbors.\n\nFigure 12.3 zooms in on specific areas of a learned word embedding space to show how semantically similar words map to representations that are close to each other. Neural networks in other domains also define embeddings. For example, a  https://www.deeplearningbook.org/contents/applications.html    nidden layer Of a convolutional network provides an \u201cImage embedaing.\u201d Usually NLP practitioners are much more interested in this idea of embeddings because natural language does not originally lie in a real-valued vector space. The hidden layer has provided a more qualitatively dramatic change in the way the data is  represented. The basic idea of using distributed representations to improve models for natural language processing is not restricted to neural networks. It may also be used with graphical models that have distributed representations in the form of multiple latent variables . 12.4.3. High-Dimensional Outputs  In many natural language applications, we often want our models to produce words (rather than characters) as the fundamental unit of the output", "3220e585-c52d-4253-a737-07f84ac17ca4": "As shown in the RBM example in section 16.7.1, all the hidden units of an RBM may be sampled simultaneously because they are conditionally independent from each other given all the visible units. Likewise, all the visible units may be sampled simultaneously because they are conditionally independent from each other given all the hidden units. Gibbs sampling approaches that update many variables simultaneously in chis way are called block Gibbs sampling. Alternate approaches to designing Markov chains to sample from pode! are possible. For example, the Metropolis-Hastings algorithm is widely used in other disciplines. In the context of the deep learning approach to undirected modeling, it is rare to use any approach other than Gibbs sampling. Improved sampling  echniques are one possible research frontier. 596  CHAPTER 17.\n\nMONTE CARLO METHODS  17.5 The Challenge of Mixing between Separated Modes  https://www.deeplearningbook.org/contents/monte_carlo.html    Lhe primary diticulty mvolved with MUMU methods Is that they have a tendenc  o M1X poorly", "0c4fb009-9a64-4396-a8a2-6c1d13003ba1": "Different kinds of algorithms use different kinds of information from the mini- batch in various ways.\n\nSome algorithms are more sensitive to sampling error than others, either because they use information that is difficult to estimate accurately with few samples, or because they use information in ways that amplify sampling errors more. Methods that compute updates based only on the gradient g are usually relatively robust and can handle smaller batch sizes, like 100. Second-order methods, which also use the Hessian matrix H and compute updates such as H~\u2014'g, typically require much larger batch sizes, like 10,000. These large batch sizes are required to minimize fluctuations in the estimates of H \u201c1g. Suppose  276  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  that His estimated perfectly but has a poor condition number. Multiplication by HZ or its inverse amplifies pre-existing errors, in this case, estimation errors in g. Very small changes in the estimate of g can thus cause large changes in the update H~1g, even if H is estimated perfectly. Of course, H is estimated only approximately, so the update H~1g will contain even more error than we would predict from applying a poorly conditioned operation to the estimate of g", "ff14dad8-f234-4ba2-8e3f-aa8df3d13519": "\u2217Ian Goodfellow is now a research scientist at Google, but did this work earlier as a UdeM student \u2020Jean Pouget-Abadie did this work while visiting Universit\u00b4e de Montr\u00b4eal from Ecole Polytechnique.\n\n\u2021Sherjil Ozair is visiting Universit\u00b4e de Montr\u00b4eal from Indian Institute of Technology Delhi \u00a7Yoshua Bengio is a CIFAR Senior Fellow. 1All code and hyperparameters available at http://www.github.com/goodfeli/adversarial This framework can yield speci\ufb01c training algorithms for many kinds of model and optimization algorithm. In this article, we explore the special case when the generative model generates samples by passing random noise through a multilayer perceptron, and the discriminative model is also a multilayer perceptron. We refer to this special case as adversarial nets. In this case, we can train both models using only the highly successful backpropagation and dropout algorithms  and sample from the generative model using only forward propagation. No approximate inference or Markov chains are necessary", "45c56263-512a-4493-9180-6f9a7bb42b1a": "This model has exactly the same connectivity as the locally connected layer. The difference lies not in which units interact with each other, but in how the parameters are shared. The locally connected layer has no parameter sharing. The convolutional layer uses the same two weights repeatedly across the entire input, as indicated by the repetition of the letters labeling each edge. (Bottom)A fully connected layer resembles a locally connected layer in the sense that each edge has its own parameter (there are too many to label explicitly with letters in this diagram).\n\nIt does not, however, have the restricted connectivity of the locally connected layer. the output width, this is the same as a locally connected layer. Zijk= D> Vigem\u20141ktn\u20141Ki umn g%eelb%ee (9.10)  lymyn  where percent is the modulo operation, with \u00a2 % = 0, (\u00a2 + 1)%t = 1, and so on. It is straightforward to generalize this equation to use a different tiling range for each dimension. CHAPTER 9", "4aab16b7-08ab-4b31-a524-27ed61397532": "Here we simply note that the presence of the Fisher information matrix causes this kernel to be invariant under a nonlinear re-parameterization of the density model \u03b8 \u2192 \u03c8(\u03b8). Exercise 6.13 In practice, it is often infeasible to evaluate the Fisher information matrix. One approach is simply to replace the expectation in the de\ufb01nition of the Fisher information with the sample average, giving This is the covariance matrix of the Fisher scores, and so the Fisher kernel corresponds to a whitening of these scores. More simply, we can just omit the Fisher Section 12.1.3 information matrix altogether and use the noninvariant kernel An application of Fisher kernels to document retrieval is given by Hofmann . A \ufb01nal example of a kernel function is the sigmoidal kernel given by whose Gram matrix in general is not positive semide\ufb01nite. This form of kernel has, however, been used in practice , possibly because it gives kernel expansions such as the support vector machine a super\ufb01cial resemblance to neural network models", "72ee6847-f6a9-412e-a34d-0376c83cf05f": "we 'hall assume that tbe \"alue of M is g;\\\u00b7en. Latcr in this chapter, we shall consider techniques to determine an appropriate value of IV! from the data.\n\nTo begin with, consider the projection onto a one-dimensional space (M = 1). We can define the direction of this space using a D-dimensional vector Ul, which for convenience (and without loss of generality) we shall choose to be a unit vector so that ufUl = 1 (note that we are only interested in the direction defined by Ul, not in the magnitude of Ul itself). Each data point X n is then projected onto a scalar value ufX n . The mean of the projected data is ufx where x is the sample set mean given by and the variance of the projected data is given by where S is the data covariance matrix defined by We now maximize the projected variance UfSUl with respect to Ul. Clearly, this has to be a constrained maximization to prevent Ilulll ..... 00. The appropriate constraint comes from the normalization condition ufUl = 1", "58688d39-d3ed-40d1-b483-95f6058a3d38": "Of course, this interpretation applies only when examples are not reused. Nonetheless, it is usually best to make several passes through the training set, unless the training set is extremely large. When multiple such epochs are used,  278  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  only the first epoch follows the unbiased gradient of the generalization error, but of course, the additional epochs usually provide enough benefit due to decreased training error to offset the harm they cause by increasing the gap between training error and test error. With some datasets growing rapidly in size, faster than computing power, it is becoming more common for machine learning applications to use each training example only once or even to make an incomplete pass through the training set", "2898bde4-0e63-4534-a817-91d6e20b1051": "REPRESENTATION LEARNING  examples to nearby points in input space.\n\nMany machine learning algorithms leverage this idea, but it is insufficient to overcome the curse of dimensionality. Linearity: Many learning algorithms assume that relationships between some variables are linear. This allows the algorithm to make predictions even very far from the observed data, but can sometimes lead to overly extreme predictions. Most simple machine learning algorithms that do not make the smoothness assumption instead make the linearity assumption. These are in fact different assumptions\u2014linear functions with large weights applied to high-dimensional spaces may not be very smooth. See Goodfellow ef al. for a further discussion of the limitations of the linearity assumption. e Multiple explanatory factors: Many representation learning algorithms are motivated by the assumption that the data is generated by multiple underlying explanatory factors, and that most tasks can be solved easily given the state  of each of these factors. Section 15.3 describes how this view motivates semi- supervised learning via representation learning. Learning the structure of p(a) requires learning some of the same features that are useful for modeling p(y | a) because both refer to the same underlying explanatory factors", "56959ae9-d417-4566-9617-9471ab32500f": "We can obtain a poorly matched q but reduce the computational cost by using an imperfect optimization procedure, or by using a perfect optimization procedure over a restricted family of q distributions. 19.2 Expectation Maximization  The first algorithm we introduce based on maximizing a lower bound C is the expectation maximization (EM) algorithm, a popular training algorithm for models with latent variables. We describe here a view on the EM algorithm developed by Neal and Hinton .\n\nUnlike most of the other algorithms we describe in this chapter, EM is not an approach to approximate inference, but rather an approach to learning with an approximate posterior. The EM algorithm consists of alternating between two steps until convergence:  https://www.deeplearningbook.org/contents/inference.html    - ation step): (0) : are ; atthe beginhing of the step Set AO Pay phe yD To a indices i of the training examples y we want to train on (both batch and minibatch variants are valid)", "09bc6d88-2e8e-40cf-90dc-621ed56e7aac": "Alternately, the product of a global gate (covering a whole group of units, such as an entire layer) and a local gate (per unit) could be used to combine global control and local control. Several investigations over architectural variations of the LSTM and GRU, however, found no variant that would clearly beat both of these across a wide range of tasks . Greff et al. found that a crucial ingredient is the forget gate, while Jozefowicz et al. found that adding a bias of 1 to the LSTM forget gate, a practice advocated by Gers et al. , makes the LSTM as strong as the best of the explored architectural variants. 10.11 Optimization for Long-Term Dependencies  Section 8.2.5 and section 10.7 have described the vanishing and exploding gradient problems that occur when optimizing RNNs over many time steps. An interesting idea proposed by Martens and Sutskever  is that second derivatives may vanish at the same time that first derivatives vanish", "d8a240c9-8904-4bfb-a630-b79a43b512e8": "Like RBMs and DBNs, DBMs lack intralayer connections. DBMs are less closely tied to RBMs than DBNs are. When initializing a DBM from a stack of RBMs, it is necessary to modify the RBM parameters slightly. Some kinds of DBMs may be trained without first training a set of RBMs. 654  CHAPTER 20. DEEP GENERATIVE MODELS  we see later, there are extensions to other types of visible and hidden units.\n\nMore formally, let the observed layer consist of a set of n, binary random variables, which we refer to collectively with the vector v. We refer to the latent, or hidden, layer of n;, binary random variables as h.  Like the general Boltzmann machine, the restricted Boltzmann machine is an energy-based model with the joint probability distribution specified by its energy function:  1 P(v=v,h=h) Z exp (\u2014E(v,h))", "802a1171-7d7e-4a58-9f05-b617f811f9b7": "17.1 General value functions were \ufb01rst explicitly identi\ufb01ed by Sutton and colleagues . Ring (in preparation) developed an extensive thought experiment with GVFs (\u201cforecasts\u201d) that has been in\ufb02uential despite not yet having been published. The \ufb01rst demonstrations of multi-headed learning in reinforcement learning were by Jaderberg et al. .\n\nBellemare, Dabney and Munos  showed that predicting more things about the distribution of reward could signi\ufb01cantly speed learning to optimize its expectation, an instance of auxiliary tasks. Many others have since taken up this line of research. The general theory of classical conditioning as learned predictions together with built-in, re\ufb02exive reactions to the predictions has not to our knowledge been clearly articulated in the psychological literature. Modayil and Sutton  describe it as an approach to the engineering of robots and other agents, calling it \u201cPavlovian control\u201d to allude to its roots in classical conditioning. 17.2 The formalization of temporally abstract courses of action as options was introduced by Sutton, Precup, and Singh , building on prior work by Parr  and Sutton , and on classical work on Semi-MDPs . Precup\u2019s  PhD thesis developed option ideas fully", "3debce2c-bf03-453a-a5ec-54eee6abb7d1": "Why do we use \u2014v(t) and viscous drag in particular? Part of the reason to use \u2014v(t) is mathematical convenience\u2014an integer power of the velocity is easy to work with. Yet other physical systems have other kinds of drag based on other integer powers of the velocity. For example, a particle traveling through the air experiences turbulent drag, with force proportional to the square of the velocity, while a particle moving along the ground experiences dry friction, with a force of constant magnitude. We can reject each of these options. Turbulent drag, proportional to the square of the velocity, becomes very weak when the velocity is small. It is not powerful enough to force the particle to come to rest. A particle with a nonzero initial velocity that experiences only the force of turbulent drag  295  https://www.deeplearningbook.org/contents/optimization.html    CHAPTER 8.\n\nOPTIMIZATION FOR TRAINING DEEP MODELS  will move away from its initial position forever, with the distance from the starting point growing like O(logt). We must therefore use a lower power of the velocity. If we use a power of zero, representing dry friction, then the force is too strong", "ea5844a7-0b5a-48d1-a0ff-1f52d2c6081c": "16.2.1 Directed Models  One kind of structured probabilistic model is the directed graphical model, otherwise known as the belief network or Bayesian network? .\n\nDirected graphical models are called \u201cdirected\u201d because their edges are directed, that is, they point from one vertex to another. This direction is represented in the drawing with an arrow. The direction of the arrow indicates which variable\u2019s probability distribution is defined in terms of the other\u2019s. Drawing an arrow from a to b means that we define the probability distribution over b via a conditional distribution, with a as one of the variables on the right side of the conditioning  ? Judea Pearl suggested using the term \u201cBayesian network\u201d when one wishes to \u201cemphasize the judgmental\u201d nature of the values computed by the network, i.e., to highlight that they usually represent degrees of belief rather than frequencies of events. 560  CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  Alice Bob Carol  Orr  Figure 16.2: A directed graphical model depicting the relay race example. Alice\u2019s finishing time to influences Bob\u2019s finishing time t1, because Bob does not get to start running until Alice finishes", "a5f9ed66-a055-4801-bfe9-6553d303ce3e": "The probability of we detecting the positive sample correctly is:  p(Xpos|c) p(C = pos|X,\u00a2) = P(\u00aeposl\u00a2) L:-1,.....itzpos P(X) ples) F(X p05 \u20ac) = sc) = = = i= i [P (x,e) Ia, iz; P(X) N plas) a S(x;,\u00a2) J=1  p(x;) where the scoring function is f(x, \u00a2) o oe :  The InfoNCE loss optimizes the negative log probability of classifying the positive sample correctly:  x\u2019eX ,  The fact that f(z, c) estimates the density ratio wel) has a connection with mutual information  optimization. To maximize the the mutual information between input xz and context vector c, we have:  Ieee) = C re0) og POE), = Spe tog Mae  where the logarithmic term in blue is estimated by f", "5964ddbb-b39f-4b2e-81f0-2fc1d7875f4b": "However, these must be restricted to a feed-forward architecture, in other words to one having no closed directed cycles, to ensure that the outputs are deterministic functions of the inputs. This is illustrated with a simple example in Figure 5.2.\n\nEach (hidden or output) unit in such a network computes a function given by where the sum runs over all units that send connections to unit k (and a bias parameter is included in the summation). For a given set of values applied to the inputs of the network, successive application of (5.10) allows the activations of all units in the network to be evaluated including those of the output units. The approximation properties of feed-forward networks have been widely studied  and found to be very general. Neural networks are therefore said to be universal approximators. For example, a two-layer network with linear outputs can uniformly approximate any continuous function on a compact input domain to arbitrary accuracy provided the network has a suf\ufb01ciently large number of hidden units. This result holds for a wide range of hidden unit activation functions, but excluding polynomials", "d71e15c8-7d8f-4fad-84f0-29bbba80ff3b": "The first version of this idea is called the mixture of experts , in which the gater outputs a set of probabilities or weights (obtained via a softmax nonlinearity), one per expert, and the final output is obtained by the weighted combination of the output of the experts. In that case, the use of the gater does not offer a reduction in computational cost, but if a single expert is chosen by the gater for each example, we obtain the hard mixture of experts , which can considerably accelerate training and inference time. This strategy works well when the number of gating decisions is small because it is not combinatorial. But  https://www.deeplearningbook.org/contents/applications.html    when we want to select different subsets of units or parameters, it is not possible to use a \u201csoft switch\u201d because it requires gnumerating (and computing outputs for) all the gater configurations. To deal with this problem, several approaches have been explored to train combinatorial gaters. Bengio et al. experiment with  several estimators of the gradient on the gating probabilities, while Bacon ez al.\n\nand Bengio ef al", "f2a933da-81ea-4745-8203-35a76797d9fa": "In PAC learning we say that a function f(x; D), drawn from a space F of such functions on the basis of the training set D, has good generalization if its expected error rate is below some pre-speci\ufb01ed threshold \u03f5, so that where I(\u00b7) is the indicator function, and the expectation is with respect to the distribution p(x, t). The quantity on the left-hand side is a random variable, because it depends on the training set D, and the PAC framework requires that (7.75) holds, with probability greater than 1 \u2212 \u03b4, for a data set D drawn randomly from p(x, t). Here \u03b4 is another pre-speci\ufb01ed parameter, and the terminology \u2018probably approximately correct\u2019 comes from the requirement that with high probability (greater than 1\u2212\u03b4), the error rate be small (less than \u03f5).\n\nFor a given choice of model space F, and for given parameters \u03f5 and \u03b4, PAC learning aims to provide bounds on the minimum size N of data set needed to meet this criterion", "d34acf04-c6d1-40a0-8b86-34508df99839": "Consider \ufb01rst the distribution over \u03b1.\n\nKeeping only terms that have a functional dependence on \u03b1, we have ln q\u22c6(\u03b1) = ln p(\u03b1) + Ew  + const We recognize this as the log of a gamma distribution, and so identifying the coef\ufb01cients of \u03b1 and ln \u03b1 we obtain Similarly, we can \ufb01nd the variational re-estimation equation for the posterior distribution over w. Again, using the general result (10.9), and keeping only those terms that have a functional dependence on w, we have Because this is a quadratic form, the distribution q\u22c6(w) is Gaussian, and so we can complete the square in the usual way to identify the mean and covariance, giving Note the close similarity to the posterior distribution (3.52) obtained when \u03b1 was treated as a \ufb01xed parameter. The difference is that here \u03b1 is replaced by its expectation E under the variational distribution. Indeed, we have chosen to use the same notation for the covariance matrix SN in both cases", "8353dedd-008e-4cbf-bfe9-ae1e4ba8e55f": "The standard SGD method for incrementally \ufb01nding the vector v that minimizes the expected squared error \ufffd2 is known as the Least Mean Square (LMS) rule (here augmented where \u03b2 > 0 is another step-size parameter. We can use this method to e\u21b5ectively achieve (11.28) with O(d) storage and per-step computation. Given a stored estimate vt approximating (11.28), we can update our main parameter vector wt using SGD methods based on (11.27). The simplest such rule is This algorithm is called GTD2. Note that if the \ufb01nal inner product (x> A slightly better algorithm can be derived by doing a few more analytic steps before either TD(0) with gradient correction (TDC) or, alternatively, as GTD(0). ample. As intended, the PBE falls to zero, but note that the individual components of the parameter vector do not approach zero. In fact, these values are still far from an optimal solution, \u02c6v(s) = 0, for all s, for which w would have to be proportional to (1, 1, 1, 1, 1, 1, 4, \u22122)>", "59be364b-2dfb-4c30-851d-1b819361eb93": "With the diffusion inversion objective, the learner can learn the shape of the density around the data points more precisely as well as remove spurious modes that could show up far from the data points. Another approach to sample generation is the approximate Bayesian com- putation (ABC) framework . In this approach, samples are rejected or modified to make the moments of selected functions of the samples  712  ee eee oe oe  https://www.deeplearningbook.org/contents/generative_models.html    UAL bay\u2019 2U, i OLA WLU  match those of the desired distribution. While this idea uses the moments of the samples as in moment matching, it is different from moment matching because it modifies the samples themselves, rather than training the model to automatically emit samples with the correct moments. Bachman and Precup  showed how to use ideas from ABC in the context of deep learning, by using ABC to shape the MCMC trajectories of GSNs. We expect that many other possible approaches to generative modeling await discovery.\n\n20.14 Evaluating Generative Models  Researchers studying generative models often need to compare one generative model to another, usually in order to demonstrate that a newly invented generative model is better at capturing some distribution than the pre-existing models", "b84bb6bc-2f03-47a1-bfd4-218540a306dc": "However, in practice, the overall label density d\ufffd is insuf\ufb01ciently precise to determine the transition points of interest, given a user time-cost trade-off preference (characterized by the advantage tolerance parameter \u03b3 in Algorithm 1). We show this in Table 1 using our application data sets from Sect. 4.1. For example, we see that the Chem and EHR label matrices have equivalent label densities; however, modeling the labeling function accuracies has a much greater effect for EHR than for Chem. Instead of simply considering the average label density d\ufffd, we instead develop a best-case heuristic based on looking at the ratio of positive to negative labels for each data point", "c6985bb9-a0b3-4032-b249-a1f7e5947f1f": "Finally, we owe thanks to the many careful readers of drafts of the second edition that we posted on the internet. They found many errors that we had missed and alerted us to potential points of confusion. We \ufb01rst came to focus on what is now known as reinforcement learning in late 1979. We were both at the University of Massachusetts, working on one of the earliest projects to revive the idea that networks of neuronlike adaptive elements might prove to be a promising approach to arti\ufb01cial adaptive intelligence. The project explored the \u201cheterostatic theory of adaptive systems\u201d developed by A. Harry Klopf. Harry\u2019s work was a rich source of ideas, and we were permitted to explore them critically and compare them with the long history of prior work in adaptive systems. Our task became one of teasing the ideas apart and understanding their relationships and relative importance. This continues today, but in 1979 we came to realize that perhaps the simplest of the ideas, which had long been taken for granted, had received surprisingly little attention from a computational perspective", "79e46a79-24e4-40bd-8230-33c5b87b4777": "N. Vapnik . Support vector networks. Machine Learning 20, 273\u2013297. Cotter, N. E. The Stone-Weierstrass theorem and its application to neural networks. IEEE Transactions on Neural Networks 1(4), 290\u2013295. Cover, T. and P. Hart . Nearest neighbor pattern classi\ufb01cation. IEEE Transactions on Information Theory IT-11, 21\u201327. Cover, T. M. and J. A. Thomas . Elements of Information Theory. Wiley. Cowell, R. G., A. P. Dawid, S. L. Lauritzen, and D. J. Spiegelhalter . Probabilistic Networks and Expert Systems. Springer. Duda, R. O., P. E. Hart, and D. G. Stork", "2b0ade2a-f405-4688-b8a3-1e477388aa65": "Speech Recognition  The task of speech recognition is to map an acoustic signal containing a spoken natural language utterance into the corresponding sequence of words intended by the speaker. Let X = (a, aw .., a (7)) denote the sequence of acoustic input vectors (traditionally produced by splitting the audio into 20ms frames). Most speech recognition systems preprocess the input using specialized hand-designed features, but some  deep learning systems learn features from raw input.\n\nLet y = (yi, y,--., yn) denote the target output sequence (usually a sequence of words or characters). The automatic speech recognition (ASR) task consists of creating a function fAgp that computes the most probable linguistic sequence y given the acoustic sequence X:  fagsn(X) = arg max P*(y |X = X), (12.4) y  where P* is the true conditional distribution relating the inputs X to the targets y", "b268447d-ae2e-4348-9884-a06630fb2cc5": "In this (somewhat incomplete) view, the identity providing the functional derivatives is the same as what we would obtain for a vector 0 \u20ac IR\u201d indexed by positive integers:  (a) (a) \u2014 05,3) = \u20149(9;,%). 19.47 Bg, Db) = 89 (19.47) Many results in other machine learning publications are presented using the more general Euler-Lagrange equation, which allows g to depend on the derivatives of f as well as the value of f, but we do not need this fully general form for the results presented in this book. To optimize a function with respect to a vector, we take the gradient of the function with respect to the vector and solve for the point where every element of the gradient is equal to zero. Likewise, we can optimize a functional by solving for the function where the functional derivative at every point is equal to zero. As an example of how this process works, consider the problem of finding the probability distribution function over x \u20ac R that has maximal differential entropy.\n\nRecall that the entropy of a probability distribution p(\u00ab) is defined as  Hp] = Ez log p()", "c7c91a72-8c85-425d-ad1d-be876b2b8a65": "Rather than being  viewed as an unreliable technology that must be supported by other techniques, gradient-based learning in feedforward networks has been viewed since 2012 as a powerful technology that can be applied to many other machine learning tasks. In 2006, the community used unsupervised learning to support supervised learning, and now, ironically, it is more common to use supervised learning to support unsupervised learning. Feedforward networks continue to have unfulfilled potential. In the future, we expect they will be applied to many more tasks, and that advances in optimization algorithms and model design will improve their performance even further.\n\nThis chapter has primarily described the neural network family of models. In the subsequent chapters, we turn to how to use these models\u2014how to regularize and train them. 223  https://www.deeplearningbook.org/contents/mlp.html", "93579c18-756c-48d7-ab3c-d37c402d4f6c": "In simple linear regression, we Section 3.1.4 minimize a regularized error function given by To obtain sparse solutions, the quadratic error function is replaced by an \u03f5-insensitive error function , which gives zero error if the absolute difference between the prediction y(x) and the target t is less than \u03f5 where \u03f5 > 0. A simple example of an \u03f5-insensitive error function, having a linear cost associated with errors outside the insensitive region, is given by We therefore minimize a regularized error function given by where y(x) is given by (7.1). By convention the (inverse) regularization parameter, denoted C, appears in front of the error term. As before, we can re-express the optimization problem by introducing slack variables.\n\nFor each data point xn, we now need two slack variables \u03ben \u2a7e 0 and \ufffd\u03ben \u2a7e 0, where \u03ben > 0 corresponds to a point for which tn > y(xn) + \u03f5, and \ufffd\u03ben > 0 corresponds to a point for which tn < y(xn) \u2212 \u03f5, as illustrated in Figure 7.7", "2a4aa241-22fd-41e6-9ec0-22960ac61a3a": "The complete Nesterov momentum algorithm is presented in algorithm 8.3.\n\nIn the convex batch gradient case, Nesterov momentum brings the rate of convergence of the excess error from O(1/k) (after k steps) to O(1/k) as shown by Nesterov . Unfortunately, in the stochastic gradient case, Nesterov momentum does not improve the rate of convergence. 8.4 Parameter Initialization Strategies  Some optimization algorithms are not iterative by nature and simply solve for a solution point. Other optimization algorithms are iterative by nature but, when applied to the right class of optimization problems, converge to acceptable solutions in an acceptable amount of time regardless of initialization. Deep learning training algorithms usually do not have either of these luxuries. Training algorithms for  https://www.deeplearningbook.org/contents/optimization.html    296  CHAPTER 8", "5488e3bb-9ea3-48b4-9ddb-ad08e4c190d5": "To estimate v\u21e1(s), we simply scale the returns by the ratios and average the results: When importance sampling is done as a simple average in this way it is called ordinary importance sampling. An important alternative is weighted importance sampling, which uses a weighted or zero if the denominator is zero.\n\nTo understand these two varieties of importance sampling, consider the estimates of their \ufb01rst-visit methods after observing a single return from state s. In the weighted-average estimate, the ratio \u21e2t:T (t)\u22121 for the single return cancels in the numerator and denominator, so that the estimate is equal to the observed return independent of the ratio (assuming the ratio is nonzero). Given that this return was the only one observed, this is a reasonable estimate, but its expectation is vb(s) rather than v\u21e1(s), and in this statistical sense it is biased. In contrast, the \ufb01rst-visit version of the ordinary importance-sampling estimator (5.5) is always v\u21e1(s) in expectation (it is unbiased), but it can be extreme. Suppose the ratio were ten, indicating that the trajectory observed is ten times as likely under the target policy as under the behavior policy", "047d1449-4ec5-47e3-9ed0-51c3d00156e0": "Instead, the encoder is an optimization algorithm, which solves an optimization problem in which we seek the single most likely code value:  h* = f(x) = argmax p(h | x).\n\n(13.15) h When combined with equation 13.13 and equation 13.12, this yields the following  https://www.deeplearningbook.org/contents/linear_factors.html    optimization problem:  arg max p(h | x) (13.16)  =arg max log p(h | x) (13.17) h  =arg min Alf + dll \u2014 Wh (13.18)  where we have dropped terms not depending on h and divided by positive scaling factors to simplify the equation. Due to the imposition of an L' norm on h, this procedure will yield a sparse h* (see section 7.1.2). To train the model rather than just perform inference, we alternate between minimization with respect to h and minimization with respect to W. In this presentation, we treat 6 as a hyperparameter. Typically it is set to 1 because its role in this optimization problem is shared with A, and there is no need for both hyperparameters", "27a65ec2-2224-4c36-a05a-493042e773a8": "There is no accepted guideline for when to call a model an energy-based model and when to call it a Boltzmann machine. The term Boltzmann machine was first introduced to describe a model with exclusively binary variables, but today many models such as the mean-covariance restricted Boltzmann machine incorporate real valued variables as well. While Boltzmann machines were originally defined to encompass both models with and without la- tent variables, the term Boltzmann machine is today most often used to designate models with latent variables, while Boltzmann machines without latent variables are more often called Markov random fields or log-linear models. Cliques in an undirected graph correspond to factors of the unnormalized probability function.\n\nBecause exp(a) exp(b) = exp(a+b), this means that different cliques in the undirected graph correspond to the different terms of the energy function. In other words, an energy-based model is just a special kind of Markov network: the exponentiation makes each term in the energy function correspond to a factor for a different clique. See figure 16.5 for an example of how to read the form of the energy function from an undirected graph structure", "b0452767-7a9c-4f6c-8db9-e8dabd0041b6": "The primary goal of manual hyperparameter search is to adjust the effective capacity of the model to match the complexity of the task.\n\nEffective capacity is constrained by three factors: the representational capacity of the model, the ability of the learning algorithm to successfully minimize the cost function used to train the model, and the degree to which the cost function and training procedure regularize the model. A model with more layers and more hidden units per layer has higher representational capacity\u2014it is capable of representing more complicated functions. It cannot necessarily learn all these functions though, if the training algorithm cannot discover that certain functions do a good job of minimizing the training cost, or if regularization terms such as weight decay forbid some of these functions. The generalization error typically follows a U-shaped curve when plotted as a function of one of the hyperparameters, as in figure 5.3. At one extreme, the hyperparameter value corresponds to low capacity, and generalization error is high because training error is high. This is the underfitting regime. At the other extreme, the hyperparameter value corresponds to high capacity, and the generalization error is high because the gap between training and test error is high", "ce9b3b79-2590-408f-be75-6f7b863dc9dd": "Now suppose that we make a nonsingular linear transformation of the data variables x ---t Ax, where A is a D x D matrix.\n\nIf JLML' W ML and <PML represent the maximum likelihood solution corresponding to the original untransformed data, show that AJLML' AWML, and A <PMLAT will represent the corresponding maximum likelihood solution for the transformed data set. Finally, show that the form of the model is preserved in two cases: (i) A is a diagonal matrix and <P is a diagonal matrix. This corresponds to the case of factor analysis. The transformed <P remains diagonal, and hence factor analysis is covariant under component-wise re-scaling of the data variables; (ii) A is orthogonal and <P is proportional to the unit matrix so that <P = 0-21. This corresponds to probabilistic PCA. The transformed <P matrix remains proportional to the unit matrix, and hence probabilistic PCA is covariant under a rotation of the axes of data space, as is the case for conventional PCA", "b628f3aa-8e15-4d9f-8a8f-aa5b7ef09508": "In arelated spirit, the tangent prop algorithm  (figure 7.9) trains a neural net classifier with an extra penalty to make each output f(a) of the neural net locally invariant to known factors of variation. These factors of variation correspond to movement along the manifold near which examples of the same class concentrate. Local invariance is achieved by requiring Vaf (x) to be orthogonal to the known manifold tangent vectors v\u00ae at x, or equivalently that the directional derivative of f at x in the directions v be small by adding a regularization penalty 2:  2  MF) =D ((Weh(@)' ) (7.67)  a  This regularizer can of course be scaled by an appropriate hyperparameter, and for most neural networks, we would need to sum over many outputs rather than the lone output f(a) described here for simplicity.\n\nAs with the tangent distance algorithm, the tangent vectors are derived a priori, usually from the formal knowledge of the effect of transformations, such as translation, rotation, and scaling in images. 267  CHAPTER 7", "92734d0c-7342-47e6-8ecc-959b8811647e": "The idea is \ufb01rst to initialize the variables {xi}, which we do by simply setting xi = yi for all i. Then we take one node xj at a time and we evaluate the total energy for the two possible states xj = +1 and xj = \u22121, keeping all other node variables \ufb01xed, and set xj to whichever state has the lower energy. This will either leave the probability unchanged, if xj is unchanged, or will increase it. Because only one variable is changed, this is a simple local computation that can be performed Exercise 8.13 ef\ufb01ciently. We then repeat the update for another site, and so on, until some suitable stopping criterion is satis\ufb01ed.\n\nThe nodes may be updated in a systematic way, for instance by repeatedly raster scanning through the image, or by choosing nodes at random. If we have a sequence of updates in which every site is visited at least once, and in which no changes to the variables are made, then by de\ufb01nition the algorithm will have converged to a local maximum of the probability. This need not, however, correspond to the global maximum", "dba2718d-eaa1-406a-9703-5b3dd5fce2bf": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4885\u20134901. Jekaterina Novikova, Ond\u02c7rej Du\u0161ek, and Verena Rieser. 2017. The e2e dataset: New challenges for end-toend generation. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 201\u2013206. Richard Yuanzhe Pang and He He. 2021. Text generation by learning from demonstrations. In International Conference on Learning Representations. Ramakanth Pasunuru and Mohit Bansal. 2017. Multitask video captioning with video and entailment generation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1273\u20131283. Ramakanth Pasunuru and Mohit Bansal. 2018", "2a059a7d-2311-4aba-9e8d-f6aab0884b06": "When that symbol is generated, the sampling process stops. In the training set, we insert this symbol as an extra member of the sequence, immediately after x\u2018) in each training example.\n\nAnother option is to introduce an extra Bernoulli output to the model that represents the decision to either continue generation or halt generation at each time step. This approach is more general than the approach of adding an extra symbol to the vocabulary, because it may be applied to any RNN, rather than  awle- DNINTAD 4 a4 22-4 eek 2 Aarne nn Af aL. Te nee ee Le nd te  https://www.deeplearningbook.org/contents/rnn.html    ULLLy PVULNINS Lllat VULPUL &@ SCYUCLICE UL SYLUIVUIS. PUL CxaLIpIE, lb Wlay VE aPpplicu LU an RNN that emits a sequence of real numbers. The new output unit is usually a sigmoid unit trained with the cross-entropy loss", "6d23aab4-40ad-497b-be37-a20392ad781d": "For this reason, the use of gradient information forms the basis of practical algorithms for training neural networks.\n\nThe simplest approach to using gradient information is to choose the weight update in (5.27) to comprise a small step in the direction of the negative gradient, so that w(\u03c4+1) = w(\u03c4) \u2212 \u03b7\u2207E(w(\u03c4)) (5.41) where the parameter \u03b7 > 0 is known as the learning rate. After each such update, the gradient is re-evaluated for the new weight vector and the process repeated. Note that the error function is de\ufb01ned with respect to a training set, and so each step requires that the entire training set be processed in order to evaluate \u2207E. Techniques that use the whole data set at once are called batch methods. At each step the weight vector is moved in the direction of the greatest rate of decrease of the error function, and so this approach is known as gradient descent or steepest descent. Although such an approach might intuitively seem reasonable, in fact it turns out to be a poor algorithm, for reasons discussed in Bishop and Nabney . For batch optimization, there are more ef\ufb01cient methods, such as conjugate gradients and quasi-Newton methods, which are much more robust and much faster than simple gradient descent", "e44295a1-5b46-4419-a108-92fc727de9a2": "This is accomplished by making the kernel smaller than the input. For example, when processing an image, the input image might have thousands or millions of pixels, but we can detect small, meaningful features such as edges with kernels that occupy only tens or hundreds of pixels. This means that we need to store fewer parameters, which both reduces the memory requirements of the model and improves its statistical efficiency. It also means that computing the output  330  CHAPTER 9. CONVOLUTIONAL NETWORKS  requires fewer operations. These improvements in efficiency are usually quite large. If there are m inputs and n outputs, then matrix multiplication requires m x n parameters, and the algorithms used in practice have O(m x n) runtime (per example). If we limit the number of connections each output may have to k, then the sparsely connected approach requires only k x n parameters and O(k x n) runtime. For many practical applications, it is possible to obtain good performance on the machine learning task while keeping k several orders of magnitude smaller than m", "f8217136-7eab-4c84-9a7b-0ff46b65280d": "Accuracy (%) \u2014_ Real Realassynt Syntasreal Syntassynt selected as real T1 (DCGAN, 128 x 128) 70 26 24 6 44 Tic (DCGAN, 128 x 128) 71 24 26 3 47 T2 (DCGAN, 128 x 128) 64 22 28 8 42 FLAIR (DCGAN, 128 x 128) 54 12 38 8 42 Concat (DCGAN, 128 x 128) 77 34 16 7 43 Concat (DCGAN, 64 x 64) 54 13 37 9 41 T1 (WGAN, 128 x 128) 64 20 30 6 44 Tic (WGAN, 128 x 128) 55 13 37 8 42 T2 (WGAN, 128 x 128) 58 19 31 11 39 FLAIR (WGAN, 128 x 128) 62 16 34 4 46 Concat (WGAN, 128 x 128) 66 31 19 15 35 Concat (WGAN, 64 x 64) 53 18 32 15 35 a b c 33 80F a7 54 50 50 48  \u00a3 g\u00b0 gz  3\u00b0 a 30  iw 2\u00b0 2  3 ~\u00bb = 9 3  Zz Z Z  Fig.\n\n23 Trends in applying GANs to medical image analysis   Shorten and Khoshgoftaar J Big Data  6:60   Train AAE and Sample latent vector Interpolate Decode new latent vector with AAE decoder  ncode from edge of  training samples latent distribution  Fig", "e271007b-41f9-40ff-9055-ebcdb512de56": "presented parsimony and self-consistency as the guiding principles for learning from data. Those uni\ufb01ed treatments shed new light on the sets of originally specialized methods and foster new progress in the respective \ufb01elds. LeCun  presented a modeling architecture to construct autonomous intelligent agents that combines concepts such as world model and hierarchical joint embedding. Our standardized formalism of the learning objective is complementary and o\ufb00ers a general framework for training the relevant model architectures. The framework also covers the key learning ingredients mentioned in LeCun , including the self-supervised learning (Section 4.1.2) and intrinsic motivation (Section 4.3.2). Integrating diverse sources of information in training has been explored in previous work, which is often dedicated to speci\ufb01c tasks. Roth  presented di\ufb00erent ways of deriving supervision signals in di\ufb00erent scenarios. Y. Zhu et al. discussed the integration of physical and other knowledge in solving vision problems.\n\nThe distant or weak supervision approaches  automatically create (noisy) instance labels from heuristics, which are then used in the supervised training procedure", "c97d54d9-4fd0-47cc-997c-dab0d1b96795": "Although its ability to quickly and accurately answer natural language questions stands out as Watson\u2019s major achievement, all of its sophisticated decision strategies contributed to its impressive defeat of human champions. According to Tesauro et al. : ... it is plainly evident that our strategy algorithms achieve a level of quantitative precision and real-time performance that exceeds human capabilities. This is particularly true in the cases of DD wagering and endgame buzzing, where humans simply cannot come close to matching the precise equity and con\ufb01dence estimates and complex decision calculations performed by Watson. Most computers use dynamic random access memory (DRAM) as their main memory because of its low cost and high capacity. The job of a DRAM memory controller is to e\ufb03ciently use the interface between the processor chip and an o\u21b5-chip DRAM system to provide the high-bandwidth and low-latency data transfer necessary for high-speed program execution.\n\nA memory controller needs to deal with dynamically changing patterns of read/write requests while adhering to a large number of timing and resource constraints required by the hardware. This is a formidable scheduling problem, especially with modern processors with multiple cores sharing the same DRAM", "4537b59d-a487-456b-b2fa-9a4a4951a462": "While the vocabulary itself is large and predicting a missing word involves some uncertainty, it\u2019s possible to produce alist of all the possible words in the vocabulary together with a probability estimate of the words\u2019 appearance at that location. Typical machine learning systems do so by treating the prediction problem as a classification problem and computing scores for each outcome using a giant so-called softmax layer, which transforms raw scores into a probability distribution over words. With this technique, the uncertainty of the prediction is represented by a probability distribution over all possible outcomes, provided that there isa finite number of possible outcomes.\n\nIn CV, on the other hand, the analogous  https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/  Self-supervised learning: The dark matter of intelligence   task of predicting \u201cmissing\u201d frames ina video, missing patches in an image, or missing segment in a speech signal involves a prediction of high- dimensional continuous objects rather than discrete outcomes. There are an infinite number of possible video frames that can plausibly follow a given video clip. It is not possible to explicitly represent all the possible video frames and associate a prediction score to them", "018f0cbe-77de-4e36-968c-48546bdb6c84": "and taking the exponential, we obtain an upper bound on the logistic sigmoid itself of the form which is plotted for two values of \u03bb on the left-hand plot in Figure 10.12. We can also obtain a lower bound on the sigmoid having the functional form of a Gaussian. To do this, we follow Jaakkola and Jordan  and make transformations both of the input variable and of the function itself. First we take the log of the logistic function and then decompose it so that We now note that the function f(x) = \u2212 ln(ex/2 + e\u2212x/2) is a convex function of the variable x2, as can again be veri\ufb01ed by \ufb01nding the second derivative", "93622785-7540-4983-8328-44ac69521739": "We can therefore \ufb01rst use the training data to determine the posterior distribution over the coef\ufb01cients w and then we can discard the training data and use the posterior distribution for w to make predictions of\ufffdt for new input observations \ufffdx. Section 3.3 A related graphical structure arises in an approach to classi\ufb01cation called the naive Bayes model, in which we use conditional independence assumptions to simplify the model structure. Suppose our observed variable consists of a D-dimensional vector x = (x1, . , xD)T, and we wish to assign observed values of x to one of K classes. Using the 1-of-K encoding scheme, we can represent these classes by a Kdimensional binary vector z.\n\nWe can then de\ufb01ne a generative model by introducing a multinomial prior p(z|\u00b5) over the class labels, where the kth component \u00b5k of \u00b5 is the prior probability of class Ck, together with a conditional distribution p(x|z) for the observed vector x. The key assumption of the naive Bayes model is that, conditioned on the class z, the distributions of the input variables x1, . , xD are independent", "a1a183b0-28e7-46f5-b0f4-2d98de55eaf9": "In this appendix we show that under an optimal discriminator, energy-based GANs (EBGANs)  optimize the total variation distance between the real and generated distributions. Energy-based GANs are trained in a similar fashion to GANs, only under a di\ufb00erent loss function. They have a discriminator D who tries to minimize LD(D, g\u03b8) = Ex\u223cPr + Ez\u223cp(z)+] Very importantly, D is constrained to be non-negative, since otherwise the trivial solution for D would be to set everything to arbitrarily low values.\n\nThe original EBGAN paper used only Ez\u223cp(z) for the loss of the generator, but this is obviously equivalent to our de\ufb01nition since the term Ex\u223cPr does not dependent on \u03b8 for a \ufb01xed discriminator (such as when backproping to the generator in EBGAN training) and thus minimizing one or the other is equivalent. We say that a measurable function D\u2217 : X \u2192 ). Theorem 4. Let Pr be a the real data distribution over a compact space X. Let g\u03b8 : Z \u2192 X be a measurable function (such as any neural network)", "8c7c25d6-5da5-41e4-afb1-13fc82d7022c": "The probability mass associated with this region is given by Now suppose that we have collected a data set comprising N observations drawn from p(x).\n\nBecause each data point has a probability P of falling within R, the total number K of points that lie inside R will be distributed according to the binomial distribution Section 2.1 Using (2.11), we see that the mean fraction of points falling inside the region is E = P, and similarly using (2.12) we see that the variance around this mean is var = P(1 \u2212 P)/N. For large N, this distribution will be sharply peaked around the mean and so K \u2243 NP. (2.244) If, however, we also assume that the region R is suf\ufb01ciently small that the probability density p(x) is roughly constant over the region, then we have Note that the validity of (2.246) depends on two contradictory assumptions, namely that the region R be suf\ufb01ciently small that the density is approximately constant over the region and yet suf\ufb01ciently large (in relation to the value of that density) that the number K of points falling inside the region is suf\ufb01cient for the binomial distribution to be sharply peaked. We can exploit the result (2.246) in two different ways", "d1c5a629-dbbe-48cc-9e22-d19c235a0af3": "To be applicable in a problem domain, Snorkel requires three main ingredients: \ufb01rst, a set of labeling functions that users can write; second, a discriminative model to train; and third, a preferably large amount of unlabeled data. While there are many current and exciting future directions for enabling users to more easily write labeling functions in a more diverse set of circumstances, as discussed in this section, there are some tasks or datasets where this will remain a gating limitation. Next, Snorkel\u2019s ease of use implicitly relies on easily available discriminative models, such as the increasingly commoditized architectures available in the open-source today, e.g., text, image, and other data types; however, this is not the case in every domain.\n\nFinally, Snorkel bene\ufb01ts from settings where unlabeled data is readily available, as demonstrated empirically in this paper and theoretically in prior work , which is not always the case. This section is an overview of techniques for managing weak supervision,manyofwhicharesubsumedinSnorkel.Wealso contrast it with related forms of supervision. Combining Weak Supervision Sources The main challenge of weak supervision is how to combine multiple sources", "c48e0be6-781d-44d9-828f-6763f93f3cb0": "The contributions can be summarized as follows. On the technical side, we propose a new RL formulation for text generation based on soft Q-Learning. This new formulation allows us to seamlessly take advantage of the RL literature\u2019s latest successful techniques (notably the path con2More recently, Deng et al. extend this line of work to optimize discrete text prompts with reinforcement learning. sistency algorithm) to overcome the longstanding challenges (e.g., sparse reward and large action space) in text generation. On the empirical side, we conduct studies on a wide variety of text generation tasks with limited data (i.e., generating from noisy/negative data, adversarial text generation, prompt generation). We propose their RL formulations, and show that our general approach consistently improves over not only previous text RL algorithms, but also diverse task-specialized methods. The challenges have led to dif\ufb01culties of the two major families of RL approaches applied to text generation problems, as detailed below. Policy-based RL techniques directly parameterize the policy \u03c0\u03b8 with parameters \u03b8", "b99a6e66-3cdc-452a-85a4-950bb3e68c93": "It has several di\u21b5erent de\ufb01nitions depending upon the nature of the task and whether one wishes to discount delayed reward.\n\nThe undiscounted formulation is appropriate for episodic tasks, in which the agent\u2013environment interaction breaks naturally into episodes; the discounted formulation is appropriate for continuing tasks, in which the interaction does not naturally break into episodes but continues without limit. We try to de\ufb01ne the returns for the two kinds of tasks such that one set of equations can apply to both the episodic and continuing cases. A policy\u2019s value functions assign to each state, or state\u2013action pair, the expected return from that state, or state\u2013action pair, given that the agent uses the policy. The optimal value functions assign to each state, or state\u2013action pair, the largest expected return achievable by any policy. A policy whose value functions are optimal is an optimal policy. Whereas the optimal value functions for states and state\u2013action pairs are unique for a given MDP, there can be many optimal policies. Any policy that is greedy with respect to the optimal value functions must be an optimal policy. The Bellman optimality equations are special consistency conditions that the optimal value functions must satisfy and that can, in principle, be solved for the optimal value functions, from which an optimal policy can be determined with relative ease", "ed75ba6e-4f00-426d-841d-6856b76c4d29": "For concreteness and implementation on a serial computer, however, we fully specify the order in which they occur within a time step. In Dyna-Q, the acting, model-learning, and direct RL processes require little computation, and we assume they consume just a fraction of the time. The remaining time in each step can be devoted to the planning process, which is inherently computation-intensive. Let us assume that there is time in each step, after acting, model-learning, and direct RL, to complete n iterations (Steps 1\u20133) of the Q-planning algorithm. In the pseudocode algorithm for Dyna-Q in the box below, Model(s, a) denotes the contents of the (predicted next state and reward) for state\u2013action pair (s, a). Direct reinforcement learning, model-learning, and planning are implemented by steps (d), (e), and (f), respectively. If (e) and (f) were omitted, the remaining algorithm would be one-step tabular Q-learning. Example 8.1: Dyna Maze Consider the simple maze shown inset in Figure 8.2", "3dc9263e-0e9e-4f28-a7d9-d87e56b649a3": "The tilings in Figure 9.12 (middle) are also denser and thinner on the left, promoting discrimination along the horizontal dimension at lower values along that dimension. The diagonal stripe tiling in Figure 9.12 (right) will promote generalization along one diagonal. In higher dimensions, axis-aligned stripes correspond to ignoring some of the dimensions in some of the tilings, that is, to hyperplanar slices. Irregular tilings such as shown in Figure 9.12 (left) are also possible, though rare in practice and beyond the standard software. In practice, it is often desirable to use di\u21b5erent shaped tiles in di\u21b5erent tilings. For example, one might use some vertical stripe tilings and some horizontal stripe tilings. This would encourage generalization along either dimension. However, with stripe tilings alone it is not possible to learn that a particular conjunction of horizontal and vertical coordinates has a distinctive value (whatever is learned for it will bleed into states with the same horizontal and vertical coordinates)", "734e12a9-9733-4a78-bcfc-8f4da7ea0d10": "Let us compute the TDE for these values. The \ufb01rst transition of each episode is 4.\n\nBecause the reward is zero on these transitions, and \u03b3 = 1, these changes are The second transition is similar; it is either up from B\u2019s 3 Now let\u2019s compute the TDE for the true values (B at 1, C at 0, and A at 1 has zero error because the starting value, either 1 or 0 depending on whether the transition is from B or C, always exactly matches the immediate reward and return. Thus the squared TD error is 1 A tabular representation is used in the A-split example, so the true state values can be exactly represented, yet the naive residual-gradient algorithm \ufb01nds di\u21b5erent values, and these values have lower TDE than do the true values. Minimizing the TDE is naive; by penalizing all TD errors it achieves something more like temporal smoothing than accurate prediction. A better idea would seem to be minimizing the Bellman error. If the exact values are learned, the Bellman error is zero everywhere. Thus, a Bellman-error-minimizing algorithm should have no trouble with the A-split example", "6422c69a-d380-4908-a33d-1b8914b67661": "They further reduced risk by prohibiting bets that would cause the wrong-answer afterstate value to decrease below a certain limit. These measures slightly reduced Watson\u2019s expectation of winning, but they signi\ufb01cantly reduced downside risk, not only in terms of average risk per DD bet, but even more so in extreme-risk scenarios where a risk-neutral Watson would bet most or all of its bankroll.\n\nWhy was the TD-Gammon method of self-play not used to learn the critical value function \u02c6v? Learning from self-play in Jeopardy! would not have worked very well because Watson was so di\u21b5erent from any human contestant. Self-play would have led to exploration of state space regions that are not typical for play against human opponents, particularly human champions. In addition, unlike backgammon, Jeopardy! is a game of imperfect information because contestants do not have access to all the information in\ufb02uencing their opponents\u2019 play. In particular, Jeopardy! contestants do not know how much con\ufb01dence their opponents have for responding to clues in the various categories. Self-play would have been something like playing poker with someone who is holding the same cards that you hold", "28b121db-648a-47b0-a687-235fc4c60af8": "For example, some generative models are better at assigning high probability to most realistic points, while other generative models are better at rarely assigning high probability to unrealistic points. These differences can result from whether a generative model is designed to minimize Dxy(pdatal|Pmodel) Of DKL(Pmodel||Paata), aS illustrated in figure 3.6. Unfortunately, even when we restrict the use of each metric to the task it is most suited for, all the metrics currently in use continue to have serious weaknesses. One of the most important research topics in generative modeling is therefore not  https://www.deeplearningbook.org/contents/generative_models.html    just how to improve generative models, but in fact, designing new techniques to measure our progress. CHAPTER 20. DEEP GENERATIVE MODELS  20.15 Conclusion  Training generative models with hidden units is a powerful way to make models understand the world represented in the given training data", "1f92509b-c1f7-4ecd-8dfa-02f1c4bd8c4d": "The primary mechanism for discarding information in a convolutional recognition network is the pooling layer. The generator network seems to need to add information. We cannot put the inverse of a pooling layer into the generator network because most pooling functions are not invertible. A simpler operation is to merely increase the spatial size of the representation. An approach that seems to perform acceptably is to use an \u201cun-pooling\u201d as introduced by Dosovitskiy et al. This layer corresponds to the inverse of the max-pooling operation under certain simplifying conditions. First, the stride of the max-pooling operation is constrained to be equal to the width of the pooling region. Second, the maximum input within each pooling region is assumed to be the input in the upper-left corner.\n\nFinally, all nonmaximal inputs within each pooling region are assumed to be zero. These are very strong and unrealistic assumptions, but they do allow the max-pooling operator to be inverted. The inverse un-pooling operation allocates  https://www.deeplearningbook.org/contents/generative_models.html    CHAPTER 20", "32f3abf0-a98b-4288-b235-6b993e32894a": "We can decompose a second-order equation into two coupled \ufb01rstorder equations by introducing intermediate momentum variables r, corresponding to the rate of change of the state variables z, having components where the zi can be regarded as position variables in this dynamics perspective. Thus for each position variable there is a corresponding momentum variable, and the joint space of position and momentum variables is called phase space. Without loss of generality, we can write the probability distribution p(z) in the form p(z) = 1 where E(z) is interpreted as the potential energy of the system when in state z. The system acceleration is the rate of change of momentum and is given by the applied force, which itself is the negative gradient of the potential energy It is convenient to reformulate this dynamical system using the Hamiltonian framework", "3caa1b3d-6431-4a43-8c74-7cab3d65189a": "Suppose you wanted to learn in about \u2327 experiences with substantially the same feature vector.\n\nA good rule of thumb for setting the step-size parameter of linear SGD methods is then where x is a random feature vector chosen from the same distribution as input vectors will be in the SGD. This method works best if the feature vectors do not vary greatly in length; ideally x>x is a constant. Exercise 9.5 Suppose you are using tile coding to transform a seven-dimensional continuous state space into binary feature vectors to estimate a state value function \u02c6v(s,w) \u21e1 v\u21e1(s). You believe that the dimensions do not interact strongly, so you decide to use eight tilings of each dimension separately (stripe tilings), for 7 \u21e5 8 = 56 tilings. In addition, in case there are some pairwise interactions between the dimensions, you also take all pairs of dimensions and tile each pair conjunctively with rectangular tiles. You make two tilings for each pair of dimensions, making a grand total of 21 \u21e5 2 + 56 = 98 tilings", "87b75a49-1f63-4d2d-9ed6-3f9ec6055680": "In this case, the regularization does not move the optimal value of w; to zero but instead just shifts it in that direction by a  distance equal to Fen  A similar process happens when w* < 0, but with the LZ! penalty making w; less negative by Fey or 9. In comparison to L? regularization, L! regularization results in a solution that is more sparse. Sparsity in this context refers to the fact that some parameters have an optimal value of zero. The sparsity of L!\n\nregularization is a qualitatively different behavior than arises with L? regularization. Equation 7.13 gave the solution w for L? regularization. If we revisit that equation using the assumption of a diagonal and positive definite Hessian H that we introduced for our analysis of nonzero. This demonstrates that L? regularization does not cause the parameters to become sparse, while L! regularization may do so for large enough a.  L* regularization, we find that a; = . If we was nonzero, then w; remains  The sparsity property induced by L! regularization has been used extensively as a feature selection mechanism", "e95e5b7f-4337-41b3-9f1d-448405190bfa": "5.28 (\u22c6) www Consider a neural network, such as the convolutional network discussed in Section 5.5.6, in which multiple weights are constrained to have the same value. Discuss how the standard backpropagation algorithm must be modi\ufb01ed in order to ensure that such constraints are satis\ufb01ed when evaluating the derivatives of an error function with respect to the adjustable parameters in the network.\n\n5.32 (\u22c6 \u22c6) Show that the derivatives of the mixing coef\ufb01cients {\u03c0k}, de\ufb01ned by (5.146), with respect to the auxiliary parameters {\u03b7j} are given by Hence, by making use of the constraint \ufffd 5.33 (\u22c6) Write down a pair of equations that express the Cartesian coordinates (x1, x2) for the robot arm shown in Figure 5.18 in terms of the joint angles \u03b81 and \u03b82 and the lengths L1 and L2 of the links. Assume the origin of the coordinate system is given by the attachment point of the lower arm. These equations de\ufb01ne the \u2018forward kinematics\u2019 of the robot arm. 5.34 (\u22c6) www Derive the result (5.155) for the derivative of the error function with respect to the network output activations controlling the mixing coef\ufb01cients in the mixture density network", "1ed168c0-ace2-4509-b33a-9c9693e59e48": "Afterwards, other machine learning techniques became more popular until the modern deep learning renaissance that began in 2006. The core ideas behind modern feedforward networks have not changed sub- stantially since the 1980s. The same back-propagation algorithm and the same approaches to gradient descent are still in use. Most of the improvement in neural network performance from 1986 to 2015 can be attributed to two factors. First, larger datasets have reduced the degree to which statistical generalization is a challenge for neural networks. Second, neural networks have become much larger, because of more powerful computers and better software infrastructure. A small  221  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  number of algorithmic changes have also improved the performance of neural networks noticeably. One of these algorithmic changes was the replacement of mean squared error with the cross-entropy family of loss functions.\n\nMean squared error was popular in the 1980s and 1990s but was gradually replaced by cross-entropy losses and the principle of maximum likelihood as ideas spread between the statistics community and the machine learning community", "5b7aec7e-6e33-418d-87bb-bbb207c73f90": "This hypothesis (though not in these exact words) was \ufb01rst explicitly stated by Montague, Dayan, and Sejnowski , who showed how the TD error concept from reinforcement learning accounts for many features of the phasic is not available until time t + 1. The TD error available at t is actually \u03b4t\u22121 = Rt + \u03b3V (St) \u2212 V (St\u22121). Because we are thinking of time steps as very small, or even in\ufb01nitesimal, time intervals, one should not attribute undue importance to this one-step time shift. activity of dopamine neurons in mammals. The experiments that led to this hypothesis were performed in the 1980s and early 1990s in the laboratory of neuroscientist Wolfram Schultz.\n\nSection 15.4 describes these in\ufb02uential experiments, Section 15.6 explains how the results of these experiments align with TD errors, and the Bibliographical and Historical Remarks section at the end of this chapter includes a guide to the literature surrounding the development of this in\ufb02uential hypothesis. Montague et al. compared the TD errors of the TD model of classical conditioning with the phasic activity of dopamine-producing neurons during classical conditioning experiments", "29ffe50d-92d8-481e-a923-cd3e195e3f12": "This would require a general language for predictions, so that the agent can systematically explore a large space of possible predictions, sifting through them for the ones that are most useful. In particular, both POMDP and PSR approaches can be applied with approximate states. The semantics of the state is useful in forming the state-update function, as it is in these two approaches and in the k-order approach. There is not a strong need for the semantics to be correct in order to retain useful information in the state. Some approaches to state augmentation, such as Echo state networks , keep almost arbitrary information about the history and can nevertheless perform well.\n\nThere are many possibilities and we expect more work and ideas in this area. Learning the state-update function for an approximate state is a major part of the representation learning problem as it arises in reinforcement learning. A major advantage of reinforcement learning over supervised learning is that reinforcement learning does not rely on detailed instructional information: generating a reward signal does not depend on knowledge of what the agent\u2019s correct actions should be. But the success of a reinforcement learning application strongly depends on how well the reward signal frames the goal of the application\u2019s designer and how well the signal assesses progress in reaching that goal", "0b0615f8-42aa-4e9c-b249-dd62a5d8ad26": "Consider the problem of \ufb01nding the maximum of a function f(x1, x2) subject to a constraint relating x1 and x2, which we write in the form One approach would be to solve the constraint equation (E.1) and thus express x2 as a function of x1 in the form x2 = h(x1). This can then be substituted into f(x1, x2) to give a function of x1 alone of the form f(x1, h(x1)). The maximum with respect to x1 could then be found by differentiation in the usual way, to give the stationary value x\u22c6 1, with the corresponding value of x2 given by x\u22c6 2 = h(x\u22c6 1). One problem with this approach is that it may be dif\ufb01cult to \ufb01nd an analytic solution of the constraint equation that allows x2 to be expressed as an explicit function of x1. Also, this approach treats x1 and x2 differently and so spoils the natural symmetry between these variables. A more elegant, and often simpler, approach is based on the introduction of a parameter \u03bb called a Lagrange multiplier. We shall motivate this technique from a geometrical perspective", "afb6c2e7-7ea0-4bfa-b0d0-0d7923dcc261": "Since the validation set is used to \u201ctrain\u201d the hyperparameters, the validation set error will underestimate the generalization error, though typically by a smaller amount than the training error does. After all hyperparameter optimization is complete, the generalization error may be estimated using the test set. In practice, when the same test set has been used repeatedly to evaluate performance of different algorithms over many years, and especially if we consider all the attempts from the scientific community at beating the reported state-of- the-art performance on that test set, we end up having optimistic evaluations with the test set as well. Benchmarks can thus become stale and then do not reflect the true field performance of a trained system. Thankfully, the community tends to move on to new (and usually more ambitious and larger) benchmark datasets. 119  CHAPTER 5. MACHINE LEARNING BASICS  https://www.deeplearningbook.org/contents/ml.html    5.3.1 Cross-Validation  Dividing the dataset into a fixed training set and a fixed test set can be problematic if it results in the test set being small.\n\nA small test set implies statistical uncertainty around the estimated average test error, making it difficult to claim that algorithm A works better than algorithm B on the given task", "9bb26302-4161-4cdf-9827-d3b22aab3bce": "\u2022 A stochastic data augmentation module that transforms any given data example randomly resulting in two correlated views of the same example, denoted \u02dcxi and \u02dcxj, which we consider as a positive pair. In this work, we sequentially apply three simple augmentations: random cropping followed by resize back to the original size, random color distortions, and random Gaussian blur. As shown in Section 3, the combination of random crop and color distortion is crucial to achieve a good performance. \u2022 A neural network base encoder f(\u00b7) that extracts representation vectors from augmented data examples. Our framework allows various choices of the network architecture without any constraints. We opt for simplicity and adopt the commonly used ResNet  to obtain hi = f(\u02dcxi) = ResNet(\u02dcxi) where hi \u2208 Rd is the output after the average pooling layer. \u2022 A small neural network projection head g(\u00b7) that maps representations to the space where contrastive loss is applied.\n\nWe use a MLP with one hidden layer to obtain zi = g(hi) = W (2)\u03c3(W (1)hi) where \u03c3 is a ReLU nonlinearity", "74c86393-f336-461d-92d6-1ef78855b45b": "We denote the transpose of a matrix A as A!, and it is defined such that  (A'),;=A (2.3)  ja  Vectors can be thought of as matrices that contain only one column. The transpose of a vector is therefore a matrix with only one row. Sometimes we  7 \u2122  https://www.deeplearningbook.org/contents/linear_algebra.html       Ain Ai ; , Ait Agi Asi A=fA; tt. Al= ; ; mints |oara'. A scalar can be thought of as a matrix with only a single entry. From this, we can see that a scalar is its own transpose: a = al,  We can add matrices to each other, as long as they have the same shape, just by adding their corresponding elements: C = A + B where Cy; = Ay; + By;.\n\nWe can also add a scalar to a matrix or multiply a matrix by a scalar, just by performing that operation on each element of a matrix: D = a-B +c where Di; =a: Bij +c", "ead1ec63-ca36-42ee-91f7-624c23a9a833": "Adaptive step-size for online temporal di\u21b5erence learning. In Proceedings of the Annual Conference of the Association for the Advancement of Arti\ufb01cial Intelligence (AAAI). Daniel, J. W. Splines and e\ufb03ciency in dynamic programming.\n\nJournal of Mathematical Dann, C., Neumann, G., Peters, J. Policy evaluation with temporal di\u21b5erences: A survey and comparison. Journal of Machine Learning Research, 15:809\u2013883. Daw, N. D., Courville, A. C., Touretzky, D. S. Timing and partial observability in the dopamine system. In Advances in Neural Information Processing Systems 15 , pp. 99\u2013106. MIT Press, Cambridge, MA. Daw, N. D., Courville, A. C., Touretzky, D. S. Representation and timing in theories of the dopamine system. Neural Computation, 18(7):1637\u20131677. Daw, N", "9efdcb14-ecc0-406b-9f49-3f3cb745872c": "A DBN with | hidden layers contains / weight matrices: Ww... ; Ww. It also contains | + 1 bias vectors bO),...,6, with bo) providing the biases for the visible layer. The probability distribution represented by the DBN is given by  P(nO ROY) oc exp (0 AO 4 o-DT AUD 4 pl-DT yo n) (20.17)  P(A) =1| RED) = o (iu 0 4 WPT A ne) Vi,vkE1,...,1\u20142, (20.18)  Plo = 1) RY) = 9 (4 + WHY) vi. (20.19) In the case of real-valued visible units, substitute vol (w: BO 4 WOT pO, -*) (20.20) 657  CHAPTER 20. DEEP GENERATIVE MODELS  with 8 diagonal for tractability.\n\nGeneralizations to other exponential family visible units are straightforward, at least in theory. A DBN with only one hidden layer is just an RBM. To generate a sample from a DBN, we first run several steps of Gibbs sampling on the top two hidden layers", "a6698f5b-4faf-41af-a12d-2a7fcbf16327": "The simplest approach is to ignore the symmetry constraint and show that the resulting solution is symmetric as required. Exercise 2.34 Alternative derivations of this result, which impose the symmetry and positive de\ufb01niteness constraints explicitly, can be found in Magnus and Neudecker . The result is as expected and takes the form which involves \u00b5ML because this is the result of a joint maximization with respect to \u00b5 and \u03a3. Note that the solution (2.121) for \u00b5ML does not depend on \u03a3ML, and so we can \ufb01rst evaluate \u00b5ML and then use this to evaluate \u03a3ML. If we evaluate the expectations of the maximum likelihood solutions under the true distribution, we obtain the following results Exercise 2.35 We see that the expectation of the maximum likelihood estimate for the mean is equal to the true mean. However, the maximum likelihood estimate for the covariance has an expectation that is less than the true value, and hence it is biased", "25ad91cc-d2ce-4545-ab01-c962f9d883d4": "For complicated graphs, there can be exponentially many of these wasted computations, making a naive implementation of the chain rule infeasible. In other cases, computing the same subexpression twice could be a valid way to reduce memory consumption at the cost of higher runtime. We begin with a version of the back-propagation algorithm that specifies the actual gradient computation directly (algorithm 6.2 along with algorithm 6.1 for the associated forward computation), in the order it will actually be done and according to the recursive application of chain rule. One could either directly perform these computations or view the description of the algorithm as a symbolic specification of the computational graph for computing the back-propagation. However, this formulation does not make explicit the manipulation and the construction of the symbolic graph that performs the gradient computation. Such a formulation is presented in section 6.5.6, with algorithm 6.5, where we also generalize to nodes that contain arbitrary tensors", "a7877b5d-7f27-406d-876c-8991aef7228b": ".\n\nActor-unit eligibility traces in that network were traces of just At\u21e5x(St) instead of the full (At\u2212\u21e1(1|St, \u2713))x(St). That work did not bene\ufb01t from the policy-gradient theory presented in Chapter 13 or the contributions of Williams , who showed how an ANN of Bernoulli-logistic units could implement a policy-gradient method. Reynolds and Wickens  proposed a three-factor rule for synaptic plasticity in the corticostriatal pathway in which dopamine modulates changes in corticostriatal synaptic e\ufb03cacy. They discussed the experimental support for this kind of learning rule and its possible molecular basis. The de\ufb01nitive demonstration of spike-timing-dependent plasticity (STDP) is attributed to Markram, L\u00a8ubke, Frotscher, and Sakmann , with evidence from earlier experiments by Levy and Steward  and others that the relative timing of pre- and postsynaptic spikes is critical for inducing changes in synaptic e\ufb03cacy", "81eb6417-cca7-4847-b1b7-332e1245f0e5": "Simulation and the Monte Carlo Method. Wiley, New York. Rumelhart, D. E., Hinton, G. E., Williams, R. J. .\n\nLearning internal representations by error propagation. In D. E. Rumelhart and J. L. McClelland (Eds. ), Parallel Distributed Processing: Explorations in the Microstructure of Cognition, vol. I, Foundations. Bradford/MIT Press, Cambridge, MA. Rummery, G. A. Problem Solving with Reinforcement Learning. Ph.D. thesis, University Rummery, G. A., Niranjan, M. On-line Q-learning using connectionist systems. Technical Report CUED/F-INFENG/TR 166. Engineering Department, Cambridge University. Ruppert, D. E\ufb03cient estimations from a slowly convergent Robbins-Monro process. Cornell University Operations Research and Industrial Engineering Technical Report No. 781", "a3c36f25-5c6c-4cf0-9ad5-767f23684bbb": "Efros, et al. Ensemble of exemplar-SVMs for object detection and beyond. In ICCV, volume 1, page 6. Citeseer, 2011. G. A. Miller. WordNet: a lexical database for English. Communications of the ACM, 38(11):39\u201341, 1995. M. Mirza and S. Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014. M. Norouzi, S. Bengio, N. Jaitly, M. Schuster, Y. Wu, D. Schuurmans, et al.\n\nReward augmented maximum likelihood for neural structured prediction. In Advances In Neural Information Processing Systems, pages 1723\u20131731, 2016. D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E", "12de2c69-b15a-44e7-979f-66a4bd5bd30e": "Let's label the state, action, and reward at time step tas S;, A;, and R;, respectively.\n\nThus the interaction sequence is fully described by one episode (also known as \u201ctrial\u201d or \u201ctrajectory\") and the sequence ends at the terminal state Sy:  Si, Aj, Ry, So, Ap, re) Sr Terms you will encounter a lot when diving into different categories of RL algorithms:  Model-based: Rely on the model of the environment; either the model is known or the algorithm learns it explicitly. Model-free: No dependency on the model during learning. On-policy: Use the deterministic outcomes or samples from the target policy to train the algorithm. Off-policy: Training on a distribution of transitions or episodes produced by a different behavior policy rather than that produced by the target policy. Model: Transition and Reward  The model is a descriptor of the environment. With the model, we can learn or infer how the  https://lilianweng.github.io/posts/2018-02-19-rl-overview/  A (Long) Peek into Reinforcement Learning | Lil'Log   environment would interact with and provide feedback to the agent", "c74c8a8d-81b9-4b37-9123-08e4cff5802d": "Now let us suppose that the mean is known and we wish to infer the variance. Again, our calculations will be greatly simpli\ufb01ed if we choose a conjugate form for the prior distribution. It turns out to be most convenient to work with the precision \u03bb \u2261 1/\u03c32. The likelihood function for \u03bb takes the form The corresponding conjugate prior should therefore be proportional to the product of a power of \u03bb and the exponential of a linear function of \u03bb. This corresponds to the gamma distribution which is de\ufb01ned by Here \u0393(a) is the gamma function that is de\ufb01ned by (1.141) and that ensures that (2.146) is correctly normalized. The gamma distribution has a \ufb01nite integral if a > 0, Exercise 2.41 and the distribution itself is \ufb01nite if a \u2a7e 1. It is plotted, for various values of a and b, in Figure 2.13", "9abbdb8b-f951-4aa9-9c37-62816cb92b12": "The graphical model that describes this problem is shown in Figure 8.7, and the corresponding joint distribution of all of the random variables in this model, conditioned on the deterministic parameters, is then given by The required predictive distribution for \ufffdt is then obtained, from the sum rule of probability, by integrating out the model parameters w so that where we are implicitly setting the random variables in t to the speci\ufb01c values observed in the data set. The details of this calculation were discussed in Chapter 3. There are many situations in which we wish to draw samples from a given probability distribution.\n\nAlthough we shall devote the whole of Chapter 11 to a detailed discussion of sampling methods, it is instructive to outline here one technique, called ancestral sampling, which is particularly relevant to graphical models. Consider a joint distribution p(x1, . , xK) over K variables that factorizes according to (8.5) corresponding to a directed acyclic graph. We shall suppose that the variables have been ordered such that there are no links from any node to any lower numbered node, in other words each node has a higher number than any of its parents. Our goal is to draw a sample \ufffdx1, . , \ufffdxK from the joint distribution", "e817e366-89d1-4718-9859-0bd941bdb414": "For any policy \u21e1 and any state s, the following consistency condition holds between the value of s and the value of its possible successor states: where it is implicit that the actions, a, are taken from the set A(s), that the next states, s0, are taken from the set S (or from S+ in the case of an episodic problem), and that the rewards, r, are taken from the set R. Note also how in the last equation we have merged the two sums, one over all the values of s0 and the other over all the values of r, into one sum over all the possible values of both. We use this kind of merged sum often to simplify formulas. Note how the \ufb01nal expression can be read easily as an expected value. It is really a sum over all values of the three variables, a, s0, and r. For each triple, we compute its probability, \u21e1(a|s)p(s0, r|s, a), weight the quantity in brackets by that probability, then sum over all possibilities to get an expected value. Equation (3.14) is the Bellman equation for v\u21e1", "a2bb9630-3382-4049-b3f2-2b96b309fb17": "Substituting (8.65) into (8.64) we obtain where ne(fs) denotes the set of variable nodes that are neighbours of the factor node fs, and ne(fs) \\ x denotes the same set but with node x removed.\n\nHere we have de\ufb01ned the following messages from variable nodes to factor nodes We have therefore introduced two distinct kinds of message, those that go from factor nodes to variable nodes denoted \u00b5f\u2192x(x), and those that go from variable nodes to factor nodes denoted \u00b5x\u2192f(x). In each case, we see that messages passed along a link are always a function of the variable associated with the variable node that link connects to. The result (8.66) says that to evaluate the message sent by a factor node to a variable node along the link connecting them, take the product of the incoming messages along all other links coming into the factor node, multiply by the factor associated with that node, and then marginalize over all of the variables associated with the incoming messages. This is illustrated in Figure 8.47", "7d3db17a-e6dc-425d-860f-f87964657dd5": "Our evaluation is designed to support the following three main claims: \u2022 Snorkel outperforms distant supervision baselines In distant supervision , one of the most popular forms of weak supervision used in practice, an external knowledge base is heuristically aligned with input data to serve as noisy training labels. By allowing users to easily incorporate a broader, more heterogeneous set of weak supervision sources\u2014for example, pattern matching, structure-based, and other more complex heuristics\u2014 Snorkel exceeds models trained via distant supervision by an average of 132%.\n\nSnorkel approaches hand supervision We see that by writing tens of labeling functions, we were able to approach or match results using hand-labeled training data which took weeks or months to assemble, coming within 2.11% of the F1 score of hand supervision on relation extraction tasks and an average 5.08% accuracy or AUC on cross-modal tasks, for an average 3.60% across all tasks. \u2022 Snorkel enables a new interaction paradigm We measure Snorkel\u2019s ef\ufb01ciency and ease of use by reporting on a user study of biomedical researchers from across the USA", "2c4544b7-3f57-45c0-b825-9d3a67223400": "STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  graph must be designed to connect those variables that are tightly coupled and omit edges between other variables. An entire field of machine learning called structure learning is devoted to this problem. For a good reference on structure learning, see . Most structure learning techniques are a form of greedy search. A structure is proposed and a model with that structure is trained, then given a score. The score rewards high training set accuracy and penalizes model complexity. Candidate structures with a small number of edges added or removed are then proposed as the next step of the search. The search proceeds to a new structure that is expected to increase the score. Using latent variables instead of adaptive structure avoids the need to perform discrete searches and multiple rounds of training. A fixed structure over visible and hidden variables can use direct interactions between visible and hidden units to impose indirect interactions between visible units. Using simple parameter learning techniques, we can learn a model with a fixed structure that imputes the right structure on the marginal p(v).\n\nLatent variables have advantages beyond their role in efficiently capturing p(v)", "15f3a876-aba2-4520-8928-3ef70a3b114e": "Reinforcement learning can be applied in either case. A model is not required, but models can easily be used if they are available or can be learned (Chapter 8). On the other hand, there are reinforcement learning methods that do not need any kind of environment model at all. Model-free systems cannot even think about how their environments will change in response to a single action. The tic-tac-toe player is model-free in this sense with respect to its opponent: it has no model of its opponent of any kind. Because models have to be reasonably accurate to be useful, model-free methods can have advantages over more complex methods when the real bottleneck in solving a problem is the di\ufb03culty of constructing a su\ufb03ciently accurate environment model. Model-free methods are also important building blocks for model-based methods. In this book we devote several chapters to model-free methods before we discuss how they can be used as components of more complex model-based methods", "2e9ac166-7122-4d75-bf79-b7b722e903ca": "Training often halts while the surrogate loss function still has large derivatives, which is very different from the pure optimization setting, where an optimization algorithm is considered to have converged when the gradient becomes very small. 8.1.3. Batch and Minibatch Algorithms  One aspect of machine learning algorithms that separates them from general optimization algorithms is that the objective function usually decomposes as a sum over the training examples.\n\nOptimization algorithms for machine learning typically compute each update to the parameters based on an expected value of the cost function estimated using only a subset of the terms of the full cost function. For example, maximum likelihood estimation problems, when viewed in log space, decompose into a sum over each example:  https://www.deeplearningbook.org/contents/optimization.html    OML = arg max \u00bb log pmodel (x) , y. 0). (8.4)  Maximizing this sum is equivalent to maximizing the expectation over the empirical distribution defined by the training set:  J(0) = Exyy~Daata log Pmodel(&, yw Q). (8.5) Most of the properties of the objective function J used by most of our opti-  mization algorithms are also expectations over the training set", "06e56466-d089-407c-aed7-0c7a9bb20cc4": "However, this expression can be made arbitrarily large simply by increasing the magnitude of w. To solve this problem, we could constrain w to have unit length, so that \ufffd a Lagrange multiplier to perform the constrained maximization, we then \ufb01nd that Appendix E w \u221d (m2 \u2212 m1). There is still a problem with this approach, however, as illustrated Exercise 4.4 in Figure 4.6. This shows two classes that are well separated in the original twodimensional space (x1, x2) but that have considerable overlap when projected onto the line joining their means. This dif\ufb01culty arises from the strongly nondiagonal covariances of the class distributions. The idea proposed by Fisher is to maximize a function that will give a large separation between the projected class means while also giving a small variance within each class, thereby minimizing the class overlap. The projection formula (4.20) transforms the set of labelled data points in x into a labelled set in the one-dimensional space y. The within-class variance of the transformed data from class Ck is therefore given by where yn = wTxn", "1a94031b-58e7-408c-b35d-5ab8ef32a109": "This code-as-supervision approach can then inherit the traditional advantages of code such as modularity, debuggability, and higher-level abstraction layers.\n\nIn particular, enabling this last element\u2014even higher-level, more declarative ways of specifying labeling functions\u2014has been a major motivation of the Snorkel project. Since Snorkel\u2019s release, various extensions have explored higher-level, more declarative interfaces for labeling training data by building on top of Snorkel (Fig. 15). One idea, motivated by the dif\ufb01culty of writing labeling functions directly overimageorvideodata,isto\ufb01rstcomputeasetoffeaturesor primitives over the raw data using unsupervised approaches, and then write labeling functions over these building blocks . For example, if the goal is to label instances of people riding bicycles, we could \ufb01rst run an off-the-shelf pre-trained algorithm to put bounding boxes around people and bicycles, and then write labeling functions over the dimensions or relative locations of these bounding boxes.20 In medical imaging tasks, anatomical segmentation masks provide a similarly intuitive semantic abstraction for writing labeling functions over", "4f0eadc3-8163-4220-a7b1-6c0b688c3bd4": "This regularizer is also known as weight decay and has been discussed at length in Chapter 3. The effective model complexity is then determined by the choice of the regularization coef\ufb01cient \u03bb.\n\nAs we have seen previously, this regularizer can be interpreted as the negative logarithm of a zero-mean Gaussian prior distribution over the weight vector w. One of the limitations of simple weight decay in the form (5.112) is that is inconsistent with certain scaling properties of network mappings. To illustrate this, consider a multilayer perceptron network having two layers of weights and linear output units, which performs a mapping from a set of input variables {xi} to a set of output variables {yk}", "e0f2b659-dc23-425a-adb7-0f23a31e97e2": ", wM}. takes any vector v and projects it onto the space spanned by the columns of \u03a6. Use this result to show that the least-squares solution (3.15) corresponds to an orthogonal projection of the vector t onto the manifold S as shown in Figure 3.2. Find an expression for the solution w\u22c6 that minimizes this error function. Give two alternative interpretations of the weighted sum-of-squares error function in terms of (i) data dependent noise variance and (ii) replicated data points. together with a sum-of-squares error function of the form Now suppose that Gaussian noise \u03f5i with zero mean and variance \u03c32 is added independently to each of the input variables xi. By making use of E = 0 and E = \u03b4ij\u03c32, show that minimizing ED averaged over the noise distribution is equivalent to minimizing the sum-of-squares error for noise-free input variables with the addition of a weight-decay regularization term, in which the bias parameter w0 is omitted from the regularizer", "aaa51305-0f68-401d-92de-d952ca8a6261": "Every-visit MC is less straightforward, but its estimates also converge quadratically to v\u21e1(s) . The use of Monte Carlo methods is best illustrated through an example. Example 5.1: Blackjack The object of the popular casino card game of blackjack is to obtain cards the sum of whose numerical values is as great as possible without exceeding 21. All face cards count as 10, and an ace can count either as 1 or as 11. We consider the version in which each player competes independently against the dealer. The game begins with two cards dealt to both dealer and player.\n\nOne of the dealer\u2019s cards is face up and the other is face down. If the player has 21 immediately (an ace and a 10-card), it is called a natural. He then wins unless the dealer also has a natural, in which case the game is a draw. If the player does not have a natural, then he can request additional cards, one by one (hits), until he either stops (sticks) or exceeds 21 (goes bust). If he goes bust, he loses; if he sticks, then it becomes the dealer\u2019s turn", "32de5098-1981-4667-9080-1402d1051095": "In Proceedings of the Association for the Advancement of Arti\ufb01cial Intelligence, pp. 259\u2013264. midbrain dopamine neurons. The Journal of Neuroscience, 33(11):4693\u20134709. Florian, R. V. Reinforcement learning through modulation of spike-timing-dependent synaptic plasticity. Neural Computation, 19(6):1468\u20131502. Fogel, L. J., Owens, A. J., Walsh, M. J. Arti\ufb01cial Intelligence through Simulated Evolution. French, R. M. Catastrophic forgetting in connectionist networks. Trends in cognitive Fr\u00b4emaux, N., Sprekeler, H., Gerstner, W. Functional requirements for reward-modulated spike-timing-dependent plasticity. The Journal of Neuroscience, 30(40): 13326\u201313337 Friedman, J. H., Bentley, J. L., Finkel, R. A", "dd645e71-0e4e-4df8-ba67-7ad3e041641c": "However, the dot product between two  vectors is commutative: aly=y'a. (2.8)  The transpose of a matrix product has a simple form: (AB)'=B'AT. (2.9)  This enables us to demonstrate equation 2.8 by exploiting the fact that the value of such a product is a scalar and therefore equal to its own transpose:  w'y= (ely) =y'e (2.10)  Since the focus of this textbook is not linear algebra, we do not attempt to develop a comprehensive list of useful properties of the matrix product here, but the reader should be aware that many more exist. We now know enough linear algebra notation to write down a system of linear equations:  https://www.deeplearningbook.org/contents/linear_algebra.html    Ax=b (2.11) where A \u20ac R\"*\u201d is a known matrix, b \u20ac R\u201d\u2122 is a known vector, and # \u20ac R\u201d isa vector of unknown variables we would like to solve for. Each element x; of x is one  of these unknown variables.\n\nEach row of A and each element of b provide another constraint. We can rewrite equation 2.11 as  Aj ,:", "607ae552-8116-43bd-88ff-8c9fc36d7ee2": "For \ufb01nite MDPs, we can precisely de\ufb01ne an optimal policy in the following way. Value functions de\ufb01ne a partial ordering over policies. A policy \u21e1 is de\ufb01ned to be better than or equal to a policy \u21e10 if its expected return is greater than or equal to that of \u21e10 for all states. In other words, \u21e1 \u2265 \u21e10 if and only if v\u21e1(s) \u2265 v\u21e10(s) for all s 2 S. There is always at least one policy that is better than or equal to all other policies. This is an optimal policy. Although there may be more than one, we denote all the optimal policies by \u21e1\u21e4. They share the same state-value function, called the optimal state-value function, denoted v\u21e4, and de\ufb01ned as Optimal policies also share the same optimal action-value function, denoted q\u21e4, and for all s 2 S and a 2 A(s). For the state\u2013action pair (s, a), this function gives the expected return for taking action a in state s and thereafter following an optimal policy", "812cc5f5-0594-42f2-9208-5c433c6da5f8": "(19.43) j#i  \u00bb  641  https://www.deeplearningbook.org/contents/inference.html    CHAPTER 19. APPROXIMATE INFERENCE  To apply the fixed-point update inference rule, we solve for the h, that sets equation 19.43 to 0:  . 1 . hj =o | bj +v' BW. - 5W.8W., - S- W.| BW. ih; . (19.44) JAI  At this point, we can see that there is a close connection between recurrent neural networks and inference in graphical models. Specifically, the mean field fixed-point equations defined a recurrent neural network. The task of this network is to perform inference.\n\nWe have described how to derive this network from a model description, but it is also possible to train the inference network directly. Several ideas based on this theme are described in chapter 20. In the case of binary sparse coding, we can see that the recurrent network connection specified by equation 19.44 consists of repeatedly updating the hidden units based on the changing values of the neighboring hidden units", "042acc0a-4501-4489-9b08-d8346609734d": "System identi\ufb01cation. In A. Proch\u00b4azka, J. Uhl\u00b4\u0131\u02c6r, P. W. J. Rayner, and N. G. Kingsbury (Eds. ), Signal Analysis and Prediction, pp. 163\u2013173. Springer Science + Business Media New York, LLC. Ljung, L., S\u00a8oderstrom, T. Theory and Practice of Recursive Identi\ufb01cation. MIT Press, Ljungberg, T., Apicella, P., Schultz, W. Responses of monkey dopamine neurons during learning of behavioral reactions. Journal of Neurophysiology, 67(1):145\u2013163. Lovejoy, W. S. A survey of algorithmic methods for partially observed Markov decision processes. Annals of Operations Research, 28(1):47\u201366. Luce, D. Individual Choice Behavior. Wiley, New York. Ludvig, E. A., Bellemare, M. G., Pearson, K", "ddc8061e-74cc-4d07-9f6c-abc0a7794bbb": "By performing a variational maximization of (2.279) and using Lagrange multipliers to enforce the constraints (2.280), (2.281), and (2.282), show that the maximum likelihood distribution is given by the Gaussian (2.43). 2.15 (\u22c6 \u22c6) Show that the entropy of the multivariate Gaussian N(x|\u00b5, \u03a3) is given by 2.16 (\u22c6 \u22c6 \u22c6) www Consider two random variables x1 and x2 having Gaussian distributions with means \u00b51, \u00b52 and precisions \u03c41, \u03c42 respectively. Derive an expression for the differential entropy of the variable x = x1 + x2. To do this, \ufb01rst \ufb01nd the distribution of x by using the relation and completing the square in the exponent. Then observe that this represents the convolution of two Gaussian distributions, which itself will be Gaussian, and \ufb01nally make use of the result (1.110) for the entropy of the univariate Gaussian", "0162c71d-7ecd-4c85-80ec-6a12da2c759e": "Whatever signal is added up in this way in a value-function-like prediction, we call it the cumulant of that prediction.\n\nWe formalize it in a cumulant signal Ct 2 R. Using this, a general value function, or GVF, is written As with conventional value functions (such as v\u21e1 or q\u21e4) this is an ideal function that we seek to approximate with a parameterized form, which we might continue to denote \u02c6v(s,w), although of course there would have to be a di\u21b5erent w for each prediction, that is, for each choice of \u21e1, \u03b3, and C. Because a GVF has no necessary connection to reward, it is perhaps a misnomer to call it a value function. One could simply call it a prediction or, to make it more distinctive, a forecast (Ring, in preparation). Whatever it is called, it is in the form of a value function and thus can be learned in the usual ways using the methods developed in this book for learning approximate value functions. Along with the learned predictions, we might also learn policies to maximize the predictions in the usual ways by Generalized Policy Iteration (Section 4.6) or by actor\u2013critic methods", "2582fdb8-2fb3-4eb3-bae8-7f4eeb92a931": "Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2018. Semantically equivalent adversarial rules for debugging NLP models. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 856\u2013865, Melbourne, Australia. Association for Computational Linguistics. Anthony Rios and Ramakanth Kavuluru. 2018. Fewshot and zero-shot multi-label learning for structured label spaces. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3132\u20133142, Brussels, Belgium. Association for Computational Linguistics. Timo Schick and Hinrich Sch\u00a8utze. 2021a. Exploiting cloze-questions for few-shot text classi\ufb01cation and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 255\u2013269, Online.\n\nAssociation for Computational Linguistics. Timo Schick and Hinrich Sch\u00a8utze", "96dad46c-3ace-4f0d-9024-d6c23f68deac": "The maximum likelihood framework makes it straightforward to learn the covariance of the Gaussian too, or to make the covariance of the Gaussian be a function of the input. However, the covariance must be constrained to be a positive definite matrix for all inputs. It is difficult to satisfy such constraints with a linear output layer, so typically other output units are used to parametrize the covariance. Approaches to modeling the covariance are described shortly, in section 6.2.2.4. Because linear units do not saturate, they pose little difficulty for gradient- based optimization algorithms and may be used with a wide variety of optimization algorithms. 6.2.2.2 Sigmoid Units for Bernoulli Output Distributions Many tasks require predicting the value of a binary variable y. Classification problems with two classes can be cast in this form. The maximum likelihood approach is to define a Bernoulli distribution over y conditioned on a.\n\nA Bernoulli distribution is defined by just a single number. The neural net needs to predict only P(y = 1). For this number to be a valid probability, it must lie in the interval", "c3632ce1-fedd-44eb-bf7c-b20798ef6129": "We also test the pretrained results on a wide range of datasets for transfer learning. To evaluate the learned representations, we follow the widely used linear evaluation protocol , where a linear classi\ufb01er is trained on top of the frozen base network, and test accuracy is used as a proxy for representation quality. Beyond linear evaluation, we also compare against state-of-the-art on semi-supervised and transfer learning. Default setting. Unless otherwise speci\ufb01ed, for data augmentation we use random crop and resize (with random \ufb02ip), color distortions, and Gaussian blur (for details, see Appendix A). We use ResNet-50 as the base encoder network, and a 2-layer MLP projection head to project the representation to a 128-dimensional latent space. As the loss, we use NT-Xent, optimized using LARS with learning rate of 4.8 (= 0.3 \u00d7 BatchSize/256) and weight decay of 10\u22126. We train at batch size 4096 for 100 epochs.3 Furthermore, we use linear warmup for the \ufb01rst 10 epochs, and decay the learning rate with the cosine decay schedule without restarts . Data augmentation de\ufb01nes predictive tasks", "bb826455-adba-4280-8b2b-7167127a6308": "True online Emphatic TD(\u03bb): Quick reference and implementation guide. ArXiv:1507.07147. Code is available in Python and C++ by downloading the source \ufb01les of this arXiv paper as a zip archive. Sutton, R. S., Barto, A. G. Toward a modern theory of adaptive networks: Expectation Sutton, R. S., Barto, A. G. An adaptive network that constructs and uses an internal model of its world. Cognition and Brain Theory, 3:217\u2013246. Sutton, R. S., Barto, A. G. A temporal-di\u21b5erence model of classical conditioning. In Proceedings of the Ninth Annual Conference of the Cognitive Science Society, pp. 355-378. Erlbaum, Hillsdale, NJ. Sutton, R. S., Barto, A. G. Time-derivative models of Pavlovian reinforcement. In M. Gabriel and J. Moore (Eds", "0297b9e6-493f-4cab-9f4d-25db3d3db5b2": "Its gradient is thus given by  T T Oh+D) do) Val = (Sx) (Vary) + (sen (Vo L) 10.20) 2 =W diag (: \u2014 (nD) ) (Vien Lb) +V' (VowL), 10.21)  where diag (1 \u2014 Att) *) indicates the diagonal matrix containing the elements 1- (ni) Y. This is the Jacobian of the hyperbolic tangent associated with the hidden unit 7 at time t + 1. Once the gradients on the internal nodes of the computational graph are obtained, we can obtain the gradients on the parameter nodes. Because the  379  https://www.deeplearningbook.org/contents/rnn.html    CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  parameters are shared across many time steps, we must take some care when denoting calculus operations involving these variables", "f7574952-f1f9-4990-a9cb-7c78c9cd656f": "The value of zn is not directly observed, however, but only a noise corrupted version xn = zn + \u03ben where the random variable \u03be is governed by some distribution g(\u03be). Consider a set of observations {xn, tn}, where n = 1, . , N, together with a corresponding sum-of-squares error function de\ufb01ned by averaging over the distribution of input noise to give By minimizing E with respect to the function y(z) using the calculus of variations (Appendix D), show that optimal solution for y(x) is given by a Nadaraya-Watson kernel regression solution of the form (6.45) with a kernel of the form (6.46). 6.21 (\u22c6 \u22c6) www Consider a Gaussian process regression model in which the kernel function is de\ufb01ned in terms of a \ufb01xed set of nonlinear basis functions. Show that the predictive distribution is identical to the result (3.58) obtained in Section 3.3.2 for the Bayesian linear regression model. To do this, note that both models have Gaussian predictive distributions, and so it is only necessary to show that the conditional mean and variance are the same", "833cdf10-5408-44d0-8ae7-db74347fe1cc": "Here we assume the hyperbolic tangent activation function. Also, the figure does not specify exactly what form the output and loss function take. Here we assume that the output is discrete, as if the RNN is used to predict words or characters. A natural way to represent discrete variables is to regard the output o as giving the unnormalized log probabilities of each possible value of the discrete variable.\n\nWe can then apply the softmax operation as a post-processing step to obtain a vector y of normalized probabilities over the output. Forward propagation begins with a specification of the initial state Ah. Then, for each time step from  https://www.deeplearningbook.org/contents/rnn.html    t=1 tot =7, we apply the following update equations:  a) = b+ wr) 400, (10.8) AO = tanh(a), (10.9) 0) = c+Vn%, (10.10) g = softmax(o\u201d), (10.11)  where the parameters are the bias vectors b and \u00a2 along with the weight matrices U, V and W, respectively, for input-to-hidden, hidden-to-output and hidden- to-hidden connections", "2f335015-d1c1-4b3f-8e8b-57176fa354a0": "(13.84) This is precisely analogous to the propagation of scaled variables \ufffd\u03b1(zn) given by (13.59) in the discrete case of the hidden Markov model, and so the recursion equation now takes the form Substituting for the conditionals p(zn|zn\u22121) and p(xn|zn), using (13.75) and (13.76), respectively, and making use of (13.84), we see that (13.85) becomes Here we are supposing that \u00b5n\u22121 and Vn\u22121 are known, and by evaluating the integral in (13.86), we wish to determine values for \u00b5n and Vn.\n\nThe integral is easily evaluated by making use of the result (2.115), from which it follows that \ufffd N(zn|Azn\u22121, \u0393)N(zn\u22121|\u00b5n\u22121, Vn\u22121) dzn\u22121 We can now combine this result with the \ufb01rst factor on the right-hand side of (13.86) by making use of (2.115) and (2.116) to give Here we have made use of the matrix inverse identities (C.5) and (C.7) and also de\ufb01ned the Kalman gain matrix Thus, given the values of \u00b5n\u22121 and Vn\u22121, together with the new observation xn, we can evaluate the Gaussian marginal for zn having mean \u00b5n and covariance Vn, as well as the normalization coef\ufb01cient cn", "c385cffb-be3f-489b-875c-6d27dcc1c258": "We now note that the exponent is an even function of the components of z and, because the integrals over these are taken over the range (\u2212\u221e, \u221e), the term in z in the factor (z + \u00b5) will vanish by symmetry. Thus E = \u00b5 (2.59) and so we refer to \u00b5 as the mean of the Gaussian distribution. We now consider second order moments of the Gaussian. In the univariate case, we considered the second order moment given by E. For the multivariate Gaussian, there are D2 second order moments given by E, which we can group together to form the matrix E. This matrix can be written as where again we have changed variables using z = x \u2212 \u00b5. Note that the cross-terms involving \u00b5zT and \u00b5Tz will again vanish by symmetry. The term \u00b5\u00b5T is constant and can be taken outside the integral, which itself is unity because the Gaussian distribution is normalized", "ac1d8ca7-408b-4f50-b2d6-2133610e2b55": "For data augmentation, we use the same Inception crop (\ufb02ip and resize to 32x32) as ImageNet,15 and color distortion (strength=0.5), leaving out Gaussian blur.\n\nWe pretrain with learning rate in {0.5, 1.0, 1.5}, temperature in {0.1, 0.5, 1.0}, and batch size in {256, 512, 1024, 2048, 4096}. The rest of the settings (including optimizer, weight decay, etc.) are the same as our ImageNet training. Our best model trained with batch size 1024 can achieve a linear evaluation accuracy of 94.0%, compared to 95.1% from the supervised baseline using the same architecture and batch size. The best self-supervised model that reports linear evaluation result on CIFAR-10 is AMDIM , which achieves 91.2% with a model 25\u00d7 larger than ours. We note that our model can be improved by incorporating extra data augmentations as well as using a more suitable base network. Performance under different batch sizes and training steps Figure B.7 shows the linear evaluation performance under different batch sizes and training steps", "2c50751b-9f19-4a3e-a983-f89a40ddd6b4": "Instead, \u2019  For example, we may wish to learn the variance of a conditional Gaussian for y, given x. In the simple case, where the variance o? form expression because the maximum likelihood estimator of variance is simply the empirical mean of the squared difference between observations y and their expected value. A computationally more expensive approach that does not require writing special-case code is to simply include the variance as one of the properties of the distribution p(y | x) that is controlled by w = f (a; 0). The negative log-likelihood \u2014log p(y; w(a)) will then provide a cost function with the appropriate terms  is a constant, there is a closed  necessary to make our optimization procedure incrementally learn the variance. In he simple case where the standard deviation does not depend on the input, we can make a new parameter in the network that is copied directly into w. This new parameter might be o itself or could be a parameter v representing o? or it could be a parameter { representing s, depending on how we choose to parametrize he distribution. We may wish our model to predict a different amount of variance in y for different values of x", "97b8f0fd-0ce5-439a-95a2-7a783109d111": "One interesting avenue is latent-variable predictive architectures.\n\nA latent-variable predictive architecture. Given an observation x, the model must be able to produce a set of multiple compatible predictions symbolized by an S-shaped ribbon in the diagram. As the latent variable z varies within a set, symbolized by a gray square, the output varies over the set of plausible predictions. Latent-variable predictive models contain an extra input variable (z). It is called latent because its value is never observed. With a properly trained model, as the latent variable varies over a given set, the output prediction varies over the set of plausible predictions compatible with the input x.  Latent-variable models can be trained with contrastive methods. A good  https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/  Self-supervised learning: The dark matter of intelligence   example of this is a generative adversarial network (GAN). The critic (or discriminator) can be seen as computing an energy indicating whether the input y looks good. The generator network is trained to produce contrastive samples to which the critic is trained to associate high energy", "522a036f-2daf-494f-8b06-e229621657c7": "Discriminative Models One of the key bets in Snorkel\u2019s design is that the trend of increasingly powerful, opensource machine learning tools (e.g., models, pre-trained word embeddings and initial layers, automatic tuners, etc.) will only continue to accelerate. To best take advantage of this, Snorkel creates probabilistic training labels for any discriminative model with a standard loss function. In the following experiments, we control for end model selection by using currently popular, standard choices across all settings. For text modalities, we choose a bidirectional long short term memory (LSTM) sequence model , and for the medical image classi\ufb01cation task we use a 50-layer ResNet  pre-trained on the ImageNet object classi\ufb01cation dataset . Both models are implemented in TensorFlow  and trained using the Adam optimizer , with hyperparameters selected via random grid search using a small labeled development set. Final scores are reported on a heldout labeled test set. See full version  for details. A key takeaway of the following results is that the discriminative model generalizes beyond the heuristics encoded in the labeling functions (as in Example 2.5).\n\nIn Sect", "88e9f728-3eaa-404c-a940-13417ab68acc": "Springer has provided excellent support throughout the \ufb01nal stages of preparation of this book, and I would like to thank my commissioning editor John Kimmel for his support and professionalism, as well as Joseph Piliero for his help in designing the cover and the text format and MaryAnn Brickner for her numerous contributions during the production phase. The inspiration for the cover design came from a discussion with Antonio Criminisi. I also wish to thank Oxford University Press for permission to reproduce excerpts from an earlier textbook, Neural Networks for Pattern Recognition . The images of the Mark 1 perceptron and of Frank Rosenblatt are reproduced with the permission of Arvin Calspan Advanced Technology Center", "d484deb8-1bd5-4308-867c-c5a7d7573aca": "Most of the datasets are based in English. BART  to compute the topic score in \u00a74.3.11 To compute perplexities, we use a GPT-2 model (124M parameters)  \ufb01ne-tuned on the corresponding datasets for computing perplexity in \u00a74.1 and 4.2, and a distilled GPT-2 model in \u00a74.3 without \ufb01ne-tuning.12 We study using the SNLI dataset , a dataset commonly used in training an entailment classi\ufb01er. The original dataset contains (premise, hypothesis) sentence pairs, where the hypothesis may or may not entail the premise. We sub-sampled 50, 000 training examples from the corpus such that the hypotheses have an average entailment probability of only 50% in terms of the premises, and over 2/5 examples have entailment probabilities less than 20%, which can be seen as negative (contradictive) examples.\n\nThe resulting training set poses a signi\ufb01cant challenge for the models to learn from the noises. The RL algorithms (including PG and ours) permit us to plug in arbitrary reward functions to drive learning", "50ecc28b-bb04-4a4a-be8f-53b1e1946db6": "If the convergence criterion is not satis\ufb01ed, then let The EM algorithm can also be used to \ufb01nd MAP (maximum posterior) solutions for models in which a prior p(\u03b8) is de\ufb01ned over the parameters. In this case the E Exercise 9.4 step remains the same as in the maximum likelihood case, whereas in the M step the quantity to be maximized is given by Q(\u03b8, \u03b8old) + ln p(\u03b8). Suitable choices for the prior will remove the singularities of the kind illustrated in Figure 9.7. Here we have considered the use of the EM algorithm to maximize a likelihood function when there are discrete latent variables. However, it can also be applied when the unobserved variables correspond to missing values in the data set.\n\nThe distribution of the observed values is obtained by taking the joint distribution of all the variables and then marginalizing over the missing ones. EM can then be used to maximize the corresponding likelihood function. We shall show an example of the application of this technique in the context of principal component analysis in Figure 12.11. This will be a valid procedure if the data values are missing at random, meaning that the mechanism causing values to be missing does not depend on the unobserved values", "90e2448a-f8f7-47f1-9073-a9e82420d122": "The bottom left plot shows the next misclassi\ufb01ed point to be considered, indicated by the green circle, and its feature vector is again added to the weight vector giving the decision boundary shown in the bottom right plot for which all data points are correctly classi\ufb01ed. Aside from dif\ufb01culties with the learning algorithm, the perceptron does not provide probabilistic outputs, nor does it generalize readily to K > 2 classes. The most important limitation, however, arises from the fact that (in common with all of the models discussed in this chapter and the previous one) it is based on linear combinations of \ufb01xed basis functions. More detailed discussions of the limitations of perceptrons can be found in Minsky and Papert  and Bishop . Analogue hardware implementations of the perceptron were built by Rosenblatt, based on motor-driven variable resistors to implement the adaptive parameters wj. These are illustrated in Figure 4.8. The inputs were obtained from a simple camera system based on an array of photo-sensors, while the basis functions \u03c6 could be chosen in a variety of ways, for example based on simple \ufb01xed functions of randomly chosen subsets of pixels from the input image", "33270ff2-e65e-4790-85bb-22eb473d2f5a": "Example: Gaussian Distribution Estimator of the Mean Now, consider a set of samples fa, eey aly that are independently and identically distributed  according to a Gaussian distribution p(z) = N(x; 4,07), where i \u20ac {1,...,m}.\n\nRecall that the Gaussian probability density function is given by  oy a \u2014\u2014 f +t+e\u2014\u2014~\\  https://www.deeplearningbook.org/contents/ml.html      We ney 4 aw ty p(a); pb, 0*) /2no? exp ~ # (5.31)  \u2014p=0 (5.35)  Thus we find that the sample mean is an unbiased estimator of Gaussian mean parameter. Example: Estimators of the Variance of a Gaussian Distribution For this example, we compare two different estimators of the variance parameter o? of a Gaussian distribution. We are interested in knowing if either estimator is biased. The first estimator of c? we consider is known as the sample variance m . ce) jim, (5.36) i=l  a2 m  where {i,, is the sample mean", "a7a3cfa8-bd3f-4004-8fc2-f28609a44615": "This gives rise to the zig-zag pattern of progress, where by descending to the minimum in the current gradient direction, we must reminimize  https://www.deeplearningbook.org/contents/optimization.html    the objective in the previous gradient direction. Thus, by following the gradient at the end of each line search we are, in a sense, undoing progress we have already  309 CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  Ws \\ iN  \u201430 \u201430 \u201420 \u201410 0 10 20  Figure 8.6: The method of steepest descent applied to a quadratic cost surface. The method of steepest descent involves jumping to the point of lowest cost along the line defined by the gradient at the initial point on each step. This resolves some of the problems seen with using a fixed learning rate in figure 4.6, but even with the optimal step size the algorithm still makes back-and-forth progress toward the optimum. By definition, at the minimum of the objective along a given direction, the gradient at the final point is orthogonal to that direction. made in the direction of the previous line search", "3f1eec52-6779-4614-8812-5ceb84ba2a53": "If the team members are neuron-like units, then each unit has to have the goal of increasing the amount of reward it receives over time, as the actor unit does that we described in Section 15.8. Each unit\u2019s learning algorithm has to have two essential features. First, it has to use contingent eligibility traces. Recall that a contingent eligibility trace, in neural terms, is initiated (or increased) at a synapse when its presynaptic input participates in causing the postsynaptic neuron to \ufb01re. A non-contingent eligibility trace, in contrast, is initiated or increased by presynaptic input independently of what the postsynaptic neuron does.\n\nAs explained in Section 15.8, by keeping information about what actions were taken in what states, contingent eligibility traces allow credit for reward, or blame for punishment, to be apportioned to an agent\u2019s policy parameters according to the contribution the values of these parameters made in determining the agent\u2019s action. By similar reasoning, a team member must remember its recent action so that it can either increase or decrease the likelihood of producing that action according to the reward signal that is subsequently received. The action component of a contingent eligibility trace implements this action memory", "fda29ec9-2fee-49bc-b986-3ec04c05bc80": "Then, if z is a vector valued random variable whose components are independent and Gaussian distributed with zero mean and unit variance, then y = \u00b5 + Lz will have mean \u00b5 and covariance \u03a3. Exercise 11.5 Obviously, the transformation technique depends for its success on the ability to calculate and then invert the inde\ufb01nite integral of the required distribution. Such operations will only be feasible for a limited number of simple distributions, and so we must turn to alternative approaches in search of a more general strategy. Here we consider two techniques called rejection sampling and importance sampling. Although mainly limited to univariate distributions and thus not directly applicable to complex problems in many dimensions, they do form important components in more general strategies.\n\nThe rejection sampling framework allows us to sample from relatively complex distributions, subject to certain constraints. We begin by considering univariate distributions and discuss the extension to multiple dimensions subsequently. Suppose we wish to sample from a distribution p(z) that is not one of the simple, standard distributions considered so far, and that sampling directly from p(z) is dif\ufb01cult", "9af1b702-20da-4b7e-95fa-acbb8310cd30": "Thus the probability that the tank is empty has decreased (from 0.257 to 0.111) as a result of the observation of the state of the battery. This accords with our intuition that \ufb01nding out that the battery is \ufb02at explains away the observation that the fuel gauge reads empty. We see that the state of the fuel tank and that of the battery have indeed become dependent on each other as a result of observing the reading on the fuel gauge. In fact, this would also be the case if, instead of observing the fuel gauge directly, we observed the state of some descendant of G. Note that the probability p(F = 0|G = 0, B = 0) \u2243 0.111 is greater than the prior probability p(F = 0) = 0.1 because the observation that the fuel gauge reads zero still provides some evidence in favour of an empty fuel tank. We now give a general statement of the d-separation property  for directed graphs", "d89d7d52-f057-4899-9be8-15c51db35488": "REGULARIZATION FOR DEEP LEARNING  A Vene- tate ba (TTtb ne 24 21 ANION AN teen Teen A te Ann net te 4nd 2. AR 2-H  https://www.deeplearningbook.org/contents/regularization.html  fA KEY MISIBIL (ALLO el Ub.\n\n4ULLE] ALIVULVEU Ll ULUPUUL IS LUaAL WE Call AVPLOXAI- mate persemble by evaluating ply Z) in one model: the model with all units, but with the weights going out of unit ? multiplied by the probability of including unit i. The motivation for this modification is to capture the right expected value of the  output from that unit. We call this approach the weight scaling inference rule. There is not yet any theoretical argument for the accuracy of this approximate inference rule in deep nonlinear networks, but empirically it performs very well. Because we usually use an inclusion probability of 3, the weight scaling rule usually amounts to dividing the weights by 2 at the end of training, and then using the model as usual", "c2252a9a-d95d-46ba-b828-493417d1a597": "10.13 (\u22c6 \u22c6) www Starting from (10.54), derive the result (10.59) for the optimum variational posterior distribution over \u00b5k and \u039bk in the Bayesian mixture of Gaussians, and hence verify the expressions for the parameters of this distribution given by (10.60)\u2013(10.63). 10.15 (\u22c6) Using the result (B.17), show that the expected value of the mixing coef\ufb01cients in the variational mixture of Gaussians is given by (10.69). 10.16 (\u22c6 \u22c6) www Verify the results (10.71) and (10.72) for the \ufb01rst two terms in the lower bound for the variational Gaussian mixture model given by (10.70). 10.17 (\u22c6 \u22c6 \u22c6) Verify the results (10.73)\u2013(10.77) for the remaining terms in the lower bound for the variational Gaussian mixture model given by (10.70).\n\n10.18 (\u22c6 \u22c6 \u22c6) In this exercise, we shall derive the variational re-estimation equations for the Gaussian mixture model by direct differentiation of the lower bound", "0d883006-15ad-48fa-9ca0-77b4423d272c": "data, comprise a sum of terms, one for each data point in the training set, so that Here we shall consider the problem of evaluating \u2207En(w) for one such term in the error function. This may be used directly for sequential optimization, or the results can be accumulated over the training set in the case of batch methods. Consider \ufb01rst a simple linear model in which the outputs yk are linear combinations of the input variables xi so that together with an error function that, for a particular input pattern n, takes the form which can be interpreted as a \u2018local\u2019 computation involving the product of an \u2018error signal\u2019 ynj \u2212 tnj associated with the output end of the link wji and the variable xni associated with the input end of the link. In Section 4.3.2, we saw how a similar formula arises with the logistic sigmoid activation function together with the cross entropy error function, and similarly for the softmax activation function together with its matching cross-entropy error function.\n\nWe shall now see how this simple result extends to the more complex setting of multilayer feed-forward networks", "5dad4ef2-a1a3-4a12-af90-9ff772adeeec": "Much as we can take partial derivatives of a function with respect to elements of its vector- valued argument, we can take functional derivatives, also known as variational derivatives, of a functional J with respect to individual values of the function f(a) at any specific value of 2. The functional derivative of the functional J with  respect to the value of the function f at point x is denoted \u2014\u00b0\u2014J. f(x) A complete formal development of functional derivatives is beyond the scope of this book.\n\nFor our purposes, it is sufficient to state that for differentiable functions  https://www.deeplearningbook.org/contents/inference.html    f(a) and differentiable functions g(y, #) with continuous derivatives, that  a _ 2 wv), @). Taw | 9 f@)2) de = 5 a(F@),=) (19.46)  To gain some intuition for this identity, one can think of f(a) as being a vector  643  CHAPTER 19. APPROXIMATE INFERENCE  with uncountably many elements, indexed by a real vector x", "15b3bae1-86d6-4e04-b0ce-602211e6e6e2": "10541\u201310551, 2019. Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. Decaf: A deep convolutional activation feature for generic visual recognition. In International Conference on Machine Learning, pp.\n\n647\u2013655, 2014. Dosovitskiy, A., Springenberg, J. T., Riedmiller, M., and Brox, T. Discriminative unsupervised feature learning with convolutional neural networks. In Advances in neural information processing systems, pp. 766\u2013774, 2014. Everingham, M., Van Gool, L., Williams, C. K., Winn, J., and Zisserman, A. The pascal visual object classes (voc) challenge. International Journal of Computer Vision, 88(2):303\u2013338, 2010. Fei-Fei, L., Fergus, R., and Perona, P", "d231f4b0-cc2c-41d9-9d14-0bfb68d5332d": "introduced the RNN-RBM sequence model and applied it to this task. The RNN-RBM is a generative model of a sequence of frames x) consisting of an RNN that emits the RBM parameters for each time step. Unlike previous approaches in which only the bias parameters of the RBM varied from one time step to the next, the RNN-RBM uses the RNN to emit all the parameters of the RBM, including the weights. To train the model, we need to be able to back-propagate the gradient of the loss function through the RNN.\n\nThe loss function is not applied directly to the RNN outputs. Instead, it is applied to the RBM. This means that we must approximately differentiate the loss with respect to the RBM parameters using contrastive divergence or a related algorithm. This approximate gradient may then  682  CHAPTER 20. DEEP GENERATIVE MODELS  be back-propagated through the RNN using the usual back-propagation through time algorithm. 20.8 Other Boltzmann Machines  Many other variants of Boltzmann machines are possible. Boltzmann machines may be extended with different training criteria", "3a72fc06-894e-4d08-a5d7-65e2233d1753": "This loss function enhancement enables style transfer to run much faster, increasing interest in practical applications. Additionally, Ulyanov et al. find that replacing batch normalization with instance normalization results in a significant improvement for fast stylization (Fig. 26). For the purpose of Data Augmentation, this is somewhat analogous to color space lighting transformations. Neural Style Transfer extends lighting variations and enables the encoding of different texture and artistic styles as well.\n\nThis leaves practitioners of Data Augmentation with the decision of which styles to sample from when deriving new images via Neural Style Transfer. Choosing which styles to sample from can be a challenging task. For applica-  tions such as self-driving cars it is fairly intuitive to think of transferring training Shorten and Khoshgoftaar J Big Data  6:60   Style Reconstructions  Input image  Convolutional Neural Network  Content Reconstructions  Fig", "f5030da0-ac4c-4b5e-bd24-f2335676558c": "ing polynomial function matches each of the data points exactly, but between data points (particularly near the ends of the range) the function exhibits the large oscillations observed in Figure 1.4. Intuitively, what is happening is that the more \ufb02exible polynomials with larger values of M are becoming increasingly tuned to the random noise on the target values.\n\nIt is also interesting to examine the behaviour of a given model as the size of the data set is varied, as shown in Figure 1.6. We see that, for a given model complexity, the over-\ufb01tting problem become less severe as the size of the data set increases. Another way to say this is that the larger the data set, the more complex (in other words more \ufb02exible) the model that we can afford to \ufb01t to the data. One rough heuristic that is sometimes advocated is that the number of data points should be no less than some multiple (say 5 or 10) of the number of adaptive parameters in the model. However, as we shall see in Chapter 3, the number of parameters is not necessarily the most appropriate measure of model complexity. Also, there is something rather unsatisfying about having to limit the number of parameters in a model according to the size of the available training set", "3fd1f2a8-c7c5-4341-8a29-633848972b5f": "Nucleic Acids Res. 45, D972\u2013D978  38. Pan, S.J., Yang, Q.: A survey on transfer learning. IEEE Trans. Knowl. Data Eng. 22(10), 1345\u20131359  39.\n\nParisi, F., Strino, F., Nadler, B., Kluger, Y.: Ranking and combining multiple predictors without labeled data. Proc. Natl. Acad. Sci. USA 111(4), 1253\u20131258  40. Pochampally, R., Das Sarma, A., Dong, X.L., Meliou, A., Srivastava, D.: Fusing data with correlations. In: ACM SIGMOD International Conference on Management of Data (SIGMOD)  41. Quinn, A.J., Bederson, B.B. : Human computation: A survey and taxonomy of a growing \ufb01eld. In: ACM SIGCHI Conference on Human Factors in Computing Systems (CHI)  42", "8505b425-5d39-40d4-af8c-aa0a485e962e": "It is a mapping from state s to action a and can be either deterministic or stochastic:  Deterministic: 7(s) = a.  Stochastic: r(a|s) = P,. Value Function  Value function measures the goodness of a state or how rewarding a state or an action is by a prediction of future reward. The future reward, also known as return, is a total sum of discounted rewards going forward. Let's compute the return G; starting from time t:  Gi = Rin + Rigg te = So Reve k=0  The discounting factor y \u20ac  penalize the rewards in the future, because:  The future rewards may have higher uncertainty; i.e. stock market. The future rewards do not provide immediate benefits; i.e. As human beings, we might prefer to have fun today rather than 5 years later ;). https://lilianweng.github.io/posts/2018-02-19-rl-overview/  A (Long) Peek into Reinforcement Learning | Lil'Log   Discounting provides mathematical convenience; i.e., we don't need to track future steps forever to compute return", "72fffb48-f1a1-44f8-bbab-4b1845bb5500": "The relevance vector machine or RVM  is a Bayesian sparse kernel technique for regression and classi\ufb01cation that shares many of the characteristics of the SVM whilst avoiding its principal limitations. Additionally, it typically leads to much sparser models resulting in correspondingly faster performance on test data whilst maintaining comparable generalization error. In contrast to the SVM we shall \ufb01nd it more convenient to introduce the regression form of the RVM \ufb01rst and then consider the extension to classi\ufb01cation tasks.\n\nThe relevance vector machine for regression is a linear model of the form studied in Chapter 3 but with a modi\ufb01ed prior that results in sparse solutions. The model de\ufb01nes a conditional distribution for a real-valued target variable t, given an input vector x, which takes the form where \u03b2 = \u03c3\u22122 is the noise precision (inverse noise variance), and the mean is given by a linear model of the form with \ufb01xed nonlinear basis functions \u03c6i(x), which will typically include a constant term so that the corresponding weight parameter represents a \u2018bias\u2019. The relevance vector machine is a speci\ufb01c instance of this model, which is intended to mirror the structure of the support vector machine. In particular, the basis functions are given by kernels, with one kernel associated with each of the data points from the training set", "6de5285b-9aba-450e-b4d5-e87132e65898": "How do you make sure that an agent gets enough experience to learn a high-performing policy, all the while not harming its environment, other agents, or itself (or more realistically, while keeping the probability of harm acceptably low)? This problem is also not novel or unique to reinforcement learning.\n\nRisk management and mitigation for embedded reinforcement learning is similar to what control engineers have had to confront from the beginning of using automatic control in situations where a controller\u2019s behavior can have unacceptable, possibly catastrophic, consequences, as in the control of an aircraft or a delicate chemical process. Control applications rely on careful system modeling, model validation, and extensive testing, and there is a highly-developed body of theory aimed at ensuring convergence and stability of adaptive controllers designed for use when the dynamics of the system to be controlled are not fully known. Theoretical guarantees are never iron-clad because they depend on the validity of the assumptions underlying the mathematics, but without this theory, combined with risk-management and mitigation practices, automatic control\u2014adaptive and otherwise\u2014would not be as bene\ufb01cial as it is today in improving the quality, e\ufb03ciency, and cost-e\u21b5ectiveness of processes on which we have come to rely", "a40f2cb3-d94e-4533-a752-09e56554af7a": "The task is to decide on each step whether to accept or reject the next customer, on the basis of his priority and the number of free servers, so as to maximize long-term reward without discounting. In this example we consider a tabular solution to this problem. Although there is no generalization between states, we can still consider it in the general function approximation setting as this setting generalizes the tabular setting. Thus we have a di\u21b5erential actionvalue estimate for each pair of state (number of free servers and priority of the customer at the head of the queue) and action (accept or reject). Figure 10.5 shows the solution found by di\u21b5erential semi-gradient Sarsa with parameters \u21b5 = 0.01, \u03b2 = 0.01, and \" = 0.1. The initial action values and \u00afR were zero.\n\nThe continuing, discounted problem formulation has been very useful in the tabular case, in which the returns from each state can be separately identi\ufb01ed and averaged. But in the approximate case it is questionable whether one should ever use this problem formulation. To see why, consider an in\ufb01nite sequence of returns with no beginning or end, and no clearly identi\ufb01ed states", "2b7dac81-521b-468f-af7a-b8b104d80579": "DeVries and Taylor  tested their feature space augmentation technique by extrapo- lating between the 3 nearest neighbors per sample to generate new data and compared their results against extrapolating in the input space and using affine transformations in the input space (Table 3).\n\nFeature space augmentations can be implemented with auto-encoders if it is necessary to reconstruct the new instances back into input space. It is also possible to do feature space augmentation solely by isolating vector representations from a CNN. This is done by cutting off the output layer of the network, such that the output is a low-dimensional vector rather than a class label. Vector representations are then found by training a CNN and then passing the training set through the truncated CNN. These vector representa- tions can be used to train any machine learning model from Naive Bayes, Support Vec- tor Machine, or back to a fully-connected multilayer network. The effectiveness of this technique is a subject for future work. A disadvantage of feature space augmentation is that it is very difficult to interpret the vector data", "46f605bb-6def-4b86-8a6d-415f7977a927": "Ghahramani. A unifying review of linear gaussian models. Neural computation, 1999. R. Samdani, M.-W. Chang, and D. Roth. Uni\ufb01ed expectation maximization. In ACL, 2012. R. Sennrich, B. Haddow, and A. Birch. Improving neural machine translation models with monolingual data. In ACL, 2016. A. Shrivastava, A. Gupta, and R. Girshick. Training region-based object detectors with online hard example mining. In CVPR, pages 761\u2013769, 2016. P. Y. Simard, Y. A. LeCun, J. S. Denker, and B. Victorri. Transformation invariance in pattern recognition\u2014tangent distance and tangent propagation. In Neural networks: tricks of the trade, pages 239\u2013274", "34d01983-272a-4675-826f-8adab5793982": "In Figures 2 and 3 we show samples drawn from the generator net after training.\n\nWhile we make no claim that these samples are better than samples generated by existing methods, we believe that these samples are at least competitive with the better generative models in the literature and highlight the potential of the adversarial framework. This new framework comes with advantages and disadvantages relative to previous modeling frameworks. The disadvantages are primarily that there is no explicit representation of pg(x), and that D must be synchronized well with G during training (in particular, G must not be trained too much without updating D, in order to avoid \u201cthe Helvetica scenario\u201d in which G collapses too many values of z to the same value of x to have enough diversity to model pdata), much as the negative chains of a Boltzmann machine must be kept up to date between learning steps. The advantages are that Markov chains are never needed, only backprop is used to obtain gradients, no inference is needed during learning, and a wide variety of functions can be incorporated into the model. Table 2 summarizes the comparison of generative adversarial nets with other generative modeling approaches. The aforementioned advantages are primarily computational", "5cbbada7-5906-4e77-9936-22ab64a3f56d": "In the case of classi\ufb01cation, there are various ways of using target values to represent class labels. For probabilistic models, the most convenient, in the case of two-class problems, is the binary representation in which there is a single target variable t \u2208 {0, 1} such that t = 1 represents class C1 and t = 0 represents class C2. We can interpret the value of t as the probability that the class is C1, with the values of probability taking only the extreme values of 0 and 1. For K > 2 classes, it is convenient to use a 1-of-K coding scheme in which t is a vector of length K such that if the class is Cj, then all elements tk of t are zero except element tj, which takes the value 1. For instance, if we have K = 5 classes, then a pattern from class 2 would be given the target vector Again, we can interpret the value of tk as the probability that the class is Ck. For nonprobabilistic models, alternative choices of target variable representation will sometimes prove convenient.\n\nIn Chapter 1, we identi\ufb01ed three distinct approaches to the classi\ufb01cation problem. The simplest involves constructing a discriminant function that directly assigns each vector x to a speci\ufb01c class", "d6724e45-1643-47f8-b102-0aa0486a40e8": "This requires evaluation of the Hessian matrix that comprises blocks of size M \u00d7 M in which block j, k is given by As with the two-class problem, the Hessian matrix for the multiclass logistic regression model is positive de\ufb01nite and so the error function again has a unique minimum. Exercise 4.20 Practical details of IRLS for the multiclass case can be found in Bishop and Nabney . We have seen that, for a broad range of class-conditional distributions, described by the exponential family, the resulting posterior class probabilities are given by a logistic (or softmax) transformation acting on a linear function of the feature variables.\n\nHowever, not all choices of class-conditional density give rise to such a simple form for the posterior probabilities (for instance, if the class-conditional densities are modelled using Gaussian mixtures). This suggests that it might be worth exploring other types of discriminative probabilistic model. For the purposes of this chapter, however, we shall return to the two-class case, and again remain within the framework of generalized linear models so that shown by the blue curve, given in this example by a mixture of two Gaussians, along with its cumulative distribution function f(a), shown by the red curve", "0283575d-6309-4fe4-b904-69a93553f1d6": "A distribution model would produce all possible sums and their probabilities of occurring, whereas a sample model would produce an individual sum drawn according to this probability distribution.\n\nThe kind of model assumed in dynamic programming\u2014estimates of the MDP\u2019s dynamics, p(s0, r|s, a)\u2014is a distribution model. The kind of model used in the blackjack example in Chapter 5 is a sample model. Distribution models are stronger than sample models in that they can always be used to produce samples. However, in many applications it is much easier to obtain sample models than distribution models. The dozen dice are a simple example of this. It would be easy to write a computer program to simulate the dice rolls and return the sum, but harder and more error-prone to \ufb01gure out all the possible sums and their probabilities. Models can be used to mimic or simulate experience. Given a starting state and action, a sample model produces a possible transition, and a distribution model generates all possible transitions weighted by their probabilities of occurring. Given a starting state and a policy, a sample model could produce an entire episode, and a distribution model could generate all possible episodes and their probabilities", "5039873f-6249-461a-ba66-1e02070bf1e5": "11.5 The BE was \ufb01rst proposed as an objective function for dynamic programming by Schweitzer and Seidmann .\n\nBaird  extended it to TD learning based on stochastic gradient descent. In the literature, BE minimization is often referred to as Bellman residual minimization. The earliest A-split example is due to Dayan . The two forms given here were introduced by Sutton et al. investigations to date of Gradient-TD and related methods are given by Geist and Scherrer , Dann, Neumann, and Peters , White , and Ghiassian, Patterson, White, Sutton, and White . Recent developments in the theory of Gradient-TD methods are presented by Yu . 11.8 Emphatic-TD methods were introduced by Sutton, Mahmood, and White . Full convergence proofs and other theory were later established by Yu , Hallak, Tamar, and Mannor , and Hallak, Tamar, Munos, and Mannor . Eligibility traces are one of the basic mechanisms of reinforcement learning. For example, in the popular TD(\u03bb) algorithm, the \u03bb refers to the use of an eligibility trace", "e0bd5b2f-dc0b-4087-94ab-9c4c1f522c71": "In one of the cases demonstrated in the figure, the algorithm discovered two independent factors of variation present in images of faces: angle of rotation and emotional expression. 695  CHAPTER 20. DEEP GENERATIVE MODELS  INNAVAVVVWVVVIVI IV HHH INL | IVANVNVNKN VOOM O0 DO DoH oO P pee IVVAH ADH DHOO8OD2O0 D0 HDDS HP pP Pee IW WA DAD DH... SPP PP PH INWDADH QHD", "d4e94404-3d9a-4844-9548-5c51ea99929c": "axonal arborizations in the neostriatum. The Journal of Neuroscience, 29(2):444\u2013453. Mazur, J. E. Learning and Behavior, 3rd ed. Prentice-Hall, Englewood Cli\u21b5s, NJ. McCallum, A. K. Overcoming incomplete perception with utile distinction memory. In Proceedings of the 10th International Conference on Machine Learning , pp. 190\u2013196. Morgan Kaufmann. McCallum, A. K. Reinforcement Learning with Selective Perception and Hidden State. McCloskey, M., Cohen, N. J. Catastrophic interference in connectionist networks: The sequential learning problem. Psychology of Learning and Motivation, 24:109\u2013165. McClure, S. M., Daw, N. D., Montague, P. R. A computational substrate for incentive McCulloch, W. S., Pitts, W", "07c4e050-953c-4205-aaba-20403cabec65": "\u201cstacked\u201d the four most recent frames so that the inputs to the network had dimension 84\u21e584\u21e54. This did not eliminate partial observability for all of the games, but it was helpful in making many of them more Markovian.\n\nAn essential point here is that these preprocessing steps were exactly the same for all 46 games. No game-speci\ufb01c prior knowledge was involved beyond the general understanding that it should still be possible to learn good policies with this reduced dimension and that stacking adjacent frames should help with the partial observability of some of the games. Because no game-speci\ufb01c prior knowledge beyond this minimal amount was used in preprocessing the image frames, we can think of the 84\u21e584\u21e54 input vectors as being \u201craw\u201d input to DQN. The basic architecture of DQN is similar to the deep convolutional ANN illustrated in The activation levels of DQN\u2019s output units were the estimated optimal action values of the corresponding state\u2013action pairs, for the state represented by the network\u2019s input. The assignment of output units to a game\u2019s actions varied from game to game, and because the number of valid actions varied between 4 and 18 for the games, not all output units had functional roles in all of the games", "b2426b1c-fed2-4284-995f-db57b727ef83": "The branching structure of a neuron\u2019s axon is called the neuron\u2019s axonal arbor.\n\nBecause the conduction of an action potential is an active process, not unlike the burning of a fuse, when an action potential reaches an axonal branch point it \u201clights up\u201d action potentials on all of the outgoing branches (although propagation to a branch can sometimes fail). As a result, the activity of a neuron with a large axonal arbor can in\ufb02uence many target sites. A synapse is a structure generally at the termination of an axon branch that mediates the communication of one neuron to another. A synapse transmits information from the presynaptic neuron\u2019s axon to a dendrite or cell body of the postsynaptic neuron. With a few exceptions, synapses release a chemical neurotransmitter upon the arrival of an action potential from the presynaptic neuron", "27392e36-dcc4-4c64-8160-1d0c280cd4e2": "(10.35)  t To remove the conditional i endence assumption, we can add connections from the output at time \u00a2 to the htdten unit at time t+ 1, as shown in figure 10.10. The model can then represent arbitrary probability distributions over the y sequence.\n\nThis kind of model representing a distribution over a sequence given another  https://www.deeplearningbook.org/contents/rnn.html    sequence still has one restriction, which is that the length of both sequences must be the same. We describe how to remove this restriction in section 10.4. 10.3. Bidirectional RNNs  All the recurrent networks we have considered up to now have a \u201ccausal\u201d struc- ture, meaning that the state at time \u00a2 captures only information from the past, a)... also allow information from past y values to affect the current state when the y values are available. gt), and the present input x\u201c. Some of the models we have discussed  In many applications, however, we want to output a prediction of y that may depend on the whole input sequence", "5e58e5bc-692e-41ce-b16e-517ab2caa4c3": "By contrast, the polynomial regression model described by Figure 8.5 is not generative because there is no probability distribution associated with the input variable x, and so it is not possible to generate synthetic data points from this model. We could make it generative by introducing a suitable prior distribution p(x), at the expense of a more complex model. The hidden variables in a probabilistic model need not, however, have any explicit physical interpretation but may be introduced simply to allow a more complex joint distribution to be constructed from simpler components. In either case, the technique of ancestral sampling applied to a generative model mimics the creation of the observed data and would therefore give rise to \u2018fantasy\u2019 data whose probability distribution (if the model were a perfect representation of reality) would be the same as that of the observed data. In practice, producing synthetic observations from a generative model can prove informative in understanding the form of the probability distribution represented by that model", "5d9dffcc-480d-4cf1-97b6-72e93d736216": "A view of the EM algorithm that justi\ufb01es incremental, sparse, and other variants. Learning in graphical models (pp. 355\u2013368). Springer. Norouzi, M., Bengio, S., Jaitly, N., Schuster, M., Wu, Y., Schuurmans, D., et al. Reward augmented maximum likelihood for neural structured prediction. Advances In Neural Information Processing Systems, 1723\u20131731. Nowozin, S., Cseke, B., & Tomioka, R. f-GAN: Training generative neural samplers using variational divergence minimization. Proceedings of the 30th International Conference on Neural Information Processing Systems, 271\u2013279. Paisley, J., Blei, D. M., & Jordan, M. I. Variational bayesian inference with stochastic search.\n\nProceedings of the 29th International Conference on Machine Learning. Pathak, D., Agrawal, P., Efros, A. A., & Darrell, T", "dc81fabd-68f3-4ad3-a964-d80720999232": "Multicategory support vector machines. Technical Report 1040, Department of Statistics, University of Madison, Wisconsin. Leen, T. K. From data distributions to regularization in invariant learning. Neural Computation 7, 974\u2013981. Lindley, D. V. Scoring rules and the inevitability of probability. International Statistical Review 50, 1\u201326. Liu, J. S. (Ed.) . Monte Carlo Strategies in Scienti\ufb01c Computing. Springer. L\u00a8utkepohl, H. Handbook of Matrices.\n\nWiley. MacKay, D. J. C. Bayesian interpolation. Neural Computation 4(3), 415\u2013447. MacKay, D. J. C. The evidence framework applied to classi\ufb01cation networks. Neural Computation 4(5), 720\u2013736. MacKay, D. J. C", "71091c86-0784-46c1-b388-060fa1c6bf05": "The adversarial modeling framework is most straightforward to apply when the models are both multilayer perceptrons.\n\nTo learn the generator\u2019s distribution pg over data x, we de\ufb01ne a prior on input noise variables pz(z), then represent a mapping to data space as G(z; \u03b8g), where G is a differentiable function represented by a multilayer perceptron with parameters \u03b8g. We also de\ufb01ne a second multilayer perceptron D(x; \u03b8d) that outputs a single scalar. D(x) represents the probability that x came from the data rather than pg. We train D to maximize the probability of assigning the correct label to both training examples and samples from G. We simultaneously train G to minimize log(1 \u2212 D(G(z))). In other words, D and G play the following two-player minimax game with value function V (G, D): min G max D V (D, G) = Ex\u223cpdata(x) + Ez\u223cpz(z)", "9ca51689-b4f9-46ae-b890-99005d4cd52e": "Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009. URL https://www.cs.toronto.edu/~kriz/ learning-features-2009-TR.pdf. Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classi\ufb01cation with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097\u20131105, 2012. Loshchilov, I. and Hutter, F. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. Maaten, L. v. d. and Hinton, G. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579\u20132605, 2008", "3e77a9da-97a1-4d72-a82c-f41a250fe7bd": "Because this corresponds to a linear-Gaussian model, we can use the general result (2.115) to give Exercise 6.26 Now that we have a Gaussian distribution for p(aN+1|tN), we can approximate the integral (6.76) using the result (4.153). As with the Bayesian logistic regression model of Section 4.5, if we are only interested in the decision boundary corresponding to p(tN+1|tN) = 0.5, then we need only consider the mean and we can ignore the effect of the variance. We also need to determine the parameters \u03b8 of the covariance function.\n\nOne approach is to maximize the likelihood function given by p(tN|\u03b8) for which we need expressions for the log likelihood and its gradient. If desired, suitable regularization terms can also be added, leading to a penalized maximum likelihood solution. The likelihood function is de\ufb01ned by This integral is analytically intractable, so again we make use of the Laplace approximation. Using the result (4.135), we obtain the following approximation for the log of the likelihood function where \u03a8(a\u22c6 N) = ln p(a\u22c6 N|\u03b8) + ln p(tN|a\u22c6 N)", "906bb070-0451-4fe4-a1fd-35b10dc7e8f0": "2.5 Norms  Sometimes we need to measure the size of a vector. In machine learning, we usually P  ws nee HL Aten AL een nb ee pete 3 Gee ttn 221 2 nawm ML. 1L- i TO We  https://www.deeplearningbook.org/contents/linear_algebra.html    IMCaASULE LILE SIZE VL VECLULS USIUY @ LULICLIOL CAMCU GA 4444444, PULIMIALLY, LUC 4 LOLI is given by 1  tll\u00bb = (= +\") \u2018 (2.30)  a forp\u20ac R,p>1. Norms, including the L? norm, are functions mapping vectors to non-negative values", "226af733-eb35-4f7e-b1d4-d477c521f9ce": "The problem is exacerbated in the presence of sparse reward in text generation, where the real observed signal rt is zero for all intermediate t < T; (2) The large action space (e.g., 104) in text generation results in slow updates. In particular, notice that Eq. (2) applies the gradient update to the Q\u03b8-value of the only one particular token at (out of, say, the 104 candidate tokens in the vocabulary), making the training inef\ufb01cient; (3) Besides, pure off-policy updates could be highly sensitive to the quality of training data, and miss the opportunity of on-policy exploration that maximizes the reward of interest in a more direct way. We introduce the soft Q-learning (SQL) formulation of text generation. It is seamlessly compatible with the common architecture of text generation model (Eq.1), permits easy implementation (\u00a73.1), and enables ef\ufb01cient and stable RL training in practice (\u00a73.2). Figure 2 and Algorithm 1 summarizes the resulting SQL framework.\n\nments the vanilla J(\u03c0) with the additional Shannon entropy term H with coef\ufb01cient \u03b1.3 This is appealing because it seamlessly connects the Q-values to the familiar output logits of a text generation model, which enables straightforward implementation of the SQL formulation", "1f20583f-e7c2-4b52-9b14-e5fe4a5fa3c7": "When training is complete, he auxiliary heads may be discarded. This is an alternative to the pretraining  https://www.deeplearningbook.org/contents/optimization.html    strategies, which were introduced in the previous section. In this way, one can train jointly all the layers in a single phase but change the architecture, so that intermediate layers (especially the lower ones) can get some hints about what they  322  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  should do, via a shorter path. These hints provide an error signal to lower layers.\n\n8.7.6 Continuation Methods and Curriculum Learning  As argued in section 8.2.7, many of the challenges in optimization arise from the global structure of the cost function and cannot be resolved merely by making better estimates of local update directions. The predominant strategy for overcoming this problem is to attempt to initialize the parameters in a region connected to the solution by a short path through parameter space that local descent can discover. Continuation methods are a family of strategies that can make optimization easier by choosing initial points to ensure that local optimization spends most of its time in well-behaved regions of space", "b71d80b1-4856-4b5e-b1df-7ab041d26cb7": "Using the standard results (B.27), (B.38), and (B.39), we can obtain the required moments as follows The evaluation of the variational posterior distribution begins by initializing the parameters of one of the distributions q(w) or q(\u03b1), and then alternately re-estimates these factors in turn until a suitable convergence criterion is satis\ufb01ed (usually speci\ufb01ed in terms of the lower bound to be discussed shortly). It is instructive to relate the variational solution to that found using the evidence framework in Section 3.5.\n\nTo do this consider the case a0 = b0 = 0, corresponding to the limit of an in\ufb01nitely broad prior over \u03b1. The mean of the variational posterior distribution q(\u03b1) is then given by Comparison with (9.63) shows that in the case of this particularly simple model, the variational approach gives precisely the same expression as that obtained by maximizing the evidence function using EM except that the point estimate for \u03b1 is replaced by its expected value. Because the distribution q(w) depends on q(\u03b1) only through the expectation E, we see that the two approaches will give identical results for the case of an in\ufb01nitely broad prior", "a9624261-6d16-45c2-8a71-6713c43e5262": "Provided that f is continuous and differentiable, we can then compute the gradients necessary for training using back-propagation as usual.\n\nAs an example, let us consider the operation consisting of drawing samples y from a Gaussian distribution with mean p and variance o? :  y ~N(u,07). (20.54)  Because an individual sample of y is produced not by a function, but rather by a sampling process whose output changes every time we query it, it may seem counterintuitive to take the derivatives of y with respect to the parameters of its distribution, 4 and o?. However, we can rewrite the sampling process as transforming an underlying random value z ~ N(z;0,1) to obtain a sample from the desired distribution:  y=ptoz. (20.55)  We are now able to back-propagate through the sampling operation, by regard- ing it as a deterministic operation with an extra input z. Crucially, the extra input is arandom variable whose distribution is not a function of any of the variables whose derivatives we want to calculate", "418c342f-1861-42f9-ac84-40f7cb0eac2f": "This work included a discussion of the inverse probability calculation (later termed Bayes\u2019 theorem by Poincar\u00b4e), which he used to solve problems in life expectancy, jurisprudence, planetary masses, triangulation, and error estimation. Thus (1.46) satis\ufb01es the two requirements for a valid probability density. We can readily \ufb01nd expectations of functions of x under the Gaussian distribuBecause the parameter \u00b5 represents the average value of x under the distribution, it is referred to as the mean. Similarly, for the second order moment From (1.49) and (1.50), it follows that the variance of x is given by and hence \u03c32 is referred to as the variance parameter. The maximum of a distribution is known as its mode.\n\nFor a Gaussian, the mode coincides with the mean. Exercise 1.9 We are also interested in the Gaussian distribution de\ufb01ned over a D-dimensional vector x of continuous variables, which is given by where the D-dimensional vector \u00b5 is called the mean, the D \u00d7 D matrix \u03a3 is called the covariance, and |\u03a3| denotes the determinant of \u03a3", "ee889f5a-c9ac-4cc9-b71e-4630375ae9c1": "The evidence lower bound is given by  L(v, 6,q) 19.29) = Enngllog p(h, v)] + A(q) 19.30) = Epaq|log p(h) + log p(v | h) \u2014 log g(h | v)] 19.31) = Enng [Stott + >7 log p(vi| bh) \u2014 YF log a(h | | 19.32)  i=1 i=l i=l => [hi(log o(ti) \u2014 log hj) + (1 \u2014 hy) (log", "386edbff-07be-47ad-8b1b-b341dab3bae9": "This focus became increasingly narrow as learning continued. Because the convergence theorem for RTDP applies to the simulations, we know that RTDP eventually would have focused only on relevant states, i.e., on states making up optimal paths. RTDP achieved nearly optimal control with about 50% of the computation required by sweep-based value iteration. Planning can be used in at least two ways.\n\nThe one we have considered so far in this chapter, typi\ufb01ed by dynamic programming and Dyna, is to use planning to gradually improve a policy or value function on the basis of simulated experience obtained from a model (either a sample or a distribution model). Selecting actions is then a matter of comparing the current state\u2019s action values obtained from a table in the tabular case we have thus far considered, or by evaluating a mathematical expression in the approximate methods we consider in Part II below. Well before an action is selected for any current state St, planning has played a part in improving the table entries, or the mathematical expression, needed to select the action for many states, including St. Used this way, planning is not focused on the current state. We call planning used in this way background planning", "99cf7847-c7fe-4baa-80b2-4e64408efb26": "Prompting is a new emerging way of steering the pretrained models for performing downstream tasks without changing the model parameters . A prompt is a short sequence of text tokens or embedding vectors which, by concatenating with the input and being fed together into the model, stimulates the model to perform the task of interest and produce the desired output for the input (Figure 6, left). It is thus desirable to learn the prompt that enables optimal performance of the pretrained models on the task of interest. For continuous prompt, which is a sequence of di\ufb00erentiable embedding vectors , we could naturally treat the prompt as the learnable \u03b8, and plug the resulting target model p\u03b8, now the pretrained model plus the learnable prompt, into the SE for training. On the other hand, for discrete prompt, which is a sequence of text tokens, one can instead parameterize a prompt-generation network as \u03b8 which, after training, generates optimal discrete prompts for the downstream task. For example, Deng et al.\n\nused both supervised data instances and task reward to learn the target prompt-generation models. Symbolic knowledge graphs", "43f7da0d-dec2-479a-98c9-517dc64ed0cc": "The first step is to express conventional PCA in such a form that the data vectors {xn } appear only in the form of the scalar products x~X m . Recall that the principal components are defined by the eigenvectors Ui of the covariance matrix where i = 1, ... ,D. Here the D x D sample covariance matrix S is defined by and the eigenvectors are normalized such that uT Ui = 1. Now consider a nonlinear transformation \u00a2(x) into an M -dimensional feature space, so that each data point X n is thereby projected onto a point \u00a2(xn ). We can jI) !hal L. 4>(\"'.J .. O. We dWl rctlll'1l 10 Itl,~ pol'\" .Ihonly", "0b685744-2275-4816-916d-0ffe2acca500": "It is essentially NCE loss when considering the sentence (s, 8.) as the positive pairs while other pairs (s, s\u2019) where s\u2019 \u20ac S(s), s\u2019 # s, as negatives. Car =D barks So) =- FY 5 SLE a  s\u20acD 5-\u20acC(s) s\u20acD 5-\u20acC(s  Mutual Information Maximization  IS-BERT (Info-Sentence BERT) (Zhang et al.\n\n2020; code) adopts a self-supervised learning objective based on mutual information maximization to learn good sentence embeddings in the unsupervised manners", "60df199f-45e9-462e-b788-7b665de01641": "On generalized bellman equations and temporaldi\u21b5erence learning. ArXiv:17041.04463. A summary appeared in Proceedings of the Canadian Conference on Arti\ufb01cial Intelligence, pp. 3\u201314. Springer. Page numbers in italics are recommended to be consulted \ufb01rst. Page numbers in bold contain boxed algorithms. Bioinformatics: The Machine Learning Approach, Pierre Baldi and S\u00f8ren Brunak Reinforcement Learning: An Introduction, Richard S. Sutton and Andrew G. Barto Graphical Models for Machine Learning and Digital Communication, Brendan J. Frey Principles of Data Mining, David Hand, Heikki Mannila, and Padhraic Smyth Learning Kernel Classi\ufb01ers: Theory and Algorithms, Ralf Herbrich Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond, Bernhard Sch\u00a8olkopf and Alexander J. Smola The Minimum Description Length Principle, Peter D", "5bc278b5-46b2-4111-82df-01cd579f7e82": "Sometimes, nonparametric models are just theoretical abstractions (such as an algorithm that searches over all possible probability distributions) that cannot be implemented in practice. However, we can also design practical nonparametric models by making their complexity a function of the training set size. One example of such an algorithm is nearest neighbor regression. Unlike linear regression, which has a fixed-length vector of weights, the nearest neighbor regression model simply stores the X and y from the training set. When asked to classify a test point x, the model looks up the nearest entry in the training set and returns the associated regression target. In other words, 7 = y; where\u00e9 = arg min ||X;,, \u2014 #|[.. The algorithm can also be generalized to distance metrics other than the L? norm, such as learned distance metrics . If the algorithm is allowed to break ties by averaging the y; values for all X;. that are tied for nearest, then this algorithm is able to achieve the minimum possible training error (which  113  CHAPTER 5", "acd9bc10-dace-47dc-a704-1ccd5437d56b": "If f is Markov and h and h0 are any two histories that map to the same state under f, then for any test \u2327 of any length, its probabilities given the two histories must also be the same: In other words, a Markov state summarizes all the information in the history necessary for determining any test\u2019s probability. In fact, it summarizes all that is necessary for making any prediction, including any GVF, and for behaving optimally (if f is Markov, then there is always a deterministic function \u21e1 such that choosing At .= \u21e1(f(Ht)) is optimal). The third step in extending reinforcement learning to partial observability is to deal with certain computational considerations. In particular, we want the state to be a compact summary of the history. For example, the identity function completely satis\ufb01es the conditions for a Markov f, but would nevertheless be of little use because the corresponding state St =Ht would grow with time and become unwieldy, as mentioned earlier, but more fundamentally because it would never recur; the agent would never encounter the same state twice (in a continuing task) and thus could never bene\ufb01t from a tabular learning method", "719ddf45-f4d3-46e5-a8ea-aa658db48d11": "(16.12) 584  CHAPTER 16.\n\nSTRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  ceoeccececeD Seer erccrrce CYUUvUUUUULU wevuvvuvvovvuy 9 00 09 09 00 09 09 00 00 00 SoCeecrececs AO 00 09 00 09 09 0 09 09 00 CCeccrcrrccs SHH OH eOHQo60D  HOD OVIIIIS \u00a9 08 00 09 00 09 00 0 00 09 cececcececi 4 20 00 08 00 \u00a9 00 00 00 0) 00 00 60 09 00 09 09 0 00 09 09  g 3 z 8 e z 8 z g g  Figure 16.15: Samples from a trained RBM and its weights. (Left) Samples from a model trained on MNIST, drawn using Gibbs sampling. Each column is a separate Gibbs sampling process. Each row represents the output of another 1,000 steps of Gibbs sampling", "2c628a86-76a4-4764-ae24-faf76e2ce2c2": "In the damping approach, we solve for the individually optimal values of every element of h, then move all the values in a small step in that direction. This approach is no longer guaranteed to increase \u00a3 at each step, but it works well in practice for many models.\n\nSee Koller and Friedman  for more information about choosing the degree of synchrony and damping strategies in message-passing algorithms. 19.4.2 Calculus of Variations  Before continuing with our presentation of variational learning, we must briefly introduce an important set of mathematical tools used in variational learning: calculus of variations. Many machine learning techniques are based on minimizing a function J(@) by finding the input vector @ \u20ac R\u201d for which it takes on its minimal value. This can be accomplished with multivariate calculus and linear algebra, by solving for the critical points where VgJ(@) = 0. In some cases, we actually want to solve for a function f(a), such as when we want to find the probability density function over some random variable. This is what calculus of variations enables us to do. A function of a function f is known as a functional J", "817c48ec-1229-442e-82db-158099d15f27": "Thus, the BE cannot be estim data alone; knowledge of the MDP beyond what is revealed in the data is requi Moreover, the two MDPs have di\u21b5erent minimal-BE value functions.2 For the the minimal-BE value function is the exact solution v\u03b8 = \u20d70 for any \u03b3. For the sec 2. This is a critical observation, as it is possible for an error function to be unobservable an perfectly satisfactory for use in learning settings because the value that minimizes it can b from data. For example, this is what happens with the VE. The VE is not observable from policy together completely determine the probability distribution ov Assume for the moment that the state, action, and reward sets a for any \ufb01nite sequence \u03be = \u03c60, a0, r1, . , rk, \u03c6k, there is a well de\ufb01n sibly zero) of it occuring as the initial portion of a trajectory, wh P(\u03be) = Pr{\u03c6(S0) = \u03c60, A0 = a0, R1 = r1, . , Rk = rk, \u03c6(Sk) = \u03c6k}", "4321e022-3557-4111-b11d-73fb4f5e796c": "Waltz, M. D., Fu, K. S. A heuristic approach to reinforcement learning control systems. IEEE Transactions on Automatic Control, 10(4):390\u2013398. Watkins, C. J. C. H., Dayan, P. Q-learning. Machine Learning, 8(3-4):279\u2013292. Werbos, P. J. Advanced forecasting methods for global crisis warning and models of intelligence. General Systems Yearbook, 22(12):25\u201338. Werbos, P. J. Applications of advances in nonlinear sensitivity analysis. In R. F. Drenick and F. Kozin (Eds. ), System Modeling and Optimization, pp. 762\u2013770. Springer-Verlag. Werbos, P. J. Building and understanding adaptive systems: A statistical/numerical approach to factory automation and brain research", "92370dba-9be0-4d53-a080-828cf0dbc0b4": "57  CHAPTER 3. PROBABILITY AND INFORMATION THEORY  For example, applying the definition twice, we get P(a,b,c) = P(a|b,c)P(b,c) P(b,c) P(b | c)P(c) P(a,b,c) P(a| b,c)P(b | c)P(c). 3.7 Independence and Conditional Independence  Two random variables x and y are independent if their probability distribution can be expressed as a product of two factors, one involving only x and one involving only y:  Vr ex.uev. ox =a27.v=u)=nlx=2)\\nlv = 1). (3.7)  https://www.deeplearningbook.org/contents/prob.html    Two random variables x and j are conditionally independent given a random variable z if the conditional probability distribution over x and y factorizes in this way for every value of z:  Vrex,yey,z\u20ac2, px=2,y=y|2=2)=px=2|2=2)ply=y|2=2)", "fd79c67e-3b6c-4ca7-b99a-26ca02ddebd7": "As with the Metropolis algorithm, we can gain some insight into the behaviour of Gibbs sampling by investigating its application to a Gaussian distribution.\n\nConsider a correlated Gaussian in two variables, as illustrated in Figure 11.11, having conditional distributions of width l and marginal distributions of width L. The typical step size is governed by the conditional distributions and will be of order l. Because the state evolves according to a random walk, the number of steps needed to obtain independent samples from the distribution will be of order (L/l)2. Of course if the Gaussian distribution were uncorrelated, then the Gibbs sampling procedure would be optimally ef\ufb01cient. For this simple problem, we could rotate the coordinate system in order to decorrelate the variables. However, in practical applications it will generally be infeasible to \ufb01nd such transformations. One approach to reducing random walk behaviour in Gibbs sampling is called over-relaxation", "7b1799ee-5ee5-4856-9d3d-70503e5287a2": "An important advantage of the hierarchical softmax is that it brings computa- tional benefits both at training time and at test time, if at test time we want to compute the probability of specific words. Of course, computing the probability of all |V| words will remain expensive even with the hierarchical softmax. Another important operation is selecting the most likely word in a given context. Unfortunately the tree structure does not provide an efficient and exact solution to this problem.\n\nA disadvantage is that in practice the hierarchical softmax tends to give worse test results than sampling-based methods, which we describe next. This may be due to a poor choice of word classes. 12.4.3.3 Importance Sampling  One way to speed up the training of neural language models is to avoid explicitly computing the contribution of the gradient from all the words that do not appear  https://www.deeplearningbook.org/contents/applications.html    in the next position. Every incorrect word should have low probability under the model. It can be computationally costly to enumerate all these words. Instead, it is possible to sample only a subset of the words. Using the notation introduced in  464  CHAPTER 12", "36698a60-e88c-4ce2-b06a-f173a581f920": "This projects to a point in data space given by Section 3.3.1 Note that this takes the same form as the equations for regularized linear regression and is a consequence of maximizing the likelihood function for a linear Gaussian model. Similarly, the posterior covariance is given from (12.42) by 0-2M- 1 and is independent of x. If we take the limit 0-2 ----t 0, then the posterior mean reduces to which represents an orthogonal projection of the data point onto the latent space, and so we recover the standard PCA model.\n\nThe posterior covariance in this limit is zero, however, and the density becomes singular. For 0-2 > 0, the latent projection is shifted towards the origin, relative to the orthogonal projection. Finally, we note that an important role for the probabilistic PCA model is in defining a multivariate Gaussian distribution in which the number of degrees of freedom, in other words the number of independent parameters, can be controlled whilst still allowing the model to capture the dominant correlations in the data. Recall that a general Gaussian distribution has D(D + 1)/2 independent parameters in its covariance matrix (plus another D parameters in its mean)", "bcedc184-114d-4c80-ad09-f6bf780519c4": "However, the Hessian can also be calculated exactly using an extension of the backpropagation technique. An important consideration for many applications of the Hessian is the ef\ufb01ciency with which it can be evaluated. If there are W parameters (weights and biases) in the network, then the Hessian matrix has dimensions W \u00d7 W and so the computational effort needed to evaluate the Hessian will scale like O(W 2) for each pattern in the data set.\n\nAs we shall see, there are ef\ufb01cient methods for evaluating the Hessian whose scaling is indeed O(W 2). Some of the applications for the Hessian matrix discussed above require the inverse of the Hessian, rather than the Hessian itself. For this reason, there has been some interest in using a diagonal approximation to the Hessian, in other words one that simply replaces the off-diagonal elements with zeros, because its inverse is trivial to evaluate", "0f7ce7ac-b62d-4051-92fc-5e52e498f1a0": "This problem is sometimes addressed by making predictions for new inputs x using Unfortunately, this heuristic approach suffers from the problem that the different classi\ufb01ers were trained on different tasks, and there is no guarantee that the realvalued quantities yk(x) for different classi\ufb01ers will have appropriate scales. Another problem with the one-versus-the-rest approach is that the training sets are imbalanced. For instance, if we have ten classes each with equal numbers of training data points, then the individual classi\ufb01ers are trained on data sets comprising 90% negative examples and only 10% positive examples, and the symmetry of the original problem is lost. A variant of the one-versus-the-rest scheme was proposed by Lee et al. who modify the target values so that the positive class has target +1 and the negative class has target \u22121/(K \u2212 1).\n\nWeston and Watkins  de\ufb01ne a single objective function for training all K SVMs simultaneously, based on maximizing the margin from each to remaining classes", "4be1e9ef-4c12-4cb2-85ea-d5d26dea321c": "Because such points can lie a long way to the wrong side of the ideal decision boundary, they can seriously distort the classi\ufb01er.\n\nNote that the logistic and probit regression models behave differently in this respect because the tails of the logistic sigmoid decay asymptotically like exp(\u2212x) for x \u2192 \u221e, whereas for the probit activation function they decay like exp(\u2212x2), and so the probit model can be signi\ufb01cantly more sensitive to outliers. However, both the logistic and the probit models assume the data is correctly labelled. The effect of mislabelling is easily incorporated into a probabilistic model by introducing a probability \u03f5 that the target value t has been \ufb02ipped to the wrong value , leading to a target value distribution for data point x of the form where \u03c3(x) is the activation function with input vector x. Here \u03f5 may be set in advance, or it may be treated as a hyperparameter whose value is inferred from the data. For the linear regression model with a Gaussian noise distribution, the error function, corresponding to the negative log likelihood, is given by (3.12)", "27488790-1258-4c99-8a68-1637a85ec7da": "The estimated value, \u02c6v(s,w), of any state (board position) s was meant to estimate the probability of winning starting from state s. To achieve this, rewards were de\ufb01ned as zero for all time steps except those on which the game is won. To implement the value function, TD-Gammon used a standard multilayer ANN, much like that shown to the right on the next page. (The real where wt is the vector of all modi\ufb01able parameters (in this case, the weights of the network) and et is a vector of eligibility traces, one for each component of wt, updated by with e0 = 0. The gradient in this equation can be computed e\ufb03ciently by the backpropagation procedure.\n\nFor the backgammon application, in which \u03b3 = 1 and the reward is always zero except upon winning, the TD error portion of the learning rule is usually just \u02c6v(St+1,w) \u2212 \u02c6v(St,w), as suggested in Figure 15.2. To apply the learning rule we need a source of backgammon games. Tesauro obtained an unending sequence of games by playing his learning backgammon player against itself", "687a6a1c-efcc-4782-909b-d81e848ccac9": "According to the setup in , let Paata(- ) be the data distribution over IR\" and Ppos(- ,-) be the distribution of positive pairs over R\"*\u201d.\n\nThese two distributions should satisfy:  Symmetry: VX, X*, Ppos(%X*) = Ppos(X* X)  Matching marginal: Vx, { Ppos(X,x*)dx* = paata(X)  To learn an encoder f(x) to learn a L2-normalized feature vector, the contrastive learning  objective is: Loortrastive = vos \u2014 log exp(f(x)' f(x*)/7) | mes ex Deprntsi hepa LY exep( Fla)! f0c*)/7) + Oi exp(f(x)! \u00a306; )/7) M * (x,x* )~ppos:{xi HS penta - F(x)\" F0e\")/r / log ( > exp( Fl) F057)/7))] j Assumini  1 M =~ y (x,xt \\wppoed ()\" f(**) + X~Daata |  T  Key Ingredients  Heavy Data Augmentation  https://lilianweng.github.io/posts/2021-05-31-contrastive/  Contrastive Representation Learning | Lil'Log   Given a training sample, data augmentation techniques are needed for creating noise versions of itself to feed into the loss as positive samples", "cfadb0e9-cffa-4ad0-81b1-8de8dcede169": "Then we run both the forward \u03b1 recursion and the backward \u03b2 recursion and use the results to evaluate \u03b3(zn) and \u03be(zn\u22121, zn). At this stage, we can also evaluate the likelihood function. This completes the E step, and we use the results to \ufb01nd a revised set of parameters \u03b8new using the M-step equations from Section 13.2.1. We then continue to alternate between E and M steps until some convergence criterion is satis\ufb01ed, for instance when the change in the likelihood function is below some threshold. Note that in these recursion relations the observations enter through conditional distributions of the form p(xn|zn). The recursions are therefore independent of the type or dimensionality of the observed variables or the form of this conditional distribution, so long as its value can be computed for each of the K possible states of zn. Since the observed variables {xn} are \ufb01xed, the quantities p(xn|zn) can be pre-computed as functions of zn at the start of the EM algorithm, and remain \ufb01xed throughout", "815a7da7-e836-43e7-9441-fdbf97fd5799": "Maximizing the margin leads to a particular choice of decision boundary, as shown on the right. The location of this boundary is determined by a subset of the data points, known as support vectors, which are indicated by the circles. having a common parameter \u03c32.\n\nTogether with the class priors, this de\ufb01nes an optimal misclassi\ufb01cation-rate decision boundary. However, instead of using this optimal boundary, they determine the best hyperplane by minimizing the probability of error relative to the learned density model. In the limit \u03c32 \u2192 0, the optimal hyperplane is shown to be the one having maximum margin. The intuition behind this result is that as \u03c32 is reduced, the hyperplane is increasingly dominated by nearby data points relative to more distant ones. In the limit, the hyperplane becomes independent of data points that are not support vectors. We shall see in Figure 10.13 that marginalization with respect to the prior distribution of the parameters in a Bayesian approach for a simple linearly separable data set leads to a decision boundary that lies in the middle of the region separating the data points. The large margin solution has similar behaviour", "f4809c88-c823-422e-8825-a2c291af4383": "Input: the policy \u21e1 to be evaluated Input: a feature function x : S+ ! Rd such that x(terminal, \u00b7) = 0 Algorithm parameters: step size \u21b5 > 0, trace decay rate \u03bb 2  Initialize value-function weights w 2 Rd (e.g., w = 0) The eligibility trace (12.11) used in true online TD(\u03bb) is called a dutch trace to distinguish it from the trace (12.5) used in TD(\u03bb), which is called an accumulating trace.\n\nEarlier work often used a third kind of trace called the replacing trace, de\ufb01ned only for the tabular case or for binary feature vectors such as those produced by tile coding. The replacing trace is de\ufb01ned on a component-by-component basis depending on whether the component of the feature vector was 1 or 0: Nowadays, we see replacing traces as crude approximations to dutch traces, which largely supercede them. Dutch traces usually perform better than replacing traces and have a clearer theoretical basis. Accumulating traces remain of interest for nonlinear function approximations where dutch traces are not available", "45ae22b6-d5f7-43e9-b509-b32a7f95b54d": "These datasets include MNIST hand written digit recognition, CIFAR-10/100, ImageNet, tiny-imagenet-200, SVHN (street view house numbers), Caltech-101/256, MIT places, MIT-Adobe 5K dataset, Pascal VOC, and Stan- ford Cars. The datasets most frequently discussed are CIFAR-10, CIFAR-100, and Ima- geNet. The expansion of open-source datasets has given researchers a wide variety of cases to compare performance results of Data Augmentation techniques. Most of these  datasets such as ImageNet would be classified as big data. Many experiments constrain  themselves to a subset of the dataset to simulate limited data problems. In addition to our focus on limited datasets, we will also consider the problem of class imbalance and how Data Augmentation can be a useful oversampling solution. Class imbalance describes a dataset with a skewed ratio of majority to minority samples. Leevy et al. describe many of the existing solutions to high-class imbalance across data types. Our survey will show how class-balancing oversampling in image data can be done with Data Augmentation", "de681d39-6580-4d7d-ac8e-9bf7ba71e381": "Recent approaches based on importance sampling, reweighted wake-sleep  and bidirectional Helmholtz machines  make it possible to quickly train sigmoid belief networks and reach state-of-the-art performance on benchmark tasks. A special case of sigmoid belief networks is the case where there are no latent variables. Learning in this case is efficient, because there is no need to marginalize latent variables out of the likelihood. A family of models called auto-regressive networks generalize this fully visible belief network to other kinds of variables besides binary variables and other structures of conditional distributions besides log-linear relationships. Auto-regressive networks are described in section 20.10.7. 20.10.2 Differentiable Generator Networks  Many generative models are based on the idea of using a differentiable generator network", "9f8a9308-cc51-4e62-a522-e2ac4a32b11a": "In this case, all the probability that would have gone into those missing neighbors goes into the probability of terminating on that side (thus, state 1 has a 0.5 chance of terminating on the left, and state 950 has a 0.25 chance of terminating on the right).\n\nAs usual, termination on the left produces a reward of \u22121, and termination on the right produces a reward of +1. All other transitions have a reward of zero. We use this task as a running example throughout this section. curving very slightly toward the horizontal for the last 100 states at each end. Also shown is the \ufb01nal approximate value function learned by the gradient Monte-Carlo algorithm with state aggregation after 100,000 episodes with a step size of \u21b5 = 2 \u21e5 10\u22125. For the state aggregation, the 1000 states were partitioned into 10 groups of 100 states each (i.e., states 1\u2013100 were one group, states 101\u2013200 were another, and so on). The staircase e\u21b5ect shown in the \ufb01gure is typical of state aggregation; within each group, the approximate value is constant, and it changes abruptly from one group to the next", "f4175896-578f-484d-93b0-a5f2f22d4099": "Through \ufb01ne-tuning, the model seems to make substitutions that are more coherent with the conditioning label and relevant to the original words (e.g., replacing the word \u201cstriking\u201d with \u201cbland\u201d in epoch 1 v.s. \u201ccharming\u201d in epoch 3). We next study a different problem setting where the training data of different classes are imbalanced. We show the data weighting approach greatly improves the classi\ufb01cation performance. It is also observed that, the LM data augmentation approach, which performs well in the low-data setting, fails on the class-imbalance problems. Setup Though the methods are broadly applicable to multi-way classi\ufb01cation problems, here we only study binary classi\ufb01cation tasks for simplicity. For text classi\ufb01cation, we use the SST-2 sentiment analysis benchmark ; while for image, we select class 1 and 2 from CIFAR10 for binary classi\ufb01cation. We use the same processing on both datasets to build the class-imbalance setting.\n\nSpeci\ufb01cally, we randomly select 1,000 training instances of class 2, and vary the number of class-1 instances in {20, 50, 100}. For each dataset, we use 10 validation examples in each class", "7925d9fb-c1d4-4f5d-a9e7-09eb1526ab8d": "Matsunaga et al.\n\nalso demonstrate the effectiveness of test-time augmentation on skin lesion classification, using geometric transformations such as rotation, translation, scaling, and flipping. The impact of test-time augmentation on classification accuracy is another mechanism for measuring the robustness of a classifier. A robust classifier is thus defined as having a low variance in predictions across augmentations. For example, a prediction of an image should not be much different when that same image is rotated 20\u00b0. In their experiments searching for augmentations with Reinforcement Learning, Minh et al. measure robustness by distorting test images with a 50% probability and contrasting the accu- racy on un-augmented data with the augmented data. In this study, the performance of the baseline model decreases from 74.61 to 66.87% when evaluated on augmented test images. Some classification models lie on the fence in terms of their necessity for speed. This suggests promise in developing methods that incrementally upgrade the confidence of prediction. This could be done by first outputting a prediction with little or no test- time augmentation and then incrementally adding test-time augmentations to increase the confidence of the prediction", "f8c8314d-4ab1-4a6c-b099-16c60b15d084": "Not all situations we might want to model have such a clear direction to their  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    interactions. When the interactions seem to have no intrinsic direction, or to operate in both directions, it may be more appropriate to use an undirected model.\n\nAs an example of such a situation, suppose we want to model a distribution over three binary variables: whether or not you are sick, whether or not your coworker is sick, and whether or not your roommate is sick. As in the relay race example, we can make simplifying assumptions about the kinds of interactions that take place. Assuming that your coworker and your roommate do not know each other, it is very unlikely that one of them will give the other an infection such as a cold directly. This event can be seen as so rare that it is acceptable not to model it. However, it is reasonably likely that either of them could give you a cold, and that you could pass it on to the other. We can model the indirect transmission of a cold from your coworker to your roommate by modeling the transmission of the cold from your coworker to you and the transmission of the cold from you to your roommate", "b5ecf8f7-55a6-475a-a89e-8946bb39c6b1": "Recently,  introduced a control variate schemes to reduce the high variance of the na\u00a8\u0131ve gradient estimator discussed in section 2.1, and applied to exponential family approximations of the posterior. In  some general methods, i.e. a control variate scheme, were introduced for reducing the variance of the original gradient estimator. In , a similar reparameterization as in this paper was used in an ef\ufb01cient version of a stochastic variational inference algorithm for learning the natural parameters of exponential-family approximating distributions. The AEVB algorithm exposes a connection between directed probabilistic models (trained with a variational objective) and auto-encoders. A connection between linear auto-encoders and a certain class of generative linear-Gaussian models has long been known.\n\nIn  it was shown that PCA corresponds to the maximum-likelihood (ML) solution of a special case of the linear-Gaussian model with a prior p(z) = N(0, I) and a conditional distribution p(x|z) = N(x; Wz, \u03f5I), speci\ufb01cally the case with in\ufb01nitesimally small \u03f5", "a2b297a1-67d1-44a3-836c-2721798a5a19": "To solve this sequence modeling task, Taylor et al.\n\nintroduced a conditional RBM  modeling p(a yee ) for small m. The model is an RBM over () | aD (t\u2014-m)  mim \\ 1 1 1 e us eu \u2122 1 em  https://www.deeplearningbook.org/contents/generative_models.html    Viv J WHOSE Dias paraluevers are a ulear \\YUPLIOU OL Lue preceding 1 values OL \u00a9. When we condition on different values of & and earlier variables, we get a new RBM over X. The weights in the RBM over X never change, but by conditioning on different past values, we can change the probability of different hidden units in the  RBM being active. By activating and deactivating different subsets of hidden units, we can make large changes to the probability distribution induced on x. Other variants of conditional RBM  and other variants of sequence modeling using conditional RBMs are possible . Another sequence modeling task is to model the distribution over sequences of musical notes used to compose songs. Boulanger-Lewandowski et al", "6df71be4-7bc5-47e0-8c59-8bd609602b68": "If we now identify HL with M and bL+1 with v, we obtain In this way, data points are sequentially absorbed until L+1 = N and the whole data set has been processed. This result therefore represents a procedure for evaluating the inverse of the Hessian using a single pass through the data set. The initial matrix H0 is chosen to be \u03b1I, where \u03b1 is a small quantity, so that the algorithm actually \ufb01nds the inverse of H + \u03b1I. The results are not particularly sensitive to the precise value of \u03b1. Extension of this algorithm to networks having more than one output is straightforward. Exercise 5.21 We note here that the Hessian matrix can sometimes be calculated indirectly as part of the network training algorithm.\n\nIn particular, quasi-Newton nonlinear optimization algorithms gradually build up an approximation to the inverse of the Hessian during training. Such algorithms are discussed in detail in Bishop and Nabney . As in the case of the \ufb01rst derivatives of the error function, we can \ufb01nd the second derivatives by using \ufb01nite differences, with accuracy limited by numerical precision", "24e99cf9-9d0d-4206-9fe1-b292f5f16238": "Recall from Section 14.2 that the TD model of classical conditioning is basically the semi-gradient-descent TD(\u03bb) algorithm with linear function approximation. Montague et al. made several assumptions to set up this comparison. First, because a TD error can be negative but neurons cannot have a negative \ufb01ring rate, they assumed that the quantity corresponding to dopamine neuron activity is \u03b4t\u22121 + bt, where bt is the background \ufb01ring rate of the neuron. A negative TD error corresponds to a drop in a dopamine neuron\u2019s \ufb01ring rate below its background rate.2 A second assumption was needed about the states visited in each classical conditioning trial and how they are represented as inputs to the learning algorithm. This is the same issue we discussed in Section 14.2.4 for the TD model. Montague et al.\n\nchose a complete serial compound (CSC) representation as shown in the left column of Figure 14.1, but where the sequence of short-duration internal signals continues until the onset of the US, which here is the arrival of a non-zero reward signal", "0bea73d0-3cb7-4eea-94a4-e8104e8c6725": "the same distribution as the directed graph, whose factor satis\ufb01es f(x1, x2, x3) = p(x1)p(x2)p(x3|x1, x2). (c) A different factor graph representing the same distribution with factors fa(x1) = p(x1), fb(x2) = p(x2) and fc(x1, x2, x3) = p(x3|x1, x2).\n\nFactor graphs are said to be bipartite because they consist of two distinct kinds of nodes, and all links go between nodes of opposite type. In general, factor graphs can therefore always be drawn as two rows of nodes (variable nodes at the top and factor nodes at the bottom) with links between the rows, as shown in the example in Figure 8.40. In some situations, however, other ways of laying out the graph may be more intuitive, for example when the factor graph is derived from a directed or undirected graph, as we shall see. If we are given a distribution that is expressed in terms of an undirected graph, then we can readily convert it to a factor graph", "717b59c0-5b97-4020-8758-3e73e54da4f5": "We can thus interpret the pseudoinverse as stabilizing underdetermined problems using regularization.\n\n7.4 Dataset Augmentation  The best way to make a machine learning model generalize better is to train it on more data. Of course, in practice, the amount of data we have is limited. One way to get around this problem is to create fake data and add it to the training set. For some machine learning tasks, it is reasonably straightforward to create new fake data. This approach is easiest for classification. A classifier needs to take a complicat- ed, high-dimensional input \u00ab and summarize it with a single category identity y. This means that the main task facing a classifier is to be invariant to a wide variety of transformations. We can generate new (2, y) pairs easily just by transforming the & inputs in our training set. This approach is not as readily applicable to many other tasks. For example, it is difficult to generate new fake data for a density estimation task unless we have already solved the density estimation problem. Dataset augmentation has been a particularly effective technique for a specific classification problem: object recognition", "eea2dffb-869e-4b95-aefc-9eb1d86eb388": "So far, we have viewed neural networks as a general class of parametric nonlinear functions from a vector x of input variables to a vector y of output variables.\n\nA simple approach to the problem of determining the network parameters is to make an analogy with the discussion of polynomial curve \ufb01tting in Section 1.1, and therefore to minimize a sum-of-squares error function. Given a training set comprising a set of input vectors {xn}, where n = 1, . , N, together with a corresponding set of target vectors {tn}, we minimize the error function However, we can provide a much more general view of network training by \ufb01rst giving a probabilistic interpretation to the network outputs. We have already seen many advantages of using probabilistic predictions in Section 1.5.4. Here it will also provide us with a clearer motivation both for the choice of output unit nonlinearity and the choice of error function. We start by discussing regression problems, and for the moment we consider a single target variable t that can take any real value", "2e2bedde-5b93-43fb-9b16-281f228b0e94": "We are now ready to consider how Monte Carlo estimation can be used in control, that is, to approximate optimal policies. The overall idea is to proceed according to the same pattern as in the DP chapter, that is, according to the idea of generalized policy iteration (GPI). In GPI one maintains both an approximate policy and an approximate value function. The value function is repeatedly altered to more closely approximate the value function for the current policy, and the policy is repeatedly improved with respect to the current value function, as suggested by the diagram to the right.\n\nThese two kinds of changes work against each other to some extent, as each creates a moving target for the other, but together they cause both policy and value function to approach optimality. this method, we perform alternating complete steps of policy evaluation and policy improvement, beginning with an arbitrary policy \u21e10 and ending with the optimal policy and optimal action-value function: improvement. Policy evaluation is done exactly as described in the preceding section. Many episodes are experienced, with the approximate action-value function approaching the true function asymptotically. For the moment, let us assume that we do indeed observe an in\ufb01nite number of episodes and that, in addition, the episodes are generated with exploring starts", "caba0119-2b9f-4148-ad20-ccf409025349": "If the parameters \u03c6k are independent for the different components, then this term decouples into a sum of terms one for each value of k, each of which can be maximized independently.\n\nWe are then simply maximizing the weighted log likelihood function for the emission density p(x|\u03c6k) with weights \u03b3(znk). Here we shall suppose that this maximization can be done ef\ufb01ciently. For instance, in the case of Gaussian emission densities we have p(x|\u03c6k) = N(x|\u00b5k, \u03a3k), and maximization of the function Q(\u03b8, \u03b8old) then gives For the case of discrete multinomial observed variables, the conditional distribution of the observations takes the form An analogous result holds for Bernoulli observed variables. The EM algorithm requires initial values for the parameters of the emission distribution. One way to set these is \ufb01rst to treat the data initially as i.i.d. and \ufb01t the emission density by maximum likelihood, and then use the resulting values to initialize the parameters for EM", "80dab02e-f49e-4703-9b73-4c81cf511f68": "We can write this using a labeling function generator, which creates two labeling functions. In this way, generators can be connected to large resources and create hundreds of labeling functions with a line of code. Fig. 5 Labeling functions expressing pattern matching, heuristic, and distant supervision approaches, respectively, in Snorkel\u2019s Jupyter notebook interface, for the Spouses example. Full code is available in Snorkel\u2019s Intro tutorial. https://github.com/HazyResearch/snorkel/tree/ master/tutorials/intro Fig. 6 The data viewer utility in Snorkel, showing candidate spouse relation mentions from the Spouses example, composed of person\u2013 person mention pairs Interface Implementation Snorkel\u2019s interface is designed to be accessible to subject matter expert (SME) users without advanced programming skills.\n\nAll components run in Jupyter iPython notebooks,5 including writing labeling functions.6 Users can therefore write labeling functions as arbitrary Python functions for maximum \ufb02exibility (Fig. 5). We also provide a library of labeling function primitives and generators to more declaratively program weak supervision, and a viewer utility (Fig", "2be9411d-2373-4158-bdc5-aa8040ba5062": "In both cases, hypotheses resulting from the learning experiments can be tested in the \ufb01eld by instrumenting real gliders and by comparing predictions with observed bird soaring behavior.\n\nIn this \ufb01nal chapter we touch on some topics that are beyond the scope of this book but that we see as particularly important for the future of reinforcement learning. Many of these topics bring us beyond what is reliably known, and some bring us beyond the MDP framework. Over the course of this book, our notion of value function has become quite general. With o\u21b5-policy learning we allowed a value function to be conditional on an arbitrary target policy. Then in Section 12.8 we generalized discounting to a termination function \u03b3 : S 7! , so that a di\u21b5erent discount rate could be applied at each time step in determining the return (12.17). This allowed us to express predictions about how much reward we will get over an arbitrary, state-dependent horizon. The next, and perhaps \ufb01nal, step is to generalize beyond rewards to permit predictions about arbitrary signals. Rather than predicting the sum of future rewards, we might predict the sum of the future values of a sound or color sensation, or of an internal, highly processed signal such as another prediction", "5925d07f-43ad-4ea7-90ee-3f539ee51dd2": "A key quantity in PAC learning is the Vapnik-Chervonenkis dimension, or VC dimension, which provides a measure of the complexity of a space of functions, and which allows the PAC framework to be extended to spaces containing an in\ufb01nite number of functions. The bounds derived within the PAC framework are often described as worstcase, because they apply to any choice for the distribution p(x, t), so long as both the training and the test examples are drawn (independently) from the same distribution, and for any choice for the function f(x) so long as it belongs to F. In real-world applications of machine learning, we deal with distributions that have signi\ufb01cant regularity, for example in which large regions of input space carry the same class label. As a consequence of the lack of any assumptions about the form of the distribution, the PAC bounds are very conservative, in other words they strongly over-estimate the size of data sets required to achieve a given generalization performance. For this reason, PAC bounds have found few, if any, practical applications", "36c6258f-4a44-4421-b5eb-8c6607506b9f": "We \ufb01rst present Rescorla and Wagner\u2019s model using their terminology and notation before shifting to the terminology and notation we use to describe the TD model. Here is how Rescorla and Wagner described their model. The model adjusts the \u201cassociative strength\u201d of each component stimulus of a compound CS, which is a number representing how strongly or reliably that component is predictive of a US. When a compound CS consisting of several component stimuli is presented in a classical conditioning trial, the associative strength of each component stimulus changes in a way that depends on an associative strength associated with the entire stimulus compound, called the \u201caggregate associative strength,\u201d and not just on the associative strength of each component itself. Rescorla and Wagner considered a compound CS AX, consisting of component stimuli A and X, where the animal may have already experienced stimulus A, and stimulus X might be new to the animal. Let VA, VX, and VAX respectively denote the associative strengths of stimuli A, X, and the compound AX.\n\nSuppose that on a trial the compound CS AX is followed by a US, which we label stimulus Y", "fe38b0b7-94f8-448a-b20c-55d8072df16b": "Han Guo1, Bowen Tan1, Zhengzhong Liu1,2, Eric P. Xing1,2,4, Zhiting Hu3 {hanguo, btan2, epxing}@cs.cmu.edu, hectorzliu@gmail.com, zhh019@ucsd.edu Maximum likelihood estimation (MLE) is the predominant algorithm for training text generation models. This paradigm relies on direct supervision examples, which is not applicable to many emerging applications, such as generating adversarial attacks or generating prompts to control language models. Reinforcement learning (RL) on the other hand offers a more \ufb02exible solution by allowing users to plug in arbitrary task metrics as reward. Yet previous RL algorithms for text generation, such as policy gradient (on-policy RL) and Q-learning (off-policy RL), are often notoriously inef\ufb01cient or unstable to train due to the large sequence space and the sparse reward received only at the end of sequences. In this paper, we introduce a new RL formulation for text generation from the soft Q-learning (SQL) perspective", "23e585e5-4810-4d0a-aabf-f4a0bb1f47f5": "However, if these conditionals are log concave, then sampling can be done ef\ufb01ciently using adaptive rejection sampling (assuming the corresponding variable is a scalar). If, at each stage of the Gibbs sampling algorithm, instead of drawing a sample from the corresponding conditional distribution, we make a point estimate of the variable given by the maximum of the conditional distribution, then we obtain the iterated conditional modes (ICM) algorithm discussed in Section 8.3.3. Thus ICM can be seen as a greedy approximation to Gibbs sampling. Because the basic Gibbs sampling technique considers one variable at a time, there are strong dependencies between successive samples. At the opposite extreme, if we could draw samples directly from the joint distribution (an operation that we are supposing is intractable), then successive samples would be independent.\n\nWe can hope to improve on the simple Gibbs sampler by adopting an intermediate strategy in which we sample successively from groups of variables rather than individual variables. This is achieved in the blocking Gibbs sampling algorithm by choosing blocks of variables, not necessarily disjoint, and then sampling jointly from the variables in each block in turn, conditioned on the remaining variables", "a919f757-54d9-4381-a7ab-f7072ebb92d1": "The contrastive divergence (CD, or CD-k to indicate CD with k Gibbs steps) algorithm initializes the Markov chain at each step with samples from the data distribution . This approach is presented as algorithm 18.2. Obtaining samples from the data distribution is free, because they are already available in the dataset. Initially, the data distribution is not close to the model distribution, so the negative phase is not very accurate. Fortunately, the positive phase can still accurately increase the model\u2019s probability of the data. After the positive phase has had some time to act, the model distribution is closer to the data distribution, and the negative phase starts to become accurate. Of course, CD is still an approximation to the correct negative phase. The main way in which CD qualitatively fails to implement the correct negative phase is that it fails to suppress regions of high probability that are far from actual training examples. These regions that have high probability under the model but low probability under the data-generating distribution are called spurious modes. Figure 18.2 illustrates why this happens", "586327aa-f148-4fd2-81f7-49b2dbdb0a42": "SGD methods are \u201cgradient descent\u201d methods because the overall step in wt is proportional to the negative gradient of the example\u2019s squared error (9.4). This is the direction in which the error falls most rapidly.\n\nGradient descent methods are called \u201cstochastic\u201d when the update is done, as here, on only a single example, which might have been selected stochastically. Over many examples, making small steps, the overall e\u21b5ect is to minimize an average performance measure such as the VE. It may not be immediately apparent why SGD takes only a small step in the direction of the gradient. Could we not move all the way in this direction and completely eliminate the error on the example? In many cases this could be done, but usually it is not desirable. Remember that we do not seek or expect to \ufb01nd a value function that has zero error for all states, but only an approximation that balances the errors in di\u21b5erent states. If we completely corrected each example in one step, then we would not \ufb01nd such a balance. In fact, the convergence results for SGD methods assume that \u21b5 decreases over time", "55de9a92-98f9-4259-b1ee-f3c53032b236": "The softmax function is defined to be __exp(z;)  je exp(2j) Consider what happens when all the x; are equal to some constant c. Analytically, we can see that all the outputs should be equal to t Numerically, this may not occur when c has large magnitude. If \u00a2 is very negative, then exp(c) will underflow. This means the denominator of the softmax will become 0, so the final result is undefined.\n\nWhen c is very large and positive, exp(c) will overflow, again resulting in the expression as a whole being undefined. Both of these difficulties can be resolved by instead evaluating softmax(z) where z = x \u2014 max; 2;. Simple algebra shows that the value of the softmax function is not changed analytically by adding or subtracting a scalar from the input vector. Subtracting max; x; results in the largest argument to exp being 0, which rules out the possibility of overflow. Likewise, at least one term in the denominator has a value of 1, which rules out che possibility of underflow in the denominator leading to a division by zero", "22da6e11-390c-4504-9fd5-2a61fb4c1bcc": "In order for the system Aw = b to have a solution for all values of b \u20ac R\u201d, we therefore require that the column space of A be all of R\u2122. If any point in R\u2122 is excluded from the column space, that point is a potential value of b that has no solution. The requirement that the column space of A be all of R\u2122 implies immediately that A must have at least m columns, that is, n > m. Otherwise, the dimensionality of the column space would be less than m. For example, consider a 3 x 2 matrix. The target bis 3-D, but a is only 2-D, so modifying the value of x at best enables us to trace out a 2-D plane within R\u00ae. The equation has a solution if and only if 6 lies on that plane.\n\nHaving is only a necessary condition for every point to have a solution. It is not a syfgignt condition, because it is possible for some of the columns to  https://www.deeplearningbook.org/contents/linear_algebra.html    be redundant. Consider a 2 X 2 matrix where both of the columns are identical", "e9f12c84-d7bf-4338-a65a-cf553ebe5735": "When starting from a rather improbable configuration (higher energy than the typical ones from p(x)), che chain tends to gradually reduce the energy of the state and only occasionally move to another mode.\n\nOnce the chain has found a region of low energy (for example, if the variables are pixels in an image, a region of low energy might be a connected manifold of images of the same object), which we call a mode, the chain will tend to walk around that mode (following a kind of random walk). Once in a while it will step out of that mode and generally return to it or (if it finds an escape route) move toward another mode. The problem is that successful escape routes  are rare for many interesting distributions, so the Markov chain will continue to sample the same mode longer than it should. This is very clear when we consider the Gibbs sampling algorithm (section 17.4). In this context, consider the probability of going from one mode to a nearby mode within a given number of steps. What will determine that probability is the shape of the \u201cenergy barrier\u201d between these modes. Transitions between two modes that are separated by a high energy barrier (a region of low probability) are exponentially less likely (in terms of the height of the energy barrier)", "a38b5b18-fe03-42d9-85a0-97b065e4c526": "U2, where Ul and U2 are the eigenvectors corresponding to the largest and second largest eigenvalues. An example of such a plot, for the oil flow data set, is shown in Figure 12.8. In some application. of pliTlCipal component analysis. the number of data points is smaller than t!>c dimensionality of troe data 'pace. FOI\" example. \",e might want to apply PeA to a data <el of a few hundred images, each of ,,'hich rorrespoOOs to a \"eetor in a 'pace of poIentially .....ml million dimensiOlls (COITesponding tn thfl'e enlour \"alues for each of the pi. \",ls in troe image), NOIe that in a D-<limen,ional space a set of jY points. \",'here N < D", "9c3588e1-b5ed-432a-8b7c-4cfdc5f6ca81": "His program became a \u201cbetter-than-average novice\u201d after learning from many games against itself, a variety of human opponents, and from book games in a supervised learning mode. Rote learning and other aspects of Samuel\u2019s work strongly suggest the essential idea of temporal-di\u21b5erence learning\u2014that the value of a state should equal the value of likely following states. Samuel came closest to this idea in his second learning method, his \u201clearning by generalization\u201d procedure for modifying the parameters of the value function. Samuel\u2019s method was the same in concept as that used much later by Tesauro in TD-Gammon. He played his program many games against another version of itself and performed an update after each move. The idea of Samuel\u2019s update is suggested by the backup diagram in Figure 16.2. Each open circle represents a position where the program moves next, an on-move position, and each solid circle represents a position where the opponent moves next.\n\nAn update was made to the value of each on-move position after a move by each side, resulting in a second on-move position. The update was toward the minimax value of a search launched from the second on-move position", "9aa0de90-3ff9-4d7d-ae5a-f460d59ec8a5": "Controlling the sensitivity of support vector machines. In Proceedings of the International Joint Conference on Arti\ufb01cial Intelligence (IJCAI99), Workshop ML3, pp. 55\u201360. Viola, P. and M. Jones . Robust real-time face detection. International Journal of Computer Vision 57(2), 137\u2013154. Viterbi, A. J. Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. IEEE Transactions on Information Theory IT-13, 260\u2013267. Wahba, G. A comparison of GCV and GML for choosing the smoothing parameter in the generalized spline smoothing problem. Numerical Mathematics 24, 383\u2013393. Wainwright, M. J., T. S. Jaakkola, and A. S. Willsky . A new class of upper bounds on the log partition function. IEEE Transactions on Information Theory 51, 2313\u20132335. Walker, A. M", "2bfffa72-e60f-4b9b-b14b-83eab4835a5b": "J., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. Maxout networks. In ICML\u20192013. Goodfellow, I. J., Mirza, M., Courville, A., and Bengio, Y. Multi-prediction deep Boltzmann machines. In NIPS\u20192013. Goodfellow, I. J., Warde-Farley, D., Lamblin, P., Dumoulin, V., Mirza, M., Pascanu, R., Bergstra, J., Bastien, F., and Bengio, Y. Pylearn2: a machine learning research library. arXiv preprint arXiv:1308.4214. Gregor, K., Danihelka, I., Mnih, A., Blundell, C., and Wierstra, D. Deep autoregressive networks. In ICML\u20192014. Gutmann, M. and Hyvarinen, A", "c5cebb01-fb86-4f48-8025-eab916f0db20": "Computational experiments with this bilevel optimization approach\u2014one level analogous to evolution, and the other due to reinforcement learning by individual agents\u2014have con\ufb01rmed that intuition alone is not always adequate to devise a good reward signal . The performance of a reinforcement learning agent as evaluated by the high-level objective function can be very sensitive to details of the agent\u2019s reward signal in subtle ways determined by the agent\u2019s limitations and the environments in which it acts and learns.\n\nThese experiments also demonstrated that an agent\u2019s goal should not always be the same as the goal of the agent\u2019s designer. At \ufb01rst this seems counterintuitive, but it may be impossible for the agent to achieve the designer\u2019s goal no matter what its reward signal is. The agent has to learn under various kinds of constraints, such as limited computational power, limited access to information about its environment, or limited time to learn. When there are constraints like these, learning to achieve a goal that is di\u21b5erent from the designer\u2019s goal can sometimes end up getting closer to the designer\u2019s goal than if that goal were pursued directly . Examples of this in the natural world are easy to \ufb01nd. Because we cannot directly assess the nutritional value of most foods, evolution\u2014the designer of our reward signal\u2014gave us a reward signal that makes us seek certain tastes", "643cae64-42fb-4367-8746-42675d7d7f36": "This can be achieved if the clique potentials of the undirected graph are given by the conditional distributions of the directed graph.\n\nIn order for this to be valid, we must ensure that the set of variables that appears in each of the conditional distributions is a member of at least one clique of the undirected graph. For nodes on the directed graph having just one parent, this is achieved simply by replacing the directed link with an undirected link. However, for nodes in the directed graph having more than one parent, this is not suf\ufb01cient. These are nodes that have \u2018head-to-head\u2019 paths encountered in our discussion of conditional independence. Consider a simple directed graph over 4 nodes shown in Figure 8.33. The joint distribution for the directed graph takes the form We see that the factor p(x4|x1, x2, x3) involves the four variables x1, x2, x3, and x4, and so these must all belong to a single clique if this conditional distribution is to be absorbed into a clique potential. To ensure this, we add extra links between all pairs of parents of the node x4", "1056b192-dda6-4482-8446-df34603df9fa": "By Fubini\u2019s Theorem, this implies that for almost every \u03b8 the section A\u03b8 = {z : (\u03b8, z) \u2208 A} has measure 0. Let\u2019s now \ufb01x a \u03b80 such that the measure of A\u03b80 is null (such as when the right hand side of equation (5) is well de\ufb01ned).\n\nFor this \u03b80 we have \u2207\u03b8f(g\u03b8(z))|\u03b80 is well-de\ufb01ned for almost any z, and since p(z) has a density, it is de\ufb01ned p(z)-a.e. By assumption 1 we know that Ez\u223cp(z) \u2264 Ez\u223cp(z) < +\u221e so Ez\u223cp(z) is well-de\ufb01ned for almost every \u03b80", "0947b5ab-cc36-4d08-94db-0636ae7793c2": "One important consideration to keep in mind when designing undirected models is that it is possible to specify the factors in such a way that Z does not exist. This happens if some of the variables in the model are continuous and the integral of p over their domain diverges. For example, suppose we want to model a single scalar variable x \u20ac R with a single clique potential \u00a2(x) = x\u201d. In this case,  Z= , ,...,}), then p(x) = softmax(b), so a large value of b; actually reduces p(x; = 1) for j # 7. Often, it is possible to leverage the effect of a carefully chosen domain of a variable to obtain complicated behavior from a relatively simple set of \u00a2 functions. We explore a practical application of this idea in section 20.6. 566  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    CHAPTER 16", "00cc23e0-9245-487a-a606-231b87cb2926": "Maximizing the negative returns in our version is equivalent to minimizing the costs of paths from a start state to a goal state. Examples of this kind of task are minimum-time control tasks, where each time step required to reach a goal produces a reward of \u22121, or problems like the Golf example in Section 3.5, whose objective is to hit the hole with the fewest strokes. (page 111) is a stochastic optimal path problem.\n\nComparing RTDP and the conventional DP value iteration algorithm on an example racetrack problem illustrates some of the advantages of on-policy trajectory sampling. Recall from the exercise that an agent has to learn how to drive a car around a turn like those shown in Figure 5.5 and cross the \ufb01nish line as quickly as possible while staying on the track. Start states are all the zero-speed states on the starting line; the goal states are all the states that can be reached in one time step by crossing the \ufb01nish line from inside the track. Unlike Exercise 5.12, here there is no limit on the car\u2019s speed, so the state set is potentially in\ufb01nite. However, the set of states that can be reached from the set of start states via any policy is \ufb01nite and can be considered to be the state set of the problem", "b7c04e46-6f04-432f-a0af-3dae0616f1a5": "If the joint distribution p(Z, X|\u03b8) comprises a member of the exponential family, or a product of such members, then we see that the logarithm will cancel the exponential and lead to an M step that will be typically much simpler than the maximization of the corresponding incomplete-data log likelihood function p(X|\u03b8). The operation of the EM algorithm can also be viewed in the space of parameters, as illustrated schematically in Figure 9.14.\n\nHere the red curve depicts the (incomplete data) log likelihood function whose value we wish to maximize. We start with some initial parameter value \u03b8old, and in the \ufb01rst E step we evaluate the posterior distribution over latent variables, which gives rise to a lower bound L(\u03b8, \u03b8(old)) whose value equals the log likelihood at \u03b8(old), as shown by the blue curve. Note that the bound makes a tangential contact with the log likelihood at \u03b8(old), so that both curves have the same gradient. This bound is a convex function having a unique Exercise 9.25 maximum (for mixture components from the exponential family)", "4bccc620-713f-44e2-8435-c1dd1db196d2": "An early  example is the ranking loss proposed by Collobert and Weston , which views the output of the neural language model for each word as a score and tries to make the score of the correct word a, be ranked high in comparison to the other scores a;. The ranking loss proposed then is  L =~ max(0,1\u2014 ay + a). (12.19)  466  CHAPTER 12. APPLICATIONS  The gradient is zero for the i-th term if the score of the observed word, ay, is greater than the score of the negative word a; by a margin of 1. One issue with this criterion is that it does not provide estimated conditional probabilities, which are useful in some applications, including speech recognition and text generation (including conditional text generation tasks such as translation). A more recently used training objective for neural language models is noise- contrastive estimation, which is introduced in section 18.6", "2b1a9828-aa0a-4199-bdc8-49b86eb23afa": "Instead of having separate unsupervised and supervised components in the model, one can construct models in which a generative model of either P(x) or P(x,y) shares parameters with a discriminative model of P(y | x).\n\nOne can then trade off the supervised criterion \u2014 log P(y | x) with the unsupervised or generative one (such as \u2014 log P(x) or \u2014log P(x, y)). The generative criterion then expresses a particular form of prior belief about the solution to the supervised learning problem , namely that the structure of P(x) is connected to the structure of P( ) in a way that is captured by the shared  n ao. 1 1 eu a tat oe ie! https://www.deeplearningbook.org/contents/regularization.html    pat ALLleLr 14avlol", "7791d862-bdb6-4df5-a9b7-dd3bb6a76086": "7.11 (\u22c6 \u22c6) Repeat the above exercise, but this time make use of the general result (2.115). 7.12 (\u22c6 \u22c6) www Show that direct maximization of the log marginal likelihood (7.85) for the regression relevance vector machine leads to the re-estimation equations (7.87) and (7.88) where \u03b3i is de\ufb01ned by (7.89). 7.13 (\u22c6 \u22c6) In the evidence framework for RVM regression, we obtained the re-estimation formulae (7.87) and (7.88) by maximizing the marginal likelihood given by (7.85). Extend this approach by inclusion of hyperpriors given by gamma distributions of the form (B.26) and obtain the corresponding re-estimation formulae for \u03b1 and \u03b2 by maximizing the corresponding posterior probability p(t, \u03b1, \u03b2|X) with respect to \u03b1 and \u03b2. 7.14 (\u22c6 \u22c6) Derive the result (7.90) for the predictive distribution in the relevance vector machine for regression", "583e492e-654b-44ce-ae9c-a6c6814ca623": "The probability of x = 1 will be denoted by the parameter \u00b5 so that where 0 \u2a7d \u00b5 \u2a7d 1, from which it follows that p(x = 0|\u00b5) = 1 \u2212 \u00b5. The probability distribution over x can therefore be written in the form which is known as the Bernoulli distribution. It is easily veri\ufb01ed that this distribution Exercise 2.1 is normalized and that it has mean and variance given by Now suppose we have a data set D = {x1, . , xN} of observed values of x. We can construct the likelihood function, which is a function of \u00b5, on the assumption that the observations are drawn independently from p(x|\u00b5), so that In a frequentist setting, we can estimate a value for \u00b5 by maximizing the likelihood function, or equivalently by maximizing the logarithm of the likelihood. In the case of the Bernoulli distribution, the log likelihood function is given by At this point, it is worth noting that the log likelihood function depends on the N observations xn only through their sum \ufffd n xn. This sum provides an example of a suf\ufb01cient statistic for the data under this distribution, and we shall study the important role of suf\ufb01cient statistics in some detail", "08d25c6b-7398-44a2-9982-563f5a812e6a": "This is an example of a rather common operation associated with Gaussian distributions, sometimes called \u2018completing the square\u2019, in which we are given a quadratic form de\ufb01ning the exponent terms in a Gaussian distribution, and we need to determine the corresponding mean and covariance. Such problems can be solved straightforwardly by noting that the exponent in a general Gaussian distribution N(x|\u00b5, \u03a3) can be written where \u2018const\u2019 denotes terms which are independent of x, and we have made use of the symmetry of \u03a3.\n\nThus if we take our general quadratic form and express it in the form given by the right-hand side of (2.71), then we can immediately equate the matrix of coef\ufb01cients entering the second order term in x to the inverse covariance matrix \u03a3\u22121 and the coef\ufb01cient of the linear term in x to \u03a3\u22121\u00b5, from which we can obtain \u00b5. Now let us apply this procedure to the conditional Gaussian distribution p(xa|xb) for which the quadratic form in the exponent is given by (2.70). We will denote the mean and covariance of this distribution by \u00b5a|b and \u03a3a|b, respectively", "0d3f42e9-553e-482c-a882-c6852bc47c1d": "Unsupervised learning and supervised learning are not formally defined terms. The lines between them are often blurred. Many machine learning technologies can be used to perform both tasks. For example, the chain rule of probability states that for a vector x \u20ac R\u201d, the joint distribution can be decomposed as  n  p(x) = [[\u00bb&: | X,-+-,Xi-1). (5.1)  i=l This decomposition means that we can solve the ostensibly unsupervised problem of  103  CHAPTER 5. MACHINE LEARNING BASICS  modeling p(x) by splitting it into n supervised learning problems. Alternatively, we can solve the supervised learning problem of learning p(y | x) by using traditional unsupervised learning technologies to learn the joint distribution p(x, y), then inferring x,  D(x, y) ~.\n\n(5.2)  y P(x y\u2019) Though unsupervised learning and supervised learning are not completely formal or distinct concepts, they do help roughly Categorize some of the things we do with machine learning algorithms", "93572b3e-097b-4178-b4af-0d5a9ae2f45a": "The quantities aj are known as activations.\n\nEach of them is then transformed using a differentiable, nonlinear activation function h(\u00b7) to give These quantities correspond to the outputs of the basis functions in (5.1) that, in the context of neural networks, are called hidden units. The nonlinear functions h(\u00b7) are generally chosen to be sigmoidal functions such as the logistic sigmoid or the \u2018tanh\u2019 function. Following (5.1), these values are again linearly combined to give output Exercise 5.1 where k = 1, . , K, and K is the total number of outputs. This transformation corresponds to the second layer of the network, and again the w(2) k0 are bias parameters. Finally, the output unit activations are transformed using an appropriate activation function to give a set of network outputs yk. The choice of activation function is determined by the nature of the data and the assumed distribution of target variables and follows the same considerations as for linear models discussed in Chapters 3 and 4. Thus for standard regression problems, the activation function is the identity so that yk = ak", "d4cef448-8858-4ad9-8a72-585e0c79be5c": "We \ufb01nd that a whole food diet military: This essay discusses how you can build a community with the help of friends and family.\\n\\n\\n\\n\\n \"The people around me are the ones who need help. They are the ones who need help. They are the ones who are not alone. \"\\n Michael\\n \"It\u2019s", "fba23255-bc8f-44b0-9a92-9e56b4a2f8c1": "We now look in detail at what the actor and critic learning algorithms suggest about the rules governing changes in synaptic e\ufb03cacies of corticostriatal synapses.\n\nIf the brain does implement something like the actor\u2013critic algorithm\u2014and assuming populations of dopamine neurons broadcast a common reinforcement signal to the corticostriatal synapses of both the dorsal and ventral striatum as illustrated in Figure 15.5b (which is likely an oversimpli\ufb01cation as we mentioned above)\u2014then this reinforcement signal a\u21b5ects the synapses of these two structures in di\u21b5erent ways. The learning rules for the critic and the actor use the same reinforcement signal, the TD error \u03b4, but its e\u21b5ect on learning is di\u21b5erent for these two components. The TD error (combined with eligibility traces) tells the actor how to update action probabilities in order to reach higher-valued states. Learning by the actor is like instrumental conditioning using a Law-of-E\u21b5ect-type learning rule (Section 1.7): the actor works to keep \u03b4 as positive as possible", "512a448d-cc21-49f6-844d-d859d8037e26": "As a result of the optimization, we \ufb01nd that a proportion of the hyperparameters {\u03b1i} are driven to large (in principle in\ufb01nite) values, and so the weight parameters Section 7.2.2 wi corresponding to these hyperparameters have posterior distributions with mean and variance both zero. Thus those parameters, and the corresponding basis functions \u03c6i(x), are removed from the model and play no role in making predictions for new inputs. In the case of models of the form (7.78), the inputs xn corresponding to the remaining nonzero weights are called relevance vectors, because they are identi\ufb01ed through the mechanism of automatic relevance determination, and are analogous to the support vectors of an SVM.\n\nIt is worth emphasizing, however, that this mechanism for achieving sparsity in probabilistic models through automatic relevance determination is quite general and can be applied to any model expressed as an adaptive linear combination of basis functions. Having found values \u03b1\u22c6 and \u03b2\u22c6 for the hyperparameters that maximize the marginal likelihood, we can evaluate the predictive distribution over t for a new input x", "a4a5e163-5243-46b9-a25d-c051283a6567": "REPRESENTATION LEARNING  https://www.deeplearningbook.org/contents/representation.html    Figure 15.8: Illustration of how the nearest neighbor algorithm breaks up the input space into different regions. The nearest neighbor algorithm provides an example of a learning algorithm based on a nondistributed representation. Different non-distributed algorithms may have different geometry, but they typically break the input space into regions, with a separate set of parameters for each region. The advantage of a nondistributed  approach is that, given enough parameters, i difficult optimization algorithm, because it is s  can fit the training set without solving a traightforward to choose a different output  independently for each region. The disadvantage is that such nondistributed models generalize only locally via the smoothness prior, making it difficult to learn a complicated function with more peaks and troughs than the available number of examples. Contrast  this with a distributed representation, figure  5.7", "d5d30cbd-75e1-48ad-8537-dfd90b48d78a": "Setting the derivative of (14.39) with respect to wk equal to zero gives which we can write in matrix notation as This represents a set of modi\ufb01ed normal equations corresponding to the weighted least squares problem, of the same form as (4.99) found in the context of logistic regression. Note that after each E step, the matrix Rk will change and so we will have to solve the normal equations afresh in the subsequent M step. Finally, we maximize Q(\u03b8, \u03b8old) with respect to \u03b2.\n\nKeeping only terms that depend on \u03b2, the function Q(\u03b8, \u03b8old) can be written Setting the derivative with respect to \u03b2 equal to zero, and rearranging, we obtain the M-step equation for \u03b2 in the form In Figure 14.8, we illustrate this EM algorithm using the simple example of \ufb01tting a mixture of two straight lines to a data set having one input variable x and one target variable t. The predictive density (14.34) is plotted in Figure 14.9 using the converged parameter values obtained from the EM algorithm, corresponding to the right-hand plot in Figure 14.8. Also shown in this \ufb01gure is the result of \ufb01tting a single linear regression model, which gives a unimodal predictive density", "08b90df2-0e50-4f4a-b948-e46bdae696c6": "SMOTE and the extension of Borderline-SMOTE  create new instances by interpolating new points from existing instances via k-Nearest Neighbors. The primary focus of this technique was to alleviate problems due to class imbalance, and SMOTE was primarily used for tabular and vector data. The AlexNet CNN architecture developed by Krizhevsky et al. revolutionized image classification by applying convolutional networks to the ImageNet dataset. Data Augmentation is used in their experiments to increase the dataset size by a magnitude of 2048. This is done by randomly cropping 224 x 224 patches from the original images, flipping them horizontally, and changing the intensity of the RGB channels using PCA color augmentation. This Data Augmentation helped reduce overfitting when training a deep neural network. The authors claim that their aug- mentations reduced the error rate of the model by over 1%.\n\nSince then, GANs were introduced in 2014 , Neural Style Transfer  in 2015, and Neural Architecture Search (NAS)  in 2017", "88c4bc21-dde9-48ff-a503-dbc73281dc31": "The E step again involves, for given cluster prototypes \u00b5k, assigning each data point to the cluster for which the dissimilarity to the corresponding prototype is smallest.\n\nThe computational cost of this is O(KN), as is the case for the standard K-means algorithm. For a general choice of dissimilarity measure, the M step is potentially more complex than for K-means, and so it is common to restrict each cluster prototype to be equal to one of the data vectors assigned to that cluster, as this allows the algorithm to be implemented for any choice of dissimilarity measure V(\u00b7, \u00b7) so long as it can be readily evaluated. Thus the M step involves, for each cluster k, a discrete search over the Nk points assigned to that cluster, which requires O(N 2 k) evaluations of V(\u00b7, \u00b7). One notable feature of the K-means algorithm is that at each iteration, every data point is assigned uniquely to one, and only one, of the clusters. Whereas some data points will be much closer to a particular centre \u00b5k than to any other centre, there may be other data points that lie roughly midway between cluster centres", "90b05b0a-83d5-491c-8da1-e85baee4d4d3": "He provided one unit for each conceptually distinct possibility that seemed likely to be relevant, and he scaled them to roughly the same range, in this case between 0 and 1. Given a representation of a backgammon position, the network computed its estimated value in the standard way. Corresponding to each connection from an input unit to a hidden unit was a real-valued weight. Signals from each input unit were multiplied by their corresponding weights and summed at the hidden unit.\n\nThe output, h(j), of hidden unit j was a nonlinear sigmoid function of the weighted sum: where xi is the value of the ith input unit and wij is the weight of its connection to the jth hidden unit (all the weights in the network together make up the parameter vector w). The output of the sigmoid is always between 0 and 1, and has a natural interpretation as a probability based on a summation of evidence. The computation from hidden units to the output unit was entirely analogous. Each connection from a hidden unit to the output unit had a separate weight", "5f07135b-bc56-4304-a7db-c0432d77fa30": "Equation 10.5 shows that the RNN parametrizes long-term relationships between variables efficiently, using recurrent applications of the same function f and the same parameters @ at each time step. Figure 10.8 illustrates the graphical model interpretation.\n\nIncorporating the h\u201c nodes in the graphical model decouples the past and the future, acting as an intermediate  @  https://www.deeplearningbook.org/contents/rnn.html    quantity, between them. A variable Y in the distant past may influence a variable y-\u2019 via its effect on M. The structure of this graph shows that the model can be efficiently parametrized by using the same conditional probability distributions at each time step, and that when the variables are all observed, the probability of the joint assignment of all variables can be evaluated efficiently. Even with the efficient parametrization of the graphical model, some operations remain computationally challenging. For example, it is difficult to predict missing values in the middle of the sequence. The price recurrent networks pay for their reduced number of parameters is that optimizing the parameters may be difficult. The parameter sharing used in recurrent networks relies on the assumption that the same parameters can be used for different time steps", "82cfe928-2350-4154-85e4-fcd90f89c949": "Pets and Flowers). On the remaining 5 datasets, the models are statistically tied. Full experimental details as well as results with the standard ResNet-50 architecture are provided in Appendix B.8.\n\n11The details of sampling and exact subsets can be found in https://www.tensor\ufb02ow.org/datasets/catalog/imagenet2012_subset. The idea of making representations of an image agree with each other under small transformations dates back to Becker & Hinton . We extend it by leveraging recent advances in data augmentation, network architecture and contrastive loss. A similar consistency idea, but for class label prediction, has been explored in other contexts such as semisupervised learning . Handcrafted pretext tasks. The recent renaissance of selfsupervised learning began with arti\ufb01cially designed pretext tasks, such as relative patch prediction , solving jigsaw puzzles , colorization  and rotation prediction . Although good results can be obtained with bigger networks and longer training , these pretext tasks rely on somewhat ad-hoc heuristics, which limits the generality of learned representations. Contrastive visual representation learning. Dating back to Hadsell et al", "c8e3264b-7899-479a-836a-d4ff79366c3a": "Must you wait until you get home before increasing your estimate for the initial state? According to the Monte Carlo approach you must, because you don\u2019t yet know the true return.\n\nAccording to a TD approach, on the other hand, you would learn immediately, shifting your initial estimate from 30 minutes toward 50. In fact, each estimate would be shifted toward the estimate that immediately follows it. Returning to our \ufb01rst day of driving, Figure 6.1 (right) shows the changes in the predictions recommended by the TD rule (6.2) (these are the changes made by the rule if \u21b5 = 1). Each error is proportional to the change over time of the prediction, that is, to the temporal di\u21b5erences in predictions. Besides giving you something to do while waiting in tra\ufb03c, there are several computational reasons why it is advantageous to learn based on your current predictions rather than waiting until termination when you know the actual return. We brie\ufb02y discuss some of these in the next section. Exercise 6.2 This is an exercise to help develop your intuition about why TD methods are often more e\ufb03cient than Monte Carlo methods. Consider the driving home example and how it is addressed by TD and Monte Carlo methods", "b38c211b-b5ff-4f6e-a954-ed08bf0b2601": "Language model pre-training has been shown to be effective for improving many natural language processing tasks .\n\nThese include sentence-level tasks such as natural language inference  and paraphrasing , which aim to predict the relationships between sentences by analyzing them holistically, as well as token-level tasks such as named entity recognition and question answering, where models are required to produce \ufb01ne-grained output at the token level . There are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and \ufb01ne-tuning. The feature-based approach, such as ELMo , uses task-speci\ufb01c architectures that include the pre-trained representations as additional features. The \ufb01ne-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) , introduces minimal task-speci\ufb01c parameters, and is trained on the downstream tasks by simply \ufb01ne-tuning all pretrained parameters. The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations. We argue that current techniques restrict the power of the pre-trained representations, especially for the \ufb01ne-tuning approaches", "53936941-0a2a-43e0-bfc2-47c5ac91e2dd": "The op.bprop method should always pretend that all its inputs are distinct from each other, even if they are not. For example, if the mul operator is passed two copies of x to compute 2\u201d, the op.bprop method should still return x as the derivative with respect to both inputs. The back-propagation algorithm will later add both of these arguments together to obtain 2x, which is the correct total derivative on x. Software implementations of back-propagation usually provide both the opera- tions and their bprop methods, so that users of deep learning software libraries are able to back-propagate through graphs built using common operations like matrix multiplication, exponents, logarithms, and so on. Software engineers who build a new implementation of back-propagation or advanced users who need to add their own operation to an existing library must usually derive the op.bprop method for any new operations manually. The back-propagation algorithm is formally described in algorithm 6.5. In section 6.5.2, we explained that back-propagation was developed in order to avoid computing the same subexpression in the chain rule multiple times", "f39cef0d-9e13-4752-8238-73485e72cfb8": "Let us suppose that we have a data set comprising Nk points in class Ck with N points in total, so that \ufffd wish to classify a new point x, we draw a sphere centred on x containing precisely K points irrespective of their class. Suppose this sphere has volume V and contains Kk points from class Ck.\n\nThen (2.246) provides an estimate of the density associated with each class Similarly, the unconditional density is given by We can now combine (2.253), (2.254), and (2.255) using Bayes\u2019 theorem to obtain the posterior probability of class membership If we wish to minimize the probability of misclassi\ufb01cation, this is done by assigning the test point x to the class having the largest posterior probability, corresponding to the largest value of Kk/K. Thus to classify a new point, we identify the K nearest points from the training data set and then assign the new point to the class having the largest number of representatives amongst this set. Ties can be broken at random. The particular case of K = 1 is called the nearest-neighbour rule, because a test point is simply assigned to the same class as the nearest point from the training set. These concepts are illustrated in Figure 2.27", "2119a427-c715-4e11-a6f5-3ec73b5fab72": "Most machine learning algorithms have settings called hyperparameters, which must be determined outside the learning algorithm itself; we discuss how to set these using additional data.\n\nMachine learning is essentially a form of applied statistics with increased emphasis on the use of computers to statistically estimate complicated functions and a decreased emphasis on proving confidence intervals around these functions; we therefore present the two central approaches to statistics: frequentist estimators and Bayesian inference. Most machine learning algorithms can be divided into the categories of supervised learning and unsupervised learning; we describe these categories and give some examples of simple learning algorithms from each category. Most deep learning algorithms are based on an optimization algorithm called stochastic gradient  https://www.deeplearningbook.org/contents/ml.html    96  CHAPTER 5. MACHINE LEARNING BASICS  descent. We describe how to combine various algorithm components, such as an optimization algorithm, a cost function, a model, and a dataset, to build a machine learning algorithm. Finally, in section 5.11, we describe some of the factors that have limited the ability of traditional machine learning to generalize. These challenges have motivated the development of deep learning algorithms that overcome these obstacles", "34afe4f4-df03-435e-b428-a04279c638ca": "In general, the entire class of truncated policy iteration algorithms can be thought of as sequences of sweeps, some of which use policy evaluation updates and some of which use value iteration updates. Because the max operation in (4.10) is the only di\u21b5erence between these updates, this just means that the max operation is added to some sweeps of policy evaluation. All of these algorithms converge to an optimal policy for discounted \ufb01nite MDPs. Example 4.3: Gambler\u2019s Problem A gambler has the opportunity to make bets on the outcomes of a sequence of coin \ufb02ips. If the coin comes up heads, he wins as many dollars as he has staked on that \ufb02ip; if it is tails, he loses his stake. The game ends when the gambler wins by reaching his goal of $100, or loses by running out of money", "128399c3-15af-418e-8551-2594ca455c6c": "This CR, being initiated in anticipation of the air pu\u21b5 and appropriately timed, o\u21b5ers better protection than simply initiating closure as a reaction to the irritating US.\n\nThe ability to act in anticipation of important events by learning about predictive relationships among stimuli is so bene\ufb01cial that it is widely present across the animal kingdom. Many interesting properties of classical conditioning have been observed in experiments. Beyond the anticipatory nature of CRs, two widely observed properties \ufb01gured prominently in the development of classical conditioning models: blocking and higher-order conditioning. Blocking occurs when an animal fails to learn a CR when a potential CS is presented along with another CS that had been used previously to condition the animal to produce that CR. For example, in the \ufb01rst stage of a blocking experiment involving rabbit nictitating membrane conditioning, a rabbit is \ufb01rst conditioned with a tone CS and an air pu\u21b5 US to produce the CR of closing its nictitating membrane in anticipation of the air pu\u21b5. The experiment\u2019s second stage consists of additional trials in which a second stimulus, say a light, is added to the tone to form a compound tone/light CS followed by the same air pu\u21b5 US", "d4ec7ec3-0a5e-4e3c-b0b0-dbed4bb8e453": "However, as we shall see, for a \ufb01nite training set we only need to consider the values of the function at the discrete set of input values xn corresponding to the training set and test set data points, and so in practice we can work in a \ufb01nite space. Models equivalent to Gaussian processes have been widely studied in many different \ufb01elds. For instance, in the geostatistics literature Gaussian process regression is known as kriging . Similarly, ARMA (autoregressive moving average) models, Kalman \ufb01lters, and radial basis function networks can all be viewed as forms of Gaussian process models. Reviews of Gaussian processes from a machine learning perspective can be found in MacKay , Williams , and MacKay , and a comparison of Gaussian process models with alternative approaches is given in Rasmussen . See also Rasmussen and Williams  for a recent textbook on Gaussian processes.\n\nIn order to motivate the Gaussian process viewpoint, let us return to the linear regression example and re-derive the predictive distribution by working in terms of distributions over functions y(x, w). This will provide a speci\ufb01c example of a Gaussian process", "797cb300-a563-4444-bbdf-f5246051f543": "Usually these hyperparameters are switches that specify whether or not to use some optional component of the learning algorithm, such as a preprocessing step that normalizes the input  Landen Lee ae-Lden atin date wenn 2d Att die. Lee tate. wt And Anetta. MLA. https://www.deeplearningbook.org/contents/guidelines.html    ICaLULES VY SUVLLACLILY LICL WICall ALU ULVIUIUY VD. LUC StLaALUAaALU UCVIALIVOLL. 1 LeSse hyperparameters can explore only two points on the curve. Other hyperparameters have some minimum or maximum value that prevents them from exploring some part of the curve. For example, the minimum weight decay coefficient is zero.\n\nThis  means that if the model is underfitting when weight decay is zero, we cannot enter the overfitting region by modifying the weight decay coefficient. In other words, some hyperparameters can only subtract capacity. The learning rate is perhaps the most important hyperparameter. If you have time to tune only one hyperparameter, tune the learning rate", "f57882f0-556c-43c4-8838-40e920c0d822": "This provides the basis for the (approximate) invariance of the network outputs to translations and distortions of the input image. Because we will typically need to detect multiple features in order to build an effective model, there will generally be multiple feature maps in the convolutional layer, each having its own set of weight and bias parameters. The outputs of the convolutional units form the inputs to the subsampling layer of the network. For each feature map in the convolutional layer, there is a plane of units in the subsampling layer and each unit takes inputs from a small receptive \ufb01eld in the corresponding feature map of the convolutional layer. These units perform subsampling.\n\nFor instance, each subsampling unit might take inputs from a 2 \u00d7 2 unit region in the corresponding feature map and would compute the average of those inputs, multiplied by an adaptive weight with the addition of an adaptive bias parameter, and then transformed using a sigmoidal nonlinear activation function. The receptive \ufb01elds are chosen to be contiguous and nonoverlapping so that there are half the number of rows and columns in the subsampling layer compared with the convolutional layer", "2ffea710-20d5-4193-8231-84fd72df417e": "For many applications, the resulting model can be signi\ufb01cantly more compact, and hence faster to evaluate, than a support vector machine having the same generalization performance. The price to be paid for this compactness, as with the relevance vector machine, is that the likelihood function, which forms the basis for network training, is no longer a convex function of the model parameters. In practice, however, it is often worth investing substantial computational resources during the training phase in order to obtain a compact model that is fast at processing new data. The term \u2018neural network\u2019 has its origins in attempts to \ufb01nd mathematical representations of information processing in biological systems . Indeed, it has been used very broadly to cover a wide range of different models, many of which have been the subject of exaggerated claims regarding their biological plausibility. From the perspective of practical applications of pattern recognition, however, biological realism would impose entirely unnecessary constraints. Our focus in this chapter is therefore on neural networks as ef\ufb01cient models for statistical pattern recognition.\n\nIn particular, we shall restrict our attention to the speci\ufb01c class of neural networks that have proven to be of greatest practical value, namely the multilayer perceptron", "f6b87b8e-fb28-4486-9187-ba3f43828e77": "Unfortunately, the new criterion does not lead to good likelihood or samples, but, compared to the MCMC approach, it does lead to superior classification performance and ability to reason well about missing inputs. The centering trick for the Boltzmann machine is easiest to describe if we return to the general view of a Boltzmann machine as consisting of a set of units  https://www.deeplearningbook.org/contents/generative_models.html    & with a weight matrix U and biases 6. Recall from equation 20.2 that the energy function is given by + T  E(x) =-x Ux-\u2014b x.\n\n(20.36) Using different sparsity patterns in the weight matrix U, we can implement structures of Boltzmann machines, such as RBMs or DBMs with different numbers of layers. This is accomplished by partitioning \u00ab into visible and hidden units and zeroing out elements of U for units that do not interact. The centered Boltzmann machine introduces a vector ys that is subtracted from all the states:  E\\(@;U,b) = \u2014(@ \u2014 p) 'U(a\u2014 p) \u2014 (a \u2014 p)'b", "e70ec1b5-3306-4ba5-b377-2aff9f2a60a6": "Exploration can be implemented in many ways, ranging from occasionally taking random actions intended to cover the entire space of possible actions, to model-based approaches that compute a choice of action based on its expected reward and the model\u2019s amount of uncertainty about that reward. Many factors determine the extent to which we prefer exploration or exploitation. One of the most prominent factors is the time scale we are interested in. If the agent has only a short amount of time to accrue reward, then we prefer more exploitation.\n\nIf the agent has a long time to accrue reward, then we begin with more exploration so that future actions can be planned more effectively with more knowledge. As time progresses and our learned policy improves, we move toward more exploitation. Supervised learning has no trade-off between exploration and exploitation because the supervision signal always specifies which output is correct for each input. There is no need to try out different outputs to determine if one is better than the model\u2019s current output\u2014we always know that the label is the best output. Another difficulty arising in the context of reinforcement learning, besides the exploration-exploitation trade-off, is the difficulty of evaluating and comparing different policies. Reinforcement learning involves interaction between the learner and the environment", "6f0a4c51-c60e-4121-813a-7e80b719dae1": "In Section 14.2, we discuss ways to apply the committee concept in practice, and we also give some insight into why it can sometimes be an effective procedure.\n\nOne important variant of the committee method, known as boosting, involves training multiple models in sequence in which the error function used to train a particular model depends on the performance of the previous models. This can produce substantial improvements in performance compared to the use of a single model and is discussed in Section 14.3. Instead of averaging the predictions of a set of models, an alternative form of model combination is to select one of the models to make the prediction, in which the choice of model is a function of the input variables. Thus different models become responsible for making predictions in different regions of input space. One widely used framework of this kind is known as a decision tree in which the selection process can be described as a sequence of binary selections corresponding to the traversal of a tree structure and is discussed in Section 14.4. In this case, the individual models are generally chosen to be very simple, and the overall \ufb02exibility of the model arises from the input-dependent selection process. Decision trees can be applied to both classi\ufb01cation and regression problems", "c6fefee7-4811-445f-9524-af9840cdd5a1": "Below, we generalize this analysis to tensor-valued nodes, which  is just a way to group multiple scalar values in the same node and enable more  of uJ) and the vector containing the partial derivatives  https://www.deeplearningbook.org/contents/mlp.html    orithm 6.1 A procedyre that performs the computations mapping \u201d* inputs ne to uBR to an output uy This defines a computational graph Where each node computes numerical value u\u00ae by applying a function f to the set of arguments  A\u00ae that comprises the values of previous nodes ui), j < i, with j \u20ac Pa(u\u00ae). The input to the computational graph is the vector a, and is set into the first n; nodes u) tou). The output of the computational graph is read off the last (output) node ul),  end for  fori =n, +1,...,n do AM & ful) |je Pa(u)} u\u00ae \u2014 fo (A)  end for  return u(\u201d)  206  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  efficient implementations", "95410fdc-5700-48b3-a652-9be2cb9431b6": "Convex optimization is used only as a subroutine of some deep learning algorithms. Ideas from the analysis of convex optimization algorithms can be useful for proving the convergence of deep learning algorithms, but in general, the importance of convex optimization is greatly diminished in the context of deep learning. For more information about convex optimization, see Boyd and Vandenberghe  or Rockafellar . 4.4 Constrained Optimization  Sometimes we wish not only to maximize or minimize a function f(a) over all possible values of x. Instead we may wish to find the maximal or minimal value of f(x) for values of g in some set S. This is known as constrained optimization.\n\nPoints a that lie within the set S are called feasible points in constrained optimization terminology. We often wish to find a solution that is small in some sense. A common approach in such situations is to impose a norm constraint, such as ||a|| <1. One simple approach to constrained optimization is simply to modify gradient descent taking the constraint into account. If we use a small constant step size \u20ac, we can make gradient descent steps, then project the result back into S", "6738656f-3038-42b3-938d-b90121079607": "M., White, A., Precup, D. Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction. In Proceedings of the Tenth International Conference on Autonomous Agents and Multiagent Systems, pp.\n\n761\u2013768, Taipei, Taiwan. Sutton, R. S., Pinette, B. The learning of world models by connectionist networks. In Proceedings of the Seventh Annual Conference of the Cognitive Science Society, pp. 54\u201364. Sutton, R. S., Precup, D., Singh, S. Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Arti\ufb01cial Intelligence, 112(1-2):181\u2013211. Sutton, R. S., Rafols, E., Koop, A. Temporal abstraction in temporal-di\u21b5erence networks. In Advances in neural information processing systems, pp. 1313\u20131320. Sutton, R. S., Singh, S. P., McAllester, D", "c9461e51-f89d-4b7e-b199-2c870d4f60b8": "Although such theorems are reassuring, the key problem is how to \ufb01nd suitable parameter values given a set of training data, and in later sections of this chapter we will show that there exist effective solutions to this problem based on both maximum likelihood and Bayesian approaches. The capability of a two-layer network to model a broad range of functions is illustrated in Figure 5.3. This \ufb01gure also shows how individual hidden units work collaboratively to approximate the \ufb01nal function.\n\nThe role of hidden units in a simple classi\ufb01cation problem is illustrated in Figure 5.4 using the synthetic classi\ufb01cation data set described in Appendix A. One property of feed-forward networks, which will play a role when we consider Bayesian model comparison, is that multiple distinct choices for the weight vector w can all give rise to the same mapping function from inputs to outputs . Consider a two-layer network of the form shown in Figure 5.1 with M hidden units having \u2018tanh\u2019 activation functions and full connectivity in both layers", "1747d837-dba9-4afb-9d5b-adcf729e4197": "Although there is nothing intrinsically approximate about variational methods, they do naturally lend themselves to \ufb01nding approximate solutions.\n\nThis is done by restricting the range of functions over which the optimization is performed, for instance by considering only quadratic functions or by considering functions composed of a linear combination of \ufb01xed basis functions in which only the coef\ufb01cients of the linear combination can vary. In the case of applications to probabilistic inference, the restriction may for example take the form of factorization assumptions . Now let us consider in more detail how the concept of variational optimization can be applied to the inference problem. Suppose we have a fully Bayesian model in which all parameters are given prior distributions. The model may also have latent variables as well as parameters, and we shall denote the set of all latent variables and parameters by Z. Similarly, we denote the set of all observed variables by X. For example, we might have a set of N independent, identically distributed data, for which X = {x1, . , xN} and Z = {z1, . , zN}", "1d2ed051-8319-4f1f-a1f1-fab18e831e77": "A reward signal de\ufb01nes the goal of a reinforcement learning problem.\n\nOn each time step, the environment sends to the reinforcement learning agent a single number called the reward. The agent\u2019s sole objective is to maximize the total reward it receives over the long run. The reward signal thus de\ufb01nes what are the good and bad events for the agent. In a biological system, we might think of rewards as analogous to the experiences of pleasure or pain. They are the immediate and de\ufb01ning features of the problem faced by the agent. The reward signal is the primary basis for altering the policy; if an action selected by the policy is followed by low reward, then the policy may be changed to select some other action in that situation in the future. In general, reward signals may be stochastic functions of the state of the environment and the actions taken. Whereas the reward signal indicates what is good in an immediate sense, a value function speci\ufb01es what is good in the long run. Roughly speaking, the value of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state", "e0908de8-cdc1-421b-9e15-5b03f44d8521": "with their deep visualization method. Hav- ing a human-level understanding of convolutional networks features could greatly help guide the augmentation process. Manipulating the representation power of neural networks is being used in many interesting ways to further the advancement of augmentation techniques. Traditional hand-crafted augmentation techniques such as cropping, flipping, and altering the color space are being extended with the use of GANs, Neural Style Transfer, and meta-learn- ing search algorithms.\n\nImage-to-image translation has many potential uses in Data Augmentation. Neural Style Transfer uses neural layers to translate images into new styles. This technique not only utilizes neural representations to separate \u2018style\u2019 and \u2018content\u2019 from images, but also uses neural transformations to transfer the style of one image into another. Neural Style Transfer is a much more powerful augmentation technique than traditional color space augmentations, but even these methods can be combined together. An interesting characteristic of these augmentation methods is their ability to be com- bined together. For example, the random erasing technique can be stacked on top of any of these augmentation methods. The GAN framework possesses an intrinsic property of recursion which is very interesting", "5512971e-bd02-4bdd-96eb-4bf5c14f84c8": "Unfortunately, this increases the computational costs associated with the model. If your error on the test set is higher than your target error rate, you can now  424  CHAPTER 11. PRACTICAL METHODOLOGY  Nw BR wT AN  Training error  https://www.deeplearningbook.org/contents/guidelines.html    o-? 10+ 10\u00b0  Learning rate (logarithmic scale)  Figure 11.1: Typical relationship between the learning rate and the training error. Notice the sharp rise in error when the learning is above an optimal value. This is for a fixed training time, as a smaller learning rate may sometimes only slow down training by a factor proportional to the learning rate reduction. Generalization error can follow this curve or be complicated by regularization effects arising out of having too large or too small learning rates, since poor optimization can, to some degree, reduce or prevent overfitting, and even points with equivalent training error can have different generalization error. take two kinds of actions. The test error is the sum of the training error and the gap between training and test error", "48bd9823-4c8d-4fed-95d0-9d51453ab2bb": "Optogenetic methods introduce light-sensitive proteins into selected neuron types so that these neurons can be activated or silenced by means of \ufb02ashes of laser light. The \ufb01rst experiment using optogenetic methods to study dopamine neurons showed that optogenetic stimulation producing phasic activation of dopamine neurons in mice was enough to condition the mice to prefer the side of a chamber where they received this stimulation as compared to the chamber\u2019s other side where they received no, or lower-frequency, stimulation . In another example, Steinberg et al.\n\nused optogenetic activation of dopamine neurons to create arti\ufb01cial bursts of dopamine neuron activity in rats at the times when rewarding stimuli were expected but omitted\u2014times when dopamine neuron activity normally pauses. With these pauses replaced by arti\ufb01cial bursts, responding was sustained when it would ordinarily decrease due to lack of reinforcement (in extinction trials), and learning was enabled when it would ordinarily be blocked due to the reward being already predicted (the blocking paradigm; Section 14.2.1)", "e2219e90-e823-466b-b555-6eb1b0782b87": "It would be better to select among the non-greedy actions according to their potential for actually being optimal, taking into account both how close their estimates are to being maximal and the uncertainties in those estimates.\n\nOne e\u21b5ective way of doing this is to select actions according to where ln t denotes the natural logarithm of t (the number that e \u21e1 2.71828 would have to be raised to in order to equal t), Nt(a) denotes the number of times that action a has been selected prior to time t (the denominator in (2.1)), and the number c > 0 controls the degree of exploration. If Nt(a) = 0, then a is considered to be a maximizing action. The idea of this upper con\ufb01dence bound (UCB) action selection is that the square-root term is a measure of the uncertainty or variance in the estimate of a\u2019s value. The quantity being max\u2019ed over is thus a sort of upper bound on the possible true value of action a, with c determining the con\ufb01dence level. Each time a is selected the uncertainty is presumably reduced: Nt(a) increments, and, as it appears in the denominator, the uncertainty term decreases", "0c3b69e4-49a0-4c78-9ccd-72a5c1ecc4e3": "(10.16)  In this example, we see that at time t = 2, the model is trained to maximize the conditional probability of y?) given both the a sequence so far and the previous y value from the training set. Maximum likelihood thus specifies that during training, rather than feeding the model\u2019s own output back into itself, these connections should be fed with the target values specifying what the correct output should be. This is illustrated in figure 10.6. We originally motivated teacher forcing as allowing us to avoid back-propagation through time in models that lack hidden-to-hidden connections.\n\nTeacher forcing may still be applied to models that have hidden-to-hidden connections as long as they have connections from the output at one time step to values computed in the next time step. As soon as the hidden units become a function of earlier ime steps, however, the BPTT algorithm is necessary. Some models may thus be rained with both teacher forcing and BPTT. The disadvantage of strict teacher forcing arises if the network is going to be later used in an closed-loop mode, with the network outputs (or samples from she output distribution) fed back as input", "ba8b1b5e-252b-4b05-bc6c-7af059d84902": "To do this, we start with the lowest-numbered node and draw a sample from the distribution p(x1), which we call \ufffdx1. We then work through each of the nodes in order, so that for node n we draw a sample from the conditional distribution p(xn|pan) in which the parent variables have been set to their sampled values. Note that at each stage, these parent values will always be available because they correspond to lowernumbered nodes that have already been sampled. Techniques for sampling from speci\ufb01c distributions will be discussed in detail in Chapter 11. Once we have sampled from the \ufb01nal variable xK, we will have achieved our objective of obtaining a sample from the joint distribution.\n\nTo obtain a sample from some marginal distribution corresponding to a subset of the variables, we simply take the sampled values for the required nodes and ignore the sampled values for the remaining nodes. For example, to draw a sample from the distribution p(x2, x4), we simply sample from the full joint distribution and then retain the values \ufffdx2, \ufffdx4 and discard the remaining values {\ufffdxj\u0338=2,4}", "9485df4d-69b8-473e-9a5f-cf208c49c3de": "Since the total variation is a norm, then if we have Pr and P\u03b8 two probability distributions over X, where \u03a6(P)(f) := Ex\u223cP is a linear function over Cb(X). The Riesz Representation theorem (, Theorem 10) tells us that \u03a6 is an isometric immersion. This tells us that we can e\ufb00ectively consider Prob(X) with the total variation distance as a subset of Cb(X)\u2217 with the norm distance. Thus, just to accentuate it one more time, the total variation over Prob(X) is exactly the norm distance over Cb(X)\u2217. Let us stop for a second and analyze what all this technicality meant. The main thing to carry is that we introduced a distance \u03b4 over probability distributions. When looked as a distance over a subset of Cb(X)\u2217, this distance gives the norm topology. The norm topology is very strong. Therefore, we can expect that not many functions \u03b8 \ufffd\u2192 P\u03b8 will be continuous when measuring distances between distributions with \u03b4", "b8fc3daf-5bb9-4267-8970-16afb3f8f366": "Quoc Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In International Conference on Machine Learning, pages 1188\u20131196. Hector J Levesque, Ernest Davis, and Leora Morgenstern. 2011. The winograd schema challenge. In Aaai spring symposium: Logical formalizations of commonsense reasoning, volume 46, page 47. Lajanugen Logeswaran and Honglak Lee. 2018. An ef\ufb01cient framework for learning sentence representations. In International Conference on Learning Representations. Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Contextualized word vectors. In NIPS. Oren Melamud, Jacob Goldberger, and Ido Dagan. 2016. context2vec: Learning generic context embedding with bidirectional LSTM. In CoNLL. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013", "6c332fa1-6ef8-48cf-9884-4c2990fa503a": "We typically estimate the generalization error of a machine learning model by measuring its performance on a test set of examples that were collected separately from the training set.\n\nIn our linear regression example, we trained the model by minimizing the  raining error, 1  m(train)  | [xX (train), _ yltrain) | 5, (5.14)  108  https://www.deeplearningbook.org/contents/ml.html    CHAPTER 5. MACHINE LEARNING BASICS  but we actually care about the test error, sey | |X Ces aw \u2014 y (test) | 2  How can we affect performance on the test set when we can observe only the training set? The field of statistical learning theory provides some answers. If the training and the test set are collected arbitrarily, there is indeed little we can do. If we are allowed to make some assumptions about how the training and test set are collected, then we can make some progress. The training and test data are generated by a probability distribution over datasets called the data-generating process. We typically make a set of as- sumptions known collectively as the i.i.d. assumptions", "2504c1b9-9d56-497a-9618-4b40db1deb86": "Prepare plots like Figure 2.2 for an action-value method using sample averages, incrementally computed, and another action-value method using a constant step-size parameter, \u21b5 = 0.1. Use \" = 0.1 and longer runs, say of 10,000 steps. \u21e4 All the methods we have discussed so far are dependent to some extent on the initial action-value estimates, Q1(a). In the language of statistics, these methods are biased by their initial estimates.\n\nFor the sample-average methods, the bias disappears once all actions have been selected at least once, but for methods with constant \u21b5, the bias is permanent, though decreasing over time as given by (2.6). In practice, this kind of bias is usually not a problem and can sometimes be very helpful. The downside is that the initial estimates become, in e\u21b5ect, a set of parameters that must be picked by the user, if only to set them all to zero. The upside is that they provide an easy way to supply some prior knowledge about what level of rewards can be expected. Initial action values can also be used as a simple way to encourage exploration", "d7dc080c-ced1-4c50-9225-079f05bae1b9": "However, a further part of the hypothesis motivating semi-supervised learning  https://www.deeplearningbook.org/contents/representation.html    vila unsupervised representation learning 1s that tor many Al tasks, these two properties coincide: once we are able to obtain the underlying explanations for what we observe, it generally becomes easy to isolate individual attributes from the others. Specifically, if a representation h represents many of the underlying causes of the observed a, and the outputs y are among the most salient causes, then it is easy to predict y from h.  First, let us see how semi-supervised learning can fail because unsupervised learning of p(x) is of no help to learning p(y | x). Consider, for example, the case where p(x) is uniformly distributed and we want to learn f(a) = Ely | a]. Clearly,  539  CHAPTER 15. REPRESENTATION LEARNING  Figure 15.4: Mixture model. Example of a density over x that is a mixture over three components", "df757b0b-50e3-4f4d-a829-9a45659c036f": "DEEP FEEDFORWARD NETWORKS  6.5.4 Back-Propagation Computation in Fully Connected MLP  To clarify the above definition of the back-propagation computation, let us consider the specific graph associated with a fully-connected multi layer MLP. Algorithm 6.3 first shows the forward propagation, which maps parameters to the supervised loss L(y, y) associated with a single (input,target) training example (x,y), with \u00a5 the output of the neural network when z is provided in input. Algorithm 6.4 then shows the corresponding computation to be done for applying the back-propagation algorithm to this graph. Algorithms 6.3 and 6.4 are demonstrations chosen to be simple and straightfor- ward to understand. However, they are specialized to one specific problem. Modern software implementations are based on the generalized form of back- propagation described in section 6.5.6 below, which can accommodate any compu- tational graph by explicitly manipulating a data structure for representing symbolic computation.\n\nhttps://www.deeplearningbook.org/contents/mlp.html    Algorithm 6.3 Forward propagation through a ty ical deep neural network and the computation of the cost function", "54157cac-e442-4e53-b450-40ac986926ba": "Note that if the \u00a2 functions have parameters, then Z is a function of those parameters. It is common in the literature to write Z with its arguments omitted to save space. The normalizing constant Z is known as the partition function, a term borrowed from statistical physics. Since Z is an integral or sum over all possible joint assignments of the state x, it is often intractable to compute. To be able to obtain the normalized probability distribution of an undirected model, the model structure and the definitions of the \u00a2 functions must be conducive to computing Z efficiently. In the context of deep learning, Z is usually intractable. Because of the intractability of computing Z  +A distribution defined by normalizing a product of clique potentials is also called aGibbs distribution. 565  CHAPTER 16.\n\nSTRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    exactly, we must resort to approximations. Such approximate algorithms are the topic of chapter 18", "918d9142-75db-4d3f-9482-802fdf2165ce": "For the two-class problem, we substitute this expression for the class-conditional densities into (4.58) and we see that the posterior class probability is again given by a logistic sigmoid acting on a linear function a(x) which is given by Similarly, for the K-class problem, we substitute the class-conditional density expression into (4.63) to give and so again is a linear function of x. 4.3. Probabilistic Discriminative Models For the two-class classi\ufb01cation problem, we have seen that the posterior probability of class C1 can be written as a logistic sigmoid acting on a linear function of x, for a wide choice of class-conditional distributions p(x|Ck). Similarly, for the multiclass case, the posterior probability of class Ck is given by a softmax transformation of a linear function of x", "7121e3ee-8c32-432d-8261-7f4740075204": "This gives a discrete distribution for which the entropy takes the form i p(xi)\u2206 = 1, which follows from (1.101). We now omit the second term \u2212 ln \u2206 on the right-hand side of (1.102) and then consider the limit \u2206 \u2192 0. The \ufb01rst term on the right-hand side of (1.102) will approach the integral of p(x) ln p(x) in this limit so that where the quantity on the right-hand side is called the differential entropy. We see that the discrete and continuous forms of the entropy differ by a quantity ln \u2206, which diverges in the limit \u2206 \u2192 0. This re\ufb02ects the fact that to specify a continuous variable very precisely requires a large number of bits. For a density de\ufb01ned over multiple continuous variables, denoted collectively by the vector x, the differential entropy is given by In the case of discrete distributions, we saw that the maximum entropy con\ufb01guration corresponded to an equal distribution of probabilities across the possible states of the variable", "e21b368d-e47c-4102-9f7e-fb0e6d312cba": "The hypothesis puts the actor and the value-learning part of the critic respectively in the dorsal and ventral subdivisions of the striatum, the input structure of the basal ganglia. Recall from Section 15.4 that the dorsal striatum is primarily implicated in in\ufb02uencing action selection, and the ventral striatum is thought to be critical for di\u21b5erent aspects of reward processing, including the assignment of a\u21b5ective value to sensations. The cerebral cortex, along with other structures, sends input to the striatum conveying information about stimuli, internal states, and motor activity.\n\nIn this hypothetical actor\u2013critic brain implementation, the ventral striatum sends value information to the VTA and SNpc, where dopamine neurons in these nuclei combine it with information about reward to generate activity corresponding to TD errors (though exactly how dopaminergic neurons calculate these errors is not yet understood). The \u2018TD error \u03b4\u2019 line in Figure 15.5a becomes the line labeled \u2018Dopamine\u2019 in Figure 15.5b, which represents the widely branching axons of dopamine neurons whose cell bodies are in the VTA and SNpc", "24bd2827-b1e8-455f-b046-7a9d162a0b06": ", xn\u22121). (13.56) \u03b1(zn) = p(zn|x1, . , xn)p(x1, . , xn) = We can then turn the recursion equation (13.36) for \u03b1 into one for \ufffd\u03b1 given by Note that at each stage of the forward message passing phase, used to evaluate \ufffd\u03b1(zn), we have to evaluate and store cn, which is easily done because it is the coef\ufb01cient that normalizes the right-hand side of (13.59) to give \ufffd\u03b1(zn). We can similarly de\ufb01ne re-scaled variables \ufffd\u03b2(zn) using which will again remain within machine precision because, from (13.35), the quantities \ufffd\u03b2(zn) are simply the ratio of two conditional probabilities In applying this recursion relation, we make use of the scaling factors cn that were previously computed in the \u03b1 phase", "a944ac0e-435c-4f9e-9cce-1fdf46d0138a": "Deep learning has been applied to many other applications besides the ones described here, and will surely be applied to even more after this writing. It would be impossible to describe anything remotely resembling a comprehensive coverage of such a topic. This survey provides a representative sample of what is possible as of this writing. This concludes part II, which has described modern practices involving deep networks, comprising all the most successful methods. Generally speaking, these methods involve using the gradient of a cost function to find the parameters of a model that approximates some desired function. With enough training data, this approach is extremely powerful. We now turn to part III, in which we step into the  480  CHAPTER 12. APPLICATIONS  territory of research\u2014methods that are designed to work with less training data or to perform a greater variety of tasks, where the challenges are more difficult and not as close to being solved as the situations we have described so far. https://www.deeplearningbook.org/contents/applications.html    481  https://www.deeplearningbook.org/contents/applications.html", "3f28d780-0b73-42c6-a0a9-51f8ac4f868f": "Adding context consisting of features describing individual users and the content to be delivered allows personalizing service. This has been formalized as a contextual bandit problem (or an associative reinforcement learning problem, Section 2.9) with the objective of maximizing the total number of user clicks. Li, Chu, Langford, and Schapire  applied a contextual bandit algorithm to the problem of personalizing the Yahoo! Front Page Today webpage (one of the most visited pages on the internet at the time of their research) by selecting the news story to feature.\n\nTheir objective was to maximize the click-through rate (CTR), which is the ratio of the total number of clicks all users make on a webpage to the total number of visits to the page. Their contextual bandit algorithm improved over a standard non-associative bandit algorithm by 12.5%. Theocharous, Thomas, and Ghavamzadeh  argued that better results are possible by formulating personalized recommendation as a Markov decision problem (MDP) with the objective of maximizing the total number of clicks users make over repeated visits to a website. Policies derived from the contextual bandit formulation are greedy in the sense that they do not take long-term e\u21b5ects of actions into account", "31d0520d-5904-4f73-a794-443df2f526a1": "To choose its moves, TD-Gammon considered each of the 20 or so ways it could play its dice roll and the corresponding positions that would result. The resulting positions are afterstates as discussed in Section 6.8. The network was consulted to estimate each of their values. The move was then selected that would lead to the position with the highest estimated value. Continuing in this way, with TD-Gammon making the moves for both sides, it was possible to easily generate large numbers of backgammon games. Each game was treated as an episode, with the sequence of positions acting as the states, S0, S1, S2, . .. Tesauro applied the nonlinear TD rule (16.1) fully incrementally, that is, after each individual move. The weights of the network were set initially to small random values. The initial evaluations were thus entirely arbitrary. Because the moves were selected on the basis of these evaluations, the initial moves were inevitably poor, and the initial games often lasted hundreds or thousands of moves before one side or the other won, almost by accident. After a few dozen games however, performance improved rapidly", "a70ba85c-1eee-4d0e-9270-b361b40524f8": "thesis, University Singh, S. P. (Ed.) . Special double issue on reinforcement learning, Machine Learning, Singh, S., Barto, A. G., Chentanez, N. Intrinsically motivated reinforcement learning. In Advances in Neural Information Processing Systems 17 , pp. 1281\u20131288. MIT Press, Cambridge, MA. Singh, S. P., Bertsekas, D. Reinforcement learning for dynamic channel allocation in cellular telephone systems.\n\nIn Advances in Neural Information Processing Systems 9 , pp. 974\u2013980. MIT Press, Cambridge, MA. Singh, S. P., Jaakkola, T., Jordan, M. I. Learning without state-estimation in partially observable Markovian decision problems. In Proceedings of the 11th International Conference on Machine Learning , pp. 284\u2013292. Morgan Kaufmann. Singh, S., Jaakkola, T., Littman, M", "7db5c128-7e48-4a10-89a6-936c49d1e618": "Springer-Verlag White, A. Developing a Predictive Approach to Knowledge. Ph.D. thesis, University of White, D. J. A survey of applications of Markov decision processes. Journal of the White, A., White, M. Investigating practical linear temporal di\u21b5erence learning. In Proceedings of the 2016 International Conference on Autonomous Agents and Multiagent Systems, pp. 494\u2013502. Whitt, W. Approximations of dynamic programs I. Mathematics of Operations Research, Whittle, P. Optimization over Time, vol. 1. Wiley, New York. Whittle, P. Optimization over Time, vol. 2. Wiley, New York. Wickens, J., K\u00a8otter, R. Cellular models of reinforcement. In J. C. Houk, J. L. Davis and D. G. Beiser (Eds", "0a0be1ee-4073-4c63-9ccc-892e9947bf90": "The di\u21b5erence is that whereas planning uses simulated experience generated by a model, learning methods use real experience generated by the environment. Of course this di\u21b5erence leads to a number of other di\u21b5erences, for example, in how performance is assessed and in how \ufb02exibly experience can be generated. But the common structure means that many ideas and algorithms can be transferred between planning and learning. In particular, in many cases a learning algorithm can be substituted for the key update step of a planning method.\n\nLearning methods require only experience as input, and in many cases they can be applied to simulated experience just as well as to real experience. The box below shows a simple example of a planning method based on one-step tabular Q-learning and on random samples from a sample model. This method, which we call random-sample one-step tabular Q-planning, converges to the optimal policy for the model under the same conditions that one-step tabular Q-learning converges to the optimal policy for the real environment (each state\u2013action pair must be selected an in\ufb01nite number of times in Step 1, and \u21b5 must decrease appropriately over time)", "74ff37b4-40bd-4a92-9a4b-ab2478a75545": "To compute the new parameter values we maximize with respect to \u03b1 and \u03b2 to give Exercise 9.22 The expectation maximization algorithm, or EM algorithm, is a general technique for \ufb01nding maximum likelihood solutions for probabilistic models having latent variables . Here we give a very general treatment of the EM algorithm and in the process provide a proof that the EM algorithm derived heuristically in Sections 9.2 and 9.3 for Gaussian mixtures does indeed maximize the likelihood function . Our discussion will also form the basis for the derivation of the variational inference framework. Section 10.1 Consider a probabilistic model in which we collectively denote all of the observed variables by X and all of the hidden variables by Z.\n\nThe joint distribution p(X, Z|\u03b8) is governed by a set of parameters denoted \u03b8. Our goal is to maximize the likelihood function that is given by Here we are assuming Z is discrete, although the discussion is identical if Z comprises continuous variables or a combination of discrete and continuous variables, with summation replaced by integration as appropriate. We shall suppose that direct optimization of p(X|\u03b8) is dif\ufb01cult, but that optimization of the complete-data likelihood function p(X, Z|\u03b8) is signi\ufb01cantly easier", "e133fb61-1dea-41a7-b455-7e8810971168": ", T \u2212 1: One does not obtain the same guarantees if a bootstrapping estimate of v\u21e1(St) is used a,s0,r \u21e1(a|St)p(s0, r|St, a) all depend on the current value of the weight vector wt, which implies that they will be biased and that they will not produce a true gradient-descent method.\n\nOne way to look at this is that the key step from (9.4) to (9.5) relies on the target being independent of wt. This step would not be valid if a bootstrapping estimate were used in place of v\u21e1(St). Bootstrapping methods are not in fact instances of true gradient descent . They take into account the e\u21b5ect of changing the weight vector wt on the estimate, but ignore its e\u21b5ect on the target. They include only a part of the gradient and, accordingly, we call them semi-gradient methods. Although semi-gradient (bootstrapping) methods do not converge as robustly as gradient methods, they do converge reliably in important cases such as the linear case discussed in the next section", "fbae4eee-b3c2-4501-9fd1-c2614316a441": "In classical physics, Hamilton\u2019s principal function is an action-value function; Newtonian dynamics are greedy with respect to this function . Action-value functions also played a central role in Denardo\u2019s  theoretical treatment of dynamic programming in terms of contraction mappings. The Bellman optimality equation (for v\u21e4) was popularized by Richard Bellman , who called it the \u201cbasic functional equation.\u201d The counterpart of the Bellman optimality equation for continuous time and state problems is known as the Hamilton\u2013Jacobi\u2013Bellman equation (or often just the Hamilton\u2013Jacobi equation), indicating its roots in classical physics . The term dynamic programming (DP) refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as a Markov decision process (MDP).\n\nClassical DP algorithms are of limited utility in reinforcement learning both because of their assumption of a perfect model and because of their great computational expense, but they are still important theoretically. DP provides an essential foundation for the understanding of the methods presented in the rest of this book. In fact, all of these methods can be viewed as attempts to achieve much the same e\u21b5ect as DP, only with less computation and without assuming a perfect model of the environment", "77a9a9e0-3362-482d-83e4-a15777bec588": "When the recurrent network is trained to perform a task that requires predicting  ()  https://www.deeplearningbook.org/contents/rnn.html    the future trom the past, the network typically learns to use 2 as a kind of lossy summary of the task-relevant aspects of the past sequence of inputs up to t. This summar y is in general necessarily lossy, since it maps an arbitrary length sequence (2, 2@-D a2) a) 2) to a fixed length vector h\u201c. Depending on the training criterion, this summary might selectively keep some aspects of the past sequence with more precision than other aspects. For example, if the RNN is used in statistical language modeling, typically to predict the next word given previous words, storing all the information in the input sequence up to time t may not be necessary; storing only enough information to predict the rest of the sentence is sufficient. The most demanding situation is when we ask h{) to be rich enough to allow one to approximately recover the input sequence, as in autoencoder  370  CHAPTER 10.\n\nSEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  frameworks (chapter 14)", "1a34d714-9f33-4519-8121-67eb04a2d498": "We then normalize to see that this yields a Bernoulli distribution controlled by a sigmoidal transformation of z:  log P(y) = yz, (6.20) P(y) = exp(yz), (6.21) Pty) = __exp(y2) (6.22)  y=o exP(y\u20192),  Pla\\ \u2014 w ((9 \u2014 1\\ +) (BOR)  https://www.deeplearningbook.org/contents/mlp.html    Sa 2 een Get ed Lad wees Probability distributions based on exponentiation and normalization are common throughout the statistical modeling literature. The 2 variable defining such a distribution over binary variables is called a logit. This approach to predicting the probabilities in log space is natural to use with maximum likelihood learning. Because the cost function used with maximum likelihood is \u2014log P(y | x), the log in the cost function undoes the exp of the sigmoid. Without this effect, the saturation of the sigmoid could prevent gradient- based learning from making good progress", "6918128b-6acf-4ae9-ad34-4e717386b703": "(c) Find a function f(x), called a discriminant function, which maps each input x directly onto a class label. For instance, in the case of two-class problems, f(\u00b7) might be binary valued and such that f = 0 represents class C1 and f = 1 represents class C2. In this case, probabilities play no role. Let us consider the relative merits of these three alternatives. Approach (a) is the most demanding because it involves \ufb01nding the joint distribution over both x and Ck. For many applications, x will have high dimensionality, and consequently we may need a large training set in order to be able to determine the class-conditional densities to reasonable accuracy. Note that the class priors p(Ck) can often be estimated simply from the fractions of the training set data points in each of the classes. One advantage of approach (a), however, is that it also allows the marginal density of data p(x) to be determined from (1.83).\n\nThis can be useful for detecting new data points that have low probability under the model and for which the predictions may plot) together with the corresponding posterior probabilities (right plot)", "94f8d589-ffd7-4a00-92a1-91614d59e096": "We call diagrams like that above backup diagrams because they diagram relationships that form the basis of the update or backup operations that are at the heart of reinforcement learning methods. These operations transfer value information back to a state (or a state\u2013action pair) from its successor states (or state\u2013action pairs). We use backup diagrams throughout the book to provide graphical summaries of the algorithms we discuss. (Note that, unlike transition graphs, the state nodes of backup diagrams do not necessarily represent distinct states; for example, a state might be its own successor.)\n\nExample 3.5: Gridworld Figure 3.2 (left) shows a rectangular gridworld representation of a simple \ufb01nite MDP. The cells of the grid correspond to the states of the environment. At each cell, four actions are possible: north, south, east, and west, which deterministically cause the agent to move one cell in the respective direction on the grid. Actions that would take the agent o\u21b5 the grid leave its location unchanged, but also result in a reward of \u22121. Other actions result in a reward of 0, except those that move the agent out of the special states A and B", "1dd2a04c-499c-4790-bf02-0b91f6fdefb0": "Gregory K, Richard Z, Ruslan S. Siamese neural networks for one-shot image recognition. In: ICML Deep Learning workshop; 2015. Adam S, Sergey B, Matthew B, Dean W, Timothy L. One-shot learning with memory-augmented neural networks. arXiv preprint. 2016.  . Tomas M, llya S, Kai C, Greg C, Jeffrey D. Distributed representations of words and phrases and their composition-  ality. Accepted to NIPS 2013. Jeffrey P, Richard S, Christopher DM. GloVe: global vectors for word representation. In: Proceedings of the empirical methods in natural language processing  12. 2014. Halevy A, Norvig P, Pereira F. The unreasonable effectiveness of data. IEEE Intell Syst. 2009;24:8-12. Chen S, Abhinav S, Saurabh S, Abhinav G. Revisting unreasonable effectivness of data in deep learning era. In: ICCV; 2017. p", "c8e1e49d-23d5-452f-9d56-baf47b7b3891": "Street View cars photograph the buildings and record the GPS coordinates associated with each photograph. A convolutional network recognizes the address number in each photograph, allowing the Google Maps database to add that address in the correct location. The story of how this commercial application was developed gives an example of how to follow the design methodology we advocate.\n\nWe now describe each of the steps in this process. 11.1 Performance Metrics  Determining your goals, in terms of which error metric to use, is a necessary first step because your error metric will guide all your future actions. You should also have an idea of what level of performance you desire. Keep in mind that for most applications, it is impossible to achieve absolute zero error. The Bayes error defines the minimum error rate that you can hope to achieve, even if you have infinite training data and can recover the true probability distribution. This is because your input features may not contain complete information about the output variable, or because the system might be intrinsically stochastic. You will also be limited by having a finite amount of training data. The amount of training data can be limited for a variety of reasons", "cde85b87-cfcf-483e-8597-4fc9001120e3": "In instrumental conditioning experiments learning depends on the consequences of behavior: the delivery of a reinforcing stimulus is contingent on what the animal does. In classical conditioning experiments, in contrast, the reinforcing stimulus\u2014the US\u2014is delivered independently of the animal\u2019s behavior. Instrumental conditioning is usually considered to be the same as operant conditioning, the term B. F. Skinner  introduced for experiments with behavior-contingent reinforcement, though the experiments and theories of those who use these two terms di\u21b5er in a number of ways, some of which we touch on below. We will exclusively use the term instrumental conditioning for experiments in which reinforcement is contingent upon behavior. The roots of instrumental conditioning go back to experiments performed by the American psychologist Edward Thorndike one hundred years before publication of the \ufb01rst edition of this book.\n\nboxes with di\u21b5erent escape mechanisms, Thorndike recorded the amounts of time each cat took to escape over multiple experiences in each box. He observed that the time almost invariably decreased with successive experiences, for example, from 300 seconds to 6 or 7 seconds. He described cats\u2019 behavior in a puzzle box like this: The cat that is clawing all over the box in her impulsive struggle will probably claw the string or loop or button so as to open the door", "3a770c27-f6a5-40fd-8f5b-bedfb5f3bc13": "https://www.deeplearningbook.org/contents/generative_models.html    Figure 20.4: The deep Boltzmann machine training procedure used to classify the MNIST dataset . (a)/Train an RBM by using CD to approximately maximize log P(v). (bJTrain a second RBM that models Af!) and target class y by using CD-k to approximately maximize log P(h,y), where hi)  is drawn from the first RBM\u2019s posterior conditioned on the data. Increasek from 1 to  20 during learning. (c)Combine the two RBMs into a DBM. Train it to approximately maximize log P(v, y) using stochastic maximum likelihood with k = 5. (d)Delete y from  the model. Define a new set of features A and h\u00ae) that are obtained by running mean field inference in the model lacking y. Use these features as input to an MLP whose structure is the same as an additional pass of mean field, with an additional output layer for the estimate of y. Initialize the MLP\u2019s weights to be the same as the DBM\u2019s weights", "a542c59d-434e-480a-9f52-3b36dba07bd0": "This product corresponds to p(h, v = \ufffdv) and hence is an unnormalized version of p(h|v = \ufffdv). By running the sum-product algorithm, we can ef\ufb01ciently calculate the posterior marginals p(hi|v = \ufffdv) up to a normalization coef\ufb01cient whose value can be found ef\ufb01ciently using a local computation. Any summations over variables in v then collapse into a single term. We have assumed throughout this section that we are dealing with discrete variables. However, there is nothing speci\ufb01c to discrete variables either in the graphical framework or in the probabilistic construction of the sum-product algorithm. For continuous variables the summations are simply replaced by integrations. We shall give an example of the sum-product algorithm applied to a graph of linear-Gaussian variables when we consider linear dynamical systems. Section 13.3 The sum-product algorithm allows us to take a joint distribution p(x) expressed as a factor graph and ef\ufb01ciently \ufb01nd marginals over the component variables.\n\nTwo other common tasks are to \ufb01nd a setting of the variables that has the largest probability and to \ufb01nd the value of that probability", "c372bafc-7819-4db5-bb4d-218249539df0": "The values of zk therefore satisfy zk \u2208 {0, 1} and \ufffd k zk = 1, and we see that there are K possible states for the vector z according to which element is nonzero. We shall de\ufb01ne the joint distribution p(x, z) in terms of a marginal distribution p(z) and a conditional distribution p(x|z), corresponding to the graphical model in Figure 9.4. The marginal distribution over z is speci\ufb01ed in terms of the mixing coef\ufb01cients \u03c0k, such that where the parameters {\u03c0k} must satisfy in order to be valid probabilities.\n\nBecause z uses a 1-of-K representation, we can also write this distribution in the form Similarly, the conditional distribution of x given a particular value for z is a Gaussian The joint distribution is given by p(z)p(x|z), and the marginal distribution of x is then obtained by summing the joint distribution over all possible states of z to give Exercise 9.3 where we have made use of (9.10) and (9.11). Thus the marginal distribution of x is a Gaussian mixture of the form (9.7). If we have several observations x1,", "87056ee1-0405-4edd-9bbe-0dbef80df5ba": "Recall that the goal of the leapfrog integration in hybrid Monte Carlo is to move a substantial distance through phase space to a new state that is relatively independent of the initial state and still achieve a high probability of acceptance. In order to achieve this, the leapfrog integration must be continued for a number of iterations of order \u03c3max/\u03c3min. By contrast, consider the behaviour of a simple Metropolis algorithm with an isotropic Gaussian proposal distribution of variance s2, considered earlier. In order to avoid high rejection rates, the value of s must be of order \u03c3min. The exploration of state space then proceeds by a random walk and takes of order (\u03c3max/\u03c3min)2 steps to arrive at a roughly independent state. 11.6. Estimating the Partition Function As we have seen, most of the sampling algorithms considered in this chapter require only the functional form of the probability distribution up to a multiplicative constant", "071d0652-2587-4c62-a9c4-16c872b264a9": "When a game was over, beads were added to or removed from the boxes used during play to reward or punish MENACE\u2019s decisions. Michie and Chambers  described another tic-tac-toe reinforcement learner called GLEE (Game Learning Expectimaxing Engine) and a reinforcement learning controller called BOXES. They applied BOXES to the task of learning to balance a pole hinged to a movable cart on the basis of a failure signal occurring only when the pole fell or the cart reached the end of a track. This task was adapted from the earlier work of Widrow and Smith , who used supervised learning methods, assuming instruction from a teacher already able to balance the pole.\n\nMichie and Chambers\u2019s version of pole-balancing is one of the best early examples of a reinforcement learning task under conditions of incomplete knowledge. It in\ufb02uenced much later work in reinforcement learning, beginning with some of our own studies . Michie consistently emphasized the role of trial and error and learning as essential aspects of arti\ufb01cial intelligence", "d98e2fed-f703-43a4-ab39-e468194d3d45": "Rather than limiting the model capacity by keeping the encoder and decoder shallow and the code size small, regularized autoencoders use a loss function that encourages the model to have other properties besides the ability to copy its input to its output.\n\nThese other properties include sparsity of the representation, smallness of the derivative of the representation, and robustness to noise or to missing inputs. A regularized autoencoder can be nonlinear and overcomplete but still learn something useful about the data distribution, even if  https://www.deeplearningbook.org/contents/autoencoders.html    the model capacity is great enough to learn a trivial identity function. In addition to the methods described here, which are most naturally interpreted as regularized autoencoders, nearly any generative model with latent variables  and equipped with an inference procedure (for computing latent representations given input) may be viewed as a particular form of autoencoder. Two generative modeling approaches that emphasize this connection with autoencoders are the descendants of the Helmholtz machine , such as the variational  501  CHAPTER 14. AUTOENCODERS  autoencoder (section 20.10.3) and the generative stochastic networks (section 20.12)", "2071c683-fc62-4c65-a7a4-2f6c7220be8c": "196  CHAPTER 6.\n\nDEEP FEEDFORWARD NETWORKS  The main theorem in Montufar et al. states that the number of linear regions carved out by a deep rectifier network with d inputs, depth /, and n units  per hidden layer is np =D O @ \u201c) , (6.42)  that is, exponential in depth /. In the case of maxout networks with k filters per unit, the number of linear regions is  O K+ (6.43)  Co)  https://www.deeplearningbook.org/contents/mlp.html    Of course, there is no guarantee that the kinds of functions we want to learn in applications of machine learning (and in particular for AI) share such a property. We may also want to choose a deep model for statistical reasons. Any time we choose a specific machine learning algorithm, we are implicitly stating some set of prior beliefs we have about what kind of function the algorithm should learn. Choosing a deep model encodes a very general belief that the function we want to learn should involve composition of several simpler functions", "669e523d-ba89-4ef6-8652-d5f96ea8d421": "MONTE CARLO METHODS  provided that the variance of the individual terms, Var, is bounded. To see this more clearly, consider the variance of 8, as n increases. The variance Var decreases and converges to 0, so long as Var < oo:  Var = = S> Varlf(x)] (17.6) i=l _ Var ; (17.7)  This convenient result also tells us how to estimate the uncertainty in a Monte Carlo average or equivalently the amount of expected error of the Monte Carlo approximation. We compute both the empirical average of the f(a) and their empirical variance,! and then divide the estimated variance by the number of samples n to obtain an estimator of Var. The central limit theorem tells us that the distribution of the average, \u00a7,, converges to a normal distribution with mean s and variance Var[ FO) This allows us to estimate confidence intervals around the estimate \u00a7,, using the cumulative distribution of the normal density. All this relies on our ability to easily sample from the base distribution p(x), but doing so is not always possible", "184718b7-e64e-4cca-b7c2-02bb0757c88b": "CONVOLUTIONAL NETWORKS  OOOO O (\\ TN NN  https://www.deeplearningbook.org/contents/convnets.html    Channel coordinates  Spatial coordinates  Figure 9.15: A convolutional network with the first two output channels connected to only the first two input channels, and the second two output channels connected to only the second two input channels. 348  CHAPTER 9. CONVOLUTIONAL NETWORKS  QRAQAY  https://www.deeplearningbook.org/contents/convnets.html     Figure 9.16: A comparison of locally connected layers, tiled convolution, and standard convolution. All three have the same sets of connections between units, when the same size of kernel is used. This diagram illustrates the use of a kernel that is two pixels wide. The differences between the methods lies in how they share parameters. (Top) A locally connected layer has no sharing at all. We indicate that each connection has its own weight by labeling each connection with a unique letter. (Center) Tiled convolution has a set of \u00a2 different kernels", "90910081-e7bd-416c-92eb-07b8b89e58d4": "1, we would have two candidates with true labels y1 = True and y2 = False: Data Model A design challenge is managing complex, unstructured data in a way that enables SMEs to write labeling functions over it. In Snorkel, input data is stored in a context hierarchy. It is made up of context types connected by parent/child relationships, which are stored in a relational database and made available via an object-relational mapping (ORM) layer built with SQLAlchemy.2 Each context type represents a conceptual component of data to be processed by the system or used when writing labeling functions; for example a document, an image, a paragraph, a sentence, or an embedded table. Candidates\u2014i.e., data points x\u2014are then de\ufb01ned as tuples of contexts (Fig. 4). Example 2.2 In our running CDR example, the input documents can be represented in Snorkel as a hierarchy consisting of Documents, each containing one or more Sentences, each containing one or more Spans of text. These Spans may also be tagged with metadata, such as Entity markers identifying them as chemical or disease mentions (Fig. 4)", "181dc680-45d4-4375-9b8c-467571c43112": "So far, the most striking successes in deep learning have involved discriminative models, usually those that map a high-dimensional, rich sensory input to a class label .\n\nThese striking successes have primarily been based on the backpropagation and dropout algorithms, using piecewise linear units  which have a particularly well-behaved gradient . Deep generative models have had less of an impact, due to the dif\ufb01culty of approximating many intractable probabilistic computations that arise in maximum likelihood estimation and related strategies, and due to dif\ufb01culty of leveraging the bene\ufb01ts of piecewise linear units in the generative context. We propose a new generative model estimation procedure that sidesteps these dif\ufb01culties. 1 In the proposed adversarial nets framework, the generative model is pitted against an adversary: a discriminative model that learns to determine whether a sample is from the model distribution or the data distribution. The generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistiguishable from the genuine articles", "59437b46-1b9a-4b41-a7dd-fd3cc36b3166": "An example of obtaining Markov states through a state-update function is provided by the popular Bayesian approach known as Partially Observable MDPs, or POMDPs. In this approach the environment is assumed to have a well de\ufb01ned latent state Xt that underlies and produces the environment\u2019s observations, but is never available to the agent (and is not to be confused with the state St used by the agent to make predictions and decisions). The natural Markov state, St, for a POMDP is the distribution over the latent states given the history, called the belief state.\n\nFor concreteness, assume the usual case in which there are a \ufb01nite number of hidden states, Xt 2 {1, 2, . , d}. Then the belief state is the vector St .= st 2 Rd with components st .= Pr{Xt =i | Ht}, for all possible latent states i 2 {1, 2, . , d}. The belief state remains the same size (same number of components) however t grows. It can also be incrementally updated by Bayes\u2019 rule, assuming one has complete knowledge of the internal workings of the environment", "ac5ad3b9-ae2f-4918-ba75-5b6b278e2d85": "Anachronistically, this process of \u2018marrying the parents\u2019 has become known as moralization, and the resulting undirected graph, after dropping the arrows, is called the moral graph. It is important to observe that the moral graph in this example is fully connected and so exhibits no conditional independence properties, in contrast to the original directed graph.\n\nThus in general to convert a directed graph into an undirected graph, we \ufb01rst add additional undirected links between all pairs of parents for each node in the graph and then drop the arrows on the original links to give the moral graph. Then we initialize all of the clique potentials of the moral graph to 1. We then take each conditional distribution factor in the original directed graph and multiply it into one of the clique potentials. There will always exist at least one maximal clique that contains all of the variables in the factor as a result of the moralization step. Note that in all cases the partition function is given by Z = 1. The process of converting a directed graph into an undirected graph plays an important role in exact inference techniques such as the junction tree algorithm. Section 8.4 Converting from an undirected to a directed representation is much less common and in general presents problems due to the normalization constraints", "c0d36060-8ffb-4295-b7cb-ad603315f8bd": "The  https://lilianweng.github.io/posts/2021-05-31-contrastive/  Contrastive Representation Learning | Lil'Log   coefficient \u20ac should not be too large; otherwise, all the samples will be assigned uniformly to all the clusters. The candidate set of solutions for Q requires every mapping matrix to have each row sum up to 1/K and each column to sum up to 1/B, enforcing that each prototype gets selected at least B/K times on average. SwAV relies on the iterative Sinkhorn-Knopp algorithm  to find the solution for Q", "8b9e0eb4-c07c-4f14-9991-913fbf980333": "We shall sometimes \ufb01nd it helpful to make the parameters of a model, as well as its stochastic variables, explicit. In this case, (8.6) becomes Correspondingly, we can make x and \u03b1 explicit in the graphical representation. To do this, we shall adopt the convention that random variables will be denoted by open circles, and deterministic parameters will be denoted by smaller solid circles. If we take the graph of Figure 8.4 and include the deterministic parameters, we obtain the graph shown in Figure 8.5. When we apply a graphical model to a problem in machine learning or pattern recognition, we will typically set some of the random variables to speci\ufb01c observed values, for example the variables {tn} from the training set in the case of polynomial curve \ufb01tting. In a graphical model, we will denote such observed variables by shading the corresponding nodes. Thus the graph corresponding to Figure 8.5 in which the variables {tn} are observed is shown in Figure 8.6", "8e654bad-34a8-43ee-8764-7bcf35aaed3e": "Unlike conjugate gradients, however, the success of the approach is not heavily dependent on the line search finding a point very close to the true minimum along the line. Thus, relative to conjugate gradients, BFGS has the advantage that it can spend less time refining each line search.\n\nOn the other hand, the BFGS algorithm must store the inverse Hessian matrix, M, that requires O(n?) memory, making BFGS impractical for most modern deep learning models that typically have millions of parameters. Limited Memory BFGS (or L-BFGS) The memory costs of the BFGS algorithm can be significantly decreased by avoiding storing the complete inverse Hessian approximation M. The L-BFGS algorithm computes the approximation M using the same method as the BFGS algorithm but beginning with the assumption that M-) is the identity matrix, rather than storing the approximation from one step to the next. If used with exact line searches, the directions defined by L-BFGS are mutually conjugate. However, unlike the method of conjugate gradients, this procedure remains well behaved when the minimum of the line search is reached only approximately", "ec131243-492d-405c-8513-61a99b938dc7": "Shervin Minaee, Nal Kalchbrenner, Erik Cambria, Narjes Nikzad, Meysam Chenaghlu, and Jianfeng Gao. 2021. Deep learning based text classi\ufb01cation: A comprehensive review. Takeru Miyato, Andrew M. Dai, and Ian Goodfellow. 2017. Adversarial training methods for semisupervised text classi\ufb01cation. International Conference on Learning Representations (ICLR). Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. 2018. Virtual adversarial training: a regularization method for supervised and semisupervised learning. IEEE transactions on pattern analysis and machine intelligence, 41(8):1979\u2013 1993. John Morris, Eli Li\ufb02and, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. 2020. TextAttack: A framework for adversarial attacks, data augmentation, and adversarial training in NLP", "522bfb21-eec0-41e6-96da-28944f897c53": "The advantage of using a probit function is that its convolution with a Gaussian can be expressed analytically in terms of another probit function. Speci\ufb01cally we can show that Exercise 4.26 where \u00b5a and \u03c32 a are de\ufb01ned by (4.149) and (4.150), respectively, and \u03ba(\u03c32 a) is de\ufb01ned by (4.154). Note that the decision boundary corresponding to p(C1|\u03c6, t) = 0.5 is given by \u00b5a = 0, which is the same as the decision boundary obtained by using the MAP value for w. Thus if the decision criterion is based on minimizing misclassi\ufb01cation rate, with equal prior probabilities, then the marginalization over w has no effect. However, for more complex decision criteria it will play an important role. Marginalization of the logistic sigmoid model under a Gaussian approximation to the posterior distribution will be illustrated in the context of variational inference in Figure 10.13. n \u03b1n = 1", "e31dedd3-88b4-42ad-b17b-c831c7fdb9b7": "We do not try to describe\u2014or even to name\u2014the very many brain structures and pathways, or any of the molecular mechanisms, believed to be involved in these processes. We also do not do justice to hypotheses and models that are alternatives to those that align so well with reinforcement learning. It should not be surprising that there are di\u21b5ering views among experts in the \ufb01eld. We can only provide a glimpse into this fascinating and developing story. We hope, though, that this chapter convinces you that a very fruitful channel has emerged connecting reinforcement learning and its theoretical underpinnings to the neuroscience of reward-based learning in animals. Many excellent publications cover links between reinforcement learning and neuroscience, some of which we cite in this chapter\u2019s \ufb01nal section. Our treatment di\u21b5ers from most of these because we assume familiarity with reinforcement learning as presented in the earlier chapters of this book, but we do not assume knowledge of neuroscience.\n\nWe begin with a brief introduction to the neuroscience concepts needed for a basic understanding of what is to follow. Some basic information about nervous systems is helpful for following what we cover in this chapter. Terms that we refer to later are italicized. Skipping this section will not be a problem if you already have an elementary knowledge of neuroscience", "0fa3b6df-11f5-4ca6-ae43-f39f290ca50a": "All states in MDP has \u201cMarkov\u201d property, referring to the fact that the future only depends on the current state, not the history:  P = PlSt41|S1,---, 54]  Or in other words, the future and the past are conditionally independent given the present, as the current state encapsulates all the statistics we need to decide the future.\n\nhttps://lilianweng.github.io/posts/2018-02-19-rl-overview/  A (Long) Peek into Reinforcement Learning | Lil'Log   A Markov deicison process consists of five elements M = (S,.A, P, R, y), where the symbols carry the same meanings as key concepts in the previous section, well aligned with RL problem settings:  S - aset of states; A - aset of actions; P - transition probability function;  R - reward function;  7 - discounting factor for future rewards. In an unknown environment, we do not have perfect knowledge about P and R", "d27fc914-e2af-40f4-869e-0f93ee1c1b93": "Example: Polynomial Curve Fitting We begin by introducing a simple regression problem, which we shall use as a running example throughout this chapter to motivate a number of key concepts. Suppose we observe a real-valued input variable x and we wish to use this observation to predict the value of a real-valued target variable t. For the present purposes, it is instructive to consider an arti\ufb01cial example using synthetically generated data because we then know the precise process that generated the data for comparison against any learned model. The data for this example is generated from the function sin(2\u03c0x) with random noise included in the target values, as described in detail in Appendix A.\n\nNow suppose that we are given a training set comprising N observations of x, written x \u2261 (x1, . , xN)T, together with corresponding observations of the values of t, denoted t \u2261 (t1, . , tN)T. Figure 1.2 shows a plot of a training set comprising N = 10 data points. The input data set x in Figure 1.2 was generated by choosing values of xn, for n = 1,", "6f9ee155-3fad-4576-a53e-b4abb27dbebf": "For large vocabularies, it can be very computationally expensive to represent an output distribution over the choice of a word, because the vocabulary size is large. In many applications, V contains hundreds of thousands of words. The naive approach to representing such a distribution is to apply an affine transformation from a hidden representation to the output space, then apply the softmax function.\n\nSuppose we have a vocabulary V with size |V|. The weight matrix describing the linear component of this affine transformation is very large, because its output dimension is |V|. This imposes a high memory cost to represent the matrix, and a high computational cost to multiply by it. Because the softmax is normalized across all |V| outputs, it is necessary to perform the full matrix multiplication at training time as well as test time\u2014we cannot calculate only the dot product with the weight vector for the correct output. The high computational costs of the output layer thus arise both at training time (to compute the likelihood and its gradient) and at test time (to compute probabilities for all or selected words). For specialized loss functions, the gradient can be computed efficiently , but the standard cross-entropy loss applied to a traditional softmax output layer poses many difficulties", "fe2579c7-ec82-4bdb-8879-2c79c4f3519d": "Show that if we assume negligible overlap between the components of the q mixture, the resulting lower bound differs from that for a single component q distribution through the addition of an extra term ln K!. 10.23 (\u22c6 \u22c6) www Consider a variational Gaussian mixture model in which there is no prior distribution over mixing coef\ufb01cients {\u03c0k}.\n\nInstead, the mixing coef\ufb01cients are treated as parameters, whose values are to be found by maximizing the variational lower bound on the log marginal likelihood. Show that maximizing this lower bound with respect to the mixing coef\ufb01cients, using a Lagrange multiplier to enforce the constraint that the mixing coef\ufb01cients sum to one, leads to the re-estimation result (10.83). Note that there is no need to consider all of the terms in the lower bound but only the dependence of the bound on the {\u03c0k}. 10.24 (\u22c6 \u22c6) www We have seen in Section 10.2 that the singularities arising in the maximum likelihood treatment of Gaussian mixture models do not arise in a Bayesian treatment. Discuss whether such singularities would arise if the Bayesian model were solved using maximum posterior (MAP) estimation", "0dae0ea0-a537-495d-8104-ae7426e4a84a": "Thus, the regularized objective function J(w; X,y) is given by  I(w;X,y) = al|w]|  sada a a a4 u 1 wa  https://www.deeplearningbook.org/contents/regularization.html    WILL LIE COLTESPULLGINY Braden (actually, SUD Sad leit ) Vu l(w;X,y) = asign(w) + Ve s(X,y;w), (7.20) where sign(w) is simply the sign of w applied element-wise.\n\nBy inspecting equation 7.20, we can see immediately that the effect of L! regularization is quite different from that of I? regularization. Specifically, we can see that the regularization contribution to the gradient no longer scales linearly with each w;; instead it is a constant factor with a sign equal to sign(w;). One consequence of this form of the gradient is that we will not necessarily see clean algebraic solutions to quadratic approximations of J(X,y;w) as we did for L? regularization. Our simple linear model has a quadratic cost function that we can represent via its Taylor series", "36a12ada-38aa-4026-8a32-23177849f9c3": "Pairs that are in the set are said to have the relation while those not in the set do not. For example, we can define the relation \u201cis less than\u201d on the set of entities {1, 2,3} by defining the set of ordered pairs S = { (1,2), (1,3) ,(2,3)}. Once this relation is defined, we can use it like a verb. Because (1, 2) \u20ac S, we say that 1 is less than 2. Because (2,1) \u00a2 S, we cannot say that 2 is less than 1. Of course, the entities that are related to one another need not be numbers. We could define a relation is_a_type_of containing tuples like (dog, mammal). In the context of AI, we think of a relation as a sentence in a syntactically simple and highly structured language.\n\nThe relation plays the role of a verb, while two arguments to the relation play the role of its subject and object. These sentences take the form of a triplet of tokens  (subject, verb, object) (12.21)  with values (entity ;, relation,, entity, )", "361a8ff2-2fd9-4865-8b85-0b57f406dbbc": "do  4: for each worker i = 1,...,n do 5 Sample e; ~ N\u2019(0, J) 6: Compute returns F; = F(6; + c\u20ac;) 7: end for 8 Send all scalar returns F; from each worker to every other worker 9: for each worker i = 1,...,n do 10: Reconstruct all perturbations \u20ac; for 7 = 1,..., using known random seeds 11: Set 4445 \u2014 & tat Yj=1 Fye; 12: end for 13: end for  ES, as a black-box optimization algorithm, is another approach to RL problems ( Seita discussion ). It has a couple of good characteristics  keeping it fast and easy to train:  ES does not need value function approximation; ES does not perform gradient back-propagation; ES is invariant to delayed or long-term rewards;  ES is highly parallelizable with very little data communication", "1136e283-0ce9-49aa-8dc4-52b0937dae02": "Expectation propagation makes a much better approximation by optimizing each factor in turn in the context of all of the remaining factors. It starts by initializing the factors \ufffdfi(\u03b8), and then cycles through the factors re\ufb01ning them one at a time. This is similar in spirit to the update of factors in the variational Bayes framework considered earlier. Suppose we wish to re\ufb01ne factor \ufffdfj(\u03b8). We \ufb01rst remove this factor from the product to give \ufffd i\u0338=j \ufffdfi(\u03b8). Conceptually, we will now determine a revised form of the factor \ufffdfj(\u03b8) by ensuring that the product in which we keep \ufb01xed all of the factors \ufffdfi(\u03b8) for i \u0338= j. This ensures that the approximation is most accurate in the regions of high posterior probability as de\ufb01ned by the remaining factors. We shall see an example of this effect when we apply EP to the \u2018clutter problem\u2019. To achieve this, we \ufb01rst remove the factor \ufffdfj(\u03b8) from the Section 10.7.1 current approximation to the posterior by de\ufb01ning the unnormalized distribution example considered earlier in Figures 4.14 and 10.1", "9815f236-9f63-4109-a8ab-8710824b1d7e": "This di\u21b5erence seems more fundamental than the di\u21b5erence in the objective functions emphasized by the usual terms. Often episodic tasks use an inde\ufb01nite-horizon objective function and continuing tasks an in\ufb01nite-horizon objective function, but we see this as a common coincidence rather than a fundamental di\u21b5erence. long-term consequences of control decisions is a key part of optimal control theory, which was developed in the 1950s by extending nineteenth century state-function theories of classical mechanics . In describing how a computer could be programmed to play chess, Shannon  suggested using an evaluation function that took into account the long-term advantages and disadvantages of chess positions.\n\nWatkins\u2019s  Q-learning algorithm for estimating q\u21e4 (Chapter 6) made actionvalue functions an important part of reinforcement learning, and consequently these functions are often called \u201cQ-functions.\u201d But the idea of an action-value function is much older than this. Shannon  suggested that a function h(P, M) could be used by a chess-playing program to decide whether a move M in position P is worth exploring. Michie\u2019s  MENACE system and Michie and Chambers\u2019s  BOXES system can be understood as estimating action-value functions", "bd6ea97e-e9d9-4ec3-b5fe-3a8a253de416": "CONFRONTING THE PARTITION FUNCTION  where T,, is the reverse of the transition operator defined by T, (via an application of Bayes\u2019 rule):  https://www.deeplearningbook.org/contents/partition.html    Ta(at!\n\n| a) = PAB Tala | w') = PE) Tala | a\u2019), (18.56)  Plugging the above into the expression for the joint distribution on the extended state space given in equation 18.55, we get:  P(r 5+ ++; Lyn \u20141, \u00a31) (18.57)  ~ pale mrp (as | ayy ,) PJ ese Ty (ger | a)  Pry (21) et mt i-1 Pn (@ni41) mum \" (18.58) pi(x1) Ty Pris (\u00aena) 1 1 ~ M+1 M+. re Tin (21 | \u00aepp_\u20141) B (x ~ Ty(2n; | Lp, )", "fb5caeeb-38f6-42fc-b99d-0aeb9b4fde42": "On the other hand, each time an action other than a is selected, t increases but Nt(a) does not; because t appears in the numerator, the uncertainty estimate increases. The use of the natural logarithm means that the increases get smaller over time, but are unbounded; all actions will eventually be selected, but actions with lower value estimates, or that have already been selected frequently, will be selected with decreasing frequency over time. Results with UCB on the 10-armed testbed are shown in Figure 2.4.\n\nUCB often performs well, as shown here, but is more di\ufb03cult than \"-greedy to extend beyond bandits to the more general reinforcement learning settings considered in the rest of this book. One di\ufb03culty is in dealing with nonstationary problems; methods more complex than those presented in Section 2.5 would be needed. Another di\ufb03culty is dealing with large state spaces, particularly when using function approximation as developed in Part II of this book. In these more advanced settings the idea of UCB action selection is usually not practical. in performance on the 11th step. Why is this? Note that for your answer to be fully satisfactory it must explain both why the reward increases on the 11th step and why it decreases on the subsequent steps", "b0cb9867-d7e0-4a53-92c8-42a8fdce31b7": "Springenberg, Y. Tassa, R. Munos, N. Heess, and M. Riedmiller. Maximum a posteriori policy optimisation. In ICLR, 2018. J. Andreas, M. Rohrbach, T. Darrell, and D. Klein. Learning to compose neural networks for question answering. arXiv preprint arXiv:1601.01705, 2016. S. Baluja and I. Fischer. Adversarial transformation networks: Learning to generate adversarial examples.\n\narXiv preprint arXiv:1703.09387, 2017. H.-S. Chang, E. Learned-Miller, and A. McCallum. Active bias: Training more accurate neural networks by emphasizing high variance samples. In NeurIPS, pages 1002\u20131012, 2017. E. D. Cubuk, B. Zoph, D. Mane, V", "26451dff-00c5-410f-954b-88f1109f8647": "The loss L internally computes \u00a5 = softmax(o) and compares this to the target y.\n\nThe RNN has input to hidden connections parametrized by a weight matrix U, hidden-to-hidden recurrent connections parametrized by a weight matrix W, and hidden-to-output connections parametrized by a weight matrix V. Equation 10.8 defines forward propagation in this model. (Left) The RNN and its loss drawn with recurrent connections. (Right) The same seen as a time-unfolded computational graph, where each node is now associated with one particular time instance. 373  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  a specification of the function to be computed, so the same network that simulates this Turing machine is sufficient for all problems. The theoretical RNN used for the proof can simulate an unbounded stack by representing its activations and weights with rational numbers of unbounded precision. We now develop the forward propagation equations for the RNN depicted in figure 10.3. The figure does not specify the choice of activation function for the hidden units", "cbec0b08-e65f-4c8d-a20f-b4368ccd493a": "Suppose that a influences the value of b, and b influences the value of c, but that a and c are independent given b.\n\nWe can represent the probability distribution over all three variables as a product of probability distributions over two variables:  p(a, b,c) = p(a)p(b | a)p(c | b). (3.52)  These factorizations can greatly reduce the number of parameters needed to describe the distribution. Each factor uses a number of parameters that is exponential in the number of variables in the factor. This means that we can greatly reduce the cost of representing a distribution if we are able to find a factorization into distributions over fewer variables. We can describe these kinds of factorizations using graphs. Here, we use the word \u201cgraph\u201d in the sense of graph theory: a set of vertices that may be connected to each other with edges. When we represent the factorization of a probability distribution with a graph, we call it a structured probabilistic model, or graphical model. There are two main kinds of structured probabilistic models: directed and undirected", "be86982d-47d9-4f87-9dba-9718e9441af0": "As a case study, we show two parameterizations of R\u03c6 which instantiate distinct data manipulation schemes. The \ufb01rst example learns augmentation for text data, a domain that has been less studied in the literature compared to vision and speech . The second instance focuses on automated data weighting, which is applicable to any data domains.\n\nFine-tuning Text Augmentation The recent work  developed a novel contextual augmentation approach for text data, in which a powerful pretrained language model (LM), such as BERT , is used to generate substitutions of words in a sentence. Speci\ufb01cally, given an observed sentence x\u2217, the method \ufb01rst randomly masks out a few words. The masked sentence is then fed to BERT which \ufb01lls the masked positions with new words. To preserve the original sentence class, the BERT LM is retro\ufb01tted as a labelconditional model, and trained on the task training examples. The resulting model is then \ufb01xed and used to augment data during the training of target model. We denote the augmentation distribution as g\u03c60(x|x\u2217, y\u2217), where \u03c60 is the \ufb01xed BERT LM parameters. The above process has two drawbacks", "b7169a1f-5f90-48cd-9f40-9959ef6977fa": "After having observed fa), ee al\u2122) }, if we are still quite uncertain about the value of 0, then this uncertainty is incorporated directly into any predictions we might make. In section 5.4, we discussed how the frequentist approach addresses the uncer- tainty in a given point estimate of @ by evaluating its variance. The variance of the estimator is an assessment of how the estimate might change with alternative samplings of the observed data. The Bayesian answer to the question of how to deal with the uncertainty in the estimator is to simply integrate over it, which tends to protect well against overfitting. This integral is of course just an application of the laws of probability, making the Bayesian approach simple to justify, while the frequentist machinery for constructing an estimator is based on the rather ad hoc decision to summarize all knowledge contained in the dataset with a single point estimate. The second important difference between the Bayesian approach to estimation and the maximum likelihood approach is due to the contribution of the Bayesian prior distribution. The prior has an influence by shifting probability mass density towards regions of the parameter space that are preferred a priori.\n\nIn practice, the prior often expresses a preference for models that are simpler or more smooth", "84616495-1cea-455a-b881-85b648f23741": "Tadepalli, P., Ok, D. Scaling up average reward reinforcement learning by approximating the domain models and the value function.\n\nIn Proceedings of the 13th International Conference on Machine Learning , pp. 471\u2013479. Takahashi, Y., Schoenbaum, G., and Niv, Y. Silencing the critics: Understanding the e\u21b5ects of cocaine sensitization on dorsolateral and ventral striatum in the context of an actor/critic model. Frontiers in Neuroscience, 2(1):86\u201399. Tambe, M., Newell, A., Rosenbloom, P. S. The problem of expensive chunks and its solution by restricting expressiveness. Machine Learning, 5(3):299\u2013348. Tan, M. Learning a cost-sensitive internal representation for reinforcement learning. In L. A. Birnbaum and G. C. Collins (Eds. ), Proceedings of the 8th International Workshop on Machine Learning, pp. 358\u2013362", "4d5b6b4c-3cbb-46d3-aac9-e5776a854d7c": "OPTIMIZATION FOR TRAINING DEEP MODELS  Algorithm 8.2 Stochastic gradient descent (SGD) with momentum  Require: Learning rate \u00ab, momentum parameter a  https://www.deeplearningbook.org/contents/optimization.html       Require: Initial parameter @, initial velocity v while stopping criterion not met do _ (1) (m) Sample a minibatch of \u2122 examples from the training set {a,...,@\u00b0} with  corresponding targets y @,  Compute gradient estimate: g < 1Vo ; L(f (a; 6), y). Compute velocity update: v \u00ab+ av \u2014 eg. Apply update: 0+ 6+ v.  end while  The velocity v accumulates the gradient elements Vg (4 yaa L(f (a; 0), y )). The larger a is relative to \u20ac, the more previous gradients affect the current direction. The SGD algorithm with momentum is given in algorithm 8.2", "b14dbe76-cf59-450b-8f1f-a8bed518600a": "Again, this is a constrained maximization, and to \ufb01nd the constraints we note that an \u2a7e 0 and \ufffdan \u2a7e 0 are both required because these are Lagrange multipliers. Also \u00b5n \u2a7e 0 and \ufffd\u00b5n \u2a7e 0 together with (7.59) and (7.60), require an \u2a7d C and \ufffdan \u2a7d C, and so again we have the box constraints together with the condition (7.58).\n\nSubstituting (7.57) into (7.1), we see that predictions for new inputs can be made using which is again expressed in terms of the kernel function. The corresponding Karush-Kuhn-Tucker (KKT) conditions, which state that at the solution the product of the dual variables and the constraints must vanish, are given by From these we can obtain several useful results. First of all, we note that a coef\ufb01cient an can only be nonzero if \u03f5 + \u03ben + yn \u2212 tn = 0, which implies that the data point either lies on the upper boundary of the \u03f5-tube (\u03ben = 0) or lies above the upper boundary (\u03ben > 0)", "9f1896f3-34a6-4887-a61f-891ee121d021": "MIT Press. Jordan, M. I. and R. A. Jacobs . Hierarchical mixtures of experts and the EM algorithm. Neural Computation 6(2), 181\u2013214. Jutten, C. and J. Herault . Blind separation of sources, 1: An adaptive algorithm based on neuromimetic architecture. Signal Processing 24(1), 1\u201310. Kalman, R. E. .\n\nA new approach to linear \ufb01ltering and prediction problems. Transactions of the American Society for Mechanical Engineering, Series D, Journal of Basic Engineering 82, 35\u201345. Kambhatla, N. and T. K. Leen . Dimension reduction by local principal component analysis. Neural Computation 9(7), 1493\u20131516. Kanazawa, K., D. Koller, and S. Russel . Stochastic simulation algorithms for dynamic probabilistic networks. In Uncertainty in Arti\ufb01cial Intelligence, Volume 11. Morgan Kaufmann", "b1b7ee17-c1dd-4118-aeb6-63d8a8254381": "The Gaussian kernel corresponds to a dot  https://www.deeplearningbook.org/contents/ml.html    product in an infinite-dimensional space, but. the derivation of this space is less straightforward than in our example of the min kernel over the integers. We can think of the Gaussian kernel as performing a kind of template match- ing. A training example z associated with training label y becomes a template for class y.\n\nWhen a test point a\u2019 is near x according to Euclidean distance, the Gaussian kernel has a large response, indicating that a\u2019 is very similar to the x template. The model then puts a large weight on the associated training label y. Overall, the prediction will combine many such training labels weighted by the similarity of the corresponding training examples. Support vector machines are not the only algorithm that can be enhanced using the kernel trick. Many other linear models can be enhanced in this way. The category of algorithms that employ the kernel trick is known as kernel machines, or kernel methods", "9943c46a-009d-440f-9100-1b4144e2cb54": "The approximate state will play the same role in our algorithms as before, so we continue to use the notation St for the state used by the agent, even though it may not be Markov. Perhaps the simplest example of an approximate state is just the latest observation, St .=Ot. Of course this approach cannot handle any hidden state information. It would be better to use the last k observations and actions, St .= Ot, At\u22121, Ot\u22121, . .\n\n, At\u2212k, for some k \u2265 1, which can be achieved by a state-update function that just shifts the new data in and the oldest data out. This kth-order history approach is still very simple, but can greatly increase the agent\u2019s capabilities compared to trying to use the single immediate observation directly as the state. What happens when the Markov property (17.5) is only approximately satis\ufb01ed? Unfortunately, long-term prediction performance can degrade dramatically when the one-step predictions de\ufb01ning the Markov property become even slightly inaccurate. Longer-term tests, GVFs, and state-update functions may all approximate poorly. The short-term and long-term approximation objectives are just di\u21b5erent, and there are no useful theoretical guarantees at present", "efdaf79a-2fd0-44e7-bb0c-42f4f6940d82": "log \u201cp(hwi) (19.4) p(v;0) = ae nla: A) \u2014 Fhw~a Nao alh | a) \u2014 lac nth a AV ac nla AN (10-5)  https://www.deeplearningbook.org/contents/inference.html    T7Or\\yr7T ys _ Abvowrv | vs AMOR MIM) 1 AVOrVvyv ss Vevey = \u2014fBh~cflog q(h | v) \u2014 log p(h, v: 8) (19.6)  This yields the more canonical definition of the evidence lower bound, L(v, 8,4) = Enxg  + H(q). (19.7)  For an appropriate choice of gq, \u00a3 is tractable to compute. For any choice of q, \u00a3 provides a lower bound on the likelihood. For q(h | v) that are better  631  CHAPTER 19", "1b7d9e0e-9e35-4774-b307-bade0ccf373a": "The interpretation of this summation over h is that just one model is responsible for generating the whole data set, and the probability distribution over h simply re\ufb02ects our uncertainty as to which model that is. As the size of the data set increases, this uncertainty reduces, and the posterior probabilities p(h|X) become increasingly focussed on just one of the models. This highlights the key difference between Bayesian model averaging and model combination, because in Bayesian model averaging the whole data set is generated by a single model. By contrast, when we combine multiple models, as in (14.5), we see that different data points within the data set can potentially be generated from different values of the latent variable z and hence by different components.\n\nAlthough we have considered the marginal probability p(X), the same considerations apply for the predictive density p(x|X) or for conditional distributions such as p(t|x, X, T). Exercise 14.1 The simplest way to construct a committee is to average the predictions of a set of individual models", "e2497cb9-60ff-4ab4-8fea-4699184f464b": "We can view the probabilistic PeA model from a geoerati\"e \\'iew\"\"int in \"hich a sampled '-alue of the ob\"\"Yed ,..riable is obIained by first choo,ing a ,..Iue for the latent ,'ariahle aod then >ampling the OO\",,,'e;j ,-ariable cooditioned on this lao tent \\'alue, Specifically, the V-dimen'ional OO\"''''ed '-ariable x is defined by a lin\u00b7 ea, tran,formati,,\" of the '\\/\u00b7dimen,i\"nal latcnt '-ariable z plu, additi'-e Gaussian 'noise' , <0 that w!>ere z is an M-di\"\"'nsional Gaussian lalent variable. and .", "41ee426a-7312-404c-b43e-84fc7deecb61": "The maxout layer will of course be parametrized differently from any of these other layer types, so the learning dynamics will be different even in the cases where maxout learns to implement the same function of a as one of the other layer types. Each maxout unit is now parametrized by k weight vectors instead of just one, so maxout units typically need more regularization than rectified linear units. They can work well without regularization if the training set is large and the number of pieces per unit is kept low . Maxout units have a few other benefits.\n\nIn some cases, one can gain some sta- tistical and computational advantages by requiring fewer parameters. Specifically, if the features captured by n different linear filters can be summarized without losing information by taking the max over each group of k& features, then the next layer can get by with k times fewer weights. https://www.deeplearningbook.org/contents/mlp.html    Because each unit is driven by multiple filters, maxout units haye some redun- dancy that helps them resist a phenomenon called catastrophic forgetting, in which neural networks forget how to perform tasks that they were trained on in  190  CHAPTER 6", "42bf9755-59a2-4e65-9acb-d789fb39fd6b": "showed that layers consisting of convolution followed by pooling naturally  https://www.deeplearningbook.org/contents/convnets.html    become frequency selective and translation invariant when assigned random weights. They argue that this provides an inexpensive way to choose the architecture of a convolutional network: first, evaluate the performance of several convolutional  network architectures by training only the last layer; then take the best of these architectures and train the entire architecture using a more expensive approach. An intermediate approach is to learn the features, but using methods that do not require full forward and back-propagation at every gradient step.\n\nAs with multilayer perceptrons, we use greedy layer-wise pretraining, to train the first layer in isolation, then extract all features from the first layer only once, then train the second layer in isolation given those features, and so on. In chapter 8 we described how to perform supervised greedy layer-wise pretraining, and in part III extend this to greedy layer-wise pretraining using an unsupervised criterion at each layer. The canonical example of greedy layer-wise pretraining of a convolutional model is the convolutional deep belief network", "ecacae71-52cd-4015-aef6-ad6d3e4ee9f7": "Holland\u2019s ideas in\ufb02uenced the early research of the authors on reinforcement learning, but we focused on di\u21b5erent approaches to function approximation. As function approximators, classi\ufb01ers are limited in several ways. First, they are state-aggregation methods, with concomitant limitations in scaling and in representing smooth functions e\ufb03ciently. In addition, the matching rules of classi\ufb01ers can implement only aggregation boundaries that are parallel to the feature axes. Perhaps the most important limitation of conventional classi\ufb01er systems is that the classi\ufb01ers are learned via the genetic algorithm, an evolutionary method. As we discussed in Chapter 1, there is available during learning much more detailed information about how to learn than can be used by evolutionary methods. This perspective led us to instead adapt supervised learning methods for use in reinforcement learning, speci\ufb01cally gradient-descent and ANN methods.\n\nThese di\u21b5erences between Holland\u2019s approach and ours are not surprising because Holland\u2019s ideas were developed during a period when ANNs were generally regarded as being too weak in computational power to be useful, whereas our work was at the beginning of the period that saw widespread questioning of that conventional wisdom. There remain many opportunities for combining aspects of these di\u21b5erent approaches", "0ed4d357-aeb4-4ddc-a656-68c665492865": "Outside the tree and at the leaf nodes the rollout policy is used for action selections, but at the states inside the tree something better is possible. For these states we have value estimates for at least some of the actions, so we can pick among them using an informed policy, called the tree policy, that balances exploration and exploitation. For example, the tree policy could select actions using an \"-greedy or UCB selection rule (Chapter 2). In more detail, each iteration of a basic version of MCTS consists of the following four 1. Selection. Starting at the root node, a tree policy based on the action values attached to the edges of the tree traverses the tree to select a leaf node. 2. Expansion. On some iterations (depending on details of the application), the tree is expanded from the selected leaf node by adding one or more child nodes reached from the selected node via unexplored actions. 3. Simulation.\n\nFrom the selected node, or from one of its newly-added child nodes (if any), simulation of a complete episode is run with actions selected by the rollout policy", "6d4b3d11-9941-4939-b0ce-451b40feff0b": "Under these assumptions, the Monte Carlo methods will compute each q\u21e1k exactly, for arbitrary \u21e1k. Policy improvement is done by making the policy greedy with respect to the current value function. In this case we have an action-value function, and therefore no model is needed to construct the greedy policy. For any action-value function q, the corresponding greedy policy is the one that, for each s 2 S, deterministically chooses an action with maximal action-value: Policy improvement then can be done by constructing each \u21e1k+1 as the greedy policy with respect to q\u21e1k.\n\nThe policy improvement theorem (Section 4.2) then applies to \u21e1k As we discussed in the previous chapter, the theorem assures us that each \u21e1k+1 is uniformly better than \u21e1k, or just as good as \u21e1k, in which case they are both optimal policies. This in turn assures us that the overall process converges to the optimal policy and optimal value function. In this way Monte Carlo methods can be used to \ufb01nd optimal policies given only sample episodes and no other knowledge of the environment\u2019s dynamics. We made two unlikely assumptions above in order to easily obtain this guarantee of convergence for the Monte Carlo method", "799e4483-bbef-4f07-b32b-300ca7986006": "Tsendsuren Munkhdalai and Hong Yu. \u201cMeta Networks.\" ICML. 2017. Sachin Ravi and Hugo Larochelle. \u201cOptimization as a Model for Few-Shot Learning.\" ICLR. 2017. Chelsea Finn's BAIR blog on \u201cLearning to Learn\u201d. Chelsea Finn, Pieter Abbeel, and Sergey Levine. \u201cModel-agnostic meta-learning for fast adaptation of deep networks.\" ICML 2017. Alex Nichol, Joshua Achiam, John Schulman. \u201cOn First-Order Meta-Learning Algorithms.\" arXiv preprint arXiv:1803.02999 . Slides on Reptile by Yoonho Lee.\n\nhttps://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Log   ( JL)  Object Detection Part 4: Fast Detection Flow-based Deep Generative Models  Models  https://lilianweng.github.io/posts/2018-11-30-meta-learning/", "38de022e-2e6c-4a2a-92f7-e2267a98ce32": "Tenenbaum.\n\n\u201cHuman-level concept learning through probabilistic program induction.\" Science 350.6266 : 1332-1338. Oriol Vinyals' talk on \u201cModel vs Optimization Meta Learning\u201d   Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov. \u201cSiamese neural networks for one- shot image recognition.\" ICML Deep Learning Workshop. 2015. Oriol Vinyals, et al. \u201cMatching networks for one shot learning.\" NIPS. 2016. Flood Sung, et al. \u201cLearning to compare: Relation network for few-shot learning.\" CVPR. 2018. Jake Snell, Kevin Swersky, and Richard Zemel. \u201cPrototypical Networks for Few-shot Learning.\" CVPR. 2018. Adam Santoro, et al. \u201cMeta-learning with memory-augmented neural networks.\" ICML. 2016. Alex Graves, Greg Wayne, and lvo Danihelka. \u201cNeural turing machines.\" arXiv preprint arXiv:1410.5401", "cfafbcf6-a228-476a-b337-3a80c87cd9de": "Moreover, the expected value of all these other factors is one: With a few more steps, one can show that, as suspected, all of these other factors have no e\u21b5ect in expectation, in other words, that If we repeat this process for the kth sub-term of (5.11), we get It follows then that the expectation of our original term (5.11) can be written \u02dcGt = \u21e2t:tRt+1 + \u03b3\u21e2t:t+1Rt+2 + \u03b32\u21e2t:t+2Rt+3 + \u00b7 \u00b7 \u00b7 + \u03b3T \u2212t\u22121\u21e2t:T \u22121RT . We call this idea per-decision importance sampling.\n\nIt follows immediately that there is an alternate importance-sampling estimator, with the same unbiased expectation (in the \ufb01rst-visit case) as the ordinary-importance-sampling estimator (5.5), using \u02dcGt: which we might expect to sometimes be of lower variance. Is there a per-decision version of weighted importance sampling? This is less clear. So far, all the estimators that have been proposed for this that we know of are not consistent (that is, they do not converge to the true value with in\ufb01nite data)", "e3aff6a9-055c-4272-9448-d60fd1421e33": "Including points in parameter space from the distant past that may be separated from the current point by large barriers in the cost function does not seem like a useful behavior.\n\nAs a result, when applying Polyak averaging to nonconvex problems, it is typical to use an exponentially decaying running average:  6 = at) + (1- aja, (8.39)  The running average approach is used in numerous applications. See Szegedy et al. for a recent example. CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  8.7.4 Supervised Pretraining  Sometimes, directly training a model to solve a specific task can be too ambitious  https://www.deeplearningbook.org/contents/optimization.html    if the model is complex and hard to optimize or if the task is very difficult. It is sometimes more effective to train a simpler model to solve the task, then make he model more complex. It can also be more effective to train the model to solve  a simpler task, then move on to confront the final task. These strategies that involve training simple models on simple tasks before confronting the challenge of raining the desired model to perform the desired task are collectively known as pretraining", "bfb084b0-4a07-4e92-b53c-ef01b84f3d45": "Such a path connects nodes a and b and renders them dependent. If we now observe c, as in Figure 8.18, then this observation \u2018blocks\u2019 the path from a to b and so we obtain the conditional independence property a \u22a5\u22a5 b | c. Finally, we consider the third of our 3-node examples, shown by the graph in The joint distribution can again be written down using our general result (8.5) to Consider \ufb01rst the case where none of the variables are observed. Marginalizing both sides of (8.28) over c we obtain and so a and b are independent with no variables observed, in contrast to the two previous examples. We can write this result as Now suppose we condition on c, as indicated in Figure 8.20.\n\nThe conditional distribution of a and b is then given by which in general does not factorize into the product p(a)p(b), and so Thus our third example has the opposite behaviour from the \ufb01rst two. Graphically, we say that node c is head-to-head with respect to the path from a to b because it connects to the heads of the two arrows", "4eb80dd4-f580-48f3-8ac9-d56def70681c": "If the eigenvalues of the Hessian are not all positive, for example, near a saddle point, then Newton\u2019s method can actually cause updates to move in the wrong direction. This situation can be avoided by regularizing the Hessian. Common regularization strategies include adding a constant, a, along the diagonal of the Hessian. The regularized update becomes  0* = 09 \u2014  ~' Vof (80). 8.28)  This regularization strategy is used in approximations to Newton\u2019s method, such as the Levenberg-Marquardt algorithm , and works fairly well as long as the negative eigenvalues of the Hessian are still relatively close to zero.\n\nWhen there are more extreme directions of curvature, the value of a would have to be sufficiently large to offset the negative eigenvalues", "ddf534e8-cf6e-4262-b982-69cbe065838e": "In this case, the detector learns that a loop on the bottom of the digit corresponds to an 8.\n\nEach of these individual classification rules is brittle, but if we average their output, then the detector is robust, achieving maximal confidence only when both loops of the 8 are present. toys 4 1 aot rad oa 1 AT Trar at aot 1  https://www.deeplearningbook.org/contents/regularization.html    OLlgillal Gavlaset ald COLLLALLIS SECVELal GU LICALE CXAalples . AVLOUEL o is LICL LLallled on dataset 7. The differences between which examples are included in each dataset result in differences between the trained models. See figure 7.5 for an example. Neural networks reach a wide enough variety of solution points that they can often benefit from model averaging even if all the models are trained on the same dataset. Differences in random initialization, in random selection of minibatches, in hyperparameters, or in outcomes of nondeterministic implementations of neural networks are often enough to cause different members of the ensemble to make partially independent errors", "d33e7455-046a-4bf8-bebc-792e613adebb": "In this case the ordinary importance-sampling estimate would be ten times the observed return. That is, it would be quite far from the observed return even though the episode\u2019s trajectory is considered very representative of the target policy. Formally, the di\u21b5erence between the \ufb01rst-visit methods of the two kinds of importance sampling is expressed in their biases and variances. Ordinary importance sampling is unbiased whereas weighted importance sampling is biased (though the bias converges asymptotically to zero).\n\nOn the other hand, the variance of ordinary importance sampling is in general unbounded because the variance of the ratios can be unbounded, whereas in the weighted estimator the largest weight on any single return is one. In fact, assuming bounded returns, the variance of the weighted importance-sampling estimator converges to zero even if the variance of the ratios themselves is in\ufb01nite . In practice, the weighted estimator usually has dramatically lower variance and is strongly preferred. Nevertheless, we will not totally abandon ordinary importance sampling as it is easier to extend to the approximate methods using function approximation that we explore in the second part of this book", "36820f55-42e2-400d-84d5-aad8e49dfeb1": "Fine-tuning is typically very fast, so it is reasonable to simply run an exhaustive search over the above parameters and choose the model that performs best on the development set.\n\nHere we studies the differences in recent popular representation learning models including ELMo, OpenAI GPT and BERT. The comparisons between the model architectures are shown visually in Figure 3. Note that in addition to the architecture differences, BERT and OpenAI GPT are \ufb01netuning approaches, while ELMo is a feature-based approach. The most comparable existing pre-training method to BERT is OpenAI GPT, which trains a left-to-right Transformer LM on a large text corpus. In fact, many of the design decisions in BERT were intentionally made to make it as close to GPT as possible so that the two methods could be minimally compared. The core argument of this work is that the bi-directionality and the two pretraining tasks presented in Section 3.1 account for the majority of the empirical improvements, but we do note that there are several other differences between how BERT and GPT were trained: \u2022 GPT is trained on the BooksCorpus (800M words); BERT is trained on the BooksCorpus (800M words) and Wikipedia (2,500M words)", "d2ede242-5dd6-436b-81c2-9cb65e9c3c40": "In our example of recognizing cats after having read about cats, the output is a binary variable y with y= 1 indicating \u201cyes\u201d and y = 0 indicating \u201cno.\u201d The task variable T then represents questions to be answered, such as \u201cIs there a cat in this image?\u201d If we have a training set containing unsupervised examples of objects that live in the same space as T\u2019, we may be able to infer the meaning of unseen instances of T. In our example of recognizing cats without having seen an image of the cat, it is important that we have had unlabeled text data containing sentences such as \u201ccats have four legs\u201d or \u201ccats have pointy ears.\u201d  Zero-shot learning requires T to be represented in a way that allows some sort of generalization. For example, T cannot be just a one-hot code indicating an object category. Socher et al. provide instead a distributed representation of object categories by using a learned word embedding for the word associated with each category.\n\nA similar phenomenon happens in machine translation : we have words in one language, and the relationships between words can be learned from unilingual corpora; on the other hand, we have translated sentences that relate words in one language with words in che other", "d21f4fa4-631d-4ee1-af6c-617a6ae27102": "In the self-supervised pretraining phase, the system is shown a short text (typically 1,000 words) in which some of the words have been masked or replaced. The system is trained to predict the words that were masked or replaced. In doing so, the system learns to represent the meaning of the text so that it can do a good job at filling in  \u201ccorrect\u201d words, or those that make sense in the context. Predicting missing parts of the input is one of the more standard tasks for SSL  https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/  Self-supervised learning: The dark matter of intelligence   pretraining. To complete a sentence such as \u201cThe (blank) chases the (blank) in the savanna,\u201d the system must learn that lions or cheetahs can chase antelope or wildebeests, but that cats chase mice in the kitchen, not the savanna. As a consequence of the training, the system learns to represent the meaning of words, the syntactic role of words, and the meaning of entire texts", "d03ee3af-d987-4b0c-8f20-a24b07298f9c": "This makes sense because that action is characteristic of \u21e1 (and therefore we want to learn about it) but is selected only rarely by b and thus rarely appears in the data. To make up for this we have to over-weight it when it does occur. Note that if the two policies are actually the same (the on-policy case) then the importance sampling ratio is always 1. Thus our new update (7.9) generalizes and can completely replace our earlier n-step TD update. Similarly, our previous n-step Sarsa update can be completely replaced by a simple o\u21b5-policy form: for 0 \uf8ff t < T. Note that the importance sampling ratio here starts and ends one step later than for n-step TD (7.9). This is because here we are updating a state\u2013action pair. We do not have to care how likely we were to select the action; now that we have selected it we want to learn fully from what happens, with importance sampling only for subsequent actions", "fca47e62-bf17-47ee-b8be-44d6e2a77752": "For instance in the \ufb01rst of these results, we note that every path from any one of the nodes x1, . .\n\n, xn\u22121 to the node xn passes through the node zn, which is observed. Because all such paths are head-to-tail, it follows that the conditional independence property must hold. The reader should take a few moments to verify each of these properties in turn, as an exercise in the application of d-separation. These relations can also be proved directly, though with signi\ufb01cantly greater effort, from the joint distribution for the hidden Markov model using the sum and product rules of probability. Exercise 13.10 Let us begin by evaluating \u03b3(znk). Recall that for a discrete multinomial random variable the expected value of one of its components is just the probability of that component having the value 1. Thus we are interested in \ufb01nding the posterior distribution p(zn|x1, . , xN) of zn given the observed data set x1, . , xN. This represents a vector of length K whose entries correspond to the expected values of znk", "79a67f23-968e-4839-91b6-74cbe5ac45c4": "In most situations, we set 49 to 0. If we set Ag = +I, then jm gives the same estimate of w as does frequentist linear regression with a weight decay penalty of aw! w. One difference is that the Bayesian estimate is undefined if a@ is set to zero\u2014we are not allowed to begin the Bayesian learning process with an infinitely wide prior on w. The more important difference is that the Bayesian estimate provides a covariance matrix, showing how likely all the different values of w are, rather than providing only the estimate p1,,. 5.6.1 Maximum a Posteriori (MAP) Estimation  https://www.deeplearningbook.org/contents/ml.html    While the most principled approach is to make predictions using the full Bayesian posterior distribution over the parameter %, it is still often desirable to have a single point estimate. One common reason for desiring a point estimate is that most operations involving the Bayesian posterior for most interesting models are intractable, and a point estimate offers a tractable approximation", "c17c4324-43d1-4522-9c42-35c0f2a51832": "Empirically, they observed two issues with  BERT sentence embedding: Word frequency biases the embedding space.\n\nHigh-frequency words are close to the origin, but low-frequency ones are far away from the origin. Low-frequency words  scatter sparsely. The embeddings of low-frequency words tend to be farther to their kK-NN  neighbors, while the embeddings of high-frequency words concentrate more densely. https://lilianweng.github.io/posts/2021-05-31-contrastive/     Contrastive Representation Learning | Lil'Log  BERT-flow (Li et al, 2020; code) was proposed to transform the embedding to a smooth and isotropic Gaussian distribution via normalizing flows. The BERT sentence embedding space  Standard Gaussian latent space (isotropic)  Let U/ be the observed BERT sentence embedding space and Z be the desired latent space which is a standard Gaussian", "3ae87b91-c251-43e7-9048-ba267e598fd9": "Please go through the NTM section in my other post first if you are not familiar with this matter before reading forward.\n\nAs a quick recap, NTM couples a controller neural network with external memory storage. The controller learns to read and write memory rows by soft attention, while the memory serves as a knowledge repository. The attention weights are generated by its addressing mechanism: content- based + location based. https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Log   Input Output  yf  \u201cA ON  Memory M, \u20ac RX\u201d  MANN for Meta-Learning  To use MANN for meta-learning tasks, we need to train it in a way that the memory can encode and capture information of new tasks fast and, in the meantime, any stored representation is easily and  stably accessible. The training described in Santoro et al., 2016 happens in an interesting way so that the memory is forced to hold information for longer until the appropriate labels are presented later", "a816af6e-ad73-4a51-91e0-33ba986abc76": "Consider the derivative with respect to one of the biases:  i log p(v) (19.21) _ a Pl) 19.22) p(v) id v _ 05 Un V(h.v) 19.23) p(v) (e) a; WhP(h)p(w | h) 19.24) p(v) Ln Plv | h) ze v(h) 19.25) p(v) 638  CHAPTER 19. APPROXIMATE INFERENCE  https://www.deeplearningbook.org/contents/inference.html    SO ~v  Figure 19.2: The graph structure of a binary sparse coding model with four hidden units.\n\n(Left)The graph structure of p(h, v). Note that the edges are directed, and that every two hidden units are coparents of every visible unit. (Right)The graph structure of p(h | v). To account for the active paths between coparents, the posterior distribution needs an edge between all the hidden units", "873863e3-0a1d-4851-be20-48365c933a96": ", D, as illustrated in Figure 12.2. This is to be expected because the projected points xn must lie within the principal subspace, but we can move them freely within that subspace, and so the minimum error is given by the orthogonal projection. We therefore obtain an expression for the distortion measure J as a function purely of the {ud in the form There remains the task of minimizing J with respect to the {Ui}, which must be a constrained minimization otherwise we will obtain the vacuous result Ui = O.\n\nThe constraints arise from the orthonormality conditions and, as we shall see, the solution will be expressed in terms of the eigenvector expansion of the covariance matrix. Before considering a formal solution, let us try to obtain some intuition about the result by considering the case of a two-dimensional data space D = 2 and a onedimensional principal subspace M = 1. We have to choose a direction U2 so as to minimize J = UISU2' subject to the normalization constraint uI U2 = 1", "d9438cab-1adc-4a83-a334-18588f9b9db8": "By this we mean q is defined in terms of the current parameter value of 9(); if we vary @, then p(h | v;@) will change, but q(h | v) will remain equal to p(h | v; 0). e The M-step (maximization step): Completely or partially maximize  SL(v , 0,4) (19.8)  632  CHAPTER 19. APPROXIMATE INFERENCE  with respect to 8 using your optimization algorithm of choice. This can be viewed as a coordinate ascent algorithm to maximize \u00a3. On one step, we maximize \u00a3 with respect to gq, and on the other, we maximize \u00a3 with respect to 0. Stochastic gradient ascent on latent variable models can be seen as a special case of the EM algorithm where the M-step consists of taking a single gradient step. Other variants of the EM algorithm can make much larger steps. For some model families, the M-step can even be performed analytically, jumping all the way to the optimal solution for @ given the current q.\n\nEven though the E-step involves exact inference, we can think of the EM algorithm as using approximate inference in some sense", "1f4e99c3-df8b-4dc8-8fe0-fcb14233a987": "First, the LM is \ufb01xed after \ufb01tting to the task data. In the subsequent phase of training the target model, the LM augments data without knowing the state of the target model, which can lead to sub-optimal results. Second, in the cases where the task dataset is small, the LM can be insuf\ufb01ciently trained for preserving the labels faithfully, resulting in noisy augmented samples. To address the dif\ufb01culties, it is bene\ufb01cial to apply the proposed learning data manipulation algorithm to additionally \ufb01ne-tune the LM jointly with target model training.\n\nAs discussed in section 4, this reduces to properly parameterizing the data reward function: That is, a sample (x, y) receives a unit reward when y is the true label and x is the augmented sample by the LM (instead of the exact original data x\u2217). Plugging the reward into Eq. (7), we obtain the data-augmented update for the model parameters: That is, we pick an example from the training set, and use the LM to create augmented samples, which are then used to update the target model", "435ca9a9-8f44-45a0-b82a-88ad33e74492": "The update to \u03c6 is then written as: That is, we want the expected extrinsic reward Lex(\u03b8\u2032) of the new policy \u03b8\u2032 to be maximized. Since \u03b8\u2032 is a function of \u03c6, we can directly backpropagate the gradient through \u03b8\u2032 to \u03c6. Algorithm 1 Joint Learning of Model and Data Manipulation Parameterizing Data Manipulation We now develop our approach of learning data manipulation, through a novel marriage of supervised learning and the above reward learning.\n\nSpeci\ufb01cally, from the policy optimization perspective, due to the \u03b4-function reward (Eq.3), the standard maximum likelihood learning is restricted to use only the exact training examples D in a uniform way. A natural idea of enabling data manipulation is to relax the strong restrictions of the \u03b4-function reward and instead use a relaxed reward R\u03c6(x, y|D) with parameters \u03c6. The relaxed reward can be parameterized in various ways, resulting in different types of manipulation. For example, when a sample (x, y) matches a data instance, instead of returning constant 1 by R\u03b4, the new R\u03c6 can return varying reward values depending on the matched instance, resulting in a data weighting scheme", "fef9ef17-4ccd-438a-b16e-9a88504029a9": "[ \u2014- Identity \u2014 Optimal reconstruction  https://www.deeplearningbook.org/contents/autoencoders.html    Figure 14.7: If the autoencoder learns a reconstruction function that is invariant to small perturbations near the data points, it captures the manifold structure of the data. Here the manifold structure is a collection of 0-dimensional manifolds. The dashed diagonal line indicates the identity function target for reconstruction. The optimal reconstruction  function crosses the identity function wherever there is a data point. The horizontal arrows at the bottom of the plot indicate the r(a) \u2014 x reconstruction direction vector at the base of the arrow, in input space, always pointing toward the nearest \u201cmanifold\u201d (a single data point in the 1-D case). The denoising autoencoder explicitly tries to make the derivative of the reconstruction function r(z) small around the data points. The contractive autoencoder does the same for the encoder.\n\nAlthough the derivative ofr(a) is asked to be small around the data points, it can be large between the data points", "6f14c39c-320f-4ee6-860a-7237fbddd54f": "It is easily veri\ufb01ed that \u03b4(Pm, Pn) \u2264 \u03b4(Pn, P), and in particular this tends to 0 (as does \u03b4(Pm, P)). We now show this for completeness. Let \u00b5 be a signed measure, we de\ufb01ne \u2225\u00b5\u2225T V = supA\u2286X |\u00b5(A)|. for all Borel sets A. In this case, which implies Pm(A) = 0. This means that fn is bounded by 3 Pm(and therefore Pn and P)-almost everywhere. We could have done this for any constant larger than 2 but for our purposes 3 will su\ufb01ce. Let \u03f5 > 0 \ufb01xed, and An = {fn > 1 + \u03f5}. Then, meaning that Pm({gn > 3}) = 0 and therefore gn is bounded by 3 almost everywhere for Pn, Pm and P. With the same calculation, Bn = {gn > 1 + \u03f5} and 2", "33eb16f8-0d22-4799-be46-17cbf4b69277": "Successive samples are highly correlated with one another. (Right) The corresponding weight vectors. Compare this to the samples and weights of a linear factor model, shown in figure 13.2. The samples here are much better because the RBM priorp(h) is not constrained to be factorial. The RBM can learn which features should appear together when sampling. On the other hand, the RBM posterior p(h | v) is factorial, while the sparse coding posterior p(h | v) is not, so the sparse coding model may be better for feature extraction. Other models are able to have both a nonfactorialp(h) and a nonfactorial p(h |v). Image reproduced with permission from LISA", "f4dc0330-d415-48b8-b35d-e9628c5ab601": "If we have a distribution p(x|\u03bb) governed by a parameter \u03bb, we might be tempted to propose a prior distribution p(\u03bb) = const as a suitable prior. If \u03bb is a discrete variable with K states, this simply amounts to setting the prior probability of each state to 1/K. In the case of continuous parameters, however, there are two potential dif\ufb01culties with this approach. The \ufb01rst is that, if the domain of \u03bb is unbounded, this prior distribution cannot be correctly normalized because the integral over \u03bb diverges. Such priors are called improper. In practice, improper priors can often be used provided the corresponding posterior distribution is proper, i.e., that it can be correctly normalized. For instance, if we put a uniform prior distribution over the mean of a Gaussian, then the posterior distribution for the mean, once we have observed at least one data point, will be proper.\n\nA second dif\ufb01culty arises from the transformation behaviour of a probability density under a nonlinear change of variables, given by (1.27)", "cc4da9c0-261c-4be0-90e7-2465e907580e": "Ph.D. thesis, University of Precup, D., Sutton, R. S., Dasgupta, S. .\n\nO\u21b5-policy temporal-di\u21b5erence learning with function approximation. In Proceedings of the 18th International Conference on Machine Learning , pp. 417\u2013424. Precup, D., Sutton, R. S., Paduraru, C., Koop, A., Singh, S. O\u21b5-policy learning with options and recognizers. In Advances in Neural Information Processing Systems 18 (NIPS Precup, D., Sutton, R. S., Singh, S. Eligibility traces for o\u21b5-policy policy evaluation. In Proceedings of the 17th International Conference on Machine Learning , pp. 759\u2013766. Morgan Kaufmann. Markov decision problems. Management Science, 24(11):1127\u20131137. Quartz, S., Dayan, P., Montague, P. R., Sejnowski, T. J", "60969b6d-bc21-4f81-af1f-6edfd2548e6e": "The number of elements in the Hessian is squared in the number of parameters, so with k parameters (and for even very small neural networks, the number of parameters k can be in the millions), Newton\u2019s method would require the inversion of a k x k matrix\u2014with computational complexity of O(k\u00ae).\n\nAlso, since the parameters will change with every update, the inverse Hessian has to be computed at every training iteration. As a consequence, only networks with a very small number of parameters can be practically trained via Newton\u2019s method. In the remainder of this section, we discuss alternatives that attempt to gain some of the advantages of Newton\u2019s method while side-stepping the computational hurdles. 8.6.2 Conjugate Gradients  Conjugate gradients is a method to efficiently avoid the calculation of the inverse Hessian by iteratively descending conjugate directions. The inspiration for this approach follows from a careful study of the weakness of the method of steepest descent (see section 4.3 for details), where line searches are applied iteratively in the direction associated with the gradient", "1b684767-dcf5-4982-83e4-f0e171443442": "The result is a Monte Carlo trial with actions selected \ufb01rst by the tree policy and beyond the tree by the rollout policy. 4. Backup. The return generated by the simulated episode is backed up to update, or to initialize, the action values attached to the edges of the tree traversed by the tree policy in this iteration of MCTS. No values are saved for the states and actions visited by the rollout policy beyond the tree. Figure 8.10 illustrates this by showing a backup from the terminal state of the simulated trajectory directly to the state\u2013action node in the tree where the rollout policy began (though in general, the entire return over the simulated trajectory is backed up to this state\u2013action node). MCTS continues executing these four steps, starting each time at the tree\u2019s root node, until no more time is left, or some other computational resource is exhausted. Then, \ufb01nally, an action from the root node (which still represents the current state of the environment) is selected according to some mechanism that depends on the accumulated statistics in the tree; for example, it may be an action having the largest action value of all the actions available from the root state, or perhaps the action with the largest visit count to avoid selecting outliers", "185c7225-65f5-4a24-a8cd-8b901ac17e51": "Clearly, neither force alone would be useful\u2014copying the input to the output is not useful on its own, nor is ignoring the input. Instead, the two forces together are useful because they force the hidden representation to capture information about the structure of the data-generating distribution. The important principle is that the autoencoder can afford to represent only the variations that are needed to reconstruct training examples. If the data-generating distribution concentrates  https://www.deeplearningbook.org/contents/autoencoders.html    near a low-dimensional manitold, this yields representations that implicitly capture a local coordinate system for this manifold: only the varjations tangent to the manifold around \u00a3 need to correspond to changes in h = (x), Hence the encoder learns a mapping from the input space x to a representation space, a mapping that is only sensitive to changes along the manifold directions, but that is insensitive to changes orthogonal to the manifold. 513  CHAPTER 14. AUTOENCODERS  Figure 14.6: An illustration of the concept of a tangent hyperplane. Here we create a 1-D manifold in 784-D space", "cbdec45c-157d-4375-9a91-0ca2a8116a86": "Even without any special assumption about how an animal\u2019s brain might produce overt responses from US predictions, however, the pro\ufb01les in Figure 14.4 for the CSC and MS representations increase as the time of the US approaches and reach a maximum at the time of the US, as is seen in many animal conditioning experiments. The TD model, when combined with particular stimulus representations and responsegeneration mechanisms, is able to account for a surprisingly wide range of phenomena observed in animal classical conditioning experiments, but it is far from being a perfect model.\n\nTo generate other details of classical conditioning the model needs to be extended, perhaps by adding model-based elements and mechanisms for adaptively altering some of its parameters. Other approaches to modeling classical conditioning depart signi\ufb01cantly from the Rescorla\u2013Wagner-style error-correction process. Bayesian models, for example, work within a probabilistic framework in which experience revises probability estimates. All of these models usefully contribute to our understanding of classical conditioning", "c5d4845e-9aeb-4762-97f9-8002d7bece9f": "The true value function v\u21e1 is in the larger space and can be projected down (into the subspace, using a projection operator \u21e7) to its best approximation in the value error (VE) sense.\n\nThe best approximators in the Bellman error (BE), projected Bellman error (PBE), and temporal di\u21b5erence error (TDE) senses are all potentially di\u21b5erent and are shown in the lower right. (VE, BE, and PBE are all treated as the corresponding vectors in this \ufb01gure.) The Bellman operator takes a value function in the plane to one outside, which can then be projected back. If you iteratively applied the Bellman operator outside the space (shown in gray above) you would reach the true value function, as in conventional dynamic programming. If instead you kept projecting back into the subspace at each step, as in the lower step shown in gray, then the \ufb01xed point would be the point of vector-zero PBE. projection operator \u21e7 that takes an arbitrary value function to the representable function that is closest in our norm: The representable value function that is closest to the true value function v\u21e1 is thus its projection, \u21e7v\u21e1, as suggested in Figure 11.3", "22018a9f-5267-4e60-b15e-1232f336144f": "The computational cost is only twice that for \ufb01nding the marginal of a single node, rather than N times as much. Observe that a message has passed once in each direction across each link in the graph. Note also that the normalization constant Z need be evaluated only once, using any convenient node. If some of the nodes in the graph are observed, then the corresponding variables are simply clamped to their observed values and there is no summation. To see this, note that the effect of clamping a variable xn to an observed value \ufffdxn can be expressed by multiplying the joint distribution by (one or more copies of) an additional function I(xn, \ufffdxn), which takes the value 1 when xn = \ufffdxn and the value 0 otherwise. One such function can then be absorbed into each of the potentials that contain xn. Summations over xn then contain only one term in which xn = \ufffdxn. Now suppose we wish to calculate the joint distribution p(xn\u22121, xn) for two neighbouring nodes on the chain.\n\nThis is similar to the evaluation of the marginal for a single node, except that there are now two variables that are not summed out", "70fde4fa-2905-42e3-8843-1b34df99c640": "Unlike gradient descent, these algorithms have the property that the error function always decreases at each iteration unless the weight vector has arrived at a local or global minimum.\n\nIn order to \ufb01nd a suf\ufb01ciently good minimum, it may be necessary to run a gradient-based algorithm multiple times, each time using a different randomly chosen starting point, and comparing the resulting performance on an independent validation set. There is, however, an on-line version of gradient descent that has proved useful in practice for training neural networks on large data sets . Error functions based on maximum likelihood for a set of independent observations comprise a sum of terms, one for each data point On-line gradient descent, also known as sequential gradient descent or stochastic gradient descent, makes an update to the weight vector based on one data point at a time, so that w(\u03c4+1) = w(\u03c4) \u2212 \u03b7\u2207En(w(\u03c4)). (5.43) This update is repeated by cycling through the data either in sequence or by selecting points at random with replacement. There are of course intermediate scenarios in which the updates are based on batches of data points. One advantage of on-line methods compared to batch methods is that the former handle redundancy in the data much more ef\ufb01ciently", "f1ef4e73-a369-4d6b-b223-6da9d8317600": "Svens\u00b4en, and G. E. Hinton . Distinguishing text from graphics in online handwritten ink. In F. Kimura and H. Fujisawa (Eds. ), Proceedings Ninth International Workshop on Frontiers in Handwriting Recognition, IWFHR-9, Tokyo, Japan, pp. 142\u2013147. Bishop, C. M., M. Svens\u00b4en, and C. K. I. Williams . EM optimization of latent variable density models. In D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo (Eds. ), Advances in Neural Information Processing Systems, Volume 8, pp. 465\u2013471. MIT Press. Bishop, C. M., M. Svens\u00b4en, and C. K. I. Williams", "235fcdda-0ad0-49f5-bd92-ddc71b8761f7": "In this setting, an initial spectral radius of 1.2 performs well, combined with the sparse initialization scheme described in section 8.4. 401  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  10.9 Leaky Units and Other Strategies for Multiple Time Scales  One way to deal with long-term dependencies is to design a model that operates  https://www.deeplearningbook.org/contents/rnn.html    at multiple time scales, so that some parts of the model operate at fine-grained time scales and can handle small details, while other parts operate at coarse time scales and transfer information from the distant past to the present more efficiently. Various strategies for building both fine and coarse time scales are possible. These include the addition of skip connections across time, \u201cleaky units\u201d that integrate signals with different time constants, and the removal of some of the connections used to model fine-grained time scales. 10.9.1 Adding Skip Connections through Time  One way to obtain coarse time scales is to add direct connections from variables in the distant past to variables in the present", "d5e2bc18-83a5-49e5-a31e-efeec894c321": "Nelder, J. A. and R. W. M. Wedderburn . Generalized linear models. Journal of the Royal Statistical Society, A 135, 370\u2013384. Nowlan, S. J. and G. E. Hinton . Simplifying neural networks by soft weight sharing. Neural Computation 4(4), 473\u2013493. Rao, C. R. and S. K. Mitra . Generalized Inverse of Matrices and Its Applications. Wiley. Rasmussen, C. E. Evaluation of Gaussian Processes and Other Methods for Non-Linear Regression. Ph. D. thesis, University of Toronto. Rasmussen, C. E. and J. Qui\u02dcnonero-Candela . Healing the relevance vector machine by augmentation. In L. D", "7fbee44b-b3f4-48b9-8669-037354cf7735": "Finally, we note that the equivalent kernel (3.62) satis\ufb01es an important property shared by kernel functions in general, namely that it can be expressed in the form an Chapter 6 inner product with respect to a vector \u03c8(x) of nonlinear functions, so that In Chapter 1, we highlighted the problem of over-\ufb01tting as well as the use of crossvalidation as a technique for setting the values of regularization parameters or for choosing between alternative models. Here we consider the problem of model selection from a Bayesian perspective.\n\nIn this section, our discussion will be very general, and then in Section 3.5 we shall see how these ideas can be applied to the determination of regularization parameters in linear regression. As we shall see, the over-\ufb01tting associated with maximum likelihood can be avoided by marginalizing (summing or integrating) over the model parameters instead of making point estimates of their values. Models can then be compared directly on the training data, without the need for a validation set. This allows all available data to be used for training and avoids the multiple training runs for each model associated with cross-validation. It also allows multiple complexity parameters to be determined simultaneously as part of the training process", "f4f3703f-d828-4ae7-9f9b-3666c0680801": "In the future it may be possible to more \ufb01nely vary the trade-o\u21b5 between TD and Monte Carlo methods by using variable \u03bb, but at present it is not clear how this can be done reliably and usefully.\n\nMethods using eligibility traces require more computation than one-step methods, but in return they o\u21b5er signi\ufb01cantly faster learning, particularly when rewards are delayed by many steps. Thus it often makes sense to use eligibility traces when data are scarce and cannot be repeatedly processed, as is often the case in online applications. On the other hand, in o\u270fine applications in which data can be generated cheaply, perhaps from an inexpensive simulation, then it often does not pay to use eligibility traces. In these cases the objective is not to get more out of a limited amount of data, but simply to process as much data as possible as quickly as possible. In these cases the speedup per datum due to traces is typically not worth their computational cost, and one-step methods are favored. Eligibility traces came into reinforcement learning via the fecund ideas of Klopf . Our use of eligibility traces is based on Klopf\u2019s work . We may have been the \ufb01rst to use the term \u201celigibility trace\u201d", "f9b6bf05-b6da-4d2f-9848-35166f46de62": "But before that, a more basic question to ask is, perhaps, how can we characterize learning with some special or novel forms of experience, such as logic rules and auxiliary models?\n\nWhat mathematical tools may we use for characterization, and what would the convergence guarantees, complexity, robustness, and other theoretical and statistical properties be? Inspired by how we generalized the specialized algorithms to new problems, a promising way of the theoretical analysis would be to again leverage the standard equation and repurpose the existing analyses originally dealing with supervised learning, online learning, and reinforcement learning, to now analyze the learning process with all other experiences. From standardization to automation. As in other mature engineering disciplines such as mechanical engineering, standardization is followed by automation. The standardized ML formalism opens up the possibility of automating the process of creating and improving ML algorithms and solutions. The current \u2018AutoML\u2019 practice has largely focused on automatic search of neural network architectures and hyperparameters, thanks to the well-de\ufb01ned architectural and hyperparameter search spaces. The standard equation that de\ufb01nes a structured algorithmic space would similarly suggest opportunities for automatic search or optimization of learning algorithms, which is expected to be more e\ufb03cient than direct search on the programming code space", "373290f7-9284-497e-bdcd-2c8a33fce8db": "o(\u2014b;) \u2014 log(1 \u2014 hv))| 19.33) i=1 oe 1/2 exp (Btn, \u2014 We hy? + Enng pa ae o( 5 (vi~ Wish) ) 19.34) => [hi(log o(bs) = log hi) + (1 = fi )(log o(\u2014by) \u2014 log( \u2014 hi)| 19.35)  i=1  ty [bs ci Biv Wich De sa Emataiy | (19.36)  While these equations are somewhat unappealing aesthetically, they show that \u00a3 can be expressed in a small number of simple arithmetic operations. The evidence lower bound C is therefore tractable. We can use \u00a3 as a replacement for the intractable log-likelihood.\n\nIn principle, we could simply run gradient ascent on both v and h, and this would make a perfectly acceptable combined inference and training algorithm. Usually, however, we do not do this, for two reasons. First, this would require storing h for each v. We typically prefer algorithms that do not require per example memory", "7e08f724-7c26-4283-af29-542db3aa983e": "Proceedings of the 34th International Conference on Machine Learning, pp. 1049\u20131058. ArXiv:1702.07944. Duda, R. O., Hart, P. E. Pattern Classi\ufb01cation and Scene Analysis. Wiley, New York. Du\u21b5, M. O. Q-learning for bandit problems. In Proceedings of the 12th International Conference on Machine Learning, pp. 209\u2013217. Morgan Kaufmann. Egger, D. M., Miller, N. E. Secondary reinforcement in rats as a function of information value and reliability of the stimulus. Journal of Experimental Psychology, 64:97\u2013104. Eshel, N., Tian, J., Bukwich, M., Uchida, N. Dopamine neurons share common response function for reward prediction error. Nature Neuroscience, 19(3):479\u2013486. Estes, W. K. Discriminative conditioning. I. A discriminative property of conditioned anticipation", "b3deb5e8-2da2-4ce7-a3c0-09c06a1d2cf0": "This approach has theoretical connections to denoising autoencoders, manifold learning, and probabilistic modeling. The CAE is described in more detail in section 14.7. 14.3. Representational Power, Layer Size and Depth  Autoencoders are often trained with only a single layer encoder and a single layer decoder. However, this is not a requirement. In fact, using deep encoders and decoders offers many advantages. Recall from section 6.4.1 that there are many advantages to depth in a feedfor- ward network. Because autoencoders are feedforward networks, these advantages also apply to autoencoders.\n\nMoreover, the encoder is itself a feedforward network, as is the decoder, so each of these components of the autoencoder can individually benefit from depth. One major advantage of nontrivial depth is that the universal approximator theorem guarantees that a feedforward neural network with at least one hidden layer can represent an approximation of any function (within a broad class) to an  505  CHAPTER 14. AUTOENCODERS  arbitrary degree of accuracy, provided that it has enough hidden units", "4940db58-8a47-4a11-a43c-69df89f8cda4": "Thus, it is relaxed to be:  Le = Dj; + log ( SS exp(e \u2014 Dix) + SS exp(\u20ac \u2014 D;)) (,kK)EN G DEN  In the paper, they also proposed to enhance the quality of negative samples in each batch by actively incorporating difficult negative samples given a few random positive pairs. N-pair Loss  Multi-Class N-pair loss  generalizes triplet loss to include comparison with multiple negative samples", "5069190a-ca92-4fc6-b411-418dba647f2a": "To gain some intuition for the second term, log >); exp (z;), observe  181  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  that this term can be roughly approximated by max, z;. This approximation is based on the idea that exp(z;,) is insignificant for any z, that is noticeably less than max; 2;. The intuition we can gain from this approximation is that the negative log-likelihood cost function always strongly penalizes the most active incorrect prediction. If the correct answer already has the largest input to the softmax, then the \u2014z; term and the log); exp(zj) \u00a9 max;z; = % terms will roughly cancel. This example will then contribute little to the overall training cost, which will be dominated by other examples that are not yet correctly classified", "06cbd398-eb4e-4b63-8253-5f05f93de165": "For large values of M, there is a clear advantage in working with the logistic regression model directly. We now use maximum likelihood to determine the parameters of the logistic regression model. To do this, we shall make use of the derivative of the logistic sigmoid function, which can conveniently be expressed in terms of the sigmoid function itself Exercise 4.12 For a data set {\u03c6n, tn}, where tn \u2208 {0, 1} and \u03c6n = \u03c6(xn), with n = 1, . , N, the likelihood function can be written where t = (t1, . , tN)T and yn = p(C1|\u03c6n). As usual, we can de\ufb01ne an error function by taking the negative logarithm of the likelihood, which gives the crossentropy error function in the form where yn = \u03c3(an) and an = wT\u03c6n. Taking the gradient of the error function with respect to w, we obtain Exercise 4.13 where we have made use of (4.88)", "8e7b4788-2f28-46ae-905c-38aecbae7430": "However, in the case of a mean field assumption on binary hidden units (the case we are developing here) there is no loss of generality resulting from fixing a parametrization of the model in advance. We parametrize @ as a product of Bernoulli distributions; that is, we associate the probability of each element 3 nh with a parameter. Specifically, for each ds A\u00ae = Qn) =1]|v), where i Je , and for each k, WO) = Qin? =1]|v),  where A@) \u20ac . Thus we have the following approximation to the posterior: Qa, AP |v) = Qiny?\n\n|v) Q(hy\u201d |\u00bb) (20.31) j k  > (1) S \u2014 pA) * (2) > \u2014 p72) = TOP LFA \u00abGPL \u2014 ABO  Il ll (20.32)  https://www.deeplearningbook.org/contents/generative_models.html    Of course, for DBMs with more layers, the approximate posterior parametrization can be extended in the obvious way, exploiting the bipartite structure of the graph  664  CHAPTER 20", "03f0c36e-a00f-407b-9415-836437f572a1": "Hasan, Kathy Lee, Vivek Datla, Ashequl Qadir, Joey Liu, and Oladimeji Farri. 2016. Neural paraphrase generation with stacked residual lstm networks. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\n\n2019. Language models are unsupervised multitask learners. OpenAI Blog, 1(8):9. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a uni\ufb01ed text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367. Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che. 2019. Generating natural language adversarial examples through probability weighted word saliency. In Proceedings of the 57th annual meeting of the association for computational linguistics, pages 1085\u2013 1097", "27c1a877-cdd3-49fd-abe6-a300e8201410": "Bishop, C. M. Model-based machine learning. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 371, 20120222. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners.\n\nAdvances in neural information processing systems, 33, 1877\u20131901. Chu, C., Blanchet, J., & Glynn, P. Probability functional descent: A unifying perspective on GANs, variational inference, and reinforcement learning. International Conference on Machine Learning, 1213\u20131222. Csisz\u00e1r, I. I-divergence geometry of probability distributions and minimization problems. The Annals of Probability, 146\u2013158. Dagan, I., & Engelson, S. P", "7da06583-a00d-4d75-9d88-37786317dd8d": "Machine learning methods that learn to mimic input\u2013output examples in this way are called supervised learning methods, and when the outputs are numbers, like u, the process is often called function approximation. Function approximation methods expect to receive examples of the desired input\u2013output behavior of the function they are trying to approximate. We use these methods for value prediction simply by passing to them the s 7! u of each update as a training example. We then interpret the approximate function they produce as an estimated value function.\n\nViewing each update as a conventional training example in this way enables us to use any of a wide range of existing function approximation methods for value prediction. In principle, we can use any method for supervised learning from examples, including arti\ufb01cial neural networks, decision trees, and various kinds of multivariate regression. However, not all function approximation methods are equally well suited for use in reinforcement learning. The most sophisticated arti\ufb01cial neural network and statistical methods all assume a static training set over which multiple passes are made. In reinforcement learning, however, it is important that learning be able to occur online, while the agent interacts with its environment or with a model of its environment", "358b2153-28ad-4312-b94e-a0e8a91b5a1f": "As \u03bb is increased, so an increasing number of parameters are driven to zero. Regularization allows complex models to be trained on data sets of limited size without severe over-\ufb01tting, essentially by limiting the effective model complexity. However, the problem of determining the optimal model complexity is then shifted from one of \ufb01nding the appropriate number of basis functions to one of determining a suitable value of the regularization coef\ufb01cient \u03bb. We shall return to the issue of model complexity later in this chapter. For the remainder of this chapter we shall focus on the quadratic regularizer (3.27) both for its practical importance and its analytical tractability. So far, we have considered the case of a single target variable t. In some applications, we may wish to predict K > 1 target variables, which we denote collectively by the target vector t", "6504b1b0-3d69-4c4d-af98-6c4f9e14a87f": "In this section, we have seen that the maximum entropy formalism provides an alternative insight into the classical learning frameworks of MLE, Bayesian inference, and posterior regularization.\n\nIt provides a general expression of these three paradigms as a constrained optimization problem, with a paradigm-speci\ufb01c loss on the model parameters \u03b8 and an auxiliary distribution q, over a properly designed constraint space Q where q must reside: In particular, the use of the auxiliary distribution q converts the originally highly complex problem of directly optimizing \u03b8 against data, to an alternating optimization problem over q and \u03b8, which is algorithmically easier to solve since q often acts as an easy-to-optimize proxy to the target model. The auxiliary q can also be more \ufb02exibly updated to absorb in\ufb02uence from data or constraints, o\ufb00ering a teacher-student\u2013style iterative mechanism to incrementally update \u03b8 as we will see in the sequel. By reformulating learning as a constrained optimization problem, the maximum entropy point of view also o\ufb00ers a great source of \ufb02exibility for applying many powerful tools for e\ufb03cient approximation and enhanced learning, such as variational approximation  and Xing et al", "6f8d0625-5acc-424d-9b8f-16e189212d9f": "Given the deterministic mapping z = g\u03c6(\u03f5, x) we know that q\u03c6(z|x) \ufffd 1Note that for in\ufb01nitesimals we use the notational convention dz = \ufffd Take, for example, the univariate Gaussian case: let z \u223c p(z|x) = N(\u00b5, \u03c32). In this case, a valid reparameterization is z = \u00b5 + \u03c3\u03f5, where \u03f5 is an auxiliary noise variable \u03f5 \u223c N(0, 1). Therefore, EN (z;\u00b5,\u03c32)  = EN (\u03f5;0,1)  \u2243 1 For which q\u03c6(z|x) can we choose such a differentiable transformation g\u03c6(.) and auxiliary variable \u03f5 \u223c p(\u03f5)? Three basic approaches are: 1. Tractable inverse CDF. In this case, let \u03f5 \u223c U(0, I), and let g\u03c6(\u03f5, x) be the inverse CDF of q\u03c6(z|x). Examples: Exponential, Cauchy, Logistic, Rayleigh, Pareto, Weibull, Reciprocal, Gompertz, Gumbel and Erlang distributions", "be6d4501-27e2-4412-96ad-349ef6649865": "The  computational graph is illustrated in figure 6.11. The computational graph for the gradient of this example is large enough that it would be tedious to draw or to read. This demonstrates one of the benefits of the back-propagation algorithm, which is that it can automatically generate gradients that would be straightforward but tedious for a software engineer to derive manually. We can roughly trace out the behavior of the back-propagation algorithm by looking at the forward propagation graph in figure 6.11. To train, we wish to compute both Vya)J and Vy. There are two different paths leading backward from J to the weights: one through the cross-entropy cost, and one through the weight decay cost.\n\nThe weight decay cost is relatively simple; it will always contribute 2\\W to the gradient on W.  https://www.deeplearningbook.org/contents/mlp.html    The other path through the cross- entropy cost is slightly moEs complicated. Let @ be the gradient on the unnormalized log probabilities provided by the cross_entropy operation. The back-propagation algorithm now needs to  215  CHAPTER 6", "de4d3fab-0a0c-4d9c-9413-31819f1a8689": "The major limitation is that standard language models are unidirectional, and this limits the choice of architectures that can be used during pre-training. For example, in OpenAI GPT, the authors use a left-toright architecture, where every token can only attend to previous tokens in the self-attention layers of the Transformer .\n\nSuch restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying \ufb01netuning based approaches to token-level tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the \ufb01ne-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers. BERT alleviates the previously mentioned unidirectionality constraint by using a \u201cmasked language model\u201d (MLM) pre-training objective, inspired by the Cloze task . The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked arXiv:1810.04805v2    24 May 2019 word based only on its context", "fe2350a3-6d89-4772-acf3-c0596e28e608": "Now consider a probability density px(x) that corresponds to a density py(y) with respect to the new variable y, where the suf\ufb01ces denote the fact that px(x) and py(y) are different densities.\n\nObservations falling in the range (x, x + \u03b4x) will, for small values of \u03b4x, be transformed into the range (y, y + \u03b4y) where px(x)\u03b4x \u2243 py(y)\u03b4y, and hence One consequence of this property is that the concept of the maximum of a probability density is dependent on the choice of variable. Exercise 1.4 The probability that x lies in the interval (\u2212\u221e, z) is given by the cumulative which satis\ufb01es P \u2032(x) = p(x), as shown in Figure 1.12. If we have several continuous variables x1, . , xD, denoted collectively by the vector x, then we can de\ufb01ne a joint probability density p(x) = p(x1, . , xD) such that the probability of x falling in an in\ufb01nitesimal volume \u03b4x containing the point x is given by p(x)\u03b4x", "383302cd-96ac-4558-907e-6f131b6a634a": "Now consider the case of a directed graph in which some of the nodes are instantiated with observed values. We can in principle extend the above procedure, at least in the case of nodes representing discrete variables, to give the following logic sampling approach , which can be seen as a special case of importance sampling discussed in Section 11.1.4. At each step, when a sampled value is obtained for a variable zi whose value is observed, the sampled value is compared to the observed value, and if they agree then the sample value is retained and the algorithm proceeds to the next variable in turn. However, if the sampled value and the observed value disagree, then the whole sample so far is discarded and the algorithm starts again with the \ufb01rst node in the graph.\n\nThis algorithm samples correctly from the posterior distribution because it corresponds simply to drawing samples from the joint distribution of hidden variables and data variables and then discarding those samples that disagree with the observed data (with the slight saving of not continuing with the sampling from the joint distribution as soon as one contradictory value is observed). However, the overall probability of accepting a sample from the posterior decreases rapidly as the number of observed variables increases and as the number of states that those variables can take increases, and so this approach is rarely used in practice", "3bb26910-3626-4145-9948-aa21147a09db": "Thus, we can write q\u21e4 in terms of v\u21e4 as follows: shows the contours of a possible optimal action-value function q\u21e4(s, driver). These are the values of each state if we \ufb01rst play a stroke with the driver and afterward select either the driver or the putter, whichever is better.\n\nThe driver enables us to hit the ball farther, but with less accuracy. We can reach the hole in one shot using the driver only if we are already very close; thus the \u22121 contour for q\u21e4(s, driver) covers only a small portion of the green. If we have two strokes, however, then we can reach the hole from much farther away, as shown by the \u22122 contour. In this case we don\u2019t have to drive all the way to within the small \u22121 contour, but only to anywhere on the green; from there we can use the putter. The optimal action-value function gives the values after committing to a particular \ufb01rst action, in this case, to the driver, but afterward using whichever actions are best. The \u22123 contour is still farther out and includes the starting tee", "4f8f96c9-cfdd-4912-a465-4f87d5d883e1": "This type of model is illustrated using a lattice diagram in Figure 13.10. Many applications of hidden Markov models, for example speech recognition, or on-line character recognition, make use of left-to-right architectures. As an illustration of the left-to-right hidden Markov model, we consider an example involving handwritten digits. This uses on-line data, meaning that each digit is represented by the trajectory of the pen as a function of time in the form of a sequence of pen coordinates, in contrast to the off-line digits data, discussed in Appendix A, which comprises static two-dimensional pixellated images of the ink. Examples of the online digits are shown in Figure 13.11. Here we train a hidden Markov model on a subset of data comprising 45 examples of the digit \u20182\u2019.\n\nThere are K = 16 states, each of which can generate a line segment of \ufb01xed length having one of 16 possible angles, and so the emission distribution is simply a 16 \u00d7 16 table of probabilities associated with the allowed angle values for each state index value", "7d70febb-3340-4a27-b5c3-542bd529c08d": "We call this the Bellman error at state s: which shows clearly the relationship of the Bellman error to the TD error (11.3). The Bellman error is the expectation of the TD error. The vector of all the Bellman errors, at all states, \u00af\u03b4w 2 R|S|, is called the Bellman error vector (shown as BE in Figure 11.3). The overall size of this vector, in the norm, is an overall measure of the error in the value function, called the Mean Squared Bellman Error: It is not possible in general to reduce the BE to zero (at which point vw = v\u21e1), but for linear function approximation there is a unique value of w for which the BE is minimized.\n\nThis point in the representable-function subspace (labeled min BE in Figure 11.3) is di\u21b5erent in general from that which minimizes the VE (shown as \u21e7v\u21e1). Methods that seek to minimize the BE are discussed in the next two sections. The Bellman error vector is shown in Figure 11.3 as the result of applying the Bellman operator B\u21e1 : R|S| ! R|S| to the approximate value function", "dbea66d9-bee1-4a02-8f2b-74c0d52998e1": "This representation has a single feature for each component CS present on a trial, where the feature has value 1 whenever that component is present, and 0 otherwise.4 The presence representation is not a realistic hypothesis about how stimuli are represented in an animal\u2019s brain, but as we describe below, the TD model with this representation can produce many of the timing phenomena seen in classical conditioning.\n\nFor the CSC representation (left column of Figure 14.1), the onset of each external stimulus initiates a sequence of precisely-timed short-duration internal signals that 4In our formalism, there is a di\u21b5erent state, St, for each time step t during a trial, and for a trial in which a compound CS consists of n component CSs of various durations occurring at various times throughout the trial, there is a feature, xi, for each component CSi, i = 1, . , n, where xi(St) = 1 for all times t when the CSi is present, and equals zero otherwise", "c7c5ce0c-24a0-4dc5-a433-76e90d42a2d3": "The every-visit methods for ordinary and weighed importance sampling are both biased, though, again, the bias falls asymptotically to zero as the number of samples increases. In practice, every-visit methods are often preferred because they remove the need to keep track of which states have been visited and because they are much easier to extend to approximations. A complete every-visit MC algorithm for o\u21b5-policy policy evaluation using weighted importance sampling is given in the next section on page 110. that transitions back to the nonterminal state with probability p and transitions to the terminal state with probability 1\u2212p. Let the reward be +1 on all transitions, and let \u03b3 =1. Suppose you observe one episode that lasts 10 steps, with a return of 10.\n\nWhat are the \ufb01rst-visit and every-visit estimators of the value of the nonterminal state? \u21e4 both ordinary and weighted importance-sampling methods to estimate the value of a single blackjack state (Example 5.1) from o\u21b5-policy data. Recall that one of the advantages of Monte Carlo methods is that they can be used to evaluate a single state without forming estimates for any other states", "c57ec3db-5655-4ed0-a650-9b80fe383ef5": "Xu, X., Xie, T., Hu, D., Lu, X. Kernel least-squares temporal di\u21b5erence learning. International Journal of Information Technology, 11(9):54\u201363. Yagishita, S., Hayashi-Takagi, A., Ellis-Davies, G. C. R., Urakubo, H., Ishii, S., Kasai, H. A critical time window for dopamine actions on the structural plasticity of dendritic spines. Science, 345:1616\u20131619. Yee, R. C., Saxena, S., Utgo\u21b5, P. E., Barto, A. G. Explaining temporal di\u21b5erences to create useful concepts for evaluating states. In Proceedings of the Eighth National Conference on Arti\ufb01cial Intelligence (AAAI-90), pp. 882\u2013888. AAAI Press, Menlo Park, CA. Yin, H. H., Knowlton, B", "ba7da8c8-cd14-4106-969d-de9a4f89f9a7": "We can use the technique of ancestral sampling to generate random samples Section 8.1.2 distributed according to the Gaussian mixture model. To do this, we \ufb01rst generate a value for z, which we denote \ufffdz, from the marginal distribution p(z) and then generate a value for x from the conditional distribution p(x|\ufffdz). Techniques for sampling from standard distributions are discussed in Chapter 11. We can depict samples from the joint distribution p(x, z) by plotting points at the corresponding values of x and then colouring them according to the value of z, in other words according to which Gaussian component was responsible for generating them, as shown in Figure 9.5(a). Similarly samples from the marginal distribution p(x) are obtained by taking the samples from the joint distribution and ignoring the values of z. These are illustrated in Figure 9.5(b) by plotting the x values without any coloured labels", "abb83a3f-f34e-467a-894a-9b15ed31a326": "The sum in (5.73) runs over all units j to which the input unit i sends connections (for example, over all units in the \ufb01rst hidden layer in the layered topology considered earlier). We now write down a recursive backpropagation formula to determine the derivatives \u2202yk/\u2202aj where the sum runs over all units l to which unit j sends connections (corresponding to the \ufb01rst index of wlj). Again, we have made use of (5.48) and (5.49). This backpropagation starts at the output units for which the required derivatives can be found directly from the functional form of the output-unit activation function. For instance, if we have individual sigmoidal activation functions at each output unit, then \u2202yk \u2202aj = \u03b4kj\u03c3\u2032(aj) (5.75) We can summarize the procedure for evaluating the Jacobian matrix as follows.\n\nApply the input vector corresponding to the point in input space at which the Jacobian matrix is to be found, and forward propagate in the usual way to obtain the activations of all of the hidden and output units in the network", "a10db573-dbc0-4a3b-a815-d03d8d16c271": "If performance on the training set is poor, the learning algorithm is not using the training data that is already available, so there is no reason to gather more data. Instead, try increasing the size of the model by adding more layers or adding more hidden units to each layer. Also, try improving the learning algorithm, for example by tuning the learning rate hyperparameter. If large models and carefully tuned optimization algorithms do not work well, then the problem might be the quality of the training data. The data may be too noisy or may not include the right inputs needed to predict the desired outputs. This suggests starting over, collecting cleaner data, or collecting a richer set of features. If the performance on the training set is acceptable, then measure the per-  421  CHAPTER 11. PRACTICAL METHODOLOGY  formance on a test set.\n\nIf the performance on the test set is also acceptable, then there is nothing left to be done. If test set performance is much worse than training set performance, then gathering more data is one of the most effective solutions. The key considerations are the cost and feasibility of gathering more data, the cost and feasibility of reducing the test error by other means, and the amount of data that is expected to be necessary to improve test set performance significantly", "f94782a1-1da6-4fdb-9891-d0704c9cac79": "https://arxiv.org/abs/2208. 00638 Ma, Y., Tsao, D., & Shum, H.-Y. On the principles of parsimony and self-consistency for the emergence of intelligence. Frontiers of Information Technology & Electronic Engineering, 23(9), 1298\u20131323. Mintz, M., Bills, S., Snow, R., & Jurafsky, D. Distant supervision for relation extraction without labeled data. Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, 1003\u20131011. Mnih, A., & Gregor, K. Neural variational inference and learning in belief networks. International Conference on Machine Learning, 1791\u20131799. Mohamed, S., & Lakshminarayanan, B. Learning in implicit generative models. arXiv. Neal, R. M., & Hinton, G. E", "d1149db4-a61a-49be-b640-65081dca737b": "In the positive phase, we increase log p(x) for \u00ab drawn from the data. In he negative phase, we decrease the partition function by decreasing log p(x) drawn from the model distribution. In the deep learning literature, it is common to parametrize log p in terms of an energy function (equation 16.7).\n\nIn this case, we can interpret the positive phase as pushing down on the energy of training examples and the negative phase as pushing up on the energy of samples drawn from the model, as illustrated in figure 18.1.  https://www.deeplearningbook.org/contents/partition.html    18.2 Stochastic Maximum Likelihood and Contrastive Divergence  The naive way of implementing equation 18.15 is to compute it by burning in a set of Markov chains from a random initialization every time the gradient is needed. When learning is performed using stochastic gradient descent, this means the chains must be burned in once per gradient step. This approach leads to the  605  CHAPTER 18", "bc52029e-6340-4240-ba7e-f918d59c8a27": "Moreover, there may be many different clusterings that all correspond well to some property of the real world. We may hope to find a clustering that relates to one feature but obtain a different, equally valid clustering that is not relevant to our task. For example, suppose that we run two clustering algorithms on a dataset consisting of images of red trucks, images of red cars, images of gray trucks, and images of gray cars. If we ask each clustering algorithm to find two clusters, one algorithm may find a cluster of cars and a cluster of trucks, while another may find a cluster of red vehicles and a cluster of gray vehicles. Suppose we also run a third clustering algorithm, which is allowed to determine the number of clusters. This may assign the examples to four clusters, red cars, red trucks, gray cars, and gray trucks. This  new clustering now at least captures information about both attributes, but it has lost information about similarity. Red cars are in a different cluster from gray cars, just as they are in a different cluster from gray trucks.\n\nThe output of the clustering algorithm does not tell us that red cars are more similar to gray cars than they are to gray trucks", "a481e67c-96f3-4f1f-abaa-0cd6067e0045": "Marc\u2019Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2016. Sequence level training with recurrent neural networks. In ICLR. Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. 2017. Self-critical sequence training for image captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7008\u20137024. Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. 2020. Beyond accuracy: Behavioral testing of nlp models with checklist. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4902\u2013 4912.\n\nJohn Schulman, Xi Chen, and Pieter Abbeel. 2017. Equivalence between policy gradients and soft Qlearning. arXiv preprint arXiv:1704.06440", "d713db2e-3f04-4ed0-b1d6-6d42e99f675f": "A key feature of the SE as an objective function is that the formulation is agnostic to the speci\ufb01c choices of the target model. That is, one can use the SE to train e\ufb00ectively anything with a learnable component (denoted as \u03b8 in general), ranging from black-box neural networks, to probabilistic graphical models, symbolic models, and the combinations thereof. We will mention a few examples Deep neural networks of any architectures.\n\nThe target model can be a deep neural network of an arbitrary architecture, composed of di\ufb00erent neural layers and units. Figure 5, right panel, illustrates a set of common neural components and how they are composed to form more complex ones at di\ufb00erent levels of granularity. Neural models of di\ufb00erent architectures are used for di\ufb00erent tasks of interest. For example, an image classi\ufb01er p\u03b8(y|x), with input image x and output object label y, typically consists of an encoder and a classi\ufb01cation layer (a.k.a., classi\ufb01cation head), where the encoder can be a ConvNet  or transformer  and the classi\ufb01cation layer is often a simple multilayer perceptron (MLP) consisting of several neural dense layers", "f9089829-916b-42dd-bc68-4e4ecb457f67": "For example, if a user provides two knowledge bases for distant supervision, how should a data point that matches only one knowledge base be labeled? Some researchers have used multi-instance learning to reduce the noise in weak supervision sources , essentially modeling the different weak supervision sources as soft constraints on the true label, but this approach is limited because it requires using a speci\ufb01c end model that supports multi-instance learning. Researchers have therefore considered how to estimate the accuracy of label sources without a gold standard with which to compare\u2014a classic problem \u2014and combine these estimates into labels that can be used to train an arbitrary end model. Much of this work has focused on crowdsourcing, in which workers have unknown accuracy . Such methods use generative probabilistic models to estimate a latent variable\u2014the true class label\u2014 based on noisy observations.\n\nOther methods use generative models with hand-speci\ufb01ed dependency structures to label data for speci\ufb01c modalities, such as topic models for text  or denoising distant supervision sources . Other techniques for estimating latent class labels given noisy observations include spectral methods . Snorkel is distinguished from these approaches because its generative model supports a wide range of weak supervision sources, and it learns the accuracies and correlation structure among weak supervision sources without ground truth data", "adc2c84b-1314-45a3-836f-6dc16ad3d859": "For example, video games can automatically generate textures for large objects or landscapes, rather than requiring an artist to manually label cach pixel . In some cases, we want the sampling or synthesis procedure to generate a specific kind of output given the input. For example, in a speech synthesis task, we provide a written sentence and ask the program to emit an audio waveform containing a spoken version of that sentence. This is a kind of structured output task, but with the added qualification that there is no single correct output for each input, and we explicitly desire a large amount of variation in the output, in order for the output to seem more natural and realistic.\n\ne Imputation of missing values: In this type of task, the machine learning algorithm is given a new example a \u20ac R\u201d, but with some entries x; of x  100  CHAPTER 5. MACHINE LEARNING BASICS  missing. The algorithm must provide a prediction of the values of the missing entries. e Denoising: In this type of task, the machine learning algorithm is given as input a corrupted ecample & \u20ac IR\u201d obtained by an unknown corruption process from a clean example x \u20ac R\u201d", "c775b8c2-926b-4f38-a5f9-ad334001345a": "Becker, and Z. Ghahramani (Eds. ), Advances in Neural Information Processing Systems, Volume 14, pp. 617\u2013624. MIT Press. Comon, P., C. Jutten, and J. Herault . Blind source separation, 2: problems statement. Signal Processing 24(1), 11\u201320. Corduneanu, A. and C. M. Bishop . Variational Bayesian model selection for mixture distributions. In T. Richardson and T. Jaakkola (Eds. ), Proceedings Eighth International Conference on Arti\ufb01cial Intelligence and Statistics, pp. 27\u201334. Morgan Kaufmann. Cormen, T. H., C. E. Leiserson, R. L. Rivest, and C. Stein . Introduction to Algorithms (Second ed.). MIT Press. Cortes, C. and V", "61f1132b-a403-4b21-8d87-9fb506caa8d1": "However, instead of using TD(\u03bb) as TD-Gammon did, DQN used the semi-gradient form of Q-learning. TD-Gammon estimated the values of afterstates, which were easily obtained from the rules for making backgammon moves. To use the same algorithm for the Atari games would have required generating the next states for each possible action (which would not have been afterstates in that case). This could have been done by using the game emulator to run single-step simulations for all the possible actions (which ALE makes possible). Or a model of each game\u2019s state-transition function could have been learned and used to predict next states . While these methods might have produced results comparable to DQN\u2019s, they would have been more complicated to implement and would have signi\ufb01cantly increased the time needed for learning.\n\nAnother motivation for using Q-learning was that DQN used the experience replay method, described below, which requires an o\u21b5-policy algorithm. Being model-free and o\u21b5-policy made Q-learning a natural choice. Before describing the details of DQN and how the experiments were conducted, we look at the skill levels DQN was able to achieve", "60c8f2e4-dd71-4e3b-8d34-e0f0227aae16": "If y is one of these (or closely related to one of them), then it will be easy to learn to predict y from such a representation. We also see that the conditional distribution of y given x is tied by Bayes\u2019 rule to the components in the above equation:  p(x | y)ply) (15.3)  P(x)  Thus the marginal p(x) is intimately tied to the conditional p(y | x), and knowledge of the structure of the former should be helpful to learn the latter. Therefore, in situations respecting these assumptions, semi-supervised learning should improve performance. ply | x) =  An important research problem regards the fact that most observations are formed by an extremely large number of underlying causes. Suppose y = h;, but the unsupervised learner does not know which h;", "950e4bbe-eaf8-473e-8c11-1e3539dd9936": "19.5.1 Wake-Sleep  One of the main difficulties with training a model to infer h from v is that we do not have a supervised training set with which to train the model. Given a v, we do not know the appropriate h. The mapping from v to h depends on the choice of model family, and evolves throughout the learning process as 6 changes. ML ~ ---.1-. Tan 21m n tthe (TT nt ~T AMAEL. Thn-- 24 ~1) 1ANEAN ~A2~A21--.~ 4  https://www.deeplearningbook.org/contents/inference.html    LUC WaAKE-SICCP ALYOULILILIL (AL1ULOl Cl Ub, LIIIV, ricy cb ul. 1ygIV) LESVULVES LILIS problem by drawing samples of both h and V from the model distribution.\n\nFor example, in a directed model, this can be done cheaply by performing ancestral sampling beginning at h and ending at v. The inference network can then be  trained to perform the reverse mapping: predicting which h caused the present v", "7028309d-6425-4670-ae6f-003470796296": "The multi-step o\u21b5-policy methods presented in the previous section are simple and conceptually clear, but are probably not the most e\ufb03cient. A more sophisticated approach would use per-decision importance sampling ideas such as were introduced in Section 5.9. To understand this approach, \ufb01rst note that the ordinary n-step return (7.1), like all returns, can be written recursively. For the n steps ending at horizon h, the n-step return can be written where Gh:h .= Vh\u22121(Sh).\n\n(Recall that this return is used at time h, previously denoted t + n.) Now consider the e\u21b5ect of following a behavior policy b that is not the same as the target policy \u21e1. All of the resulting experience, including the \ufb01rst reward Rt+1 and the next state St+1, must be weighted by the importance sampling ratio for time t, \u21e2t = \u21e1(At|St) b(At|St) . One might be tempted to simply weight the righthand side of the above equation, but one can do better. Suppose the action at time t would never be selected by \u21e1, so that \u21e2t is zero", "a9b3024b-003f-4257-ac6e-4ba15d83f300": "The strategy then is to \ufb01nd a compact, e\ufb03cient way of computing t from the one before. If this is done, for the linear case in which \u02c6v(s,w) = w>x(s), This algorithm has been proven to produce exactly the same sequence of weight vectors, wt, 0 \uf8ff t \uf8ff T, as the online \u03bb-return algorithm . Thus the results on the random walk task on the left of Figure 12.8 are also its results on that task. Now, however, the algorithm is much less expensive. The memory requirements of true online TD(\u03bb) are identical to those of conventional TD(\u03bb), while the per-step computation is increased by about 50% (there is one more inner product in the eligibility-trace update). Overall, the per-step computational complexity remains of O(d), the same as TD(\u03bb). Pseudocode for the complete algorithm is given in the box", "265f665f-8ea7-4ed9-9884-2e4cc6441590": "In practice, various other measures have been used to minimize the difference between f\u02c6\u03b8(xu) and f\u03b8(\u03b1(xu)), such as the KL divergence  and the mean-squared error .\n\nBecause gradients are not allowed to \ufb02ow through the model when it was fed the clean unlabeled input xu, this objective can be viewed as using the clean unlabeled datapoint to generate a synthetic target distribution for the augmented unlabeled datapoint. Xie et al. showed that consistency training can be effectively applied to semi-supervised learning for NLP. To achieve stronger results, they introduce several other tricks including con\ufb01dence thresholding, training signal annealing, and entropy minimization. Con\ufb01dence thresholding applies the unsupervised loss only when the model assigns a class probability above a pre-de\ufb01ned threshold. Training signal annealing prevents the model from over\ufb01tting on easy examples by applying the supervised loss only when the model is less con\ufb01dent about predictions. Entropy minimization trains the model to output low-entropy (highly-con\ufb01dent) predictions when fed unlabeled data", "de99a4b1-701f-4bda-a57a-0744f0cdf298": "In general, a Markov chain with transition operator T will converge, under mild conditions, to a fixed point described by the equation  d(x) = ExeqT(x' | x), (17.24) 594  CHAPTER 17. MONTE CARLO METHODS  which in the discrete case is just rewriting equation 17.23. When x is discrete, the expectation corresponds to a sum, and when x is continuous, the expectation corresponds to an integral. Regardless of whether the state is continuous or discrete, all Markov chain methods consist of repeatedly applying stochastic updates until eventually the state begins to yield samples from the equilibrium distribution.\n\nRunning the Markov chain until it reaches its equilibrium distribution is called burning in the Markov chain. After the chain has reached equilibrium, a sequence of infinitely many samples may be drawn from the equilibrium distribution. They are identically distributed, but any two successive samples will be highly correlated with each other", "715fa57d-9422-428e-ad8b-c54bfe554e74": "Widrow, Gupta, and Maitra  presented a neuron-like linear threshold unit implementing a learning process they called learning with a critic or selective bootstrap adaptation, a reinforcement-learning variant of the ADALINE algorithm. Werbos  developed an approach to prediction and control that uses ANNs trained by error backpropation to learn policies and value functions using TD-like algorithms. Barto, Sutton, and Brouwer  and Barto and Sutton  extended the idea of an associative memory network  to reinforcement learning. Barto, Anderson, and Sutton  used a two-layer ANN to learn a nonlinear control policy, and emphasized the \ufb01rst layer\u2019s role of learning a suitable representation. Hampson  was an early proponent of multilayer ANNs for learning value functions. Barto, Sutton, and Anderson  presented an actor\u2013critic algorithm in the form of an ANN learning to balance a simulated pole (see Sections 15.7 and 15.8)", "15c78ec6-1ba8-4e72-be6a-21e926b35244": "Hence prove the results (1.57) and (1.58).\n\n1.13 (\u22c6) Suppose that the variance of a Gaussian is estimated using the result (1.56) but with the maximum likelihood estimate \u00b5ML replaced with the true value \u00b5 of the mean. Show that this estimator has the property that its expectation is given by the true variance \u03c32. so that the contribution from the anti-symmetric matrix vanishes. We therefore see that, without loss of generality, the matrix of coef\ufb01cients wij can be chosen to be symmetric, and so not all of the D2 elements of this matrix can be chosen independently. Show that the number of independent parameters in the matrix wS ij is given by D(D + 1)/2. 1.15 (\u22c6 \u22c6 \u22c6) www In this exercise and the next, we explore how the number of independent parameters in a polynomial grows with the order M of the polynomial and with the dimensionality D of the input space", "cf33dea4-74be-4663-ba3f-d8cfccf769d4": "Meta-Learning: Learning to Learn Fast | Lil'Log   Posts Archive Stak Logags FAQ emojisearch.app  Meta-Learning: Learning to Learn Fast  Table of Contents    A good machine learning model often requires training with a large number of samples. Humans, in contrast, learn new concepts and skills much faster and more efficiently. Kids who have seen cats and birds only a few times can quickly tell them apart. People who know how to ride a bike are likely to discover the way to ride a motorcycle fast with little or even no demonstration. Is it possible to design a machine learning model with similar properties \u2014 learning new concepts and skills fast with a few training examples? That's essentially what meta-learning aims to solve. We expect a good meta-learning model capable of well adapting or generalizing to new tasks and new environments that have never been encountered during training time. The adaptation process, essentially a mini learning session, happens during test but with a limited exposure to the new task configurations. Eventually, the adapted model can complete new tasks. This is why meta-learning is also known as learning to learn", "7d20383b-6fdf-4942-9f21-73cb2b56de9a": "Is there a way to invert this forward-view algorithm to produce an e\ufb03cient backward-view algorithm using eligibility traces? It turns out that there is indeed an exact computationally congenial implementation of the online \u03bb-return algorithm for the case of linear function approximation. This implementation is known as the true online TD(\u03bb) algorithm because it is \u201ctruer\u201d to the ideal of the online \u03bb-return algorithm than the TD(\u03bb) algorithm is. The derivation of true online TD(\u03bb) is a little too complex to present here (see the next section and the appendix to the paper by van Seijen et al., 2016) but its strategy is simple. The sequence of weight vectors produced by the online \u03bb-return algorithm can be arranged in a triangle: One row of this triangle is produced on each time step.\n\nIt turns out that the weight vectors on the diagonal, the wt t, plays a role in bootstrapping in the n-step returns of the updates. In the \ufb01nal algorithm the diagonal weight vectors are renamed without a superscript, wt .= wt t", "47da7d22-f019-4968-9584-b403e6fdc1c9": "We now illustrate the factorized variational approximation using a Gaussian distribution over a single variable x . Our goal is to infer the posterior distribution for the mean \u00b5 and precision \u03c4, given a data set D = {x1, . , xN} of observed values of x which are assumed to be drawn independently from the Gaussian. The likelihood function is given by We now introduce conjugate prior distributions for \u00b5 and \u03c4 given by where Gam(\u03c4|a0, b0) is the gamma distribution de\ufb01ned by (2.146). Together these distributions constitute a Gaussian-Gamma conjugate prior distribution. Section 2.3.6 For this simple problem the posterior distribution can be found exactly, and again takes the form of a Gaussian-gamma distribution. However, for tutorial purposes Exercise 2.44 Note that the true posterior distribution does not factorize in this way. The optimum factors q\u00b5(\u00b5) and q\u03c4(\u03c4) can be obtained from the general result (10.9) as follows. For q\u00b5(\u00b5) we have Note that for N \u2192 \u221e this gives the maximum likelihood result in which \u00b5N = x and the precision is in\ufb01nite", "86260ecf-7078-4fca-9368-a6afa4ab5d1d": "Figure 7.2 shows an example of the classi\ufb01cation resulting from training a support vector machine on a simple synthetic data set using a Gaussian kernel of the form (6.23). Although the data set is not linearly separable in the two-dimensional data space x, it is linearly separable in the nonlinear feature space de\ufb01ned implicitly by the nonlinear kernel function. Thus the training data points are perfectly separated in the original data space. This example also provides a geometrical insight into the origin of sparsity in the SVM. The maximum margin hyperplane is de\ufb01ned by the location of the support vectors. Other data points can be moved around freely (so long as they remain outside the margin region) without changing the decision boundary, and so the solution will be independent of such data points. So far, we have assumed that the training data points are linearly separable in the feature space \u03c6(x).\n\nThe resulting support vector machine will give exact separation of the training data in the original input space x, although the corresponding decision boundary will be nonlinear. In practice, however, the class-conditional distributions may overlap, in which case exact separation of the training data can lead to poor generalization", "213f46b1-18fa-4c13-ac81-3e6f3a34afb9": "Among the few works to consider planning with learned, approximate models are those by Kuvayev and Sutton , Sutton, Szepesvari, Geramifard, and Bowling , Nouri and Littman , and Hester and Stone . The need to be selective in model construction to avoid slowing planning is well known in arti\ufb01cial intelligence. Some of the classic work is by Minton  and Tambe, Newell, and Rosenbloom . Hauskrecht, Meuleau, Kaelbling, Dean, and Boutilier  showed this e\u21b5ect in MDPs with deterministic options. Schmidhuber (1991a, b) proposed how something like curiosity would result if reward signals were a function of how quickly an agent\u2019s environment model is improving. The empowerment function proposed by Klyubin, Polani, and Nehaniv  is an information-theoretic measure of an agent\u2019s ability to control its environment that can function as an intrinsic reward signal. Baldassarre and Mirolli  is a collection of contributions by researchers studying intrinsic reward and motivation from both biological and computational perspectives, including a perspective on \u201cintrinsically-motivated reinforcement learning,\u201d to use the term introduced by Singh, Barto, and Chentenez", "566b4e97-d70c-4ac7-9bff-617a3ec704cc": "Another advantage is that there is no generalization error to the encoder. A parametric encoder must learn how to map \u00ab to h in a way that generalizes. For unusual a that do not resemble the training data, a learned parametric encoder may fail to find an h that results in accurate reconstruction or in a sparse code. For the vast majority of formulations  https://www.deeplearningbook.org/contents/linear_factors.html    ol sparse coding models, where the interence problem is convex, the optimization procedure will always find the optimal code (unless degenerate cases such as replicated weight vectors occur). Obviously, the sparsity and reconstruction costs can still rise on unfamiliar points, but this is due to generalization error in the  decoder weights, rather than to generalization error in the encoder", "0b09b09f-8036-430d-ba3c-dbffe4dabd39": "We now derive recursion relations that allow \u03b1(zn) and \u03b2(zn) to be evaluated ef\ufb01ciently. Again, we shall make use of conditional independence properties, in particular (13.25) and (13.26), together with the sum and product rules, allowing us to express \u03b1(zn) in terms of \u03b1(zn\u22121) as follows Making use of the de\ufb01nition (13.34) for \u03b1(zn), we then obtain It is worth taking a moment to study this recursion relation in some detail. Note that there are K terms in the summation, and the right-hand side has to be evaluated for each of the K values of zn so each step of the \u03b1 recursion has computational cost that scaled like O(K2). The forward recursion equation for \u03b1(zn) is illustrated using a lattice diagram in Figure 13.12. In order to start this recursion, we need an initial condition that is given by which tells us that \u03b1(z1k), for k = 1, . , K, takes the value \u03c0kp(x1|\u03c6k)", "cb1c04e4-8513-4dbf-9b89-86ceafa80ef0": "A Simple Framework for Contrastive Learning of Visual Representations Ting Chen 1 Simon Kornblith 1 Mohammad Norouzi 1 Geoffrey Hinton 1 This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive selfsupervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in de\ufb01ning effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning bene\ufb01ts from larger batch sizes and more training steps compared to supervised learning. By combining these \ufb01ndings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classi\ufb01er trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-ofthe-art, matching the performance of a supervised ResNet-50", "9dec651c-ad8e-4f87-b08c-ec919d576455": "Why does the estimated value function jump up for the last two rows in the rear? Why does it drop o\u21b5 for the whole last row on the left? Why are the frontmost values higher in the upper diagrams than in the lower? \u21e4 Exercise 5.2 Suppose every-visit MC was used instead of \ufb01rst-visit MC on the blackjack task. Would you expect the results to be very di\u21b5erent? Why or why not?\n\n\u21e4 Although we have complete knowledge of the environment in the blackjack task, it would not be easy to apply DP methods to compute the value function. DP methods require the distribution of next events\u2014in particular, they require the environments dynamics as given by the four-argument function p\u2014and it is not easy to determine this for blackjack. For example, suppose the player\u2019s sum is 14 and he chooses to stick. What is his probability of terminating with a reward of +1 as a function of the dealer\u2019s showing card? All of the probabilities must be computed before DP can be applied, and such computations are often complex and error-prone. In contrast, generating the sample games required by Monte Carlo methods is easy", "7c8ad243-b55b-413e-ab0b-e54eac1df556": "17(2), 1\u20136  33. Liang, P., Jordan, M.I., Klein, D.: Learning from measurements in exponential families. In: International Conference on Machine Learning (ICML)  34. Mann, G.S., McCallum, A.: Generalized expectation criteria for semi-supervised learning with weakly labeled data. J. Mach. Learn. Res. 11, 955\u2013984  35. Metz, C.: Google\u2019s hand-fed AI now gives answers, not just search results. Wired   36. Mintz, M., Bills, S., Snow, R., Jurafsky, D.: Distant supervision for relation extraction without labeled data. In: Meeting of the Association for Computational Linguistics (ACL)  37. Davis, A.P., Grondin, C.J., Johnson, R.J., Sciaky, D., King, B.L., McMorran, R., Wiegers, J., Wiegers, T., Mattingly, C.J. : The comparative toxicogenomics database: update 2017", "c1e9d41f-896d-452d-85c8-dc4845190ecf": "The original version we introduced above is equivalent to TD(0). Monte-Carlo Temporal-Difference Dynamic Programming V(Sx) \u00a9 V(St) + 0(G: \u2014 V(S:)) V(St) = V(St) +0 (Resa +9 (Ses) ~ V(Se)) V(S.) \u2014 Ex   S,  Policy Gradient  All the methods we have introduced above aim to learn the state/action value function and then to select actions accordingly. Policy Gradient methods instead learn the policy directly with a parameterized function respect to 6, t(als; 6). Let's define the reward function (opposite of loss function) as the expected return and train the algorithm with the goal to maximize the reward function. My next post described why the policy gradient theorem works (proof) and introduced a number of policy gradient algorithms.\n\nIn discrete space:  J (6) = V,(S1) = gl V1]  where \u00a7S; is the initial starting state", "984ada05-488e-4fb1-a389-6abe13fc1f52": "More generally, inference can be performed ef\ufb01ciently using local message passing on a broader class of graphs called trees.\n\nIn particular, we shall shortly generalize the message passing formalism derived above for chains to give the sum-product algorithm, which provides an ef\ufb01cient framework for exact inference in tree-structured graphs. In the case of an undirected graph, a tree is de\ufb01ned as a graph in which there is one, and only one, path between any pair of nodes. Such graphs therefore do not have loops. In the case of directed graphs, a tree is de\ufb01ned such that there is a single node, called the root, which has no parents, and all other nodes have one parent. If we convert a directed tree into an undirected graph, we see that the moralization step will not add any links as all nodes have at most one parent, and as a consequence the corresponding moralized graph will be an undirected tree. Examples of undirected and directed trees are shown in Figure 8.39(a) and 8.39(b). Note that a distribution represented as a directed tree can easily be converted into one represented by an undirected tree, and vice versa", "62f90841-b5a1-44d4-b9c7-7c9507f97760": "The image then passes through the optic nerve and a brain region called the lateral geniculate nucleus.\n\nThe main role, as far as we are concerned here, of both anatomical regions is primarily just to carry the signal from the eye to V1, which is located at the back of the head. A convolutional network layer is designed to capture three properties of V1:  1. V1 is arranged in a spatial map. It actually has a two-dimensional structure,  https://www.deeplearningbook.org/contents/convnets.html    mirroring the structure of the image in the retina. For example, light arriving at the lower half of the retina affects only the corresponding half of V1. Convolutional networks capture this property by having their features defined in terms of two-dimensional maps. 2. V1 contains many simple cells. A simple cell\u2019s activity can to some extent be characterized by a linear function of the image in a small, spatially localized receptive field. The detector units of a convolutional network are designed to emulate these properties of simple cells. 3. V1 also contains many complex cells", "232a842c-bf87-48f5-be2a-275ad32945c7": "In the simplest case, the learning process \u2018jumps\u2019 from option initiation to option termination, with an update only occurring when an option terminates. More subtly, updates can be made on each time step, using \u201cintra-option\u201d learning algorithms, which in general require o\u21b5-policy learning. Perhaps the most important generalization made possible by option ideas is that of the environmental model as developed in Chapters 3, 4, and 8. The conventional model of an action is the state-transition probabilities and the expected immediate reward for taking the action in each state. How do conventional action models generalize to option models? For options, the appropriate model is again of two parts, one corresponding to the state transition resulting from executing the option and one corresponding to the expected cumulative reward along the way. The reward part of an option model, analogous to the expected reward for state\u2013action pairs (3.5), is for all options ! and all states s 2 S, where \u2327 is the random time step at which the option terminates according to \u03b3!.\n\nNote the role of the overall discounting parameter \u03b3 in this equation\u2014discounting is according to \u03b3, but termination of the option is according to \u03b3!. The state-transition part of an option model is a little more subtle", "77c9e6d5-1da0-49f9-92a6-364c2b46ab10": "The conventional full return Gt can be viewed as a sum of \ufb02at partial returns as suggested above as follows: Now we need to scale the \ufb02at partial returns by an importance sampling ratio that is similarly truncated. As \u00afGt:h only involves rewards up to a horizon h, we only need the ratio of the probabilities up to h. We de\ufb01ne an ordinary importance-sampling estimator, analogous to (5.5), as h=t+1 \u03b3h\u2212t\u22121\u21e2t:h\u22121 \u00afGt:h + \u03b3T (t)\u2212t\u22121\u21e2t:T (t)\u22121 \u00afGt:T (t) and a weighted importance-sampling estimator, analogous to (5.6), as h=t+1 \u03b3h\u2212t\u22121\u21e2t:h\u22121 \u00afGt:h + \u03b3T (t)\u2212t\u22121\u21e2t:T (t)\u22121 \u00afGt:T (t) We call these two estimators discounting-aware importance sampling estimators.\n\nThey take into account the discount rate but have no e\u21b5ect (are the same as the o\u21b5-policy estimators from Section 5.5) if \u03b3 = 1", "5c341879-170c-4ed3-8789-8af9e5241f00": "Similarly, for multiple binary classi\ufb01cation problems, each output unit activation is transformed using a logistic sigmoid function so that Finally, for multiclass problems, a softmax activation function of the form (4.62) is used. The choice of output unit activation function is discussed in detail in Section 5.2.\n\nWe can combine these various stages to give the overall network function that, for sigmoidal output unit activation functions, takes the form where the set of all weight and bias parameters have been grouped together into a vector w. Thus the neural network model is simply a nonlinear function from a set of input variables {xi} to a set of output variables {yk} controlled by a vector w of adjustable parameters. This function can be represented in the form of a network diagram as shown in Figure 5.1. The process of evaluating (5.7) can then be interpreted as a forward propagation of information through the network. It should be emphasized that these diagrams do not represent probabilistic graphical models of the kind to be considered in Chapter 8 because the internal nodes represent deterministic variables rather than stochastic ones. For this reason, we have adopted a slightly different graphical notation for the two kinds of model", "abb3a234-48b1-411c-b44e-3b18795efaf1": "Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Proceedings of the 27th International Conference on Neural Information Processing Systems-Volume 2, pages 3104\u20133112. Samson Tan, Sha\ufb01q Joty, Min-Yen Kan, and Richard Socher. 2020. It\u2019s morphin\u2019 time! Combating linguistic discrimination with in\ufb02ectional perturbations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2920\u20132935, Online. Association for Computational Linguistics. Antti Tarvainen and Harri Valpola. 2017. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Advances in neural information processing systems, pages 1195\u20131204. Luke Taylor and Geoff Nitschke. 2018. Improving deep learning with generic data augmentation", "98c633ce-51a9-49e0-8892-4691f0536be5": "One is in the case of deterministic environments. If the transition to the next state is deterministic, then the two samples will necessarily be the same, and the naive algorithm is valid. The other way is to obtain two independent samples of the next state, St+1, from St, one for the \ufb01rst expectation and another for the second expectation. In real interaction with an environment, this would not seem possible, but when interacting with a simulated environment, it is. One simply rolls back to the previous state and obtains an alternate next state before proceeding forward from the \ufb01rst next state.\n\nIn either of these cases the residual-gradient algorithm is guaranteed to converge to a minimum of the BE under the usual conditions on the step-size parameter. As a true SGD method, this convergence is 1For state values there remains a small di\u21b5erence in the treatment of the importance sampling ratio \u21e2t. In the analagous action-value case (which is the most important case for control algorithms), the residual-gradient algorithm would reduce exactly to the naive version. robust, applying to both linear and nonlinear function approximators. In the linear case, convergence is always to the unique w that minimizes the BE", "e0b7f8ba-e677-4387-9e32-4388d2c21cf9": "This equation makes it clearer what happens when \u03bb = 1. In this case the main sum goes to zero, and the remaining term reduces to the conventional return. Thus, for \u03bb = 1, updating according to the \u03bb-return is a Monte Carlo algorithm. On the other hand, if \u03bb = 0, then the \u03bb-return reduces to Gt:t+1, the one-step return. Thus, for \u03bb = 0, updating according to the \u03bb-return is a one-step TD method. We are now ready to de\ufb01ne our \ufb01rst learning algorithm based on the \u03bb-return: the o\u270fine \u03bb-return algorithm. As an o\u270fine algorithm, it makes no changes to the weight vector during the episode. Then, at the end of the episode, a whole sequence of o\u270fine updates are made according to our usual semi-gradient rule, using the \u03bb-return as the target: The \u03bb-return gives us an alternative way of moving smoothly between Monte Carlo and one-step TD methods that can be compared with the n-step bootstrapping way developed in Chapter 7", "8180561e-9eeb-4a37-8b3c-0fa21a808c55": "get_inputs(V,G): This returns the list of variables that are parents of V in the computational graph G.  Each operation op is also associated with a bprop operation.\n\nThis bprop operation can compute a Jacobian-vector product as described by equation 6.47. This is how the back-propagation algorithm is able to achieve great generality. Each operation is responsible for knowing how to back-propagate through the edges in the graph that it participates in. For example, we might use a matrix  211  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  multiplication operation to create a variable C = AB. Suppose that the gradient of a scalar z with respect to C' is given by G. The matrix multiplication operation  oe tee 4 1 1 at 1 c 1 eu  https://www.deeplearningbook.org/contents/mlp.html    is FeSpOusiple LOY GEMM LWO DaCK-propagallOll FULES, OLLE LOY CaCI OL ILS LI put arguments", "bd9d1e8e-7b5a-4070-91a6-970c91f9f45a": "REGULARIZATION FOR DEEP LEARNING  and continues to be featured prominently in modern neural networks . 7.6 Semi-Supervised Learning  In the paradigm of semi-supervised learning, both unlabeled examples from P(x) and labeled examples from P(x,y) are used to estimate P(y | x) or predict y from x. In the context of deep learning, semi-supervised learning usually refers to learning a representation h = f(a). The goal is to learn a representation so that examples from the same class have similar representations. Unsupervised learning can provide useful clues for how to group examples in representation space. Examples that cluster tightly in the input space should be mapped to similar representations. A linear classifier in the new space may achieve better generalization in many cases . A long-standing variant of this approach is the application of principal components analysis as a preprocessing step before applying a classifier (on the projected data)", "e99eaefb-25a2-495a-a86c-e6a4ff9fef82": "In the variational E step, we evaluate the expected suf\ufb01cient statistics E using the current posterior distribution q(zn) over the latent variables and use this to compute a revised posterior distribution q(\u03b7) over the parameters.\n\nThen in the subsequent variational M step, we use this revised parameter posterior distribution to \ufb01nd the expected natural parameters E, which gives rise to a revised variational distribution over the latent variables. We have illustrated the application of variational methods by considering a speci\ufb01c model, the Bayesian mixture of Gaussians, in some detail. This model can be described by the directed graph shown in Figure 10.5. Here we consider more generally the use of variational methods for models described by directed graphs and derive a number of widely applicable results. The joint distribution corresponding to a directed graph can be written using the decomposition p(x) = \ufffd where xi denotes the variable(s) associated with node i, and pai denotes the parent set corresponding to node i. Note that xi may be a latent variable or it may belong to the set of observed variables", "a728ed2c-d034-42b2-ab91-1a3ee3e82dfa": "Bengio, Y., Thibodeau-Laufer, E., Alain, G., and Yosinski, J. Deep generative stochastic networks trainable by backprop. In Proceedings of the 30th International Conference on Machine Learning (ICML\u201914).\n\nBergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde-Farley, D., and Bengio, Y. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scienti\ufb01c Computing Conference (SciPy). Oral Presentation. Breuleux, O., Bengio, Y., and Vincent, P. Quickly generating representative samples from an RBM-derived process. Neural Computation, 23(8), 2053\u20132073. Glorot, X., Bordes, A., and Bengio, Y. Deep sparse recti\ufb01er neural networks. In AISTATS\u20192011. Goodfellow, I", "3d55ee5b-5b55-45b9-bccb-c86ddfccb9af": "The dashed action takes the system to one of the six upper states with equal probability, whereas the solid action takes the system to the seventh state.\n\nThe behavior policy b selects the dashed and solid actions with probabilities 6 7, so that the next-state distribution under it is uniform (the same for all nonterminal states), which is also the starting distribution for each episode. The target policy \u21e1 always takes the solid action, and so the on-policy distribution (for \u21e1) is concentrated in the seventh state. The reward is zero on all transitions. The discount rate is \u03b3 = 0.99. Consider estimating the state-value under the linear parameterization indicated by the expression shown in each state circle. For example, the estimated value of the leftmost state is 2w1 +w8, where the subscript corresponds to the component of the overall weight vector w 2 R8; this corresponds to a feature vector for the \ufb01rst state being x(1) = (2, 0, 0, 0, 0, 0, 0, 1)>. The reward is zero on all transitions, so the true value function is v\u21e1(s) = 0, for all s, which can be exactly approximated if w = 0", "e2bd99a0-9bbb-4a21-93f6-469568429e80": "Similar attacks also help dialogue generation  and text summarization . Other methods do not rely in editing input text directly; Iyyer et al. leverage round-trip translation to generate paraphrases in given syntactic templates and Zhao et al. search for adversarial examples in underlying semantic space with GANs . Some of these heuristics could be further re\ufb01ned to obtain simple adversarial data augmentation approaches. For example, McCoy et al. craft adversarial examples for natural language inference using sophisticated templates which create lexical overlap between the premise and the hypothesis to fool the model. Min et al. proposes two simple yet effective adversarial transformations that reverse the position of subject and object or the position of premise and hypothesis.\n\nThis line of work generates augmented data by manipulating the hidden representations through perturbations such as adding noise or performing interpolations with other data points. Hiddenspace perturbations augment existing data by adding perturbations to the hidden representations of tokens  or sentences . methods can generate in\ufb01nite augmented data in the \u201cvirtual vicinity\u201d of the original data space, thus improving the generalization performance of models", "a876b80a-f66f-47b0-a1a7-34d2cd9bfd9c": "We predict a non-null answer when \u02c6 si,j > snull + \u03c4, where the threshold \u03c4 is selected on the dev set to maximize F1. We did not use TriviaQA data for this model. We \ufb01ne-tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48. The results compared to prior leaderboard entries and top published work  are shown in Table 3, excluding systems that use BERT as one of their components. We observe a +5.1 F1 improvement over the previous best system.\n\nThe Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair completion examples that evaluate grounded commonsense inference . Given a sentence, the task is to choose the most plausible continuation among four choices. When \ufb01ne-tuning on the SWAG dataset, we construct four input sequences, each containing the concatenation of the given sentence (sentence A) and a possible continuation (sentence B). The only task-speci\ufb01c parameters introduced is a vector whose dot product with the  token representation C denotes a score for each choice which is normalized with a softmax layer", "b2d94be2-949c-4352-b7f7-241fff51a1d0": "The relationship between early stopping and weight decay can be made quantitative, thereby showing that the quantity \u03c4\u03b7 (where \u03c4 is the iteration index, and \u03b7 Exercise 5.25 is the learning rate parameter) plays the role of the reciprocal of the regularization typical training session, as a function of the iteration step, for the sinusoidal data set. The goal of achieving the best generalization performance suggests that training should be stopped at the point shown by the vertical dashed lines, corresponding to the minimum of the validation set error. parameter \u03bb. The effective number of parameters in the network therefore grows during the course of training. In many applications of pattern recognition, it is known that predictions should be unchanged, or invariant, under one or more transformations of the input variables.\n\nFor example, in the classi\ufb01cation of objects in two-dimensional images, such as handwritten digits, a particular object should be assigned the same classi\ufb01cation irrespective of its position within the image (translation invariance) or of its size (scale invariance). Such transformations produce signi\ufb01cant changes in the raw data, expressed in terms of the intensities at each of the pixels in the image, and yet should give rise to the same output from the classi\ufb01cation system", "c186cec7-a4b4-4de0-80b2-c7905720fb2d": "The gaps in these Elo ratings translate into predictions that AlphaGo Zero would defeat these other programs with probabilities very close to one. In a match of 100 games between AlphaGo Zero, trained as described, and the exact version of AlphaGo that defeated Lee Sedol held under the same conditions that were used in that match, AlphaGo Zero defeated AlphaGo in all 100 games. The DeepMind team also compared AlphaGo Zero with a program using an ANN with the same architecture but trained by supervised learning to predict human moves in a data set containing nearly 30 million positions from 160,000 games. They found that the supervised-learning player initially played better than AlphaGo Zero, and was better at predicting human expert moves, but played less well after AlphaGo Zero was trained for a day. This suggested that AlphaGo Zero had discovered a strategy for playing that was di\u21b5erent from how humans play. In fact, AlphaGo Zero discovered, and came to prefer, some novel variations of classical move sequences. Final tests of AlphaGo Zero\u2019s algorithm were conducted with a version having a larger ANN and trained over 29 million self-play games, which took about 40 days, again starting with random weights", "a0bcbb45-9f6f-46df-97fd-ae73676b27a4": "Later we shall give a general treatment of EM, and we shall also show how EM can be generalized to obtain the variational inference framework. Initially, we shall motivate the EM Section 10.1 algorithm by giving a relatively informal treatment in the context of the Gaussian mixture model. We emphasize, however, that EM has broad applicability, and indeed it will be encountered in the context of a variety of different models in this book. Let us begin by writing down the conditions that must be satis\ufb01ed at a maximum of the likelihood function.\n\nSetting the derivatives of ln p(X|\u03c0, \u00b5, \u03a3) in (9.14) with respect to the means \u00b5k of the Gaussian components to zero, we obtain where we have made use of the form (2.43) for the Gaussian distribution. Note that the posterior probabilities, or responsibilities, given by (9.13) appear naturally on the right-hand side. Multiplying by \u03a3\u22121 We can interpret Nk as the effective number of points assigned to cluster k. Note carefully the form of this solution", "7f6958c0-0f6d-430a-9d27-0e7ca9590f85": "Such quantities can conveniently be represented using an angular (polar) coordinate 0 \u2a7d \u03b8 < 2\u03c0. We might be tempted to treat periodic variables by choosing some direction as the origin and then applying a conventional distribution such as the Gaussian. Such an approach, however, would give results that were strongly dependent on the arbitrary choice of origin.\n\nSuppose, for instance, that we have two observations at \u03b81 = 1\u25e6 and \u03b82 = 359\u25e6, and we model them using a standard univariate Gaussian distribution. If we choose the origin at 0\u25e6, then the sample mean of this data set will be 180\u25e6 with standard deviation 179\u25e6, whereas if we choose the origin at 180\u25e6, then the mean will be 0\u25e6 and the standard deviation will be 1\u25e6. We clearly need to develop a special approach for the treatment of periodic variables. Let us consider the problem of evaluating the mean of a set of observations D = {\u03b81, . , \u03b8N} of a periodic variable. From now on, we shall assume that \u03b8 is measured in radians. We have already seen that the simple average (\u03b81+\u00b7 \u00b7 \u00b7+\u03b8N)/N will be strongly coordinate dependent", "b6affba1-914a-4ee4-9444-b760254d4ba0": "Is an observed signal more like a reward signal, a value signal, a prediction error, a reinforcement signal, or something altogether di\u21b5erent? And if it is an error signal, is it an RPE, a TD error, or a simpler error like the Rescorla\u2013Wagner error (14.3)? And if it is a TD error, does it depend on actions like the TD error of Q-learning or Sarsa?\n\nAs indicated above, probing the brain to answer questions like these is extremely di\ufb03cult. But experimental evidence suggests that one neurotransmitter, speci\ufb01cally the neurotransmitter dopamine, signals RPEs, and further, that the phasic activity of dopamine-producing neurons in fact conveys TD errors (see Section 15.1 for a de\ufb01nition of phasic activity). This evidence led to the reward prediction error hypothesis of dopamine neuron activity, which we describe next. The reward prediction error hypothesis of dopamine neuron activity proposes that one of the functions of the phasic activity of dopamine-producing neurons in mammals is to deliver an error between an old and a new estimate of expected future reward to target areas throughout the brain", "26bb3a7e-e77a-4325-989f-884bb5cac094": "Equi\"alemly,;t can be defined as tbe linear projection that minimi\"'. the average projttlion cost. defined as t~ mean squa.-ed distance !letween the data [>Oint< and tbeir p<ojtttioo, (Pearson, 19(1). The l\"J'\"OC\"s< of onhogonal projection i' illustraled in FiguTe 12.2. We con,ider each of these definitions in tum. Con,ider a dala set <If obser\"\\lations {x,,} where\" = 1..... S, and x\" i, a Euclidean variable \"'ilh dimen,ionality D. Our goal is to project If>/:: data onto a 'pace ha\"ing dimen,ionality M < D\" hile Ill3Jli\",i,illg the \"ariallCe ofthe projttted data. For the !noll..nl", "1441566f-6772-4fe6-8912-e52b3093876d": "In addition, memory-based methods allow an agent\u2019s experience to have a relatively immediate a\u21b5ect on value estimates in the neighborhood of the current state, in contrast with a parametric method\u2019s need to incrementally adjust parameters of a global approximation. Avoiding global approximation is also a way to address the curse of dimensionality.\n\nFor example, for a state space with k dimensions, a tabular method storing a global approximation requires memory exponential in k. On the other hand, in storing examples for a memory-based method, each example requires memory proportional to k, and the memory required to store, say, n examples is linear in n. Nothing is exponential in k or n. Of course, the critical remaining issue is whether a memory-based method can answer queries quickly enough to be useful to an agent. A related concern is how speed degrades as the size of the memory grows. Finding nearest neighbors in a large database can take too long to be practical in many applications. Proponents of memory-based methods have developed ways to accelerate the nearest neighbor search", "3bd0c150-6b21-4cb9-a149-d9b99af048f6": "Finally, in principle, one could use a pooling operator with unit stride.\n\nOne strategy for pixel-wise labeling of images is to produce an initial guess of the image labels, then refine this initial guess using the interactions between  https://www.deeplearningbook.org/contents/convnets.html    neighboring pixels. Repeating this refinement step several times corresponds to using the same convolutions at each stage, sharing weights between the last layers of the deep net . This makes the sequence of computations performed by the successive convolutional layers with weights shared across layers a particular  352  CHAPTER 9. CONVOLUTIONAL NETWORKS  Figure 9.17: An example of a recurrent convolutional network for pixel labeling. The input is an image tensor X, with axes corresponding to image rows, image columns, and channels (red, green, blue). The goal is to output a tensor of labels Y, witha probability distribution over labels for each pixel. This tensor has axes corresponding to image rows, image columns, and the different classes", "12d26d28-4354-4ef4-9436-753678929dbc": "Many architectures using deep learning models were proposed to resolve the problem, including DQN to stabilize the training with experience replay and occasionally frozen target network. Case Study: AlphaGo Zero  The game of Go has been an extremely hard problem in the field of Artificial Intelligence for decades until recent years. AlphaGo and AlphaGo Zero are two programs developed by a team at DeepMind.\n\nBoth involve deep Convolutional Neural Networks (CNN) and Monte Carlo Tree Search (MCTS) and both have been approved to achieve the level of professional human Go players. Different from AlphaGo that relied on supervised learning from expert human moves, AlphaGo Zero used only reinforcement learning and self-play without human knowledge beyond the basic rules. https://lilianweng.github.io/posts/2018-02-19-rl-overview/  A (Long) Peek into Reinforcement Learning | Lil'Log   With all the knowledge of RL above, let's take a look at how AlphaGo Zero works. The main component is a deep CNN over the game board configuration (precisely, a ResNet with batch normalization and ReLU)", "0dc0654c-fdb1-4826-a49e-862e28954805": "Now consider a variational approximation in which the distribution q(x) is assumed to factorize with respect to the xi so that Note that for observed nodes, there is no factor q(xi) in the variational distribution. We now substitute (10.122) into our general result (10.9) to give Any terms on the right-hand side that do not depend on xj can be absorbed into the additive constant. In fact, the only terms that do depend on xj are the conditional distribution for xj given by p(xj|paj) together with any other conditional distributions that have xj in the conditioning set.\n\nBy de\ufb01nition, these conditional distributions correspond to the children of node j, and they therefore also depend on the co-parents of the child nodes, i.e., the other parents of the child nodes besides node xj itself. We see that the set of all nodes on which q\u22c6(xj) depends corresponds to the Markov blanket of node xj, as illustrated in Figure 8.26. Thus the update of the factors in the variational posterior distribution represents a local calculation on the graph", "f7d699f6-837a-4797-8035-9fd2d8a5d802": "We define the Boltzmann machine over a d-dimensional binary random vector x \u20ac {0, 1}4.\n\nThe Boltzmann machine is an energy-based model (section 16.2.4),  https://www.deeplearningbook.org/contents/generative_models.html    CHAPTER 20. DEEP GENERATIVE MODELS  meaning we define the joint probability distribution using an energy function:  \u2014-E P(x) = exp (\u201cF(@)) (2), (20.1) Z where F(a) is the energy function, and Z is the partition function that ensures  that >>, P(w) = 1. The energy function of the Boltzmann machine is given by E(a) =\u2014#'Ua \u2014b'a, (20.2)  where U is the \u201cweight\u201d matrix of model parameters and b is the vector of bias parameters. In the general setting of the Boltzmann machine, we are given a set of training examples, each of which are n-dimensional. Equation 20.1 describes the joint probability distribution over the observed variables. While this scenario is certainly viable, it does limit the kinds of interactions between the observed variables to hose described by the weight matrix", "f447e76f-bd54-4684-ae55-ac40bf739d9a": "Rather than training G to minimize log(1 \u2212 D(G(z))) we can train G to maximize log D(G(z)). This objective function results in the same \ufb01xed point of the dynamics of G and D but provides much stronger gradients early in learning. The generator G implicitly de\ufb01nes a probability distribution pg as the distribution of the samples G(z) obtained when z \u223c pz. Therefore, we would like Algorithm 1 to converge to a good estimator of pdata, if given enough capacity and training time. The results of this section are done in a nonparametric setting, e.g. we represent a model with in\ufb01nite capacity by studying convergence in the space of probability density functions. We will show in section 4.1 that this minimax game has a global optimum for pg = pdata. We will then show in section 4.2 that Algorithm 1 optimizes Eq 1, thus obtaining the desired result. pdata(x)+pg(x). (c) After an update to G, gradient of D has guided G(z) to \ufb02ow to regions that are more likely to be classi\ufb01ed as data", "21a50735-c472-4bc7-afc3-eb5cca3a27a8": "The n-step version of Sarsa we call n-step Sarsa, and the original version presented in the previous chapter we henceforth call one-step Sarsa, or Sarsa(0). The main idea is to simply switch states for actions (state\u2013action pairs) and then use an \"-greedy policy.\n\nThe backup diagrams for n-step Sarsa (shown in Figure 7.3), like those of n-step TD (Figure 7.1), are strings of alternating states and actions, except that the Sarsa ones all start and end with an action rather a state. We rede\ufb01ne n-step returns (update targets) in terms of estimated action values: Gt:t+n .= Rt+1+\u03b3Rt+2+\u00b7 \u00b7 \u00b7+\u03b3n\u22121Rt+n+\u03b3nQt+n\u22121(St+n, At+n), n \u2265 1, 0 \uf8ff t < T \u2212n, with Gt:t+n .= Gt if t + n \u2265 T", "73188b5d-d775-4e08-95c7-3d4037588e7b": "For example, Ranzato et al. used the REINFORCE algorithm , and Bahdanau et al. used the actorcritic algorithm; Guo et al. and Shi et al. tried to relieve the sparsity problem via hierarchical and inverse RL methods, resp. They are all on-policy RL algorithms with the need of pretraining their models using MLE.\n\nRAML (Norouzi et al., 2016) implicitly relies on the quality of off-policy data; this does not necessarily apply in our experiments with limited good data. Tan et al. and Hu and Xing  offer a uni\ufb01ed view of RAML, RL, and other training methods. Another line of work focused mostly on using only off-policy data, often for of\ufb02ine training of chatbots . As a result, the opportunity of directly improving the reward (as in on-policy updates) for other rich tasks is missed. Our proposed framework combines onand off-policy training, and further offers solutions for ef\ufb01cient training from scratch in the presence of large action space and sparse sequence-level reward in text generation", "deba5959-2030-40e7-90b1-38e5ac90e2f1": "Chapter 18  Confronting the Partition Function  In section 16.2.2 we saw that many probabilistic models (commonly known as undi- rected graphical models) are defined by an unnormalized probability distribution p(x; 0). We must normalize p by dividing by a partition function Z(@) to obtain a valid probability distribution:  (x:8) = (x6) (18.1)  x; 0) = \u2014\u20147j(x; 6). P Z(0) P  The partition function is an integral (for continuous variables) or sum (for discrete variables) over the unnormalized probability of all states:  [oleae (18.2)  or  >- va). (18.3)  This operation is intractable for many interesting models. As we will see in chapter 20, several deep learning models are designed to have a tractable normalizing constant, or are designed to be used in ways that do not involve computing p(x) at all. Yet, other models directly confront the challenge of intractable partition functions. In this chapter, we describe techniques used for training and evaluating models that have intractable partition functions. https://www.deeplearningbook.org/contents/partition.html    603  CHAPTER 18", "82626eb5-a574-4727-91c2-bcfea5104e45": "Montague, Dayan, Person, and Sejnowski  presented a model of honeybee foraging using the TD error. The model is based on research by Hammer, Menzel, and colleagues  showing that the neuromodulator octopamine acts as a reinforcement signal in the honeybee. Montague et al. pointed out that dopamine likely plays a similar role in the vertebrate brain. Barto  related the actor\u2013critic architecture to basal-ganglionic circuits and discussed the relationship between TD learning and the main results from Schultz\u2019s group. Houk, Adams, and Barto  suggested how TD learning and the actor\u2013critic architecture might map onto the anatomy, physiology, and molecular mechanism of the basal ganglia. Doya and Sejnowski  extended their earlier paper on a model of birdsong learning  by including a TD-like error identi\ufb01ed with dopamine to reinforce the selection of auditory input to be memorized", "4d6f89f0-7ca6-4594-bba4-c045085182ae": "Let p\u03b8(x) \u2208 P(X) denote the distribution de\ufb01ned by the model, where X is the data space (e.g., all language text) and P(X) denotes the set of all probability distributions on X. Given a set of independent and identically distributed (i.i.d.) data examples D = {x\u2217 \u2208 X}, the most common method for estimating the parameters \u03b8 is perhaps maximum likelihood estimation (MLE). MLE learns the model by minimizing the negative log-likelihood: In a maximum entropy formulation, rather than assuming a speci\ufb01c parametric from of the target model distribution, denoted as p(x), we instead impose constraints on the model distribution. Speci\ufb01cally, in the supervised setting, the constraints require the expectation of the features T(x) to be equal to the empirical expectation: In general, there exist many distributions p \u2208 P(X) that satisfy the constraint. The principle of maximum entropy resolves the ambiguity by choosing the distribution such that its Shannon entropy, H(p) := \u2212Ep, is maximized", "c2d2b534-fcba-4ff5-b7e3-00ee91f8831c": "In many cases it is desirable to automatically induce and adapt the weights during the course of model training. In Section 9.2, we discuss how the SE framework can easily enable automated data reweighting by reusing existing algorithms that were designed to solve other seemingly unrelated problems. Data augmentation. Data augmentation expands existing data by adding synthetically modi\ufb01ed copies of existing data instances (e.g., by rotating an existing image at random angles), and is widely used for increasing data size or encouraging invariance in learned representations (e.g., object label is invariant to image rotation). The indicator function I as the similarity metric in Equation 4.2 restrictively requires exact match between the true t\u2217 and the con\ufb01guration t. Data augmentation arises as a \u2018relaxation\u2019 to the similarity metric.\n\nLet at\u2217(t) \u2265 0 be a distribution that assigns nonzero probability to not only the exact t\u2217 but also other con\ufb01gurations t related to t\u2217 in certain ways (e.g., all rotated images t of the observed image t\u2217)", "5a50bea5-d024-4f0f-9531-29ebe3812229": "To give a concrete example, consider the problem of sentiment classi\ufb01cation, where given a sentence x, we want to predict its sentiment y \u2208 {negative 0, positive 1}. A challenge for a sentiment classi\ufb01er is to understand the contrastive sense within a sentence and capture the dominant sentiment precisely. For example, if a sentence is of structure \u2018A-but-B\u2019 with the connective \u2018but\u2019, the sentiment of the half sentence after \u2018but\u2019 dominates. Let xB be the half sentence after \u2018but\u2019 and \u02dcyB \u2208  the (soft) sentiment prediction over xB by the current model, a possible way to express the knowledge as a logical rule frule(x, y) is: where I(\u00b7) is an indicator function that takes 1 when its argument is true, and 0 otherwise. Given an instantiation (a.k.a. grounding) of (x, y, \u02dcyB), the truth value of frule(x, y) can be evaluated by de\ufb01nitions in Equation 4.12. Intuitively, the frule(x, y) truth value gets closer to 1 when y and \u02dcyB are more consistent", "01e2efb9-3f3e-44a4-a034-730e392be3d8": "4.10 g' Hg  In the worst case, when g aligns with the eigenvector of H corresponding to the  maximal eigenvalue Amax, then this optimal step size is given by x . To the extent that the function we minimize can be approximated well by a \u201cquadratic function, the eigenvalues of the Hessian thus determine the scale of the learning rate. The second derivative can be used to determine whether a critical point is a local maximum, a local minimum, or a saddle point. Recall that on a critical point, f\u2019(x) =0. When the second derivative f\u201d(a) > 0, the first derivative f\u2019(x)  86  CHAPTER 4. NUMERICAL COMPUTATION  increases as we move to the right and decreases as we move to the left. This means f'(\u00ab\u2014\u20ac) < Oand f\u2019(x + \u20ac) > 0 for small enough e\u00a2. In other words, as we move right, the slope begins to point uphill to the right, and as we move left, the slope begins to point uphill to the left", "8d41e22e-5e9a-46e9-be27-9f3bd513f3ef": "Because the computational graph is a directed acyclic eraph it has at most O(n?) edges. For the kinds of graphs that are commonly used in practice, the situation is even better. Most neural network cost functions are roughly chain-structured, causing back-propagation to have O(n) cost. This is far better than the naive approach, which might need to execute exponentially many nodes. This potentially exponential cost can be seen by expanding and rewriting the recursive chain rule (equation 6.53) nonrecursively:  dul\u201d) 5 au) - aa) = \u00bb erica (6.55) path (u(*),u(\"2),..u(\")), k=2  from 7=j to \u2122m=n  Since the number of paths from node j to node n can grow exponentially in the length of these paths, the number of terms in the above sum, which is the number of such paths, can grow exponentially with the depth of the forward propagation graph", "a1813226-8f48-486c-9d2c-628cd0d19754": "What energy functions can be minimized via graph cuts? IEEE Transactions on Pattern Analysis and Machine Intelligence 26(2), 147\u2013159. Kreinovich, V. Y. Arbitrary nonlinearity is suf\ufb01cient to represent all functions by neural networks: a theorem. Neural Networks 4(3), 381\u2013 383.\n\nKrogh, A., M. Brown, I. S. Mian, K. Sj\u00a8olander, and D. Haussler . Hidden Markov models in computational biology: Applications to protein modelling. Journal of Molecular Biology 235, 1501\u20131531. Kschischnang, F. R., B. J. Frey, and H. A. Loeliger . Factor graphs and the sum-product algorithm. IEEE Transactions on Information Theory 47(2), 498\u2013519. Kuhn, H. W. and A. W. Tucker . Nonlinear programming", "4582d4f5-fa7b-43b2-83ea-af6b30efbc20": "Recall that the probability of x lying in an infinitesimally small region with volume da is given by p(a)da.\n\nSince g can expand  or contract space, the infinitesimal volume surrounding x in x space may have different volume in y space. To see how to correct the problem, we return to the scalar case. We need to preserve the property  \\py(g(x))dy| = |po(a)da}. 3.44) Solving from this, we obtain Ox _ -1/,)) |O% . Py(y) = Px(g\u201c(y)) Dy 3.45) or equivalently Og(x) Pa(x) = py(g(2)) ul . 3.46)  In higher dimensions, the derivative generalizes to the determinant of the Jacobian matrix\u2014the matrix with Jj; =  oa Thus, for real-valued vectors x and y, Jd  pale) = ny(atn))|aee (22). (3.47)  70  CHAPTER 3", "2e301df5-60db-4801-b013-35177bfa9d83": "Of course, in a practical implementation, only a single copy of each retained sample would be kept, along with an integer weighting factor recording how many times that state appears. As we shall see, as long as q(zA|zB) is positive for any values of zA and zB (this is a suf\ufb01cient but not necessary condition), the distribution of z(\u03c4) tends to p(z) as \u03c4 \u2192 \u221e. It should be emphasized, however, that the sequence z(1), z(2), . is not a set of independent samples from p(z) because successive samples are highly correlated. If we wish to obtain independent samples, then we can discard most of the sequence and just retain every M th sample.\n\nFor M suf\ufb01ciently large, the retained samples will for all practical purposes be independent. Figure 11.9 shows a simple illustrative example of sampling from a two-dimensional Gaussian distribution using the Metropolis algorithm in which the proposal distribution is an isotropic Gaussian. Further insight into the nature of Markov chain Monte Carlo algorithms can be gleaned by looking at the properties of a speci\ufb01c example, namely a simple random walk", "8dcfb99b-3000-4318-929b-6c2bae14c66e": "Again, this can be enforced using a Lagrange multiplier as before, and leads to the result so that the mixing coef\ufb01cients are equal to the fractions of data points assigned to the corresponding components.\n\nThus we see that the complete-data log likelihood function can be maximized trivially in closed form. In practice, however, we do not have values for the latent variables so, as discussed earlier, we consider the expectation, with respect to the posterior distribution of the latent variables, of the complete-data log likelihood. Using (9.10) and (9.11) together with Bayes\u2019 theorem, we see that this posterior distribution takes the form and hence factorizes over n so that under the posterior distribution the {zn} are independent. This is easily veri\ufb01ed by inspection of the directed graph in Figure 9.6 Exercise 9.5 and making use of the d-separation criterion. The expected value of the indicator Section 8.2 variable znk under this posterior distribution is then given by which is just the responsibility of component k for data point xn. The expected value of the complete-data log likelihood function is therefore given by We can now proceed as follows", "16501c5a-17f3-4004-9108-56f20bc776b9": "If separate averages are kept for each action taken in each state, then these averages will similarly converge to the action values, q\u21e1(s, a). We call estimation methods of this kind Monte Carlo methods because they involve averaging over many random samples of actual returns. These kinds of methods are presented in Chapter 5. Of course, if there are very many states, then it may not be practical to keep separate averages for each state individually. Instead, the agent would have to maintain v\u21e1 and q\u21e1 as parameterized functions (with fewer parameters than states) and adjust the parameters to better match the observed returns. This can also produce accurate estimates, although much depends on the nature of the parameterized function approximator.\n\nThese possibilities are discussed in Part II of the book. A fundamental property of value functions used throughout reinforcement learning and dynamic programming is that they satisfy recursive relationships similar to that which we have already established for the return (3.9)", "4dbbd9fb-9465-41fa-a84a-d3e8c644cd9d": "Time or space >  In self-supervised learning, the system is trained to predict hidden parts of the input (in gray) from visible parts of the input (in green). As aresult of the supervisory signals that inform self-supervised learning, the term \u201cself-supervised learning\u201d is more accepted than the previously used term \u201cunsupervised learning.\u201d Unsupervised learning is an ill-defined and misleading term that suggests that the learning uses no supervision at all. In fact, self-supervised learning is not unsupervised, as it uses far more feedback signals than standard supervised and reinforcement learning  https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/  Self-supervised learning: The dark matter of intelligence   methods do. Self-supervised learning for language versus VISION  Self-supervised learning has had a particularly profound impact on NLP, allowing us to train models such as BERT, RoBERTa, XLM-R, and others on large unlabeled text data sets and then use these models for downstream tasks.\n\nThese models are pretrained ina self-supervised phase and then fine- tuned for a particular task, such as classifying the topic of a text", "6d7850f1-23c8-450e-a874-aff916bb584e": "On the other hand, the Kantorovich-Rubinstein duality  tells us that where the supremum is over all the 1-Lipschitz functions f : X \u2192 R. Note that if we replace \u2225f\u2225L \u2264 1 for \u2225f\u2225L \u2264 K (consider K-Lipschitz for some constant K), then we end up with K \u00b7W(Pr, Pg). Therefore, if we have a parameterized family of and if the supremum in (2) is attained for some w \u2208 W (a pretty strong assumption akin to what\u2019s assumed when proving consistency of an estimator), this process would yield a calculation of W(Pr, P\u03b8) up to a multiplicative constant. Furthermore, we could consider di\ufb00erentiating W(Pr, P\u03b8) (again, up to a constant) by back-proping through equation (2) via estimating Ez\u223cp(z). While this is all intuition, we now prove that this process is principled under the optimality assumption. Theorem 3. Let Pr be any distribution", "5ea83a90-f17d-4226-9158-2323d353efbe": ", pM\u22121 that interpolate between a simple distribution p1(z) for which we can evaluate the normalization coef\ufb01cient Z1 and the desired complex distribution pM(z). We then have ZM Z1 = Z2 in which the intermediate ratios can be determined using Monte Carlo methods as discussed above. One way to construct such a sequence of intermediate systems is to use an energy function containing a continuous parameter 0 \u2a7d \u03b1 \u2a7d 1 that interpolates between the two distributions If the intermediate ratios in (11.74) are to be found using Monte Carlo, it may be more ef\ufb01cient to use a single Markov chain run than to restart the Markov chain for each ratio.\n\nIn this case, the Markov chain is run initially for the system p1 and then after some suitable number of steps moves on to the next distribution in the sequence. Note, however, that the system must remain close to the equilibrium distribution at each stage. 11.2 (\u22c6) Suppose that z is a random variable with uniform distribution over (0, 1) and that we transform z using y = h\u22121(z) where h(y) is given by (11.6). Show that y has the distribution p(y)", "3aaeb51c-5875-452e-a7b9-ea7fa670ebb1": "Possibilities like these led to the idea of \u201cintrinsically-motivated reinforcement learning\u201d that we brie\ufb02y discuss further at the end of the following section. In this book we have presented the foundations of a reinforcement learning approach to arti\ufb01cial intelligence. Roughly speaking, that approach is based on model-free and modelbased methods working together, as in the Dyna architecture of Chapter 8, combined with function approximation as developed in Part II. The focus has been on online and incremental algorithms, which we see as fundamental even to model-based methods, and on how these can be applied in o\u21b5-policy training situations.\n\nThe full rationale for the latter has been presented only in this last chapter. That is, we have all along presented o\u21b5policy learning as an appealing way to deal with the explore/exploit dilemma, but only in this chapter have we discussed learning about many diverse auxiliary tasks simultaneously with GVFs and learning about the world hierarchically in terms of temporally-abstract option models, both of which involve o\u21b5-policy learning. Much remains to be worked out, as we have indicated throughout the book and as evidenced by the directions for additional research discussed in this chapter", "2edaf538-ffca-4157-b365-9edd3f7c1f2d": "(We generate a class-balanced dataset of m = 1000 data points with binary labels, and n independent labeling functions with average accuracy 75% and a \ufb01xed 10% probability of voting.) We plot the advantage obtained by a learned generative model (GM), Aw; by an optimal model A\u2217; the upper bound \u02dcA\u2217 used in our optimizer; and the low-density bound (Proposition 1) In other words, Aw is the number of times fw correctly disagrees with f1 on a label, minus the number of times it incorrectly disagrees. Let the optimal advantage A\u2217 = Aw\u2217 be the advantage using the optimal weights w\u2217 (WMV*). be the average accuracies of the labeling functions. To build intuition, we start by analyzing the optimal advantage for three regimes of label density (see Fig. 7): Low Label Density In this sparse setting, very few data points have more than one non-abstaining label; only a small number have multiple con\ufb02icting labels", "a692f85c-013d-4882-b98e-55ecfbb5d663": "In addition to linear function approximation, Samuel experimented with lookup tables and hierarchical lookup tables called signature tables . At about the same time as Samuel\u2019s work, Bellman and Dreyfus  proposed using function approximation methods with DP. (It is tempting to think that Bellman and Samuel had some in\ufb02uence on one another, but we know of no reference to the other in the work of either.) There is now a fairly extensive literature on function approximation methods and DP, such as multigrid methods and methods using splines and orthogonal polynomials .\n\nHolland\u2019s  classi\ufb01er system used a selective feature-match technique to generalize evaluation information across state\u2013action pairs. Each classi\ufb01er matched a subset of states having speci\ufb01ed values for a subset of features, with the remaining features having arbitrary values (\u201cwild cards\u201d). These subsets were then used in a conventional state-aggregation approach to function approximation. Holland\u2019s idea was to use a genetic algorithm to evolve a set of classi\ufb01ers that collectively would implement a useful action-value function", "dc448af1-6b77-4812-b50c-5dabe34e84ef": "\u03b80, initial generator\u2019s parameters. The fact that the EM distance is continuous and di\ufb00erentiable a.e. means that we can (and should) train the critic till optimality. The argument is simple, the more we train the critic, the more reliable gradient of the Wasserstein we get, which is actually useful by the fact that Wasserstein is di\ufb00erentiable almost everywhere. For the JS, as the discriminator gets better the gradients get more reliable but the true gradient is 0 since the JS is locally saturated and we get vanishing gradients, as can be seen in Figure 1 of this paper and Theorem 2.4 of . In Figure 2 we show a proof of concept of this, where we train a GAN discriminator and a WGAN critic till optimality. The discriminator learns very quickly to distinguish between fake and real, and as expected provides no reliable gradient information. The critic, however, can\u2019t saturate, and converges to a linear function that gives remarkably clean gradients everywhere. The fact that we constrain the weights limits the possible growth of the function to be at most linear in di\ufb00erent parts of the space, forcing the optimal critic to have this behaviour", "d4e9a544-2f14-4c9c-b1f2-cc19f1557b7e": "The learning controller, labeled RL in the \ufb01gure, improved over that of FR-FCFS by from 7% to 33% over the nine applications, with an average improvement of 19%. Of course, no realizable controller can match the performance of Optimistic, which ignores all timing and resource constraints, but the learning controller\u2019s performance closed the gap with Optimistic\u2019s upper bound by an impressive 27%. Because the rationale for on-chip implementation of the learning algorithm was to allow the scheduling policy to adapt online to changing workloads, \u02d9Ipek et al. analyzed the impact of online learning compared to a previously-learned \ufb01xed policy. They trained their controller with data from all nine benchmark applications and then held the resulting action values \ufb01xed throughout the simulated execution of the applications. They found that the average performance of the controller that learned online was 8% better than that of the controller using the \ufb01xed policy, leading them to conclude that online learning is an important feature of their approach.\n\nThis learning memory controller was never committed to physical hardware because of the large cost of fabrication. Nevertheless, \u02d9Ipek et al", "436426d7-afe7-4384-af3b-0a1c55c5f944": "Most debugging strategies for neural nets are designed to get around one or both of these two difficulties. Either we design a case that is so simple that the correct behavior actually can be predicted, or we design a test that exercises one part of the neural net implementation in isolation. Some important debugging tests include the following. Visualize the model in action: When training a model to detect objects in images, view some images with the detections proposed by the model displayed superimposed on the image. When training a generative model of speech, listen to some of the speech samples it produces. This may seem obvious, but it is easy to fall into the practice of looking only at quantitative performance measurements like accuracy or log-likelihood.\n\nDirectly observing the machine learning model performing its task will help to determine whether the quantitative performance numbers it achieves seem reasonable. Evaluation bugs can be some of the most devastating bugs because they can mislead you into believing your system is performing well when it is not. Visualize the worst mistakes: Most models are able to output some sort of confidence measure for the task they perform. For example, classifiers based on a softmax output layer assign a probability to each class", "a28a7862-7be3-4d63-9add-9dce2fabe147": "If we minimize f(a) with respect to a single variable x;, then minimize it with respect to another variable 7;, and so on, repeatedly cycling through all variables, we are guaranteed to arrive at a (local) minimum. This practice is known as coordinate descent, because we optimize one coordinate at a time. More generally, block coordinate descent refers to minimizing with respect to a subset of the variables simultaneously. The term \u201ccoordinate descent\u201d is often used to refer to block coordinate descent as well as the strictly individual coordinate descent. Coordinate descent makes the most sense when the different variables in the optimization problem can be clearly separated into groups that play relatively isolated roles, or when optimization with respect to one group of variables is significantly more efficient than optimization with respect to all of the variables.\n\nFor example, consider the cost function  J(H,W) = 30 |His| + 0 (x \u2014w\"H) (8.38)  2 ij  This function describes a learning problem called sparse coding, where the goal is (0 find a weight matrix W that can linearly decode a matrix of activation values HT to reconstruct the training set X", "ec683cf3-bcfa-47bf-9446-4241fe7c25b4": "They also used a gradient-ascent algorithm called RMSProp  that accelerates learning by adjusting the step-size parameter for each weight based on a running average of the magnitudes of recent gradients for that weight. Mnih et al. modi\ufb01ed the basic Q-learning procedure in three ways. First, they used a method called experience replay \ufb01rst studied by Lin . This method stores the agent\u2019s experience at each time step in a replay memory that is accessed to perform the weight updates. It worked like this in DQN. After the game emulator executed action At in a state represented by the image stack St, and returned reward Rt+1 and image stack St+1, it added the tuple (St, At, Rt+1, St+1) to the replay memory. This memory accumulated experiences over many plays of the same game. At each time step multiple Qlearning updates\u2014a mini-batch\u2014were performed based on experiences sampled uniformly at random from the replay memory", "1c0d6142-ccbb-440f-8fab-9427cdfaf83c": "Linear factor models are some of the simplest generative models and some of the simplest models that learn a representation of data.\n\nMuch as linear classifiers and linear regression models may be extended to deep feedforward networks, these linear factor models may be extended to autoencoder networks and deep probabilistic models that perform the same tasks but with a much more powerful and flexible model family. https://www.deeplearningbook.org/contents/linear_factors.html    498  https://www.deeplearningbook.org/contents/linear_factors.html", "518daef7-c468-4cf4-aecd-5ee91753f5a6": "For M hidden units, any given weight vector will belong to a set of M! equivalent weight vectors associated with this interchange symmetry, corresponding to the M! different orderings of the hidden units. The network will therefore have an overall weight-space symmetry factor of M!2M. For networks with more than two layers of weights, the total level of symmetry will be given by the product of such factors, one for each layer of hidden units. It turns out that these factors account for all of the symmetries in weight space (except for possible accidental symmetries due to speci\ufb01c choices for the weight values). Furthermore, the existence of these symmetries is not a particular property of the \u2018tanh\u2019 function but applies to a wide range of activation functions . In many cases, these symmetries in weight space are of little practical consequence, although in Section 5.7 we shall encounter a situation in which we need to take them into account", "21fe464f-481a-4a89-b2fb-2da468ba9a13": "First, choose the general category of model based on the structure of your data. If you want to perform supervised learning with fixed-size vectors as input, use a feedforward network with fully connected layers. If the input has known topological structure (for example, if the input is an image), use a convolutional network. In these cases, you should begin by using some kind of piecewise linear unit (ReLUs or their generalizations, such as Leaky ReLUs, PreLus, or maxout).\n\nIf your input or output is a sequence, use a gated recurrent net (LSTM or GRU). A reasonable choice of optimization algorithm is SGD with momentum with a decaying learning rate (popular decay schemes that perform better or worse  https://www.deeplearningbook.org/contents/guidelines.html    on different problems include decaying linearly until reaching a fixed minimum learning rate, decaying exponentially, or decreasing the learning rate by a factor of 2-10 each time validation error plateaus). Another reasonable alternative is Adam. Batch normalization can have a dramatic effect on optimization performance, especially for convolutional networks and networks with sigmoidal nonlinearities", "ae1b2852-9e4b-406d-b1c4-76e522e551d0": "In a matter of roughly two years, most of the industrial products for speech recognition incorporated deep neural networks, and this success spurred a new wave of research into deep learning algorithms and architectures for ASR, which is ongoing today. One of these innovations was the use of convolutional networks  that replicate weights across time and frequency, improving over the earlier time-delay neural networks that replicated weights only across time.\n\nThe new two-dimensional convolutional models regard the input spectrogram not as one long vector but as an image, with one axis corresponding to time and the other to frequency of spectral components. Another important push, still ongoing, has been toward end-to-end deep learning  https://www.deeplearningbook.org/contents/applications.html    speech recognition systems that completely remove the HMM. \u2018l\u2019he hrst major breakthrough in this direction came from Graves et al. , who trained a deep LSTM RNN (see section 10.10), using MAP inference over the frame-to-phoneme alignment, as in LeCun ef al. and in the CTC framework", "40247903-d2db-4251-875e-112acbad1e3e": "We know from our own experience that with enough repetition, goal-directed behavior tends to turn into habitual behavior. Experiments show that this happens for rats too. Adams  conducted an experiment to see if extended training would convert goal-directed behavior into habitual behavior. He did this by comparing the e\u21b5ect of outcome devaluation on rats that experienced di\u21b5erent amounts of training. If extended training made the rats less sensitive to devaluation compared to rats that received less training, this would be evidence that extended training made the behavior more habitual. Adams\u2019 experiment closely followed the Adams and Dickinson  experiment just described. Simplifying a bit, rats in one group were trained until they made 100 rewarded lever-presses, and rats in the other group\u2014the overtrained group\u2014were trained until they made 500 rewarded lever-presses. After this training, the reward value of the pellets was decreased (using lithium chloride injections) for rats in both groups. Then both groups of rats were given a session of extinction training", "54382497-2af8-4ac3-bd2c-e49c2126c0f5": "The required moments are given by Appendix B We conclude this chapter by discussing an alternative form of deterministic approximate inference, known as expectation propagation or EP . As with the variational Bayes methods discussed so far, this too is based on the minimization of a Kullback-Leibler divergence but now of the reverse form, which gives the approximation rather different properties.\n\nConsider for a moment the problem of minimizing KL(p\u2225q) with respect to q(z) when p(z) is a \ufb01xed distribution and q(z) is a member of the exponential family and so, from (2.194), can be written in the form As a function of \u03b7, the Kullback-Leibler divergence then becomes where the constant terms are independent of the natural parameters \u03b7. We can minimize KL(p\u2225q) within this family of distributions by setting the gradient with respect to \u03b7 to zero, giving \u2212\u2207 ln g(\u03b7) = Ep(z). (10.186) However, we have already seen in (2.226) that the negative gradient of ln g(\u03b7) is given by the expectation of u(z) under the distribution q(z)", "8f6dca8c-adeb-49a6-a247-4ac9ea237530": "legal: In summary we have published in the journal Nature Neuroscience: A systematic review of human brain tissue has found no evidence for any association between the presence of the presence of a particular form of human neuropathy in the brain, a condition that is not normally associated with cognitive impairment. We found that politics: In summary we have a list of 10 of the best and most common types of drugs for people with HIV.\n\nThis is a very short list of recommendations from a national and international community.\\n\\n\\n\\n This article has been updated to make the of\ufb01cial state of the EU state of computers: In summary, we believe that the current system has no way of doing anything about it.\\n\\n\\n\\n The following steps are taken to get the system working.\\n\\n 1. Install a new operating system with a Linux Mint operating system\\n 2. Start a new Linux Mint operating space: In summary we have some important news from the moment of the year and some important information about these two major planets. This new discovery is the \ufb01rst to con\ufb01rm this important planet has an active life in its home planet, a planet with a mass of about 5.8 billion tons. It religion: In summary, we believe that the current administration has no way of doing anything about the Benghazi attacks", "fca0b63b-517a-4177-b571-6824b3a303e4": "The primary advantage of RBFs over binary features is that they produce approximate functions that vary smoothly and are di\u21b5erentiable. Although this is appealing, in most cases it has no practical signi\ufb01cance. Nevertheless, extensive studies have been made of graded response functions such as RBFs in the context of tile coding . All of these methods require substantial additional computational complexity (over tile coding) and often reduce performance when there are more than two state dimensions. In high dimensions the edges of tiles are much more important, and it has proven di\ufb03cult to obtain well controlled graded tile activations near the edges. An RBF network is a linear function approximator using RBFs for its features.\n\nLearning is de\ufb01ned by equations (9.7) and (9.8), exactly as in other linear function approximators. In addition, some learning methods for RBF networks change the centers and widths of the features as well, bringing them into the realm of nonlinear function approximators. Nonlinear methods may be able to \ufb01t target functions much more precisely. The downside to RBF networks, and to nonlinear RBF networks especially, is greater computational complexity and, often, more manual tuning before learning is robust and e\ufb03cient", "e428add6-1f65-4966-bf73-e606facc3d4e": "We trained generative models of images from the MNIST and Frey Face datasets3 and compared learning algorithms in terms of the variational lower bound, and the estimated marginal likelihood. The generative model (encoder) and variational approximation (decoder) from section 3 were used, where the described encoder and decoder have an equal number of hidden units.\n\nSince the Frey Face data are continuous, we used a decoder with Gaussian outputs, identical to the encoder, except that the means were constrained to the interval (0, 1) using a sigmoidal activation function at the 3Available at http://www.cs.nyu.edu/\u02dcroweis/data.html decoder output. Note that with hidden units we refer to the hidden layer of the neural networks of the encoder and decoder. Parameters are updated using stochastic gradient ascent where gradients are computed by differentiating the lower bound estimator \u2207\u03b8,\u03c6L(\u03b8, \u03c6; X) (see algorithm 1), plus a small weight decay term corresponding to a prior p(\u03b8) = N(0, I)", "11414ef5-053e-41d3-80da-6a6b8d8c428c": "If the sets of variables are independent, then their joint distribution will factorize into the product of their marginals p(x, y) = p(x)p(y). If the variables are not independent, we can gain some idea of whether they are \u2018close\u2019 to being independent by considering the Kullback-Leibler divergence between the joint distribution and the product of the marginals, given by which is called the mutual information between the variables x and y. From the properties of the Kullback-Leibler divergence, we see that I(x, y) \u2a7e 0 with equality if, and only if, x and y are independent.\n\nUsing the sum and product rules of probability, we see that the mutual information is related to the conditional entropy through Exercise 1.41 Thus we can view the mutual information as the reduction in the uncertainty about x by virtue of being told the value of y (or vice versa). From a Bayesian perspective, we can view p(x) as the prior distribution for x and p(x|y) as the posterior distribution after we have observed new data y. The mutual information therefore represents the reduction in uncertainty about x as a consequence of the new observation y", "ef74e8ba-45a7-49c8-83ac-cf0a806d62e7": "We shall refer to these additional factorizations as induced factorizations because they arise from an interaction between the factorization assumed in the variational posterior distribution and the conditional independence properties of the true joint distribution. In a numerical implementation of the variational approach it is important to take account of such additional factorizations. For instance, it would be very inef\ufb01cient to maintain a full precision matrix for the Gaussian distribution over a set of variables if the optimal form for that distribution always had a diagonal precision matrix (corresponding to a factorization with respect to the individual variables described by that Gaussian). Such induced factorizations can easily be detected using a simple graphical test based on d-separation as follows", "b43460ed-8d71-47ea-85b6-c7c01298e648": "Comparing the ssRBM to the mcRBM and the mPoT models, the ssRBM parametrizes the conditional covariance of the observation in a significantly different way. The mcRBM and mPoT both model the covariance structure of the observation  \\ rg -1 as (>; no 7d) pT 4 1) , using the activation of the hidden units h; > 0 to  678  CHAPTER 20. DEEP GENERATIVE MODELS  enforce constraints on the conditional covariance in the direction r).\n\nIn contrast, he ssRBM specifies the conditional covariance of the observations using the hidden spike activations h; = 1 to pinch the precision matrix along the direction specified by the corresponding weight vector. The ssRBM conditional covariance is similar to hat given by a different model: the product of probabilistic principal components analysis (PoPPCA) . In the overcomplete setting, sparse activations with the ssRBM parametrization permit significant variance above the nominal variance given by A7') only in the selected directions of he sparsely activated h;", "ca283b34-1a86-48b8-b94a-75cbecb6a317": "Next we turn to the problem of \ufb01nding the marginal for a node zn given all observations x1 to xN. For temporal data, this corresponds to the inclusion of future as well as past observations. Although this cannot be used for real-time prediction, it plays a key role in learning the parameters of the model.\n\nBy analogy with the hidden Markov model, this problem can be solved by propagating messages from node xN back to node x1 and combining this information with that obtained during the forward message passing stage used to compute the \ufffd\u03b1(zn). In the LDS literature, it is usual to formulate this backward recursion in terms of \u03b3(zn) = \ufffd\u03b1(zn)\ufffd\u03b2(zn) rather than in terms of \ufffd\u03b2(zn)", "9127f253-ed63-44f9-97df-829a25a17c52": "The majority (57%) of users matched or exceeded the performance of a model trained on 7 h (2500 instances) of handlabeled data Fig. 13 The pro\ufb01le of the best performing user by F1 score was a MS or Ph.D. degree in any \ufb01eld, strong Python coding skills, and intermediate to advanced experience with machine learning. Prior experience with text mining added no bene\ufb01t Participant labeling functions had a median length of 2 lines of Python code (min:2, max:12). We grouped participant-designed functions into three types: 1. Pattern-based (regular expressions, small term sets) Fig. 14 We bucketed labeling functions written by user study participants into three types\u2014pattern-based, distant supervision, and complex. Participants tended to mainly write pattern-based labeling functions, but also universally expressed more complex heuristics as well On average, 58% of participant\u2019s labeling functions where pattern-based (min:25%, max: 82%).\n\nThe best labeling function design strategy used by participants appeared to be de\ufb01ning small term sets correlated with positive and negative labels. Participants with the lowest F1 scores tended to design labeling functions with low coverage of negative labels", "90b14869-57ad-4a45-a27e-d4f2772decbe": "REPRESENTATION LEARNING  not yet know how this is possible. Many factors could explain improved human performance\u2014for example, the brain may use very large ensembles of classifiers or Bayesian inference techniques. One popular hypothesis is that the brain is able to leverage unsupervised or semi-supervised learning. There are many ways to leverage unlabeled data. In this chapter, we focus on the hypothesis that the unlabeled data can be used to learn a good representation.\n\n15.1 Greedy Layer-Wise Unsupervised Pretraining  Unsupervised learning played a key historical role in the revival of deep neural  networks, enabling researchers for the first time to train a deep supervised network without requiring architectural specializations like convolution or recurrence. We call this procedure unsupervised pretraining, or more precisely, greedy layer- wise unsupervised pretraining. This procedure is a canonical example of how a representation learned for one task (unsupervised learning, trying to capture the shape of the input distribution) can sometimes be useful for another task (supervised learning with the same input domain)", "aacca795-8dee-45d8-a8dc-c106e8a2bdf9": "With N samples {u;}\u2019_, from p and M samples {v;}\u2122\u201c, from p* , we can estimate the expectation of the  second term E,-_,-  in the denominator of contrastive learning loss:  alee {us}, fvid) = max { (ap Yo f(x)\" hr eH sen f(v,))) ,exp(-1/7)}  where T is the temperature and exp(\u20141/7) is the theoretical lower bound of  By wp: lexP(F(x)\" f(x)", "07789be8-1229-4004-965c-2f152eb704b0": "Holland\u2019s ideas led to a number of TD-related systems, including the work of Booker  and the bucket brigade of Holland , which is related to Sarsa as discussed below.\n\n6.1\u20132 Most of the speci\ufb01c material from these sections is from Sutton , including the TD(0) algorithm, the random walk example, and the term \u201ctemporaldi\u21b5erence learning.\u201d The characterization of the relationship to dynamic programming and Monte Carlo methods was in\ufb02uenced by Watkins , Werbos , and others. The use of backup diagrams was new to the \ufb01rst edition of this book. Tabular TD(0) was proved to converge in the mean by Sutton  and with probability 1 by Dayan , based on the work of Watkins and Dayan . These results were extended and strengthened by Jaakkola, Jordan, and Singh  and Tsitsiklis  by using extensions of the powerful existing theory of stochastic approximation. Other extensions and generalizations are covered in later chapters. 6.3 The optimality of the TD algorithm under batch training was established by Sutton", "b027c762-513a-4677-81e6-d1354454f6fa": "12.11 Probabilistic PCA visoo,zsbon 01 a portion 0I1he \"\" !low data setlo< Ihe !irsl 100 (lata \u00bbeinls, The left..,...nd plot oIIOWS Ihe I'O\"leoo< mean proj9c1ions oIlhfI (lata poims on lhe principal subspace.\n\nThe ,;gtrI\u00b7hi\\nd plot is obtained by firsl ran<lomly omitting 30% 0I1he variable .aloo. and lhen us>rlg EM 10 MndIe I\"\" mi...... values. Note I!IaI eac/1 data poinl1hen NoS allea., one missing mea.u,ement but lhoallhe plot i. \"\"ry ..mia, to lhe ona obtained wit\"\"\"l miss...", "e19a4698-1da8-4f04-92f6-f7ace300487a": "Here the parameter \u03b80 corresponds to the mean of the distribution, while m, which is known as the concentration parameter, is analogous to the inverse variance (precision) for the Gaussian. The normalization coef\ufb01cient in (2.179) is expressed in terms of I0(m), which is the zeroth-order Bessel function of the \ufb01rst kind  and is de\ufb01ned by For large m, the distribution becomes approximately Gaussian. The von Mises disExercise 2.52 tribution is plotted in Figure 2.19, and the function I0(m) is plotted in Figure 2.20.\n\nNow consider the maximum likelihood estimators for the parameters \u03b80 and m for the von Mises distribution. The log likelihood function is given by ln p(D|\u03b80, m) = \u2212N ln(2\u03c0) \u2212 N ln I0(m) + m Setting the derivative with respect to \u03b80 equal to zero gives To solve for \u03b80, we make use of the trigonometric identity which we recognize as the result (2.169) obtained earlier for the mean of the observations viewed in a two-dimensional Cartesian space", "e08d486f-a43d-42a3-988e-f0132fb9f869": "The total number of data points lying inside this cube will therefore be Substituting this expression into (2.246) then gives the following result for the estimated density at x where we have used V = hD for the volume of a hypercube of side h in D dimensions. Using the symmetry of the function k(u), we can now re-interpret this equation, not as a single cube centred on x but as the sum over N cubes centred on the N data points xn. As it stands, the kernel density estimator (2.249) will suffer from one of the same problems that the histogram method suffered from, namely the presence of arti\ufb01cial discontinuities, in this case at the boundaries of the cubes.\n\nWe can obtain a smoother density model if we choose a smoother kernel function, and a common choice is the Gaussian, which gives rise to the following kernel density model where h represents the standard deviation of the Gaussian components. Thus our density model is obtained by placing a Gaussian over each data point and then adding up the contributions over the whole data set, and then dividing by N so that the density is correctly normalized", "29e71f17-003a-4ded-9a42-13301906f296": "Interpolation-based methods were \ufb01rst explored in computer vision , and have more recently been generalized to the text domain  by performing interpolation between original data and token-level augmented data in the output space , between original data and adversarial data in embedding space , or between different training examples in general hidden space . Different strategies to select samples to mix have also been explored  such as k-nearest-neighbours  or sentence composition . We summarize the preceding overview of recent widely-used data augmentation methods in Table 1, characterizing them with respect to augmentation levels, the diversity of generated data, and their applicable tasks. While data augmentation (DA) can be applied in the supervised setting to produce better results when only a small labeled training dataset is available, data augmentation is also commonly used in semi-supervised learning (SSL).\n\nSSL is an alternative approach for learning from limited data that provides a framework for taking advantage of unlabeled data. Speci\ufb01cally, SSL assumes that our training set comprises labeled examples in addition to unlabeled examples drawn from the same distribution. Currently, one of the most common methods for performing SSL with deep neural networks is \u201cconsistency regularization\u201d", "c6658f95-7883-4714-a04d-f9fe42ad8845": "present the state of the art in function approximation in reinforcement learning. Some of the early work with function approximation in reinforcement learning is discussed at the end of this section. 9.3 Gradient-descent methods for minimizing mean-squared error in supervised learning are well known. Widrow and Ho\u21b5  introduced the least-meansquare (LMS) algorithm, which is the prototypical incremental gradient-descent algorithm. Details of this and related algorithms are provided in many texts . Semi-gradient TD(0) was \ufb01rst explored by Sutton , as part of the linear TD(\u03bb) algorithm that we will treat in Chapter 12. The term \u201csemi-gradient\u201d to describe these bootstrapping methods is new to the second edition of this book. The earliest use of state aggregation in reinforcement learning may have been Michie and Chambers\u2019s BOXES system . The theory of state aggregation in reinforcement learning has been developed by Singh, Jaakkola, and Jordan  and Tsitsiklis and Van Roy . State aggregation has been used in dynamic programming from its earliest days", "0becd5f9-8531-4771-86ed-426ed21a32bc": "This version achieved an Elo rating of 5,185.\n\nThe team pitted this version of AlphaGo Zero against a program called AlphaGo Master, the strongest program at the time, that was identical to AlphaGo Zero but, like AlphaGo, used human data and features. AlphaGo Master\u2019s Elo rating was 4,858, and it had defeated the strongest human professional players 60 to 0 in online games. In a 100 game match, AlphaGo Zero with the larger network and more extensive learning defeated AlphaGo Master 89 games to 11, thus providing a convincing demonstration of the problem-solving power of AlphaGo Zero\u2019s algorithm. AlphaGo Zero soundly demonstrated that superhuman performance can be achieved by pure reinforcement learning, augmented by a simple version of MCTS, and deep ANNs with very minimal knowledge of the domain and no reliance on human data or guidance. We will surely see systems inspired by the DeepMind accomplishments of both AlphaGo and AlphaGo Zero applied to challenging problems in other domains. Recently, yet a better program, AlphaZero, was described by Silver et al. that does not even incorporate knowledge of Go", "878aa2cd-48c4-46a7-ac6c-0f2f538c789e": "In other words, both refer to the same embedding network that learns an efficient embedding to reveal relationship between pairs of data points. Koch, Zemel & Salakhutdinov  proposed a method to use the siamese neural network to do one-shot image classification. First, the siamese network is trained for a verification task for telling whether two input images are in the same class. It outputs the probability of two images belonging to the same class. Then, during test time, the siamese network processes all the image pairs between a test image and every image in the support set. The final prediction is the class of the support image with the highest probability. https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Log   seh  input 1 embed 1  UK  y probability of input 1 & 2 are  in the same class  distance  embed 2  First, convolutional siamese network learns to encode two images into feature vectors via a embedding function fg which contains a couple of convolutional layers", "d0083987-9242-4e26-925d-b5507bd08d70": "Because  the form of the true posterior is determined by the choice of model, we cannot design a reduced-cost approach to computing Dkr (p(h | v)||q(h | v)) exactly. 19.4.1 Discrete Latent Variables  Variational inference with discrete latent variables is relatively straightforward. We define a distribution gq, typically one where each factor of q is just defined by a lookup table over discrete states.\n\nIn the simplest case, his binary and we make the mean field assumption that q factorizes over each individual h;. In this case we can parametrize q with a vector Ah whose entries are probabilities. Then q(hi = 1| v) = hj. After determining how to represent q, we simply optimize its parameters. With discrete latent variables, this is just a standard optimization problem. In principle the selection of g could be done with any optimization algorithm, such as gradient descent. Because this optimization must occur in the inner loop of a learning algorithm, it must be very fast. To achieve this speed, we typically use special optimization algorithms that are designed to solve comparatively small and simple problems in few iterations", "6d0a6597-4eca-4cb9-b721-01993ee14056": "This de\ufb01nes our model: where Zw is a normalizing constant. To learn this model without access to the true labels Y, we minimize the negative log marginal likelihood given the observed label matrix \ufffd: We optimize this objective by interleaving stochastic gradient descent steps with Gibbs sampling ones, similar to contrastive divergence ; for more details, see . We use the Numbskull library,8 a Python NUMBA-based Gibbs sampler. We then use the predictions, \u02dcY = p \u02c6w(Y|\ufffd), as probabilistic training labels", "cc7aeeed-1fcf-494d-832d-332f2f6037a5": "Rather than simply returning to the maximum likelihood estimate, we can still gain some of the benefit of the Bayesian approach by allowing the prior to influence the choice of the point estimate. One rational way to do this is to choose the maximum a posteriori (MAP) point estimate.\n\nThe MAP estimate chooses the point of  136  CHAPTER 5. MACHINE LEARNING BASICS  maximal posterior probability (or maximal probability density in the more common case of continuous 6):  O\\ap = argmax p(0 | x) = arg max log p(a | @) + log p(8). (5.79) 6 6  We recognize, on the righthand side, log p(a | 8), that is, the standard log- likelihood term, and log p(@), corresponding to the prior distribution. As an example, consider a linear regression model with a Gaussian prior on the weights w. If this prior is given by V(w;0, 4J? ), then the log-prior term in equation 5.79 is proportional to the familiar dw tw weight decay penalty, plus a term that does not depend on w and does not affect the learning process", "96929698-aea1-4d27-915a-fb3c64c4c538": "Other forms of prior over the parameters can be considered. For instance, we can generalize the Gaussian prior to give in which q = 2 corresponds to the Gaussian distribution, and only in this case is the prior conjugate to the likelihood function (3.10). Finding the maximum of the posterior distribution over w corresponds to minimization of the regularized error function (3.29). In the case of the Gaussian prior, the mode of the posterior distribution was equal to the mean, although this will no longer hold if q \u0338= 2.\n\nIn practice, we are not usually interested in the value of w itself but rather in making predictions of t for new values of x. This requires that we evaluate the predictive distribution de\ufb01ned by in which t is the vector of target values from the training set, and we have omitted the corresponding input vectors from the right-hand side of the conditioning statements to simplify the notation. The conditional distribution p(t|x, w, \u03b2) of the target variable is given by (3.8), and the posterior weight distribution is given by (3.49)", "6a14bad3-ab7f-4bd3-9582-ee853703e5bb": "The correct prediction would depend on the size of the house, the room the robot was in, and the age of the battery, all of which would be hard for the robot designer to know. It would be di\ufb03cult for the designer to build in a reliable algorithm for deciding whether to head back to the charger in sensory terms, but it might be easy to do this in terms of the learned prediction. We foresee many possible ways like this in which learned predictions might combine usefully with built-in algorithms for controlling behavior. Finally, perhaps the most important role for auxiliary tasks is in moving beyond the assumption we have made throughout this book that the state representation is \ufb01xed and given to the agent. To explain this role, we \ufb01rst have to take a few steps back to appreciate the magnitude of this assumption and the implications of removing it. We do that in Section 17.3.\n\nAn appealing aspect of the MDP formalism is that it can be applied usefully to tasks at many di\u21b5erent time scales. One can use it to formalize the task of deciding which muscles to twitch to grasp an object, which airplane \ufb02ight to take to arrive conveniently at a distant city, and which job to take to lead a satisfying life", "77e1e424-61f9-4db8-8d2d-ad8b6092e069": "Another way of saying this is that any policy that is greedy with respect to the optimal evaluation function v\u21e4 is an optimal policy. The term greedy is used in computer science to describe any search or decision procedure that selects alternatives based only on local or immediate considerations, without considering the possibility that such a selection may prevent future access to even better alternatives. Consequently, it describes policies that select actions based only on their short-term consequences. The beauty of v\u21e4 is that if one uses it to evaluate the short-term consequences of actions\u2014speci\ufb01cally, the one-step consequences\u2014then a greedy policy is actually optimal in the long-term sense in which we are interested because v\u21e4 already takes into account the reward consequences of all possible future behavior. By means of v\u21e4, the optimal expected long-term return is turned into a quantity that is locally and immediately available for each state. Hence, a one-step-ahead search yields the long-term optimal actions. Having q\u21e4 makes choosing optimal actions even easier. With q\u21e4, the agent does not even have to do a one-step-ahead search: for any state s, it can simply \ufb01nd any action that maximizes q\u21e4(s, a)", "55030dcb-d52b-4780-87f2-9612d07b8596": "How does this algorithm perform on Baird\u2019s counterexample? Figure 11.6 shows the trajectory in expectation of the components of the parameter vector (for the case in which It =1, for all t). There are some oscillations but eventually everything converges and the VE goes to zero. These trajectories are obtained by iteratively computing the expectation of the parameter vector trajectory without any of the variance due to sampling of transitions and rewards.\n\nWe do not show the results of applying the Emphatic-TD algorithm directly because its variance on Baird\u2019s counterexample is so high that it is nigh impossible to get consistent results in computational experiments. The algorithm converges to the optimal solution in theory on this problem, but in practice it does not. We turn to the topic of reducing the variance of all these algorithms in the next section. O\u21b5-policy learning is inherently of greater variance than on-policy learning. This is not surprising; if you receive data less closely related to a policy, you should expect to learn less about the policy\u2019s values. In the extreme, one may be able to learn nothing. You can\u2019t expect to learn how to drive by cooking dinner, for example", "088e5043-b3a0-4177-8602-51cf6b6fd373": "Here Ijj\u2032 is the j, j\u2032 element of the identity matrix. If one or both of the weights is a bias term, then the corresponding expressions are obtained simply by setting the appropriate activation(s) to 1. Inclusion of skip-layer connections is straightforward.\n\nExercise 5.23 For many applications of the Hessian, the quantity of interest is not the Hessian matrix H itself but the product of H with some vector v. We have seen that the evaluation of the Hessian takes O(W 2) operations, and it also requires storage that is O(W 2). The vector vTH that we wish to calculate, however, has only W elements, so instead of computing the Hessian as an intermediate step, we can instead try to \ufb01nd an ef\ufb01cient approach to evaluating vTH directly in a way that requires only O(W) operations. To do this, we \ufb01rst note that where \u2207 denotes the gradient operator in weight space. We can then write down the standard forward-propagation and backpropagation equations for the evaluation of \u2207E and apply (5.96) to these equations to give a set of forward-propagation and backpropagation equations for the evaluation of vTH", "191c5996-55c5-419c-95f6-2d5c1c4f38de": "In figure 20.6, we will see  how a machine learning algorithm can successfully accomplish this goal. 160  CHAPTER 5. MACHINE LEARNING BASICS  This concludes part I, which has provided the basic concepts in mathematics and machine learning that are employed throughout the remaining parts of the book. You are now prepared to embark on your study of deep learning. 2 Sah Sh So Sel Setar otras ee DD ey h  https://www.deeplearningbook.org/contents/ml.html    Pa Made oof of eter he eee ee EEEREEEEEEREREREEes Figure 5.13: Training examples from the QMUL Multiview Face Dataset , for which the subjects were asked to move in such a way as to cover the two- dimensional manifold corresponding to two angles of rotation. We would like learning  algorithms to be able to discover and disentangle such manifold coordinates. Figure 20.6 illustrates such a feat. 161  https://www.deeplearningbook.org/contents/ml.html", "551f2108-e201-4ee3-9d69-c0f3bdfbcdf2": "Bertsekas and Tsitsiklis  provide excellent coverage of these variations and their performance di\u21b5erences. DP was its implementation on a multiprocessor system with communication delays between processors and no global synchronizing clock. These algorithms are extensively discussed by Bertsekas and Tsitsiklis . Jacobi-style and Gauss\u2013Seidel-style DP algorithms are special cases of the asynchronous version. Williams and Baird  presented DP algorithms that are asynchronous at a \ufb01ner grain than the ones we have discussed: the update operations themselves are broken into steps that can be performed asynchronously.\n\nFoundational work on the linear programming approach to reinforcement learning was done by Daniela de Farias . In this chapter we consider our \ufb01rst learning methods for estimating value functions and discovering optimal policies. Unlike the previous chapter, here we do not assume complete knowledge of the environment. Monte Carlo methods require only experience\u2014sample sequences of states, actions, and rewards from actual or simulated interaction with an environment. Learning from actual experience is striking because it requires no prior knowledge of the environment\u2019s dynamics, yet can still attain optimal behavior. Learning from simulated experience is also powerful", "7bd43efe-990d-4415-a272-762d7716f1ed": ", xN, whose values in\ufb02uence either the distribution of latent variables or output variables, or both. An example is shown in Figure 13.18. This extends the HMM framework to the domain of supervised learning for sequential data. It is again easy to show, through the use of the d-separation criterion, that the Markov property (13.5) for the chain of latent variables still holds. To verify this, simply note that there is only one path from node zn\u22121 to node zn+1 and this is head-to-tail with respect to the observed node zn. This conditional independence property again allows the formulation of a computationally ef\ufb01cient learning algorithm. In particular, we can determine the parameters \u03b8 of the model by maximizing the likelihood function L(\u03b8) = p(X|U, \u03b8) where U is a matrix whose rows are given by uT n", "4595b747-137b-4dda-9249-1e999572e985": "Since the late 1960\u2019s, many arti\ufb01cial intelligence researchers presumed that there are no general principles to be discovered, that intelligence is instead due to the possession of a vast number of special purpose tricks, procedures, and heuristics.\n\nIt was sometimes said that if we could just get enough relevant facts into a machine, say one million, or one billion, then it would become intelligent. Methods based on general principles, such as search or learning, were characterized as \u201cweak methods,\u201d whereas those based on speci\ufb01c knowledge were called \u201cstrong methods.\u201d This view is still common today, but not dominant. From our point of view, it was simply premature: too little e\u21b5ort had been put into the search for general principles to conclude that there were none. Modern arti\ufb01cial intelligence now includes much research looking for general principles of learning, search, and decision making. It is not clear how far back the pendulum will swing, but reinforcement learning research is certainly part of the swing back toward simpler and fewer general principles of arti\ufb01cial intelligence. A good way to understand reinforcement learning is to consider some of the examples and possible applications that have guided its development. \u2022 A master chess player makes a move", "8637f682-768a-4210-81c0-83eea30af73f": "The mapping from context to action is also called a policy. The feedback loop between the learner and the data distribution (which now depends on the actions of the learner) is a central research issue in the reinforcement learning and bandits literature. Reinforcement learning requires choosing a trade-off between exploration and exploitation.\n\nExploitation refers to taking actions that come from the current,  https://www.deeplearningbook.org/contents/applications.html    best version of the learned policy\u2014actions that we know will achieve a high reward. 476  CHAPTER 12. APPLICATIONS  Exploration refers to taking actions specifically to obtain more training data. If we know that given context 2, action a gives us a reward of 1, we do not know whether that is the best possible reward. We may want to exploit our current policy and continue taking action a to be relatively sure of obtaining a reward of 1. However, we may also want to explore by trying action a\u2019. We do not know what will happen if we try action a\u2019. We hope to get a reward of 2, but we run the risk of getting a reward of 0. Either way, we at least gain some knowledge", "73074fb3-e06a-4b2a-b3b7-24dfd3a6d97a": "In auto-regressive networks, the idea is to train the network to be able to cope with any order by randomly sampling orders and providing the information to hidden units specifying which of the inputs are observed (on the right side of the conditioning bar) and which are to be predicted and are thus considered missing (on the left side of the conditioning bar). This is nice because it allows one to use a trained auto-regressive network to perform any inference problem (i.e., predict or sample from the probability distribution over any subset of variables given any subset) extremely efficiently. Finally, since many orders of variables are possible  706  CHAPTER 20. DEEP GENERATIVE MODELS  (n! for n variables) and each order o of variables yields a different p(x | 0), we can form an ensemble of models for many values of o:  k  1 . Pensemble(X) = k So p(x | of), (20.84) i=l  This ensemble model usually generalizes better and assigns higher probability to the test set than does an individual model defined by a single ordering", "8d40b164-dc34-49c3-9e6c-8b1b045b2131": "For the backgammon application, in which \u03b3 = 1 and the reward is always zero except upon winning, the TD error portion of the learning rule is usually just \u02c6v(St+1,w) \u2212 \u02c6v(St,w), as suggested in Figure 15.2. To apply the learning rule we need a source of backgammon games. Tesauro obtained an unending sequence of games by playing his learning backgammon player against itself. To choose its moves, TD-Gammon considered each of the 20 or so ways it could play its dice roll and the corresponding positions that would result.\n\nThe resulting positions are afterstates as discussed in Section 6.8. The network was consulted to estimate each of their values. The move was then selected that would lead to the position with the highest estimated value. Continuing in this way, with TD-Gammon making the moves for both sides, it was possible to easily generate large numbers of backgammon games. Each game was treated as an episode, with the sequence of positions acting as the states, S0, S1, S2, . .. Tesauro applied the nonlinear TD rule (15.1) fully incrementally, that is, after each individual move", "d4590c6a-53bc-4533-9954-a7f7096a853b": "11 We simply \ufb01ne-tune the whole base network on the labeled data without regularization (see Appendix B.5). Table 7 shows the comparisons of our results against recent methods . The supervised baseline from  is strong due to intensive search of hyper-parameters (including augmentation). Again, our approach signi\ufb01cantly improves over state-of-the-art with both 1% and 10% of the labels. Interestingly, \ufb01ne-tuning our pretrained ResNet-50 (2\u00d7, 4\u00d7) on full ImageNet are also signi\ufb01cantly better then training from scratch (up to 2%, see Appendix B.2). Transfer learning. We evaluate transfer learning performance across 12 natural image datasets in both linear evaluation (\ufb01xed feature extractor) and \ufb01ne-tuning settings. Following Kornblith et al. , we perform hyperparameter tuning for each model-dataset combination and select the best hyperparameters on a validation set. Table 8 shows results with the ResNet-50 (4\u00d7) model. When \ufb01ne-tuned, our self-supervised model signi\ufb01cantly outperforms the supervised baseline on 5 datasets, whereas the supervised baseline is superior on only 2 (i.e", "adcd7745-72f5-407d-93c2-6c18d9094478": "Like the standardized formulation of the objective function, we would like to quest for a standardized optimization algorithm that is generally applicable to optimizing the objective under vastly di\ufb00erent speci\ufb01cations.\n\nYet it seems still unclear whether such a universal solver exists In general, the SE in Equation 3.1 or (3.2), with both the model parameters \u03b8 and the auxiliary distribution q to be learned, can naturally be optimized with an alternating-projection style procedure, which we have referred to as the teacher-student mechanism. 7.2. The Teacher-Student Mechanism. We have seen a special case of the teacher-student mechanism in Equation 3.3 for solving the speci\ufb01cally instantiated SE (e.g., with cross entropy as the divergence function D). The optimization procedure is also an example of alternating projection (Figure 4). Speci\ufb01cally, the teacher q(n+1) is the projection of the student p\u03b8(n) onto the set de\ufb01ned by the experience, and the student p\u03b8(n+1) is the projection of the teacher q(n+1) onto the set of model distributions. 7.2.1. The Teacher Step", "709f99e4-ecf5-4e8e-bbbf-e7745c00a30f": "Neuroscientists have discovered a form of Hebbian plasticity called spike-timing-dependent plasticity (STDP) that lends plausibility to the existence of actor-like synaptic plasticity in the brain. STDP is a Hebbian-style plasticity, but changes in a synapse\u2019s e\ufb03cacy depend on the relative timing of presynaptic and postsynaptic action potentials. The dependence can take di\u21b5erent forms, but in the one most studied, a synapse increases in strength if spikes incoming via that synapse arrive shortly before the postsynaptic neuron \ufb01res. If the timing relation is reversed, with a presynaptic spike arriving shortly after the postsynaptic neuron \ufb01res, then the strength of the synapse decreases. STDP is a type of Hebbian plasticity that takes the activation time of a neuron into account, which is one of the ingredients needed for actor-like learning.\n\nThe discovery of STDP has led neuroscientists to investigate the possibility of a threefactor form of STDP in which neuromodulatory input must follow appropriately-timed pre- and postsynaptic spikes", "46f5987f-9b8a-4d08-956b-4605ebf12123": "Let us consider the derivation of the update equation for the factor q(Z). The log of the optimized factor is given by We now make use of the decomposition (10.41).\n\nNote that we are only interested in the functional dependence of the right-hand side on the variable Z. Thus any terms that do not depend on Z can be absorbed into the additive normalization constant, giving Substituting for the two conditional distributions on the right-hand side, and again absorbing any terms that are independent of Z into the additive constant, we have where D is the dimensionality of the data variable x. Taking the exponential of both sides of (10.45) we obtain Requiring that this distribution be normalized, and noting that for each value of n the quantities znk are binary and sum to 1 over all values of k, we obtain Exercise 10.12 We see that the optimal solution for the factor q(Z) takes the same functional form as the prior p(Z|\u03c0). Note that because \u03c1nk is given by the exponential of a real quantity, the quantities rnk will be nonnegative and will sum to one, as required", "a11941e6-f8ff-4dfb-9ea2-a862fcde9e3b": "This is the idea that approximate policy and value functions should interact in such a way that they both move toward their optimal values. One of the two processes making up GPI drives the value function to accurately predict returns for the current policy; this is the prediction problem. The other process drives the policy to improve locally (e.g., to be \"-greedy) with respect to the current value function. When the \ufb01rst process is based on experience, a complication arises concerning maintaining su\ufb03cient exploration. We can classify TD control methods according to whether they deal with this complication by using an on-policy or o\u21b5-policy approach. Sarsa is an on-policy method, and Q-learning is an o\u21b5-policy method. Expected Sarsa is also an o\u21b5-policy method as we present it here.\n\nThere is a third way in which TD methods can be extended to control which we did not include in this chapter, called actor\u2013critic methods. These methods are covered in full in Chapter 13. The methods presented in this chapter are today the most widely used reinforcement learning methods", "1e46d16f-46c0-4da3-93fb-221553b04e75": "The notation  is used to denote the closed interval from a to b, that is the interval including the values a and b themselves, while (a, b) denotes the corresponding open interval, that is the interval excluding a and b. Similarly,  where y(x) is some function. The concept of a functional is discussed in Appendix D. The notation g(x) = O(f(x)) denotes that |f(x)/g(x)| is bounded as x \u2192 \u221e. For instance if g(x) = 3x2 + 2, then g(x) = O(x2). The expectation of a function f(x, y) with respect to a random variable x is denoted by Ex. In situations where there is no ambiguity as to which variable is being averaged over, this will be simpli\ufb01ed by omitting the suf\ufb01x, for instance E. If the distribution of x is conditioned on another variable z, then the corresponding conditional expectation will be written Ex. Similarly, the variance is denoted var, and for vector variables the covariance is written cov", "9ee1b88a-a648-4197-b0ae-51489c9cea26": "DEEP FEEDFORWARD NETWORKS  the past . Rectified linear units and all these generalizations of them are based on the principle that models are easier to optimize if their behavior is closer to linear. This same general principle of using linear behavior to obtain easier optimization also applies in other contexts besides deep linear networks. Recurrent networks can learn from sequences and produce a sequence of states and outputs. When training them, one needs to propagate information through several time steps, which is much easier when some linear computations (with some directional derivatives being of magnitude near 1) are involved. One of the best-performing recurrent network architectures, the LSTM, propagates information through time via summation\u2014a particular straightforward kind of linear activation. This is discussed further in section 10.10.\n\n6.3.2 Logistic Sigmoid and Hyperbolic Tangent  Prior to the introduction of rectified linear units, most neural networks used the logistic sigmoid activation function  (2) =o(2) (6.38) or the hyperbolic tangent activation function g(z) = tanh(z). (6.39)  These activation functions are closely related because tanh(z) = 20(2z) \u2014 1", "7922848e-6599-4180-97fe-03f1f1881db9": "These include stopping training when performance begins to decrease on validation data di\u21b5erent from the training data (cross validation), modifying the objective function to discourage complexity of the approximation (regularization), and introducing dependencies among the weights to reduce the number of degrees of freedom (e.g., weight sharing). A particularly e\u21b5ective method for reducing over\ufb01tting by deep ANNs is the dropout method introduced by Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov . During training, units are randomly removed from the network (dropped out) along with their connections. This can be thought of as training a large number of \u201cthinned\u201d networks. Combining the results of these thinned networks at test time is a way to improve generalization performance. The dropout method e\ufb03ciently approximates this combination by multiplying each outgoing weight of a unit by the probability that that unit was retained during training. Srivastava et al. found that this method signi\ufb01cantly improves generalization performance", "5788d4d1-df63-4c5b-b5da-a7b954a2332b": "Let us now consider an idea for avoiding this large extraneous variance. The essence of the idea is to think of discounting as determining a probability of termination or, equivalently, a degree of partial termination.\n\nFor any \u03b3 2 [0, 1), we can think of the return G0 as partly terminating in one step, to the degree 1 \u2212 \u03b3, producing a return of just the \ufb01rst reward, R1, and as partly terminating after two steps, to the degree (1 \u2212 \u03b3)\u03b3, producing a return of R1 + R2, and so on. The latter degree corresponds to terminating on the second step, 1 \u2212 \u03b3, and not having already terminated on the \ufb01rst step, \u03b3. The degree of termination on the third step is thus (1 \u2212 \u03b3)\u03b32, with the \u03b32 re\ufb02ecting that termination did not occur on either of the \ufb01rst two steps. The partial returns here are called \ufb02at partial returns: where \u201c\ufb02at\u201d denotes the absence of discounting, and \u201cpartial\u201d denotes that these returns do not extend all the way to termination but instead stop at h, called the horizon (and T is the time of termination of the episode)", "bd2f5048-430c-4776-8827-b6a3ca13f379": "To describe the back-propagation algorithm more precisely, it is helpful to have a more precise computational graph language. Many ways of formalizing computation as graphs are possible. Here, we use each node in the graph to indicate a variable. The variable may be a scalar, vector, matrix, tensor, or even a variable of another type. To formalize our graphs, we also need to introduce the idea of an operation. An operation is a simple function of one or more variables. Our graph language is accompanied by a set of allowable operations. Functions more complicated than the operations in this set may be described by composing many operations together. Without loss of generality, we define an operation to return only a single output variable. This does not lose generality because the output variable can have multiple entries, such as a vector.\n\nSoftware implementations of back-propagation usually support operations with multiple outputs, but we avoid this case in our description because it introduces many extra details that are not important to conceptual understanding. If a variable y is computed by applying an operation to a variable x, then we draw a directed edge from x to y", "5e9835b4-060b-477b-aaba-e123ef96ea50": "Classes that are linearly separable in the feature space \u03c6(x) need not be linearly separable in the original observation space x. Note that as in our discussion of linear models for regression, one of the basis functions is typically set to a constant, say \u03c60(x) = 1, so that the corresponding parameter w0 plays the role of a bias. For the remainder of this chapter, we shall include a \ufb01xed basis function transformation \u03c6(x), as this will highlight some useful similarities to the regression models discussed in Chapter 3. For many problems of practical interest, there is signi\ufb01cant overlap between the class-conditional densities p(x|Ck).\n\nThis corresponds to posterior probabilities p(Ck|x), which, for at least some values of x, are not 0 or 1. In such cases, the optimal solution is obtained by modelling the posterior probabilities accurately and then applying standard decision theory, as discussed in Chapter 1. Note that nonlinear transformations \u03c6(x) cannot remove such class overlap. Indeed, they can increase the level of overlap, or create overlap where none existed in the original observation space. However, suitable choices of nonlinearity can make the process of modelling the posterior probabilities easier", "fbb943fc-3e49-40e7-a148-dcaa2a7234af": "from the input to the hidden state,  https://www.deeplearningbook.org/contents/rnn.html    392  CHAPTER 10.\n\nSEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  (a) (b) (c)  Figure 10.13: A recurrent neural network can be made deep in many ways . (a) The hidden recurrent state can be broken down into groups organized hierarchically. (b) Deeper computation (e.g., an MLP) can be introduced in the input-to- hidden, hidden-to-hidden, and hidden-to-output parts. This may lengthen the shortest path linking different time steps. (c) The path-lengthening effect can be mitigated by introducing skip connections. 2. from the previous hidden state to the next hidden state, and  3. from the hidden state to the output. With the RNN architecture of figure 10.3, each of these three blocks is associated with a single weight matrix. In other words, when the network is unfolded, each of these blocks corresponds to a shallow transformation", "02fe4d17-40a2-4e78-bfe5-eeb4b3135c93": "Initially, Rosenblatt simulated the perceptron on an IBM 704 computer at Cornell in 1957, but by the early 1960s he had built special-purpose hardware that provided a direct, parallel implementation of perceptron learning.\n\nMany of his ideas were encapsulated in \u201cPrinciples of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms\u201d published in 1962. Rosenblatt\u2019s work was criticized by Marvin Minksy, whose objections were published in the book \u201cPerceptrons\u201d, co-authored with Seymour Papert. This book was widely misinterpreted at the time as showing that neural networks were fatally \ufb02awed and could only learn solutions for linearly separable problems. In fact, it only proved such limitations in the case of single-layer networks such as the perceptron and merely conjectured (incorrectly) that they applied to more general network models. Unfortunately, however, this book contributed to the substantial decline in research funding for neural computing, a situation that was not reversed until the mid-1980s. Today, there are many hundreds, if not thousands, of applications of neural networks in widespread use, with examples in areas such as handwriting recognition and information retrieval being used routinely by millions of people", "901efd8a-2253-436b-911b-b8b936c26f45": "For the Bayesian logistic regression model, the marginal likelihood takes the form We \ufb01rst note that the conditional distribution for t can be written as where a = wT\u03c6.\n\nIn order to obtain a lower bound on p(t), we make use of the variational lower bound on the logistic sigmoid function given by (10.144), which Note that because this bound is applied to each of the terms in the likelihood function separately, there is a variational parameter \u03ben corresponding to each training set observation (\u03c6n, tn). Using a = wT\u03c6, and multiplying by the prior distribution, we obtain the following bound on the joint distribution of t and w where \u03be denotes the set {\u03ben} of variational parameters, and Evaluation of the exact posterior distribution would require normalization of the lefthand side of this inequality. Because this is intractable, we work instead with the right-hand side. Note that the function on the right-hand side cannot be interpreted as a probability density because it is not normalized. Once it is normalized to give a variational posterior distribution q(w), however, it no longer represents a bound", "4575180b-db33-4fe2-8926-608f999068bd": "The usual Fourier series representation of a function of one dimension having period \u2327 represents the function as a linear combination of sine and cosine functions that are each periodic with periods that evenly divide \u2327 (in other words, whose frequencies are integer multiples of a fundamental frequency 1/\u2327). But if you are interested in approximating an aperiodic function de\ufb01ned over a bounded interval, then you can use these Fourier basis features with \u2327 set to the length of the interval. The function of interest is then just one period of the periodic linear combination of the sine and cosine features. Furthermore, if you set \u2327 to twice the length of the interval of interest and restrict attention to the approximation over the half interval , then you can use just the cosine features.\n\nThis is possible because you can represent any even function, that is, any function that is symmetric about the origin, with just the cosine basis. So any function over the half-period  can be approximated as closely as desired with enough cosine features", "6a7ceed8-6b5e-4219-9281-ef2cc3bd9305": "If y is a d-vector, then he network must output an n X d matrix containing all \u201d of these d-dimensional vectors.\n\nLearning these means with maximum likelihood is slightly more complicated than learning the means of a distribution with only one output mode. We only want to update the mean for the component that actually produced the observation. In practice, we do not know which component produced each observation. The expression for the negative log-likelihood naturally weights each example\u2019s contribution to the loss for each component by the probability that the component produced the example. 3. Covariances SO (a): these specify the covariance matrix for each component i. As when learning a single Gaussian component, we typically use a diagonal matrix to avoid needing to compute determinants. As with learning the means of the mixture, maximum likelihood is complicated by needing to assign partial responsibility for each point to each mixture component. Gradient descent will automatically follow the correct process if given the correct specification of the negative log-likelihood under the mixture model", "a9e04a10-5393-490a-9950-1b8a487ebe38": "Most applications of sparse coding also involve weight decay or a constraint on the norms of the columns of W, to prevent che pathological solution with extremely small H and large W.  The function J is not convex. However, we can divide the inputs to the raining algorithm into two sets: the dictionary parameters W and the code representations H. Minimizing the objective function with respect to either one of hese sets of variables is a convex problem. Block coordinate descent thus gives  317  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  us an optimization strategy that allows us to use efficient convex optimization algorithms, by alternating between optimizing W with HA fixed, then optimizing Hf with W fixed. Coordinate descent is not a very good strategy when the value of one variable strongly influences the optimal value of another variable, as in the function f(x) = (x1 2?+a x + rs where a is a positive constant. The first term encourages  (  \u2014-7  https://www.deeplearningbook.org/contents/optimization.html    the two variables to have similar value, while the second term encourages them to be near zero", "39eac7e9-568e-48c0-90f7-6d4f514e82aa": "Next the graph is triangulated, which involves \ufb01nding chord-less cycles containing four or more nodes and adding extra links to eliminate such chord-less cycles.\n\nFor instance, in the graph in Figure 8.36, the cycle A\u2013C\u2013B\u2013D\u2013A is chord-less a link could be added between A and B or alternatively between C and D. Note that the joint distribution for the resulting triangulated graph is still de\ufb01ned by a product of the same potential functions, but these are now considered to be functions over expanded sets of variables. Next the triangulated graph is used to construct a new tree-structured undirected graph called a join tree, whose nodes correspond to the maximal cliques of the triangulated graph, and whose links connect pairs of cliques that have variables in common. The selection of which pairs of cliques to connect in this way is important and is done so as to give a maximal spanning tree de\ufb01ned as follows. Of all possible trees that link up the cliques, the one that is chosen is one for which the weight of the tree is largest, where the weight for a link is the number of nodes shared by the two cliques it connects, and the weight for the tree is the sum of the weights for the links", "87282f49-ca8d-4455-8dd4-17f1531da8ad": "both provide examples of strategies based on regularizing the average activation across several examples, = > h\u00ae, to be near some target value, such as a vector with .01 for each entry. Other approaches obtain representational sparsity with a hard constraint on the activation values. For example, orthogonal matching pursuit  encodes an input x with the representation h that solves the constrained optimization problem  arg min 2 (7.49)  o<k : nny (le \u2014 Wall  https://www.deeplearningbook.org/contents/regularization.html    where ||/||\u00b0 is the number of nonzero entries of A. This problem can be solved efficiently when is constrained to be orthogonal. This method is often called OMP-4, with the value of / specified to indicate the number of nonzero features  allowed. Coates and Ng  demonstrated that OMP-1 can be a very effective feature extractor for deep architectures. Essentially any model that has hidden units can be made sparse. Throughout this book, we see many examples of sparsity regularization used in various contexts. 252  CHAPTER 7", "6b4f2d81-ba71-4234-bbc3-972ea7dd7d09": "Andreae\u2019s later work  placed more emphasis on learning from a teacher, but still included learning by trial and error, with the generation of novel events being one of the system\u2019s goals. A feature of this work was a \u201cleakback process,\u201d elaborated more fully in Andreae , that implemented a credit-assignment mechanism similar to the backing-up update operations that we describe.\n\nUnfortunately, his pioneering research was not well known and did not greatly impact subsequent reinforcement learning research. Recent summaries are available (Andreae, 2017a,b). More in\ufb02uential was the work of Donald Michie. In 1961 and 1963 he described a simple trial-and-error learning system for learning how to play tic-tac-toe (or naughts and crosses) called MENACE (for Matchbox Educable Naughts and Crosses Engine). It consisted of a matchbox for each possible game position, each matchbox containing a number of colored beads, a di\u21b5erent color for each possible move from that position. By drawing a bead at random from the matchbox corresponding to the current game position, one could determine MENACE\u2019s move", "85efd683-eee2-4002-b6e3-8f6e6e994b23": "A popular choice is to iterate fixed-point equations, in other words,  to solve 9 \u2014L=0 (19.18) hj for h;. We repeatedly update different elements of h until we satisfy a convergence criterion. 637  CHAPTER 19. APPROXIMATE INFERENCE  To make this more concrete, we show how to apply variational inference to the binary sparse coding model (we present here the model developed by Henniges et al. but demonstrate traditional, generic mean field applied to the model, while they introduce a specialized algorithm). This derivation goes into considerable mathematical detail and is intended for the reader who wishes to fully resolve any ambiguity in the high-level conceptual description of variational inference and learning we have presented so far.\n\nReaders who do not plan to derive  https://www.deeplearningbook.org/contents/inference.html    or implement variational earning, algorithms may sately skip to the next section without missing any new high-level concepts. Readers who proceed with the binary sparse coding example are encouraged to review the list of useful properties of functions that commonly arise in probabilistic models in section 3.10", "f75891c8-faa1-4987-935b-886913c81a9f": "Each element of the vector may be interpreted as playing a role analogous to a neuron. Rather than thinking of the layer as representing a single vector-to-vector function, we can also think of the layer as consisting of many units that act in parallel, each representing a vector-to-scalar function. Each unit resembles a neuron in the sense that it receives input from many other units and computes its own activation value. The idea of using many layers of vector-valued representations is drawn from neuroscience. The choice of the functions f(a) used to compute these representations is also loosely guided by neuroscientific observations about the functions that biological neurons compute. Modern neural network research, however, is guided by many mathematical and engineering disciplines, and the goal of neural networks is not to perfectly model the brain. It is best to think of feedforward networks as function approximation machines that are designed to achieve statistical generalization, occasionally drawing some insights from what we know about the brain, rather than as models of brain function.\n\nOne way to understand feedforward networks is to begin with linear models and consider how to overcome their limitations", "a9a2e31b-ad03-46d6-89af-3e0922b3de7c": "The total number of network outputs is given by (K + 2)L, as compared with the usual K outputs for a network, which simply predicts the conditional means of the target variables.\n\nThe mixing coef\ufb01cients must satisfy the constraints which can be achieved using a set of softmax outputs Similarly, the variances must satisfy \u03c32 k(x) \u2a7e 0 and so can be represented in terms of the exponentials of the corresponding network activations using Finally, because the means \u00b5k(x) have real components, they can be represented directly by the network output activations The adaptive parameters of the mixture density network comprise the vector w of weights and biases in the neural network, that can be set by maximum likelihood, or equivalently by minimizing an error function de\ufb01ned to be the negative logarithm of the likelihood. For independent data, this error function takes the form where we have made the dependencies on w explicit. In order to minimize the error function, we need to calculate the derivatives of the error E(w) with respect to the components of w. These can be evaluated by using the standard backpropagation procedure, provided we obtain suitable expressions for the derivatives of the error with respect to the output-unit activations", "e4afb28d-0639-4ab1-b854-6aa3479a3f29": "O\u21b5-policy eligibility traces deal e\u21b5ectively with the \ufb01rst part of the challenge, correcting for the expected value of the targets, but not at all with the second part of the challenge, having to do with the distribution of updates. Algorithmic strategies for meeting the second part of the challenge of o\u21b5-policy learning with eligibility traces are summarized in Section 12.11. Several methods have been proposed over the years to extend Q-learning to eligibility traces. The original is Watkins\u2019s Q(\u03bb), which decays its eligibility traces in the usual way as long as a greedy action was taken, then cuts the traces to zero after the \ufb01rst non-greedy action. The backup diagram for Watkins\u2019s Q(\u03bb) is shown in Figure 12.12.\n\nIn Chapter 6, we uni\ufb01ed Q-learning and Expected Sarsa in the o\u21b5-policy version of the latter, which includes Q-learning as a special case, and generalizes it to arbitrary target policies, and in the previous section of this chapter we completed our treatment of Expected Sarsa by generalizing it to o\u21b5-policy eligibility traces", "59cb792f-789a-4347-9f22-172e308f18dd": "The action-value function e\u21b5ectively caches the results of all one-step-ahead searches.\n\nIt provides the optimal expected long-term return as a value that is locally and immediately available for each state\u2013action pair. Hence, at the cost of representing a function of state\u2013action pairs, instead of just of states, the optimal actionvalue function allows optimal actions to be selected without having to know anything about possible successor states and their values, that is, without having to know anything about the environment\u2019s dynamics. Example 3.8: Solving the Gridworld Suppose we solve the Bellman equation for v\u21e4 for the simple grid task introduced in Example 3.5 and shown again in Figure 3.5 (left). Recall that state A is followed by a reward of +10 and transition to state A0, while state B is followed by a reward of +5 and transition to state B0. Figure 3.5 (middle) shows the optimal value function, and Figure 3.5 (right) shows the corresponding optimal policies. Where there are multiple arrows in a cell, all of the corresponding actions are optimal. (3.19), we can explicitly give the Bellman optimality equation for the recycling robot example", "45d2d757-bee3-413c-a0f4-653bc0523101": "Thus, the overall e\u21b5ect was that of a backing-up over one full move of real events and then a search over possible events, as suggested by Figure 16.2. Samuel\u2019s actual algorithm was signi\ufb01cantly more complex than this for computational reasons, but this was the basic idea. Samuel did not include explicit rewards. Instead, he \ufb01xed the weight of the most important feature, the piece advantage feature, which measured the number of pieces the program had relative to how many its opponent had, giving higher weight to kings, and including re\ufb01nements so that it was better to trade pieces when winning than when losing. Thus, the goal of Samuel\u2019s program was to improve its piece advantage, which in checkers is highly correlated with winning. However, Samuel\u2019s learning method may have been missing an essential part of a sound temporal-di\u21b5erence algorithm. Temporal-di\u21b5erence learning can be viewed as a way of making a value function consistent with itself, and this we can clearly see in Samuel\u2019s method. But also needed is a way of tying the value function to the true value of the states", "8af241c3-8945-46f7-99cc-d69587f9d31b": "We therefore seek a method that can quickly identify an appropriate dependency structure from the labeling function outputs \ufffd alone. Naively, we could include all dependencies of interest, such as all pairwise correlations, in the generative model and perform parameter estimation. However, this approach is impractical. For 100 labeling functions and 10,000 data points, estimating parameters with all possible correlations takes roughly 45 min.\n\nWhen multiplied over repeated runs of hyperparameter searching and development cycles, this cost greatly inhibits labeling function development. We therefore turn to our method for automatically selecting which dependencies to model without access to ground truth . It uses a pseudolikelihood estimator, which does not require any sampling or other approximations to compute the objective gradient exactly. It is much faster than maximum likelihood estimation, taking 15 s to select pairwise correlations to be modeled among 100 labeling functions with 10,000 data points. However, this approach relies on a selection threshold hyperparameter \u03f5 which induces a trade-off space between predictive performance and computational cost. Such structure learning methods, whether pseudolikelihood or likelihood-based, crucially depend on a selection threshold \u03f5 for deciding which dependencies to add to the generative model", "07260e92-9a97-44f8-a882-3a4a91ec50bc": "This is an undiscounted, episodic task. The reward is \u22121 on all transitions until the terminal state is reached. The terminal state is shaded in the \ufb01gure (although it is shown in two places, it is formally one state). The expected reward function is thus r(s, a, s0) = \u22121 for all states s, s0 and actions a. Suppose the agent follows the equiprobable random policy (all actions equally likely). The left side of Figure 4.1 shows the sequence of value functions {vk} computed by iterative policy evaluation. The \ufb01nal estimate is in fact v\u21e1, which in this case gives for each state the negation of the expected number of steps from that state until termination.\n\nExercise 4.2 In Example 4.1, suppose a new state 15 is added to the gridworld just below state 13, and its actions, left, up, right, and down, take the agent to states 12, 13, 14, and 15, respectively. Assume that the transitions from the original states are unchanged", "5baf8692-7769-4703-a771-adea682e8640": "This is easiest to see in the binary case: the number of possible binary functions on vectors v \u20ac {0, 1}\u201d is 2?\" and selecting one such function requires 2\u201d bits, which will in general require O(2\u201d) degrees of freedom. In summary, a feedforward network with a single layer is sufficient to represent any function, but the layer may be infeasibly large and may fail to learn and generalize correctly. In many circumstances, using deeper models can reduce the number of units required to represent the desired function and can reduce the amount of generalization error.\n\nVarious families of functions can be approximated efficiently by an architecture with depth greater than some value d, but they require a much larger model if depth is restricted to be less than or equal to d. In many cases, the number of hidden units required by the shallow model is exponential in n. Such results  195  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  Figure 6.5: An intuitive, geometric explanation of the exponential advantage of deeper rectifier networks formally by Montufar ef al. (Left) An absolute value rectification unit has the same output for every pair of mirror points in its input", "693cea65-2323-47b5-a193-e51c04ed4dd9": "Sometimes, we can solve for the step size that makes the directional derivative vanish. Another approach is to evaluate f (a \u2014 \u20acVaf(x)) for several values of \u20ac and choose the one that results in the smallest objective function value. This last strategy is called a line search. Steepest descent converges when every element of the gradient is zero (or, in practice, very close to zero).\n\nIn some cases, we may be able to avoid running this iterative algorithm and just jump directly to the critical point by solving the equation Vaz f(x) = 0 for a. Although gradient descent is limited to optimization in continuous spaces, the general concept of repeatedly making a small move (that is approximately the best small move) toward better configurations can be generalized to discrete spaces. Ascending an objective function of discrete parameters is called hill climbing . 4.3.1 Beyond the Gradient: Jacobian and Hessian Matrices  https://www.deeplearningbook.org/contents/numerical.html    Sometimes we need to find all the partial derivatives of a function whose input and output are both vectors. The matrix containing all such partial derivatives is known as a Jacobian matrix", "569cd45f-185c-4c12-9337-04de301c085f": "Specifically, if we have a function f :R\u2122\u201d\u2014 R\u201d, then the Jacobian matrix J \u00a2 R\"*\u2122 of f is defined such that Jj; = f(x):  J  We are also sometimes interested in a derivative of a derivative. This is known as a second derivative. For example, for a function f : RR\u201d > R, the derivative  with respect to a; of the derivative of f with respect to x; is denoted as aaa  In a single dimension, we can denote ae f by f\"(z). The second derivative tells us how the first derivative will change as we vary the input. This is important because it tells us whether a gradient step will cause as much of an improvement as we would expect based on the gradient alone.\n\nWe can think of the second derivative as measuring curvature. Suppose we have a quadratic function (many functions that arise in practice are not quadratic but can be approximated well as quadratic, at least locally). If such a function has a second derivative of zero, then there is no curvature", "90aeb05c-6a09-4b2a-9a5a-13a5d28ff56e": "the short-corridor gridword (Example 13.1). Here the approximate state-value function used in the baseline is \u02c6v(s,w) = w. That is, w is a single component, w. Although the REINFORCE-with-baseline method learns both a policy and a state-value function, we do not consider it to be an actor\u2013critic method because its state-value function is used only as a baseline, not as a critic. That is, it is not used for bootstrapping (updating the value estimate for a state from the estimated values of subsequent states), but only as a baseline for the state whose estimate is being updated. This is a useful distinction, for only through bootstrapping do we introduce bias and an asymptotic dependence on the quality of the function approximation. As we have seen, the bias introduced through bootstrapping and reliance on the state representation is often bene\ufb01cial because it reduces variance and accelerates learning", "a09b93a4-2da9-4c94-bc08-48be9e624131": "We therefore choose a prior, called the beta distribution, given by where \u0393(x) is the gamma function de\ufb01ned by (1.141), and the coef\ufb01cient in (2.13) ensures that the beta distribution is normalized, so that Exercise 2.5 The parameters a and b are often called hyperparameters because they control the distribution of the parameter \u00b5. Figure 2.2 shows plots of the beta distribution for various values of the hyperparameters. The posterior distribution of \u00b5 is now obtained by multiplying the beta prior (2.13) by the binomial likelihood function (2.9) and normalizing. Keeping only the factors that depend on \u00b5, we see that this posterior distribution has the form where l = N \u2212 m, and therefore corresponds to the number of \u2018tails\u2019 in the coin example", "881322ea-b1c5-4eb6-a39e-8e7ccc2df93c": "Several widely used techniques are examples of linear-Gaussian models, such as probabilistic principal component analysis, factor analysis, and linear dynamical systems . We shall make extensive use of the results of this section in later chapters when we consider some of these techniques in detail. Consider an arbitrary directed acyclic graph over D variables in which node i represents a single continuous random variable xi having a Gaussian distribution. The mean of this distribution is taken to be a linear combination of the states of its parent nodes pai of node i where wij and bi are parameters governing the mean, and vi is the variance of the conditional distribution for xi. The log of the joint distribution is then the log of the product of these conditionals over all nodes in the graph and hence takes the form where x = (x1, . , xD)T and \u2018const\u2019 denotes terms independent of x. We see that this is a quadratic function of the components of x, and hence the joint distribution p(x) is a multivariate Gaussian", "49eae1ad-5e8a-4145-acc4-542d43ac5c25": "The term \u201coptimal control\u201d came into use in the late 1950s to describe the problem of designing a controller to minimize or maximize a measure of a dynamical system\u2019s behavior over time. One of the approaches to this problem was developed in the mid-1950s by Richard Bellman and others through extending a nineteenth century theory of Hamilton and Jacobi.\n\nThis approach uses the concepts of a dynamical system\u2019s state and of a value function, or \u201coptimal return function,\u201d to de\ufb01ne a functional equation, now often called the Bellman equation. The class of methods for solving optimal control problems by solving this equation came to be known as dynamic programming . Bellman  also introduced the discrete stochastic version of the optimal control problem known as Markov decision processes (MDPs). Ronald Howard  devised the policy iteration method for MDPs. All of these are essential elements underlying the theory and algorithms of modern reinforcement learning. Dynamic programming is widely considered the only feasible way of solving general stochastic optimal control problems. It su\u21b5ers from what Bellman called \u201cthe curse of dimensionality,\u201d meaning that its computational requirements grow exponentially with the number of state variables, but it is still far more e\ufb03cient and more widely applicable than any other general method", "8e8de449-9d35-4f23-b250-2ba35b2a2777": "This solution relies on at least three assumptions that are rarely true in practice: (1) we accurately know the dynamics of the environment; (2) we have enough computational resources to complete the computation of the solution; and (3) the Markov property. For the kinds of tasks in which we are interested, one is generally not able to implement this solution exactly because various combinations of these assumptions are violated. For example, although the \ufb01rst and third assumptions present no problems for the game of backgammon, the second is a major impediment. Because the game has about 1020 states, it would take thousands of years on today\u2019s fastest computers to solve the Bellman equation for v\u21e4, and the same is true for \ufb01nding q\u21e4.\n\nIn reinforcement learning one typically has to settle for approximate solutions. Many di\u21b5erent decision-making methods can be viewed as ways of approximately solving the Bellman optimality equation. For example, heuristic search methods can be viewed as expanding the right-hand side of (3.19) several times, up to some depth, forming a \u201ctree\u201d of possibilities, and then using a heuristic evaluation function to approximate v\u21e4 at the \u201cleaf\u201d nodes", "56b10336-be8b-479f-8563-5009844672f2": "They are frequently referred to in the literature as \u201challucinations\u201d or \u201cfantasy particles.\u201d In fact, the negative phase has been proposed as a possible explanation  606  CHAPTER 18.\n\nCONFRONTING THE PARTITION FUNCTION  The positive phase The negative phase  \u2014 = Pmodel (a)  Figure 18.1: The view of algorithm 18.1 as having a \u201cpositive phase\u201d and a \u201cnegative phase.\u201d (Left)In the positive phase, we sample points from the data distribution and push up on their unnormalized probability. This means points that are likely in the data get pushed up on more. (Right)In the negative phase, we sample points from the model distribution and push down on their unnormalized probability. This counteracts the positive phase\u2019s tendency to just add a large constant to the unnormalized probability everywhere. When the data distribution and the model distribution are equal, the positive phase has the same chance to push up at a point as the negative phase has to push down. When this occurs, there is no longer any gradient (in expectation), and training must terminate. for dreaming in humans and other animals , the idea being that the brain maintains a probabilistic model of the world and follows the  a", "a4920f96-7ee2-489b-bd14-8b7e8e9e1ad4": "From the tee, the best sequence of actions is two drives and one putt, sinking the ball in three strokes. Because v\u21e4 is the value function for a policy, it must satisfy the self-consistency condition given by the Bellman equation for state values (3.14). Because it is the optimal value function, however, v\u21e4\u2019s consistency condition can be written in a special form without reference to any speci\ufb01c policy. This is the Bellman equation for v\u21e4, or the Bellman optimality equation. Intuitively, the Bellman optimality equation expresses the fact that the value of a state under an optimal policy must equal the expected return for the best action from that state: The last two equations are two forms of the Bellman optimality equation for v\u21e4.\n\nThe Bellman optimality equation for q\u21e4 is The backup diagrams in the \ufb01gure below show graphically the spans of future states and actions considered in the Bellman optimality equations for v\u21e4 and q\u21e4. These are the same as the backup diagrams for v\u21e1 and q\u21e1 presented earlier except that arcs have been added at the agent\u2019s choice points to represent that the maximum over that choice is taken rather than the expected value given some policy", "632360a3-ae18-4fa8-9d08-1e222634e31b": "For a typical dice roll there might be 20 di\u21b5erent ways of playing. In considering future moves, such as the response of the opponent, one must consider the possible dice rolls as well. The result is that the game tree has an e\u21b5ective branching factor of about 400. This is far too large to permit e\u21b5ective use of the conventional heuristic search methods that have proved so e\u21b5ective in games like chess and checkers.\n\nOn the other hand, the game is a good match to the capabilities of TD learning methods. Although the game is highly stochastic, a complete description of the game\u2019s state is available at all times. The game evolves over a sequence of moves and positions until \ufb01nally ending in a win for one player or the other, ending the game. The outcome can be interpreted as a \ufb01nal reward to be predicted. On the other hand, the theoretical results we have described so far cannot be usefully applied to this task. The number of states is so large that a lookup table cannot be used, and the opponent is a source of uncertainty and time variation. TD-Gammon used a nonlinear form of TD(\u03bb)", "6429a3f8-bb52-4707-82f3-8daf00372759": "This cost is generally negligible, because it is acceptable to store these parameters in a slower and larger form of memory (for example, training in GPU memory, but storing the optimal parameters in host memory or on a disk drive). Since the best parameters are written to infrequently and never read during training, these occasional slow writes have little effect on the total training time. Early stopping is an unobtrusive form of regularization, in that it requires  https://www.deeplearningbook.org/contents/regularization.html    almost no change in the underlying training procedure, the objective function, or the set of allowable parameter values. This means that it is easy to use early stopping without damaging the learning dynamics. This is in contrast to weight decay, where one must be careful not to use too much weight decay and trap the network in a bad local minimum corresponding to a solution with pathologically small weights. Early stopping may be used either alone or in conjunction with other regulariza-  ion strategies.\n\nEven when using regularization strategies that modify the objective unction to encourage better generalization, it is rare for the best generalization to occur at a local minimum of the training objective. Early stopping requires a validation set, which means some training data is not fed to the model", "32fb13c9-c906-4ddd-bf6a-278b55c1dedf": "~ p(x) - ~ Sample an ~ Tn ( (xt? | wi?) (k) (k)  k _ Sample a(t) i~ Tn \u20142(Xtin\u2014 1 | Bin)  In  (k)  \u2014 Sample @,) ~ Thy xi | vw.)\n\ne end  For sample k, we can derive the importance weight by chaining together the importance weights for the jumps between the intermediate distributions given in  equation 18.49: k ~ k k) Pn (a?) Broly?) PL (wt 5)  k k)\\ 0\u00b0 x k)\\* Bo(a\\n\u2019) Bn (a?) Brn s(h,?) w/  (18.52)  To avoid numerical issues such as overflow, it is probably best to compute log w(*) by adding and subtracting log probabilities, rather than computing w\u201c*) by multiplying and dividing probabilities", "16bac2bf-6d03-4210-995c-8d837a561438": "Since the moves were selected on the basis of these evaluations, the initial moves were inevitably poor, and the initial games often lasted hundreds or thousands of moves before one side or the other won, almost by accident.\n\nAfter a few dozen games however, After playing about 300,000 games against itself, TD-Gammon 0.0 as described above learned to play approximately as well as the best previous backgammon computer programs. This was a striking result because all the previous high-performance computer programs had used extensive backgammon knowledge. For example, the reigning champion program at the time was, arguably, Neurogammon, another program written by Tesauro that used a neural network but not TD learning. Neurogammon\u2019s network was trained on a large training corpus of exemplary moves provided by backgammon experts, and, in addition, started with a set of features specially crafted for where wt is the vector of all modi\ufb01able parameters (in this case, the weights of the network) and et is a vector of eligibility traces, one for each component of wt, updated by with e0 = 0. The gradient in this equation can be computed e\ufb03ciently by the backpropagation procedure", "a96edce1-a0a4-4220-8eb6-ef7949bfe5ee": "It has been proved in many different settings that organizing computation hrough the composition of many nonlinearities and a hierarchy of reused features can give an exponential boost to statistical efficiency, on top of the exponential boost given by using a distributed representation.\n\nMany kinds of networks (e.g., with saturating nonlinearities, Boolean gates, sum/products, or RBF units) with a single hidden layer can be shown to be universal approximators. A model family that is a universal approximator can approximate a large class of functions including all continuous functions) up to any nonzero tolerance level, given enough hidden units. However, the required number of hidden units may be very large. Theoretical results concerning the expressive power of deep architectures state that here are families of functions that can be represented efficiently by an architecture of depth k, but that would require an exponential number of hidden units (with respect to the input size) with insufficient depth (depth 2 or depth k \u2014 1). In section 6.4.1, we saw that deterministic feedforward networks are universal approximators of functions", "bcfeecfa-a3f7-4496-845c-3e5c630cb24d": "On the other hand, when we refer to a signal in the brain, we mean a physiological event such as a burst of action potentials or the secretion of a neurotransmitter. Labeling a neural signal by its function, for example calling the phasic activity of a dopamine neuron a reinforcement signal, means that the neural signal behaves like, and is conjectured to function like, the corresponding theoretical signal.\n\nactivity related to reward processing can be found in nearly every part of the brain, and it is di\ufb03cult to interpret results unambiguously because representations of di\u21b5erent reward-related signals tend to be highly correlated with one another. Experiments need to be carefully designed to allow one type of reward-related signal to be distinguished with any degree of certainty from others\u2014or from an abundance of other signals not related to reward processing. Despite these di\ufb03culties, many experiments have been conducted with the aim of reconciling aspects of reinforcement learning theory and algorithms with neural signals, and some compelling links have been established. To prepare for examining these links, in the rest of this section we remind the reader of what various reward-related signals mean according to reinforcement learning theory", "e65d0d29-4e4e-47f9-8498-721a3ea3a324": "Schwenk and Gauvain  and Schwenk  built upon this approach by splitting the vocabulary V into a shortlist L of most frequent words (handled by the neural net) and a tail T = V\\L of more rare words (handled by an n-gram model). To be able to combine the two predictions, the neural net also has to predict the probability that a word appearing after context C belongs to the tail list. This may be achieved by adding an extra sigmoid output unit to provide an estimate of P(i \u20ac T | C). The extra output can then be used to achieve an estimate of the probability distribution over all words in V as follows:  Py=i|C) =laPly=t|C,ie L)1-PGeT|C)) +lietP(y =i| C,ie T)P(iE T| C), (12.10)  where P(y =i | C,i \u20ac L) is provided by the neural language model and P(y =? | C,i \u20ac T) is provided by the n-gram model", "12db44ba-e4e8-4882-825a-7a00c6286e8e": "\u2018l\u2019he n-gram-based models used for machine translation include not just traditional back-off -gram models  but also maximum entropy language models , in  which an affine-softmax layer predicts the next word given the presence of frequent n-grams in the context. Traditional language models simply report the probability of a natural language sentence. Because machine translation involves producing an output sentence given an input sentence, it makes sense to extend the natural language model to be conditional.\n\nAs described in section 6.2.1.1, it is straightforward to extend a model that defines a marginal distribution over some variable to define a conditional distribution over that variable given a context C\u2019, where C might be a single variable or a list of variables. Devlin et al. beat the state-of-the-art in some statistical machine translation benchmarks by using an MLP to score a phrase ty, tz,...,tr in the target language given a phrase s1,59,...,8,, in the source language. The MEP estimates P(t;, ta,...,t, | $1,82,---,8,). The estimate formed by this MLP replaces the estimate provided by conditional n-gram models", "4a9505bb-9a3e-430d-a3d1-e0281149c650": "If \u21b5 is chosen larger than is optimal, however, then the \u03bb-return algorithm is only a little worse whereas TD(\u03bb) is much worse and may even be unstable.\n\nThis is not catastrophic for TD(\u03bb) on this problem, as these higher parameter values are not what one would want to use anyway, but for other problems it can be a signi\ufb01cant weakness. Linear TD(\u03bb) has been proved to converge in the on-policy case if the step-size parameter is reduced over time according to the usual conditions (2.7). Just as discussed in Section 9.4, convergence is not to the minimum-error weight vector, but to a nearby weight vector that depends on \u03bb. The bound on solution quality presented in that section (9.14) can now be generalized to apply for any \u03bb. For the continuing discounted case, \u03bb approaches 1, the bound approaches the minimum error (and it is loosest at \u03bb = 0). In practice, however, \u03bb = 1 is often the poorest choice, as will be illustrated later in Figure 12.14", "192e4a6a-7e0b-4ce6-b769-2d9c0391dcc8": "If the probability of a real-valued variable x falling in the interval (x, x + \u03b4x) is given by p(x)\u03b4x for \u03b4x \u2192 0, then p(x) is called the probability density over x. This is illustrated in Figure 1.12. The probability that x will lie in an interval (a, b) is then given by Because probabilities are nonnegative, and because the value of x must lie somewhere on the real axis, the probability density p(x) must satisfy the two conditions Under a nonlinear change of variable, a probability density transforms differently from a simple function, due to the Jacobian factor. For instance, if we consider a change of variables x = g(y), then a function f(x) becomes \ufffdf(y) = f(g(y))", "65c660b8-db95-4769-bb54-fd3a3d322147": "Show that the variable y = b tan z + c has a Cauchy distribution given by (11.16). 11.9 (\u22c6 \u22c6) By making use of the technique discussed in Section 11.1.1 for sampling from a single exponential distribution, devise an algorithm for sampling from the piecewise exponential distribution de\ufb01ned by (11.17). 11.10 (\u22c6) Show that the simple random walk over the integers de\ufb01ned by (11.34), (11.35), and (11.36) has the property that E = E + 1/2 and hence by induction that E = \u03c4/2. satis\ufb01es detailed balance as de\ufb01ned by (11.40). 11.12 (\u22c6) Consider the distribution shown in Figure 11.15.\n\nDiscuss whether the standard Gibbs sampling procedure for this distribution is ergodic, and therefore whether it would sample correctly from this distribution 11.13 (\u22c6 \u22c6) Consider the simple 3-node graph shown in Figure 11.16 in which the observed node x is given by a Gaussian distribution N(x|\u00b5, \u03c4 \u22121) with mean \u00b5 and precision \u03c4", "bb0ab8a9-98f4-49e4-8901-724b675c2a6a": "(Left) To perform grid search, we provide a set of values for each hyperparameter. The search algorithm runs training for every joint hyperparameter setting in the cross product of these sets. (Right) To perform random search, we provide a probability distribution over joint hyperparameter configurations. Usually most of these hyperparameters are independent rom each other. Common choices for the distribution over a single hyperparameter include uniform and log-uniform (to sample from a log-uniform distribution, take theexp of a sample from a uniform distribution). The search algorithm then randomly samples joint hyperparameter configurations and runs training with each of them. Both grid search and random search evaluate the validation set error and return the best configuration.\n\nThe figure illustrates the typical case where only some hyperparameters have a significant influence on the result. In this illustration, only the hyperparameter on the horizontal axis has a significant effect. Grid search wastes an amount of computation that is exponential in the number of noninfluential hyperparameters, while random search tests a unique value of every influential hyperparameter on nearly every trial. Figure reproduced with ermission from Bergstra and Bengio", "5017c295-664c-41a9-a46c-67313186db36": "The computation time needed by a rollout algorithm depends on the number of actions that have to be evaluated for each decision, the number of time steps in the simulated trajectories needed to obtain useful sample returns, the time it takes the rollout policy to make decisions, and the number of simulated trajectories needed to obtain good Monte Carlo action-value estimates.\n\nBalancing these factors is important in any application of rollout methods, though there are several ways to ease the challenge. Because the Monte Carlo trials are independent of one another, it is possible to run many trials in parallel on separate processors. Another approach is to truncate the simulated trajectories short of complete episodes, correcting the truncated returns by means of a stored evaluation function (which brings into play all that we have said about truncated returns and updates in the preceding chapters). It is also possible, as Tesauro and Galperin  suggest, to monitor the Monte Carlo simulations and prune away candidate actions that are unlikely to turn out to be the best, or whose values are close enough to that of the current best that choosing them instead would make no real di\u21b5erence (though Tesauro and Galperin point out that this would complicate a parallel implementation)", "01f3edc9-44fc-43c2-93a2-e4aea4069c08": "Their original dataset contains 182 CT scans, (53 Cysts, 64 Metastases, and 65 Hemangiomas).\n\nAfter using classical augmentations to achieve 78.6% sensitivity and 88.4% specificity, they observed an increase to 85.7% sen- sitivity and 92.4% specificity once they added the DCGAN-generated samples. Another architecture of interest is known as Progressively Growing GANs . This architecture trains a series of networks with progressive resolution complexity. These resolutions range from 4 x 4 to 8 x 8 and so on until outputs of size 1024 x 1024 are achieved. This is built on the concept that GANs can accept images as input as well as random vectors. Therefore, the series of GANs work by passing samples from a lower resolution GAN up to higher-resolution GANs. This has produced very amaz- ing results on facial images. In addition to improving the resolution size of GANs, another interesting architecture that increases the quality of outputs is the CycleGAN  proposed by Zhu et al. Cycle- GAN introduces an additional Cycle-Consistency loss function to help stabilize GAN training", "7a2f0bbc-0dbb-4870-b45f-f2735e55eff3": "It is worth emphasizing that this is achieved using purely graphical operations! The junction tree is exact for arbitrary graphs and is ef\ufb01cient in the sense that for a given graph there does not in general exist a computationally cheaper approach. Unfortunately, the algorithm must work with the joint distributions within each node (each of which corresponds to a clique of the triangulated graph) and so the computational cost of the algorithm is determined by the number of variables in the largest clique and will grow exponentially with this number in the case of discrete variables. An important concept is the treewidth of a graph , which is de\ufb01ned in terms of the number of variables in the largest clique.\n\nIn fact, it is de\ufb01ned to be as one less than the size of the largest clique, to ensure that a tree has a treewidth of 1. Because there in general there can be multiple different junction trees that can be constructed from a given starting graph, the treewidth is de\ufb01ned by the junction tree for which the largest clique has the fewest variables. If the treewidth of the original graph is high, the junction tree algorithm becomes impractical. For many problems of practical interest, it will not be feasible to use exact inference, and so we need to exploit effective approximation methods", "2f3fc0f1-4438-420c-ada5-61902a5e2279": "Deep networks for speech recognition eventually shifted from being based on pretraining and Boltzmann machines to being based on techniques such as rectified linear units and dropout . By that time, several of the major speech groups in industry had started exploring deep learning in collaboration with academic researchers. Hinton  454  CHAPTER 12. APPLICATIONS  et al.\n\ndescribe the breakthroughs achieved by these collaborators, which are now deployed in products such as mobile phones. Later, as these groups explored larger and larger labeled datasets and incorpo- rated some of the methods for initializing, training, and setting up the architecture of deep nets, they realized that the unsupervised pretraining phase was either unnecessary or did not bring any significant improvement. These breakthroughs in recognition performance for word error rate in speech recognition were unprecedented (around 30 percent improvement) and were follow- ing a long period, of about ten years, during which error rates did not improve much with the traditional GMM-HMM technology, in spite of the continuously growing size of training sets (see figure 2.4 of Deng and Yu ). This created a rapid shift in the speech recognition community toward deep learning", "ecb918b0-61df-43ff-80f8-3ee559fb665f": "A hard-assignment version of the Gaussian mixture model with general covariance matrices, known as the elliptical K-means algorithm, has been considered by Sung and Poggio . So far in this chapter, we have focussed on distributions over continuous variables described by mixtures of Gaussians. As a further example of mixture modelling, and to illustrate the EM algorithm in a different context, we now discuss mixtures of discrete binary variables described by Bernoulli distributions.\n\nThis model is also known as latent class analysis . As well as being of practical importance in its own right, our discussion of Bernoulli mixtures will also lay the foundation for a consideration of hidden Markov models over discrete variables. Section 13.2 Consider a set of D binary variables xi, where i = 1, . , D, each of which is governed by a Bernoulli distribution with parameter \u00b5i, so that where x = (x1, . , xD)T and \u00b5 = (\u00b51, . , \u00b5D)T. We see that the individual variables xi are independent, given \u00b5", "61724d3a-7c40-43e6-bad8-f40854da729b": "Alopex: A correlation-based learning algorithm for feedforward and recurrent neural networks. N eural Computation, 6(3): 469\u2013490. Urbanowicz, R. J., Moore, J. H. Learning classi\ufb01er systems: A complete introduction, review, and roadmap. Journal of Arti\ufb01cial Evolution and Applications. 10.1155/2009/736398. Valentin, V. V., Dickinson, A., O\u2019Doherty, J. P. Determining the neural substrates of goal-directed learning in the human brain. The Journal of Neuroscience, 27(15):4019\u20134026. van Hasselt, H. Double Q-learning. In Advances in Neural Information Processing Systems 23 , pp. 2613\u20132621. Curran Associates, Inc. van Hasselt, H. Insights in Reinforcement Learning: Formal Analysis and Empirical Evaluation of Temporal-di\u21b5erence Learning. SIKS dissertation series number 2011-04. van Hasselt, H", "1ed89888-9edf-4d7b-a7b4-4e4dd4406753": "We therefore wish to make a free form (variational) optimization of L(q) with respect to all of the distributions qi(Zi), which we do by optimizing with respect to each of the factors in turn. To achieve this, we \ufb01rst substitute (10.5) into (10.3) and then dissect out the dependence on one of the factors qj(Zj). Denoting qj(Zj) by simply qj to keep the notation uncluttered, we then obtain where we have de\ufb01ned a new distribution \ufffdp(X, Zj) by the relation Here the notation Ei\u0338=j denotes an expectation with respect to the q distributions over all variables zi for i \u0338= j, so that Now suppose we keep the {qi\u0338=j} \ufb01xed and maximize L(q) in (10.6) with respect to all possible forms for the distribution qj(Zj). This is easily done by recognizing that (10.6) is a negative Kullback-Leibler divergence between qj(Zj) and \ufffdp(X, Zj)", "f7ef34a8-9f01-496e-a808-3aff60cf9e4c": "This is the solution asymptotically found by Monte Carlo methods, albeit often very slowly. The projection operation is discussed more fully in the box on the next page. TD methods \ufb01nd di\u21b5erent solutions.\n\nTo understand their rationale, recall that the For a linear function approximator, the projection operation is linear, which implies that it can be represented as an |S| \u21e5 |S| matrix: where, as in Section 9.4, D denotes the |S| \u21e5 |S| diagonal matrix with the \u00b5(s) on the diagonal, and X denotes the |S| \u21e5 d matrix whose rows are the feature vectors x(s)>, one for each state s. If the inverse in (11.14) does not exist, then the pseudoinverse is substituted. Using these matrices, the squared norm of a vector can be written and the approximate linear value function can be written The true value function v\u21e1 is the only value function that solves (11.13) exactly. If an approximate value function vw were substituted for v\u21e1, the di\u21b5erence between the right and left sides of the modi\ufb01ed equation could be used as a measure of how far o\u21b5 vw is from v\u21e1", "8021866e-fc4b-4f74-ba12-48f4b00fc515": "This gives a more principled approach to multiclass classi\ufb01cation than the pairwise method used in the support vector machine and also provides probabilistic predictions for new data points. The principal disadvantage is that the Hessian matrix has size MK \u00d7MK, where M is the number of active basis functions, which gives an additional factor of K3 in the computational cost of training compared with the two-class RVM. The principal disadvantage of the relevance vector machine is the relatively long training times compared with the SVM. This is offset, however, by the avoidance of cross-validation runs to set the model complexity parameters. Furthermore, because it yields sparser models, the computation time on test points, which is usually the more important consideration in practice, is typically much less. Exercises 7.1 (\u22c6 \u22c6) www Suppose we have a data set of input vectors {xn} with corresponding target values tn \u2208 {\u22121, 1}, and suppose that we model the density of input vectors within each class separately using a Parzen kernel density estimator (see Section 2.5.1) with a kernel k(x, x\u2032)", "8580f851-60dd-45b3-8130-aa5f9ab02c6b": "These methods retrieve a set of training examples from memory whose states are judged to be the most relevant to the query state, where relevance usually depends on the distance between states: the closer a training example\u2019s state is to the query state, the more relevant it is considered to be, where distance can be de\ufb01ned in many di\u21b5erent ways. After the query state is given a value, the local approximation is discarded. The simplest example of the memory-based approach is the nearest neighbor method, which simply \ufb01nds the example in memory whose state is closest to the query state and returns that example\u2019s value as the approximate value of the query state. In other words, if the query state is s, and s0 7! g is the example in memory in which s0 is the closest state to s, then g is returned as the approximate value of s. Slightly more complicated are weighted average methods that retrieve a set of nearest neighbor examples and return a weighted average of their target values, where the weights generally decrease with increasing distance between their states and the query state", "ff266cfc-26ca-44d7-864e-ee1303593f1f": "Each measurement comprises the duration of Appendix A the eruption in minutes (horizontal axis) and the time in minutes to the next eruption (vertical axis).\n\nWe see that the data set forms two dominant clumps, and that a simple Gaussian distribution is unable to capture this structure, whereas a linear superposition of two Gaussians gives a better characterization of the data set. Such superpositions, formed by taking linear combinations of more basic distributions such as Gaussians, can be formulated as probabilistic models known as mixture distributions . In Figure 2.22 we see that a linear combination of Gaussians can give rise to very complex densities. By using a suf\ufb01cient number of Gaussians, and by adjusting their means and covariances as well as the coef\ufb01cients in the linear combination, almost any continuous density can be approximated to arbitrary accuracy. We therefore consider a superposition of K Gaussian densities of the form which is called a mixture of Gaussians. Each Gaussian density N(x|\u00b5k, \u03a3k) is called a component of the mixture and has its own mean \u00b5k and covariance \u03a3k", "d1800b99-cd4a-495d-8458-a3a0b574402c": "Indeed, the most recent work on echo state networks advocates using a spectral radius much larger than unity . Everything we have said about back-propagation via repeated matrix multipli- cation applies equally to forward propagation in a network with no nonlinearity, where the state h\u00a2+) = AOTW.\n\nWhen a linear map W' always shrinks h as measured by the L? norm, then  400  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  we say that the map is contractive. When the spectral radius is less than one, the mapping from h\u00ae to n+ is contractive, so a small change becomes smaller after each time step. This necessarily makes the network forget information about the past when we use a finite level of precision (such as 32-bit integers) to store the state vector. The Jacobian matrix tells us how a small change of h\u00ae propagates one step forward, or equivalently, how the gradient on h\u201c+)) propagates one step backward,  ae wad 1417 T yeo4 ane! https://www.deeplearningbook.org/contents/rnn.html    during back-propagation", "67138732-7de3-412f-83a9-9b2969a880cc": "A simplified view of this distinction can be illustrated in the context of  linear regression:  CT |  ry] of 3  https://www.deeplearningbook.org/contents/regularization.html  0 0 -2 0 0 ; 0-1 0 3 0 \u20185 5 60 0 0 0 \u2014 A 0 0 -1 0 \u2014-4 ?\n\n(7.46) 0 O 0 -5 O 4 AeR\u2122 | | 0 2 -1 2 -5 4 1 a rr | a |       19 =|-1 5 4 2 -3 -2 4 2 3 1 2 3 0 -3 o (7.47) 23 5 4 -2 2 -5 -1 f  y \u20ac R\u201d\u2122 BeR\u2122\u201d he R\u201d  In the first expression, we have an example of a sparsely parametrized linear regression model. In the second, we have linear regression with a sparse representa-  251  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  tion h of the data x. That is, h is a function of x that, in some sense, represents the information present in x, but does so with a sparse vector", "7f94a595-b55d-4c7c-9cb8-642961fb10aa": "3.24 (\u22c6 \u22c6) Repeat the previous exercise but now use Bayes\u2019 theorem in the form and then substitute for the prior and posterior distributions and the likelihood function in order to derive the result (3.118). In the previous chapter, we explored a class of regression models having particularly simple analytical and computational properties. We now discuss an analogous class of models for solving classi\ufb01cation problems. The goal in classi\ufb01cation is to take an input vector x and to assign it to one of K discrete classes Ck where k = 1, . , K. In the most common scenario, the classes are taken to be disjoint, so that each input is assigned to one and only one class. The input space is thereby divided into decision regions whose boundaries are called decision boundaries or decision surfaces.\n\nIn this chapter, we consider linear models for classi\ufb01cation, by which we mean that the decision surfaces are linear functions of the input vector x and hence are de\ufb01ned by (D \u2212 1)-dimensional hyperplanes within the D-dimensional input space. Data sets whose classes can be separated exactly by linear decision surfaces are said to be linearly separable. For regression problems, the target variable t was simply the vector of real numbers whose values we wish to predict", "395a6c60-c58c-4b18-901f-1f596af0b657": "If we denote Exercise 14.12 the mixing coef\ufb01cients by \u03c0k, then the mixture distribution can be written where \u03b8 denotes the set of all adaptive parameters in the model, namely W = {wk}, \u03c0 = {\u03c0k}, and \u03b2. The log likelihood function for this model, given a data set of observations {\u03c6n, tn}, then takes the form where t = (t1, . , tN)T denotes the vector of target variables. In order to maximize this likelihood function, we can once again appeal to the EM algorithm, which will turn out to be a simple extension of the EM algorithm for unconditional Gaussian mixtures of Section 9.2. We can therefore build on our experience with the unconditional mixture and introduce a set Z = {zn} of binary latent variables where znk \u2208 {0, 1} in which, for each data point n, all of the elements k = 1, . , K are zero except for a single value of 1 indicating which component of the mixture was responsible for generating that data point", "324d7de9-299d-415e-a4a7-6c53ef38e326": "Why not take the minimization of the expected square of the TD error as the objective? In the general function-approximation case, the one-step TD error with discounting is A possible objective function then is what one might call the Mean Squared TD Error: The last equation is of the form needed for SGD; it gives the objective as an expectation that can be sampled from experience (remember the experience is due to the behavior policy b). Thus, following the standard SGD approach, one can derive the per-step update based on a sample of this expected value: which you will recognize as the same as the semi-gradient TD algorithm (11.2) except for the additional \ufb01nal term. This term completes the gradient and makes this a true SGD algorithm with excellent convergence guarantees. Let us call this algorithm the naive residual-gradient algorithm . Although the naive residual-gradient algorithm converges robustly, it does not necessarily converge to a desirable place", "b6721d66-2817-4bc5-9f76-4629cbdfd631": "Barto and Anandan  introduced an associative reinforcement learning algorithm called the associative reward-penalty (AR\u2212P) algorithm. They proved a convergence result by combining theory of stochastic learning automata with theory of pattern classi\ufb01cation. Barto  and Barto and Jordan  described results with teams of AR\u2212P units connected into multi-layer ANNs, showing that they could learn nonlinear functions, such as XOR and others, with a globally-broadcast reinforcement signal. Barto  extensively discussed this approach to ANNs and how this type of learning rule is related to others in the literature at that time. Williams  mathematically analyzed and broadened this class of learning rules and related their use to the error backpropagation method for training multilayer ANNs.\n\nWilliams  described several ways that backpropagation and reinforcement learning can be combined for training ANNs. Williams  showed that a special case of the AR\u2212P algorithm is a REINFORCE algorithm, although better results were obtained with the general AR\u2212P algorithm . The third phase of interest in teams of reinforcement learning agents was in\ufb02uenced by increased understanding of the role of dopamine as a widely broadcast neuromodulator and speculation about the existence of reward-modulated STDP", "692e8401-b082-4f75-bca2-c579e0a1d250": "Note that we did not require any properties of the reward baseline other than that it does not depend on the selected action. For example, we could have set it to zero, or to 1000, and the algorithm would still be an instance of stochastic gradient ascent.\n\nThe choice of the baseline does not a\u21b5ect the expected update of the algorithm, but it does a\u21b5ect the variance of the update and thus the rate of convergence (as shown, e.g., in Figure 2.5). Choosing it as the average of the rewards may not be the very best, but it is simple and works well in practice. So far in this chapter we have considered only nonassociative tasks, that is, tasks in which there is no need to associate di\u21b5erent actions with di\u21b5erent situations. In these tasks the learner either tries to \ufb01nd a single best action when the task is stationary, or tries to track the best action as it changes over time when the task is nonstationary. However, in a general reinforcement learning task there is more than one situation, and the goal is to learn a policy: a mapping from situations to the actions that are best in those situations", "d535782b-c0fb-48e6-be15-467f8bf82ac8": "For text data augmentation, we augment each minibatch by generating two or three samples for each data points (each with 1, 2 or 3 substitutions), and use both the samples and the original data to train the model.\n\nFor data weighting, to avoid exploding value, we update the weight of each data point in a minibatch by decaying the previous weight value 1Code available at https://github.com/tanyuqian/learning-data-manipulation with a factor of 0.1 and then adding the gradient. All experiments were implemented with PyTorch (pytorch.org) and were performed on a Linux machine with 4 GTX 1080Ti GPUs and 64GB RAM. All reported results are averaged over 15 runs \u00b1 one standard deviation. We study the problem where only very few labeled examples for each class are available. Both of our augmentation and weighting boost base model performance, and are superior to respective comparison methods. We also observe that augmentation performs better than weighting in the low-data setting. Setup For text classi\ufb01cation, we use the popular benchmark datasets, including SST-5 for 5-class sentence sentiment , IMDB for binary movie review sentiment , and TREC for 6-class question types", "aea5f113-4bd4-41af-bd98-333b66c4f70b": "Here there is a continuum of parameters all of which lead to the same predictive density, in contrast to the discrete nonidentifiability associated with component re-labelling in the mixture setting.\n\nIf we consider the case of M = D, so that there is no reduction of dimensionality, then U M = U and L M = L. Making use of the orthogonality properties UUT = I and RRT = I, we see that the covariance C of the marginal distribution for x becomes and so we obtain the standard maximum likelihood solution for an unconstrained Gaussian distribution in which the covariance matrix is given by the sample covariance. Conventional PCA is generally formulated as a projection of points from the Ddimensional data space onto an M -dimensional linear subspace. Probabilistic PCA, however, is most naturally expressed as a mapping from the latent space into the data space via (12.33). For applications such as visualization and data compression, we can reverse this mapping using Bayes' theorem. Any point x in data space can then be summarized by its posterior mean and covariance in latent space. From (12.42) the mean is given by where M is given by (12.41)", "cc04d838-fc06-4397-93a7-02edab9e81e4": "2.9 The term associative search and the corresponding problem were introduced by Barto, Sutton, and Brouwer . The term associative reinforcement learning has also been used for associative search , but we prefer to reserve that term as a synonym for the full reinforcement learning problem . (And, as we noted, the modern literature also uses the term \u201ccontextual bandits\u201d for this problem.) We note that Thorndike\u2019s Law of E\u21b5ect (quoted in Chapter 1) describes associative search by referring to the formation of associative links between situations (states) and actions. According to the terminology of operant, or instrumental, conditioning , a discriminative stimulus is a stimulus that signals the presence of a particular reinforcement contingency. In our terms, di\u21b5erent discriminative stimuli correspond to di\u21b5erent states.\n\n2.10 Bellman  was the \ufb01rst to show how dynamic programming could be used to compute the optimal balance between exploration and exploitation within a Bayesian formulation of the problem. The Gittins index approach is due to Gittins and Jones . Du\u21b5  showed how it is possible to learn Gittins indices for bandit problems through reinforcement learning", "57ae1e68-61a7-4df7-b111-523feeebf75c": "Addison-Wesley, Reading, MA. Barto, A. G. Connectionist learning for control: An overview. In T. Miller, R. S. Sutton, Barto, A. G. Some learning tasks from a control perspective. In L. Nadel and D. L. Stein (Eds. ), 1990 Lectures in Complex Systems, pp. 195\u2013223. Addison-Wesley, Redwood City, CA. Barto, A. G. Reinforcement learning and adaptive critic methods. In D. A. White and D. A. Sofge (Eds. ), Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches, pp. 469\u2013491. Van Nostrand Reinhold, New York. Barto, A. G. Adaptive critics and the basal ganglia. In J. C", "2dcfa992-c10d-4415-abd4-0319ad454a2c": "The input z is sampled from a uniform distribution, and the output y is sampled from  Pmodel(y | @). The neural network is able to learn nonlinear mappings from the input to the parameters of the output distribution. These parameters include the probabilities governing which of three mixture components will generate the output as well as the parameters for each mixture component. Each mixture component is Gaussian with predicted mean and variance. All these aspects of the output distribution are able to vary with respect to the input x, and to do so in nonlinear ways. a high degree of quality in these real-valued domains. An example of a mixture density network is shown in figure 6.4.\n\nIn general, we may wish to continue to model larger vectors y containing more variables, and to impose richer and richer structures on these output variables. For example, if we want our neural network to output a sequence of characters that forms a sentence, we might continue to use the principle of maximum likelihood applied to our model p(y;w(a)). In this case, the model we use to describe y would become complex enough to be beyond the scope of this chapter", "326b31d2-1c96-4d72-aed1-b0e63e36ffa2": "This general idea might be termed backward focusing of planning computations. As the frontier of useful updates propagates backward, it often grows rapidly, producing many state\u2013action pairs that could usefully be updated. But not all of these will be equally useful. The values of some states may have changed a lot, whereas others may have changed little. The predecessor pairs of those that have changed a lot are more likely to also change a lot. In a stochastic environment, variations in estimated transition probabilities also contribute to variations in the sizes of changes and in the urgency with which pairs need to be updated. It is natural to prioritize the updates according to a measure of their urgency, and perform them in order of priority. This is the idea behind prioritized sweeping.\n\nA queue is maintained of every state\u2013action pair whose estimated value would change nontrivially if updated , prioritized by the size of the change. When the top pair in the queue is updated, the e\u21b5ect on each of its predecessor pairs is computed", "aa06549f-430e-437f-af81-6fad2779e1d0": "4.1.1, we see that on relation extraction applications the discriminative model improves performance over the generative model primarily by increasing recall by 43.15% on average. In Sect. 4.1.2, the discriminative model classi\ufb01es entirely new modalities of data to which the labeling functions cannot be applied. DataSet DetailsAdditional informationabout thesizes of the datasets is included in Table 4. Speci\ufb01cally, we report the size of the (unlabeled) training set and hand-labeled development and test sets, in terms of number of candidates. Note that the development and test sets can be orders of magnitude smaller than the training sets. Labeled development and test sets were either used when already available as part of a benchmark dataset, or labeled with the help of our collaborators, limited to several hours of labeling time maximum. Note that test Snorkel\u2019s generative and discriminative models consistently improve over distant supervision, measured in F1, the harmonic mean of precision (P) and recall (R). We compare with hand-labeled data when available, coming within an average of 1 F1 point Fig.10 Precision\u2013recall curves for the relation extraction tasks", "077ebd65-fca8-4ab0-b596-b812e2864028": "For real-valued parameters it is common to use a Gaussian as a prior distribution,  1 _ p(w) = N(w; po, Ao) exp \u20145(w \u2014 po) Ag '(w\u2014 po); (5.73) where f1p and Ag are the prior \u2018is ibution mean vector and colariance matrix respectively.+  With the prior thus specified, we can now proceed in determining the posterior  https://www.deeplearningbook.org/contents/ml.html    distribution over the model parameters: p(w | X,y) x ply | X,w)p(w) (5.74)  \u2018Unless there is a reason to use a particular covariance structure, we typically assume a diagonal covariance matrix Ao = diag(Xo). 135  CHAPTER 5", "d674b73d-0736-44f7-adf1-275979c6b090": "There are, however, some substantial advantages.\n\nFirst of all, the singularities that arise in maximum likelihood when a Gaussian component \u2018collapses\u2019 onto a speci\ufb01c data point are absent in the Bayesian treatment. Indeed, these singularities are removed if we simply introduce a prior and then use a MAP estimate instead of maximum likelihood. Furthermore, there is no over-\ufb01tting if we choose a large number K of components in the mixture, as we saw in Figure 10.6. Finally, the variational treatment opens up the possibility of determining the optimal number of components in the mixture without resorting to techniques such as cross validation. Section 10.2.4 We can also straightforwardly evaluate the lower bound (10.3) for this model. In practice, it is useful to be able to monitor the bound during the re-estimation in order to test for convergence. It can also provide a valuable check on both the mathematical expressions for the solutions and their software implementation, because at each step of the iterative re-estimation procedure the value of this bound should not decrease", "ccd1de71-cfda-46b7-a1a3-6c387345b83b": "This result differs slightly from standard belief propagation in that messages are passed in both directions at the same time. We can easily modify the EP procedure to give the standard form of the sum-product algorithm by updating just one of the factors at a time, for instance if we re\ufb01ne only \ufffdfb3(x3), then \ufffdfb2(x2) is unchanged by de\ufb01nition, while the re\ufb01ned version of \ufffdfb3(x3) is again given by (10.235). If we are re\ufb01ning only one term at a time, then we can choose the order in which the re\ufb01nements are done as we wish. In particular, for a tree-structured graph we can follow a two-pass update scheme, corresponding to the standard belief propagation schedule, which will result in exact inference of the variable and factor marginals. The initialization of the approximation factors in this case is unimportant.\n\nNow let us consider a general factor graph corresponding to the distribution where \u03b8i represents the subset of variables associated with factor fi. We approximate this using a fully factorized distribution of the form where \u03b8k corresponds to an individual variable node. Suppose that we wish to re\ufb01ne the particular term \ufffdfjl(\u03b8l) keeping all other terms \ufb01xed", "a67ae691-5040-4f05-bf71-9d715767304a": "Again, if we examine the results (2.150) and (2.151) for the posterior distribution of \u03bb, we see that for a0 = b0 = 0, the posterior depends only on terms arising from the data and not from the prior.\n\nThroughout this chapter, we have focussed on the use of probability distributions having speci\ufb01c functional forms governed by a small number of parameters whose values are to be determined from a data set. This is called the parametric approach to density modelling. An important limitation of this approach is that the chosen density might be a poor model of the distribution that generates the data, which can result in poor predictive performance. For instance, if the process that generates the data is multimodal, then this aspect of the distribution can never be captured by a Gaussian, which is necessarily unimodal. In this \ufb01nal section, we consider some nonparametric approaches to density estimation that make few assumptions about the form of the distribution. Here we shall focus mainly on simple frequentist methods. The reader should be aware, however, that nonparametric Bayesian methods are attracting increasing interest", "19d0ac41-1579-4dc4-ae9f-62691dde43c1": "A pallidus-habenulaBrowne, C.B., Powley, E., Whitehouse, D., Lucas, S.M., Cowling, P.I., Rohlfshagen, P., Tavener, S., Perez, D., Samothrakis, S., Colton, S. A survey of monte carlo tree search methods. IEEE Transactions on Computational Intelligence and AI in Games, 4(1):1\u201343.\n\nand inhibitory learning pathways to selectively respond to unexpected rewarding cues. The Journal of Neuroscience, 19(23):10502\u201310511. Bryson, A. E., Jr. Optimal control\u20141950 to 1985. IEEE Control Systems, 13(3):26\u201333. Buchanan, B. G., Mitchell, T., Smith, R. G., Johnson, C. R., Jr. Models of learning systems. Encyclopedia of Computer Science and technology, 11. Buhusi, C. V., Schmajuk, N. A", "4e22bca3-de16-4338-8e54-d8ed43cb8c42": "compared the results of two algorithms for learning ad recommendation policies. The \ufb01rst algorithm, which they called greedy optimization, had the goal of maximizing only the probability of immediate clicks. As in the standard contextual bandit formulation, this algorithm did not take the long-term e\u21b5ects of recommendations into account. The other algorithm, a reinforcement learning algorithm based on an MDP formulation, aimed at improving the number of clicks users made over multiple visits to a website. They called this latter algorithm life-time value (LTV) optimization. Both algorithms faced challenging problems because the reward signal in this domain is very sparse because users usually do not click on ads, and user clicking is very random so that Data sets from the banking industry were used for training and testing these algorithms. The data sets consisted of many complete trajectories of customer interaction with a bank\u2019s website that showed each customer one out of a collection of possible o\u21b5ers. If a customer clicked, the reward was 1, and otherwise it was 0. One data set contained approximately 200,000 interactions from a month of a bank\u2019s campaign that randomly o\u21b5ered one of 7 o\u21b5ers", "08a5941f-7473-4a6c-a778-32c55038297b": "functional dependence of the Gaussian on x is through the quadratic form which appears in the exponent. The quantity \u2206 is called the Mahalanobis distance from \u00b5 to x and reduces to the Euclidean distance when \u03a3 is the identity matrix. The Gaussian distribution will be constant on surfaces in x-space for which this quadratic form is constant.\n\nFirst of all, we note that the matrix \u03a3 can be taken to be symmetric, without loss of generality, because any antisymmetric component would disappear from the exponent. Now consider the eigenvector equation for the covariance matrix Exercise 2.17 where i = 1, . , D", "be5f02aa-4275-4023-a14e-b9bf3efd9f0a": "If we consider only those instances for which X = xi, then the fraction of such instances for which Y = yj is written p(Y = yj|X = xi) and is called the conditional probability of Y = yj given X = xi. It is obtained by \ufb01nding the fraction of those points in column i that fall in cell i,j and hence is given by From (1.5), (1.6), and (1.8), we can then derive the following relationship which is the product rule of probability. So far we have been quite careful to make a distinction between a random variable, such as the box B in the fruit example, and the values that the random variable can take, for example r if the box were the red one. Thus the probability that B takes the value r is denoted p(B = r).\n\nAlthough this helps to avoid ambiguity, it leads to a rather cumbersome notation, and in many cases there will be no need for such pedantry. Instead, we may simply write p(B) to denote a distribution over the random variable B, or p(r) to denote the distribution evaluated for the particular value r, provided that the interpretation is clear from the context", "59465d29-1908-4727-88fd-1631c165a095": "For comparison with the support vector machine, we \ufb01rst reformulate maximum likelihood logistic regression using the target variable t \u2208 {\u22121, 1}. To do this, we note that p(t = 1|y) = \u03c3(y) where y(x) is given by (7.1), and \u03c3(y) is the logistic sigmoid function de\ufb01ned by (4.59).\n\nIt follows that p(t = \u22121|y) = 1 \u2212 \u03c3(y) = \u03c3(\u2212y), where we have used the properties of the logistic sigmoid function, and so we can write From this we can construct an error function by taking the negative logarithm of the likelihood function that, with a quadratic regularizer, takes the form Exercise 7.6 For comparison with other error functions, we can divide by ln(2) so that the error function passes through the point (0, 1). This rescaled error function is also plotted in Figure 7.5 and we see that it has a similar form to the support vector error function. The key difference is that the \ufb02at region in ESV(yt) leads to sparse solutions. Both the logistic error and the hinge loss can be viewed as continuous approximations to the misclassi\ufb01cation error", "0104ff55-2a3a-4bdf-b473-1b5d950b1da6": "The log function in the negative log-likelihood cost function undoes the exp of some output units. We will discuss the interaction between the cost function and the choice of output unit in section 6.2.2. One unusual property of the cross-entropy cost used to perform maximum likelihood estimation is that it usually does not have a minimum value when applied to the models commonly used in practice. For discrete output variables, most models are parametrized in such a way that they cannot represent a probability of zero or one, but can come arbitrarily close to doing so. Logistic regression is an example of such a model.\n\nFor real-valued output variables, if the model can control the density of the output distribution (for example, by learning the variance parameter of a Gaussian output distribution) then it becomes possible to assign extremely high density to the correct training set outputs, resulting in cross-entropy approaching negative infinity. Regularization techniques described in chapter 7 provide several different ways of modifying the learning problem so that the model cannot reap unlimited reward in this way. 6.2.1.2. Learning Conditional Statistics  Instead of learning a full probability distribution p(y | x; 6), we often want to learn just one conditional statistic of y given x", "f3cca97e-1bd4-4b34-98bf-4cd624c5c1e6": "Arora, S., Hazan, E., & Kale, S. The multiplicative weights update method: A metaalgorithm and applications. Theory of Computing, 8(1), 121\u2013164. Bach, S. H., Broecheler, M., Huang, B., & Getoor, L. Hinge-loss Markov random \ufb01elds and probabilistic soft logic. The Journal of Machine Learning Research, 18(1), 3846\u20133912. Bach, S. H., Rodriguez, D., Liu, Y., Luo, C., Shao, H., Xia, C., Sen, S., Ratner, A., Hancock, B., Alborzi, H., et al. Snorkel drybell: A case study in deploying weak supervision at industrial scale. Proceedings of the 2019 International Conference on Management of Data, 362\u2013375. Bauschke, H. H., & Borwein, J. M. On projection algorithms for solving convex feasibility problems. SIAM review, 38(3), 367\u2013426", "f81d69fc-efd5-4099-873f-6e7b4fefd682": "( Gre) ( LC)  https://lilianweng.github.io/posts/2021-05-31-contrastive/  Contrastive Representation Learning | Lil'Log   What are Diffusion Models? Reducing Toxicity in Language Models  https://lilianweng.github.io/posts/2021-05-31-contrastive/", "865ad095-41b9-43df-8178-d4ff7daa6059": "This effect can be offset, at the expense of some additional one-off computation, by constructing tree-based search structures to allow (approximate) near neighbours to be found ef\ufb01ciently without doing an exhaustive search of the data set. Nevertheless, these nonparametric methods are still severely limited. On the other hand, we have seen that simple parametric models are very restricted in terms of the forms of distribution that they can represent. We therefore need to \ufb01nd density models that are very \ufb02exible and yet for which the complexity of the models can be controlled independently of the size of the training set, and we shall see in subsequent chapters how to achieve this. 2.2 (\u22c6 \u22c6) The form of the Bernoulli distribution given by (2.2) is not symmetric between the two values of x.\n\nIn some situations, it will be more convenient to use an equivalent formulation for which x \u2208 {\u22121, 1}, in which case the distribution can be written where \u00b5 \u2208 . Show that the distribution (2.261) is normalized, and evaluate its mean, variance, and entropy. Use this result to prove by induction the following result which is known as the binomial theorem, and which is valid for all real values of x", "97d3edee-6da7-4216-9b67-652a516f2532": "Neurons, the main components of nervous systems, are cells specialized for processing and transmitting information using electrical and chemical signals. They come in many forms, but a neuron typically has a cell body, dendrites, and a single axon. Dendrites are structures that branch from the cell body to receive input from other neurons (or to also receive external signals in the case of sensory neurons). A neuron\u2019s axon is a \ufb01ber that carries the neuron\u2019s output to other neurons (or to muscles or glands). A neuron\u2019s output consists of sequences of electrical pulses called action potentials that travel along the axon. Action potentials are also called spikes, and a neuron is said to \ufb01re when it generates a spike. In models of neural networks it is common to use real numbers to represent a neuron\u2019s \ufb01ring rate, the average number of spikes per some unit of time. A neuron\u2019s axon can branch widely so that the neuron\u2019s action potentials reach many targets", "0710638b-4129-4741-90b5-6871bc1f7586": "We begin our discussion of support vector machines by returning to the two-class classi\ufb01cation problem using linear models of the form where \u03c6(x) denotes a \ufb01xed feature-space transformation, and we have made the bias parameter b explicit. Note that we shall shortly introduce a dual representation expressed in terms of kernel functions, which avoids having to work explicitly in feature space. The training data set comprises N input vectors x1, . , xN, with corresponding target values t1, . , tN where tn \u2208 {\u22121, 1}, and new data points x are classi\ufb01ed according to the sign of y(x). We shall assume for the moment that the training data set is linearly separable in feature space, so that by de\ufb01nition there exists at least one choice of the parameters w and b such that a function of the form (7.1) satis\ufb01es y(xn) > 0 for points having tn = +1 and y(xn) < 0 for points having tn = \u22121, so that tny(xn) > 0 for all training data points. There may of course exist many such solutions that separate the classes exactly", "28d405bf-fcd2-42b7-a78e-179a0ca00fa2": "i=1 Because the arg max does not change when we rescale the cost function, we can divide by m to obtain a version of the dxiterion that is expressed as an expectation with respect to the empirical distribution pgata defined by the training data: Oui =argmaxE  ,,,, log pmodel(@; 9). (5.59)  a x~p  https://www.deeplearningbook.org/contents/ml.html   oO  One way to interpret maximum likelihood estimation is to view it as minimizing the dissimilarity between the empirical distribution pfdata, defined by the training set and the model distribution, with the degree of dissimilarity between the two measured by the KL divergence. The KL divergence is given by  Dx (Baata || Pmodel) =i X~Paata  .\n\n(5.60) The term on the left is a function only of the data-generating process, not the  model. This means when we train the model to minimize the KL divergence, we need only minimize  \u2014 Ex Peata LOE Pmodel(\u00ae)] , (5.61) which is of course the same as the maximization in equation 5.59", "c487a5a0-a588-4a87-b3c9-2d580389c5dc": "Pseudorandom number generators can also use nonlinear transformations of simple distributions. For example, inverse transform sampling  draws a scalar z from U(0,1) and applies a nonlinear transformation to a scalar x. In this case g(z) is given by the inverse of the cumulative distribution function F(z) = [\u201d,.p(v)dv. If we are able to specify p(x ), integrate over x, and invert the resulting function, we can sample from p(z) without using machine learning.\n\nTo generate samples from more complicated distributions that are difficult to specify directly, difficult to integrate over, or whose resulting integrals are difficult to invert, we use a feedforward network to represent a parametric family of nonlinear functions g, and use training data to infer the parameters selecting the desired function. We can think of g as providing a nonlinear change of variables that transforms the distribution over z into the desired distribution over x. Recall from equation 3.47 that, for invertible, differentiable, continuous g,  ) pe(2) = palate) faet( 39)", "0947b940-2d48-40e8-8516-b3f21ce72c8f": "Because the vector k is a function of the test point input value xN+1, we see that the predictive distribution is a Gaussian whose mean and variance both depend on xN+1. An example of Gaussian process regression is shown in Figure 6.8. The only restriction on the kernel function is that the covariance matrix given by (6.62) must be positive de\ufb01nite. If \u03bbi is an eigenvalue of K, then the corresponding eigenvalue of C will be \u03bbi + \u03b2\u22121. It is therefore suf\ufb01cient that the kernel matrix k(xn, xm) be positive semide\ufb01nite for any pair of points xn and xm, so that \u03bbi \u2a7e 0, because any eigenvalue \u03bbi that is zero will still give rise to a positive eigenvalue for C because \u03b2 > 0.\n\nThis is the same restriction on the kernel function discussed earlier, and so we can again exploit all of the techniques in Section 6.2 to construct Note that the mean (6.66) of the predictive distribution can be written, as a funcN t", "7802a7e0-8d53-45a0-a31d-0b6944b4d281": "By de\ufb01nition, the maximum likelihood solution \u03b8ML is a stationary point of the log likelihood function and hence satis\ufb01es \u2202 \u2202\u03b8 Exchanging the derivative and the summation, and taking the limit N \u2192 \u221e we have and so we see that \ufb01nding the maximum likelihood solution corresponds to \ufb01nding the root of a regression function. We can therefore apply the Robbins-Monro procedure, which now takes the form case, the random variable z corresponds to the derivative of the log likelihood function and is given by (x \u2212 \u00b5ML)/\u03c32, and its expectation that de\ufb01nes the regression function is a straight line given by (\u00b5 \u2212 \u00b5ML)/\u03c32. The root of the regression function corresponds to the maximum likelihood estimator \u00b5ML", "c354ebe8-c722-4371-8cd6-7068743e7d1f": "With the sampling procedure thus defined and the importance weights given in equation 18.52, the estimate of the ratio of partition functions is given by:  To verify that this procedure defines a valid importance sampling scheme, we can show  that the AIS procedure corresponds to simple importance sampling on an extended state space, with points sampled over the product space  . To do this, we define the distribution over the extended space as P(r y+ ++, Lp, 21) (18.54) = pi (21) Ty, 1 (rn | \u00a91) Ty, 2(Ln\u20142 | Ln, 1) Tn (tn | Ln); (18.55) 625  CHAPTER 18", "6448c180-56b4-419a-a082-ec518b2f14b8": "The most important results in statistical learning theory show that the discrepancy between training error and generalization error is bounded from above by a quantity that grows as the model capacity grows but shrinks as the number of training examples increases . These bounds provide intellectual justification that machine learning algorithms can work, but they are  https://www.deeplearningbook.org/contents/ml.html    rarely used in practice when working with deep learning algorithms. This is in part because the bounds are often quite loose and in part because it can be quite difficult to determine the capacity of deep learning algorithms. The problem of determining the capacity of a deep learning model is especially difficult because che effective capacity is limited by the capabilities of the optimization algorithm, and we have little theoretical understanding of the general nonconvex optimization problems involved in deep learning.\n\nWe must remember that while simpler functions are more likely to generalize to have a small gap between training and test error), we must still choose a sufficiently complex hypothesis to achieve low training error. Typically, training error decreases until it asymptotes to the minimum possible error value as model capacity increases (assuming the error measure has a minimum value). Typically,  112  CHAPTER 5", "2b35e30a-fcff-4edf-9afd-4e0b26caf905": "Robot shaping: Developing autonomous agents through Doya, K., Sejnowski, T. J. A novel reinforcement model of birdsong vocalization learning. In Advances in Neural Information Processing Systems 7 , pp. 101\u2013108. MIT Press, Cambridge, MA. Doya, K., Sejnowski, T. J. A computational model of birdsong learning by auditory Auditory Processing and Neural Modeling, pp. 77\u201388. Springer, Boston, MA.\n\nDoyle, P. G., Snell, J. L. Random Walks and Electric Networks. The Mathematical Association of America. Carus Mathematical Monograph 22. Dreyfus, S. E., Law, A. M. The Art and Theory of Dynamic Programming. Academic Du, S. S., Chen, J., Li, L., Xiao, L., Zhou, D. Stochastic variance reduction methods for policy evaluation", "e4f13856-431b-4fb4-9ec0-0cbd43cc1ce6": "As soon as we have fit this function f(a), we can generate a training set containing infinitely many examples, simply by applying f to randomly sampled points a. We then train the new, smaller model to match f(a) on these points. To most efficiently use the capacity of the new, small model, it is best to sample the new 2 points from a distribution resembling the actual test inputs that will be supplied to the model later.\n\nThis can be done  https://www.deeplearningbook.org/contents/applications.html    by corrupting training examples or by drawing points from a generative model trained on the original training set. Alternatively, one can train the smaller model only on the original training points, but train it to copy other features of the model, such as its posterior distribution over the incorrect classes . 12.1.5 Dynamic Structure  One strategy for accelerating data-processing systems in general is to build systems that have dynamic structure in the graph describing the computation needed to process an input. Data-processing systems can dynamically determine which subset of many neural networks should be run on a given input. Individual neural networks can also exhibit dynamic structure internally by determining which subset of features (hidden units) to compute given information from the input", "d3c2510c-2d2f-4ece-8b3c-f88dca129a81": "If the e\u21b5ect is greater than some small threshold, then the pair is inserted in the queue with the new priority (if there is a previous entry of the pair in the queue, then insertion results in only the higher priority entry remaining in the queue). In this way the e\u21b5ects of changes are e\ufb03ciently propagated backward until quiescence. The full algorithm for the case of deterministic environments is given in the box on the next page. Extensions of prioritized sweeping to stochastic environments are straightforward. The model is maintained by keeping counts of the number of times each state\u2013action pair has been experienced and of what the next states were. It is natural then to update each pair not with a sample update, as we have been using so far, but with an expected update, taking into account all possible next states and their probabilities of occurring. Prioritized sweeping is just one way of distributing computations to improve planning e\ufb03ciency, and probably not the best way", "12a7da7b-f259-4a55-b300-16a39224d3ae": "Next we seek an ef\ufb01cient procedure for evaluating the quantities \u03b3(znk) and \u03be(zn\u22121,j, znk), corresponding to the E step of the EM algorithm. The graph for the hidden Markov model, shown in Figure 13.5, is a tree, and so we know that the posterior distribution of the latent variables can be obtained ef\ufb01ciently using a twostage message passing algorithm. In the particular context of the hidden Markov Section 8.4 model, this is known as the forward-backward algorithm , or the Baum-Welch algorithm . There are in fact several variants of the basic algorithm, all of which lead to the exact marginals, according to the precise form of the messages that are propagated along the chain .\n\nWe shall focus on the most widely used of these, known as the alpha-beta algorithm. As well as being of great practical importance in its own right, the forwardbackward algorithm provides us with a nice illustration of many of the concepts introduced in earlier chapters", "4c4162e2-bfe8-4293-b170-e88b74cf7bc9": "We now develop the mean field approach for the example | with two hidden  layers.\n\nLet Q(AY , bh) | v) be the approximation of P(AO A 2) | v). The mean field assumption implies that Q(h, R\u00ae | v) \u201cHa Q(al? | v) Tews (20.29)  The mean field approximation attempts to find a member of this family of distributions that best fits the true posterior P(h\u201c, h@) | v). Importantly, the inference process must be run again to find a different distribution Q every time we use a new value of v.  One can conceive of many ways of measuring how well Q(h | v) fits P(h | v). The mean field approach is to minimize  aC) Qh, h\u00ae) | v) L(Q||P) = Xa Q(R 2) | v) log (eee ro) (20.30)  In general, we do not have to provide a parametric form of the approximating distribution beyond enforcing the independence assumptions. The variational approximation procedure is generally able to recover a functional form of the approximate distribution", "5012a9c5-65b6-4ab8-91c1-53233a008006": "We will not go through the details, but in fact all the ideas of this section extend easily to stochastic policies. In particular, the policy improvement theorem carries through as stated for the stochastic case. In addition, if there are ties in policy improvement steps such as (4.9)\u2014that is, if there are several actions at which the maximum is achieved\u2014then in the stochastic case we need not select a single action from among them. Instead, each maximizing action can be given a portion of the probability of being selected in the new greedy policy. Any apportioning scheme is allowed as long as all submaximal actions are given zero probability. The last row of Figure 4.1 shows an example of policy improvement for stochastic policies. Here the original policy, \u21e1, is the equiprobable random policy, and the new policy, \u21e10, is greedy with respect to v\u21e1. The value function v\u21e1 is shown in the bottom-left diagram and the set of possible \u21e10 is shown in the bottom-right diagram. The states with multiple arrows in the \u21e10 diagram are those in which several actions achieve the maximum in (4.9); any apportionment of probability among these actions is permitted", "b3d4aa55-98ff-4316-8d85-cdd1bd331a2c": "An important class of kernels is the family of local kernels, where k(u,v) is large when u = v and decreases as u and v grow further apart from each other.\n\nA local kernel can be thought of as a similarity function that performs template matching, by measuring how closely a test example x  https://www.deeplearningbook.org/contents/ml.html    resembles each training example Much of the modern motivation for dee earning is derived from studying the limitations of local template matching an how deep models are able to succeed in cases where local template matching fails Bengio et al., 2006b). Decision trees also suffer from the limitations of exclusively smoothness-based learning, because they break the input space into as many regions as there are eaves and use a separate parameter (or sometimes many parameters for extensions of decision trees) in each region. If the target function requires a tree with at  east n leaves to be represented accurately, then at least n training examples are required to fit the tree. A multiple of n is needed to achieve some level of statistical confidence in the predicted output. In general, to distinguish O(k) regions in input space, all these methods require O(k) examples", "e9c4c12f-49b6-4656-971c-ea209777e83f": "If the processes must share computational resources, then the division can be handled almost arbitrarily\u2014by whatever organization is most convenient and e\ufb03cient for the task at hand. In this chapter we have touched upon a number of dimensions of variation among state-space planning methods. One dimension is the variation in the size of updates.\n\nThe smaller the updates, the more incremental the planning methods can be. Among the smallest updates are one-step sample updates, as in Dyna. Another important dimension is the distribution of updates, that is, of the focus of search. Prioritized sweeping focuses backward on the predecessors of states whose values have recently changed. On-policy trajectory sampling focuses on states or state\u2013action pairs that the agent is likely to encounter when controlling its environment. This can allow computation to skip over parts of the state space that are irrelevant to the prediction or control problem. Realtime dynamic programming, an on-policy trajectory sampling version of value iteration, illustrates some of the advantages this strategy has over conventional sweep-based policy iteration. Planning can also focus forward from pertinent states, such as states actually encountered during an agent-environment interaction", "9e3425eb-49db-46a4-a59c-7bd8d149f248": "Mnih et al. compared the scores of DQN with the scores of the best performing learning system in the literature at the time, the scores of a professional human games tester, and the scores of an agent that selected actions at random. The best system from the literature used linear function approximation with features designed using some knowledge about Atari 2600 games . DQN learned on each game by interacting with the game emulator for 50 million frames, which corresponds to about 38 days of experience with the game. At the start of learning on each game, the weights of DQN\u2019s network were reset to random values. To evaluate DQN\u2019s skill level after learning, its score was averaged over 30 sessions on each game, each lasting up to 5 minutes and beginning with a random initial game state. The professional human tester played using the same emulator (with the sound turned o\u21b5 to remove any possible advantage over DQN which did not process audio). After 2 hours of practice, the human played about 20 episodes of each game for up to 5 minutes each and was not allowed to take any break during this time.\n\nDQN learned to play better than the best previous reinforcement learning systems on all but 6 of the games, and played better than the human player on 22 of the games", "16dd4da3-e96d-4f43-beaa-ba3a2af01d89": "399  CHAPTER 10.\n\nSEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  The important question is therefore: how do we set the input and recurrent weights so that a rich set of histories can be represented in the recurrent neural network state? The answer proposed in the reservoir computing literature is to view the recurrent net as a dynamical system, and set the input and recurrent weights such that the dynamical system is near the edge of stability. The original idea was to make the eigenvalues of the Jacobian of the state-to- state transition function be close to 1. As explained in section 8.2.5, an important characteristic of a recurrent network is the eigenvalue spectrum of the Jacobians  J\u00ae = py. Of particular importance is the spectral radius of J, defined to  be the maximum of the absolute values of its eigenvalues", "3dd3bcab-1a3b-4fd1-91de-ab8a38c9b5a8": "See Section 4.8.) No special attention was paid to the ordering of the updates; other orderings could have produced faster convergence. Initial values were all zero for each run of both methods. DP was judged to have converged when the maximum change in a state value over a sweep was less than 10\u22124, and RTDP was judged to have converged when the average time to cross the \ufb01nish line over 20 episodes appeared to stabilize at an asymptotic number of steps. This version of RTDP updated only the value of the current state on each step. Both methods produced policies averaging between 14 and 15 steps to cross the \ufb01nish line, but RTDP required only roughly half of the updates that DP did. This is the result of RTDP\u2019s on-policy trajectory sampling. Whereas the value of every state was updated in each sweep of DP, RTDP focused updates on fewer states.\n\nIn an average run, RTDP updated the values of 98.45% of the states no more than 100 times and 80.51% of the states no more than 10 times; the values of about 290 states were not updated at all in an average run", "d47fd888-6343-4f63-a999-a6f13cdce7c9": "Much more so than earlier research, this research considers details of synaptic plasticity and other constraints from neuroscience. Publications include the following (chronologically and alphabetically): Bartlett and Baxter , Xie and Seung , Baras and Meir , Farries and Fairhall , Florian , Izhikevich , Pecevski, Maass, and Legenstein , Legenstein, Pecevski, and Maass , Kolodziejski, Porr, and W\u00a8org\u00a8otter , Urbanczik and Senn , and Vasilaki, Fr\u00b4emaux, Urbanczik, Senn, and Gerstner", "3f8ff02b-7025-43ac-9f67-4e0f6217e42f": "Neuron, 88(3):528\u2013538. He, K., Zhang, X., Ren, S., Sun, J. Deep residual learning for image recognition. In Hebb, D. O. The Organization of Behavior: A Neuropsychological Theory. John Wiley and Sons Inc., New York. Reissued by Lawrence Erlbaum Associates Inc., Mahwah NJ, 2002. Hengst, B. Hierarchical approaches. In M. Wiering and M. van Otterlo (Eds. ), Reinforcement Learning: State-of-the-Art, pp. 293\u2013323. Springer-Verlag Berlin Heidelberg. Herrnstein, R. J. On the Law of E\u21b5ect. Journal of the Experimental Analysis of Behavior, Hersh, R., Griego, R. J. Brownian motion and potential theory. Scienti\ufb01c American, Hester, T., Stone, P. Learning and using models. In M", "bc6f55b1-a851-44c7-89cc-1ee11c6d8f1b": "This test is intended to detect the case where the model overfits the training set and just  https://www.deeplearningbook.org/contents/generative_models.html    reproduces training instances. It is even possible to simultaneously underfit and 714  CHAPTER 20. DEEP GENERATIVE MODELS  overfit yet still produce samples that individually look good. Imagine a generative model trained on images of dogs and cats that simply learns to reproduce the training images of dogs. Such a model has clearly overfit, because it does not produces images that were not in the training set, but it has also underfit, because it assigns no probability to the training images of cats. Yet a human observer would judge each individual image of a dog to be high quality.\n\nIn this simple example, it would be easy for a human observer who can inspect many samples to determine that the cats are absent. In more realistic settings, a generative model trained on data with tens of thousands of modes may ignore a small number of modes, and a human observer would not easily be able to inspect or remember enough images to detect the missing variation", "fe89f04b-c488-4d1e-942e-47d318389f6b": "In other words, whereas \u03b4 is reduced to the degree that a normal reward is predicted by antecedent events (Section 15.6), the contribution to \u03b4 due to an addictive stimulus does not decrease as the reward signal becomes predicted: drug rewards cannot be \u201cpredicted away.\u201d The model does this by preventing \u03b4 from ever becoming negative when the reward signal is due to an addictive drug, thus eliminating the error-correcting feature of TD learning for states associated with administration of the drug. The result is that the values of these states increase without bound, making actions leading to these states preferred above all others. Addictive behavior is much more complicated than this result from Redish\u2019s model, but the model\u2019s main idea may be a piece of the puzzle. Or the model might be misleading. Dopamine appears not to play a critical role in all forms of addiction, and not everyone is equally susceptible to developing addictive behavior. Moreover, the model does not include the changes in many circuits and brain regions that accompany chronic drug taking, for example, changes that lead to a drug\u2019s diminishing e\u21b5ect with repeated use.\n\nIt is also likely that addiction involves model-based processes", "5809884b-5487-4ebd-be0e-93dae71cd7f9": "For example, x could be a photo of a car, and y a photo of the same car that was taken from a slightly different location at a different time of day, so that the car in y is shifted, rotated, larger, smaller, and displaying slightly different colors and shadows than the car in x. Joint embedding, Siamese networks  A particular well-suited deep learning architecture to do so is the so-called Siamese networks or joint embedding architecture.\n\nThe idea goes back to papers from Geoff Hinton\u2019s lab and Yann LeCun\u2019s group in the early 1990s ( here and here) and mid-2000s (here, here, and here). It was relatively ignored for along time but has enjoyed a revival since late 2019. A joint embedding architecture is composed of two identical (or almost identical) copies of the same network. One network is fed with x and the other with y. The networks produce output vectors called embeddings, which represent x and y. A third module, joining the networks at the head, computes the energy as the distance between the two embedding vectors", "1a219190-2fd8-4336-bf8b-ac8a6833eaf2": "Another disadvantage is that it is not straightforward to back-propagate through the nonparametric encoder, which makes it difficult to pretrain a sparse coding model with an unsupervised criterion and then fine-tune it using a supervised criterion. Modified versions of sparse coding that permit approximate derivatives do exist but are not widely used . Sparse coding, like other linear factor models, often produces poor samples, as shown in figure 13.2. This happens even when the model is able to reconstruct the data well and provide useful features for a classifier. The reason is that each individual feature may be learned well, but the factorial prior on the hidden code results in the model including random subsets of all the features in each generated sample.\n\nThis motivates the development of deeper models that can impose a nonfactorial distribution on the deepest code layer, as well as the development of more sophisticated shallow models. 494  CHAPTER 13", "ccbd7ed6-510f-434a-9390-5dc5a1a33b53": "9.18 (\u22c6 \u22c6) Consider a Bernoulli mixture model as discussed in Section 9.3.3, together with a prior distribution p(\u00b5k|ak, bk) over each of the parameter vectors \u00b5k given by the beta distribution (2.13), and a Dirichlet prior p(\u03c0|\u03b1) given by (2.38). Derive the EM algorithm for maximizing the posterior probability p(\u00b5, \u03c0|X). 9.19 (\u22c6 \u22c6) Consider a D-dimensional variable x each of whose components i is itself a multinomial variable of degree M so that x is a binary vector with components xij where i = 1, . , D and j = 1, . , M, subject to the constraint that \ufffd j xij = 1 for all i.\n\nSuppose that the distribution of these variables is described by a mixture of the discrete multinomial distributions considered in Section 2.2 so that The parameters \u00b5kij represent the probabilities p(xij = 1|\u00b5k) and must satisfy 0 \u2a7d \u00b5kij \u2a7d 1 together with the constraint \ufffd j \u00b5kij = 1 for all values of k and i", "47a13ce2-6779-4a04-bb58-d0165754fccd": "It con- trols the effective capacity of the model in a more complicated way than other hyperparameters\u2014the effective capacity of the model is highest when the learning rate is correct for the optimization problem, not when the learning rate is especially large or especially small. The learning rate has a U-shaped curve for training error, illustrated in figure 11.1. When the learning rate is too large, gradient descent can inadvertently increase rather than decrease the training error. In the idealized quadratic case, this occurs if the learning rate is at least twice as large as its optimal value . When the learning rate is too small, training is not only slower but may become permanently stuck with a high training error. This effect is poorly understood (it would not happen for a convex loss function). Tuning the parameters other than the learning rate requires monitoring both training and test error to diagnose whether your model is overfitting or underfitting, then adjusting its capacity appropriately. If your error on the training set is higher than your target error rate, you have no choice but to increase capacity.\n\nIf you are not using regularization and you are confident that your optimization algorithm is performing correctly, then you must add more layers to your network or add more hidden units", "167927b4-648a-4ba8-84fa-6b3e58931fad": "Gibbs, Cool, Land, Kehoe, and Gormezano  describe second-order conditioning of the rabbit\u2019s nictitating membrane response and its relationship to conditioning with serial-compound stimuli.\n\nFinch and Culler  reported obtaining \ufb01fth-order conditioning of a dog\u2019s foreleg withdrawal \u201cwhen the motivation of the animal is maintained through the various orders.\u201d 14.2.2 The idea built into the Rescorla\u2013Wagner model that learning occurs when animals are surprised is derived from Kamin . Models of classical conditioning other than Rescorla and Wagner\u2019s include the models of Klopf , Grossberg , Mackintosh , Moore and Stickney , Pearce and Hall , and Courville, Daw, and Touretzky . Schmajuk  reviews models of classical conditioning. Wagner  provides a modern psychological perspective on the Rescorla-Wagner model and similar elemental theories of learning. 14.2.3 An early version of the TD model of classical conditioning appeared in Sutton and Barto , which also included the early model\u2019s prediction that temporal primacy overrides blocking, later shown by Kehoe, Schreurs, and Graham  to occur in the rabbit nictitating membrane preparation", "1ac2e552-e4bc-4b1b-bd1e-06376fe60b84": "Section 10.2.4 describes several ways of constructing an RNN that represents a conditional distribution over a sequence given some input, and section 10.4 describes how to accomplish this conditioning when the input is a sequence. In all cases, one model first reads the input sequence and emits a data structure that summarizes the input sequence. We call this summary the \u201ccontext\u201d C. The context C' may be a list of vectors, or it may be a vector or tensor. The model that reads the input to produce C may be an RNN  or a convolutiona network . A second model, usually an RNN, then reads the context C and generates a sentence in the target language. This general idea of an encoder-decoder framework for machine translation is illustrated in figure 12.5. To generate an entire sentence conditioned on the source sentence, the mode must have a way to represent the entire source sentence. Earlier models were able to represent only individual words or phrases", "3a05c4e3-26f7-4841-8274-79607bb940d6": "Now suppose we extend the model to an M th order Markov chain, so that the joint distribution is built up from conditionals p(xn|xn\u2212M, . , xn\u22121).\n\nIf the variables are discrete, and if the conditional distributions are represented by general conditional probability tables, then the number of parameters in such a model will have KM\u22121(K \u2212 1) parameters. Because this grows exponentially with M, it will often render this approach impractical for larger values of M. For continuous variables, we can use linear-Gaussian conditional distributions in which each node has a Gaussian distribution whose mean is a linear function of its parents. This is known as an autoregressive or AR model . An alternative approach is to use a parametric model for p(xn|xn\u2212M, . , xn\u22121) such as a neural network. This technique is sometimes called a tapped delay line because it corresponds to storing (delaying) the previous M values of the observed variable in order to predict the next value", "ec073658-2493-42ce-9ad5-1eb5208df3c5": "Whereas PCA works with a single random variable, CCA considers two (or more) variables and tries to find a corresponding pair of linear subspaces that have high cross-correlation, so that each component within one of the subspaces is correlated with a single component from the other subspace.\n\nIts solution can be expressed in terms of a generalized eigenvector problem. We can illustrate the use of PCA for data compression by considering the offline digits data set. Because each eigenvector of the covariance matrix is a vector ;n the OIigi\",,1 D-<limensional space. we can represent tho eigenw:cto<s as imago< of tho same silO as ,1>0 data poi\",,_ 11,. first Ih'e .ig.n,'occOfS. along wich tl>o corresponding .igen,'slue,. are <IIo\"'n in Figure 12,3, A plO! ofll>o complete spect\"'m uf oigo\",\u00b7alue,", "fc1daa59-79b8-4496-a3b6-627051abe22c": "REGULARIZATION FOR DEEP LEARNING  7.11 Bagging and Other Ensemble Methods  Bagging (short for bootstrap aggregating) is a technique for reducing general- ization error by combining several models .\n\nThe idea is to train several different models separately, then have all the models vote on the output for test examples. This is an example of a general strategy in machine learning called model averaging. Techniques employing this strategy are known as ensemble methods. The reason that model averaging works is that different models will usually not make all the same errors on the test set. Consider for example a set of k regression models. Suppose that each model makes an error \u20ac; on each example, with the errors drawn from a zero-mean  multivariate normal distribution with variances Ele?] = v and covariances Ele;e;] = c. Then the error made by the average prediction of all the ensemble models is 4 e;. The expected squared error of the ensemble predictor is  2 1 1 E (3X5) = pF Sot e+ doce; ]]", "f755bd57-2547-44af-a1ad-85b9b5246df2": "Unfortunately, even this does not resolve the entire problem. The no free lunch theorem for machine learning  states that, averaged over all possible data-generating distributions, every classification algorithm has the  114  CHAPTER 5.\n\nMACHINE LEARNING BASICS  Bayes error  Train (quadratic)  Test (quadratic)  Test (optimal capacity)  Error (MSE)  Train (optimal capacity  Number of training examples  ty (polynomial degree)  https://www.deeplearningbook.org/contents/ml.html    b  on  Optimal capaci  Number of training examples  Figure 5.4: The effect of the training dataset size on the train and test error, as well as on the optimal model capacity. We constructed a synthetic regression problem based on adding a moderate amount of noise to a degree-5 polynomial, generated a single test set, and then generated several different sizes of training set. For each size, we generated 40 different training sets in order to plot error bars showing 95 percent confidence intervals. (Top)The MSE on the training and test set for two different models: a quadratic model, and a model with degree chosen to minimize the test error. Both are fit in closed form", "8bc97504-19f4-4a20-96a9-b25a314cd5e0": "Speci\ufb01cally, we tune learning rate in {0.01, 0.1, 0.3, 0.5, 1.0} for both loss functions. We further tune the margin in {0, 0.4, 0.8, 1.6} for margin loss, the temperature in {0.1, 0.2, 0.5, 1.0} for logistic loss. For simplicity, we only consider the negatives from one augmentation view (instead of both sides), which slightly impairs performance but ensures fair comparison. C. Further Comparison to Related Methods As we have noted in the main text, most individual components of SimCLR have appeared in previous work, and the improved performance is a result of a combination of these design choices. Table C.1 provides a high-level comparison of the design choices of our method with those of previous methods. Compared with previous work, our design choices are generally simpler.\n\nIn below, we provide an in-depth comparison of our method to the recently proposed contrastive representation learning methods: \u2022 DIM/AMDIM  achieve global-to-local/local-to-neighbor prediction by predicting the middle layer of ConvNet. The ConvNet is a ResNet that has bewen modi\ufb01ed to place signi\ufb01cant constraints on the receptive \ufb01elds of the network (e.g", "069682e7-2c60-4718-8bc4-81da992f744b": "An even simpler  N(e; p87!) = exp ( Me 1)\" ate 1). (3.24)  version is the isotropic Gaussian distribution, whose covariance matrix is a scalar times the identity matrix. 3.9.4 Exponential and Laplace Distributions  In the context of deep learning, we often want to have a probability distribution with a sharp point at 2 = 0. To accomplish this, we can use the exponential distribution: p(@; A) = A150 exp (\u2014Az) . (3.25)  The exponential distribution uses the indicator function 1,>9 to assign probability zero to all negative values of x. A closely related probability distribution that allows us to place a sharp peak of probability mass at an arbitrary point py is the Laplace distribution  1 _ Laplace(z; 1, y) = a exp ( Je 5 ) .\n\n(3.26)  3.9.5 The Dirac Distribution and Empirical Distribution  In some cases, we wish to specify that all the mass in a probability distribution clusters around a single point. This can be accomplished by defining a PDF using the Dirac delta function, 6(2):  p(x) = 6(@ \u2014 p)", "3eeb2207-f922-4101-abc3-33d39e9ba578": "In practice, the error backpropagation method is considerably faster, but the reinforcement learning team method is more plausible as a neural mechanism, especially in light of what is being learned about reward-modulated STDP as discussed in Section 15.8.\n\nExploration through independent exploration by team members is only the simplest way for a team to explore; more sophisticated methods are possible if the team members coordinate their actions to focus on particular parts of the collective action space, either by communicating with one another or by responding to common inputs. There are also mechanisms more sophisticated than contingent eligibility traces for addressing structural credit assignment, which is easier in a team problem when the set of possible collective actions is restricted in some way. An extreme case is a winner-take-all arrangement (for example, the result of lateral inhibition in the brain) that restricts collective actions to those to which only one, or a few, team members contribute. In this case the winners get the credit or blame for resulting reward or punishment. Details of learning in cooperative games (or team problems) and non-cooperative game problems are beyond the scope of this book. The Bibliographical and Historical Remarks section at the end of this chapter cites a selection of the relevant publications, including extensive references to research on implications for neuroscience of collective reinforcement learning", "0ea6bd1f-0f3b-4d6d-94ec-1789799cefa9": "Image and object-aware Random Erasing Fig.\n\n12 Example of random erasing on object detection tasks   Table 2 Results of Cutout Regularization , plus denotes using traditional augmentation methods, horizontal flipping and cropping  Method c10 c10+ 100 \u00a2c100+ SVHN ResNetI8  10.6340.26 4.724021 36684057 22464031 - ResNet18 + cutout 9.3140.18 3.994013 349840.29 21964024 - WideResNet  6.97 \u00a30.22 3.87+0.08  26.0640.22 188+0.08 1.60+0.05 WideResNet + cutout 5.5440.08 3.08+0.16 23.9440.15 184140.27 1.3040,03 Shake-shake regularization  = 2.86 = 15.85 = Shake-shake regularization + cutout = \u2014 2564007 9 - 15204021 -  A 2.56% error rate is obtained on CIFAR-10 using cutout and traditional augmentation methods The italic value denote high performance according to the comparative metrics  filling in a missing piece of an image. Using a diverse collection of GAN inpainters, the random erasing augmentation could seed very interesting extrapolations", "93e9546f-9017-4a6d-a3da-af72f987a722": "It may be critical for the reward part of an animal\u2019s environment model. Another structure involved in model-based behavior is the hippocampus, a structure critical for memory and spatial navigation. A rat\u2019s hippocampus plays a critical role in the rat\u2019s ability to navigate a maze in the goal-directed manner that led Tolman to the idea that animals use models, or cognitive maps, in selecting actions (Section 14.5). The hippocampus may also be a critical component of our human ability to imagine new experiences .\n\nThe \ufb01ndings that most directly implicate the hippocampus in planning\u2014the process needed to enlist an environment model in making decisions\u2014come from experiments that decode the activity of neurons in the hippocampus to determine what part of space hippocampal activity is representing on a moment-to-moment basis. When a rat pauses at a choice point in a maze, the representation of space in the hippocampus sweeps forward (and not backwards) along the possible paths the animal can take from that point . Furthermore, the spatial trajectories represented by these sweeps closely correspond to the rat\u2019s subsequent navigational behavior", "5e4b00cf-a525-4119-8e45-1b46eb8c1618": "Consistency regularization-based SSL (or \u201cconsistency training\u201d for short) regularizes a model by enforcing that its output doesn\u2019t change signi\ufb01cantly when the input is perturbed. In practice, the input is perturbed by applying data augmentation, and consistency is enforced through a loss term that measures the difference between the model\u2019s predictions on a clean input and a corresponding perturbed version of the same input. Formally, let f\u03b8 be a model with parameters \u03b8, f\u02c6\u03b8 be a \ufb01xed copy of the model where no gradients are allowed to \ufb02ow, xl be a labeled datapoint with label y, xu be an unlabeled datapoint, and \u03b1(x) be a data augmentation method. Then, a typical loss function for consistency training is CE(f\u03b8(xl), y) + \u03bbuCE(f\u02c6\u03b8(xu), f\u03b8(\u03b1(xu))) where CE is the cross entropy loss and \u03bbu is a tunable hyperparameter that determines the weight of the consistency regularization term", "d1b75898-af34-4016-b703-a5988b78b387": "One disadvantage of Monte Carlo methods is that they learn nothing from an episode until it is over. For example, if a Monte Carlo control method takes an action that produces a very poor reward but does not end the episode, then the agent\u2019s tendency to repeat the action will be undiminished during the episode. Online TD(1), on the other hand, learns in an n-step TD way from the incomplete ongoing episode, where the n steps are all the way up to the current step. If something unusually good or bad happens during an episode, control methods based on TD(1) can learn immediately and alter their behavior on that same episode. It is revealing to revisit the 19-state random walk example (Example 7.1) to see how well TD(\u03bb) does in approximating the o\u270fine \u03bb-return algorithm. The results for both algorithms are shown in Figure 12.6. For each \u03bb value, if \u21b5 is selected optimally for it (or smaller), then the two algorithms perform virtually identically", "493014c1-4711-4c99-ba16-02b6560ec154": "Later he moved to Paris, narrowly escaping with his life during the French revolution thanks to the personal intervention of Lavoisier (the French chemist who discovered oxygen) who himself was later executed at the guillotine. Lagrange made key contributions to the calculus of variations and the foundations of dynamics.\n\nIn Appendix E, we show that a constrained optimization of this form satis\ufb01es the Karush-Kuhn-Tucker (KKT) conditions, which in this case require that the following three properties hold Thus for every data point, either an = 0 or tny(xn) = 1. Any data point for which an = 0 will not appear in the sum in (7.13) and hence plays no role in making predictions for new data points. The remaining data points are called support vectors, and because they satisfy tny(xn) = 1, they correspond to points that lie on the maximum margin hyperplanes in feature space, as illustrated in Figure 7.1. This property is central to the practical applicability of support vector machines. Once the model is trained, a signi\ufb01cant proportion of the data points can be discarded and only the support vectors retained", "c6916f9c-519b-47ca-91f1-83627250100d": "Testing their test-time augmentation scheme on medical image segmentation, they found that it outperformed the single-prediction baseline and drop- out-based multiple predictions. They also found better uncertainty estimation when using test-time augmentation, reducing highly confident but incorrect predictions. Their test-time augmentation method uses a Monte Carlo simulation in order to obtain parameters for different augmentations such as flipping, scaling, rotation, and transla- tions, as well as noise injections.\n\nTest-time augmentation can be found in the Alexnet paper , which applies CNNs to the ImageNet dataset. In their experiments, they average the predictions on ten ran- domly cropped patches. These patches consist of one extracted from the center, four corner croppings, and the equivalent regions on the horizontally flipped images. These predictions are averaged to form the final output. He et al. use the same 10-crop test- ing procedure to evaluate their ResNet CNN architecture (Fig. 32). Shorten and Khoshgoftaar J Big Data   6:60 0.90 . = . 0.85 . eo s . * . a \" 0.80", "a3a5c23c-8782-4871-b5df-a18d2d2df3e0": "There is no special term for a matrix whose rows or columns are orthogonal but not orthonormal.\n\n2.7 Eigendecomposition  Many mathematical objects can be understood better by breaking them into constituent parts, or finding some properties of them that are universal, not caused by the way we choose to represent them. For example, integers can be decomposed into prime factors. The way we represent the number 12 will change depending on whether we write it in base ten or in binary, but it will always be true that 12 = 2 x 2x 3. From this representation we can conclude useful properties, for example, that 12 is not divisible by 5, and that any integer multiple of 12 will be divisible by 3. Much as we can discover something about the true nature of an integer by decomposing it into prime factors, we can also decompose matrices in ways that show us information about their functional properties that is not obvious from the representation of the matrix as an array of elements. One of the most widely used kinds of matrix decomposition is called eigen- decomposition, in which we decompose a matrix into a set of eigenvectors and eigenvalues", "636f95eb-e806-4fb6-9ff0-33ce5cf7a51c": "Each state is associated with a value function  https://lilianweng.github.io/posts/2018-02-19-rl-overview/  A (Long) Peek into Reinforcement Learning | Lil'Log   V(s) predicting the expected amount of future rewards we are able to receive in this state by acting the corresponding policy. In other words, the value function quantifies how good a state is. Both policy and value functions are what we try to learn in reinforcement learning. value/policy acting planning direct RL model eae N_ learning  The interaction between the agent and the environment involves a sequence of actions and observed rewards in time, t = 1, 2,..., 7\u2019. During the process, the agent accumulates the knowledge about the environment, learns the optimal policy, and makes decisions on which action to take next so as to efficiently learn the best policy", "471405d5-88a8-4a23-bae9-acea8cdc0caf": "Inspired by the mechanisms of dropout regularization, random erasing can be seen as analogous to dropout except in the input data space rather than embed- ded into the network architecture. This technique was specifically designed to combat image recognition challenges due to occlusion. Occlusion refers to when some parts of the object are unclear. Random erasing will stop this by forcing the model to learn more descriptive features about an image, preventing it from overfitting to a certain visual fea- ture in the image. Aside from the visual challenge of occlusion, in particular, random erasing is a promising technique to guarantee a network pays attention to the entire image, rather than just a subset of it. Random erasing works by randomly selecting an n x m patch of an image and masking it with either 0 s, 255 s, mean pixel values, or random values. On the CIFAR-10 dataset this resulted in an error rate reduction from 5.17 to 4.31%. The best patch fill method was found to be random values. The fill method and size of the masks are the only parameters that need to be hand-designed during implementation (Figs", "de79edb9-6c5f-4fe2-bfaf-d0e67028568f": "This is achieved using the conditional expectation, explaining the choice of terminology. In the M-step, the likelihood function is maximized under the assumption that the missing data are known. The estimate of the missing data from the E-step are used in lieu of the actual missing data. Convergence is assured since the algorithm is guaranteed to increase the likelihood at each iteration. Let X be random vector which results from a parameterized family. We wish to \ufb01nd \u03b8 such that P(X|\u03b8) is a maximum. This is known as the Maximum Likelihood (ML) estimate for \u03b8.\n\nIn order to estimate \u03b8, it is typical to introduce the log likelihood function de\ufb01ned as, The likelihood function is considered to be a function of the parameter \u03b8 given the data X. Since ln(x) is a strictly increasing function, the value of \u03b8 which maximizes P(X|\u03b8) also maximizes L(\u03b8). The EM algorithm is an iterative procedure for maximizing L(\u03b8). Assume that after the nth iteration the current estimate for \u03b8 is given by \u03b8n", "04510c81-592b-4b66-9213-42c5950cb3c4": "Jia and Liang  Belinkov and Bisk , Zhao et al. Ribeiro et al. , McCoy et al. Min et al. , Tan et al. Hsu et al. , Hsu et al. Wu et al. , Chen et al. Malandrakis et al. , Shen et al. Miao et al. , Chen et al. Cheng et al. , Chen et al. Guo et al. kinds of operations can be further combined , where each example is randomly augmented with one of insertion, deletion, and swapping. These noise-injection methods can ef\ufb01ciently be applied to training, and show improvements when they augment simple models trained on small training sets. However, the improvements might be unstable due to the possibility that random perturbations change the meanings of sentences", "ce7352fb-46b1-4e7f-b360-4d51baaf0067": "given the latent variable z. In essence. the factor analysis model is explaining the observed covariance structure of the data by representing the independent variance associated with each coordinate in the matrix 1J.' and capturing the covariance between variables in the matrix W. In the factor analysis literature. the columns of W. which capture the correlations between observed variables. are calledfaclOr loadings. and the diagonal elements of 1J.'. which represent the independent noise variances for each of the variables, are called llniqllenesses. The origins of factor analysis are as old as those of PCA. and discussions of factor analysis can be found in the books by Everitt . Bartholomew , and Basilevsky . Links between factor analysis and PCA were investigated by Lilwley  and Anderson  who showed that at stationary points of the likelihood function. for a faclOr analysis model with 1J.'\n\n=  showed that the maximum of the log likelihood function occurs when the eigenvectors comprising Ware chosen to be the principal eigenvectors. Making use of (2.115)", "7ec3bfec-14fc-42cd-8cf6-7f7d7ae1a92b": "We can now write the ratio a as  https://www.deeplearningbook.org/contents/partition.html    1 I Zt Zyt Z0 = Zo Zn\" Zr (18.47) Zn, Zn2 coe Zim-1 21 (18.48) Zo Zn, Zin \u20142 Zin \u20141 n-1 Zn, =|[\u2014. (18.49) : Zn; j=0 J Provided the distributions p,, and py, 41, for all0 < j < n \u20141, are sufficiently close, we can reliably estimate each of the factors 7+ using simple importance ny  sampling and then use these to obtain an estimate of 4B", "cf222515-d828-4656-98e2-d19108779d72": "The hyperparameters \u03b1, \u03b2 \u2265 0 enable trade-o\ufb00s between these components. Experience function. Perhaps the most powerful in terms of impacting the learning outcome and utility is the experience functions f (\u03b8) k . An experience function f (\u03b8)(t) \u2208 R measures the goodness of a con\ufb01guration t in light of any given experience. The superscript (\u03b8) highlights that the experience in some settings (e.g., reward experience as in Section 4.3) could depend on or be coupled with the target model parameters \u03b8. In the following, we omit the superscript when there is no ambiguity. As discussed in Section 4, all diverse forms of experience that can be utilized for model training, such as data examples, constraints, logical rules, rewards, and adversarial discriminators, can be encoded as an experience function.\n\nThe experience function hence provides a uni\ufb01ed language to express all exogenous information about the target model, which we consider as an essential ingredient for panoramic learning to \ufb02exibly incorporate diverse experience in learning. Based on the uniform treatment of experience, a standardized optimization program as above can be formulated to identify the desired model", "0a34e6d2-d609-4844-82c2-4d2c7cc2e85e": "We can think of this model as taking the parameters 0 of the nonconditional model and turning them into w, where the bias parameters within w are now a function of the input. Rather than receiving only a single vector a as input, the RNN may receive a sequence of vectors 2) as input. The RNN described in equation 10.8 corre-  385  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  oaN \u2018 \u2018\\ byodd / dL Le  https://www.deeplearningbook.org/contents/rnn.html    Figure 10.9: An RNN that maps a fixed-length vector x into a distribution over sequences Y. This RNN is appropriate for tasks such as image captioning, where a single image is used as input to a model that then produces a sequence of words describing the image.\n\nEach element yO of the observed output sequence serves both as input (for the current time step) and, during training, as target (for the previous time step). 386  CHAPTER 10", "3b3cefcd-a853-48cc-bc0a-eb9304f1c177": "Figures 5, 6, and 7 show samples generated for these three architectures using both the WGAN and GAN algorithms. We refer the reader to Appendix F for full sheets of generated samples. Samples were not cherry-picked. In no experiment did we see evidence of mode collapse for the WGAN algorithm. There\u2019s been a number of works on the so called Integral Probability Metrics (IPMs) . Given F a set of functions from X to R, we can de\ufb01ne as an integral probability metric associated with the function class F. It is easily veri\ufb01ed that if for every f \u2208 F we have \u2212f \u2208 F (such as all examples we\u2019ll consider), then dF is nonnegative, satis\ufb01es the triangular inequality, and is symmetric. Thus, dF is a pseudometric over Prob(X). While IPMs might seem to share a similar formula, as we will see di\ufb00erent classes of functions can yeald to radically di\ufb00erent metrics", "627fa750-0c91-4826-ae8e-d84005ebbc5e": "During back-propagation, we will receive a tensor G - P \u2014 _0 such that G; 54 = Zak J(V,K). To train the network, we need to compute the derivatives with respect to the weights in the kernel. To do so, we can use a function  0 OK; i,t. (GV, 8)ijkl J(V,K) = 32 Gann Vi(m\u20141)xsth(n\u2014i)xstt- (9-11)  mn  If this layer is not the bottom layer of the network, we will need to compute the gradient with respect to V to back-propagate the error further down. To do so, we can use a function  A(K,G, $) 55,6 \u201cWa K) (9.12) = 2 YoY KiimpGain- (9.13) lym n,p q  s.t. 8.t. (l-1)xs+m=j (n\u20141)xs+p=k  Autoencoder networks, described in chapter 14, are feedforward networks trained to copy their input to their output", "8507bff0-f9b1-427f-ae11-0e9731f4b2a1": "The Rescorla\u2013Wagner model showed how traditional contiguity theories of conditioning\u2014that temporal contiguity of stimuli was a necessary and su\ufb03cient condition for learning\u2014could be adjusted in a simple way to account for blocking . The Rescorla\u2013Wagner model provides a simple account of blocking and some other features of classical conditioning, but it is not a complete or perfect model of classical 3The only di\u21b5erences between the LMS rule and the Rescorla\u2013Wagner model are that for LMS the input vectors xt can have any real numbers as components, and\u2014at least in the simplest version of the LMS rule\u2014the step-size parameter \u21b5 does not depend on the input vector or the identity of the stimulus setting the prediction target. conditioning. Di\u21b5erent ideas account for a variety of other observed e\u21b5ects, and progress is still being made toward understanding the many subtleties of classical conditioning.\n\nThe TD model, which we describe next, though also not a complete or perfect model model of classical conditioning, extends the Rescorla\u2013Wagner model to address how within-trial and between-trial timing relationships among stimuli can in\ufb02uence learning and how higher-order conditioning might arise", "daed687e-b30f-4510-9eef-740dbbb7276e": "This is intrinsically a dif\ufb01cult problem as we have to generalize from a \ufb01nite data set.\n\nFurthermore the observed data are corrupted with noise, and so for a given \ufffdx there is uncertainty as to the appropriate value for \ufffdt. Probability theory, discussed in Section 1.2, provides a framework for expressing such uncertainty in a precise and quantitative manner, and decision theory, discussed in Section 1.5, allows us to exploit this probabilistic representation in order to make predictions that are optimal according to appropriate criteria. For the moment, however, we shall proceed rather informally and consider a simple approach based on curve \ufb01tting. In particular, we shall \ufb01t the data using a polynomial function of the form y(x, w) = w0 + w1x + w2x2 + . + wMxM = where M is the order of the polynomial, and xj denotes x raised to the power of j. The polynomial coef\ufb01cients w0, . , wM are collectively denoted by the vector w", "8fd46ffd-8d3b-44ef-a2cd-cc6a6a006ae4": "More informally, we may also discuss prior beliefs as directly influencing the function itself and influencing the parameters only indirectly, as a result of the relationship between the parameters and the function. Additionally, we informally  https://www.deeplearningbook.org/contents/ml.html    discuss prior beliefs as being expressed implicitly by choosing algorithms that are biased toward choosing some class of functions over another, even though these biases may not be expressed (or even be possible to express) in terms of a probability distribution representing our degree of belief in various functions. Among the most widely used of these implicit \u201cpriors\u201d is the smoothness prior, or local constancy prior. This prior states that the function we learn should not change very much within a small region. Many simpler algorithms rely exclusively on this prior to generalize well, and as a result, they fail to scale to the statistical challenges involved in solving AI- level tasks. Throughout this book, we describe how deep learning introduces additional (explicit and implicit) priors in order to reduce the generalization error on sophisticated tasks. Here, we explain why the smoothness prior alone is insufficient for these tasks.\n\nThere are many different ways to implicitly or explicitly express a prior belief that the learned function should be smooth or locally constant", "8220573f-4987-4a26-94a7-52dd3e8a3aa9": "This cost function is given by  J(0) = \u2014Ex,y~aata log Pmodel(y | 2x). (6.12)  The specific form of the cost function changes from model to model, depending on the specific form of log pmode!- The expansion of the above equation typically yields some terms that do not depend on the model parameters and may be dis- carded. For example, as we saw in section 5.5.1, if pmoaea(y | \u00a9) =Ny; f(x; 0), D), then we recover the mean squared error cost,  J(0) = SExy~paasally \u2014 f(a; 9)||? + const, (6.13)  up to a scaling factor of 4 and a term that does not depend on @. The discarded constant is based on the variance of the Gaussian distribution, which in this case we chose not to parametrize. Previously, we saw that the equivalence between maximum likelihood estimation with an output distribution and minimization of  174  CHAPTER 6", "c632800d-534d-40dd-9cef-ab6e271209df": "When the  https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/  Self-supervised learning: The dark matter of intelligence   model is shown distorted versions of the same image, the parameters of the networks can easily be adjusted so that their outputs move closer together. This will ensure that the network will produce nearly identical representations (or embedding) of an object, regardless of the particular view of that object. Joint embedding architecture. The function C at the top produces a scalar energy that measures the distance between the representation vectors (embeddings) produced by two identical twin networks sharing the same parameters (w). When x and y are slightly different versions of the same image, the system is trained to produce a low energy, which forces the model to produce similar embedding vectors for the two images.\n\nThe difficult part is to train the model so that it produces high energy (i.e., different embeddings) for images that are different. The difficulty is to make sure that the networks produce high energy, i.e. different embedding vectors, when x and y are different images", "343ffbef-a720-431c-8d92-dc97e38051f1": "9.1 The Convolution Operation  In its most general form, convolution is an operation on two functions of a real- valued argument. To motivate the definition of convolution, we start with examples of two functions we might use. Suppose we are tracking the location of a spaceship with a laser sensor.\n\nOur laser sensor provides a single output x(t), the position of the spaceship at time t. Both x and \u00a2 are real valued, that is, we can get a different reading from the laser sensor at any instant in time. Now suppose that our laser sensor is somewhat noisy. To obtain a less noisy estimate of the spaceship\u2019s position, we would like to average several measurements. Of course, more recent measurements are more relevant, so we will want this to be a weighted average that gives more weight to recent measurements. We can do this with a weighting function w(a), where a is the age of a measurement. If we apply such a weighted average operation at every moment, we obtain a new function s providing a smoothed estimate of the position of the spaceship:  s(t) = / x(a)w(t \u2014 a)da", "e089ce03-44f1-4852-a0d7-e79a64e3e98c": "Proceedings of the 24th International Conference on Machine Learning (ICML 2007 ), pp. 273\u2013280. Gelperin, A., Hop\ufb01eld, J. J., Tank, D. W. The logic of limax learning. In A. Selverston (Ed. ), Model Neural Networks and Behavior, pp. 247\u2013261. Plenum Press, New York. Genesereth, M., Thielscher, M. General game playing. Synthesis Lectures on Arti\ufb01cial Gershman, S. J., Moustafa, A. A., Ludvig, E. A. Time representation in reinforcement learning models of the basal ganglia. Frontiers in Computational Neuroscience, 7:194. Gershman, S. J., Pesaran, B., Daw, N. D. Human reinforcement learning subdivides structured action spaces by learning e\u21b5ector-speci\ufb01c values", "56b95347-f636-4124-8bd2-42afe909214c": "The advantage of semi-supervised learning via unsupervised pretraining with many unlabeled examples and few labeled examples was made particularly clear in 2011 with unsupervised pretraining winning two international transfer learning competitions , in settings where the number of labeled examples in the target task was small (from a handful to dozens of examples per class). These effects were also documented in carefully controlled experiments by Paine ei al. Other factors are likely to be involved. For example, unsupervised pretraining is likely to be most useful when the function to be learned is extremely complicated. Unsupervised learning differs from regularizers like weight decay because it does not bias the learner toward discovering a simple function but rather leads the learner toward discovering feature functions that are useful for the unsupervised learning task.\n\nIf the true underlying functions are complicated and shaped by regularities of the input distribution, unsupervised learning can be a more appropriate regularizer. These caveats aside, we now analyze some success cases where unsupervised pretraining is known to cause an improvement and explain what is known about why this improvement occurs. Unsupervised pretraining has usually been used to improve classifiers and is usually most interesting from the point of view of reducing  530  CHAPTER 15. REPRESENTATION LEARNING  test set error", "4b0d9ab6-6b7b-4b59-9ff5-93a5162143c9": "Let Z be a random variable (e.g Gaussian) over another space Z. Let g : Z \u00d7 Rd \u2192 X be a function, that will be denoted g\u03b8(z) with z the \ufb01rst coordinate and \u03b8 the second. Let P\u03b8 denote the distribution of g\u03b8(Z). Then, 2. If g is locally Lipschitz and satis\ufb01es regularity assumption 1, then W(Pr, P\u03b8) is continuous everywhere, and di\ufb00erentiable almost everywhere. The following corollary tells us that learning by minimizing the EM distance makes sense (at least in theory) with neural networks. Corollary 1. Let g\u03b8 be any feedforward neural network4 parameterized by \u03b8, and p(z) a prior over z such that Ez\u223cp(z) < \u221e (e.g. Gaussian, uniform, etc.). 3 The argument for why this happens, and indeed how we arrived to the idea that Wasserstein is what we should really be optimizing is displayed in Appendix A", "c0527600-e119-46ad-9a1b-97c890c64ba2": "The SVD and the bilinear prediction of equation 12.20 both performed very well in the competition for the Netflix prize , aiming at predicting ratings for films, based only on previous ratings by a large set of anonymous users. Many machine learning experts participated in this competition, which took place between 2006 and 2009. It raised the level of research in recommender systems using advanced machine learning and yielded improvements in recommender systems. Even though the simple bilinear prediction, or SVD, did not win by itself, it was a component of the ensemble models presented by most of the competitors, including the winners . Beyond these bilinear models with distributed representations, one of the first uses of neural networks for collaborative filtering is based on the RBM undirected probabilistic model .\n\nRBMs were an important element of the ensemble of methods that won the Netflix competition . More advanced variants on the idea of factorizing the ratings matrix have also been explored in the neural networks community . Collaborative filtering systems have a basic limitation, however: when a new item or a new user is introduced, its lack of rating history means that there is no way to evaluate its similarity with other items or users, or the degree of association between, say, that new user and existing items", "4d43ef57-5006-4fb2-8c0e-f442fb47f1da": "end while  305  CHAPTER 8.\n\nOPTIMIZATION FOR TRAINING DEEP MODELS  (uncentered) second-order moment; however, it lacks the correction factor. Thus, unlike in Adam, the RMSProp second-order moment estimate may have high bias early in training. Adam is generally regarded as being fairly robust to the choice of hyperparameters, though the learning rate sometimes needs to be changed from  https://www.deeplearningbook.org/contents/optimization.html   the suggested default. 8.5.4 Choosing the Right Optimization Algorithm  We have discussed a series of related algorithms that each seek to address the challenge of optimizing deep models by adapting the learning rate for each model parameter. At this point, a natural question is: which algorithm should one choose? Unfortunately, there is currently no consensus on this point. Schaul ef al. presented a valuable comparison of a large number of optimization algorithms across a wide range of learning tasks", "6246f7e6-a3dd-4abd-9ceb-128b218c75f9": "Satopaa, V., Albrecht, J., Irwin, D., Raghavan, B.: Finding a \u201ckneedle\u201d in a haystack: Detecting knee points in system behavior. In: International Conference on Distributed Computing Systems Workshops  52. Scudder, H.J. : Probability of error of some adaptive patternrecognition machines. IEEE Trans. Infom. Theory 11, 363\u2013371  53. Settles, B.: Active learning literature survey. Technical report, University of Wisconsin-Madison Department of Computer Sciences  54. Settles, B.: Active Learning. Synthesis Lectures on Arti\ufb01cial Intelligence and Machine Learning. Morgan and Claypool Publishers  55. Stewart, R., Ermon, S.: Label-free supervision of neural networks with physics and other domain knowledge", "d781b498-e196-4e2f-80d0-e293932666d7": "It is a perfectly flat line, and its value can be predicted using only the gradient. If the gradient is 1, then we can make a step of size \u00a2\u20ac along the negative gradient, and the cost function will decrease by e\u00a2. If the second derivative is negative, the function curves downward, so the cost function will actually decrease by more than \u00a2. Finally, if the second derivative is positive, the function curves upward, so the cost function can decrease by less than e. See  84  CHAPTER 4. NUMERICAL COMPUTATION  Negative curvature No curvature Positive curvature & S & S a S x x x  Figure 4.4: The second derivative determines the curvature of a function. Here we show quadratic functions with various curvature", "02480aba-c17a-4666-8856-047879d525c7": "Without loss of generality, we can take xa to form the \ufb01rst M components of x, with xb comprising the remaining D \u2212 M components, so that We also de\ufb01ne corresponding partitions of the mean vector \u00b5 given by and of the covariance matrix \u03a3 given by which is known as the precision matrix. In fact, we shall see that some properties of Gaussian distributions are most naturally expressed in terms of the covariance, whereas others take a simpler form when viewed in terms of the precision. We therefore also introduce the partitioned form of the precision matrix corresponding to the partitioning (2.65) of the vector x. Because the inverse of a symmetric matrix is also symmetric, we see that \u039baa and \u039bbb are symmetric, while Exercise 2.22 \u039bT ab = \u039bba. It should be stressed at this point that, for instance, \u039baa is not simply given by the inverse of \u03a3aa. In fact, we shall shortly examine the relation between the inverse of a partitioned matrix and the inverses of its partitions", "43ed0e86-e129-4934-87e4-1d3661382ea3": "We can describe a curve in a D-dimensional data space using a vector-valued function f (). ), which is a vector each of whose elements is a function of the scalar ).. There are many possible ways to parameterize the curve, of which a natural choice is the arc length along the curve. For any given point xin data space, we can find the point on the curve that is closest in Euclidean distance. We denote this point by >.. = gf(X) because it depends on the particular curve f(>\"). For a continuous data density p(x), a principal curve is defined as one for which every point on the curve is the mean of all those points in data space that project to it, so that For a given continuous density, there can be many principal curves. In practice, we are interested in finite data sets, and we also wish to restrict attention to smooth curves. Hastie and Stuetzle  propose a two-stage iterative procedure for finding such principal curves, somewhat reminiscent of the EM algorithm for PCA", "6805c351-2a4e-4085-8627-0413d5653b01": "By control we mean that an agent in\ufb02uences its environment to bring about states or events that the agent prefers: the agent exerts control over its environment. This is the sense of control used by control engineers. In psychology, on the other hand, control typically means that an animal\u2019s behavior is in\ufb02uenced by\u2014is controlled by\u2014the stimuli the animal receives (stimulus control) or the reinforcement schedule it experiences. Here the environment is controlling the agent. Control in this sense is the basis of behavior modi\ufb01cation therapy. Of course, both of these directions of control are at play when an agent interacts with its environment, but our focus is on the agent as controller; not the environment as controller. A view equivalent to ours, and perhaps more illuminating, is that the agent is actually controlling the input it receives from its environment . This is not what psychologists mean by stimulus control.\n\nSometimes reinforcement learning is understood to refer solely to learning policies directly from rewards (and penalties) without the involvement of value functions or environment models. This is what psychologists call stimulus-response, or S-R, learning", "9965af60-6764-4de4-904b-b97a77f87d0a": "It helps to think of the network as if it were 18 separate networks, one for estimating the optimal action value of each possible action. In reality, these networks shared their initial layers, but the output units learned to use the features extracted by these layers in di\u21b5erent ways. DQN\u2019s reward signal indicated how a games\u2019s score changed from one time step to the next: +1 whenever it increased, \u22121 whenever it decreased, and 0 otherwise.\n\nThis standardized the reward signal across the games and made a single step-size parameter work well for all the games despite their varying ranges of scores. DQN used an \"-greedy policy, with \" decreasing linearly over the \ufb01rst million frames and remaining at a low value for the rest of the learning session. The values of the various other parameters, such as the learning step size, discount rate, and others speci\ufb01c to the implementation, were selected by performing informal searches to see which values worked best for a small selection of the games. These values were then held \ufb01xed for all of the games. After DQN selected an action, the action was executed by the game emulator, which returned a reward and the next video frame", "561aced7-6e7c-4e62-95f6-3ad1394b5e65": "We defined back-propagation only for computing a scalar output gradient, but back-propagation can be extended to compute a Jacobian (either of k different scalar nodes in the graph, or of a tensor-valued node containing k values). A naive implementation may then need k times more computation: for each scalar internal node in the original forward graph, the naive implementation computes k gradients instead of a single gradient.\n\nWhen the number of outputs of the graph is larger than the number of inputs, it is sometimes preferable to use another form of automatic differentiation called forward mode accumulation. Forward mode  218  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  accumulation has been proposed for obtaining real-time computation of gradients in recurrent networks, for example . This approach also avoids the need to store the values and gradients for the whole graph, trading off computational efficiency for memory", "e5671cf5-2ad8-452f-8700-4488310d8fad": "As before, we see that this is a quadratic function of the components of z, and hence p(z) is Gaussian distribution.\n\nTo \ufb01nd the precision of this Gaussian, we consider the second order terms in (2.102), which can be written as and so the Gaussian distribution over z has precision (inverse covariance) matrix given by The covariance matrix is found by taking the inverse of the precision, which can be done using the matrix inversion formula (2.76) to give Exercise 2.29 Similarly, we can \ufb01nd the mean of the Gaussian distribution over z by identifying the linear terms in (2.102), which are given by Using our earlier result (2.71) obtained by completing the square over the quadratic form of a multivariate Gaussian, we \ufb01nd that the mean of z is given by Next we \ufb01nd an expression for the marginal distribution p(y) in which we have marginalized over x. Recall that the marginal distribution over a subset of the components of a Gaussian random vector takes a particularly simple form when expressed in terms of the partitioned covariance matrix. Speci\ufb01cally, its mean and Section 2.3 covariance are given by (2.92) and (2.93), respectively", "9eb019df-722c-4aba-bfff-3061dc959b28": "It can also update the values of an arbitrary collection of other states at each step; for example, it can update the values of states visited in a limited-horizon look-ahead search from the current state.\n\nFor these problems, with each episode beginning in a state randomly chosen from the set of start states and ending at a goal state, RTDP converges with probability one to a policy that is optimal for all the relevant states provided: 1) the initial value of every goal state is zero, 2) there exists at least one policy that guarantees that a goal state will be reached with probability one from any start state, 3) all rewards for transitions from non-goal states are strictly negative, and 4) all the initial values are equal to, or greater than, their optimal values (which can be satis\ufb01ed by simply setting the initial values of all states to zero). This result was proved by Barto, Bradtke, and Singh  by combining results for asynchronous DP with results about a heuristic search algorithm known as learning real-time A* due to Korf . Tasks having these properties are examples of stochastic optimal path problems, which are usually stated in terms of cost minimization instead of as reward maximization as we do here", "ddd7bee8-5f90-4e1c-974d-e6cfb69ce4ee": "We can gain some insight into the behaviour of the hybrid Monte Carlo algorithm by considering its application to a multivariate Gaussian. For convenience, consider a Gaussian distribution p(z) with independent components, for which the Hamiltonian is given by Our conclusions will be equally valid for a Gaussian distribution having correlated components because the hybrid Monte Carlo algorithm exhibits rotational isotropy. During the leapfrog integration, each pair of phase-space variables zi, ri evolves independently. However, the acceptance or rejection of the candidate point is based on the value of H, which depends on the values of all of the variables. Thus, a signi\ufb01cant integration error in any one of the variables could lead to a high probability of rejection.\n\nIn order that the discrete leapfrog integration be a reasonably good approximation to the true continuous-time dynamics, it is necessary for the leapfrog integration scale \u03f5 to be smaller than the shortest length-scale over which the potential is varying signi\ufb01cantly. This is governed by the smallest value of \u03c3i, which we denote by \u03c3min", "1dd531bc-da88-4115-926f-d23403abc505": "This is a very interesting story, and I think it has been a very nice surprise. This is a very nice and well thought out piece that is a must for the science: In summary we use this approach to evaluate if the number of data points (in the dataset) that are relevant for each data set is the same (in this case, the data are not in one data set). In this approach we can test the data points in a different way.\n\nmilitary: In summary we have some important news from the moment of the year and some important information from the moment of the year.\\n\\n\\n\\n\\n We\u2019ve also added an additional update for our new feature, which includes:\\n \u2022 Improved access and access in all of the main legal: This essay discusses how you can build a community of dedicated people. If you\u2019re a member of a community of people who want to contribute to the environment, you\u2019ll also be helping them build communities in order to support the local economy, and the future of the city. The latest report politics: This essay discusses how we can build on previous research \ufb01ndings about the role religion plays in human development in human development. This is a very interesting and highly entertaining story", "58c7fc12-a8b6-4431-a419-5aa6054c1176": "The simplest possible is marginal independence, P(h) = ; P(h;), but linear dependencies or those captured by a shallow autoencoder are also reasonable assumptions. This can be seen in many laws of physics and is assumed when plugging a linear predictor or a factorized prior on top of a learned representation. The concept of representation learning ties together all the many forms of  deep learning. Feedforward and recurrent networks, autoencoders and deep proba- bilistic models all learn and exploit representations. Learning the best possible representation remains an exciting avenue of research. 554  https://www.deeplearningbook.org/contents/representation.html", "68143111-3b7c-4396-8e6f-9301b389fe0d": "5.11.1 The Curse of Dimensionality  Many machine learning problems become exceedingly difficult when the number of dimensions in the data is high. This phenomenon is known as the curse of dimensionality. Of particular concern is that the number of possible distinct configurations of a set of variables increases exponentially as the number of variables increases. The curse of dimensionality arises in many places in computer science, especially in machine learning. 152  CHAPTER 5.\n\nMACHINE LEARNING BASICS  Figure 5.9: As the number of relevant dimensions of the data increases (from left to right), the number of configurations of interest may grow exponentially. (Left)In this one-dimensional example, we have one variable for which we only care to distinguish 10 regions of interest. With enough examples falling within each of these regions (each region corresponds to a cell in the illustration), learning algorithms can easily generalize correctly. A straightforward way to generalize is to estimate the value of the target function within each region (and possibly interpolate between neighboring regions). (Center)With two  https://www.deeplearningbook.org/contents/ml.html    dimensions, 1t 1s more dithcult to distinguish 1U ditterent values ot each variable", "64fdceba-3e16-49de-9edd-22b27da5e497": "However, there may be inequivalent solutions as well, and these will generally yield different values for the optimized hyperparameters.\n\nIn order to compare different models, for example neural networks having different numbers of hidden units, we need to evaluate the model evidence p(D). This can be approximated by taking (5.175) and substituting the values of \u03b1 and \u03b2 obtained from the iterative optimization of these hyperparameters. A more careful evaluation is obtained by marginalizing over \u03b1 and \u03b2, again by making a Gaussian approximation . In either case, it is necessary to evaluate the determinant |A| of the Hessian matrix. This can be problematic in practice because the determinant, unlike the trace, is sensitive to the small eigenvalues that are often dif\ufb01cult to determine accurately. The Laplace approximation is based on a local quadratic expansion around a mode of the posterior distribution over weights. We have seen in Section 5.1.1 that any given mode in a two-layer network is a member of a set of M!2M equivalent modes that differ by interchange and sign-change symmetries, where M is the number of hidden units", "95dba04b-d902-4025-8d78-9f2bb0f7fe46": "CONVOLUTIONAL NETWORKS  https://www.deeplearningbook.org/contents/convnets.html    the same underlying features, then the max-pooled units become invariant to the learned transformation (see figure 9.9). Convolutional layers are hard coded to be  invariant specifically to translation. Other operations besides convolution are usually necessary to implement a convolutional network.\n\nTo perform learning, one must be able to compute the gradient with respect to the kernel, given the gradient with respect to the outputs. In some simple cases, this operation can be performed using the convolution operation, but many cases of interest, including the case of stride greater than 1, do not have this property. Recall that convolution is a linear operation and can thus be described as a matrix multiplication (if we first reshape the input tensor into a flat vector). The matrix involved is a function of the convolution kernel. The matrix is sparse, and each element of the kernel is copied to several elements of the matrix. This view helps us to derive some of the other operations needed to implement a convolutional network. Multiplication by the transpose of the matrix defined by convolution is one such operation", "530cffce-ee17-4a83-8d70-83dee24899e4": "Many linear parametric models can be re-cast into an equivalent \u2018dual representation\u2019 in which the predictions are also based on linear combinations of a kernel function evaluated at the training data points. As we shall see, for models which are based on a \ufb01xed nonlinear feature space mapping \u03c6(x), the kernel function is given by the relation k(x, x\u2032) = \u03c6(x)T\u03c6(x\u2032). (6.1) From this de\ufb01nition, we see that the kernel is a symmetric function of its arguments so that k(x, x\u2032) = k(x\u2032, x). The kernel concept was introduced into the \ufb01eld of pattern recognition by Aizerman et al. in the context of the method of potential functions, so-called because of an analogy with electrostatics. Although neglected for many years, it was re-introduced into machine learning in the context of largemargin classi\ufb01ers by Boser et al. giving rise to the technique of support vector machines", "047bb90d-d1b7-4d49-8614-1439e2c50341": "Curiosity-driven exploration by selfsupervised prediction. Proceedings of the 34th International Conference on Machine LearningVolume 70, 2778\u20132787. Pathak, D., Gandhi, D., & Gupta, A. Self-supervised exploration via disagreement. International Conference on Machine Learning, 5062\u20135071. Peyr\u00e9, G., Cuturi, M. et al. Computational optimal transport: With applications to data science. Foundations and Trends\u00ae in Machine Learning, 11(5-6), 355\u2013607. Ranganath, R., Gerrish, S., & Blei, D. Black box variational inference. Arti\ufb01cial Intelligence and Statistics, 814\u2013822. Ratner, A., Bach, S. H., Ehrenberg, H., Fries, J., Wu, S., & R\u00e9, C. Snorkel: Rapid training data creation with weak supervision. Proceedings of the VLDB Endowment. International Conference on Very Large Data Bases, 11(3), 269", "ea6cc1f2-9c37-4871-9eef-b77981916fb1": "The games most di\ufb03cult for DQN\u2014especially Montezuma\u2019s Revenge on which DQN learned to perform about as well as the random player\u2014require deep planning beyond what DQN was designed to do.\n\nFurther, learning control skills through extensive practice, like DQN learned how to play the Atari games, is just one of the types of learning humans routinely accomplish. Despite these limitations, DQN advanced the state-of-the-art in machine learning by impressively demonstrating the promise of combining reinforcement learning with modern methods of deep learning. The ancient Chinese game of Go has challenged arti\ufb01cial intelligence researchers for many decades. Methods that achieve human-level skill, or even superhuman-level skill, in other games have not been successful in producing strong Go programs. Thanks to a very active community of Go programmers and international competitions, the level of Go program play has improved signi\ufb01cantly over the years. Until recently, however, no Go program had been able to play anywhere near the level of a human Go master. A team at DeepMind  developed the program AlphaGo that broke this barrier by combining deep ANNs (Section 9.6), supervised learning, Monte Carlo tree search (MCTS, Section 8.11), and reinforcement learning", "5c552f99-1e1b-45e4-931a-881cfbb6b81f": "Recall that the vector (Wi;,W2,,...,Wn,) is denoted W, ;. 705  CHAPTER 20. DEEP GENERATIVE MODELS  of hidden unit nv ) (j > 7) are shared among the groups: Whee = Wee (20.83)  The remaining weights, where j < i, are zero. Larochelle and Murray  chose this sharing scheme so that forward propagation in a NADE model would loosely resemble the computations performed in mean field inference to fill in missing inputs in an RBM. This mean field inference corresponds to running a recurrent network with shared weights, and the first step of that inference is the same as in NADE.\n\nThe only difference is that with NADE, the output weights connecting the hidden units to the output are parametrized independently from the weights connecting the input units to the hidden units. In the RBM, the hidden-to-output weights are the transpose of the input-to-hidden weights. The NADE architecture can be extended to mimic not just one time step of the mean field recurrent inference but k steps. This approach is called NADE-k", "c4b7f21b-9e4d-4c5b-b9a4-8a756c447ef0": "If we make a further generalization to allow the mixing coef\ufb01cients also to depend on the inputs then we obtain a mixture of experts model. Finally, if we allow each component in the mixture model to be itself a mixture of experts model, then we obtain a hierarchical mixture of experts. One of the many advantages of giving a probabilistic interpretation to the linear regression model is that it can then be used as a component in more complex probabilistic models. This can be done, for instance, by viewing the conditional distribution representing the linear regression model as a node in a directed probabilistic graph. Here we consider a simple example corresponding to a mixture of linear regression models, which represents a straightforward extension of the Gaussian mixture model discussed in Section 9.2 to the case of conditional Gaussian distributions. We therefore consider K linear regression models, each governed by its own weight parameter wk.\n\nIn many applications, it will be appropriate to use a common noise variance, governed by a precision parameter \u03b2, for all K components, and this is the case we consider here. We will once again restrict attention to a single target variable t, though the extension to multiple outputs is straightforward", "d15d7964-f0a3-4725-a023-d77084c16e5e": "The multiplicity W is also known as the weight of the macrostate. We can interpret the bins as the states xi of a discrete random variable X, where p(X = xi) = pi.\n\nThe entropy of the random variable X is then Distributions p(xi) that are sharply peaked around a few values will have a relatively low entropy, whereas those that are spread more evenly across many values will have higher entropy, as illustrated in Figure 1.30. Because 0 \u2a7d pi \u2a7d 1, the entropy is nonnegative, and it will equal its minimum value of 0 when one of the pi = 1 and all other pj\u0338=i = 0. The maximum entropy con\ufb01guration can be found by maximizing H using a Lagrange multiplier to enforce the normalization constraint Appendix E on the probabilities. Thus we maximize H for the broader distribution. The largest entropy would arise from a uniform distribution that would give H = \u2212 ln(1/30) = 3.40", "f415f695-837f-4ff4-a832-260e7bb25f79": "Recall from Figure 4.1 that the perpendicular distance of a point x from a hyperplane de\ufb01ned by y(x) = 0 where y(x) takes the form (7.1) is given by |y(x)|/\u2225w\u2225. Furthermore, we are only interested in solutions for which all data points are correctly classi\ufb01ed, so that tny(xn) > 0 for all n. Thus the distance of a point xn to the decision surface is given by The margin is given by the perpendicular distance to the closest point xn from the data set, and we wish to optimize the parameters w and b in order to maximize this distance.\n\nThus the maximum margin solution is found by solving where we have taken the factor 1/\u2225w\u2225 outside the optimization over n because w does not depend on n. Direct solution of this optimization problem would be very complex, and so we shall convert it into an equivalent problem that is much easier to solve. To do this we note that if we make the rescaling w \u2192 \u03baw and b \u2192 \u03bab, then the distance from any point xn to the decision surface, given by tny(xn)/\u2225w\u2225, is unchanged", "e2da01a1-8d50-467c-a954-fdc60fa91f04": "Let q(y|x) represent an arbitrary auxiliary distribution acting as a surrogate of the true posterior p(y|x), which is known as a variational distribution. Then, for each instance x\u2217 \u2208 D, we have: where the inequality holds because KL divergence is always nonnegative. The free energy upper bound contains two terms: the \ufb01rst one is the entropy of the variational distribution, which captures the intrinsic randomness (i.e., amount of information carried by an auxiliary distribution); the second term, now written as \u2212Eq(y|x\u2217)\u02dcpd(x\u2217) , by taking into account the empirical distribution \u02dcpd from which the instance x\u2217 is drawn, is the cross entropy between the distributions q(y|x\u2217)\u02dcpd(x\u2217) and p\u03b8(x\u2217, y), driving the two to be close and thereby allowing q to approximate p", "081d9c35-a200-4f23-af9f-5923b0f9ad73": "However, most of these methods make strong assumptions about stationarity and prior knowledge that are either violated or impossible to verify in applications and in the full reinforcement learning problem that we consider in subsequent chapters. The guarantees of optimality or bounded loss for these methods are of little comfort when the assumptions of their theory do not apply. In this book we do not worry about balancing exploration and exploitation in a sophisticated way; we worry only about balancing them at all. In this chapter we present several simple balancing methods for the k-armed bandit problem and show that they work much better than methods that always exploit. The need to balance exploration and exploitation is a distinctive challenge that arises in reinforcement learning; the simplicity of our version of the k-armed bandit problem enables us to show this in a particularly clear form. We begin by looking more closely at methods for estimating the values of actions and for using the estimates to make action selection decisions, which we collectively call action-value methods.\n\nRecall that the true value of an action is the mean reward when that action is selected. One natural way to estimate this is by averaging the rewards actually received: predicate denotes the random variable that is 1 if predicate is true and 0 if it is not", "e99f9617-de0c-4ae7-b002-f9cfc519fa78": "We used the Comparative Toxicogenomics Database (CTD)  for distant supervision, and additionally wrote labeling functions capturing language patterns and informationfromthecontexthierarchy.ToevaluateSnorkel\u2019s ability to discover previously unknown information, we randomly removed half of the relations in CTD and evaluated on candidates not contained in the remaining half. Spouses Our fourth task is to identify mentions of spouse relationships in a set of news articles from the Signal Media dataset .\n\nWe used all pairs of person mentions (tagged with SpaCy\u2019s NER module14) co-occurring in the same sentence as our candidate set. To obtain hand-labeled data for evaluation, we crowdsourced labels for the candidates via Amazon Mechanical Turk, soliciting labels from three workers for each example and assigning the majority vote. We then wrote labeling functions that encoded language patterns and distant supervision from DBpedia . In the cross-modal setting, we write labeling functions over one data modality (e.g., a text report, or the votes of crowdworkers) and use the resulting labels to train a classi\ufb01er de\ufb01ned over a second, totally separate modality (e.g., an image or the text of a tweet)", "196a28fe-b64b-4628-95b7-f926dd986d70": "Thus, g can remove information that may be useful for the downstream task, such as the color or orientation of objects. By leveraging the nonlinear transformation g(\u00b7), more information can be formed and maintained in h. To verify this hypothesis, we conduct experiments that use either h or g(h) to learn to predict the transformation applied during the pretraining. Here we set g(h) = W (2)\u03c3(W (1)h), with the same input and output dimensionality . Table 3 shows h contains much more information about the transformation applied, while g(h) loses information. Further analysis can 5.1. Normalized cross entropy loss with adjustable temperature works better than alternatives We compare the NT-Xent loss against other commonly used contrastive loss functions, such as logistic loss , and margin loss . Table 2 shows the objective function as well as the gradient to the input of the loss function. Looking at the gradient, we observe 1) \u21132 normalization (i.e", "2b8cc661-63a3-4e08-9552-08f59377569f": "For example, this allows the RVM to be used to help construct an emission density in a nonlinear extension of the linear dynamical system for tracking faces in video sequences . Section 13.3 So far, we have considered the RVM for binary classi\ufb01cation problems. For K > 2 classes, we again make use of the probabilistic approach in Section 4.3.4 in which there are K linear models of the form shows the decision boundary and the data points, with the relevance vectors indicated by circles. Comparison with the results shown in Figure 7.4 for the corresponding support vector machine shows that the RVM gives a much sparser model. The right-hand plot shows the posterior probability given by the RVM output in which the proportion of red (blue) ink indicates the probability of that point belonging to the red (blue) class. which are combined using a softmax function to give outputs The log likelihood function is then given by where the target values tnk have a 1-of-K coding for each data point n, and T is a matrix with elements tnk.\n\nAgain, the Laplace approximation can be used to optimize the hyperparameters , in which the model and its Hessian are found using IRLS", "3a0c7744-00eb-48f6-abaa-919b9f312860": "..2 At each time step t, the agent receives some representation of the environment\u2019s state, St 2 S, and on that basis selects an action, At 2 A(s).3 One time step later, in part as a consequence of its action, the agent receives a numerical reward, Rt+1 2 R \u21e2 R, and \ufb01nds itself in a new state, St+1.4 The MDP and agent together thereby give rise to a sequence or trajectory that begins like this: In a \ufb01nite MDP, the sets of states, actions, and rewards (S, A, and R) all have a \ufb01nite number of elements. In this case, the random variables Rt and St have well de\ufb01ned discrete probability distributions dependent only on the preceding state and action.\n\nThat is, for particular values of these random variables, s0 2 S and r 2 R, there is a probability of those values occurring at time t, given particular values of the preceding state and action: for all s0, s 2 S, r 2 R, and a 2 A(s). The function p de\ufb01nes the dynamics of the MDP", "78960f75-5af2-4f7d-a43d-89a1ef3dc69c": "Development and application of CMAC neural Kumar, P. R., Varaiya, P. Stochastic Systems: Estimation, Identi\ufb01cation, and Adaptive Kumar, P. R. A survey of some results in stochastic adaptive control. SIAM Journal of Kumar, V., Kanal, L. N. The CDP, A unifying formulation for heuristic search, dynamic Arti\ufb01cial Intelligence, pp. 1\u201337. Springer-Verlag, Berlin. Kuvayev, L., Sutton, R.S. Model-based reinforcement learning with an approximate, learned model. Proceedings of the Ninth Yale Workshop on Adaptive and Learning Systems, Lagoudakis, M., Parr, R. Least squares policy iteration. Journal of Machine Learning Lai, T. L., Robbins, H. Asymptotically e\ufb03cient adaptive allocation rules. Advances in stochastic games with incomplete information: A uni\ufb01ed approach. SIAM Journal of Control and Optimization, 20(4):541\u2013552", "ff1473a6-49b0-4fa2-90e7-d94f5a7cd197": "To do this, we consider a small sphere centred on the point x at which we wish to estimate the density p(x), and we allow the radius of the sphere to grow until it contains precisely K data points. The estimate of the density p(x) is then given by (2.246) with V set to the volume of the resulting sphere.\n\nThis technique is known as K nearest neighbours and is illustrated in Figure 2.26, for various choices of the parameter K, using the same data set as used in Figure 2.24 and Figure 2.25. We see that the value of K now governs the degree of smoothing and that again there is an optimum choice for K that is neither too large nor too small. Note that the model produced by K nearest neighbours is not a true density model because the integral over all space diverges. Exercise 2.61 We close this chapter by showing how the K-nearest-neighbour technique for density estimation can be extended to the problem of classi\ufb01cation. To do this, we apply the K-nearest-neighbour density estimation technique to each class separately and then make use of Bayes\u2019 theorem", "2c5f9abd-a79b-4e74-ae66-260b3da184cd": "Here we consider a linear regression model whose parameters are determined by minimizing a regularized sum-of-squares error function given by where \u03bb \u2a7e 0. If we set the gradient of J(w) with respect to w equal to zero, we see that the solution for w takes the form of a linear combination of the vectors \u03c6(xn), with coef\ufb01cients that are functions of w, of the form where \u03a6 is the design matrix, whose nth row is given by \u03c6(xn)T. Here the vector a = (a1, . , aN)T, and we have de\ufb01ned Instead of working with the parameter vector w, we can now reformulate the leastsquares algorithm in terms of the parameter vector a, giving rise to a dual representation. If we substitute w = \u03a6Ta into J(w), we obtain where t = (t1, . , tN)T", "e996fe59-3676-43f1-b1f3-8c16348c936a": "Given these feature vectors, you suspect that you still have to average out some noise, so you decide that you want learning to be gradual, taking about 10 presentations with the same feature vector before learning nears its asymptote. What step-size parameter \u21b5 should you use? Why? \u21e4 Arti\ufb01cial neural networks (ANNs) are widely used for nonlinear function approximation. An ANN is a network of interconnected units that have some of the properties of neurons, the main components of nervous systems.\n\nANNs have a long history, with the latest advances in training deeply-layered ANNs (deep learning) being responsible for some of the most impressive abilities of machine learning systems, including reinforcement learning systems. In Chapter 16 we describe several impressive examples of reinforcement learning systems that use ANN function approximation. network, that is, there are no paths within the network by which a unit\u2019s output can in\ufb02uence its input. The network in the \ufb01gure has an output layer consisting of two output units, an input layer with four input units, and two \u201chidden layers\u201d: layers that are neither input nor output layers. A real-valued weight is associated with each link", "54631066-701a-4491-8ff7-438a8bbbf644": "In the first stage, the layer performs several convolutions in parallel to produce a set of linear activations.\n\nIn the second stage, each linear activation is run through a nonlinear activation function, such as the rectified linear activation function. This stage is sometimes called the detector stage. In the third stage, we use a pooling function to modify the output of the layer further. A pooling function replaces the output of the net at a certain location with a summary statistic of the nearby outputs. For example, the max pooling  operation reports the maximum output within a rectangular neighborhood. Other popular pooling functions include the average of a rectangular neighborhood, the L? norm of a rectangular neighborhood, or a weighted average based on the distance from the central pixel. 335  CHAPTER 9", "fbf45316-f9b3-419a-bb8e-ce104dda4830": "of all the eigenvalues of the matrix. The absolute value of the determinant can be thought of as a measure of how much multiplication by the matrix expands or contracts  space. If the determinant is 0, then space is contracted completely along at least one dimension, causing it to lose all its volume. If the determinant is 1, then the transformation preserves volume. 2.12 Example: Principal Components Analysis  One simple machine learning algorithm, principal components analysis (PCA), can be derived using only knowledge of basic linear algebra. 45  CHAPTER 2. LINEAR ALGEBRA  Suppose we have a collection of m points fa, eey x (\u2122)\\ in R\u201d and we want to apply lossy compression to these points. Lossy compression means storing the points in a way that requires less memory but may lose some precision. We want to lose as little precision as possible. One way we can encode these points is to represent a lower-dimensional version of them.\n\nFor each point x \u20ac IR\u201d we will find a corresponding code vector cMeR!. If J is smaller than n, storing the code points will take less memory than storing the original data", "f49d64e0-ac40-423d-90fb-70facbe1974a": "The MoCo dictionary is not differentiable as a queue, so we cannot rely on back-propagation to update the key encoder f;.\n\nOne naive way might be to use the same encoder for both fa and fy. Differently, MoCo proposed to use a momentum-based update with a momentum coefficient  m \u20ac [0, 1). Say, the parameters of f, and fi, are labeled as 0, and 0;, respectively. 0, < mOz + (1\u2014 m)Oq  gradient contrastive loss  A i (~ similarity 7  q Kio ky kg ... { queue { momentum cw encoder 4 a eed rey ak\u00aeY k\u00aeY |, (FIFO queue)  https://lilianweng.github.io/posts/2021-05-31-contrastive/  Contrastive Representation Learning | Lil'Log   The advantage of MoCo compared to SimCLR is that MoCo decouples the batch size from the number of negatives, but SimCLR requires a large batch size in order to have enough negative samples and suffers performance drops when their batch size is reduced", "260c1ce3-76b1-47d9-b52c-220bf4575462": "The augmentation should significantly change its visual appearance but keep the semantic meaning unchanged. Basic Image Augmentation  There are many ways to modify an image while retaining its semantic meaning. We can use any one of the following augmentation or a composition of multiple operations.\n\ne Random cropping and then resize back to the original size. e Random color distortions  e Random Gaussian blur  e Random color jittering e Random horizontal flip e Random grayscale conversion  e Multi-crop augmentation: Use two standard resolution crops and sample a set of additional low  resolution crops that cover only small parts of the image. Using low resolution crops reduces the compute cost. (SwAV)  e And many more ...  https://lilianweng.github.io/posts/2021-05-31-contrastive/  Contrastive Representation Learning | Lil'Log  Augmentation Strategies  Many frameworks are designed for learning good data augmentation strategies (i.e. a composition of multiple transforms). Here are a few common ones. AutoAugment : Inspired by NAS, AutoAugment frames the problem of learning best data augmentation operations (i.e", "af08251d-bd08-4d03-b571-0f4d3cb1ff9b": "A simple example is the PCA algorithm, which copies its input \u00ab to an approximate reconstruction r using the function W'We. It is common for more general autoencoders to use multiplication by the transpose of the weight matrix just as PCA does. To make such models convolutional, we can use the function h to perform the transpose of the convolution operation.\n\nSuppose we have hidden units H in the same format as Z and we define a reconstruction  R = h(K,H,s). (9.14)  To train the autoencoder, we will receive the gradient with respect to R as a tensor E. To train the decoder, we need to obtain the gradient with respect to K. This is given by g(H,E,s). To train the encoder, we need to obtain the gradient with respect to H. This is given by c(K,E,s). It is also possible to differentiate through g using c and h, but these operations are not needed for the back-propagation algorithm on any standard network architectures. https://www.deeplearningbook.org/contents/convnets.html    CHAPTER 9", "ab6dfbc1-2e8f-4f56-8bc5-39b5f32d93c8": "To finish the minimization problem, we must choose the values to ensure that all our constraints are satisfied. We are free to choose any values, because the gradient of the Lagrangian with respect to the \\ variables is zero as long as the constraints are satisfied. To satisfy all the constraints, we may set 4; = 1 \u2014logaV2z7, 2 = 0, and A3 = sb to obtain  p(x) =N (2; 1,07). (19.54) This is one reason for using the normal distribution when we do not know the true distribution. Because the normal distribution has the maximum entropy, we impose the least possible amount of structure by making this assumption. While examining the critical points of the Lagrangian functional for the entropy, we found only one critical point, corresponding to maximizing the entropy for fixed variance. What about the probability distribution function that minimizes the entropy? Why did we not find a second critical point corresponding to the  wan tester? TL", "7c0485d7-9e81-4ca4-8848-f7e58c81aa5d": "Starting at the \ufb01rst node of the chain, we can then work along the chain and evaluate \u03b1(zn) for every latent node. Because each step of the recursion involves multiplying by a K \u00d7 K matrix, the overall cost of evaluating these quantities for the whole chain is of O(K2N).\n\nWe can similarly \ufb01nd a recursion relation for the quantities \u03b2(zn) by making use of the conditional independence properties (13.27) and (13.28) giving p(xn+1, . , xN|zn, zn+1)p(zn+1|zn) p(xn+2, . , xN|zn+1)p(xn+1|zn+1)p(zn+1|zn). (13.38) for evaluation of the \u03b2 variables", "bc7190a7-5b50-4c9c-87d3-204b063b943b": "DSC 291 Machine Learning with Few Labels   DSC291-Winter2023  Logistics Lectures Homework Project  Viachine Learning with Few Labels  DSC 291 + Winter 2023 - UC San Diego  Machine learning is about computational methods that enable machines to learn concepts from experience. Many of the successful results of machine learning rely on supervised learning with massive amount of data labels. However, in many real problems we do not have enough labeled data, but instead have access to other forms of experience, such as structured knowledge, constraints, feedback signals from environment, auxiliary models from related tasks, etc. This course focuses on those learning settings with few labels, where one has to go beyond supervised learning and use other learning methods. This course is designed to give students a holistic understanding of related problems and methodologies (such as zero/few-shot learning, self/weakly-supervised learning, transfer learning, meta-learning, reinforcement learning, adversarial learning, knowledge constrained learning, panoramic learning), different possible perspectives of formulating the same problems, the underlying connections between the diversity of algorithms, and open questions in the field", "c983718e-f7b8-4786-a586-f483a98208ec": "We now allow the probability distribution of zn to depend on the state of the previous latent variable zn\u22121 through a conditional distribution p(zn|zn\u22121).\n\nBecause the latent variables are K-dimensional binary variables, this conditional distribution corresponds to a table of numbers that we denote by A, the elements of which are known as transition probabilities. They are given by Ajk \u2261 p(znk = 1|zn\u22121,j = 1), and because they are probabilities, they satisfy 0 \u2a7d Ajk \u2a7d 1 with \ufffd has K(K\u22121) independent parameters. We can then write the conditional distribution explicitly in the form The initial latent node z1 is special in that it does not have a parent node, and so it has a marginal distribution p(z1) represented by a vector of probabilities \u03c0 with elements \u03c0k \u2261 p(z1k = 1), so that The transition matrix is sometimes illustrated diagrammatically by drawing the states as nodes in a state transition diagram as shown in Figure 13.6 for the case of K = 3", "4d9c44c4-f535-47af-8d18-72818a062a68": "6.13 (\u22c6) Show that the Fisher kernel, de\ufb01ned by (6.33), remains invariant if we make a nonlinear transformation of the parameter vector \u03b8 \u2192 \u03c8(\u03b8), where the function \u03c8(\u00b7) is invertible and differentiable.\n\n6.15 (\u22c6) By considering the determinant of a 2 \u00d7 2 Gram matrix, show that a positivede\ufb01nite kernel function k(x, x\u2032) satis\ufb01es the Cauchy-Schwartz inequality 6.16 (\u22c6 \u22c6) Consider a parametric model governed by the parameter vector w together with a data set of input values x1, . , xN and a nonlinear feature mapping \u03c6(x). Suppose that the dependence of the error function on w takes the form where g(\u00b7) is a monotonically increasing function. By writing w in the form show that the value of w that minimizes J(w) takes the form of a linear combination of the basis functions \u03c6(xn) for n = 1, . , N. 6.17 (\u22c6 \u22c6) www Consider the sum-of-squares error function (6.39) for data having noisy inputs, where \u03bd(\u03be) is the distribution of the noise", "91656003-da98-4eab-8bc0-17367c2df42b": "One of the most pressing areas for future reinforcement learning research is to adapt and extend methods developed in control engineering with the goal of making it acceptably safe to fully embed reinforcement learning agents into physical environments. In closing, we return to Simon\u2019s call for us to recognize that we are designers of our future and not simply spectators.\n\nBy decisions we make as individuals, and by the in\ufb02uence we can exert on how our societies are governed, we can work toward ensuring that the bene\ufb01ts made possible by a new technology outweigh the harm it can cause. There is ample opportunity to do this in the case of reinforcement learning, which can help improve the quality, fairness, and sustainability of life on our planet, but which can also release new perils. A threat already here is the displacement of jobs caused by applications of arti\ufb01cial intelligence. Still there are good reasons to believe that the bene\ufb01ts of arti\ufb01cial intelligence can outweigh the disruption it causes. As to safety, hazards possible with reinforcement learning are not completely di\u21b5erent from those that have been managed successfully for related applications of optimization and control methods. As reinforcement learning moves out into the real world in future applications, developers have an obligation to follow best practices that have evolved for similar technologies, while at the same time extending them to make sure that Prometheus keeps the upper hand", "8042c84e-d1eb-4a4b-86a1-bb196bab3853": "In particular, the assumption that the Hessian matrix has full rank is often not valid since many of the parameters are not \u2018well-determined\u2019. We can use the result (4.137) to obtain a more accurate estimate Section 3.5.3 of the model evidence starting from the Laplace approximation, as we illustrate in the context of neural networks in Section 5.7. We now turn to a Bayesian treatment of logistic regression. Exact Bayesian inference for logistic regression is intractable. In particular, evaluation of the posterior distribution would require normalization of the product of a prior distribution and a likelihood function that itself comprises a product of logistic sigmoid functions, one for every data point. Evaluation of the predictive distribution is similarly intractable. Here we consider the application of the Laplace approximation to the problem of Bayesian logistic regression . Recall from Section 4.4 that the Laplace approximation is obtained by \ufb01nding the mode of the posterior distribution and then \ufb01tting a Gaussian centred at that mode", "6f4a643e-e48f-4320-8b95-14dc250dcb81": "It is more common in the MDP literature to describe the dynamics in terms of the state transition probabilities p(s0|s, a) and expected next rewards r(s, a). In reinforcement learning, however, we more often have to refer to individual actual or sample rewards (rather than just their expected values). Our notation also makes it plainer that St and Rt are in general jointly determined, and thus must have the same time index. In teaching reinforcement learning, we have found our notation to be more straightforward conceptually and easier to understand.\n\nThe bioreactor example is based on the work of Ungar  and Miller and Williams . The recycling robot example was inspired by the can-collecting robot built by Jonathan Connell . Kober and Peters  present a collection of robotics applications of reinforcement learning. three types of tasks: (1) \ufb01nite-horizon tasks, in which interaction terminates after a particular \ufb01xed number of time steps; (2) inde\ufb01nite-horizon tasks, in which interaction can last arbitrarily long but must eventually terminate; and (3) in\ufb01nite-horizon tasks, in which interaction does not terminate. Our episodic and continuing tasks are similar to inde\ufb01nite-horizon and in\ufb01nite-horizon tasks, respectively, but we prefer to emphasize the di\u21b5erence in the nature of the interaction", "5211ee67-10f0-42e1-a87c-aee39c2bb3d0": "As we shall see, in the limit of an in\ufb01nite number of basis functions, a Bayesian neural network with an appropriate prior reduces to a Gaussian process, thereby providing a deeper link between neural networks and kernel methods.\n\nSection 6.4.7 In Chapter 3, we discussed regression models based on linear combinations of \ufb01xed basis functions, although we did not discuss in detail what form those basis functions might take. One choice that has been widely used is that of radial basis functions, which have the property that each basis function depends only on the radial distance (typically Euclidean) from a centre \u00b5j, so that \u03c6j(x) = h(\u2225x \u2212 \u00b5j\u2225). Historically, radial basis functions were introduced for the purpose of exact function interpolation . Given a set of input vectors {x1, . , xN} along with corresponding target values {t1, . , tN}, the goal is to \ufb01nd a smooth function f(x) that \ufb01ts every target value exactly, so that f(xn) = tn for n = 1, . , N", "3bd216be-b506-41df-b162-66d8f1cd91b4": "This can be obtained by setting n = N in (13.33) and replacing \u03b1(zN) with its de\ufb01nition (13.34) to give which we see will be correct provided we take \u03b2(zN) = 1 for all settings of zN.\n\nIn the M step equations, the quantity p(X) will cancel out, as can be seen, for instance, in the M-step equation for \u00b5k given by (13.20), which takes the form However, the quantity p(X) represents the likelihood function whose value we typically wish to monitor during the EM optimization, and so it is useful to be able to evaluate it. If we sum both sides of (13.33) over zn, and use the fact that the left-hand side is a normalized distribution, we obtain Thus we can evaluate the likelihood function by computing this sum, for any convenient choice of n. For instance, if we only want to evaluate the likelihood function, then we can do this by running the \u03b1 recursion from the start to the end of the chain, and then use this result for n = N, making use of the fact that \u03b2(zN) is a vector of 1s", "fd1a2cf6-c0b8-4378-9528-e4f31ca1cb15": "For example, consider the state transition diagram: Here the solid square represents the special absorbing state corresponding to the end of an episode. Starting from S0, we get the reward sequence +1, +1, +1, 0, 0, 0, . .. Summing these, we get the same return whether we sum over the \ufb01rst T rewards (here T = 3) or over the full in\ufb01nite sequence. This remains true even if we introduce discounting. Thus, we can de\ufb01ne the return, in general, according to (3.8), using the convention of omitting episode numbers when they are not needed, and including the possibility that \u03b3 = 1 if the sum remains de\ufb01ned (e.g., because all episodes terminate). Alternatively, we can write including the possibility that T = 1 or \u03b3 = 1 (but not both).\n\nWe use these conventions throughout the rest of the book to simplify notation and to express the close parallels between episodic and continuing tasks", "eaf29876-49c7-44ec-87e8-b87c3c960015": "5.2 Capacity, Overfitting and Underfitting  The central challenge in machine learning is that our algorithm must perform well on new, previously unseen inputs\u2014not just those on which our model was trained. The ability to perform well on previously unobserved inputs is called generalization. Typically, when training a machine learning model, we have access to a training set; we can compute some error measure on the training set, called the training error; and we reduce this training error. So far, what we have described is simply an optimization problem. What separates machine learning from optimization is chat we want the generalization error, also called the test error, to be low as well. The generalization error is defined as the expected value of the error on a new input. Here the expectation is taken across different possible inputs, drawn from the distribution of inputs we expect the system to encounter in practice", "7d0b7b5c-8629-4443-92fd-5901f344fdd3": "The half-rectifying nonlinearity was intended to capture these properties of biological neurons: (1) For some inputs, biological neurons  222  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  are completely inactive. (2) For some inputs, a biological neuron\u2019s output is proportional to its input.\n\n(3) Most of the time, biological neurons operate in the regime where they are inactive (i.e., they should have sparse activations). When the modern resurgence of deep learning began in 2006, feedforward networks continued to have a bad reputation. From about 2006 to 2012, it was widely believed that feedforward networks would not perform well unless they were assisted by other models, such as probabilistic models. Today, it is now known that with the right resources and engineering practices, feedforward networks  https://www.deeplearningbook.org/contents/mlp.html    perform very well. Today, gradient-based learning in feedforward networks is used as a tool to develop probabilistic models, such as the variational autoencoder and generative adversarial networks, described in chapter 20", "7244cb78-7e45-479d-9830-a9525c3fc457": "Most of the remaining chapters introduce additional representation learning algorithms that develop these criteria in different ways or introduce other criteria. 144  CHAPTER 5. MACHINE LEARNING BASICS  https://www.deeplearningbook.org/contents/ml.html    20 e  10 \u00b0  g& 0 8 \u201410 C} \u201420 \u201420 -10 0 10.20 \u201420 -10 0 10.20 Ly ZL  Figure 5.8: PCA learns a linear projection that aligns the direction of greatest variance with the axes of the new space. (Left)The original data consist of samples ofa. In this space, the variance might occur along directions that are not axis aligned. (Right)/The transformed data z =a! W now varies most along the axis z;. The direction of second-most variance is now along z. 5.8.1 Principal Components Analysis  In section 2.12, we saw that the principal components analysis algorithm provides a means of compressing data.\n\nWe can also view PCA as an unsupervised learning algorithm that learns a representation of data. This representation is based on two of the criteria for a simple representation described above", "d2c95c9d-0bd4-4ecf-bb09-d9ca5d944b14": "In this example, we do not use any pooling, so only the convolution operation itself shrinks the network size. (Top)In this convolutional network, we do not use any implicit zero padding. This causes the representation to shrink by five pixels at each layer. Starting from an input of sixteen pixels, we are only able to have three convolutional layers, and the last layer does not ever move the kernel, so arguably only two of the layers are truly convolutional. The rate of shrinking can be mitigated by using smaller kernels, but smaller kernels are less expressive, and some shrinking is inevitable in this kind of architecture.\n\n(Bottom)By adding five implicit zeros to each layer, we prevent the representation from shrinking with depth. This allows us to make an arbitrarily deep convolutional network. terms of test set classification accuracy) lies somewhere between \u201cvalid\u201d and \u201csame\u201d convolution. In some cases, we do not actually want to use convolution, but want to use locally connected layers instead . In this case, the adjacency matrix in the graph of our MLP is the same, but every connection has its own weight, specified by a 6-D tensor W", "536677df-8084-43a6-9bdc-5c4055331335": "Starting from the previous state x, inject corruption noise, sampling # from C(~  https://www.deeplearningbook.org/contents/generative_models.html    Figure 20.11: Each step of the Markov chain associated with a trained denoising autoen- coder, which generates the samples from the probabilistic model implicitly trained by the denoising log-likelihood criterion. Each step consists in (a) injecting noise via corruption process C\u2019 in state a, yielding @, (b) encoding it with function f, yielding h = f(z), (c) decoding the result with function g, yielding parameters w for the reconstruction distribution, and (d) given w, sampling a new state from the reconstruction distribution p(x | w = o( f(#))).\n\nIn the typical squared reconstruction error case, g(h) = \u00a3, which estimates E, corruption consists of adding Gaussian noise, and sampling from p(x | w) consists of adding Gaussian noise a second time to the reconstruction&", "6f75f057-1462-4e45-9f48-897dfbdec7e7": "Next consider the maximization with respect to the parameter vector wk of the kth linear regression model.\n\nSubstituting for the Gaussian distribution, we see that the function Q(\u03b8, \u03b8old), as a function of the parameter vector wk, takes the form where the constant term includes the contributions from other weight vectors wj for j \u0338= k. Note that the quantity we are maximizing is similar to the (negative of the) standard sum-of-squares error (3.12) for a single linear regression model, but with the inclusion of the responsibilities \u03b3nk. This represents a weighted least squares problem, in which the term corresponding to the nth data point carries a weighting coef\ufb01cient given by \u03b2\u03b3nk, which could be interpreted as an effective precision for each data point. We see that each component linear regression model in the mixture, governed by its own parameter vector wk, is \ufb01tted separately to the whole data set in the M step, but with each data point n weighted by the responsibility \u03b3nk that model k takes for that data point", "dc8d5455-d8d5-4294-8bf9-8bcc95c0517b": "Personalized ad recommendation for life-time value optimization guarantees. In Proceedings of the Twenty-Fourth International Joint Conference on Arti\ufb01cial Intelligence. AAAI Press, Palo Alto, CA. Thistlethwaite, D. A critical review of latent learning and related experiments. PsychoInternational Conference on Machine Learning, JMLR W&CP 32(1), pp. 441\u2013448. Thomas, P. S. Safe Reinforcement Learning. Ph.D. thesis, University of Massachusetts, Thomas, P. S., Brunskill, E. Policy gradient methods for reinforcement learning with function approximation and action-dependent baselines. ArXiv:1706.06643. Thomas, P. S., Theocharous, G., Ghavamzadeh, M. High-con\ufb01dence o\u21b5-policy evaluation. In Proceedings of the Twenty-Ninth AAAI Conference on Arti\ufb01cial Intelligence, pp. 3000\u20133006. AAAI Press, Menlo Park, CA.\n\nThompson, W. R", "7331e252-603e-4b2a-8913-78432ec608ed": "E\ufb03cient planning in MDPs by small backups. In: Proceedings of the 30th International Conference on Machine Learning , pp. 361\u2013369. van Seijen, H., Sutton, R. S. True online TD(\u03bb). In Proceedings of the 31st International Conference on Machine Learning , pp. 692\u2013700. JMLR W&CP 32(1), van Seijen, H., Mahmood, A. R., Pilarski, P. M., Machado, M. C., Sutton, R. S. True online temporal-di\u21b5erence learning. Journal of Machine Learning Research, 17(145):1\u201340. van Seijen, H., Van Hasselt, H., Whiteson, S., Wiering, M. A theoretical and empirical analysis of Expected Sarsa. In IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning, pp. 177\u2013184", "29e44166-c3a5-4708-b3c9-3a9f9082b319": "Our SQL formulation allows us to additionally incorporate the multi-step variant of the PCL training  to resolve the issue.\n\nSpeci\ufb01cally, by applying a telescoping sum on the consistency equation (Eq.6) starting from t up to T, we arrive at the multi-step (9) We can see the objective side-steps the need to bootstrap intermediate value functions V\u00af\u03b8(st\u2032) for t\u2032 > t. Instead, it directly uses the non-zero end reward rT to derive the update for \u03b8. Please see Figure 2 (right) for an illustration. In practice, we combine the single- and multi-step objectives (Eqs.7 and 9) together for training. Joint On- and Off-policy Training. Finally, we highlight that the behavior policy \u03c0\u2032 involved in the objectives Eqs. (7) and (9) can be an arbitrary policy. For example, \u03c0\u2032 can be a (possibly noisy) text dataset, or a set of text samples produced by other generation models, resulting in off-policy training. We can also set \u03c0\u2032 to be the current generation model \u03c0\u03b8 to be learned, resulting in on-policy training", "806b52d0-26c2-4bcf-bb07-7fdeed6d3c02": "Newton\u2019s method is based on using a second-order Taylor series expansion to approximate f(a) near some point \u00ab):  1 f(a) \u00a9 F(a) + (ea) Vef(e) +5 (w\u20142) TH(f)(@ )(a\u2014#). (4.11) If we then solve for the critical point of this function, we obtain a = 2 \u2014 H(f)(e) Ve f(a). (4.12)  When f is a positive definite quadratic function, Newton\u2019s method consists of applying equation 4.12 once to jump to the minimum of the function directly. When f is not truly quadratic but can be locally approximated as a positive definite quadratic, Newton\u2019s method consists of applying equation 4.12 multiple times. Iteratively updating the approximation and jumping to the minimum of the approximation can reach the critical point much faster than gradient descent would", "f230c403-d63d-4352-864c-dfee66443158": "A further limitation of the Gaussian distribution is that it is intrinsically unimodal (i.e., has a single maximum) and so is unable to provide a good approximation to multimodal distributions. Thus the Gaussian distribution can be both too \ufb02exible, in the sense of having too many parameters, while also being too limited in the range of distributions that it can adequately represent. We will see later that the introduction of latent variables, also called hidden variables or unobserved variables, allows both of these problems to be addressed. In particular, a rich family of multimodal distributions is obtained by introducing discrete latent variables leading to mixtures of Gaussians, as discussed in Section 2.3.9.\n\nSimilarly, the introduction of continuous latent variables, as described in Chapter 12, leads to models in which the number of free parameters can be controlled independently of the dimensionality D of the data space while still allowing the model to capture the dominant correlations in the data set. Indeed, these two approaches can be combined and further extended to derive a very rich set of hierarchical models that can be adapted to a broad range of practical applications", "c94aef80-e32a-4f63-9d0a-e68e9ace2b50": "Examples include the knowledge of expected feature values, maximum margin structures (Section 2.3), logical rules, and so on. The knowledge generally imposes constraints that we want the target model to satisfy. The experience function in the standard equation is a natural vehicle for incorporating such knowledge constraints in learning. Given a con\ufb01guration t, the experience function f(t) measures the degree to which the con\ufb01guration satis\ufb01es the constraints. As an example, we consider \ufb01rst-order logic (FOL) rules, which provide an expressive declarative language to encode complex symbolic knowledge . More concretely, let frule(t) be an FOL rule w.r.t. the variables t. For \ufb02exibility, we use soft logic  to formulate the rule.\n\nSoft logic allows continuous truth values from the interval  instead of {0, 1}, and the Boolean logical operators are rede\ufb01ned as: Here & and \u2227 are two di\ufb00erent approximations to logical conjunction: & is useful as a selection operator (e.g., A&B = B when A = 1, and A&B = 0 when A = 0), while \u2227 is an averaging operator", "386b919f-1b3f-4308-9297-c789bf4db0f1": "For PCA and probabilistic PCA, if we rotate the coordinate system in data space, then we obtain exactly the same fit to the data but with the W matrix transformed by the corresponding rotation matrix. However, for factor analysis, the analogous property is that if we make a component-wise re-scaling of the data vectors, then this is absorbed into a corresponding re-scaling of the elements of \\)i.\n\nIn Chapter 6, we saw how the technique of kernel substitution allows us to take an algorithm expressed in terms of scalar products of the form x T x' and generalize that algorithm by replacing the scalar products with a nonlinear kernel. Here we apply this technique of kernel substitution to principal component analysis, thereby obtaining a nonlinear generalization called kernel peA . Consider a data set {xn } of observations, where n = 1, ... , N, in a space of dimensionality D. In order to keep the notation uncluttered, we shall assume that we have already subtracted the sample mean from each of the vectors X n , so that Ln Xn = O", "e10451e6-7168-4c74-be55-9b0c9d89f554": "1 SULLWAaLE UL PLELMENLAaALLOLs, to avoid numerical problems, it is best to write the negative log-likelihood as a function of 2, rather than as a function of \u00a5 = o(2). If the sigmoid function underflows to zero, then taking the logarithm of % yields negative infinity. 6.2.2.3 Softmax Units for Multinoulli Output Distributions  Any time we wish to represent a probability distribution over a discrete variable with n possible values, we may use the softmax function. This can be seen as a generalization of the sigmoid function, which was used to represent a probability distribution over a binary variable. Softmax functions are most often used as the output of a classifier, to represent the probability distribution over n different classes. More rarely, softmax functions  180  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  can be used inside the model itself, if we wish the model to choose between one of n different options for some internal variable", "b73bf66c-a014-46cb-9af7-8f649972a60b": "The \ufb01eld has developed strong mathematical foundations and impressive applications.\n\nThe computational study of reinforcement learning is now a large \ufb01eld, with hundreds of active researchers around the world in diverse disciplines such as psychology, control theory, arti\ufb01cial intelligence, and neuroscience. Particularly important have been the contributions establishing and developing the relationships to the theory of optimal control and dynamic programming. The overall problem of learning from interaction to achieve goals is still far from being solved, but our understanding of it has improved signi\ufb01cantly. We can now place component ideas, such as temporal-di\u21b5erence learning, dynamic programming, and function approximation, within a coherent perspective with respect to the overall problem. Our goal in writing this book was to provide a clear and simple account of the key ideas and algorithms of reinforcement learning. We wanted our treatment to be accessible to readers in all of the related disciplines, but we could not cover all of these perspectives in detail. For the most part, our treatment takes the point of view of arti\ufb01cial intelligence and engineering. Coverage of connections to other \ufb01elds we leave to others or to another time. We also chose not to produce a rigorous formal treatment of reinforcement learning", "27a7386a-d046-4a71-99c6-ccd9e8c2d277": "Then an LSTM is trained with a read attention vector over the support set as part of the hidden state:  hy, c; = LSTM(f'(x), , er-1)  h; =h + f'(x) k  J\u00bb a(by-1, 9(x:))9(:)  i=1  Tt-1  exp(hy_, 9(x:))  a(hy_1, 9(x;)) = softmax(hy_,g(x;)) = 5-1 exp(hy_, 9(x;)) Eventually f(x, S\u2019) = hy if we do K steps of \u201cread\u201d. This embedding method is called \u201cFull Contextual Embeddings (FCE)\". Interestingly it does help improve the performance on a hard task (few-shot classification on mini ImageNet), but makes no difference on a simple task (Omniglot)", "0cdc9eff-422d-4037-9bb7-312b78470546": "), Advances in Neural Information Processing Systems, Volume 11, pp. 592\u2013598. MIT Press. Tipping, M. E. Sparse Bayesian learning and the relevance vector machine. Journal of Machine Learning Research 1, 211\u2013244. Tipping, M. E. and C. M. Bishop . Probabilistic principal component analysis. Technical Report NCRG/97/010, Neural Computing Research Group, Aston University. Tipping, M. E. and C. M. Bishop . Mixtures of probabilistic principal component analyzers. Neural Computation 11(2), 443\u2013482. Tipping, M. E. and C. M. Bishop . Probabilistic principal component analysis. Journal of the Royal Statistical Society, Series B 21(3), 611\u2013622. Tipping, M. E. and A. Faul . Fast marginal likelihood maximization for sparse Bayesian models", "7b463819-f88e-4d74-9c8f-0703d2ae1fea": "The derivative of the function then describes how the output value varies as we make in\ufb01nitesimal changes to the input value. Similarly, we can de\ufb01ne a functional as a mapping that takes a function as the input and that returns the value of the functional as the output. An example would be the entropy H, which takes a probability distribution p(x) as the input and returns the quantity as the output. We can the introduce the concept of a functional derivative, which expresses how the value of the functional changes in response to in\ufb01nitesimal changes to the input function . The rules for the calculus of variations mirror those of standard calculus and are discussed in Appendix D. Many problems can be expressed in terms of an optimization problem in which the quantity being optimized is a functional. The solution is obtained by exploring all possible input functions to \ufb01nd the one that maximizes, or minimizes, the functional. Variational methods have broad applicability and include such areas as \ufb01nite element methods  and maximum entropy", "3e1ed19b-233f-4f4f-9870-1fc06e6eeeb9": "12.4.1 n-grams  A language model defines a probability distribution over sequences of tokens in a natural language.\n\nDepending on how the model is designed, a token may be a word, a character, or even a byte. Tokens are always discrete entities. The earliest successful language models were based on models of fixed-length sequences of tokens called n-grams. An n-gram is a sequence of n tokens. Models based on n-grams define the conditional probability of the n-th token given the preceding n\u2014 1 tokens. The model uses products of these conditional distributions to define the probability distribution over longer sequences:  P(21,...,2,) = P(21,..-,%n\u20141) Il P(x, | Vt\u2014-nti,-++,%t-1)- (12.5) t=n This decomposition is justified by the chain rule of probability. The probability distribution over the initial sequence P(2,,...,2%,\u20141) may be modeled by a different model with a smaller value of n.  456  CHAPTER 12", "2fa6d3ef-1948-4e83-8314-027a2eef8f30": "CONVOLUTIONAL NETWORKS  is used exactly once when computing the output of a layer. It is multiplied by one element of the input and then never revisited. As a synonym for parameter sharing, one can say that a network has tied weights , because the value of the weight applied to one input is tied to the value of a weight applied elsewhere. In a convolutional neural net, each member of the kernel is used at every position of the input (except perhaps some of the boundary pixels, depending on the design decisions regarding the boundary). The parameter sharing used by the convolution operation means that rather than learning a separate set of parameters for every location, we learn only one set. This does not affect the runtime of forward propagation\u2014it is still O(k x n)\u2014but it does further reduce the storage requirements of the model to k parameters.\n\nRecall that k is usually several orders of magnitude smaller than m. Since m and n are usually roughly the same size, k is practically insignificant compared to . Convolution is thus dramatically more efficient than dense matrix multiplication in terms of the memory requirements  https://www.deeplearningbook.org/contents/convnets.html    and statistical efficiency", "ebcada12-a13f-40e8-87c2-b94dd9426dea": "Note that this does not represent a probabilistic graphical model, because the nodes are not separate variables but rather states of a single variable, and so we have shown the states as boxes rather than circles. It is sometimes useful to take a state transition diagram, of the kind shown in shown for the case of the hidden Markov model in Figure 13.7. The speci\ufb01cation of the probabilistic model is completed by de\ufb01ning the conditional distributions of the observed variables p(xn|zn, \u03c6), where \u03c6 is a set of parameters governing the distribution.\n\nThese are known as emission probabilities, and might for example be given by Gaussians of the form (9.11) if the elements of x are continuous variables, or by conditional probability tables if x is discrete. Because xn is observed, the distribution p(xn|zn, \u03c6) consists, for a given value of \u03c6, of a vector of K numbers corresponding to the K possible states of the binary vector zn. gram of Figure 13.6 over time, we obtain a lattice, or trellis, representation of the latent states. Each column of this diagram corresponds to one of the latent variables zn", "3b1ef0cb-0542-4a04-afdb-a75266ae7acf": "Each core reads parameters without a lock, then computes a gradient, then increments the parameters without a lock.\n\nThis reduces the average amount of improvement that each gradient descent step yields, because some of the cores overwrite each other\u2019s progress, but the increased rate of production of steps causes the learning process to be faster overall. Dean et al. pioneered the multimachine implementation of this lock-free approach  https://www.deeplearningbook.org/contents/applications.html    0 gt radient descent, where the parameters are managed by a parameter server rat er than stored i in shared memory. Distributed asynchronous gradient descent remains the primary strategy for training large deep networks and is used by most major deep learning groups in industry . Academic deep learning researchers typically cannot afford the same scale of distributed learning systems, but some research has focused on how to build distributed networks with relatively low-cost hardware available in the university setting . 12.1.4 Model Compression  In many commercial applications, it is much more important that the time and memory cost of running inference in a machine learning model be low than that the time and memory cost of training be low. For applications that do not require  442  CHAPTER 12", "034d7f34-ff17-4daa-9737-676d4ee1bc3f": "Like score matching and pseudolikelihood, NCE does not work if only a lower bound on P is available.\n\nSuch a lower bound could be used to construct a lower bound on pyoint(y = 1 | x), but it can only be used to construct an upper bound on  Pjoint(Y = 9 | x), which appears in half the terms of the NCE objective. Likewise, a lower bound on pyoise is not useful, because it provides only an upper bound on Djoint(Y =1 | x). When the model distribution is copied to define a new noise distribution before each gradient step, NCE defines a procedure called self-contrastive estimation,  whose expected gradient is equivalent to the expected gradient of maximum likelihood . The special case of NCE where the noise samples are those generated by the model suggests that maximum likelihood can be interpreted as a procedure that forces a model to constantly learn to distinguish reality from its own evolving beliefs, while noise contrastive estimation achieves some reduced computational cost by only forcing the model to distinguish reality from a fixed baseline (the noise model). 620  CHAPTER 18", "2ba03036-c299-4e9e-a23b-b41f1494b3d0": "However, if we wish to develop a learning process based on maximizing L(v,h,q), then it is helpful to think of MAP inference as a procedure that provides a value of qg. In this sense, we can think of MAP inference as approximate inference, because it does not provide the optimal gq.\n\nRecall from section 19.1 that exact inference consists of maximizing  L(v,0,q) = Eng llog p(h, \u00bb)] + H(q) (19.10)  with respect to qg over an unrestricted family of probability distributions, using an exact optimization algorithm. We can derive MAP inference as a form of approximate inference by restricting the family of distributions gq may be drawn from. Specifically, we require q to take on a Dirac distribution:  g(h | v) = 5(h~ p). (19.11)  https://www.deeplearningbook.org/contents/inference.html    This means that we can now control \u00a2 entirely via #", "93fa2c98-e927-4760-a1fb-16323a5ab896": "This is illustrated schematically for a single continuous variable in Figure 11.1. We shall suppose that such expectations are too complex to be evaluated exactly using analytical techniques. The general idea behind sampling methods is to obtain a set of samples z(l) (where l = 1, . , L) drawn independently from the distribution p(z).\n\nThis allows the expectation (11.1) to be approximated by a \ufb01nite sum As long as the samples z(l) are drawn from the distribution p(z), then E = E and so the estimator \ufffdf has the correct mean. The variance of the estimator is given by Exercise 11.1 is the variance of the function f(z) under the distribution p(z). It is worth emphasizing that the accuracy of the estimator therefore does not depend on the dimensionality of z, and that, in principle, high accuracy may be achievable with a relatively small number of samples z(l). In practice, ten or twenty independent samples may suf\ufb01ce to estimate an expectation to suf\ufb01cient accuracy. The problem, however, is that the samples {z(l)} might not be independent, and so the effective sample size might be much smaller than the apparent sample size", "89270794-84ee-4671-a53c-15f202aa48ff": "This exponential increase is the result of discounting in the TD model learning rule. With the presence representation (Figure 14.4 middle), the US prediction is nearly constant while the stimulus is present because there is only one weight, or associative strength, to be learned for each stimulus. Consequently, the TD model with the presence representation cannot recreate many features of CR timing. With an MS representation (Figure 14.4 right), the development of the TD model\u2019s US prediction is more complicated. After 200 trials the prediction\u2019s pro\ufb01le is a reasonable approximation of the US prediction curve produced with the CSC representation.\n\nThe US prediction curves shown in Figure 14.4 were not intended to precisely match pro\ufb01les of CRs as they develop during conditioning in any particular animal experiment, but they illustrate the strong in\ufb02uence that the stimulus representation has on predictions derived from the TD model. Further, although we can only mention it here, how the stimulus representation interacts with discounting and eligibility traces is important in determining properties of the US prediction pro\ufb01les produced by the TD model. Another dimension beyond what we can discuss here is the in\ufb02uence of di\u21b5erent responsegeneration mechanisms that translate US predictions into CR pro\ufb01les; the pro\ufb01les shown in Figure 14.4 are \u201craw\u201d US prediction pro\ufb01les", "355c99fc-d409-44b2-a2ac-86690423bed2": "Training can therefore be stopped at the point of smallest error with respect to the validation data set, as indicated in Figure 5.12, in order to obtain a network having good generalization performance.\n\nThe behaviour of the network in this case is sometimes explained qualitatively in terms of the effective number of degrees of freedom in the network, in which this number starts out small and then to grows during the training process, corresponding to a steady increase in the effective complexity of the model. Halting training before biases in a two-layer network having a single input, a single linear output, and 12 hidden units having \u2018tanh\u2019 activation functions. The priors are governed by four hyperparameters \u03b1b the precisions of the Gaussian distributions of the \ufb01rst-layer biases, \ufb01rst-layer weights, second-layer biases, and second-layer weights, respectively. We see that the parameter \u03b1w 2 governs the vertical scale of functions (note the different vertical axis ranges on the top two diagrams), \u03b1w 1 governs the horizontal scale of variations in the 1 governs the horizontal range over which variations occur. The parameter \u03b1b effect is not illustrated here, governs the range of vertical offsets of the functions", "f59e8bcb-8778-4dbf-abff-ada69a6821dd": "2071\u20132074. IEEE. Neal, R. M. Probabilistic inference using Markov chain Monte Carlo methods.\n\nTechnical Report CRG-TR-93-1, Department of Computer Science, University of Toronto, Canada. Neal, R. M. Monte Carlo implementation of Gaussian process models for Bayesian regression and classi\ufb01cation. Technical Report 9702, Department of Computer Statistics, University of Toronto. Neal, R. M. Suppressing random walks in Markov chain Monte Carlo using ordered overrelaxation. In M. I. Jordan (Ed. ), Learning in Graphical Models, pp. 205\u2013228. MIT Press. Neal, R. M. and G. E. Hinton . A new view of the EM algorithm that justi\ufb01es incremental and other variants. In M. I. Jordan (Ed. ), Learning in Graphical Models, pp. 355\u2013368. MIT Press", "1961e94d-e177-4635-8762-c75a16619c10": "But suppose we are generous and grant the broad outlines of everything that we have done in the book and everything that has been outlined so far in this chapter. What would remain after that? Of course we can\u2019t know for sure what will be required, but we can make some guesses. In this section we highlight six further issues which it seems to us will still need to be addressed by future research. First, we still need powerful parametric function approximation methods that work well in fully incremental and online settings. Methods based on deep learning and ANNs are a major step in this direction but, still, only work well with batch training on large data sets, with training from extensive o\u270fine self play, or with learning from the interleaved experience of multiple agents on the same task.\n\nThese and other settings are ways of working around a basic limitation of today\u2019s deep learning methods, which struggle to learn rapidly in the incremental, online settings that are most natural for the reinforcement learning algorithms emphasized in this book. The problem is sometimes described as one of \u201ccatastrophic interference\u201d or \u201ccorrelated data.\u201d When something new is learned it tends to replace what has previously been learned rather than adding to it, with the result that the bene\ufb01t of the older learning is lost", "d0f821a6-429a-46a5-b661-c394ede5f1fb": "Another way of seeing this close relationship is to compare the backup diagrams for these algorithms on page 59 (policy evaluation) and on the left of Figure 3.4 (value iteration). These two are the natural backup operations for computing v\u21e1 and v\u21e4. Finally, let us consider how value iteration terminates. Like policy evaluation, value iteration formally requires an in\ufb01nite number of iterations to converge exactly to v\u21e4. In practice, we stop once the value function changes by only a small amount in a sweep.\n\nThe box below shows a complete algorithm with this kind of termination condition. Algorithm parameter: a small threshold \u2713 > 0 determining accuracy of estimation Initialize V (s), for all s 2 S+, arbitrarily except that V (terminal) = 0 Value iteration e\u21b5ectively combines, in each of its sweeps, one sweep of policy evaluation and one sweep of policy improvement. Faster convergence is often achieved by interposing multiple policy evaluation sweeps between each policy improvement sweep", "dd43e069-00d7-4eb1-9fca-28bba71a5304": "We further demonstrate the sensitive of MLE+PG and SQL w.r.t the reward scale as a key hyperparameter. Figure A.2 (middle and right) shows the training curves of the two methods with varying reward scales. We can see SQL is signi\ufb01cantly more robust as reward scale changes, while MLE+PG tends to collapse with improper reward scale con\ufb01gurations. and otherwise same with the reward function used in training. We use a transformer model  based on Texar-Pytorch  by default, with 64 hidden dimension, 3 blocks, and 4 heads.\n\nFor experiments that involve policy gradient training, we initialize the model with maximum likelihood training by default unless speci\ufb01ed otherwise. We train soft Q-learning model from scratch with both off-policy (using data) and on-policy (using samples) by default except in \u00a74.1 and \u00a74.3, in which we \ufb01nd it bene\ufb01cial to warm-up the model with just off-policy training. We apply similar tuning budgets to both soft Q-learning model, and policy-gradient (mostly the reward scale and top-k), based on performance on the validation dataset and sample qualities. Most of the experiments are conducted using Nvidia 1080 or 2080 series GPUs with around 12GB memory", "aae812a4-0a91-48b1-83f0-c21aa752da99": "9.7 with Ut .= P \u03b3\u02c6v(s0,wt)]) with updates according to the on-policy distribution will also converge to the TD \ufb01xed point.\n\nOne-step semi-gradient action-value methods, such as semi-gradient Sarsa(0) covered in the next chapter converge to an analogous \ufb01xed point and an analogous bound. For episodic tasks, there is a slightly di\u21b5erent but related bound . There are also a few technical conditions on the rewards, features, and decrease in the step-size parameter, which we have omitted here. The full details can be found in the original paper . Critical to the these convergence results is that states are updated according to the on-policy distribution. For other update distributions, bootstrapping methods using function approximation may actually diverge to in\ufb01nity. Examples of this and a discussion of possible solution methods are given in Chapter 11. Example 9.2: Bootstrapping on the 1000-state Random Walk State aggregation is a special case of linear function approximation, so let\u2019s return to the 1000-state random walk to illustrate some of the observations made in this chapter", "6342e3d6-b0ba-4ba8-a96b-e142f3a8815a": "In the 1960s the terms \u201creinforcement\u201d and \u201creinforcement learning\u201d were used in the engineering literature for the \ufb01rst time to describe engineering uses of trial-and-error learning .\n\nParticularly in\ufb02uential was Minsky\u2019s paper \u201cSteps Toward Arti\ufb01cial Intelligence\u201d , which discussed several issues relevant to trial-and-error learning, including prediction, expectation, and what he called the basic credit-assignment problem for complex reinforcement learning systems: How do you distribute credit for success among the many decisions that may have been involved in producing it? All of the methods we discuss in this book are, in a sense, directed toward solving this problem. Minsky\u2019s paper is well worth reading today. In the next few paragraphs we discuss some of the other exceptions and partial exceptions to the relative neglect of computational and theoretical study of genuine trial-and-error learning in the 1960s and 1970s. One exception was the work of the New Zealand researcher John Andreae, who developed a system called STeLLA that learned by trial and error in interaction with its environment. This system included an internal model of the world and, later, an \u201cinternal monologue\u201d to deal with problems of hidden state (Andreae, 1963, 1969a,b)", "39aee48f-4557-4e6b-b7c1-05c364566079": "We number the time steps of each episode starting anew from zero. Therefore, we have to refer not just to St, the state representation at time t, but to St,i, the state representation at time t of episode i (and similarly for At,i, Rt,i, \u21e1t,i, Ti, etc.). However, it turns out that when we discuss episodic tasks we almost never have to distinguish between di\u21b5erent episodes.\n\nWe are almost always considering a particular single episode, or stating something that is true for all episodes. Accordingly, in practice we almost always abuse notation slightly by dropping the explicit reference to episode number. That is, we write St to refer to St,i, and so on. We need one other convention to obtain a single notation that covers both episodic and continuing tasks. We have de\ufb01ned the return as a sum over a \ufb01nite number of terms in one case (3.7) and as a sum over an in\ufb01nite number of terms in the other (3.8). These two can be uni\ufb01ed by considering episode termination to be the entering of a special absorbing state that transitions only to itself and that generates only rewards of zero", "002ee87b-a612-494c-b162-5c4287e87905": "If we take an unweighted majority vote to resolve con\ufb02icts, we end up with null (tie-vote) labels. If we could correctly estimate the source accuracies, we would resolve con\ufb02icts in the direction of Source 1. We would still need to pass this information on to the end modelbeingtrained.SupposethatwetooklabelsfromSource 1 where available, and otherwise took labels from Source 2. Then, the expected training set accuracy would be 60.3%\u2014 only marginally better than the weaker source. Instead we should represent training label lineage in end model training, weighting labels generated by high-accuracy sources more.\n\nIn recent work, we developed data programming as a paradigm for addressing both of these challenges by modeling multiple label sources without access to ground truth, and generating probabilistic training labels representing the lineage of the individual labels. We prove that, surprisingly, we can recover source accuracy and correlation structure without hand-labeled training data . However, there are many Fig. 2 In Snorkel, rather than labeling training data by hand, users write labeling functions, which programmatically label data points or abstain. These labeling functions will have different unknown accuracies and correlations", "0dd9b1b7-d1bf-4e08-b74c-671b27363e3f": "By causing estimates to be more accurate sooner, sample updates will have a second advantage in that the values backed up from the successor states will be more accurate. These results suggest that sample updates are likely to be superior to expected updates on problems with large stochastic branching factors and too many states to be solved exactly. equally likely to occur. Suppose instead that the distribution was highly skewed, that some of the b states were much more likely to occur than most. Would this strengthen or weaken the case for sample updates over expected updates? Support your answer. \u21e4 In this section we compare two ways of distributing updates.\n\nThe classical approach, from dynamic programming, is to perform sweeps through the entire state (or state\u2013action) space, updating each state (or state\u2013action pair) once per sweep. This is problematic on large tasks because there may not be time to complete even one sweep. In many tasks the vast majority of the states are irrelevant because they are visited only under very poor policies or with very low probability. Exhaustive sweeps implicitly devote equal time to all parts of the state space rather than focusing where it is needed. As we discussed in Chapter 4, exhaustive sweeps and the equal treatment of all states that they imply are not necessary properties of dynamic programming", "e7d30ac9-208f-4c36-80f4-1b362b468aed": "The counterexample is formed by extending the w-to-2w example (from earlier in this section) with a terminal state, as shown to the right.\n\nAs before, the estimated value of the \ufb01rst state is w, and the estimated value of the second state is 2w. The reward is zero on all transitions, so the true values are zero at both states, which is exactly representable with w = 0. If we set wk+1 at each step so as to minimize the VE between the estimated value and the expected one-step return, then we have Another way to try to prevent instability is to use special methods for function approximation. In particular, stability is guaranteed for function approximation methods that do not extrapolate from the observed targets. These methods, called averagers, include nearest neighbor methods and locally weighted regression, but not popular methods such as tile coding and arti\ufb01cial neural networks (ANNs)", "4ab0ce12-3521-45d6-be9f-5b34fa70b5a9": "One advantage of a probabilistic latent variable formulation is that it helps to motivate and formulate generalizations of basic ICA. For instance, independent factor analysis  considers a model in which the number of latent and observed variables can differ, the observed variables are noisy, and the individual latent variables have flexible distributions modelled by mixtures of Gaussians. The log likelihood for this model is maximized using EM, and the reconstruction of the latent variables is approximated using a variational approach. Many other types of model have been considered, and there is now a huge literature on ICA and its applications . In Chapter 5 we considered neural networks in the context of supervised learning, where the role of the network is to predict the output variables given values for the input variables. However, neural networks have also been applied to unsupervised learning where they have been used for dimensionality reduction. This is achieved by using a network having the same number of outputs as inputs, and optimizing the weights so as to minimize some measure of the reconstruction error between inputs and outputs with respect to a set of training data.\n\nConsider first a multilayer perceptron of the form shown in Figure 12.18, having D inputs, D output units and M hidden units, with M < D", "5748ebb6-f5a2-4d4a-991a-6fa9c15b3422": "If we wish to back-propagate through a sampling process that produces discrete-valued samples, it may still be possible to estimate a gradient on w, using reinforcement learning algorithms such as variants of the REINFORCE algorithm , discussed in section 20.9.1.\n\nIn neural network applications, we typically choose z to be drawn from some simple distribution, such as a unit uniform or unit Gaussian distribution, and achieve more complex distributions by allowing the deterministic portion of the network to reshape its input. The idea of propagating gradients or optimizing through stochastic operations dates back to the mid-twentieth century  and was first used for machine learning in the context of reinforcement learning . More recently, it has been applied to variational approximations  and stochastic and generative neural networks . Many networks, such as denoising autoencoders or networks regularized with dropout, are also naturally designed to take noise  685  CHAPTER 20. DEEP GENERATIVE MODELS  as an input without requiring any special reparametrization to make the noise independent from the model", "dfe7e2c7-bb90-4849-a260-05870549c6b7": "6.3 (\u22c6) The nearest-neighbour classi\ufb01er (Section 2.5.2) assigns a new input vector x to the same class as that of the nearest input vector xn from the training set, where in the simplest case, the distance is de\ufb01ned by the Euclidean metric \u2225x \u2212 xn\u22252. By expressing this rule in terms of scalar products and then making use of kernel substitution, formulate the nearest-neighbour classi\ufb01er for a general nonlinear kernel. 6.4 (\u22c6) In Appendix C, we give an example of a matrix that has positive elements but that has a negative eigenvalue and hence that is not positive de\ufb01nite. Find an example of the converse property, namely a 2 \u00d7 2 matrix with positive eigenvalues yet that has at least one negative element.\n\n6.6 (\u22c6) Verify the results (6.15) and (6.16) for constructing valid kernels. 6.8 (\u22c6) Verify the results (6.19) and (6.20) for constructing valid kernels. 6.9 (\u22c6) Verify the results (6.21) and (6.22) for constructing valid kernels", "d9feb58f-766c-4f90-9cfe-92d432aea4e7": "They found that SML results in the best test set log-likelihood for an RBM, and that if the RBM\u2019s hidden units are used as features for an SVM classifier, SML results in the best classification accuracy. SML is vulnerable to becoming inaccurate if the stochastic gradient algorithm can move the model faster than the Markov chain can mix between steps. This can happen if kis too small or \u20ac is too large. The permissible range of values is unfortunately highly problem dependent. There is no known way to test formally whether the chain is successfully mixing between steps. Subjectively, if the learning rate is too high for the number of Gibbs steps, the human operator will be able to observe much more variance in the negative phase samples across gradient steps than across different Markov chains.\n\nFor example, a model trained on MNIST might sample exclusively 7s on one step. The learning process will then push down strongly on the mode corresponding to 7s, and the model might sample exclusively 9s on the next step. CHAPTER 18. CONFRONTING THE PARTITION FUNCTION  Care must be taken when evaluating the samples from a model trained with SML", "a5886e2c-57f9-4192-9cd4-5c7fb0e88cd4": ", ST , occurring under any policy \u21e1 is = \u21e1(At|St)p(St+1|St, At)\u21e1(At+1|St+1) \u00b7 \u00b7 \u00b7 p(ST |ST \u22121, AT \u22121) where p here is the state-transition probability function de\ufb01ned by (3.4). Thus, the relative probability of the trajectory under the target and behavior policies (the importancesampling ratio) is Although the trajectory probabilities depend on the MDP\u2019s transition probabilities, which are generally unknown, they appear identically in both the numerator and denominator, and thus cancel. The importance sampling ratio ends up depending only on the two policies and the sequence, not on the MDP. Recall that we wish to estimate the expected returns (values) under the target policy, but all we have are returns Gt due to the behavior policy.\n\nThese returns have the wrong expectation E = vb(s) and so cannot be averaged to obtain v\u21e1. This is where importance sampling comes in", "a1f4e960-6fd3-44aa-87a0-1d45f66e952d": "The results are consistent with our observations on ImageNet, although the largest batch size of 4096 seems to cause a small degradation in performance on CIFAR-10. 15It is worth noting that, although CIFAR-10 images are much smaller than ImageNet images and image size does not differ among examples, cropping with resizing is still a very effective augmentation for contrastive learning. A Simple Framework for Contrastive Learning of Visual Representations Optimal temperature under different batch sizes Figure B.8 shows the linear evaluation of model trained with three different temperatures under various batch sizes.\n\nWe \ufb01nd that when training to convergence (e.g. training epochs > 300), the optimal temperature in {0.1, 0.5, 1.0} is 0.5 and seems consistent regardless of the batch sizes. However, the performance with \u03c4 = 0.1 improves as batch size increases, which may suggest a small shift of optimal temperature towards 0.1. B.10. Tuning For Other Loss Functions The learning rate that works best for NT-Xent loss may not be a good learning rate for other loss functions. To ensure a fair comparison, we also tune hyperparameters for both margin loss and logistic loss", "2055745c-4405-4ef9-b56b-d1fd376cc868": "Regression functions, such as this, which make predictions by taking linear combinations of the training set target values are known as linear smoothers. Note that the equivalent kernel depends on the input values xn from the data set because these appear in the de\ufb01nition of SN.\n\nThe equivalent kernel is illustrated for the case of Gaussian basis functions in Figure 3.10 in which the kernel functions k(x, x\u2032) have been plotted as a function of x\u2032 for three different values of x. We see that they are localized around x, and so the mean of the predictive distribution at x, given by y(x, mN), is obtained by forming a weighted combination of the target values in which data points close to x are given higher weight than points further removed from x. Intuitively, it seems reasonable that we should weight local evidence more strongly than distant evidence. Note that this localization property holds not only for the localized Gaussian basis functions but also for the nonlocal polynomial and sigmoidal basis functions, as illustrated in Figure 3.11", "fdaf0071-de66-4bc7-a3d9-deb11b8c6052": "Wiering and M. van Otterlo (Eds. ), Reinforcement Learning: State-of-the-Art, pp. 111\u2013141. Springer-Verlag Berlin Heidelberg. Hesterberg, T. C. , Advances in Importance Sampling, Ph.D. thesis, Statistics Department, Hilgard, E. R. Theories of Learning, Second Edition. Appleton-Century-Cofts, Inc., Hilgard, E. R., Bower, G. H. Theories of Learning. Prentice-Hall, Englewood Cli\u21b5s, NJ. Hinton, G. E. Distributed representations. Technical Report CMU-CS-84-157. Department of Computer Science, Carnegie-Mellon University, Pittsburgh, PA. Hochreiter, S., Schmidhuber, J. .\n\nLTSM can solve hard time lag problems. In Advances in Holland, J. H", "b217f3ec-b39d-477e-9adc-cb5bee0b3b71": "We report the results of both methods. Comparison Methods. We compare our approach with a variety of previous methods that were designed for speci\ufb01c manipulation schemes: (1) For text data augmentation, we compare with the latest model-based augmentation  which uses a \ufb01xed conditional BERT language model for word substitution (section 4.2). As with base models, we also tried \ufb01tting the augmentatin model to both the training data and the joint training-validation data, and did not observe signi\ufb01cant difference. Following , we also study a conventional approach that replaces words with their synonyms using WordNet . (2) For data weighting, we compare with the state-of-the-art approach  that dynamically re-estimates sample weights in each iteration based on the validation set gradient directions. We follow  and also evaluate the commonly-used proportion method that weights data by inverse class frequency. Training. For both the BERT classi\ufb01er and the augmentation model (which is also based on BERT), we use Adam optimization with an initial learning rate of 4e-5. For ResNets, we use SGD optimization with a learning rate of 1e-3", "a82a37d5-cf79-4f9e-a817-18378b8efdce": "The key di\u21b5erence between instrumental and classical conditioning experiments is that in the former the reinforcing stimulus is contingent upon the animal\u2019s behavior, whereas in the latter it is not.\n\nLearning to predict via a TD algorithm corresponds to classical conditioning, and we described the TD model of classical conditioning as one instance in which reinforcement learning principles account for some details of animal learning behavior. This model generalizes the in\ufb02uential Rescorla\u2013Wagner model by including the temporal dimension where events within individual trials in\ufb02uence learning, and it provides an account of second-order conditioning, where predictors of reinforcing stimuli become reinforcing themselves. It also is the basis of an in\ufb02uential view of the activity of dopamine neurons in the brain, something we take up in Chapter 15. Learning by trial and error is at the base of the control aspect of reinforcement learning. We presented some details about Thorndike\u2019s experiments with cats and other animals that led to his Law of E\u21b5ect, which we discussed here and in Chapter 1 (page 15). We pointed out that in reinforcement learning, exploration does not have to be limited to \u201cblind groping\u201d; trials can be generated by sophisticated methods using innate and previously learned knowledge as long as there is some exploration. We discussed the training method B", "ed9e0420-ffab-4e1b-ae23-64d1f96678f6": "The purpose of this essay is to shed light on the importance of a public referendum, on a question of whether the decision of an EU member states to remain in the European Union is constitutional and thus in accord with constitutional guarantees of sovereignty computers: macintoshintoshintoshintosh This essay discusses hardware devices and software systems for Mac OS X, MacOS X and Linux. To view the latest version of Macintosh OS: Mac 8.7.x\\n\\n For more information or for information about Macintosh systems, visit Mac MacSystems.\\n More space: legal space science and space This essay discusses science for teens, adults and teenagers.\\n\\n When the idea of studying space was \ufb01rst implemented as a method to test, the question was: What if a student has been \"comfortable\" with space without its body?\n\nWhat would their body like to be religion: space legal religion religion religion This essay discusses an alternative religion that focuses on the role of a particular religion and views some form of religious ethics as the form when the law is applied to that particular religious community", "edb2d0e3-0e34-4e8f-97f0-fb4fc08c50fb": "Table 5 shows that without normalization and proper temperature scaling, performance is signi\ufb01cantly worse. Without \u21132 normalization, the contrastive task accuracy is higher, but the resulting representation is worse under linear evaluation. 5.2. Contrastive learning bene\ufb01ts (more) from larger batch sizes and longer training 10A linear learning rate scaling is used here. Figure B.1 shows using a square root learning rate scaling can improve performance of ones with small batch sizes.\n\nsupervised learning , in contrastive learning, larger batch sizes provide more negative examples, facilitating convergence (i.e. taking fewer epochs and steps for a given accuracy). Training longer also provides more negative examples, improving the results. In Appendix B.1, results with even longer training steps are provided. In this subsection, similar to Kolesnikov et al. ; He et al. , we use ResNet-50 in 3 different hidden layer widths (width multipliers of 1\u00d7, 2\u00d7, and 4\u00d7). For better convergence, our models here are trained for 1000 epochs. Linear evaluation", "809f632b-e45b-4a9a-bfca-e415d7b4d96e": "If we have two classes, class 0 and class 1, then we need only specify the probability of one of these classes. The probability of class 1 determines the probability of class 0, because these two values must add up to 1. The normal distribution over real-valued numbers that we used for linear regression is parametrized in terms of a mean. Any value we supply for this mean is valid. A distribution over a binary variable is slightly more complicated, because its mean must always be between 0 and 1. One way to solve this problem is to use the logistic sigmoid function to squash the output of the linear function into the  Vin aN  https://www.deeplearningbook.org/contents/ml.html    interval (U, 1) and interpret that value as a probability: T Ply =1| 2:6) =o(8 2). (5.81)  This approach is known as logistic regression (a somewhat strange name since we use the model for classification rather than regression). In the case of linear regression, we were able to find the optimal weights by solving the normal equations. Logistic regression is somewhat more difficult. There is no closed-form solution for its optimal weights", "931faf0d-9493-41cb-84b6-ddd842fd4e9b": "Figure 5.9 shows an example of the effect of different values of M for the sinusoidal regression problem. The generalization error, however, is not a simple function of M due to the presence of local minima in the error function, as illustrated in Figure 5.10.\n\nHere we see the effect of choosing multiple random initializations for the weight vector for a range of values of M. The overall best validation set performance in this case occurred for a particular solution having M = 8. In practice, one approach to choosing M is in fact to plot a graph of the kind shown in Figure 5.10 and then to choose the speci\ufb01c solution having the smallest validation set error. There are, however, other ways to control the complexity of a neural network model in order to avoid over-\ufb01tting. From our discussion of polynomial curve \ufb01tting in Chapter 1, we see that an alternative approach is to choose a relatively large value for M and then to control complexity by the addition of a regularization term to the error function. The simplest regularizer is the quadratic, giving a regularized error graphs show the result of \ufb01tting networks having M = 1, 3 and 10 hidden units, respectively, by minimizing a sum-of-squares error function using a scaled conjugate-gradient algorithm", "616b3a7d-614d-455a-89aa-5a7be37ce552": "We assume that its true value function, v\u21e1, is too complex to be represented exactly as an approximation. Thus v\u21e1 is not in the subspace; in the \ufb01gure it is depicted as being above the planar subspace of representable functions. If v\u21e1 cannot be represented exactly, what representable value function is closest to it? This turns out to be a subtle question with multiple answers. To begin, we need a measure of the distance between two value functions.\n\nGiven two value functions v1 and v2, we can talk about the vector di\u21b5erence between them, v = v1 \u2212 v2. If v is small, then the two value functions are close to each other. But how are we to measure the size of this di\u21b5erence vector? The conventional Euclidean norm is not appropriate because, as discussed in Section 9.2, some states are more important than others because they occur more frequently or because we are more interested in them (Section 9.11). As in Section 9.2, let us use the distribution \u00b5 : S ! to specify the degree to which we care about di\u21b5erent states being accurately valued (often taken to be the on-policy distribution)", "eec7921e-73b9-4f92-9796-b6007b0ba480": "Because the distribution q is determined using the old parameter values rather than the new values and is held \ufb01xed during the M step, it will not equal the new posterior distribution p(Z|X, \u03b8new), and hence there will be a nonzero KL divergence. The increase in the log likelihood function is therefore greater than the increase in the lower bound, as shown in Figure 9.13. If we substitute q(Z) = p(Z|X, \u03b8old) into (9.71), we see that, after the E step, the lower bound takes the form where the constant is simply the negative entropy of the q distribution and is therefore independent of \u03b8. Thus in the M step, the quantity that is being maximized is the expectation of the complete-data log likelihood, as we saw earlier in the case of mixtures of Gaussians. Note that the variable \u03b8 over which we are optimizing appears only inside the logarithm", "d040a0d7-71fd-4a30-a176-02ec4a5dc562": "In particular, we see that p(\u00b5|\u03bb) is a Gaussian whose precision is a linear function of \u03bb and that p(\u03bb) is a gamma distribution, so that the normalized prior takes the form where we have de\ufb01ned new constants given by \u00b50 = c/\u03b2, a = 1 + \u03b2/2, b = d\u2212c2/2\u03b2. The distribution (2.154) is called the normal-gamma or Gaussian-gamma distribution and is plotted in Figure 2.14. Note that this is not simply the product of an independent Gaussian prior over \u00b5 and a gamma prior over \u03bb, because the precision of \u00b5 is a linear function of \u03bb. Even if we chose a prior in which \u00b5 and \u03bb were independent, the posterior distribution would exhibit a coupling between the precision of \u00b5 and the value of \u03bb.\n\nIn the case of the multivariate Gaussian distribution N \ufffd dimensional variable x, the conjugate prior distribution for the mean \u00b5, assuming the precision is known, is again a Gaussian", "9dfa45e8-b08d-4b7d-b457-4420c00fae74": "2 A taxonomy of image data augmentations covered; the colored lines in the figure depict which data augmentation method the corresponding meta-learning scheme uses, for example, meta-learning using Neural Style Transfer is covered in neural augmentation   test-time augmentation, curriculum learning, and the impact of resolution are covered in this survey under the \u201cDesign considerations for image Data Augmentation\u201d section. Descriptions of individual augmentation techniques will be enumerated in the \u201cImage Data Augmentation techniques\u201d section. A quick taxonomy of the Data Augmentations is depicted below in Fig. 2. Before discussing image augmentation techniques, it is useful to frame the context of the problem and consider what makes image recognition such a difficult task in the first place. In classic discriminative examples such as cat versus dog, the image recognition software must overcome issues of viewpoint, lighting, occlusion, background, scale, and more.\n\nThe task of Data Augmentation is to bake these translational invariances into the dataset such that the resulting models will perform well despite these challenges. It is a generally accepted notion that bigger datasets result in better Deep Learning models", "64dc995f-8df9-4efb-8588-96ce6f4f3426": "Although the introduction of regularization terms can control over-\ufb01tting for models with many parameters, this raises the question of how to determine a suitable value for the regularization coef\ufb01cient \u03bb. Seeking the solution that minimizes the regularized error function with respect to both the weight vector w and the regularization coef\ufb01cient \u03bb is clearly not the right approach since this leads to the unregularized solution with \u03bb = 0. As we have seen in earlier chapters, the phenomenon of over-\ufb01tting is really an unfortunate property of maximum likelihood and does not arise when we marginalize over parameters in a Bayesian setting.\n\nIn this chapter, we shall consider the Bayesian view of model complexity in some depth. Before doing so, however, it is instructive to consider a frequentist viewpoint of the model complexity issue, known as the biasvariance trade-off. Although we shall introduce this concept in the context of linear basis function models, where it is easy to illustrate the ideas using simple examples, the discussion has more general applicability. In Section 1.5.5, when we discussed decision theory for regression problems, we considered various loss functions each of which leads to a corresponding optimal prediction once we are given the conditional distribution p(t|x)", "c2070454-6a62-4eb6-98f4-d93d9af13265": "Any consistent estimator will, given enough capacity, make Pmodel into a set of Dirac distributions centered on the training points.\n\nSmoothing by q helps to reduce this problem, at the loss of the asymptotic consistency property described in section 5.4.5. Kingma and LeCun  introduced a procedure for performing regularized score matching with the smoothing distribution q being normally distributed noise. Recall from section 14.5.1 that several autoencoder training algorithms are equivalent to score matching or denoising score matching. These autoencoder training algorithms are therefore a way of overcoming the partition function problem. 18.6 Noise-Contrastive Estimation  Most techniques for estimating models with intractable partition functions do not provide an estimate of the partition function. SML and CD estimate only the gradient of the log partition function, rather than the partition function itself. Score matching and pseudolikelihood avoid computing quantities related to the partition function altogether. Noise-contrastive estimation (NCE)  takes a different strategy", "3ad59d0c-358f-4377-b303-9041817501e2": "Next, for each row k of the Jacobian matrix, corresponding to the output unit k, backpropagate using the recursive relation (5.74), starting with (5.75) or (5.76), for all of the hidden units in the network. Finally, use (5.73) to do the backpropagation to the inputs. The Jacobian can also be evaluated using an alternative forward propagation formalism, which can be derived in an analogous way to the backpropagation approach given here. Exercise 5.15 Again, the implementation of such algorithms can be checked by using numerical differentiation in the form which involves 2D forward propagations for a network having D inputs. We have shown how the technique of backpropagation can be used to obtain the \ufb01rst derivatives of an error function with respect to the weights in the network. Backpropagation can also be used to evaluate the second derivatives of the error, given by \u22022E Note that it is sometimes convenient to consider all of the weight and bias parameters as elements wi of a single vector, denoted w, in which case the second derivatives form the elements Hij of the Hessian matrix H, where i, j \u2208 {1,", "1bb863f2-14dc-41f2-9e23-a44ec64de89c": "The VAE framework is straightforward to extend to a wide range of model architectures. This is a key advantage over Boltzmann machines, which require extremely careful model design to maintain tractability. VAEs work very well with a diverse family of differentiable operators. One particularly sophisticated VAE is the deep recurrent attention writer (DRAW) model .\n\nDRAW uses a recurrent encoder and recurrent decoder combined with an attention mechanism. The generation process for the DRAW model consists of sequentially visiting different small image patches and drawing the values of the pixels at those points. VAEs can also be extended to generate sequences by defining variational RNNs  by using a recurrent encoder and decoder within the VAE framework. Generating a sample from a traditional RNN involves only nondeterministic operations at the output space. Variational RNNs also have random variability at the potentially more abstract level captured by  694  CHAPTER 20. DEEP GENERATIVE MODELS  the VAE latent variables", "a80133e3-11a4-4134-9574-2b9bb86adc09": "With a large list of potential augmentations and a mostly continuous space of magnitudes, it is easy to conceptualize the enormous size of the augmentation search space. Combining  augmentations such as cropping, flipping, color shifts, and random erasing can result in Shorten and Khoshgoftaar J Big Data  6:60   e-4-4-0) decoder  = \u00a9@ sel  \u2014\u2014 Sequence Classifier  Static Classifier  Fig. 13 Architecture diagram of the feature space augmentation framework presented by DeVries and Taylor (75]  Fig. 14 Examples of interpolated instances in the feature space on the handwritten \u2018@' character   massively inflated dataset sizes. However, this is not guaranteed to be advantageous. In domains with very limited data, this could result in further overfitting.\n\nTherefore, it is important to consider search algorithms for deriving an optimal subset of augmented data to train Deep Learning models with. More on this topic will be discussed in Design  Considerations of Data Augmentation. Data Augmentations based on Deep Learning Feature space augmentation All of the augmentation methods discussed above are applied to images in the input space", "978c93e6-3d7e-4045-af2e-ac694a0af3a9": "In Table 1, however, we do count such instances as improvements over majority vote, as these instances have an effect on the training of the end discriminative model (they yield additional training labels).\n\nthe modeling advantage grows as the number of labeling functions increases, and that our optimizer approximation closely tracks it; thus, the optimizer can save execution time by choosing to skip the generative model and run majority vote instead during the initial cycles of iterative development. Inthissubsection,weconsidermodelingadditionalstatistical structure beyond the independent model. We study the tradeoff between predictive performance and computational cost, and describe how to automatically select a good point in this trade-off space. Structure Learning We observe many Snorkel users writing labeling functions that are statistically dependent. Examples we have observed include: \u2022 Functions that are variations of each other, such as checking for matches against similar regular expressions. \u2022 Functions that operate on correlated inputs, such as raw tokens of text and their lemmatizations. \u2022 Functions that use correlated sources of knowledge, such as distant supervision from overlapping knowledge bases. Modeling such dependencies is important because they affect our estimates of the true labels", "9bcc07e3-32ab-4c9e-87ea-6ce7bbf75138": "We can also express these results in terms of the corresponding partitioned covariance matrix.\n\nTo do this, we make use of the following identity for the inverse of a partitioned matrix Exercise 2.24 From these we obtain the following expressions for the mean and covariance of the conditional distribution p(xa|xb) Comparing (2.73) and (2.82), we see that the conditional distribution p(xa|xb) takes a simpler form when expressed in terms of the partitioned precision matrix than when it is expressed in terms of the partitioned covariance matrix. Note that the mean of the conditional distribution p(xa|xb), given by (2.81), is a linear function of xb and that the covariance, given by (2.82), is independent of xa. This represents an example of a linear-Gaussian model. Section 8.1.4 We have seen that if a joint distribution p(xa, xb) is Gaussian, then the conditional distribution p(xa|xb) will again be Gaussian. Now we turn to a discussion of the marginal distribution given by which, as we shall see, is also Gaussian", "749a4f36-8476-45fd-b3f4-0cbf4f471f56": "AUTOENCODERS  https://www.deeplearningbook.org/contents/autoencoders.html    images . To produce binary codes for semantic hashing, one typically uses an encoding function with sigmoids on the final layer. The sigmoid units must be trained to be saturated to nearly 0 or nearly 1 for all input values.\n\nOne trick that can accomplish this is simply to inject additive noise just before the sigmoid nonlinearity during training. The magnitude of the noise should increase over time. To fight that noise and preserve as much information as possible, the network must increase the magnitude of the inputs to the sigmoid function, until saturation occurs. The idea of learning a hashing function has been further explored in several directions, including the idea of training the representations to optimize a loss more directly linked to the task of finding nearby examples in the hash table . 523  https://www.deeplearningbook.org/contents/autoencoders.html", "1d3d3bd9-76e2-459d-b920-667bf2d66665": "8.7 (\u22c6 \u22c6) Using the recursion relations (8.15) and (8.16), show that the mean and covariance of the joint distribution for the graph shown in Figure 8.14 are given by (8.17) and (8.18), respectively. 8.9 (\u22c6) www Using the d-separation criterion, show that the conditional distribution for a node x in a directed graph, conditioned on all of the nodes in the Markov blanket, is independent of the remaining variables in the graph. 8.10 (\u22c6) Consider the directed graph shown in Figure 8.54 in which none of the variables is observed. Show that a \u22a5\u22a5 b | \u2205. Suppose we now observe the variable d. Show that in general a \u0338\u22a5\u22a5 b | d. 8.11 (\u22c6 \u22c6) Consider the example of the car fuel system shown in Figure 8.21, and suppose that instead of observing the state of the fuel gauge G directly, the gauge is seen by the driver D who reports to us the reading on the gauge. This report is either that the gauge shows full D = 1 or that it shows empty D = 0", "b4eb4732-4551-4931-be24-92be97694c49": "Leen, and K. R. M\u00a8uller (Eds.\n\n), Advances in Neural Information Processing Systems, Volume 12, pp. 914\u2013920. MIT Press. Hornik, K. Approximation capabilities of multilayer feedforward networks. Neural Networks 4(2), 251\u2013257. Hornik, K., M. Stinchcombe, and H. White . Multilayer feedforward networks are universal approximators. Neural Networks 2(5), 359\u2013366. Hotelling, H. Analysis of a complex of statistical variables into principal components. Journal of Educational Psychology 24, 417\u2013441. Hotelling, H. Relations between two sets of variables. Biometrika 28, 321\u2013377. Hyv\u00a8arinen, A. and E. Oja . A fast \ufb01xed-point algorithm for independent component analysis. Neural Computation 9(7), 1483\u20131492. Isard, M. and A. Blake", "2b64a52c-898a-4cee-824a-ae2bac6e44ef": "Then, in the episodic case we de\ufb01ne performance as where v\u21e1\u2713 is the true value function for \u21e1\u2713, the policy determined by \u2713. From here on in our discussion we will assume no discounting (\u03b3 = 1) for the episodic case, although for completeness we do include the possibility of discounting in the boxed algorithms. With function approximation it may seem challenging to change the policy parameter in a way that ensures improvement. The problem is that performance depends on both the action selections and the distribution of states in which those selections are made, and that both of these are a\u21b5ected by the policy parameter. Given a state, the e\u21b5ect of the policy parameter on the actions, and thus on reward, can be computed in a relatively straightforward way from knowledge of the parameterization. But the e\u21b5ect of the policy on the state distribution is a function of the environment and is typically unknown", "094b0730-640c-40bb-9f72-4affccb8f00e": "The effective dimensionality of the principal subspace is then determined by the number of finite O:i values, and the corresponding vectors Wi can be thought of as 'relevant' for modelling the data distribution. In this way, the Bayesian approach is automatically making the trade-off between improving the fit to the data, by using a larger number of vectors Wi with their corresponding eigenvalues Ai each tuned to the data, and reducing the complexity of the model by suppressing some of the Wi vectors. The origins of this sparsity were discussed earlier in the context of relevance vector machines.\n\nThe values of O:i are re-estimated during training by maximizing the log marginal likelihood given by where the log ofp(XIW, J-L, 0'2) is given by (12.43). Note that for simplicity we also treat J-L and 0'2 as parameters to be estimated, rather than defining priors over these parameters. Because this integration is intractable, we make use of the Laplace approximation", "79aa1031-901a-44f9-aee1-d05df855b80e": "Note that Monte Carlo methods cannot easily be used here because termination is not guaranteed for all policies. If a policy was ever found that caused the agent to stay in the same state, then the next episode would never end. Online learning methods such as Sarsa do not have this problem because they quickly learn during the episode that such policies are poor, and switch to something else. gridworld assuming eight possible actions, including the diagonal moves, rather than the Exercise 6.10: Stochastic Wind (programming) Re-solve the windy gridworld task with King\u2019s moves, assuming that the e\u21b5ect of the wind, if there is any, is stochastic, sometimes varying by 1 from the mean values given for each column. That is, a third of the time you move exactly according to these values, as in the previous exercise, but also a third of the time you move one cell above that, and another third of the time you move one cell below that", "adba93f7-7dd7-483a-9177-54e5604eab6f": "If we had an unlimited supply of data (and unlimited computational resources), we could in principle \ufb01nd the regression function h(x) to any desired degree of accuracy, and this would represent the optimal choice for y(x). However, in practice we have a data set D containing only a \ufb01nite number N of data points, and consequently we do not know the regression function h(x) exactly. If we model the h(x) using a parametric function y(x, w) governed by a parameter vector w, then from a Bayesian perspective the uncertainty in our model is expressed through a posterior distribution over w. A frequentist treatment, however, involves making a point estimate of w based on the data set D, and tries instead to interpret the uncertainty of this estimate through the following thought experiment. Suppose we had a large number of data sets each of size N and each drawn independently from the distribution p(t, x). For any given data set D, we can run our learning algorithm and obtain a prediction function y(x; D).\n\nDifferent data sets from the ensemble will give different functions and consequently different values of the squared loss", "9d09de0e-da5a-42b6-afa2-271224d2fc24": "demonstrated that shallow networks with a broad family of non-polynomial activation functions, including rectified linear units, have universal approximation properties, but these results do not address the questions of depth or efficiency\u2014they specify only that a sufficiently wide rectifier network could represent any function. \\lontufar ef al. showed that functions representable with a deep rectifier net can require an exponential number of hidden units with a shallow (one hidden layer) network. More precisely, they showed that piecewise linear networks (which can be obtained from rectifier nonlinearities or maxout units) can represent functions with a number of regions that is exponential in the depth of the network. Figure 6.5 illustrates how a network with absolute value rectification creates mirror images of the function computed on top of some hidden unit, with respect to the input of that hidden unit. Each hidden unit specifies where to fold the input space in order to create mirror responses (on both sides of the absolute value nonlinearity). By composing these folding operations, we obtain an exponentially large number of piecewise linear regions that can capture all kinds of regular (e.g., repeating) patterns", "8c50075d-3d74-4503-8fff-ab2a803e3c59": "https://www.deeplearningbook.org/contents/generative_models.html    20.2.1 Conditional Distributions  Though P(v) is intractable, the bipartite graph structure of the RBM has the special property of its conditional distributions P(h | v) and P(v | h) being factorial and relatively simple to compute and sample from.\n\nDeriving the conditional distributions from the joint distribution is straightfor-  ward: P(h,v) P(h = . (hy = (20.7) 1 1 T T T _ = 20.8 PZ? te vt+ch+v wh} (20.8) 1 = Fev {eth + v'wh} (20.9) 655 CHAPTER 20. DEEP GENERATIVE MODELS 1 Nh MH tT = zi exp S- cjhjy + Sov W. jh; (20.10) j=l j=l Nh t = Zi Il exp {eshj +u W.shj} . (20.11)  j=l  Since we are conditioning on the visible units v, we can treat these as constant with respect to the distribution P(h |v)", "4c5f63e7-823d-4ba4-9d76-479590e62900": "Producing high resolution out- puts from GANs is very difficult due to issues with training stability and mode collapse. Many of the newer GAN architectures such as StackGAN  and Progressively-Grow- ing GANs  are designed to produce higher resolution images. In addition to these architectures, the use of super-resolution networks such as SRGAN could be an effective technique for improving the quality of outputs from a DCGAN  model.\n\nOnce it is practical to produce high resolution outputs from GAN samples, these outputs will be very useful for Data Augmentation. Final dataset size  A necessary component of Data Augmentation is the determination of the final data- set size. For example, if all images are horizontally flipped and added to the dataset, the resulting dataset size changes from N to 2N. One of the main considerations with respect to final dataset size is the additional memory and compute constraints associ- ated with augmenting data. Practitioners have the choice between using generators  which transform data on the fly during training or transforming the data beforehand and Shorten and Khoshgoftaar J Big Data  6:60   storing it in memory. Transforming data on the fly can save memory, but will result in slower training", "607b0b1a-c19c-4bbf-853d-745454166889": "This pseudocode highlights a few optimizations possible in the special case of binary features (features are either active (=1) or inactive (=0). increase the e\ufb03ciency of control algorithms over one-step methods and even over n-step methods. The reason for this is illustrated by the gridworld example below. The \ufb01rst panel shows the path taken by an agent in a single episode.\n\nThe initial estimated values were zero, and all rewards were zero except for a positive reward at the goal location marked by G. The arrows in the other panels show, for various algorithms, which action-values would be increased, and by how much, upon reaching the goal. A one-step method would increment only the last action value, whereas an n-step method would equally increment the last n actions\u2019 values, and an eligibility trace method would update all the action values up to the beginning of the episode, to di\u21b5erent degrees, fading with recency. The fading strategy is often the best", "19005ecb-7e9f-4a79-9c14-e5487dfbae2b": "Snorkel automatically models and combines their outputs using a generative model, then uses the resulting probabilistic labels to train a discriminative model practical aspects of implementing and applying this abstraction that have not been previously considered. We present Snorkel, the \ufb01rst end-to-end system for combining weak supervision sources to rapidly create training data (Fig. 2). We built Snorkel as a prototype to study how people could use data programming, a fundamentally new approach to building machine learning applications.\n\nThrough weekly hackathons and of\ufb01ce hours held at Stanford University over the past year, we have interacted with a growing user community around Snorkel\u2019s open-source implementation.1 We have observed SMEs in industry, science, and government deploying Snorkel for knowledge base construction, image analysis, bioinformatics, fraud detection, and more. From this experience, we have distilled three principles that have shaped Snorkel\u2019s design: 1. Bring All Sources to Bear The system should enable users to opportunistically use labels from all available weak supervision sources. 2", "c2b251e8-d062-4fa3-bd5d-dcaac590a4f5": "we have \",'.nled Ihal tile '\"Ine ,II for ,lie dl,nen,ionalit)\" of tile principal .ubspace is gi\"en, In praclice.\n\n\".-e nlmt cOOose a suilable ,..I\"\" according 10 the application. For ,isuali,a,ion. we ge\"\"\",ny choose .\\1 = 2. whereas for OIher application, the approrrialC choice for ,1/ ma)\" be less dea,. One appmao:h i. 10 pi\", the eigen\"alue 'peclrum for lhe data set. analog,\u2022.\" 10 the example in Figure 12.4 for the off_line digits dala SCI, and look to see if lite eige\",,,I...", "a57d42d6-e7ef-4819-bfe4-98f3b26ee780": "As with the separable case, we can re-cast the SVM for nonseparable distributions in terms of the minimization of a regularized error function. This will also allow us to highlight similarities, and differences, compared to the logistic regression model. Section 4.3.2 We have seen that for data points that are on the correct side of the margin boundary, and which therefore satisfy yntn \u2a7e 1, we have \u03ben = 0, and for the remaining points we have \u03ben = 1 \u2212 yntn. Thus the objective function (7.21) can be written (up to an overall multiplicative constant) in the form where \u03bb = (2C)\u22121, and ESV(\u00b7) is the hinge error function de\ufb01ned by where + denotes the positive part. The hinge error function, so-called because of its shape, is plotted in Figure 7.5. It can be viewed as an approximation to the misclassi\ufb01cation error, i.e., the error function that ideally we would like to minimize, which is also shown in Figure 7.5. When we considered the logistic regression model in Section 4.3.2, we found it convenient to work with target variable t \u2208 {0, 1}", "d5d4092a-d255-4666-80d0-b9bd3513e9c4": "To derive an SGD method for the PBE (assuming linear function approximation) we begin by expanding and rewriting the objective (11.22) in matrix terms: To turn this into an SGD method, we have to sample something on every time step that has this quantity as its expected value. Let us take \u00b5 to be the distribution of states visited under the behavior policy. All three of the factors above can then be written in terms of expectations under this distribution. For example, the last factor can be written which is just the expectation of the semi-gradient TD(0) update (11.2).\n\nThe \ufb01rst factor is the transpose of the gradient of this update: Substituting these expectations for the three factors in our expression for the gradient of the PBE, we get It might not be obvious that we have made any progress by writing the gradient in this form. It is a product of three expressions and the \ufb01rst and last are not independent. They both depend on the next feature vector xt+1; we cannot simply sample both of these expectations and then multiply the samples. This would give us a biased estimate of the gradient just as in the naive residual-gradient algorithm. Another idea would be to estimate the three expectations separately and then combine them to produce an unbiased estimate of the gradient", "5da5814a-7827-4425-975a-64fc0c32afc8": "At the time, the performance of ASR based on neural nets approximately matched the performance of GMM-HMM systems.\n\nFor example, Robinson and Fallside  achieved 26 percent phoneme error rate on the TIMIT  corpus (with 39 phonemes to discriminate among), which was better than or comparable to HMM-based systems. Since then, TIMIT has been a benchmark for phoneme recognition, playing a role similar to the role MNIST plays for object recognition. Nonetheless, because of the complex engineering involved in software systems for speech recognition and the effort that had been invested in building these systems on the basis of GMM-HMMs, the industry did not see a compelling argument for switching to neural networks. As a consequence, until the late 2000s, both academic and industrial research in using neural nets for speech recognition mostly focused on using neural nets to learn extra features for GMM-HMM systems. Later, with much larger and deeper models and much larger datasets, recognition accuracy was dramatically improved by using neural networks to replace GMMs for the task of associating acoustic features to phonemes (or subphonemic states). Starting in 2009, speech researchers applied a form of deep learning based on unsupervised learning to speech recognition", "6146430d-b025-4d04-aefd-6f201794c051": "This part of the model characterizes the probability of each possible resulting state (as in (3.4)), but now this state may result after various numbers of time steps, each of which must be discounted di\u21b5erently. The model for option ! speci\ufb01es, for each state s that ! might start executing in, and for each state s0 that ! might terminate in, Note that, because of the factor of \u03b3k, this p(s0|s, !) is no longer a transition probability and no longer sums to one over all values of s0. (Nevertheless, we continue to use the \u2018|\u2019 notation in p.) The above de\ufb01nition of the transition part of an option model allows us to formulate Bellman equations and dynamic programming algorithms that apply to all options, including low-level actions as a special case. For example, the general Bellman equation for the state values of a hierarchical policy \u21e1 is where \u2326(s) denotes the set of options available in state s. If \u2326(s) includes only the low-level actions, then this equation reduces to a version of the usual Bellman equation (3.14), except of course \u03b3 is included in the new p (17.3) and thus does not appear", "9ac5b8d0-a5aa-4c57-955b-f98a524920fd": "Advances in Applied Probability, 27(4):1054\u20131078. Institute of Technology, Cambridge MA. AI-TR 1085, MIT Arti\ufb01cial Intelligence Laboratory. Agre, P. E., Chapman, D. What are plans for? Robotics and Autonomous Systems, Aizerman, M. A., Braverman, E. \u00b4I., Rozonoer, L. I. Probability problem of pattern recognition learning and potential functions method. Avtomat. i Telemekh, 25(9):1307\u20131323. Albus, J. S. A theory of cerebellar function. Mathematical Biosciences, 10(1-2):25\u201361. Albus, J. S. Brain, Behavior, and Robotics. Byte Books, Peterborough, NH. Aleksandrov, V. M., Sysoev, V. I., Shemeneva, V. V. Stochastic optimization of systems", "9b482a1d-1b6e-44a1-a9d0-a3098257842d": "Because \u2206wposterior < \u2206wprior this term is negative, and it increases in magnitude as the ratio \u2206wposterior/\u2206wprior gets smaller. Thus, if parameters are \ufb01nely tuned to the data in the posterior distribution, then the penalty term is large. For a model having a set of M parameters, we can make a similar approximation for each parameter in turn. Assuming that all parameters have the same ratio of \u2206wposterior/\u2206wprior, we obtain Thus, in this very simple approximation, the size of the complexity penalty increases linearly with the number M of adaptive parameters in the model.\n\nAs we increase the complexity of the model, the \ufb01rst term will typically decrease, because a more complex model is better able to \ufb01t the data, whereas the second term will increase due to the dependence on M. The optimal model complexity, as determined by the maximum evidence, will be given by a trade-off between these two competing terms. We shall later develop a more re\ufb01ned version of this approximation, based on a Gaussian approximation to the posterior distribution", "30b7b839-59b1-42c5-acc5-c904a8db8cc7": "In these cases, the computational cost of using dropout and larger models may outweigh che benefit of regularization. When extremely few labeled training examples are available, dropout is less effective. Bayesian neural networks  outperform dropout on the Alternative Splicing Dataset , where fewer than 5,000 examples are available .\n\nWhen additional unlabeled data is available, unsupervised feature learning can gain an advantage over dropout. Wager et al. showed that, when applied to linear regression, dropout is equivalent to L* weight decay, with a different weight decay coefficient for each input feature. The magnitude of each feature\u2019s weight decay coefficient is determined by its variance. Similar results hold for other linear models. For deep  wee dnl Ann need te nad Arete and be ete I24.--  https://www.deeplearningbook.org/contents/regularization.html    IHWUUECIS, ULOPUUL 1b LOL CYULVAIELIL LU WEIXIIL UCCay. The stochasticity used while training with dropout is not necessary for the 262  CHAPTER 7", "65933cb5-38bd-44d0-9117-8b291cc805e7": "Thus the number of terms in the summation grows exponentially with the length of the chain.\n\nIn fact, the summation in (13.11) corresponds to summing over exponentially many paths through the lattice diagram in Figure 13.7. We have already encountered a similar dif\ufb01culty when we considered the inference problem for the simple chain of variables in Figure 8.32. There we were able to make use of the conditional independence properties of the graph to re-order the summations in order to obtain an algorithm whose cost scales linearly, instead of exponentially, with the length of the chain. We shall apply a similar technique to the hidden Markov model. A further dif\ufb01culty with the expression (13.11) for the likelihood function is that, because it corresponds to a generalization of a mixture distribution, it represents a summation over the emission models for different settings of the latent variables. Direct maximization of the likelihood function will therefore lead to complex expressions with no closed-form solutions, as was the case for simple mixture models Section 9.2 (recall that a mixture model for i.i.d. data is a special case of the HMM)", "85b5c055-92cd-4b86-8330-12cbd53b7325": "If we choose M = D - 1 then, if all ai values are finite, the model represents a full-covariance Gaussian, while if all the ai go to infinity the model is equivalent to an isotropic Gaussian, and so the model can encompass all pennissible values for the effective dimensionality of the principal subspace.\n\nIt is also possible to consider smaller values of M, which will save on computational cost but which will limit the maximum dimensionality of the subspace. A comparison of the results of this algorithm with standard probabilistic PCA is shown in Figure 12.14. Bayesian PCA provides an opportunity to illustrate the Gibbs sampling algorithm discussed in Section 11.3. Figure 12.15 shows an example of the samples from the hyperparameters In ai for a data set in D = 4 dimensions in which the dimensionality of the latent space is M = 3 but in which the data set is generated from a probabilistic PCA model having one direction of high variance, with the remaining directions comprising low variance noise. This result shows clearly the presence of three distinct modes in the posterior distribution", "e3337546-a0e9-4859-9571-f074b03eeea1": "The \ufb01rst 8 feature planes were raw representations of the positions of the current player\u2019s stones in the current and seven past board con\ufb01gurations: a feature value was 1 if a player\u2019s stone was on the corresponding point, and was 0 otherwise. The next 8 feature planes similarly coded the positions of the opponent\u2019s stones. A \ufb01nal input feature plane had a constant value indicating the color of the current play: 1 for black; 0 for white.\n\nBecause repetition is not allowed in Go and one player is given some number of \u201ccompensation points\u201d for not getting the \ufb01rst move, the current board position is not a Markov state of Go. This is why features describing past board positions and the color feature were needed. The network was \u201ctwo-headed,\u201d meaning that after a number of initial layers, the network split into two separate \u201cheads\u201d of additional layers that separately fed into two sets of output units. In this case, one head fed 362 output units producing 192 + 1 move probabilities p, one for each possible stone placement plus pass; the other head fed just one output unit producing the scalar v, an estimate of the probability that the current player will win from the current board position", "ce4f8ba8-34fc-476a-9295-ec1e7cf84b47": "It is interesting to investigate the nature of this downsampling and resulting perfor-  mance comparison. Wu et al. compare the tradeoff between accuracy and speed Shorten and Khoshgoftaar J Big Data  6:60   Original image Low-resolution model High-resolution model  Rank Score Class Rank Score Class 0.2287 ant 1 0.103 lacewing  0.0997 damselfiy 2 0.074 dragonfly 0.057 nematode 3 0.074 damselfly 0.0546 chainlinkfence 4 0.063 walkino stic 0.0522 __long-horned $ 0.039 __long-horned  0.0307 walking stick 6 0.027 leafhopper  0.0287 dragonfly 7 0.025 nail  0.0267 tiger beetle 8 0.023 grasshopper 9 0.019 ant  0.0225 doormat fi  fly hammer American gar chainlink padlock tree frog cicada screwdriver harvestman Fig. 33 Classifications of the Image to the right by different resolution models trained by Wu et al", "d252c215-5351-4c2e-b990-742d4d88a934": "We shall begin by discussing Bayesian networks, also known as directed graphical models, in which the links of the graphs have a particular directionality indicated by arrows. The other major class of graphical models are Markov random \ufb01elds, also known as undirected graphical models, in which the links do not carry arrows and have no directional signi\ufb01cance.\n\nDirected graphs are useful for expressing causal relationships between random variables, whereas undirected graphs are better suited to expressing soft constraints between random variables. For the purposes of solving inference problems, it is often convenient to convert both directed and undirected graphs into a different representation called a factor graph. In this chapter, we shall focus on the key aspects of graphical models as needed for applications in pattern recognition and machine learning. More general treatments of graphical models can be found in the books by Whittaker , Lauritzen , Jensen , Castillo et al. , Jordan , Cowell et al. , and Jordan . In order to motivate the use of directed graphs to describe probability distributions, consider \ufb01rst an arbitrary joint distribution p(a, b, c) over three variables a, b, and c", "c635356d-e57d-4cb0-98f5-c52ee0b124c2": "Because convolutional networks usually use multichannel convolution, the linear operations they are based on are not guaranteed to be commutative, even if kernel flipping is used. These multichannel operations are only commutative if each operation has the same number of output channels as input channels. https://www.deeplearningbook.org/contents/convnets.html    Assume we have a 4-D kernel tensor K with element Ki.J,k,! giving the connection strength between a unit in channel ? of the output and a unit in channel J of the input, with an offset of k rows and / columns between the output unit and the  input unit.\n\nAssume our input consists of observed data V with element V;_; , giving the value of the input unit within channel 7 at row j and column k. Assume our output consists of Z with the same format as V", "34254404-6ab3-48da-83a6-91a49586e15b": "All of the methods we have explored so far in this book have three key ideas in common: \ufb01rst, they all seek to estimate value functions; second, they all operate by backing up values along actual or possible state trajectories; and third, they all follow the general strategy of generalized policy iteration (GPI), meaning that they maintain an approximate value function and an approximate policy, and they continually try to improve each on the basis of the other. These three ideas are central to the subjects covered in this book. We suggest that value functions, backing up value updates, and GPI are powerful organizing principles potentially relevant to any model of intelligence, whether arti\ufb01cial or natural. Two of the most important dimensions along which the methods vary are shown in ranging from one-step TD updates to full-return Monte Carlo updates. Between these is a spectrum including methods based on n-step updates (and in Chapter 12 we will extend this to mixtures of n-step updates such as the \u03bb-updates implemented by eligibility traces).\n\nDynamic programming methods are shown in the extreme upper-right corner of the space because they involve one-step expected updates", "c1e7b6ab-c7cc-4557-a458-102fbc3508e5": "Chapter 8  Optimization for Training Deep Models  Deep learning algorithms involve optimization in many contexts. For example, performing inference in models such as PCA involves solving an optimization problem. We often use analytical optimization to write proofs or design algorithms. Of all the many optimization problems involved in deep learning, the most difficult is neural network training. It is quite common to invest days to months of time on hundreds of machines to solve even a single instance of the neural network training problem. Because this problem is so important and so expensive, a specialized set of optimization techniques have been developed for solving it. This chapter  presents these optimization techniques for neural network training. If you are unfamiliar with the basic principles of gradient-based optimization, we suggest reviewing chapter 4. That chapter includes a brief overview of numerical optimization in general. This chapter focuses on one particular case of optimization: finding the param- eters @ of a neural network that significantly reduce a cost function J(@), which typically includes a performance measure evaluated on the entire training set as well as additional regularization terms. We begin with a description of how optimization used as a training algorithm for a machine learning task differs from pure optimization", "1db9b230-4700-442e-a494-b6055a131a7e": "CONFRONTING THE PARTITION FUNCTION  Algorithm 18.2 The contrastive divergence algorithm, using gradient ascent as the optimization procedure  Set \u20ac, the step size, to a small positive number. Set k, the number of Gibbs steps, high enough to allow a Markov chain sampling from p(x;@) to mix when initialized from paata. Perhaps 1-20 to train an RBM on a small image patch. while not converged do Sample a minibatch of m examples {x), wee xl} from the training set go to\", Vo log p(x; 6). for i= 1 tom do xO Lx, end for for i=1 tok do for 7 = 1 tom do x) < gibbs_update(x/)), end for end for g-g-s 1, Velog p(x; 6). Oc O+\u00a2g.\n\nend while 5\u00b0  https://www.deeplearningbook.org/contents/partition.html       the Markov chains from a distribution that is very close to the model distribution, so that the burn in operation does not take as many steps", "9a8d05c3-c11d-4666-8586-1abba6fbc771": "Other Forms of Supervision Work on semi-supervised learning considers settings with some labeled data and a much larger set of unlabeled data, and then leverages various domain- and task-agnostic assumptions about smoothness, low-dimensional structure, or distance metrics to heuristically label the unlabeled data . Work on active learning aims to automatically estimate which data points are optimal to label, thereby hopefully reducing the total number of examples that need to be manually annotated . Transfer learning considers the strategy of repurposing models trained on different datasets or tasks where labeled training data is more abundant . Another type of supervision is selftraining  and co-training , which involves training a model or pair of models on data that they labeled themselves.\n\nWeak supervision is distinct in that the goal is to solicit input directly from SMEs, however at a higher level of abstraction and/or in an inherently noisier form. Snorkel is focused on managing weak supervision sources, but combing its methods with these other types of supervision is straightforward. Related Data Management Problems Researchers have considered related problems in data management, such as data fusion  and truth discovery", "439124e3-ec50-4887-b203-25fd061ab3de": "Initialize grad_table, a data structure associating tensors to their gradients grad_table < 1 for V in T do build_grad(V,G,G',grad_table) end for Return grad_table restricted to T  Algorithm 6.6 The inner loop subroutine build_grad(V,G,G\u2019, grad_table) of the back-propagation algorithm, called by the back-propagation algorithm defined in algorithm 6.5.\n\nRequire: V, the variable whose gradient should be added to G and grad_table Require: G, the graph to modify Require: G\u2019, the restriction of G to nodes that participate in the gradient Require: grad_table, a data structure mapping nodes to their gradients if V isin grad_table then Return grad_table end if iol for C in get_consumers(V,G\u2019) do op + get_operation(C) D< build_grad(C,G,G\u2019, grad_table) G < op.bprop(get_inputs(C,G\u2019),V, D) iHitl end for Gey ,G6\u201d grad_table=G Insert G and the operations creating it into G Return G  213  https://www.deeplearningbook.org/contents/mlp.html    CHAPTER 6", "ac00a3bb-82df-4080-8cff-9545769ed545": "Kernel functions numerically express how relevant knowledge about any state is to any other state.\n\nAs an example, the strengths of generalization for tile coding shown in Figure 9.11 correspond to di\u21b5erent kernel functions resulting from uniform and asymmetrical tile o\u21b5sets. Although tile coding does not explicitly use a kernel function in its operation, it generalizes according to one. In fact, as we discuss more below, the strength of generalization resulting from linear parametric function approximation can always be described by a kernel function. Kernel regression is the memory-based method that computes a kernel weighted average of the targets of all examples stored in memory, assigning the result to the query state. If D is the set of stored examples, and g(s0) denotes the target for state s0 in a stored example, then kernel regression approximates the target function, in this case a value function depending on D, as The weighted average method described above is a special case in which k(s, s0) is non-zero only when s and s0 are close to one another so that the sum need not be computed over all of D", "b2f4f0d5-534a-4c9c-afbd-e20f9fb8c1f4": "Greedy algorithms break a problem into many components, then solve for he optimal version of each component in isolation. Unfortunately, combining the individually optimal components is not guaranteed to yield an optimal complete solution. Nonetheless, greedy algorithms can be computationally much cheaper chan algorithms that solve for the best joint solution, and the quality of a greedy solution is often acceptable if not optimal.\n\nGreedy algorithms may also be followed by a fine-tuning stage in which a joint optimization algorithm searches for an optimal solution to the full problem. Initializing the joint optimization algorithm with a greedy solution can greatly speed it up and improve the quality of the solution it finds. Pretraining, and especially greedy pretraining, algorithms are ubiquitous in deep learning. In this section, we describe specifically those pretraining algorithms that break supervised learning problems into other simpler supervised learning problems. This approach is known as greedy supervised pretraining. In the original  version of greedy supervised pretraining, each stage consists of a supervised learning training task involving only a subset of the layers in the final neural network", "2f953ab1-23e9-4804-9b01-8da5f1c059a2": "The algorithms discussed in this chapter will be equally applicable if we \ufb01rst make a \ufb01xed nonlinear transformation of the input variables using a vector of basis functions \u03c6(x) as we did for regression models in Chapter 3. We begin by considering classi\ufb01cation directly in the original input space x, while in Section 4.3 we shall \ufb01nd it convenient to switch to a notation involving basis functions for consistency with later chapters.\n\nA discriminant is a function that takes an input vector x and assigns it to one of K classes, denoted Ck. In this chapter, we shall restrict attention to linear discriminants, namely those for which the decision surfaces are hyperplanes. To simplify the discussion, we consider \ufb01rst the case of two classes and then investigate the extension to K > 2 classes. The simplest representation of a linear discriminant function is obtained by taking a linear function of the input vector so that where w is called a weight vector, and w0 is a bias (not to be confused with bias in the statistical sense). The negative of the bias is sometimes called a threshold. An input vector x is assigned to class C1 if y(x) \u2a7e 0 and to class C2 otherwise", "709ea0cb-e802-4476-a3ea-9238eb0d0837": "Deep neural networks have been successfully applied to Com- puter Vision tasks such as image classification, object detection, and image segmenta- tion thanks to the development of convolutional neural networks (CNNs). These neural networks utilize parameterized, sparsely connected kernels which preserve the spatial characteristics of images. Convolutional layers sequentially downsample the spatial resolution of images while expanding the depth of their feature maps. This series of convolutional transformations can create much lower-dimensional and more useful rep- resentations of images than what could possibly be hand-crafted.\n\nThe success of CNNs  has spiked interest and optimism in applying Deep Learning to Computer Vision tasks. \u00a9 The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made", "c3ba5b9b-8971-4e82-8c9a-7fcc46b91054": "The algorithm proposed with the manifold tangent classifier is therefore simple: (1) use an autoencoder to learn the manifold structure by unsupervised learning, and (2) use these tangents to regularize a neural net classifier as in tangent prop (equation 7.67). In this chapter, we have described most of the general strategies used to regularize neural networks. Regularization is a central theme of machine learning and as such will be revisited periodically in most of the remaining chapters. Another central theme of machine learning is optimization, described next. https://www.deeplearningbook.org/contents/regularization.html    270  https://www.deeplearningbook.org/contents/regularization.html", "4d843f43-175c-4629-97a9-b3aeeb60df6c": "For example, in queuing tasks there are actions such as assigning customers to servers, rejecting customers, or discarding information. In such cases the actions are in fact de\ufb01ned in terms of their immediate e\u21b5ects, which are completely known. It is impossible to describe all the possible kinds of specialized problems and corresponding specialized learning algorithms. However, the principles developed in this book should apply widely. For example, afterstate methods are still aptly described in terms of generalized policy iteration, with a policy and (afterstate) value function interacting in essentially the same way.\n\nIn many cases one will still face the choice between on-policy and o\u21b5-policy methods for managing the need for persistent exploration. In this chapter we introduced a new kind of learning method, temporal-di\u21b5erence (TD) learning, and showed how it can be applied to the reinforcement learning problem. As usual, we divided the overall problem into a prediction problem and a control problem. TD methods are alternatives to Monte Carlo methods for solving the prediction problem. In both cases, the extension to the control problem is via the idea of generalized policy iteration (GPI) that we abstracted from dynamic programming", "45013dcd-0e72-414b-bee0-7b3adfc80770": "When node c is unobserved, it \u2018blocks\u2019 the path, and the variables a and b are independent. However, conditioning on c \u2018unblocks\u2019 the path and renders a and b dependent. There is one more subtlety associated with this third example that we need to consider. First we introduce some more terminology. We say that node y is a descendant of node x if there is a path from x to y in which each step of the path follows the directions of the arrows. Then it can be shown that a head-to-head path will become unblocked if either the node, or any of its descendants, is observed. Exercise 8.10 In summary, a tail-to-tail node or a head-to-tail node leaves a path unblocked unless it is observed in which case it blocks the path. By contrast, a head-to-head node blocks a path if it is unobserved, but once the node, and/or at least one of its descendants, is observed the path becomes unblocked. It is worth spending a moment to understand further the unusual behaviour of the graph of Figure 8.20", "c9965478-51fd-44ff-9bc7-092f95b76e6c": "For a deeper treatment of recurrent networks as dynamical systems, see Doya , Bengio et al. , and Siegelmann and Sontag , with a review in Pascanu ef al. The remaining sections of this chapter discuss various approaches that have been proposed to reduce the difficulty of learning long- term dependencies (in some cases allowing an RNN to learn dependencies across hundreds of steps), but the problem of learning long-term dependencies remains one of the main challenges in deep learning. 10.8 Echo State Networks  https://www.deeplearningbook.org/contents/rnn.html    (t) The re rent eights mapping from nt to bh\u201d and the input weights mapping from x et ne are some of the most difficult parameters to learn in a recurrent network.\n\nOw proposed  approach to avoiding this difficulty is to set the recurrent weights such that the recurrent hidden units do a good job of capturing the history of past inputs, and only learn the output weights. This is the idea that was independently proposed for echo state networks, or ESNs , and liquid state machines", "1ec7e5ca-350c-428d-8aef-ec4ad614191d": "Lastly, we observe that the two types of manipulation tend to excel in different contexts: augmentation shows superiority over weighting with a small amount of data available, while weighting is better at addressing class imbalance problems.\n\nThe way we derive the manipulation algorithm represents a general means of problem solving through algorithm extrapolation between learning paradigms, which we discuss more in section 6. Rich types of data manipulation have been increasingly used in modern machine learning pipelines. Previous work each has typically focused on a particular manipulation type. Data augmentation that perturbs examples without changing the labels is widely used especially in vision  and speech  domains. Common heuristic-based methods on images include cropping, mirroring, rotation , and so forth. Recent work has developed automated augmentation approaches . Xie et al. additionally use large-scale unlabeled data. Cubuk et al. , Ratner et al. learn to induce the composition of data transformation operators. Instead of treating data augmentation as a policy in reinforcement learning , we formulate manipulation as a reward function and use ef\ufb01cient stochastic gradient descent to learn the manipulation parameters", "51fe8d31-5a89-46a4-9cc8-39b656d1e2dd": "Due to the separation properties of the graphical model, we can equivalently condition on only the neighbors of x;. Unfortunately, after we have made one pass through the graphical model and sampled all n variables, we still do not have a fair sample from p(x). Instead, we must repeat the process and resample all n variables using the updated values of their neighbors. Asymptotically, after many repetitions, this process converges to sampling from the correct distribution.\n\nIt can be difficult to determine when the samples have reached a sufficiently accurate approximation of the desired distribution. Sampling techniques for undirected models are an advanced topic, covered in more detail in chapter 17. 578  CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    16.4 Advantages of Structured Modeling  The primary advantage of using structured probabilistic models is that they allow us to dramatically reduce the cost of representing probability distributions as well as learning and inference", "f4aa4fd8-b7fb-418e-8c56-714e7b2c3324": "See figure 16.6 for a depiction of how active and inactive paths in an undirected model look when drawn in this way. See figure 16.7 for an example of reading separation from an undirected graph. Similar concepts apply to directed models, except that in the context of directed models, these concepts are referred to as d-separation. The \u201cd\u201d stands for \u201cdependence.\u201d D-separation for directed graphs is defined the same as separation for undirected graphs: We say that a set of variables A is d-separated from another  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    set of variables B given a third set of variables S if the graph structure implies that A is independent from B given S.  As with undirected models, we can examine the independences implied by the graph by looking at what active paths exist in the graph.\n\nAs before, two variables are dependent if there is an active path between them and d-separated if no such  569  CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  Figure 16.7: An example of reading separation properties from an undirected graph", "4298e552-d98d-4770-8166-b978b4ee2c7b": "The di\ufb03culty is that gravity is stronger than the car\u2019s engine, and even at full throttle the car cannot accelerate up the steep slope.\n\nThe only solution is to \ufb01rst move away from the goal and up the opposite slope on the left. Then, by applying full throttle the car can build up enough inertia to carry it up the steep slope even though it is slowing down the whole way. This is a simple example of a continuous control task where things have to get worse in a sense (farther from the goal) before they can get better. Many control methodologies have great di\ufb03culties with tasks of this kind unless explicitly aided by a human designer. The reward in this problem is \u22121 on all time steps until the car moves past its goal position at the top of the mountain, which ends the episode. There are three possible actions: full throttle forward (+1), full throttle reverse (\u22121), and zero throttle (0). The car moves according to a simpli\ufb01ed physics. Its position, xt, and velocity, \u02d9xt, are updated by where the bound operation enforces \u22121.2 \uf8ff xt+1 \uf8ff 0.5 and \u22120.07 \uf8ff \u02d9xt+1 \uf8ff 0.07", "1256416f-1130-4f68-8e52-87db0149482f": "Probability can be seen as the extension of logic to deal with uncertainty. Logic provides a set of formal rules for determining what propositions are implied to be true or false given the assumption that some other set of propositions is true or false. Probability theory provides a set of formal rules for determining the likelihood of a proposition being true given the likelihood of other propositions. 3.2. Random Variables  A random variable is a variable that can take on different values randomly. We typically denote the random variable itself with a lowercase letter in plain typeface, and the values it can take on with lowercase script letters. For example, x; and x2 are both possible values that the random variable x can take on.\n\nFor vector-valued variables, we would write the random variable as x and one of its values as w. On its own, a random variable is just a description of the states that are possible; it must be coupled with a probability distribution that specifies how likely each of these states are. Random variables may be discrete or continuous. A discrete random variable is one that has a finite or countably infinite number of states", "ef668ae5-032f-44e8-80db-011db0e02db8": "(20.72) This implicitly imposes a probability distribution over x: -1 a (gy (#)), (20.73)  Or det (52) Of course, this formula may be difficult, to evaluate, depending on the choice of g, So we often use indirect means of lealrning g, lrather than trying to maximize log p(x) directly. https://www.deeplearningbook.org/contents/generative_models.html    in some cases, rather than using y to provide a sample of \u201c directly, we use y to define a conditional distribution over Z. For example, we could use a generator net whose final layer consists of sigmoid outputs to provide the mean parameters of Bernoulli distributions:  P(x: = 1| 2) = g(z)i- (20.74)  In this case, when we use g to define p(a | z), we impose a distribution over a by marginalizing z:  p(x) = Ezp(a | 2)", "0707ee78-1ebf-4e3c-84af-75777c3ef3c9": "Somewhere in the middle lies the optimal model capacity, which achieves the lowest possible generalization error, by adding a medium generalization gap to a medium amount of training error. For some hyperparameters, overfitting occurs when the value of the hyper- parameter is large.\n\nThe number of hidden units in a layer is one such example,  423  CHAPTER 11. PRACTICAL METHODOLOGY  because increasing the number of hidden units increases the capacity of the model. For some hyperparameters, overfitting occurs when the value of the hyperparame- ter is small. For example, the smallest allowable weight decay coefficient of zero corresponds to the greatest effective capacity of the learning algorithm. Not every hyperparameter will be able to explore the entire U-shaped curve. Many hyperparameters are discrete, such as the number of units in a layer or the number of linear pieces in a maxout unit, so it is only possible to visit a few points along the curve. Some hyperparameters are binary", "c4d9869a-2eba-4eee-bff8-caf270faa63d": "Rather than estimating the density of Pr which may not exist, we can de\ufb01ne a random variable Z with a \ufb01xed distribution p(z) and pass it through a parametric function g\u03b8 : Z \u2192 X (typically a neural network of some kind) that directly generates samples following a certain distribution P\u03b8. By varying \u03b8, we can change this distribution and make it close to the real data distribution Pr. This is useful in two ways. First of all, unlike densities, this approach can represent distributions con\ufb01ned to a low dimensional manifold. Second, the ability to easily generate samples is often more useful than knowing the numerical value of the density (for example in image superresolution or semantic segmentation when considering the conditional distribution of the output image given the input image).\n\nIn general, it is computationally di\ufb03cult to generate samples given an arbitrary high dimensional density . Variational Auto-Encoders (VAEs)  and Generative Adversarial Networks (GANs)  are well known examples of this approach. Because VAEs focus on the approximate likelihood of the examples, they share the limitation of the standard models and need to \ufb01ddle with additional noise terms", "12e7f654-34f1-4a24-8565-091343d610c0": "This policy is the same as the \u201cbasic\u201d strategy of Thorp  with the sole exception of the leftmost notch in the policy for a usable ace, which is not present in Thorp\u2019s strategy. We are uncertain of the reason for this discrepancy, but con\ufb01dent that what is shown here is indeed the optimal policy for the version of blackjack we have described. How can we avoid the unlikely assumption of exploring starts? The only general way to ensure that all actions are selected in\ufb01nitely often is for the agent to continue to select them. There are two approaches to ensuring this, resulting in what we call on-policy methods and o\u21b5-policy methods.\n\nOn-policy methods attempt to evaluate or improve the policy that is used to make decisions, whereas o\u21b5-policy methods evaluate or improve a policy di\u21b5erent from that used to generate the data. The Monte Carlo ES method developed above is an example of an on-policy method. In this section we show how an on-policy Monte Carlo control method can be designed that does not use the unrealistic assumption of exploring starts. O\u21b5-policy methods are considered in the next section", "f34827b6-d095-4588-8c61-7f7f21d94367": "where M denotes the set of all misclassi\ufb01ed patterns. The contribution to the error associated with a particular misclassi\ufb01ed pattern is a linear function of w in regions of w space where the pattern is misclassi\ufb01ed and zero in regions where it is correctly classi\ufb01ed. The total error function is therefore piecewise linear. We now apply the stochastic gradient descent algorithm to this error function.\n\nSection 3.1.3 The change in the weight vector w is then given by where \u03b7 is the learning rate parameter and \u03c4 is an integer that indexes the steps of the algorithm. Because the perceptron function y(x, w) is unchanged if we multiply w by a constant, we can set the learning rate parameter \u03b7 equal to 1 without of generality. Note that, as the weight vector evolves during training, the set of patterns that are misclassi\ufb01ed will change. The perceptron learning algorithm has a simple interpretation, as follows. We cycle through the training patterns in turn, and for each pattern xn we evaluate the perceptron function (4.52)", "b260fd55-a1f6-4a0a-81f5-f1c6a6d71478": "Algorithm 8.1 shows how to follow this estimate of the gradient downhill. A crucial parameter for the SGD algorithm is the learning rate. Previously, we have described SGD as using a fixed learning rate e. In practice, it is necessary to gradually decrease the learning rate over time, so we now denote the learning rate at iteration k as \u20ac,. This is because the SGD gradient estimator introduces a source of noise (the random sampling of m training examples) that does not vanish even when we arrive at a minimum. By comparison, the true gradient of the total cost function becomes small and then 0 when we approach and reach a minimum using batch gradient  290  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  Algorithm 8.1 Stochastic gradient descent (SGD) update  Require: Learning rate schedule \u20ac;,@,..", "30addc68-6d43-4ebb-a134-0ff56090b7a8": "Here P(\u0398) is the space of all probability distributions over \u03b8.\n\nThe optimal solution of q(\u03b8) is precisely the the posterior distribution p(\u03b8|D) due to the Bayes\u2019 theorem (Equation 2.14). The proof is straightforward by noticing that the objective can be rewritten as minq KL(q(\u03b8)\u2225p(\u03b8|D)). Similar to the case of duality between MLE and maximum entropy (Equation 2.4), the same entropy maximization principle can cast Bayesian inference as a constrained optimization problem. As Jaynes  commented, this fresh interpretation of Bayes\u2019 theorem \u201ccould make the use of Bayesian methods more attractive and widespread, and stimulate new developments in the general theory of inference.\u201d (Jaynes, 1988, p.280) The next subsection reviews how entropy maximization as a \u201cuseful tool in generating probability distributions\u201d (Jaynes, 1988, p.280) has related to and resulted in more general learning and inference frameworks, such as posterior regularization. 2.3. Posterior Regularization", "6e43cf5b-d801-42de-9744-e1eafe79a698": "Note, however, that these eigenvectors need not be normalized. To determine the appropriate normalization, we re-scale Ui ex: X TVi by a constant such that Ilui II = 1, which, assuming Vi has been normalized to unit length, gives In summary, to apply this approach we first evaluate XXT and then find its eigenvectors and eigenvalues and then compute the eigenvectors in the original data space using (12.30). The formulation of PCA discussed in the previous section was based on a linear projection of the data onto a subspace of lower dimensionality than the original data space.\n\nWe now show that PCA can also be expressed as the maximum likelihood solution of a probabilistic latent variable model. This reformulation of PCA, known as probabilistic peA, brings several advantages compared with conventional PCA: \u2022 Probabilistic PCA represents a constrained form of the Gaussian distribution in which the number of free parameters can be restricted while still allowing the model to capture the dominant correlations in a data set", "21f15f35-b113-4c43-b435-44ba3f125aa1": "The MDP on the right has three of which, B and B\u2032, are represented identically and must be given the same ap value. We can imagine that the value of state A is given by the \ufb01rst componen the value of B and B\u2032 is given by the second. Notice that the observable data for the two MDPs. In both cases the agent will see single occurrences of A fol 0, then some number of Bs each followed by a \u22121, except the last which is fol 1, then we start all over again with a single A and a 0, etc. All the details ar as well; in both MDPs, the probability of a string of k Bs is 2\u2212k. Now conside function v\u03b8 = \u20d70.\n\nIn the \ufb01rst MDP, this is an exact solution, and the overall BE the second MDP, this solution produces an error in both B and B\u2032 of 1, for an of 2/3 if the three states are equally weighted by d. The t which generate the same data, have di\u21b5erent BEs", "37b2aa43-62b5-484f-8b77-c6e9662f17ab": "For this to exactly reproduce inference in P,, we must omit the final character from each sequence when we  https://www.deeplearningbook.org/contents/applications.html    train Pr-1, As an example, we demonstrate how a trigram model computes the probability of the sentence \u201cTHE DOG RAN AWAY.\u201d The first words of the sentence cannot be  handled by the default formula based on conditional probability because there is no context at the beginning of the sentence. Instead, we must use the marginal prob- ability over words at the start of the sentence. We thus evaluate P3(THE DOG RAN). Finally, the last word may be predicted using the typical case, of using the condi- tional distribution P(AWAY | DOG RAN). Putting this together with equation 12.6, we obtain:  P(THE DOG RAN AWAY) = P3(THE DOG RAN) P3(DOG RAN AWAY)/P (DOG RAN)", "bfa12fd1-cfc1-4748-8ae5-18812bbf492f": "Despite great progress in many areas, the gulf between arti\ufb01cial intelligence and the intelligence of humans, and even of other animals, remains great. Superhuman performance can be achieved in some domains, even formidable domains like Go, but it remains a signi\ufb01cant challenge to develop systems that are like us in being complete, interactive agents having general adaptability and problem-solving skills, emotional sophistication, creativity, and the ability to learn quickly from experience. With its focus on learning by interacting with dynamic environments, reinforcement learning, as it develops over the future, will be a critical component of agents with these abilities.\n\nReinforcement learning\u2019s connections to psychology and neuroscience (Chapters 14 and 15) underscore its relevance to another longstanding goal of arti\ufb01cial intelligence: shedding light on fundamental questions about the mind and how it emerges from the brain. Reinforcement learning theory is already contributing to our understanding of the brain\u2019s reward, motivation, and decision-making processes, and there is good reason to believe that through its links to computational psychiatry, reinforcement learning theory will contribute to methods for treating mental disorders, including drug abuse and addiction. Another contribution that reinforcement learning can make over the future is as an aid to human decision making", "6e14c540-3ff0-4509-b8b6-a530bffbbdaa": "To make things more compact, we abbreviate the states high and low, and the actions search, wait, and recharge respectively by h, l, s, w, and re. Because there are only two states, the Bellman optimality equation consists of two equations.\n\nThe equation for v\u21e4(h) can be written as follows: p(h|h, s) + p(l|h, s), p(h|h, w) + p(l|h, w) Following the same procedure for v\u21e4(l) yields the equation For any choice of rs, rw, \u21b5, \u03b2, and \u03b3, with 0 \uf8ff \u03b3 < 1, 0 \uf8ff \u21b5, \u03b2 \uf8ff 1, there is exactly one pair of numbers, v\u21e4(h) and v\u21e4(l), that simultaneously satisfy these two nonlinear equations. Explicitly solving the Bellman optimality equation provides one route to \ufb01nding an optimal policy, and thus to solving the reinforcement learning problem. However, this solution is rarely directly useful. It is akin to an exhaustive search, looking ahead at all possibilities, computing their probabilities of occurrence and their desirabilities in terms of expected rewards", "15955453-9fc4-4c2b-b607-f2c95d856f45": "We have seen that one of the dif\ufb01culties with the Metropolis algorithm is the sensitivity to step size. If this is too small, the result is slow decorrelation due to random walk behaviour, whereas if it is too large the result is inef\ufb01ciency due to a high rejection rate. The technique of slice sampling  provides an adaptive step size that is automatically adjusted to match the characteristics of the distribution. Again it requires that we are able to evaluate the unnormalized distribution \ufffdp(z). Consider \ufb01rst the univariate case. Slice sampling involves augmenting z with an additional variable u and then drawing samples from the joint (z, u) space. We shall see another example of this approach when we discuss hybrid Monte Carlo in Section 11.5. The goal is to sample uniformly from the area under the distribution the region 0 \u2a7d u \u2a7d ep(z(\u03c4)), which then de\ufb01nes a \u2018slice\u2019 through the distribution, shown by the solid horizontal lines.\n\n(b) Because it is infeasible to sample directly from a slice, a new sample of z is drawn from a region zmin \u2a7d z \u2a7d zmax, which contains the previous value z(\u03c4)", "eab87655-7014-443f-926d-0f445155947a": "With Perhaps discounting can be saved by choosing an objective that sums discounted values over the distribution with which states occur under the policy: The proposed discounted objective orders policies identically to the undiscounted (average reward) objective. The discount rate \u03b3 does not in\ufb02uence the ordering! In fact, the lack of a policy improvement theorem is also a theoretical lacuna for the total-episodic and average-reward settings. Once we introduce function approximation we can no longer guarantee improvement for any setting. In Chapter 13 we introduce an alternative class of reinforcement learning algorithms based on parameterized policies, and there we have a theoretical guarantee called the \u201cpolicy-gradient theorem\u201d which plays a similar role as the policy improvement theorem.\n\nBut for methods that learn action values we seem to be currently without a local improvement guarantee  may provide a part of the answer). We do know that \"-greedi\ufb01cation may sometimes result in an inferior policy, as policies may chatter among good policies rather than converge . This is an area with multiple open theoretical questions. In order to generalize to n-step bootstrapping, we need an n-step version of the TD error", "7a129e9a-d111-4826-bdfb-ef2bd434a717": "Instead, we pre-train BERT using two unsupervised tasks, described in this section. This step is presented in the left part of Figure 1.\n\nTask #1: Masked LM Intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than either a left-to-right model or the shallow concatenation of a left-toright and a right-to-left model. Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly \u201csee itself\u201d, and the model could trivially predict the target word in a multi-layered context. former is often referred to as a \u201cTransformer encoder\u201d while the left-context-only version is referred to as a \u201cTransformer decoder\u201d since it can be used for text generation. In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a \u201cmasked LM\u201d (MLM), although it is often referred to as a Cloze task in the literature", "d961a491-54e2-4121-aed9-1848671b63f2": "(2.4)  https://www.deeplearningbook.org/contents/linear_algebra.html    The product operation is defined by Cig = do Aine. (2.5)  Note that the standard product of two matrices is not just a matrix containing the product of the individual elements. Such an operation exists and is called the element-wise product, or Hadamard product, and is denoted as A \u00a9 B.\n\nThe dot product between two vectors 2 and y of the same dimensionality is the matrix product \u00ab'y. We can think of the matrix product C = AB as computing C;,; as the dot product between row i of A and column j of B. 32  CHAPTER 2. LINEAR ALGEBRA  Matrix product operations have many useful properties that make mathematical analysis of matrices more convenient. For example, matrix multiplication is distributive:  A(B+C)=AB+ AC. (2.6)  It is also associative: A(BC) = (AB)C. (2.7) Matrix multiplication is not commutative (the condition AB = BA does not always hold), unlike scalar multiplication", "925fff5d-7999-47b6-854e-4e25a1bdcc56": "Therefore, assume that trial-type, or state, s is described by a real-valued vector of features x(s) = (x1(s), x2(s), . , xd(s))> where xi(s) = 1 if CSi, the ith component of a compound CS, is present on the trial and 0 otherwise.\n\nThen if the d-dimensional vector of associative strengths is w, the aggregate associative strength for trial-type s is Now temporally let t denote the number of a complete trial and not its usual meaning as a time step (we revert to t\u2019s usual meaning when we extend this to the TD model below), and assume that St is the state corresponding to trial t. Conditioning trial t updates the associative strength vector wt to wt+1 as follows: where \u21b5 is the step-size parameter, and\u2014because here we are describing the Rescorla\u2013 Wagner model\u2014\u03b4t is the prediction error Rt is the target of the prediction on trial t, that is, the magnitude of the US, or in Rescorla and Wagner\u2019s terms, the associative strength that the US on the trial can support", "4cdc32a3-5fb3-4875-9980-c95d3cdf860c": "The fovea only observes an area about the size of a thumbnail held at arms length. Though we feel as if we can see an entire scene in high resolution, this is an illusion created by the subconscious part of our brain, as it stitches together several glimpses of small areas. Most convolutional networks actually receive large full-resolution photographs as input. The human brain makes several eye movements called saccades to glimpse the most visually salient or task-relevant parts of a scene. Incorporating similar attention mechanisms into deep learning models is an active research direction. In the context of deep learning, attention mechanisms have been most successful for natural  360  CHAPTER 9. CONVOLUTIONAL NETWORKS  language processing, as described in section 12.4.5.1. Several visual models with foveation mechanisms have been developed but so far have not become the dominant approach . -_ om 1", "5f611151-f9bd-4116-8383-45006870bbe4": "For logistic regression, there is no longer a closed-form solution, due to the nonlinearity of the logistic sigmoid function. However, the departure from a quadratic form is not substantial. To be precise, the error function is concave, as we shall see shortly, and hence has a unique minimum. Furthermore, the error function can be minimized by an ef\ufb01cient iterative technique based on the Newton-Raphson iterative optimization scheme, which uses a local quadratic approximation to the log likelihood function. The Newton-Raphson update, for minimizing a function E(w), takes the form  w(new) = w(old) \u2212 H\u22121\u2207E(w).\n\n(4.92) where H is the Hessian matrix whose elements comprise the second derivatives of E(w) with respect to the components of w. Let us \ufb01rst of all apply the Newton-Raphson method to the linear regression model (3.3) with the sum-of-squares error function (3.12). The gradient and Hessian of this error function are given by which we recognize as the standard least-squares solution", "b5045982-33e1-40ea-ad11-70ed74ca0775": "We have focused on Boltzmann machines trained to approximately maximize the generative criterion log p(v). It is also possible to train discriminative RBMs that aim to maximize log p(y | v) instead . This approach often performs the best when using a linear combination of both the generative and the discriminative criteria. Unfortunately, RBMs do not seem to be as powerful supervised learners as MLPs, at least using existing methodology. https://www.deeplearningbook.org/contents/generative_models.html    Most Boltzmann machines used in practice have only second-order interactions in their energy functions, meaning that their energy functions are the sum of many terms, and each individual term includes only the product between two random variables. An example of such a term is vjWj;,;h;.\n\nIt is also possible to train higher-order Boltzmann machines  whose energy function terms involve the products between many variables. Three-way interactions between a hidden unit and two different images can model spatial transformations from one frame of video to the next . Multiplication by a one-hot class variable can change the relationship between visible and hidden units depending on which class is present", "6139a390-ec8d-40ba-9732-b96c0dee55b8": "we \ufb01t a model comprising a linear combination of Gaussian basis functions to data sets of various sizes and then look at the corresponding posterior distributions. Here the green curves correspond to the function sin(2\u03c0x) from which the data points were generated (with the addition of Gaussian noise). Data sets of size N = 1, N = 2, N = 4, and N = 25 are shown in the four plots by the blue circles. For each plot, the red curve shows the mean of the corresponding Gaussian predictive distribution, and the red shaded region spans one standard deviation either side of the mean. Note that the predictive uncertainty depends on x and is smallest in the neighbourhood of the data points. Also note that the level of uncertainty decreases as more data points are observed. The plots in Figure 3.8 only show the point-wise predictive variance as a function of x. In order to gain insight into the covariance between the predictions at different values of x, we can draw samples from the posterior distribution over w, and then plot the corresponding functions y(x, w), as shown in Figure 3.9", "cb41a9fe-a48b-45a3-b5f1-e52ae19c55e3": "from the data distribution (x\u2217, y\u2217) \u223c pd(x, y).\n\nFor an arbitrary con\ufb01guration (x0, y0), its probability pd(x0, y0) under the data distribution can be seen as measuring the expected similarity between (x0, y0) and true data samples (x\u2217, y\u2217), and be written as pd(x0, y0) = Epd(x\u2217,y\u2217) \ufffd I(x\u2217,y\u2217)(x0, y0) \ufffd . Here the similarity measure is I(x\u2217,y\u2217)(x, y), an indicator function that takes the value 1 if (x, y) equals (x\u2217, y\u2217) and 0 otherwise (we will see other similarity measures shortly). In practice, we are given an empirical distribution \u02dcpd(x, y) by observing a collection of instances D on which the expected similarity is evaluated: The experience function f accommodates the data instance experience straightforwardly as below: With this from of f, we show that the SE derives the conventional supervised MLE algorithm. Supervised MLE", "8b6a1539-8a70-463c-a9ef-343a98cd97ed": "Compared to the vanilla and Wasserstein GANs above, the fake samples from the generator are now weighted by the exponentiated discriminator score exp{f\u03c6(n)(t)} when used to update the discriminator. Intuitively, the mechanism assigns higher weights to samples that can fool the discriminator better, while low-quality samples are downplayed to avoid degrading the discriminator performance during training.\n\nBesides the generative adversarial learning, in Section 4 we brie\ufb02y mentioned that many of the conventional experience can also bene\ufb01t from the idea of introducing adaptive or learnable components, for example, data instances with automatically induced data weights or learned augmentation 1We can alternatively derive the objective from Equation 6.3, by setting \u03b1 = \u03b2 = 1, D to the cross entropy, and H to the Shannon entropy. Note that KL(q\u2225p\u03b8) = \u2212H(q) \u2212 Eq. Thus the solution of q can be derived as in Equation 3.3. policies (Section 4.1.4). We discuss more in Section 9.2 about how the uni\ufb01ed SE as the basis can e\ufb00ortlessly derive e\ufb03cient approaches for joint model and experience optimization. 6.1.2. Online Experience From the Environment", "c2f5cb61-aa79-48ff-81e9-5d6289d8fc34": "Algorithm 7.3 Meta-algorithm using early stopping to determine at what objec- tive value we start to overfit, then continue training until that value is reached. Let X (in) and y(in) be the training set. Split X(train) and yltrain) into (X(subtrain) | X (valid)) and (gysubtrain) ylalid)) respectively. Run early stopping (algorithm 7.1) starting from random @ using X (subtrain) and ysubtrain) for training data and X(lid) and y(lid) for validation data. This updates 6. ee J(0,_X Subtrain) | y(subtrain)) while J(9, X(!D yal) > \u20ac do  Train on Xi\") and yftrain) for n steps.\n\nend while  retraining the model from scratch but is not as well behaved. For example, the objective on the validation set may not ever reach the target value, so this strategy is not even guaranteed to terminate. This procedure is presented more formally in algorithm 7.3", "625a5130-8d65-465c-b06b-d3c5eb0985e9": "We used the validation sets speci\ufb01ed by the dataset creators to select hyperparameters for FGVC Aircraft, PASCAL VOC A Simple Framework for Contrastive Learning of Visual Representations 2007, DTD, and Oxford 102 Flowers. For other datasets, we held out a subset of the training set for validation while performing hyperparameter tuning. After selecting the optimal hyperparameters on the validation set, we retrained the model using the selected parameters using all training and validation images. We report accuracy on the test set. Transfer Learning via a Linear Classi\ufb01er We trained an \u21132-regularized multinomial logistic regression classi\ufb01er on features extracted from the frozen pretrained network. We used L-BFGS to optimize the softmax cross-entropy objective and we did not apply data augmentation.\n\nAs preprocessing, all images were resized to 224 pixels along the shorter side using bicubic resampling, after which we took a 224 \u00d7 224 center crop. We selected the \u21132 regularization parameter from a range of 45 logarithmically spaced values between 10\u22126 and 105", "c005f222-f8d0-426a-9fc2-de6115bbf3be": "Including the term may help the hidden unit activations remain reasonable even when the weights rapidly increase in magnitude. One way to define the energy function on a Gaussian-Bernoulli RBM is thus: 1 E(v,h) = 50 (8 \u00a9v)\u2014(v@B)'Wh-bdb\u2018h, (20.42)  674  CHAPTER 20. DEEP GENERATIVE MODELS  https://www.deeplearningbook.org/contents/generative_models.html    but we may also add extra terms or parametrize the energy in terms of the variance rather than precision if we choose. In this derivation, we have not included a bias term on the visible units, but one could easily be added. One final source of variability in the parametrization of a Gaussian-Bernoulli RBM is the choice of how to treat the precision matrix. It may be either fixed to a constant (perhaps estimated based on the marginal precision of the data) or learned. It may also be a scalar times the identity matrix, or it may be a diagonal matrix", "a7312e43-faa1-4509-8f91-b1f52ff0e544": "For example, TD methods may be relevant to predicting \ufb01nancial data, life spans, election outcomes, weather patterns, animal behavior, demands on power stations, or customer purchases. It was only when TD methods were analyzed as pure prediction methods, independent of their use in reinforcement learning, that their theoretical properties \ufb01rst came to be well understood. Even so, these other potential applications of TD learning methods have not yet been extensively explored. As we outlined in Chapter 1, the idea of TD learning has its early roots in animal learning psychology and arti\ufb01cial intelligence, most notably the work of Samuel  and Klopf . Samuel\u2019s work is described as a case study in Section 16.2. Also related to TD learning are Holland\u2019s  early ideas about consistency among value predictions. These in\ufb02uenced one of the authors (Barto), who was a graduate student from 1970 to 1975 at the University of Michigan, where Holland was teaching", "d2319b70-5df8-45a2-91fa-0d2d7c2f7b83": "If \u21b5 is the step size of the primary learning process, and \u03b2 is the step size of the secondary learning process, then these convergence proofs will typically require that in the limit \u03b2 ! 0 and \u21b5 Gradient-TD methods are currently the most well understood and widely used stable o\u21b5-policy methods. There are extensions to action values and control , to eligibility traces (GTD(\u03bb) and GQ(\u03bb), Maei, 2011; Maei and Sutton, 2010), and to nonlinear function approximation . There have also been proposed hybrid algorithms midway between semi-gradient TD and gradient TD .\n\nHybrid-TD algorithms behave like Gradient-TD algorithms in states where the target and behavior policies are very di\u21b5erent, and behave like semigradient algorithms in states where the target and behavior policies are the same. Finally, the Gradient-TD idea has been combined with the ideas of proximal methods and control variates to produce more e\ufb03cient methods . We turn now to the second major strategy that has been extensively explored for obtaining a cheap and e\ufb03cient o\u21b5-policy learning method with function approximation", "7517184e-d02c-4915-9143-1e12e60fd9f2": "Berns, McClure, Pagnoni, and Montague , Breiter, Aharon, Kahneman, Dale, and Shizgal , Pagnoni, Zink, Montague, and Berns , and O\u2019Doherty, Dayan, Friston, Critchley, and Dolan  described functional brain imaging studies supporting the existence of signals like TD errors in the human brain. 15.6 This section roughly follows Barto  in explaining how TD errors mimic the main results from Schultz\u2019s group on the phasic responses of dopamine neurons. 15.7 This section is largely based on Takahashi, Schoenbaum, and Niv  and Niv . To the best of our knowledge, Barto  and Houk, Adams, and Barto  \ufb01rst speculated about possible implementations of actor\u2013critic algorithms in the basal ganglia.\n\nOn the basis of functional magnetic resonance imaging of human subjects while engaged in instrumental conditioning, O\u2019Doherty, Dayan, Schultz, Deichmann, Friston, and Dolan  suggested that the actor and the critic are most likely located respectively in the dorsal and ventral striatum", "69a40989-7a9f-43f8-bab1-f87faf3b41e4": "This kind of model is described in detail in section 20.5.1; for the present discussion, it suffices to know that it is a model that provides an explicit Pmodei(x; 0). When the RBM is trained using denoising score matching , its learning algorithm is equivalent to denoising training in the corresponding autoencoder. With a fixed noise level, regularized score matching is not a consistent estimator; it instead recovers a blurred version of the distribution. If the noise level is chosen to approach 0 when the number of examples approaches infinity, however, then consistency is recovered. Denoising score matching is discussed in more detail in section 18.5. Other connections between autoencoders and RBMs exist. Score matching applied to RBMs yields a cost function that is identical to reconstruction error combined with a regularization term similar to the contractive penalty of the CAE .\n\nBengio and Delalleau  showed that an autoen- coder gradient provides an approximation to contrastive divergence training of RBMs", "825de930-fc0c-4c4a-a301-826098c240e2": "Noting that z and r are independent in the distribution p(z, r), we see that the conditional distribution p(r|z) is a Gaussian from which it is straightforward to sample. Exercise 11.16 In a practical application of this approach, we have to address the problem of performing a numerical integration of the Hamiltonian equations. This will necessarily introduce numerical errors and so we should devise a scheme that minimizes the impact of such errors. In fact, it turns out that integration schemes can be devised for which Liouville\u2019s theorem still holds exactly. This property will be important in the hybrid Monte Carlo algorithm, which is discussed in Section 11.5.2.\n\nOne scheme for achieving this is called the leapfrog discretization and involves alternately updating discrete-time approximations \ufffdz and \ufffdr to the position and momentum variables using We see that this takes the form of a half-step update of the momentum variables with step size \u03f5/2, followed by a full-step update of the position variables with step size \u03f5, followed by a second half-step update of the momentum variables", "235615e5-c4d7-4af5-90f0-2efc4fe0966f": "If \" = 0.1, then these two policies achieve a value (at the start state) of less than \u221244 and \u221282, respectively, as shown in the graph. A method can do signi\ufb01cantly better if it can learn a speci\ufb01c probability with which to select right. The best probability is about 0.59, which achieves a value of about \u221211.6. Perhaps the simplest advantage that policy parameterization may have over actionvalue parameterization is that the policy may be a simpler function to approximate. Problems vary in the complexity of their policies and action-value functions. For some, the action-value function is simpler and thus easier to approximate. For others, the policy is simpler. In the latter case a policy-based method will typically learn faster and yield a superior asymptotic policy .\n\nFinally, we note that the choice of policy parameterization is sometimes a good way of injecting prior knowledge about the desired form of the policy into the reinforcement learning system. This is often the most important reason for using a policy-based learning method. In addition to the practical advantages of policy parameterization over \"-greedy action selection, there is also an important theoretical advantage", "5f999b31-c646-4f44-86d2-9eb5359cb0e7": "Many numerical packages provide for the evaluation of a closely related function de\ufb01ned by and known as the erf function or error function (not to be confused with the error function of a machine learning model). It is related to the probit function by Exercise 4.21 The generalized linear model based on a probit activation function is known as probit regression. We can determine the parameters of this model using maximum likelihood, by a straightforward extension of the ideas discussed earlier. In practice, the results found using probit regression tend to be similar to those of logistic regression. We shall, however, \ufb01nd another use for the probit model when we discuss Bayesian treatments of logistic regression in Section 4.5. One issue that can occur in practical applications is that of outliers, which can arise for instance through errors in measuring the input vector x or through mislabelling of the target value t", "5468c0c2-29b9-4192-85f7-aa723f43ef72": "This strategy avoids the high cost of  245  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  Algorithm 7.2 A meta-algorithm for using early stopping to determine how long to train, then retraining on all the data. Let X (ain) and y(train) be the training set. Split Xx (train) and yltrain) into (X(subtrain) | X (valid) and (y(subtrain) | yoralid))  respectively. (oubtrain subtrain  ~ + . oo wos \u2014 . * + a . ww  https://www.deeplearningbook.org/contents/regularization.html    Kapeany stopping (algorithm /.1) staring trom taygom 7 using and y Gath for training data and ath and yay for validation data. This returns 7 , the optimal number of steps. Set 6 to random values again. Train on X (train) and y(tain) for i* steps", "c60b2d57-2490-4af1-8d02-aaf96221465d": ", \\yiaytens (Zulu ) mtroduced an alternative initialization scheme called Sparse initialization, in which each unit is initialized to have exactly k nonzero weights. The idea is to keep the total amount of input to the unit independent from the number of inputs m without making the magnitude  of individual weight elements shrink with m. Sparse initialization helps to achieve more diversity among the units at initialization time. However, it also imposes a very strong prior on the weights that are chosen to have large Gaussian values. Because it takes a long time for gradient descent to shrink \u201cincorrect\u201d large values, this initialization scheme can cause problems for units, such as maxout units, that have several filters that must be carefully coordinated with each other.\n\nWhen computational resources allow it, it is usually a good idea to treat the initial scale of the weights for each layer as a hyperparameter, and to choose these scales using a hyperparameter search algorithm described in section 11.4.2, such as random search. The choice of whether to use dense or sparse initialization  300  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  can also be made a hyperparameter", "3b6ae05e-63b4-439c-a640-5bbbf76985f9": "convolutional networks) used for the encoders and decoders, trained jointly with AEVB; (ii) time-series models (i.e. dynamic Bayesian networks); (iii) application of SGVB to the global parameters; (iv) supervised models with latent variables, useful for learning complicated noise distributions. Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. 2013.\n\nDavid M Blei, Michael I Jordan, and John W Paisley. Variational Bayesian inference with Stochastic Search. In Proceedings of the 29th International Conference on Machine Learning (ICML-12), pages 1367\u20131374, 2012. Yoshua Bengio and \u00b4Eric Thibodeau-Laufer. Deep generative stochastic networks trainable by backprop. arXiv preprint arXiv:1306.1091, 2013. Luc Devroye. Sample-based non-uniform random variate generation. In Proceedings of the 18th conference on Winter simulation, pages 260\u2013265. ACM, 1986", "7c5740cd-7caf-4443-ac83-1a914c9250f5": "As they stated the hypothesis, it referred to reward prediction errors (RPEs) but not speci\ufb01cally to TD errors; however, their development of the hypothesis made it clear that they were referring to TD errors. The earliest recognition of the TD-error/dopamine connection of which we are aware is that of Montague, Dayan, Nowlan, Pouget, and Sejnowski , who proposed a TD-error-modulated Hebbian learning rule motivated by results on dopamine signaling from Schultz\u2019s group. The connection was also pointed out in an abstract by Quartz, Dayan, Montague, and Sejnowski .\n\nMontague and Sejnowski  emphasized the importance of prediction in the brain and outlined how predictive Hebbian learning modulated by TD errors could be implemented via a di\u21b5use neuromodulatory system, such as the dopamine system. Friston, Tononi, Reeke, Sporns, and Edelman  presented a model of value-dependent learning in the brain in which synaptic changes are mediated by a TD-like error provided by a global neuromodulatory signal (although they did not single out dopamine)", "1f304e75-52b4-4465-8cbf-a120fc62bde4": "Because tanh is similar to the identity function near 0, training a deep neural network g = w!\n\ntanh(U' tanh(V '\u00ab)) resembles training a linear model g =  191  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  w'U!V's as long as the activations of the network can be kept small. This makes training the tanh network easier. Sigmoidal activation functions are more common in settings other than feed- forward networks. Recurrent networks, many probabilistic models, and some autoencoders have additional requirements that rule out the use of piecewise linear activation functions and make sigmoidal units more appealing despite the drawbacks of saturation. 6.3.3. Other Hidden Units  Many other types of hidden units are possible but are used less frequently. In general, a wide variety of differentiable functions perform perfectly well. Many unpublished activation functions perform just as well as the popular ones", "8a9857ee-060c-4779-ac0e-dd552f0ab984": "In addition to avoiding undesirable real-world consequences, learning from simulated experience can make virtually unlimited data available for learning, generally at less cost than needed to obtain real experience, and because simulations typically run much faster than real time, learning can often occur more quickly than if it relied on real experience. Nevertheless, the full potential of reinforcement learning requires reinforcement learning agents to be embedded into the \ufb02ow of real-world experience, where they act, explore, and learn in our world, and not just in their worlds. After all, reinforcement learning algorithms\u2014at least those upon which we focus in this book\u2014are designed to learn online, and they emulate many aspects of how animals are able to survive in nonstationary and hostile environments. Embedding reinforcement learning agents in the real world can be transformative in realizing the promises of arti\ufb01cial intelligence to amplify and extend human abilities.\n\nA major reason for wanting a reinforcement learning agent to act and learn in the real world is that it is often di\ufb03cult, sometimes impossible, to simulate real-world experience with enough \ufb01delity to make the resulting policies, whether derived by reinforcement learning or by other methods, work well\u2014and safely\u2014when directing real actions", "c75a6f11-02da-4589-9213-485f04fbcaed": "The joint distribution for this graph takes the form We shall consider the speci\ufb01c case in which the N nodes represent discrete variables each having K states, in which case each potential function \u03c8n\u22121,n(xn\u22121, xn) comprises an K \u00d7 K table, and so the joint distribution has (N \u2212 1)K2 parameters. Let us consider the inference problem of \ufb01nding the marginal distribution p(xn) for a speci\ufb01c node xn that is part way along the chain. Note that, for the moment, there are no observed nodes.\n\nBy de\ufb01nition, the required marginal is obtained by summing the joint distribution over all variables except xn, so that In a naive implementation, we would \ufb01rst evaluate the joint distribution and then perform the summations explicitly. The joint distribution can be represented as a set of numbers, one for each possible value for x. Because there are N variables each with K states, there are KN values for x and so evaluation and storage of the joint distribution, as well as marginalization to obtain p(xn), all involve storage and computation that scale exponentially with the length N of the chain. We can, however, obtain a much more ef\ufb01cient algorithm by exploiting the conditional independence properties of the graphical model", "babe9ac6-e06d-4c1f-a6e3-d5670db926b7": "By the end of the 1990s, this system deployed by NCR was reading over 10 percent of all the checks in the United States. Later, several OCR and handwriting recognition systems based on convolutional nets were deployed by Microsoft .\n\nSee chapter 12 for more details  https://www.deeplearningbook.org/contents/convnets.html    on such applications and more modern applications of convolutional networks. See 10. et al. for a more in-depth history of convolutional networks up to  Convolutional networks were also used to win many contests. The current intensity of commercial interest in deep learning began when Krizhevsky et al. 365  CHAPTER 9. CONVOLUTIONAL NETWORKS  2012) won the ImageNet object recognition challenge, but convolutional networks had been used to win other machine learning and computer vision contests with ess impact for years earlier. Convolutional nets were some of the first working deep networks trained with back-propagation. It is not entirely clear why convolutional networks succeeded when general back-propagation networks were considered to have failed", "9a04aac0-4eec-4458-be7f-790e7d626b5a": "For every state k of each variable xn (corresponding to column n of the diagram) the function \u03c6(xn) de\ufb01nes a unique state at the previous variable, indicated by the black lines.\n\nThe two paths through the lattice correspond to con\ufb01gurations that give the global maximum of the joint probability distribution, and either of these can be found by tracing back along the black lines in the opposite direction to the arrow. corresponding to the graph shown in Figure 8.38. Suppose we take node xN to be the root node. Then in the \ufb01rst phase, we propagate messages from the leaf node x1 to the root node using which follow from applying (8.94) and (8.93) to this particular graph. The initial message sent from the leaf node is simply The most probable value for xN is then given by Now we need to determine the states of the previous variables that correspond to the same maximizing con\ufb01guration. This can be done by keeping track of which values of the variables gave rise to the maximum state of each variable, in other words by storing quantities given by To understand better what is happening, it is helpful to represent the chain of variables in terms of a lattice or trellis diagram as shown in Figure 8.53", "4f05dd96-820c-44f2-8c0f-6422f2698bd1": "Although the assumption of a linear-Gaussian model leads to ef\ufb01cient algorithms for inference and learning, it also implies that the marginal distribution of the observed variables is simply a Gaussian, which represents a signi\ufb01cant limitation. One simple extension of the linear dynamical system is to use a Gaussian mixture as the initial distribution for z1. If this mixture has K components, then the forward recursion equations (13.85) will lead to a mixture of K Gaussians over each hidden variable zn, and so the model is again tractable. For many applications, the Gaussian emission density is a poor approximation. If instead we try to use a mixture of K Gaussians as the emission density, then the posterior \ufffd\u03b1(z1) will also be a mixture of K Gaussians. However, from (13.85) the posterior \ufffd\u03b1(z2) will comprise a mixture of K2 Gaussians, and so on, with \ufffd\u03b1(zn) being given by a mixture of Kn Gaussians. Thus the number of components grows exponentially with the length of the chain, and so this model is impractical", "208b6b51-c8cb-460f-b968-dd80924344fa": "For example, it can be shown that the linear function used by the support vector machine can  be re-written as m  wlatb=b+ qa'a\u2122, (5.82) i=1 where a is a training example, and a is\u00a5rvector of coefficients. Rewriting the learning algorithm this way enables us to repiate x with the output of a given feature function (a) and the dot product with a function k(aw,2\u00ae) = g(a) (a) called o 4  [ne Mo! . : +.\n\n1 : Tr\\T ure  https://www.deeplearningbook.org/contents/ml.html    a Keruel, \u2018|he* operator represents an Inner product analogous to YW) Ye). For some feature spaces, we may not use literally the vector inner product. In some infinite dimensional spaces, we need to use other kinds of inner products, for example, inner products based on integration rather than summation. A complete  development of these kinds of inner products is beyond the scope of this book", "38236dd8-02c8-4820-80bd-7d3f0d4f0de5": "We test them on different types of benchmark datasets including: (i) news classi\ufb01cation tasks including AG News  and 20 Newsgroup ; (ii) topic classi\ufb01cation tasks including Yahoo Answers  and PubMed news classi\ufb01cation  (iii) inference tasks including MNLI, QNLI and RTE ; (iv) similarity and paraphrase tasks including QQP and MRPC ; and (v) single-sentence tasks including SST-2 and CoLA . For all datasets, we experiment with 10 labeled data points per class 2 in a supervised setup, and an additional 5000 unlabeled data points per class in the semi-supervised setup. We use BERTbase  as the base language model and use the same hyper-parameters across all datasets/methods.\n\nWe utilize accuracy as the evaluation metric for all datasets except for CoLA (which uses Matthews correlation) and PubMed (which uses accuracy and Macro-F1 score). Because the performance can be heavily dependent on the speci\ufb01c datapoints chosen , for each dataset, we sample labeled data from the original dataset with 3 different seeds to form different training sets, and report the average result", "0d5f06cf-6429-4a51-86a9-b7afd4623cf1": "In practice, we could \ufb01rst train the model with only off-policy data for warming up, and then continue with joint on- and off-policy training to further maximize the reward.\n\nWe show broad applications of the proposed RL text generation framework to a variety of problems Algorithm 1 Ef\ufb01cient Soft Q-Learning for Text Generation Input: Q\u03b8 (i.e., generation model logit function f\u03b8 in Eq.1) Reward function r(s, t) Training examples D (for off-policy updates; optional) 1: Initialize \u03b8 and target model parameters \u00af\u03b8 4: Draw a batch of on-policy samples {\u03c4on} by decoding with policy \u03c0\u03b8(at | st) (Eq.4) 5: Compute Q\u03b8(st, at) values (the model logits) and target Q\u00af\u03b8(st, at) for (st, at) \u2208 {\u03c4off} \u222a {\u03c4on} 8: Update the target model parameters \u00af\u03b8 by \u00af\u03b8 \u2190 \u03c1\u00af\u03b8 + (1 \u2212 \u03c1)\u03b8 with update rate \u03c1 Output: The trained Q\u03b8\u2217 and the induced generator \u03c0\u03b8\u2217 where no clean supervision data is available", "36289ee4-9db7-419e-b09a-280587f0627f": "LINEAR ALGEBRA  Depending on the structure of the problem, it may not be possible to design a unique mapping from A to B. If A is taller than it is wide, then it is possible for this equation to have no solution. If A is wider than it is tall, then there could be multiple possible solutions. The Moore-Penrose pseudoinverse enables us to make some headway in these cases. The pseudoinverse of A is defined as a matrix  At= lim(A'A +al)7!A\u2122. (2.46)  Practical algorithms for computing the pseudoinverse are based not on this defini- tion, but rather on the formula  At=vptu |, (2.47) where U, Dand V are the singular value decomposition of A, and the pseudoinverse D* of a diagonal matrix D is obtained by taking the reciprocal of its nonzero elements then taking the transpose of the resulting matrix. When A has more columns than rows, then solving a linear equation using the pseudoinverse provides one of the many possible solutions.\n\nSpecifically, it provides the solution 2 = At y with minimal Euclidean norm ||a||2 among all possible solutions", "1accc04a-0121-4b00-a505-3501d7732d2f": "First we remove the current estimate \ufffdfn(\u03b8) from q(\u03b8) by division using (10.205) to give q\\n(\u03b8), which has mean and inverse variance given by Exercise 10.38 Next we evaluate the normalization constant Zn using (10.206) to give Similarly, we compute the mean and variance of qnew(\u03b8) by \ufb01nding the mean and variance of q\\n(\u03b8)fn(\u03b8) to give Exercise 10.39 has a simple interpretation as the probability of the point xn not being clutter.\n\nThen we use (10.207) to compute the re\ufb01ned factor \ufffdfn(\u03b8) whose parameters are given by This re\ufb01nement process is repeated until a suitable termination criterion is satis\ufb01ed, for instance that the maximum change in parameter values resulting from a complete problem, showing fn(\u03b8) in blue, efn(\u03b8) in red, and q\\n(\u03b8) in green. Notice that the current form for q\\n(\u03b8) controls the range of \u03b8 over which efn(\u03b8) will be a good approximation to fn(\u03b8). pass through all factors is less than some threshold", "f1540ed1-37bd-4347-a4da-ddc91ed015d0": "For example, something like PatchShuffle Regularization could be implemented with a convolution layer.\n\nThis could be achieved by modifying the standard convolution layer parameters such that the padding parameters preserve spatial resolution and the sub- sequent activation layer keeps pixel values between 0 and 255, in contrast to something  like a sigmoid activation which maps pixels to values between 0 and 1. Therefore kernel Shorten and Khoshgoftaar J Big Data  6:60   0.1 0.9 3.1 4.2 0.9 1.3 3.1 4.2 0.5 1.3 0.6 1.2 0.5 0.1 0.6 1.2 4.3 2.7 2.3 0.3 0.4 3.8 2.3 0.3  3.8 0.4 1.9 0.7 2.7 4.3 0.7 1.9  Fig. 5 Examples of applying the PatchShuffle regularization technique   original  original 2x2 4x4 6x6 8x8 2x2 4x4 6x6 8x8  Fig. 6 Pixels in an x n window are randomly shifted with a probability parameter p  filters can be better implemented as a layer of the network rather than as an addition to the dataset through Data Augmentation", "fc7b890a-9b2f-4d2d-b6cb-3c3fe75d6480": "cosine similarity) along with temperature effectively weights different examples, and an appropriate temperature can help the model learn from hard negatives; and 2) unlike cross-entropy, other objective functions do not weigh the negatives by their relative hardness.\n\nAs a result, one must apply semi-hard negative mining  for these loss functions: instead of computing the gradient over all loss terms, one can compute the gradient using semi-hard negative terms (i.e., those that are within the loss margin and closest in distance, but farther than positive examples). To make the comparisons fair, we use the same \u21132 normalization for all loss functions, and we tune the hyperparameters, and report their best results.8 Table 4 shows that, while (semi-hard) negative mining helps, the best result is still much worse than our default NT-Xent loss. 8Details can be found in Appendix B.10. For simplicity, we only consider the negatives from one augmentation view. A Simple Framework for Contrastive Learning of Visual Representations We next test the importance of the \u21132 normalization (i.e. cosine similarity vs dot product) and temperature \u03c4 in our default NT-Xent loss", "dd569a3a-76eb-48e0-a8ba-b057f38e7d27": "If Z is produced by convolving K across V without flipping K, then  Zi juke = S- Vij+m\u20141,k+n\u2014-1Ki,Lmn (9.7)  lym,n  where the summation over 1, m and n is over all values for which the tensor indexing operations inside the summation are valid. In linear algebra notation,  342  CHAPTER 9. CONVOLUTIONAL NETWORKS  we index into arrays using a 1 for the first entry. This necessitates the \u20141 in the above formula. Programming languages such as C and Python index starting from 0, rendering the above expression even simpler. We may want to skip over some positions of the kernel to reduce the computa- tional cost (at the expense of not extracting our features as finely). We can think of this as downsampling the output of the full convolution function", "e7606ab7-2ce9-413b-bf3a-45dc7118a53d": "In particular, the connection of the experience function f with the reward in Section 4.3 naturally inspires us to repurpose known techniques from the fertile reinforcement learning (RL) literature, especially those of learning reward functions such as inverse RL  or learning implicit reward , for learning the experience function f\u03c6 in our problems. For instance, following , one can acquire and update the experience function at each iteration in Equation 6.2 through min\u03c6 \u2212Et\u2217\u223c\u02dcpdata , where q taking the form in Equation 3.3 now depends on \u03c6.\n\nThe resulting procedure induces an importance reweighting scheme that is shown to stabilize the discriminator training in GANs , as well as learn meaningful constraints . Figure 7 provides a demonstration that learning the constraints together with the target model within the dynamic SE framework leads to substantial improvement. It has been a constant aspiration to search for basic principles that unify the di\ufb00erent paradigms in machine learning . Extensive e\ufb00orts have been made to build unifying views of methods on particular fronts", "adf23ba6-7556-4996-9c8d-17a1949e5e5e": "But if the reward prediction error hypothesis is correct\u2014even if it accounts for only some features of a dopamine neuron\u2019s activity\u2014this traditional view of dopamine neuron activity is not entirely correct: phasic responses of dopamine neurons signal reward prediction errors, not reward itself. In reinforcement learning\u2019s terms, a dopamine Reinforcement learning theory and algorithms help reconcile the reward-predictionerror view with the conventional notion that dopamine signals reward. In many of the algorithms we discuss in this book, \u03b4 functions as a reinforcement signal, meaning that it is the main driver of learning. For example, \u03b4 is the critical factor in the TD model of classical conditioning, and \u03b4 is the reinforcement signal for learning both a value function and a policy in an actor\u2013critic architecture (Sections 13.5 and 15.7). Action-dependent forms of \u03b4 are reinforcement signals for Q-learning and Sarsa. The reward signal Rt is a crucial component of \u03b4t\u22121, but it is not the complete determinant of its reinforcing e\u21b5ect in these algorithms", "ce1fbb3c-b196-4ae8-8cf9-f209c7d14b96": "During the 18th century, issues regarding probability arose in connection with gambling and with the new concept of insurance. One particularly important problem concerned so-called inverse probability.\n\nA solution was proposed by Thomas Bayes in his paper \u2018Essay towards solving a problem in the doctrine of chances\u2019, which was published in 1764, some three years after his death, in the Philosophical Transactions of the Royal Society. In fact, Bayes only formulated his theory for the case of a uniform prior, and it was Pierre-Simon Laplace who independently rediscovered the theory in general form and who demonstrated its broad applicability. tion of probability. Consider the example of polynomial curve \ufb01tting discussed in Section 1.1. It seems reasonable to apply the frequentist notion of probability to the random values of the observed variables tn. However, we would like to address and quantify the uncertainty that surrounds the appropriate choice for the model parameters w. We shall see that, from a Bayesian perspective, we can use the machinery of probability theory to describe the uncertainty in model parameters such as w, or indeed in the choice of model itself. Bayes\u2019 theorem now acquires a new signi\ufb01cance", "3f2883b3-c187-439d-965d-142f27e5e576": "Bob\u2019s finishing time depends on Alice\u2019s, because Bob does not have the opportunity to start his lap until Alice has completed hers. If Alice finishes faster, Bob will finish faster, all else being equal. Finally, Carol\u2019s finishing time depends on both her teammates. If Alice is slow, Bob will probably finish late too. As a consequence, Carol will have quite a late starting time and thus is likely to have a late finishing time as well. However, Carol\u2019s finishing time depends only indirectly on Alice\u2019s finishing time via Bob\u2019s. If we already know Bob\u2019s finishing time, we will not be able to estimate Carol\u2019s  559  CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  finishing time better by finding out what Alice\u2019s finishing time was. This means we can model the relay race using only two interactions: Alice\u2019s effect on Bob and Bob\u2019s effect on Carol. We can omit the third, indirect interaction between Alice and Carol from our model.\n\nStructured probabilistic models provide a formal framework for modeling only direct interactions between random variables. This allows the models to have significantly fewer parameters and therefore be estimated reliably from less data", "2d09151b-1e02-43b7-b9b6-c4c47b01e895": "Caspi, R., Billington, R., Ferrer, L., Foerster, H., Fulcher, C.A., Keseler, I.M., Kothari, A., Krummenacker, M., Latendresse, M., Mueller, L.A., Ong, Q., Paley, S., Subhraveti, P., Weaver, D.S., Karp, P.D. : The MetaCyc database of metabolic pathways and enzymes and the BioCyc collection of pathway/genome databases. Nucleic Acids Res. 44(D1), D471\u2013D480  9. Chapelle, O., Sch\u00f6lkopf, B., Zien, A. (eds. ): Semi-Supervised Learning. Adaptive Computation and Machine Learning, MIT Press  10. Corney, D., Albakour, D., Martinez, M., Moussa, S.: What do a million news articles look like? In: Workshop on Recent Trends in News Information Retrieval  11", "d15e0ceb-edfe-44b2-85ab-970f0fad9a54": "In practice, though, we cannot actually represent our Markov chain in terms of a matrix. The number of states that our probabilistic model can visit is exponentially large in the number of variables, so it is infeasible to represent v, A, or the eigenvalues of A. Because of these and other obstacles, we usually do not know whether a Markov chain has mixed.\n\nInstead, we simply run the Markov chain for an amount of time that we roughly estimate to be sufficient, and use  595  CHAPTER 17. MONTE CARLO METHODS  heuristic methods to determine whether the chain has mixed. These heuristic methods include manually inspecting samples or measuring correlations between successive samples. 17.4 Gibbs Sampling  Oe 1 1 a wd \u2018 1 1 c eon od a afa-N 1 1 WW  https://www.deeplearningbook.org/contents/monte_carlo.html    DO lar we nave described flow to draw samples Irom a distribution \u00a5\\) by repeatedly updating x + @ ~ T(x | x). We have not described how to ensure that q(@) is a useful distribution", "f1aa56c5-6999-41fa-8caa-76039b7ab9df": "When we compute any statistic using a finite number of samples, our estimate of the true underlying parameter is uncertain, in the sense that we could have obtained other samples from the same distribution and their statistics would have  125  CHAPTER 5. MACHINE LEARNING BASICS  been different. The expected degree of variation in any estimator is a source of error that we want to quantify. The standard error of the mean is given by  (5.46)  where o? is the true variance of the samples x\u2019.\n\nThe standard error is often estimated by using an estimate of o. Unfortunately, neither the square root of the sample variance nor the square root of the unbiased estimator of the variance provide an unbiased estimate of the standard deviation. Both approaches tend to underestimate the true standard deviation but are still used in practice. The square root of the unbiased estimator of the variance is less of an underestimate. For large m, the approximation is quite reasonable. The standard error of the mean is very useful in machine learning experiments. We often estimate the generalization error by computing the sample mean of the error on the test set. The number of examples in the test set determines the accuracy of this estimate", "f607de7c-b05c-418a-bc2f-9ec4ef1ef014": "Alternately, one can manually search for the best initial scales. A good rule of thumb for choosing the initial scales is to look at the range or standard deviation of activations or gradients on a single minibatch of data. If the weights are too small, the range of activations across the minibatch will shrink as the activations propagate forward through the network. By repeatedly identifying the first layer with unacceptably small activations and increasing its weights, it is possible to eventually obtain a network with reasonable initial activations throughout. If learning is still too slow at this point, it can be useful to look at the range or standard deviation of the gradients as well as the activations. This procedure can in principle be automated and is generally less computationally costly than hyperparameter optimization based on validation set error because it is based on feedback from the behavior of the initial model on a single batch of data, rather than on feedback from a trained model on the validation set.\n\nWhile long used heuristically, this protocol has recently been specified more formally and studied by Mishkin and Matas . So far we have focused on the initialization of the weights. Fortunately, initialization of other parameters is typically easier", "1c57abf9-1b27-4151-9f1f-bfde92d79f91": "In this approach, we parametrize the representation as $(a; 60) and use the optimization algorithm to find the @ that corresponds to a good representation. If we wish, this approach can capture the benefit of the first approach by being highly generic\u2014we do so by using a very broad family \u00a2(a;@). Deep learning can also capture the benefit of the second approach. Human practitioners can encode their knowledge to help generalization by  \u00b0 ae Wh MN iad .\n\non o oom  https://www.deeplearningbook.org/contents/mlp.html    designing families Ye; 9) that they expect will perform well. Lhe advantage is that the human designer only needs to find the right general function family rather than finding precisely the right function. This general principle of improving models by learning features extends beyond the feedforward networks described in this chapter. It is a recurring theme of deep learning that applies to all the kinds of models described throughout this book. Feedforward networks are the application of this principle to learning  166  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  deterministic mappings from \u00ab to y that lack feedback connections", "259e50f3-e6a6-4cd7-ac47-683239a26b21": "Unfortunately, if f() were linear, then the feedforward network as a whole would remain a linear function of its input. Ignoring the intercept terms for the moment, suppose fH (aw) = W'a and f@)(h) =h'w. Then f(x) = a'Ww. We could represent this function as f(x) =a'w' where w\u2019 = Ww. Clearly, we must use a nonlinear function to describe the features. Most neural networks do so using an affine transformation controlled by learned parameters, followed by a fixed nonlinear function called an activation function. We use that strategy here, by defining h =9(W'a\u00ab + 0), where W provides the weights of a linear transformation and c the biases. Previously, to describe a linear regression model, we used a vector of weights and a scalar bias parameter to describe an  168  CHAPTER 6", "3b211048-7c21-4487-97f1-fb8432da2d3f": "The gradients are not only large but also consistent. The second derivative of the rectifying operation is 0 almost everywhere, and the derivative of the rectifying operation is 1 everywhere that the unit is active. This means that the gradient direction is far more useful for learning than it would be with activation functions that introduce second-order effects. Rectified linear units are typically used on top of an affine transformation: h=g(W'x +b). (6.36)  When initializing the parameters of the affine transformation, it can be a good practice to set all elements of b to a small positive value, such as 0.1.\n\nDoing so makes it very likely that the rectified linear units will be initially active for most inputs in the training set and allow the derivatives to pass through. Several generalizations of rectified linear units exist. Most of these general- izations perform comparably to rectified linear units and occasionally perform better. One drawback to rectified linear units is that they cannot learn via gradient- based methods on examples for which their activation is zero. Various generaliza- tions of rectified linear units guarantee that they receive gradient everywhere", "15c4f966-5879-42e0-8eaa-916e4ceeb176": "CHAPTER 15.\n\nREPRESENTATION LEARNING  https://www.deeplearningbook.org/contents/representation.html     states that if u \u00a9 v, then the target function f to be learned has the property that f(u) & f(v) in general. There are many ways of formalizing such an assumption,  but the end result is that if we have an example (a, y) for which we know that f(x) & y, then we choose an estimator f that approximately satisfies these constraints while changing as little as possible when we move to a nearby input x+e. This assumption is clearly very useful, but it suffers from the curse of dimensionality: to learn a target function that increases and decreases many times in many different regions,! we may need a number of examples that is at least as large as the number of distinguishable regions. One can think of each of these regions as a category or symbol: by having a separate degree of freedom for each symbol (or region), we can learn an arbitrary decoder mapping from symbol to value. However, this does not allow us to generalize to new symbols for new regions", "883f0420-4286-457d-9001-9a4d66459194": "4.15 (\u22c6 \u22c6) Show that the Hessian matrix H for the logistic regression model, given by (4.97), is positive de\ufb01nite. Here R is a diagonal matrix with elements yn(1 \u2212 yn), and yn is the output of the logistic regression model for input vector xn. Hence show that the error function is a concave function of w and that it has a unique minimum. 4.16 (\u22c6) Consider a binary classi\ufb01cation problem in which each observation xn is known to belong to one of two classes, corresponding to t = 0 and t = 1, and suppose that the procedure for collecting training data is imperfect, so that training points are sometimes mislabelled. For every data point xn, instead of having a value t for the class label, we have instead a value \u03c0n representing the probability that tn = 1. Given a probabilistic model p(t = 1|\u03c6), write down the log likelihood function appropriate to such a data set. 4.18 (\u22c6) Using the result (4.91) for the derivatives of the softmax activation function, show that the gradients of the cross-entropy error (4.108) are given by (4.109)", "79ce984c-8d7c-4e80-821a-e2ac51c76c8b": "When we accumulate a running average po of some value v\u201c) by applying the update po) + apt) +(1- av, the a parameter is an example of a linear self- connection from pot-) to pe, When a is near one, the running average remembers information about the past for a long time, and when a is near zero, information about the past is rapidly discarded. Hidden units with linear self-connections can behave similarly to such running averages.\n\nSuch hidden units are called leaky  402  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  weet  https://www.deeplearningbook.org/contents/rnn.html    UuLLLLS. Skip connections through @ time steps are a way of ensuring that a unit can always learn to be influenced by a value from @ time steps earlier. The use of a linear self-connection with a weight near one is a different way of ensuring that the  unit can access values from the past. The linear self-connection approach allows this effect to be adapted more smoothly and flexibly by adjusting the real valued a rather than by adjusting the integer-valued skip length", "2e0b8a95-d0f8-41c2-9da0-fe236dce3750": "Boykov, Y., O. Veksler, and R. Zabih . Fast approximate energy minimization via graph cuts. IEEE Transactions on Pattern Analysis and Machine Intelligence 23(11), 1222\u20131239. Choudrey, R. A. and S. J. Roberts . Variational mixture of Bayesian independent component analyzers. Neural Computation 15(1), 213\u2013252. Clifford, P. Markov random \ufb01elds in statistics. In G. R. Grimmett and D. J. A. Welsh (Eds. ), Disorder in Physical Systems. A Volume in Honour of John M. Hammersley, pp. 19\u201332. Oxford University Press. Collins, M., S. Dasgupta, and R. E. Schapire .\n\nA generalization of principal component analysis to the exponential family. In T. G. Dietterich, S", "087257ca-53bc-4791-a2d7-ce8676a5d2f3": "to the activations on the reconstructed input. Recirculation is regarded as more biologically plausible than back-propagation but is rarely used for machine learning applications. 14.1 Undercomplete Autoencoders  Copying the input to the output may sound useless, but we are typically not interested in the output of the decoder. Instead, we hope that training the autoencoder to perform the input copying task will result in h taking on useful properties. One way to obtain useful features from the autoencoder is to constrain h to have a smaller dimension than a. An autoencoder whose code dimension is less than the input dimension is called undercomplete.\n\nLearning an undercomplete representation forces the autoencoder to capture the most salient features of the training data. The learning process is described simply as minimizing a loss function L(x, 9(f(@))), (14.1)  where L is a loss function penalizing g(f(a)) for being dissimilar from g, such as the mean squared error. When the decoder is linear and L is the mean squared error, an undercomplete autoencoder learns to span the same subspace as PCA", "4764e0b6-ae0b-4b1d-b59d-da3981134cdd": "By contrast, suppose that the emission densities p(xn|zn) comprise a mixture of K Gaussians each of which has a mean that is linear in zn. Then even if \ufffd\u03b1(z1) is Gaussian, the quantity \ufffd\u03b1(z2) will be a mixture of K Gaussians, \ufffd\u03b1(z3) will be a mixture of K2 Gaussians, and so on, and exact inference will not be of practical value. We have seen that the hidden Markov model can be viewed as an extension of the mixture models of Chapter 9 to allow for sequential correlations in the data. In a similar way, we can view the linear dynamical system as a generalization of the continuous latent variable models of Chapter 12 such as probabilistic PCA and factor analysis.\n\nEach pair of nodes {zn, xn} represents a linear-Gaussian latent variable model for that particular observation. However, the latent variables {zn} are no longer treated as independent but now form a Markov chain. Because the model is represented by a tree-structured directed graph, inference problems can be solved ef\ufb01ciently using the sum-product algorithm", "6c4b0f8b-a561-4a8e-912a-f6c692404d91": "In our Comments on Terminology at the end of the previous chapter, we said that Rt is like a reward signal in an animal\u2019s brain and not an object or event in the animal\u2019s environment. In reinforcement learning, the reward signal (along with an agent\u2019s environment) de\ufb01nes the problem a reinforcement learning agent is trying to solve. In this respect, Rt is like a signal in an animal\u2019s brain that distributes primary reward to sites throughout the brain. But it is unlikely that a unitary master reward signal like Rt exists in an animal\u2019s brain.\n\nIt is best to think of Rt as an abstraction summarizing the overall e\u21b5ect of a multitude of neural signals generated by many systems in the brain that assess the rewarding or punishing qualities of sensations and states. Reinforcement signals in reinforcement learning are di\u21b5erent from reward signals. The function of a reinforcement signal is to direct the changes a learning algorithm makes in an agent\u2019s policy, value estimates, or environment models", "db5a7171-9100-4c40-8349-f61945761ef8": "We will assume that the data has  145  https://www.deeplearningbook.org/contents/ml.html  CHAPTER 5.\n\nMACHINE LEARNING BASICS  a mean of zero, E{a] =0. If this is not the case, the data can easily be centered by subtracting the mean from all examples in a preprocessing step. The unbiased sample covariance matrix associated with X is given by 1 Var = \u2014\u2014X 1X. (5.85) m\u2014-1  PCA finds a representation (through linear transformation) z = W 'x, where Var is diagonal. In section 2.12, we saw that the principal components of a design matrix X are given by the eigenvectors of X'X. From this view,  x'X=Waw'. (5.86)  In this section, we exploit an alternative derivation of the principal components. The principal components may also be obtained via singular value decomposition (SVD). Specifically, they are the right singular vectors of X. To see this, let W be the right singular vectors in the decomposition X =USW '", "c18aee1e-8714-4423-b32f-5d02e2f7155f": "This latter idea can be interpreted as an ensemble approach, and it helps to reduce generalization error. Other kinds of preprocessing are applied to both the training and the test set with the goal of putting each example into a more canonical form to reduce the amount of variation that the model needs to account for. Reducing the amount of  448  CHAPTER 12. APPLICATIONS  https://www.deeplearningbook.org/contents/applications.html    variation in the data can reduce both generalization error and the size of the model needed to fit the training set.\n\nSimpler tasks may be solved by smaller models, and simpler solutions are more likely to generalize well. Preprocessing of this kind is usually designed to remove some kind of variability in the input data that is easy for a human designer to describe and that the human designer is confident has no relevance to the task. When training with large datasets and large models, this kind of preprocessing is often unnecessary, and it is best to just let the model learn which kinds of variability it should become invariant to. For example, the AlexNet system for classifying ImageNet has only one preprocessing step: subtracting the mean across training examples of each pixel", "3bbc82e2-78bd-4fdf-9eb9-21186fcbecb8": "Qg and @,, are two learning rates for policy and value function parameter updates, respectively. A3C  Asynchronous Advantage Actor-Critic , short for A3C, is a classic policy gradient method with the special focus on parallel training. In A3C, the critics learn the state-value function, V(s; w), while multiple actors are trained in parallel and get synced with global parameters from time to time. Hence, A3C is good for parallel training by default, i.e. on one machine with multi-core CPU. The loss function for state-value is to minimize the mean squared error, J,,(w) = (G; \u2014 V(s; w))? and we use gradient descent to find the optimal w. This state-value function is used as the baseline in the policy gradient update. Here is the algorithm outline:  We have global parameters, 8 and w; similar thread-specific parameters, 8' and w'", "a7386dd2-a622-4d00-b7f8-0a939d22a32a": "Accumulate squared gradient: r+r+g\u00a9g. Compute update: A@ \u2014 Tv \u00a9g. (Division and square root applied element-wise) Apply update: 0+ 6+ A@.\n\nend while  Algorithm 8.5 The RMSProp algorithm Require: Global learning rate \u20ac, decay rate p Require: Initial parameter 0 Require: Small constant 5, usually 10~\u00ae, used to stabilize division by small numbers Initialize accumulation variables r = 0 while stopping criterion not met do Sample a minibatch of m examples from the training set fa, nr 7 (my with corresponding targets y @, Compute gradient: g + +Vo Dd, Lf (e@\u00ae; 0), y). Accumulate squared gradient: r < pr + (1\u2014p)g Og. Compute parameter update: A@ = Ter Og. (ZS applied element-wise)  Apply update: 0+ 6+ A@. end while  CHAPTER 8", "92b066a6-afc4-4247-ac71-82b530fcec54": "We usually assume that the environment is a \ufb01nite MDP. That is, we assume that its state, action, and reward sets, S, A, and R, are \ufb01nite, and that its dynamics are given by a set of probabilities p(s0, r|s, a), for all s 2 S, a 2 A(s), r 2 R, and s0 2 S+ (S+ is S plus a terminal state if the problem is episodic). Although DP ideas can be applied to problems with continuous state and action spaces, exact solutions are possible only in special cases. A common way of obtaining approximate solutions for tasks with continuous states and actions is to quantize the state and action spaces and then apply \ufb01nite-state DP methods. The methods we explore in Part II are applicable to continuous problems and are a signi\ufb01cant extension of that approach. The key idea of DP, and of reinforcement learning generally, is the use of value functions to organize and structure the search for good policies", "0473a98f-d0cc-4750-9402-2e3e1acc397d": "The joint distribution for a directed graph is de\ufb01ned by (11.4). Each sample from the joint distribution is obtained by \ufb01rst setting those variables zi that are in the evidence set equal to their observed values. Each of the remaining variables is then sampled independently from a uniform distribution over the space of possible instantiations. To determine the corresponding weight associated with a sample z(l), we note that the sampling distribution \ufffdq(z) is uniform over the possible choices for z, and that \ufffdp(z|x) = \ufffdp(z), where x denotes the subset of variables that are observed, and the equality follows from the fact that every sample z that is generated is necessarily consistent with the evidence. Thus the weights rl are simply proportional to p(z). Note that the variables can be sampled in any order. This approach can yield poor results if the posterior distribution is far from uniform, as is often the case in practice. An improvement on this approach is called likelihood weighted sampling  and is based on ancestral sampling of the variables. For each variable in turn, if that variable is in the evidence set, then it is just set to its instantiated value", "0d993714-3412-4dad-bf0c-5e518bf10bf1": "Note that when this network is used as an encoder q\u03c6(z|x), then z and x are swapped, and the weights and biases are variational parameters \u03c6. We derived the following marginal likelihood estimator that produces good estimates of the marginal likelihood as long as the dimensionality of the sampled space is low (less then 5 dimensions), and suf\ufb01cient samples are taken.\n\nLet p\u03b8(x, z) = p\u03b8(z)p\u03b8(x|z) be the generative model we are sampling from, and for a given datapoint x(i) we would like to estimate the marginal likelihood p\u03b8(x(i)). The estimation process consists of three stages: 1. Sample L values {z(l)} from the posterior using gradient-based MCMC, e.g. Hybrid Monte Carlo, using \u2207z log p\u03b8(z|x) = \u2207z log p\u03b8(z) + \u2207z log p\u03b8(x|z). 2. Fit a density estimator q(z) to these samples {z(l)}. 3. Again, sample L new values from the posterior", "fb9e7555-053d-4e1a-ad62-2f8876f2ff34": "To see this, we introduce an additional binary random variable o, with p(o = 1|x, y) \u221d exp{Q(x, y)/\u03c1}. Here o = 1 is interpreted as the event that maximum reward is obtained, p(o = 1|x, y) is seen as the \u2018conditional likelihood\u2019, and \u03c1 is the temperature. The goal of learning is to maximize the marginal likelihood of optimality: log p(o = 1), which, however, is intractable to solve. Much like how the standard equation applied to unsupervised MLE provides a surrogate variational objective for the marginal data likelihood (Section 4.1.3), here the standard equation also derives a variational bound for log p(o = 1) (up to a constant factor) with the above speci\ufb01cation of (f, \u03b1, \u03b2): Following the teacher-student procedure in Equation 3.3, the teacher-step produces the q solution: The subsequent student-step involves approximation by \ufb01xing \u03b8 = \u03b8(n) in Q\u03b8(x, y) in the above variational objective, and minimizes only Eq(n)(x,y)  w.r.t. \u03b8. 4.3.2", "11f6b8e5-db27-41e2-9f5b-1e501a059c0d": "This is a slightly more complex situation than that considered so far because different models may have different structure and indeed different dimensionality for the hidden variables Z.\n\nWe cannot therefore simply consider a factorized approximation q(Z)q(m), but must instead recognize that the posterior over Z must be conditioned on m, and so we must consider q(Z, m) = q(Z|m)q(m). We can readily verify the following decomposition based on this variational distribution Exercise 10.10 where the Lm is a lower bound on ln p(X) and is given by Here we are assuming discrete Z, but the same analysis applies to continuous latent variables provided the summations are replaced with integrations. We can maximize Lm with respect to the distribution q(m) using a Lagrange multiplier, with the result Exercise 10.11 However, if we maximize Lm with respect to the q(Z|m), we \ufb01nd that the solutions for different m are coupled, as we expect because they are conditioned on m. We proceed instead by \ufb01rst optimizing each of the q(Z|m) individually by optimization of (10.35), and then subsequently determining the q(m) using (10.36)", "0848f5ea-2304-44e7-a514-de567dea3ad0": "DEEP FEEDFORWARD NETWORKS  Algorithm 6.4 Backward computation for the deep neural network of algo- rithm 6.3, which uses, in addition to the input a, a target y. This computation yields the gradients on the activations a*) for each layer k, starting from the output layer and going backwards to the first hidden layer. From these gradients, which can be interpreted as an indication of how each layer\u2019s output should change to reduce error, one can obtain the gradient on the parameters of each layer.\n\nThe gradients on weights and biases can be immediately used as part of a stochas- tic gradient update (performing the update right after the gradients have been computed) or used with other gradient-based optimization methods. After the forward computation, compute the gradient on the output layer: 9< VoJ =Valy)  fof gnyert; the", "63174ef4-f2e4-4626-ae1f-1e29fe5daaa6": "With this more compact notation, we can write the two fundamental rules of probability theory in the following form. Here p(X, Y ) is a joint probability and is verbalized as \u201cthe probability of X and Y \u201d. Similarly, the quantity p(Y |X) is a conditional probability and is verbalized as \u201cthe probability of Y given X\u201d, whereas the quantity p(X) is a marginal probability and is simply \u201cthe probability of X\u201d. These two simple rules form the basis for all of the probabilistic machinery that we use throughout this book. From the product rule, together with the symmetry property p(X, Y ) = p(Y, X), we immediately obtain the following relationship between conditional probabilities which is called Bayes\u2019 theorem and which plays a central role in pattern recognition and machine learning", "944219de-286a-4bfb-ab44-0269ceeec671": "These distributions are thus invisible to our method of solving for a specific point where the functional  645  CHAPTER 19. APPROXIMATE INFERENCE  derivatives are zero. This is a limitation of the method. Distributions such as the Dirac must be found by other methods, such as guessing the solution and then proving that it is correct. 19.4.3 Continuous Latent Variables  When our graphical model contains continuous latent variables, we can still perform variational inference and learning by maximizing \u00a3. However, we must now use calculus of variations when maximizing \u00a3 with respect to q(h | v). In most cases, practitioners need not solve any calculus of variations problems themselves", "904f7a4b-5035-480f-9983-23dd25ad7499": "These tasks di\u21b5er greatly in their time scales, yet each can be usefully formulated as an MDP that can be solved by planning or learning processes as described in this book. All involve interaction with the world, sequential decision making, and a goal usefully conceived of as accumulating rewards over time, and so all can be formulated as MDPs. Although all these tasks can be formulated as MDPs, one might think that they cannot be formulated as a single MDP. They involve such di\u21b5erent time scales, such di\u21b5erent notions of choice and action! It would be no good, for example, to plan a \ufb02ight across a continent at the level of muscle twitches. Yet for other tasks, grasping, throwing darts, or hitting a baseball, low-level muscle twitches may be just the right level. People do all these things seamlessly without appearing to switch between levels. Can the MDP framework be stretched to cover all the levels simultaneously? Perhaps it can", "94dc5e48-8e11-4ddc-95af-429b32e46fac": "In order to evaluate the predictive Section 4.4 distribution (6.76), we seek a Gaussian approximation to the posterior distribution over aN+1, which, using Bayes\u2019 theorem, is given by where we have used p(tN|aN+1, aN) = p(tN|aN).\n\nThe conditional distribution p(aN+1|aN) is obtained by invoking the results (6.66) and (6.67) for Gaussian process regression, to give We can therefore evaluate the integral in (6.77) by \ufb01nding a Laplace approximation for the posterior distribution p(aN|tN), and then using the standard result for the convolution of two Gaussian distributions", "0cc72ac4-f3e4-4c1d-a808-a1e5d3ad3f90": "Simple random search provides a competitive approach to reinforcement learning In: Advances in neural information processing systems (NIPS); 2018. David GL. Distinctive image features from scale-invariant keypoints. Int J Comput Vis. 2004;2004:91-110. Navneet D, Bill T. Histograms of oriented gradients for human detection. In: CVPR, 2005. Sutton RS, Reinforcement AG. Learning: an introduction. New York: MIT Press; 1998. Mingyang G, Kele X, Bo D, Huaimin W, Lei Z. Learning data augmentation policies using augmented random search. arXiv preprint. 2018. Tran NM, Mathieu S, Hoang TL, Martin W. Automated image data preprocessing with deep reinforcement learning. arXiv preprints. 2018. Hochreiter S. The vanishing gradient problem during learning recurrent neural nets and problem solutions", "1509040f-2eaf-4f39-a6c9-8fe5d3d6d47b": "Following , we aim to control the generation to have one of 7 topics (e.g., \u201cscience\u201d); the generated prompt is prepended to one of 20 input sentences for the pretrained LM to generate continuation sentences. Figure 5 shows the architecture of prompt-based controllable generation. We compare our SQL method with MLE+PG as before. Since the prompt length could impact the generated sentences, we conducted experiments with maximum prompt length 5, 10, and 15. As ablation study, we also evaluate the SQL algorithm with only off-policy updates (i.e., without on-policy exploration), denoted as SQL(off), and compare it with vanilla MLE training. Finally, we also compare with two specialized controllable generation techniques based on pretrained LMs, namely PPLM  and GeDi , following similar procedures using their open-sourced code. We use a distilled GPT2 model5 as the pretrained LM to be controlled.\n\nFor rewards, we use the topic accuracy of the continuation sentences measured by a zero-shot classi\ufb01er, plus the the log-likelihood of continuation sentences as the language quality reward measured by a distilled GPT-2.6 Results", "1179510f-9ae0-4b6e-add4-be436f4dcdeb": "In particular, Samuel\u2019s program was based on Shannon\u2019s minimax procedure to \ufb01nd the best move from the current position.\n\nWorking backward through the search tree from the scored terminal positions, each position was given the score of the position that would result from the best move, assuming that the machine would always try to maximize the score, while the opponent would always try to minimize it. Samuel called this the \u201cbacked-up score\u201d of the position. When the minimax procedure reached the search tree\u2019s root\u2014the current position\u2014it yielded the best move under the assumption that the opponent would be using the same evaluation criterion, shifted to its point of view. Some versions of Samuel\u2019s programs used sophisticated search control methods analogous to what are known as \u201calpha-beta\u201d cuto\u21b5s . Samuel used two main learning methods, the simplest of which he called rote learning. It consisted simply of saving a description of each board position encountered during play together with its backed-up value determined by the minimax procedure", "483ce021-232a-404e-b583-2298eebe48e7": "Below we give a more satisfactory, though more abstract, description of an earliest reward-predicting state.) The latest reward-predicting state in a trial is the state immediately preceding the trial\u2019s rewarding state. This is the state near the far right end of the time line in Figure 15.4. Note that the rewarding state of a trial does not predict the return for that trial: the value of this state would come to predict the return over all the following trials, which here we are assuming to be zero in this episodic formulation. learning.\u2019 Because the reward signal is zero throughout the trial except when the rewarding state is reached, and all the V -values are zero, the TD error is also zero until it becomes R? at the rewarding state. This follows because \u03b4t\u22121 = Rt + Vt \u2212 Vt\u22121 = Rt + 0 \u2212 0 = Rt, which is zero until it equals R? when the reward occurs. Here Vt and Vt\u22121 are respectively the estimated values of the states visited at times t and t \u2212 1 in a trial", "0083354d-c7a1-4e75-90a0-2395c36b1815": "These are the true values; the estimated values can only approximate these because they are constrained by the parameterization. There are two components to the parameter vector w = (w1, w2)>, and the parameterization is as written inside each state. The estimated values of the \ufb01rst two states are given by w1 alone and thus must be the same even though their true values are di\u21b5erent. Similarly, the estimated values of the third and fourth states are given by w2 alone and must be the same even though their true values are di\u21b5erent. Suppose that we are interested in accurately valuing only the leftmost state; we assign it an interest of 1 while all the other states are assigned an interest of 0, as indicated above the states. First consider applying gradient Monte Carlo algorithms to this problem", "6827aa05-5d49-4218-b857-579098796191": "The region where pretrained networks arrive is smaller, suggesting that pretraining reduces the variance of the estimation process, which can in turn reduce the risk of severe over fitting. In other words, unsupervised pretraining initializes neural network parameters into a region that they do not escape, and the results following this initialization are more consistent and less likely to be very bad than without this initialization. Erhan et al. also provide some answers to when pretraining works best\u2014 the mean and variance of the test error were most reduced by pretraining for deeper networks. Keep in mind that these experiments were performed before the invention and popularization of modern techniques for training very deep networks (rectified linear units, dropout and batch normalization) so less is known about the effect of unsupervised pretraining in conjunction with contemporary approaches. An important question is how unsupervised pretraining can act as a regularizer. One hypothesis is that pretraining encourages the learning algorithm to discover features that relate to the underlying causes that generate the observed data.\n\nThis is an important idea motivating many other algorithms besides unsupervised pretraining and is described further in section 15.3. Compared to other forms of unsupervised learning, unsupervised pretraining has the disadvantage of operating with two separate training phases", "b2bf777f-37ab-41ee-8070-dcf3f311ea3e": "Association for Computational Linguistics. Aur\u00b4elie Herbelot and Marco Baroni. 2017.\n\nHigh-risk learning: acquiring new word vectors from tiny data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 304\u2013309, Copenhagen, Denmark. Association for Computational Linguistics. Wei-Ning Hsu, Hao Tang, and James Glass. 2018. Unsupervised adaptation with interpretable disentangled representations for distant conversational speech recognition. Interspeech 2018. Wei-Ning Hsu, Yu Zhang, and James Glass. 2017. Unsupervised domain adaptation for robust speech recognition via variational autoencoder-based data augmentation. 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P. Xing. 2017. Toward controlled generation of text. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1587\u20131596", "ee487110-32bb-447b-9d64-5ae395393902": "Using the general result (4.135) for an integral evaluated using the Laplace approxiIf we substitute for p(t|w\u22c6) and p(w\u22c6|\u03b1) and then set the derivative of the marginal likelihood with respect to \u03b1i equal to zero, we obtain Exercise 7.19 De\ufb01ning \u03b3i = 1 \u2212 \u03b1i\u03a3ii and rearranging then gives we can write the approximate log marginal likelihood in the form This takes the same form as (7.85) in the regression case, and so we can apply the same analysis of sparsity and obtain the same fast learning algorithm in which we fully optimize a single hyperparameter \u03b1i at each step. Figure 7.12 shows the relevance vector machine applied to a synthetic classi\ufb01cation data set. We see that the relevance vectors tend not to lie in the region of the Appendix A decision boundary, in contrast to the support vector machine.\n\nThis is consistent with our earlier discussion of sparsity in the RVM, because a basis function \u03c6i(x) centred on a data point near the boundary will have a vector \u03d5i that is poorly aligned with the training data vector t. One of the potential advantages of the relevance vector machine compared with the SVM is that it makes probabilistic predictions", "eb5fa552-a16a-4e8b-809b-44f0e85907eb": "Learning representations by maximizing mutual information across views. In Advances in Neural Information Processing Systems, pp. 15509\u201315519, 2019. Becker, S. and Hinton, G. E. Self-organizing neural network that discovers surfaces in random-dot stereograms. Nature, 355 :161\u2013163, 1992. Berg, T., Liu, J., Lee, S. W., Alexander, M. L., Jacobs, D. W., and Belhumeur, P. N. Birdsnap: Large-scale \ufb01ne-grained visual categorization of birds. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2019\u20132026. IEEE, 2014. Berthelot, D., Carlini, N., Goodfellow, I., Papernot, N., Oliver, A., and Raffel, C. A. Mixmatch: A holistic approach to semisupervised learning. In Advances in Neural Information Processing Systems, pp", "0fcffe7b-6f16-461e-9388-e4dcbafed802": "Consider a particular instance of such a graph corresponding to a problem with three binary random variables relating to the fuel system on a car, as shown in Figure 8.21.\n\nThe variables are called B, representing the state of a battery that is either charged (B = 1) or \ufb02at (B = 0), F representing the state of the fuel tank that is either full of fuel (F = 1) or empty (F = 0), and G, which is the state of an electric fuel gauge and which indicates either full (G = 1) or empty nodes represent the state of the battery (B), the state of the fuel tank (F) and the reading on the electric fuel gauge (G). See the text for details. (G = 0). The battery is either charged or \ufb02at, and independently the fuel tank is either full or empty, with prior probabilities Given the state of the fuel tank and the battery, the fuel gauge reads full with probabilities given by so this is a rather unreliable fuel gauge! All remaining probabilities are determined by the requirement that probabilities sum to one, and so we have a complete speci\ufb01cation of the probabilistic model. Before we observe any data, the prior probability of the fuel tank being empty is p(F = 0) = 0.1", "168b6023-966f-4da4-97e0-dde1d66a3616": "If we present to the \ufb01lter the set of all possible distributions p(x) over the set of variables x, then the subset of distributions that are passed by the \ufb01lter will be denoted DF, for directed factorization. This is illustrated in Figure 8.25. Alternatively, we can use the graph as a different kind of \ufb01lter by \ufb01rst listing all of the conditional independence properties obtained by applying the d-separation criterion to the graph, and then allowing a distribution to pass only if it satis\ufb01es all of these properties. If we present all possible distributions p(x) to this second kind of \ufb01lter, then the d-separation theorem tells us that the set of distributions that will be allowed through is precisely the set DF. It should be emphasized that the conditional independence properties obtained from d-separation apply to any probabilistic model described by that particular directed graph.\n\nThis will be true, for instance, whether the variables are discrete or continuous or a combination of these. Again, we see that a particular graph is describing a whole family of probability distributions", "926a0526-1178-488e-b818-e0b0599b8946": "Historically, there have been numerous e\ufb00orts in establishing a uni\ufb01ed machine learning framework that can bridge these complementary paradigms so that advantages in model design, solver e\ufb03ciency, side-information incorporation, and theoretical guarantees can be translated across paradigms. As a prelude of our presentation of the \u2018standard equation\u2019 framework toward this goal, here we begin with a recapitulation of the maximum entropy view of statistical learning. By naturally marrying the probabilistic frameworks with the optimization-theoretic frameworks, the maximum entropy viewpoint had played an important historical role in o\ufb00ering the same lens to understanding several popular methodologies such as maximum likelihood learning, Bayesian inference, and large margin learning. 2.1.\n\nMaximum Likelihood Estimation (MLE). We start with the maximum entropy perspective of the maximum likelihood learning. 2.1.1. Supervised MLE. We consider an arbitrary probabilistic model (e.g., a neural network or probabilistic graphical model for, say, language generation) with parameters \u03b8 \u2208 \u0398 to be learned", "da9fca5f-7712-46b7-b830-2b9994a9b618": "The introduction of deep belief networks in 2006 began the current deep learning renaissance. Prior to the introduction of deep belief networks, deep models were considered too difficult to optimize. Kernel machines with convex objective functions dominated the research landscape. Deep belief networks demonstrated that deep architectures can be successful by outperforming kernelized support vector machines on the MNIST dataset .\n\nToday, deep belief networks have mostly fallen out of favor and are rarely used, even compared to other unsupervised or generative learning algorithms, but they are still deservedly recognized for their important role in deep learning history. https://www.deeplearningbook.org/contents/generative_models.html    Deep belief networks are generative models with several layers of latent variables. he latent variables are typically binary, while the visible units may be binary or real. There are no intralayer connections. Usually, every unit in each layer is  connected to every unit in each neighboring layer, though it is possible to construct more sparsely connected DBNs. The connections between the top two layers are undirected. The connections between all other layers are directed, with the arrows pointed toward the layer that is closest to the data. See figure 20.1b for an example", "a84ca409-6ed6-4d3b-ba51-0e09bc01c809": "The advantage of eliminating hidden-to-hidden recurrence is that, for any loss function based on comparing the prediction at time t to the training target at time ft, all the time steps are decoupled. Training can thus be parallelized, with the gradient for each step t computed in isolation.\n\nThere is no need to compute the output for the previous time step first, because the training set provides the ideal value of that output. Models that have recurrent connections from their outputs leading back into the model may be trained with teacher forcing. Teacher forcing is a procedure that emerges from the maximum likelihood criterion, in which during training the model receives the ground truth output y as input at time t+1. We can see this by examining a sequence with two time steps. The conditional maximum  377  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  https://www.deeplearningbook.org/contents/rnn.html    likelihood criterion is log p (yy | a) 2) (10.15)  =logp (y\u00ae | y, a, 22) + log p (y | 2,2)", "73f35d97-df81-4330-9ebf-1bbfd2dee464": "The backup diagram for a compound update consists of the backup diagrams for each of the component updates with a horizontal line above them and the weighting fractions below. For example, the compound update for the case mentioned at the start of this section, mixing half of a two-step return and half of a four-step return, has the diagram shown to the right. A compound update can only be done when the longest of its component updates is complete. The update at the right, for example, could only be done at time t+4 for the estimate formed at time t. In general one would like to limit the length of the longest component update because of the corresponding delay in the updates. The TD(\u03bb) algorithm can be understood as one particular way of averaging n-step updates. This average contains all the n-step updates, each weighted proportionally to \u03bbn\u22121 (where \u03bb 2 ), and is normalized by a factor of 1\u2212\u03bb to ensure that the weights sum to 1 (Figure 12.1).\n\nThe resulting update is toward a return, called the \u03bb-return, de\ufb01ned in its state-based form by we want, we can separate these post-termination terms from the main sum, yielding as indicated in the \ufb01gures", "70f48dcd-f237-4614-8dec-d9340d66d068": "Some software libraries such as Theano  are able to perform some kinds of algebraic substitution to improve over the graph proposed by the pure back-propagation algorithm.\n\nWhen the forward graph G has a single output node and each partial derivative ous can be computed with a constant amount of computation, back-propagation guarantees that the number of computations for the gradient computation is of  the same order as the number of computations for the forward computation: this du  eye) needs to  can be seen in algorithm 6.2, because each local partial derivative  https://www.deeplearningbook.org/contents/mlp.html    be computed only once along with an associated multiplication and addition for the recursive chain-rule formulation (equation 6.53). e overall computation is therefore O(+ edges). It can potentially be reduced, however, by simplifying the computational graph constructed by back-propagation, and this is an NP-complete task. Implementations such as Theano and TensorFlow use heuristics based on matching known simplification patterns to iteratively attempt to simplify the graph", "9613b8eb-621e-4c32-b84e-80854bb171b4": "This is because the earliest models in the stack are encouraged to copy more information up to their latent variables, thereby making it available to the later models. This should be thought of more as an often-exploitable side effect of CD training rather than a  https://www.deeplearningbook.org/contents/partition.html    principled design advantage. Sutskever and Tieleman. showed that the CD update direction is not the gradient of any function. This allows for situations where CD could cycle forever,  but in practice this is not a serious problem. A different strategy that resolves many of the problems with CD is to initial- ize the Markov chains at each gradient step with their states from the previous gradient step. This approach was first discovered under the name stochastic max- imum likelihood (SML) in the applied mathematics and statistics community  and later independently rediscovered under the name persistent contrastive divergence (PCD, or PCD-k to indicate the use of k Gibbs steps per update) in the deep learning community . See algorithm 18.3.\n\nThe basic idea of this approach is that, as long as the steps taken by the stochastic gradient algorithm are small, the model from the previous step will be similar to the model from the current step", "ad176828-9a2e-4cab-9fbc-897278e57a64": "It is composed of an encoder RNN that reads the input sequence as well as a decoder RNN that generates the output sequence (or computes the probability of a given output sequence).\n\nThe final hidden state of the encoder RNN is used to compute a generally fixed-size context variable C, which represents a semantic summary of the input sequence and is given as input to the decoder RNN. 391  https://www.deeplearningbook.org/contents/rnn.html    CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  tecture and were the first to obtain state-of-the-art translation using this approach. The former system is based on scoring proposals generated by another machine translation system, while the latter uses a standalone recurrent network to generate the translations. These authors respectively called this architecture, illustrated in figure 10.12, the encoder-decoder or sequence-to-sequence architecture. The idea is very simple: (1) An encoder or reader or input RNN processes the input sequence. The encoder emits the context C\u2019, usually as a simple function of its final hidden state", "712049d0-219c-457e-bbf3-ed945386c547": "Uncovering structure in an agent\u2019s experience can certainly be useful in reinforcement learning, but by itself does not address the reinforcement learning problem of maximizing a reward signal. We therefore consider reinforcement learning to be a third machine learning paradigm, alongside supervised learning and unsupervised learning and perhaps other paradigms. One of the challenges that arise in reinforcement learning, and not in other kinds of learning, is the trade-o\u21b5 between exploration and exploitation.\n\nTo obtain a lot of reward, a reinforcement learning agent must prefer actions that it has tried in the past and found to be e\u21b5ective in producing reward. But to discover such actions, it has to try actions that it has not selected before. The agent has to exploit what it has already experienced in order to obtain reward, but it also has to explore in order to make better action selections in the future. The dilemma is that neither exploration nor exploitation can be pursued exclusively without failing at the task. The agent must try a variety of actions and progressively favor those that appear to be best. On a stochastic task, each action must be tried many times to gain a reliable estimate of its expected reward. The exploration\u2013exploitation dilemma has been intensively studied by mathematicians for many decades, yet remains unresolved", "60d2081c-2821-4bef-bd44-33f432ffa98c": "State features are not restricted to describing the external stimuli that an animal experiences; they can describe neural activity patterns that external stimuli produce in an animal\u2019s brain, and these patterns can be history-dependent, meaning that they can be persistent patterns produced by sequences of external stimuli. Of course, we do not know exactly what these neural activity patterns are, but a real-time model like the TD model allows one to explore the consequences on learning of di\u21b5erent hypotheses about the internal representations of external stimuli. For these reasons, the TD model does not commit to any particular state representation.\n\nIn addition, because the TD model includes discounting and eligibility traces that span time intervals between stimuli, the model also makes it possible to explore how discounting and eligibility traces interact with stimulus representations in making predictions about the results of classical conditioning experiments. Below we describe some of the state representations that have been used with the TD model and some of their implications, but for the moment we stay agnostic about the representation and just assume that each state s is represented by a feature vector x(s) = (x1(s), x2(s), . , xn(s))>", "1873ebbb-7028-411b-84c8-c189487fc4f3": "(Recall that in linear function approximation, r\u02c6v(St,wt) is just the feature vector, xt, in which case the eligibility trace vector is just a sum of past, fading, input vectors.) The trace is said to indicate the eligibility of each component of the weight vector for undergoing learning changes should a reinforcing event occur. The reinforcing events we are concerned with are the moment-by-moment one-step TD errors. The TD error for state-value prediction is In TD(\u03bb), the weight vector is updated on each step proportional to the scalar TD error and the vector eligibility trace: Input: the policy \u21e1 to be evaluated Input: a di\u21b5erentiable function \u02c6v : S+ \u21e5 Rd ! R such that \u02c6v(terminal,\u00b7) = 0 Algorithm parameters: step size \u21b5 > 0, trace decay rate \u03bb 2  Initialize value-function weights w arbitrarily (e.g., w = 0) TD(\u03bb) is oriented backward in time", "5e1aa931-f356-41e9-baf0-2300de1e7a85": "Responding to multiple distances in the same direction requires hidden units with collinear weight vectors but different biases. Such coordination can be difficult for the learning algorithm to discover. Additionally, many shallow graphical models have problems with representing multiple separated modes along the same line. GCN avoids these problems by reducing each example to a direction rather than a direction and a distance. Counterintuitively, there is a preprocessing operation known as sphering that is not the same operation as GCN. Sphering does not refer to making the data lie on a spherical shell, but rather refers to rescaling the principal components to have equal variance, so that the multivariate normal distribution used by PCA has spherical contours. Sphering is more commonly known as whitening. Global contrast normalization will often fail to highlight image features we would like to have stand out, such as edges and corners.\n\nIf we have a scene with a large dark area and a large bright area (such as a city square with half the image in the shadow of a building), then global contrast normalization will ensure that there is a large difference between the brightness of the dark area and the brightness of the light area", "4a015c30-68dc-459b-a6bb-317ac33a7d58": "The resulting decision boundaries correspond to surfaces along which the posterior probabilities p(Ck|x) are constant and so will be given by linear functions of x, and therefore the decision boundaries are linear in input space. The prior probabilities p(Ck) enter only through the bias parameter w0 so that changes in the priors have the effect of making parallel shifts of the decision boundary and more generally of the parallel contours of constant posterior probability. For the general case of K classes we have, from (4.62) and (4.63), We see that the ak(x) are again linear functions of x as a consequence of the cancellation of the quadratic terms due to the shared covariances. The resulting decision boundaries, corresponding to the minimum misclassi\ufb01cation rate, will occur when two of the posterior probabilities (the two largest) are equal, and so will be de\ufb01ned by linear functions of x, and so again we have a generalized linear model.\n\nIf we relax the assumption of a shared covariance matrix and allow each classconditional density p(x|Ck) to have its own covariance matrix \u03a3k, then the earlier cancellations will no longer occur, and we will obtain quadratic functions of x, giving rise to a quadratic discriminant", "596eb527-93f9-4f1a-ba96-257aa7a92750": "We thus trade-off expressivity and ef\ufb01ciency by allowing users to write labeling functions at two levels of abstraction: custom Python functions and declarative operators.\n\nHand-De\ufb01ned Labeling Functions In its most general form, a labeling function is just an arbitrary snippet of code, usually written in Python, which accepts as input a Candidate object and either outputs a label or abstains. Often these functions are similar to extract\u2013transform\u2013load scripts, expressing basic patterns or heuristics, but may use supporting code or resources and be arbitrarily complex. Writing labeling functions by hand is supported by the ORM layer, which maps the context hierarchy and associated metadata to an object-oriented syntax, allowing the user to easily traverse the structure of the input data. Example 2.3 In our running example, we can write a labeling function that checks if the word \u201ccauses\u201d appears between the chemical and disease mentions. If it does, it outputs True if the chemical mention is \ufb01rst and False if the disease mention is \ufb01rst", "ae2cc97f-2593-4619-9b64-f5a417878ae7": "Xie, Q., Dai, Z., Hovy, E., Luong, M.-T., and Le, Q. V. Unsupervised data augmentation. arXiv preprint arXiv:1904.12848, 2019. Ye, M., Zhang, X., Yuen, P. C., and Chang, S.-F. Unsupervised embedding learning via invariant and spreading instance feature. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6210\u20136219, 2019. You, Y., Gitman, I., and Ginsburg, B.\n\nLarge batch training of convolutional networks. arXiv preprint arXiv:1708.03888, 2017. Zhai, X., Oliver, A., Kolesnikov, A., and Beyer, L. S4l: Selfsupervised semi-supervised learning. In The IEEE International Conference on Computer Vision (ICCV), October 2019. Zhang, R., Isola, P., and Efros, A. A", "3901c8af-5701-4981-8129-69f07b98126c": "On the asymptotic behaviour of posterior distributions. Journal of the Royal Statistical Society, B 31(1), 80\u201388. Walker, S. G., P. Damien, P. W. Laud, and A. F. M. Smith . Bayesian nonparametric inference for random distributions and related functions (with discussion).\n\nJournal of the Royal Statistical Society, B 61(3), 485\u2013527. Watson, G. S. Smooth regression analysis. Sankhy\u00afa: The Indian Journal of Statistics. Series A 26, 359\u2013372. Webb, A. R. Functional approximation by feed-forward networks: a least-squares approach to generalisation. IEEE Transactions on Neural Networks 5(3), 363\u2013371. Weisstein, E. W. CRC Concise Encyclopedia of Mathematics. Chapman and Hall, and CRC. Weston, J. and C. Watkins", "fe27b7a7-80ed-47f6-9583-af3c4759f67e": "arXiv preprint. 2016. Philip TJ, Amir AA, Stephen B, Toby B, Boguslaw O. Style augmentation: data augmentation via style randomiza- tion. arXiv e-prints. 2018. Josh T, Rachel F, Alex R, Jonas S, Wojciech Z, Pieter A. Domain randomization for transferring deep neural networks from simulation to the real world. arXiv preprint. 2017. Ashish S, Tomas P, Oncel T, Josh S, Wenda W, Russ W. Learning from simulated and unsupervised images through adversarial training. In: Conference on computer vision and pattern recognition, 2017. Stephan RR, Vibhav V, Stefan R, Viadlen K. Playing for data: ground truth from computer games. In: European. conference on computer vision (ECCV); 2016. Brostow Gabriel J, Fauqueur Julien, Cipolla Roberto. Semantic object classes in video: a high-definition ground truth database", "1705607b-d56a-4074-80b1-59bdd3ce8d4c": "For classi\ufb01cation problems, however, we wish to predict discrete class labels, or more generally posterior probabilities that lie in the range (0, 1).\n\nTo achieve this, we consider a generalization of this model in which we transform the linear function of w using a nonlinear function f( \u00b7 ) so that In the machine learning literature f( \u00b7 ) is known as an activation function, whereas its inverse is called a link function in the statistics literature. The decision surfaces correspond to y(x) = constant, so that wTx + w0 = constant and hence the decision surfaces are linear functions of x, even if the function f(\u00b7) is nonlinear. For this reason, the class of models described by (4.3) are called generalized linear models . Note, however, that in contrast to the models used for regression, they are no longer linear in the parameters due to the presence of the nonlinear function f(\u00b7). This will lead to more complex analytical and computational properties than for linear regression models. Nevertheless, these models are still relatively simple compared to the more general nonlinear models that will be studied in subsequent chapters", "fcb8dfc1-8185-40d3-a62b-176889638828": "(b) Initial configurat\"'\" 01 too principalsul>sl>a<:<t defined by W, shown in md, tOO\"lhfIr with the fK'(Ijeclions 01 the latll<11 points Z inlo too <lata space, giItoo by ZWT , shown in cyan, (oj Alter\"\"\" M step, too laten! SI'B\u00abl P>as been update<! wiIh Z r>el(l nxed. (d) Me' tt>e success.... E Slep, It>e \"\"'-'eo 01 Z havu been up<!atll<:1. ~ '\" Ihogoooal r>rojecliQn$, with W h&k! fixed. (e) Aft...", "973ab367-a862-4d4f-bf91-0ce258e4560b": "While the results suggest that the family of algorithms with adaptive learning rates (represented by RMSProp and AdaDelta)  Algorithm 8.7 The Adam algorithm  Require: Step size \u00ab (Suggested default: 0.001) Require: Exponential decay rates for moment estimates, p, and p2 in [0,1).\n\n(Suggested defaults: 0.9 and 0.999 respectively) Require: Small constant 6 used for numerical stabilization (Suggested default: 10-8) Require: Initial parameters 0 Initialize 1st and 2nd moment variables s = 0, r = 0 Initialize time step t = 0 while stopping criterion not met do Sample a minibatch of m examples from the training set {a), a (m) with corresponding targets y (i), Compute gradient: g + +Ve Dd, Lf (e; 8), y) tHt+l1 Update biased first moment estimate: s + p18 + (1\u2014 pi)g Update biased second moment estimate: r + por + (1\u2014 p2)g Og Correct bias in first moment: $ <\u2014 Tce  Correct bias in second moment: 7 + TE 2  Compute update: A@ = mean, (operations applied element-wise) Apply update: 6+ 6+ A@ end while  306  CHAPTER 8", "de29bc3d-b796-465e-a72c-bd2b1e6d47bc": "This is the idea that one might decide on a step-by-step basis whether one wanted to take the action as a sample, as in Sarsa, or consider the expectation over all actions instead, as in the tree-backup update. Then, if one chose always to sample, one would obtain Sarsa, whereas if one chose never to sample, one would get the tree-backup algorithm. Expected Sarsa would be the case where one chose to sample for all steps except for the last one. And of course there would be many other possibilities, as suggested by the last diagram in the \ufb01gure.\n\nTo increase the possibilities even further we can consider a continuous variation between sampling and expectation. Let \u03c3t 2  denote the degree of sampling on step t, with \u03c3 = 1 denoting full sampling and \u03c3 = 0 denoting a pure expectation with no sampling. The random variable \u03c3t might be set as a function of the state, action, or state\u2013action pair at time t. We call this proposed new algorithm n-step Q(\u03c3). Now let us develop the equations of n-step Q(\u03c3)", "685da0f6-8e6d-42b9-8520-c8a323b0c61e": "(20.81)  This drives the discriminator to attempt to learn to correctly classify samples as real  https://www.deeplearningbook.org/contents/generative_models.html    or fake. Simultaneously, the generator attempts to fool the classifier into believing its samples are real. At convergence, the generator\u2019s samples are indistinguishable from real data, and the discriminator outputs 5 everywhere. The discriminator may then be discarded. The main motivation for the design of GANs is that the learning process requires neither approximate inference nor approximation of a partition function gradient. When maxgv(g, d) is convex in 6 (such as the case where optimization is performed directly in the space of probability density functions), the procedure is guaranteed to converge and is asymptotically consistent. Unfortunately, learning in GANs can be difficult in practice when g and d are represented by neural networks and max, v(g,d) is not convex. Goodfellow  identified nonconvergence as an issue that may cause GANs to underfit", "9cc33927-0870-46a2-a2d3-5ed01a9d8fa4": "However, in a batch setting we have the opportunity to re-use the data points many times in order to achieve improved accuracy, and it is this idea that is exploited in expectation propagation.\n\nFurthermore, if we apply ADF to batch data, the results will have an undesirable dependence on the (arbitrary) order in which the data points are considered, which again EP can overcome. One disadvantage of expectation propagation is that there is no guarantee that the iterations will converge. However, for approximations q(\u03b8) in the exponential family, if the iterations do converge, the resulting solution will be a stationary point of a particular energy function , although each iteration of EP does not necessarily decrease the value of this energy function. This is in contrast to variational Bayes, which iteratively maximizes a lower bound on the log marginal likelihood, in which each iteration is guaranteed not to decrease the bound. It is possible to optimize the EP cost function directly, in which case it is guaranteed to converge, although the resulting algorithms can be slower and more complex to implement", "a91f4c54-b05f-4574-a37a-838e2b508800": "This is reminiscent of the choice of model complexity in polynomial curve \ufb01tting discussed in Chapter 1 where the degree M of the polynomial, or alternatively the value \u03b1 of the regularization parameter, was optimal for some intermediate value, neither too large nor too small. Armed with these insights, we turn now to a discussion of two widely used nonparametric techniques for density estimation, kernel estimators and nearest neighbours, which have better scaling with dimensionality than the simple histogram model. Let us suppose that observations are being drawn from some unknown probability density p(x) in some D-dimensional space, which we shall take to be Euclidean, and we wish to estimate the value of p(x). From our earlier discussion of locality, let us consider some small region R containing x", "d49b534d-ff7b-4e1f-a521-df94161b6037": "If we attempted to represent p(to,t1,t2) with a table, it would need to store 999,999 values (100 values of to x 100 values of t1 x 100 values of tg, minus 1, since the probability of one of the configurations is made redundant by the constraint that the sum of the probabilities be 1).\n\nIf instead, we  make a table only for each of the conditional probability distributions, then the distribution over tg requires 99 values, the table defining t; given to requires 9,900 values, and so does the table defining tg given t1. This comes to a total of 19,899 values. This means that using the directed graphical model reduced our number of parameters by a factor of more than 50! 561  CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  In general, to model n discrete variables each having k values, the cost of the single table approach scales like O(k\"), as we have observed before. Now suppose we build a directed graphical model over these variables", "0e9d5cd0-96ca-4503-8372-1e5e786465d4": "That is, \u03b4t is the error in V (St), available at time t + 1.\n\nAlso note that if the array V does not change during the episode (as it does not in Monte Carlo methods), then the Monte Carlo error can be written as a sum of TD errors: = \u03b4t + \u03b3\u03b4t+1 + \u03b32\u03b4t+2 + \u00b7 \u00b7 \u00b7 + \u03b3T \u2212t\u22121\u03b4T \u22121 + \u03b3T \u2212t\ufffd = \u03b4t + \u03b3\u03b4t+1 + \u03b32\u03b4t+2 + \u00b7 \u00b7 \u00b7 + \u03b3T \u2212t\u22121\u03b4T \u22121 + \u03b3T \u2212t\ufffd This identity is not exact if V is updated during the episode (as it is in TD(0)), but if the step size is small then it may still hold approximately. Generalizations of this identity play an important role in the theory and algorithms of temporal-di\u21b5erence learning. Exercise 6.1 If V changes during the episode, then (6.6) only holds approximately; what would the di\u21b5erence be between the two sides? Let Vt denote the array of state values used at time t in the TD error (6.5) and in the TD update (6.2)", "94f18545-8768-4012-87f4-8280506a3e32": "We therefore see that the mixing coef\ufb01cients satisfy the requirements to be probabilities. From the sum and product rules, the marginal density is given by which is equivalent to (2.188) in which we can view \u03c0k = p(k) as the prior probability of picking the kth component, and the density N(x|\u00b5k, \u03a3k) = p(x|k) as the probability of x conditioned on k. As we shall see in later chapters, an important role is played by the posterior probabilities p(k|x), which are also known as responsibilities. From Bayes\u2019 theorem these are given by We shall discuss the probabilistic interpretation of the mixture distribution in greater detail in Chapter 9. The form of the Gaussian mixture distribution is governed by the parameters \u03c0, \u00b5 and \u03a3, where we have used the notation \u03c0 \u2261 {\u03c01, . , \u03c0K}, \u00b5 \u2261 {\u00b51, . , \u00b5K} and \u03a3 \u2261 {\u03a31, . \u03a3K}.\n\nOne way to set the values of these parameters is to use maximum likelihood. From (2.188) the log of the likelihood function is given by where X = {x1,", "29f1077d-1bbb-4a0c-90a7-e8aed89b6021": "Advances in Neural Information Processing Systems, 470\u2013476. Jaynes, E. T. : Comment. The American Statistician, 42(4), 280\u2013281. Retrieved October 27, 2022, from http://www.jstor.org/ stable/2685144 Johnson, M. J., Duvenaud, D., Wiltschko, A. B., Datta, S. R., & Adams, R. P. Composing graphical models with neural networks for structured representations and fast inference. Proceedings of the 30th International Conference on Neural Information Processing Systems, 2954\u20132962. Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., & Saul, L. K. An introduction to variational methods for graphical models. Machine learning, 37(2), 183\u2013233. Knoblauch, J., Jewson, J., & Damoulas, T. .\n\nGeneralized variational inference: Three arguments for deriving new posteriors", "30e95f94-6046-4263-b1c6-f35a8e39a344": "It encourages individual hidden units to learn features that work well with random collections of other features.\n\nThis increases the versatility of the features formed by the hidden units so that the network does not overly specialize to rarely-occurring cases. Hinton, Osindero, and Teh  took a major step toward solving the problem of training the deep layers of a deep ANN in their work with deep belief networks, layered networks closely related to the deep ANNs discussed here. In their method, the deepest layers are trained one at a time using an unsupervised learning algorithm. Without relying on the overall objective function, unsupervised learning can extract features that capture statistical regularities of the input stream. The deepest layer is trained \ufb01rst, then with input provided by this trained layer, the next deepest layer is trained, and so on, until the weights in all, or many, of the network\u2019s layers are set to values that now act as initial values for supervised learning. The network is then \ufb01ne-tuned by backpropagation with respect to the overall objective function. Studies show that this approach generally works much better than backpropagation with weights initialized with random values", "a94c4876-e712-4491-9cd6-c76187934de3": "To complete the match, we use here the same performance measure\u2014an unweighted average of the RMS error over all states and over the \ufb01rst 10 episodes\u2014rather than a VE objective as is otherwise more appropriate when using function approximation. The semi-gradient n-step TD algorithm used in the example above is the natural extension of the tabular n-step TD algorithm presented in Chapter 7 to semi-gradient function approximation. Pseudocode is given in the box below", "d4c8423b-e0a1-4cba-8e1c-bc45ef7f54d0": "Neural network algorithms require the same performance characteristics as the real-time graphics algorithms described above. Neural networks usually involve large and numerous buffers of parameters, activation values, and gradient values, each of which must be completely updated during every step of training. These buffers are large enough to fall outside the cache of a traditional desktop computer, so the memory bandwidth of the system often becomes the rate-limiting factor. GPUs offer a compelling advantage over CPUs because of their high memory bandwidth.\n\nNeural network training algorithms typically do not involve much branching or sophisticated control, so they are appropriate for GPU hardware. Since neural networks can be divided into multiple individual \u201cneurons\u201d that can be processed independently from the other neurons in the same layer, neural networks easily benefit from the parallelism of GPU computing. GPU hardware was originally so specialized that it could be used only for graphics tasks. Over time, GPU hardware became more flexible, allowing custom subroutines to be used to transform the coordinates of vertices or to assign colors to pixels. In principle, there was no requirement that these pixel values actually be based on a rendering task. These GPUs could be used for scientific computing by writing the output of a computation to a buffer of pixel values", "4be7aec8-0cc0-47b9-827f-2dc14a43421a": "In transfer learning, the learner must perform two or more different tasks, but we assume that many of the factors that explain the variations in P, are relevant to the variations that need to be captured for learning P). This is typically understood in a supervised learning context, where the input is the same but the target may be of a different nature. For example, we may learn about one set of visual categories, such as cats and dogs, in the first setting, then learn about a different set of visual categories, such as ants and wasps, in the second setting.\n\nIf there is significantly more data in the first setting (sampled from P;), then that may help to learn representations that are useful to quickly generalize from only very few examples drawn from P2. Many visual categories share low-level notions of edges and visual shapes, the effects of geometric changes, changes in lighting, and so on. In general, transfer learning, multitask learning (section 7.7), and domain adaptation can be achieved via representation learning when there exist features that are useful for the different settings or tasks, corresponding to underlying factors that appear in more than one setting. This is illustrated in figure 7.2, with shared lower layers and task-dependent upper layers", "646b2732-9313-47b6-988d-d76a98b2c445": "If the network has enough hidden units, it can approximate a given nonlinear function to any desired accuracy. The downside of having such a flexible model is that the marginalization over the latent variables, required in order to obtain the likelihood function, is no longer analytically tractable. Instead, the likelihood is approximated using Monte Carlo techniques by drawing samples from the Gaussian prior. The marginalization over the latent variables then becomes a simple sum with one term for each sample. However, because a large number of sample points may be required in order to give an accurate representation of the marginal, this procedure can be computationally costly.\n\nIf we consider more restricted forms for the nonlinear function, and make an appropriate choice of the latent variable distribution, then we can construct a latent variable model that is both nonlinear and efficient to train. The generative topographic mapping, or GTM  uses a latent distribution that is defined by a finite regular grid of delta functions over the (typically two-dimensional) latent space. Marginalization over the latent space then simply involves summing over the contributions from each of the grid locations", "83748420-4aa9-49d7-8817-9c8eb83da6f0": "Experts agree that the major stumbling block to creating stronger-than-amateur Go programs is the di\ufb03culty of de\ufb01ning an adequate position evaluation function. A good evaluation function allows search to be truncated at a feasible depth by providing relatively easy-to-compute predictions of what deeper search would likely yield. According to M\u00a8uller : \u201cNo simple yet reasonable evaluation function will ever be found for Go.\u201d A major step forward was the introduction of MCTS to Go programs. The strongest programs at the time of AlphaGo\u2019s development all included MCTS, but master-level skill remained elusive. Recall from Section 8.11 that MCTS is a decision-time planning procedure that does not attempt to learn and store a global evaluation function. Like a rollout algorithm (Section 8.10), it runs many Monte Carlo simulations of entire episodes (here, entire Go games) to select each action (here, each Go move: where to place a stone or to resign). Unlike a simple rollout algorithm, however, MCTS is an iterative procedure that incrementally extends a search tree whose root node represents the current environment state", "bb066bd4-fbf3-41bf-beac-b5ae20a2d6da": "In contrast, the weighted importance-sampling algorithm would give an estimate of exactly 1 forever after the \ufb01rst episode that ended with the left action.\n\nAll returns not equal to 1 (that is, ending with the right action) would be inconsistent with the target policy and thus would have a \u21e2t:T (t)\u22121 of zero and contribute neither to the numerator nor denominator of (5.6). The weighted importancesampling algorithm produces a weighted average of only the returns consistent with the target policy, and all of these would be exactly 1. We can verify that the variance of the importance-sampling-scaled returns is in\ufb01nite in this example by a simple calculation. The variance of any random variable X is the expected value of the deviation from its mean \u00afX, which can be written Thus, if the mean is \ufb01nite, as it is in our case, the variance is in\ufb01nite if and only if the expectation of the square of the random variable is in\ufb01nite. Thus, we need only show that the expected square of the importance-sampling-scaled return is in\ufb01nite: To compute this expectation, we break it down into cases based on episode length and termination", "0d4495f6-8aff-4924-aeae-47409741babc": "Section 4.4.1 We can gain further insight into Bayesian model comparison and understand how the marginal likelihood can favour models of intermediate complexity by considering Figure 3.13. Here the horizontal axis is a one-dimensional representation of the space of possible data sets, so that each point on this axis corresponds to a speci\ufb01c data set. We now consider three models M1, M2 and M3 of successively increasing complexity. Imagine running these models generatively to produce example data sets, and then looking at the distribution of data sets that result. Any given model can generate a variety of different data sets since the parameters are governed by a prior probability distribution, and for any choice of the parameters there may be random noise on the target variables. To generate a particular data set from a speci\ufb01c model, we \ufb01rst choose the values of the parameters from their prior distribution p(w), and then for these parameter values we sample the data from p(D|w).\n\nA simple model (for example, based on a \ufb01rst order polynomial) has little variability and so will generate data sets that are fairly similar to each other. Its distribution p(D) is therefore con\ufb01ned to a relatively small region of the horizontal axis", "cea846d7-c726-43bd-aeb0-ba3555cef589": "When combined with decision theory, discussed in Section 1.5, it allows us to make optimal predictions given all the information available to us, even though that information may be incomplete or ambiguous. We will introduce the basic concepts of probability theory by considering a simple example. Imagine we have two boxes, one red and one blue, and in the red box we have 2 apples and 6 oranges, and in the blue box we have 3 apples and 1 orange. This is illustrated in Figure 1.9. Now suppose we randomly pick one of the boxes and from that box we randomly select an item of fruit, and having observed which sort of fruit it is we replace it in the box from which it came. We could imagine repeating this process many times. Let us suppose that in so doing we pick the red box 40% of the time and we pick the blue box 60% of the time, and that when we remove an item of fruit from a box we are equally likely to select any of the pieces of fruit in the box. In this example, the identity of the box that will be chosen is a random variable, which we shall denote by B", "d0f16932-110c-4906-b5da-1745c822331c": "PMLR. Zikun Hu, Xiang Li, Cunchao Tu, Zhiyuan Liu, and Maosong Sun. 2018. Few-shot charge prediction with discriminative legal attributes. In Proceedings of the 27th International Conference on Computational Linguistics, pages 487\u2013498, Santa Fe, New Mexico, USA. Association for Computational Linguistics. Aminul Huq and Mst. Tasnim Pervin. 2020. Adversarial attacks and defense on texts: A survey. Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber, and Hal Daum\u00b4e III. 2015. Deep unordered composition rivals syntactic methods for text classi\ufb01cation.\n\nIn Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1681\u20131691, Beijing, China. Association for Computational Linguistics. Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettlemoyer", "69b49548-8319-4f06-afcb-fac40aac59d9": "If we stored only the nonzero entries of the matrix, then both matrix multiplication and convolution would require the same number of floating-point operations to compute. The matrix would still need to contain 2 x 319 x 280 = 178,640 entries. Convolution is an extremely efficient way of describing transformations that apply the same linear  transformation of a small local region across the entire input. Photo credit: Paula Goodfellow. efficiency of a linear function for detecting edges in an image.\n\nIn the case of convolution, the particular form of parameter sharing causes the ayer to have a property called equivariance to translation. To say a function is equivariant means that if the input changes, the output changes in the same way. Specifically, a function f(a) is equivariant to a function g if f(g(a)) = g(f(x)). In he case of convolution, if we let g be any function that translates the input, that is, shifts it, then the convolution function is equivariant to g. For example, let I be a function giving image brightness at integer coordinates", "40ba4174-7fc7-4432-9813-cbb2c874bca4": "The derivative of the log likelihood with respect to the model parameters w is then given by where an = wT\u03c6n, and we have used yn = f(an) together with the result (4.119) for E. We now see that there is a considerable simpli\ufb01cation if we choose a particular form for the link function f \u22121(y) given by which gives f(\u03c8(y)) = y and hence f \u2032(\u03c8)\u03c8\u2032(y) = 1. Also, because a = f \u22121(y), we have a = \u03c8 and hence f \u2032(a)\u03c8\u2032(y) = 1. In this case, the gradient of the error function reduces to For the Gaussian s = \u03b2\u22121, whereas for the logistic model s = 1. In Section 4.5 we shall discuss the Bayesian treatment of logistic regression. As we shall see, this is more complex than the Bayesian treatment of linear regression models, discussed in Sections 3.3 and 3.5. In particular, we cannot integrate exactly over the parameter vector w since the posterior distribution is no longer Gaussian", "82b53aa1-7fd4-4c1f-b3a3-12920ad1807e": "From the results (8.66) and (8.69) derived earlier for the sum-product algorithm, we can readily write down the max-sum algorithm in terms of message passing simply by replacing \u2018sum\u2019 with \u2018max\u2019 and replacing products with sums of logarithms to give The initial messages sent by the leaf nodes are obtained by analogy with (8.70) and (8.71) and are given by while at the root node the maximum probability can then be computed, by analogy with (8.63), using So far, we have seen how to \ufb01nd the maximum of the joint distribution by propagating messages from the leaves to an arbitrarily chosen root node. The result will be the same irrespective of which node is chosen as the root. Now we turn to the second problem of \ufb01nding the con\ufb01guration of the variables for which the joint distribution attains this maximum value. So far, we have sent messages from the leaves to the root.\n\nThe process of evaluating (8.97) will also give the value xmax for the most probable value of the root node variable, de\ufb01ned by At this point, we might be tempted simply to continue with the message passing algorithm and send messages from the root back out to the leaves, using (8.93) and (8.94), then apply (8.98) to all of the remaining variable nodes", "151533d3-8a86-4025-8170-2b04a5871ff2": "For example, imagine we have a regression task with 2 \u20ac R!\u00b0\u00b0 drawn from an isotropic Gaussian distribution, but only a single variable x, is relevant to the output. Suppose further that this feature simply encodes the output directly, that y = a1 in all cases. Nearest neighbor regression will not be able to detect this simple pattern. 141  CHAPTER 5. MACHINE LEARNING BASICS  The nearest neighbor of most points 2 will be determined by the large number of features a through x199, not by the lone feature z;.\n\nThus the output on small training sets will essentially be random. Another type of learning algorithm that also breaks the input space into regions and has separate parameters for each region is the decision tree  and its many variants. As shown in figure 5.7, each node of the decision tree is associated with a region in the input space, and internal nodes break that region into one subregion for each child of the node (typically using an axis-aligned cut)", "e7013290-d600-47aa-802c-f87b8dc1d899": "Initialize using one basis function \u03d51, with hyperparameter \u03b11 set using (7.101), with the remaining hyperparameters \u03b1j for j \u0338= i initialized to in\ufb01nity, so that only \u03d51 is included in the model. 3. Evaluate \u03a3 and m, along with qi and si for all basis functions. 4. Select a candidate basis function \u03d5i. 5. If q2 i > si, and \u03b1i < \u221e, so that the basis vector \u03d5i is already included in the model, then update \u03b1i using (7.101). 8. If solving a regression problem, update \u03b2. 9. If converged terminate, otherwise go to 3. Note that if q2 i \u2a7d si and \u03b1i = \u221e, then the basis function \u03d5i is already excluded from the model and no action is required.\n\nIn practice, it is convenient to evaluate the quantities The quality and sparseness variables can then be expressed in the form Note that when \u03b1i = \u221e, we have qi = Qi and si = Si", "20915a31-eb4c-41ee-93ec-9d763f9822b8": "Second-order optimization algorithms may roughly be understood as dividing the first derivative by the second derivative (in higher dimension, multiplying the gradient by the inverse Hessian).\n\nIf the second derivative shrinks at a similar rate to the first  https://www.deeplearningbook.org/contents/rnn.html    derivative, then the ratio of first and second derivatives may remain relatively constant. Unfortunately, second-order methods have many drawbacks, including high computational cost, the need for a large minibatch, and a tendency to be  attracted to saddle points. Martens and Sutskever  found promising results using second-order methods. Later, Sutskever et al. found that simpler methods such as Nesterov momentum with careful initialization could achieve similar results. See Sutskever  for more detail. Both of these approaches have largely been replaced by simply using SGD (even without momentum) applied to LSTMs. This is part of a continuing theme in machine learning that it is often much easier to design a model that is easy to optimize than it is to design a more powerful optimization algorithm. 408  CHAPTER 10", "6cdc8e2c-34fc-46a6-8511-3891e706de36": "If a box is chosen at random with probabilities p(r) = 0.2, p(b) = 0.2, p(g) = 0.6, and a piece of fruit is removed from the box (with equal probability of selecting any of the items in the box), then what is the probability of selecting an apple? If we observe that the selected fruit is in fact an orange, what is the probability that it came from the green box? 1.4 (\u22c6 \u22c6) www Consider a probability density px(x) de\ufb01ned over a continuous variable x, and suppose that we make a nonlinear change of variable using x = g(y), so that the density transforms according to (1.27).\n\nBy differentiating (1.27), show that the location \ufffdy of the maximum of the density in y is not in general related to the location \ufffdx of the maximum of the density over x by the simple functional relation \ufffdx = g(\ufffdy) as a consequence of the Jacobian factor. This shows that the maximum of a probability density (in contrast to a simple function) is dependent on the choice of variable. Verify that, in the case of a linear transformation, the location of the maximum transforms in the same way as the variable itself", "fe22abb1-e59f-442f-9540-918922f39344": "The update must be chosen to be small enough to avoid traversing too much upward curvature. We typically use learning rates that decay slowly enough that consecutive steps have approximately the same learning rate. A step size that is appropriate for a relatively linear part of the landscape is often inappropriate and causes uphill motion if we enter a more curved part of the landscape on the next step. A simple type of solution has been in use by practitioners for many years: clipping the gradient. There are different instances of this idea . One option is to clip the parameter gradient from a minibatch element-wise , just before the parameter update", "b2da4200-0ec8-4af7-a1fe-a981a2c6e6ca": "As in Section 5.5.4, we shall consider a transformation governed by a single parameter \u03be and described by the function s(x, \u03be), with s(x, 0) = x. We shall also consider a sum-of-squares error function. The error function for untransformed inputs can be written (in the in\ufb01nite data set limit) in the form as discussed in Section 1.5.5. Here we have considered a network having a single output, in order to keep the notation uncluttered. If we now consider an in\ufb01nite number of copies of each data point, each of which is perturbed by the transformation in which the parameter \u03be is drawn from a distribution p(\u03be), then the error function de\ufb01ned over this expanded data set can be written as We now assume that the distribution p(\u03be) has zero mean with small variance, so that we are only considering small transformations of the original input vectors.\n\nWe can then expand the transformation function as a Taylor series in powers of \u03be to give where \u03c4 \u2032 denotes the second derivative of s(x, \u03be) with respect to \u03be evaluated at \u03be = 0", "fb3f599b-98af-4a39-a347-d1b7dbe30469": "In other words, Monte Carlo methods do not bootstrap as we de\ufb01ned it in the previous chapter. In particular, note that the computational expense of estimating the value of a single state is independent of the number of states. This can make Monte Carlo methods particularly attractive when one requires the value of only one or a subset of states. One can generate many sample episodes starting from the states of interest, averaging returns from only these states, ignoring all others. This is a third advantage Monte Carlo methods can have over DP methods (after the ability to learn from actual experience and from simulated experience). frame forming a closed loop is dunked in soapy water to form a soap surface or bubble conforming at its edges to the wire frame. If the geometry of the wire frame is irregular but known, how can you compute the shape of the surface?\n\nThe shape has the property that the total force on each point exerted by neighboring points is zero (or else the shape would change). This means that the surface\u2019s height at any point is the average of its heights at points in a small circle around that point. In addition, the surface must meet at its boundaries with the wire frame", "8e9f6c7d-0122-4a28-8e02-aad8fd454ec9": "Consider the term involving zzT.\n\nAgain, we can make use of the eigenvector expansion of the covariance matrix given by (2.45), together with the completeness of the set of eigenvectors, to write where we have made use of the eigenvector equation (2.45), together with the fact that the integral on the right-hand side of the middle line vanishes by symmetry unless i = j, and in the \ufb01nal line we have made use of the results (1.50) and (2.55), together with (2.48). Thus we have For the speci\ufb01c case of a Gaussian distribution, we can make use of E = \u00b5, together with the result (2.62), to give Because the parameter matrix \u03a3 governs the covariance of x under the Gaussian distribution, it is called the covariance matrix. Although the Gaussian distribution (2.43) is widely used as a density model, it suffers from some signi\ufb01cant limitations. Consider the number of free parameters in the distribution. A general symmetric covariance matrix \u03a3 will have D(D + 1)/2 independent parameters, and there are another D independent parameters in \u00b5, givExercise 2.21 ing D(D + 3)/2 parameters in total", "e440bff0-7222-4b9c-89fd-4ab7c35f86bd": "The teacher step in Equation 3.3 has a closed-form solution for the teacher q(n+1) due to the choice of cross entropy as the divergence function D in SE: In the more general case where other complex divergence functions are used (as those in Section 5), a closed-form teacher is usually not available. The probabilistic functional descent (PFD) mentioned in Section 5.2, with approximations to the in\ufb02uence function using convex duality, o\ufb00ers a possible way of solving for q for a broader class of divergences, such as Jensen-Shannon divergence and Wasserstein distance.\n\nIt thus presents a promising venue for future research to develop more generic solvers for the broad learning problems characterized by SE. On the other hand, as can be seen in the student step discussed shortly, sometimes we do not necessarily need a closed-form teacher q(n+1) in the learning, but only need to be able to draw samples from q(n+1). Probability functional descent. Generally, the SE (Equation 3.1 or 3.2) de\ufb01nes a loss over the auxiliary distribution q, denoted as J(q), which is a functional on the auxiliary distribution space Q(T )", "b8cccb3f-600f-48fb-8a3d-62413108e04e": "This vector representation of a value function has as many components as there are states.\n\nIn most cases where we want to use function approximation, this would be far too many components to represent the vector explicitly. Nevertheless, the idea of this vector is conceptually useful. In the following, we treat a value function and its vector representation interchangeably. To develop intuitions, consider the case with three states S = {s1, s2, s3} and two parameters w = (w1, w2)>. We can then view all value functions/vectors as points in a three-dimensional space. The parameters provide an alternative coordinate system over a two-dimensional subspace. Any weight vector w = (w1, w2)> is a point in the two-dimensional subspace and thus also a complete value function vw that assigns values to all three states. With general function approximation the relationship between the full space and the subspace of representable functions could be complex, but in the case of linear value-function approximation the subspace is a simple plane, as suggested by Figure 11.3. Now consider a single \ufb01xed policy \u21e1", "c6915159-d44f-48e8-b6a5-d291d33064ab": "Instead, we must minimize the Frobenius norm of the matrix of errors computed over all dimensions and all points:  es () _ (ei) 2)\u201d eubj Tp D* = argmin a\u2019 \u2014r(a\u00ae);) subject to DD\u2019 D= I. (2.68) D \u2014 ij  To derive the algorithm for finding D*, we start by considering the case where 1 =1. In this case, D is just a single vector, d. Substituting equation 2.67 into equation 2.68 and simplifying D into d, the problem reduces to  d =argmin \u2014 ||a \u2014 dd! a||3 subject to ||d||2 = 1. (2.69) ds,  \u00bb  https://www.deeplearningbook.org/contents/linear_algebra.html      _ \u2018Lhe above formulation 1s the most direct way of performing the substitution but is not the aypst stylistically pleasing way to write the equation. It places the scalar value d@' &\u201d on the right of the vector @", "54992a33-8889-462d-940c-292522bf8aef": "It is important to distinguish between lossless data compression, in which the goal is to be able to reconstruct the original data exactly from the compressed representation, and lossy data compression, in which we accept some errors in the reconstruction in return for higher levels of compression than can be achieved in the lossless case. We can apply the K-means algorithm to the problem of lossy data compression as follows. For each of the N data points, we store only the identity k of the cluster to which it is assigned. We also store the values of the K cluster centres \u00b5k, which typically requires signi\ufb01cantly less data, provided we choose K \u226a N. Each data point is then approximated by its nearest centre \u00b5k. New data points can similarly be compressed by \ufb01rst \ufb01nding the nearest \u00b5k and then storing the label k instead of the original data vector.\n\nThis framework is often called vector quantization, and the vectors \u00b5k are called code-book vectors. The image segmentation problem discussed above also provides an illustration of the use of clustering for data compression. Suppose the original image has N pixels comprising {R, G, B} values each of which is stored with 8 bits of precision. Then to transmit the whole image directly would cost 24N bits", "64a82269-9e36-4bbf-b748-5f4796c0fda5": "One recent example of the use of higher-order interactions is a Boltzmann machine with two groups of hidden units, one group that interacts with both the visible units v and the class label y, and another group that interacts only with the v input values . This can be interpreted as encouraging some hidden units to learn to model the input using features that are relevant to the class, but also to learn extra hidden units that explain nuisance details necessary for the samples of v to be realistic without determining the class of the example. Another use of higher-order interactions is to gate some features. Sohn et al. introduced a Boltzmann machine with third-order interactions and binary mask variables associated with each visible unit. When these masking variables are set to zero, they remove the influence of a visible unit on the hidden units. This allows visible units that are not relevant to the classification problem to be removed from the inference pathway that estimates the class.\n\nMore generally, the Boltzmann machine framework is a rich space of models permitting many more model structures than have been explored so far. Developing  683  CHAPTER 20", "fff8b051-add6-4cf2-869d-9c29042f29e5": "Snorkel includes a Spark7 integration layer, enabling labeling functions to be run across a cluster.\n\nOnce the set of candidates is cached as a Spark data frame, only the closure of the labeling functions and the resulting labels need to be communicated to and from the workers. This is particularly helpful in Snorkel\u2019s iterative work\ufb02ow. Distributing a large unstructured data set across a cluster is relatively expensive, but only has to be performed once. Then, as users re\ufb01ne their labeling functions, they can be rerun ef\ufb01ciently. The core operation of Snorkel is modeling and integrating the noisy signals provided by a set of labeling functions. Using the recently proposed approach of data programming , we model the true class label for a data point as a latent variable in a probabilistic model. In the simplest case, we model each labeling function as a noisy \u201cvoter\u201d which is independent\u2014i.e., makes errors that are uncorrelated with the other labeling functions. This de\ufb01nes a generative model of the votes of the labeling functions as noisy signals about the true label. We can also model statistical dependencies between the labeling functions to improve predictive performance", "a8f17c98-3784-4724-9d13-a08133293f98": "Special Issue on Real-World Applications of Uncertain Reasoning. 42, 647\u2013 666. Jerrum, M. and A. Sinclair . The Markov chain Monte Carlo method: an approach to approximate counting and integration. In D. S. Hochbaum (Ed. ), Approximation Algorithms for NP-Hard Problems. PWS Publishing. Jolliffe, I. T. Principal Component Analysis (Second ed.). Springer. Jordan, M. I. An Introduction to Probabilistic Graphical Models. In preparation. Jordan, M. I., Z. Ghahramani, T. S. Jaakkola, and L. K. Saul . An introduction to variational methods for graphical models. In M. I. Jordan (Ed. ), Learning in Graphical Models, pp. 105\u2013 162", "927d943e-f9ea-4b70-ac3c-c67f798eb728": "For batch methods, a similar effect can be achieved by replicating each data point a number of times and transforming each copy independently. The use of such augmented data can lead to signi\ufb01cant improvements in generalization , although it can also be computationally costly. Approach 2 leaves the data set unchanged but modi\ufb01es the error function through the addition of a regularizer. In Section 5.5.5, we shall show that this approach is closely related to approach 2. left. On the right, the top row shows three examples of warped digits, with the corresponding displacement \ufb01elds shown on the bottom row. These displacement \ufb01elds are generated by sampling random displacements \u2206x, \u2206y \u2208 (0, 1) at each pixel and then smoothing by convolution with Gaussians of width 0.01, 30 and 60 respectively. One advantage of approach 3 is that it can correctly extrapolate well beyond the range of transformations included in the training set. However, it can be dif\ufb01cult to \ufb01nd hand-crafted features with the required invariances that do not also discard information that can be useful for discrimination", "541855dd-28c4-404e-83b4-eed09430d44c": "A more efficient approach, however, is  da wand ales ale canta nn 2 nae eee (4 mad LL Aad aee 2 TL etn Af  https://www.deeplearningbook.org/contents/applications.html    LU LCaU LUC WILLULE SCLLLELICE UL aLABLaApIL (lo &Eb LUC CULLLEAL ALU LUE BIS\u2019 UL WIA is being expressed), then produce the translated words one at a time, each time focusing on a different part of the input sentence to gather the semantic details required to produce the next output word. That is exactly the idea that Bahdanau  et al. first introduced. The attention mechanism used to focus on specific parts of the input sequence at each time step is illustrated in figure 12.6.\n\nWe can think of an attention-based system as having three components:  1. A process that reads raw data (such as source words in a source sentence) and converts them into distributed representations, with one feature vector associated with each word position. 2. A list of feature vectors storing the output of the reader", "f47affeb-754c-4896-98c5-9ccfc29acc29": "The validation set classification error decreases to a low level. network training task, one can monitor the squared gradient norm g'9 and the gH g term.\n\nIn many cases, the gradient norm does not shrink significantly throughout learning, but the g' Hg term grows by more than an order of magnitude. The result is that learning becomes very slow despite the presence of a strong gradient because the learning rate must be shrunk to compensate for even stronger curvature. Figure 8.1 shows an example of the gradient increasing significantly during the successful training of a neural network. Though ill-conditioning is present in other settings besides neural network training, some of the techniques used to combat it in other contexts are less applicable to neural networks. For example, Newton\u2019s method is an excellent tool for minimizing convex functions with poorly conditioned Hessian matrices, but as we argue in subsequent sections, Newton\u2019s method requires significant modification before it can be applied to neural networks. CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  8.2.2 Local Minima  One of the most prominent features of a convex optimization problem is that it can be reduced to the problem of finding a local minimum", "55f1c119-b49a-42f6-97fa-b24097211e0a": "Jelmer MW, Tim L, Max AV, Ivana |. Generative adversarial networks for noise reduction in low-dose CT. In: IEEE  Transactions on Medical Imaging. 2017. Ohad S, Tammy RR. Accelerated magnetic resonance imaging by adversarial neural network. In: DLMIA/ML-CDS@ MICCAI, 2017. Wang Y, Biting Y, Wang L, Chen Z, Lalush DS, Lin W, Xi W, Zhou J, Shen D, Zhou L. 3D conditional generative adver- sarial networks for high-quality PET image estimation at low dose. Neurolmage. 2018;174:550-62. Dwarikanath M, Behzad B. Retinal vasculature segmentation using local saliency maps and generative adversarial networks for image super resolution. arXiv preprint. 2017  Francesco C, Aldo M, Claudio S, Giorgio T. Biomedical data augmentation using generative adversarial neural networks", "bc2df156-87e0-46d0-87f9-185849ccb798": "To do this, show that the derivative of the lefthand side with respect to \u00b5 is equal to the derivative of the right-hand side, and then integrate both sides with respect to \u00b5 and then show that the constant of integration vanishes. Note that before differentiating the left-hand side, it is convenient \ufb01rst to introduce a change of variable given by a = \u00b5 + \u03c3z so that the integral over a is replaced by an integral over z. When we differentiate the left-hand side of the relation (4.152), we will then obtain a Gaussian integral over z that can be evaluated analytically. In Chapters 3 and 4 we considered models for regression and classi\ufb01cation that comprised linear combinations of \ufb01xed basis functions. We saw that such models have useful analytical and computational properties but that their practical applicability was limited by the curse of dimensionality.\n\nIn order to apply such models to largescale problems, it is necessary to adapt the basis functions to the data. Support vector machines (SVMs), discussed in Chapter 7, address this by \ufb01rst de\ufb01ning basis functions that are centred on the training data points and then selecting a subset of these during training", "fdc76bf5-2026-4b3a-8987-2e677149335a": "The Rescorla\u2013Wagner model accounts for the acquisition of CRs in a way that explains blocking.\n\nAs long as the aggregate associative strength, VAX, of the stimulus compound is below the asymptotic level of associative strength, RY, that the US Y can support, the prediction error RY\u2212VAX is positive. This means that over successive trials the associative strengths VA and VX of the component stimuli increase until the aggregate associative strength VAX equals RY, at which point the associative strengths stop changing (unless the US changes). When a new component is added to a compound CS to which the animal has already been conditioned, further conditioning with the augmented compound produces little or no increase in the associative strength of the added CS component because the error has already been reduced to zero, or to a low value. The occurrence of the US is already predicted nearly perfectly, so little or no error\u2014or surprise\u2014is introduced by the new CS component. Prior learning blocks learning to the new component", "68b29acf-48d1-48ff-b046-15771b723dae": "Ideally, successive samples from a Markov chain designed to sample rom p(&) would be completely independent from each other and would visit many different regions in x space proportional to their probability. Instead, especially in high-dimensional cases, MCMC samples become very correlated. We refer o such behavior as slow mixing or even failure to mix. MCMC methods with slow mixing can be seen as inadvertently performing something resembling noisy gradient descent on the energy function, or equivalently noisy hill climbing on the probability, with respect to the state of the chain (the random variables being sampled). The chain tends to take small steps (in the space of the state of the Markov chain), from a configuration \u00ab-) to a configuration \u00ab, with the energy F(a) generally lower or approximately equal to the energy E(a\u2019\u2014)), with a preference for moves that yield lower energy configurations", "92f75de6-0fd7-40b8-b34a-e8f1b6a6cbd7": "However, in prior work, only sentence embeddings are transferred to down-stream tasks, where BERT transfers all parameters to initialize end-task model parameters. Pre-training data The pre-training procedure largely follows the existing literature on language model pre-training. For the pre-training corpus we use the BooksCorpus (800M words)  and English Wikipedia (2,500M words). For Wikipedia we extract only the text passages and ignore lists, tables, and headers. It is critical to use a document-level corpus rather than a shuf\ufb02ed sentence-level corpus such as the Billion Word Benchmark  in order to extract long contiguous sequences. Fine-tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream tasks\u2014 whether they involve single text or text pairs\u2014by swapping out the appropriate inputs and outputs.\n\nFor applications involving text pairs, a common pattern is to independently encode text pairs before applying bidirectional cross attention, such as Parikh et al. ; Seo et al. BERT instead uses the self-attention mechanism to unify these two stages, as encoding a concatenated text pair with self-attention effectively includes bidirectional cross attention between two sentences", "acc5626b-3fe7-42b2-885e-e73d605e4512": "Show that maximization of the function Q(\u03b8, \u03b8old) with respect to the mean and covariance parameters of the Gaussians gives rise to the M-step equations (13.20) and (13.21). 13.8 (\u22c6 \u22c6) www For a hidden Markov model having discrete observations governed by a multinomial distribution, show that the conditional distribution of the observations given the hidden variables is given by (13.22) and the corresponding M step equations are given by (13.23). Write down the analogous equations for the conditional distribution and the M step equations for the case of a hidden Markov with multiple binary output variables each of which is governed by a Bernoulli conditional distribution. Hint: refer to Sections 2.1 and 2.2 for a discussion of the corresponding maximum likelihood solutions for i.i.d. data if required.\n\n13.9 (\u22c6 \u22c6) www Use the d-separation criterion to verify that the conditional independence properties (13.24)\u2013(13.31) are satis\ufb01ed by the joint distribution for the hidden Markov model de\ufb01ned by (13.6)", "0fcbaebd-1a41-4af6-b4db-77e1f15aacdf": ", xN)T, the noise variance \u03c32, and the hyperparameter \u03b1 representing the precision of the Gaussian prior over w, all of which are parameters of the model rather than random variables. Focussing just on the random variables for the moment, we see that the joint distribution is given by the product of the prior p(w) and N conditional distributions p(tn|w) for n = 1, . , N so that This joint distribution can be represented by a graphical model shown in Figure 8.3. When we start to deal with more complex models later in the book, we shall \ufb01nd it inconvenient to have to write out multiple nodes of the form t1, . , tN explicitly as in Figure 8.3.\n\nWe therefore introduce a graphical notation that allows such multiple nodes to be expressed more compactly, in which we draw a single representative node tn and then surround this with a box, called a plate, labelled with N indicating that there are N nodes of this kind. Re-writing the graph of Figure 8.3 in this way, we obtain the graph shown in Figure 8.4", "05d11564-c5d9-4425-a376-0da8b4bd29b1": "As each data point arrives, the posterior is updated by making use of the bound (10.151) and then normalized to give an updated posterior distribution.\n\nThe predictive distribution is obtained by marginalizing over the posterior distribution, and takes the same form as for the Laplace approximation discussed in Section 4.5.2. Figure 10.13 shows the variational predictive distributions for a synthetic data set. This example provides interesting insights into the concept of \u2018large margin\u2019, which was discussed in Section 7.1 and which has qualitatively similar behaviour to the Bayesian solution. So far, we have treated the hyperparameter \u03b1 in the prior distribution as a known constant. We now extend the Bayesian logistic regression model to allow the value of this parameter to be inferred from the data set. This can be achieved by combining the global and local variational approximations into a single framework, so as to maintain a lower bound on the marginal likelihood at each stage. Such a combined approach was adopted by Bishop and Svens\u00b4en  in the context of a Bayesian treatment of the hierarchical mixture of experts model", "4a346daf-f25b-4960-ab53-4a2908374a6e": "5.9 (\u22c6) www The error function (5.21) for binary classi\ufb01cation problems was derived for a network having a logistic-sigmoid output activation function, so that 0 \u2a7d y(x, w) \u2a7d 1, and data having target values t \u2208 {0, 1}.\n\nDerive the corresponding error function if we consider a network having an output \u22121 \u2a7d y(x, w) \u2a7d 1 and target values t = 1 for class C1 and t = \u22121 for class C2. What would be the appropriate choice of output unit activation function? 5.10 (\u22c6) www Consider a Hessian matrix H with eigenvector equation (5.33). By setting the vector v in (5.39) equal to each of the eigenvectors ui in turn, show that H is positive de\ufb01nite if, and only if, all of its eigenvalues are positive. 5.11 (\u22c6 \u22c6) www Consider a quadratic error function de\ufb01ned by (5.32), in which the Hessian matrix H has an eigenvalue equation given by (5.33)", "9ea6b7ad-fb62-4ee3-a8c3-9f49b2bd3bde": "1  https://www.deeplearningbook.org/contents/convnets.html    Jr the Kernel 1s W elements wide In each dimension, then nalve multidimensional convolution requires O(w\") runtime and parameter storage space, while separable convolution requires O(w x d) runtime and parameter storage space. Of course, not every convolution can be represented in this way.\n\nDevising faster ways of performing convolution or approximate convolution without harming the accuracy of the model is an active area of research. Even tech- niques that improve the efficiency of only forward propagation are useful because in the commercial setting, it is typical to devote more resources to deployment of a network than to its training. 9.9 Random or Unsupervised Features  Typically, the most expensive part of convolutional network training is learning the features. The output layer is usually relatively inexpensive because of the small number of features provided as input to this layer after passing through several layers of pooling. When performing supervised training with gradient descent, every gradient step requires a complete run of forward propagation and  356  CHAPTER 9. CONVOLUTIONAL NETWORKS  backward propagation through the entire network", "fd253bca-50ea-4703-922c-0e9aa1184cb8": "Most successful applications of reinforcement learning owe much to sets of features carefully handcrafted based on human knowledge and intuition about the speci\ufb01c problem to be tackled. A team of researchers at Google DeepMind developed an impressive demonstration that a deep multi-layer ANN can automate the feature design process . Multi-layer ANNs have been used for function approximation in reinforcement learning ever since the 1986 popularization of the backpropagation algorithm as a method for learning internal representations (Rumelhart, Hinton, and Williams, 1986; see Section 9.6). Striking results have been obtained by coupling reinforcement learning with backpropagation. The results obtained by Tesauro and colleagues with TD-Gammon and Watson discussed above are notable examples. These and other applications bene\ufb01ted from the ability of multi-layer ANNs to learn task-relevant features. However, in all the examples of which we are aware, the most impressive demonstrations required the network\u2019s input to be represented in terms of specialized features handcrafted for the given problem", "62afd4ea-30e1-47e8-b11e-9892c4fc7f5c": "We can see that L? regularization causes the learning algorithm to \u201cperceive\u201d the input X as having higher variance, which makes it shrink the weights on features whose covariance with the output target is low compared to this added variance. 7.1.2 L Regularization  While L? weight decay is the most common form of weight decay, there are other ways to penalize the size of the model parameters. Another option is to use L! regularization. 230  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  Formally, L! regularization on the model parameter w is defined as  (8) = ||w]h = > les, (7.18)  that is, as the sum of absolute values of the individual parameters.? We will now discuss the effect of L' regularization on the simple linear regression model, with no bias parameter, that we studied in our analysis of L? regularization. In particular, we are interested in delineating the differences between L! and L? forms of regularization. As with L? weight decay, L! weight decay controls the strength of the regularization by scaling the penalty Q using a positive hyperparameter a", "2fa3c757-ea5e-4784-99e7-63dbfc733782": "DEEP FEEDFORWARD NETWORKS  Original a space Learned h space  x2 ho  Z1 hy  Figure 6.1: Solving the XOR problem by learning a representation.\n\nThe bold numbers printed on the plot indicate the value that the learned function must output at each point. (Left) A linear model applied directly to the original input cannot implement the XOR  https://www.deeplearningbook.org/contents/mlp.html    function. When tT! = 0, the model\u2019s output must increase as Z7 increases. When 7! = 1 the model\u2019s output must decrease as \u00a92 increases. A linear model must apply a fixed coefficient w2 to 2. The linear model therefore cannot use the value of 11 to change  the coefficient on x2 and cannot solve this problem. (Right) In the transformed space represented by the features extracted by a neural network, a linear model can now solve the problem. In our example solution, the two points that must have output 1 have been collapsed into a single point in feature space", "25007ecf-f065-4243-8dfa-940f17c6d6cc": "from the University of Michigan, was hired as post doctoral researcher on the project. Meanwhile, Sutton, an undergraduate studying computer science and psychology at Stanford, had been corresponding with Harry regarding their mutual interest in the role of stimulus timing in classical conditioning. Harry suggested to the UMass group that Sutton would be a great addition to the project. Thus, Sutton became a UMass graduate student, whose Ph.D. was directed by Barto, who had become an Associate Professor.\n\nThe study of reinforcement learning as presented in this book is rightfully an outcome of that project instigated by Harry and inspired by his ideas. Further, Harry was responsible for bringing us, the authors, together in what has been a long and enjoyable interaction. By dedicating this book to Harry we honor his essential contributions, not only to the \ufb01eld of reinforcement learning, but also to our collaboration. We also thank Professors Arbib, Kilmer, and Spinelli for the opportunity they provided to us to begin exploring these ideas. Finally, we thank AFOSR for generous support over the early years of our research, and the NSF for its generous support over many of the following years. We have very many people to thank for their inspiration and help with this second edition", "fc708021-3b52-45d3-ba96-b319954ba036": "Using (7.76) and (7.81), this is given by Exercise 7.14 Thus the predictive mean is given by (7.76) with w set equal to the posterior mean m, and the variance of the predictive distribution is given by where \u03a3 is given by (7.83) in which \u03b1 and \u03b2 are set to their optimized values \u03b1\u22c6 and \u03b2\u22c6. This is just the familiar result (3.59) obtained in the context of linear regression. Recall that for localized basis functions, the predictive variance for linear regression models becomes small in regions of input space where there are no basis functions. In the case of an RVM with the basis functions centred on data points, the model will therefore become increasingly certain of its predictions when extrapolating outside the domain of the data , which of course is undesirable. The predictive distribution in Gaussian process regression does not Section 6.4.2 suffer from this problem. However, the computational cost of making predictions with a Gaussian processes is typically much higher than with an RVM. data set.\n\nHere the noise precision parameter \u03b2 is also determined through evidence maximization. We see that the number of relevance vectors in the RVM is significantly smaller than the number of support vectors used by the SVM", "687eb1be-3d7f-42af-87d1-207d521e2d0f": "It can, however, be shown that the expected committee error will not exceed the expected error of the constituent models, so that ECOM \u2a7d EAV.\n\nIn order to achieve more signi\ufb01cant improvements, we turn to a more Exercise 14.3 sophisticated technique for building committees, known as boosting. Boosting is a powerful technique for combining multiple \u2018base\u2019 classi\ufb01ers to produce a form of committee whose performance can be signi\ufb01cantly better than that of any of the base classi\ufb01ers. Here we describe the most widely used form of boosting algorithm called AdaBoost, short for \u2018adaptive boosting\u2019, developed by Freund and Schapire . Boosting can give good results even if the base classi\ufb01ers have a performance that is only slightly better than random, and hence sometimes the base classi\ufb01ers are known as weak learners. Originally designed for solving classi\ufb01cation problems, boosting can also be extended to regression . The principal difference between boosting and the committee methods such as bagging discussed above, is that the base classi\ufb01ers are trained in sequence, and each base classi\ufb01er is trained using a weighted form of the data set in which the weighting coef\ufb01cient associated with each data point depends on the performance of the previous classi\ufb01ers", "4230e170-a190-48d3-b71a-690b831ebfe5": "OPTIMIZATION FOR TRAINING DEEP MODELS  16 1.0 14 20.9 3 12 50.8 3 a Fal fe} 5 10 50-7 2 8 20.6 I fe} oO = 2 6 305 & 4 0.4 2 20.3  https://www.deeplearningbook.org/contents/optimization.html    an ed are \u20142 0.1 \u201450 0 50 100 150 200 250 0 50 100 150 200 250 Training time (epochs) Training time (epochs)  Figure 8.1: Gradient descent often does not arrive at a critical point of any kind. In this example, the gradient norm increases throughout training of a convolutional network used for object detection. (Left)A scatterplot showing how the norms of individual gradient evaluations are distributed over time. To improve legibility, only one gradient norm is plotted per epoch. The running average of all gradient norms is plotted as a solid curve. The gradient norm clearly increases over time, rather than decreasing as we would expect if the training process converged to a critical point. (Right)Despite the increasing gradient, the training process is reasonably successful", "baa3e88b-2218-4134-8f65-0d6ff657121e": "The Monte Carlo methods for this are essentially the same as just presented for state values, except now we talk about visits to a state\u2013action pair rather than to a state.\n\nA state\u2013 action pair s, a is said to be visited in an episode if ever the state s is visited and action a is taken in it. The every-visit MC method estimates the value of a state\u2013action pair as the average of the returns that have followed all the visits to it. The \ufb01rst-visit MC method averages the returns following the \ufb01rst time in each episode that the state was visited and the action was selected. These methods converge quadratically, as before, to the true expected values as the number of visits to each state\u2013action pair approaches in\ufb01nity. The only complication is that many state\u2013action pairs may never be visited. If \u21e1 is a deterministic policy, then in following \u21e1 one will observe returns only for one of the actions from each state. With no returns to average, the Monte Carlo estimates of the other actions will not improve with experience. This is a serious problem because the purpose of learning action values is to help in choosing among the actions available in each state", "fad368da-e45c-4474-a7cb-4ea6d0a78fc6": "Most machine learning algorithms simply experience a dataset. A dataset can be described in many ways.\n\nIn all cases, a dataset is a collection of examples, which are in turn collections of features. One common way of describing a dataset is with a design matrix. A design matrix is a matrix containing a different example in each row. Each column of the matrix corresponds to a different feature. For instance, the Iris dataset contains 150 examples with four features for each example. This means we can represent the dataset with a design matrix X \u20ac R!\u00ae\u00b0*4, where Xj, is the sepal length of plant i, X;2 is the sepal width of plant 7, etc. We describe most of the learning algorithms in this book in terms of how they operate on design matrix datasets. Of course, to describe a dataset as a design matrix, it must be possible to describe each example as a vector, and each of these vectors must be the same size. This is not always possible. For example, if you have a collection of photographs with different widths and heights, then different photographs will contain different numbers of pixels, so not all the photographs may be described with the same  104  CHAPTER 5", "e8b265b1-5599-418f-bfed-453a05a35af3": "But we made no attempt to provide comprehensive coverage of the \ufb01eld, which has exploded in many di\u21b5erent directions. We apologize for having to leave out all but a handful of these contributions. As in the \ufb01rst edition, we chose not to produce a rigorous formal treatment of reinforcement learning, or to formulate it in the most general terms.\n\nHowever, our deeper understanding of some topics since the \ufb01rst edition required a bit more mathematics to explain; we have set o\u21b5 the more mathematical parts in shaded boxes that the nonmathematically-inclined may choose to skip. We also use a slightly di\u21b5erent notation than was used in the \ufb01rst edition. In teaching, we have found that the new notation helps to address some common points of confusion. It emphasizes the di\u21b5erence between random variables, denoted with capital letters, and their instantiations, denoted in lower case. For example, the state, action, and reward at time step t are denoted St, At, and Rt, while their possible values might be denoted s, a, and r", "1dffead8-f711-48dc-b7d8-ca2897a58d1b": "Their classifier localizes a face using essentially a sliding window approach in which many windows are examined and rejected if they do not contain faces. Another version of cascades uses the earlier models to implement a sort of hard attention mechanism: the early members of the cascade localize an object, and later members of the cascade perform further processing given the location of the object. For example, Google transcribes address numbers from Street View imagery using a two-step cascade that first locates the address number with one machine learning model and then transcribes it with another . Decision trees themselves are an example of dynamic structure, because each node in the tree determines which of its subtrees should be evaluated for each input. A simple way to accomplish the union of deep learning and dynamic structure  444  CHAPTER 12.\n\nAPPLICATIONS  is to train a decision tree in which each node uses a neural network to make the splitting decision , though this has typically not been done with the primary goal of accelerating inference computations. In the same spirit, one can use a neural network called the gater to select which one out of several expert networks will be used to compute the output, given the current input", "307252f6-da72-4fe4-a508-28f63f10b7c3": "As a consequence of the conditional independence property (13.5) this likelihood function can be maximized ef\ufb01ciently using an EM algorithm in which the E step involves forward and backward recursions.\n\nExercise 13.18 Another variant of the HMM worthy of mention is the factorial hidden Markov model , in which there are multiple independent Markov chains of latent variables, and the distribution of the observed variable at a given time step is conditional on the states of all of the corresponding latent variables at that same time step. Figure 13.19 shows the corresponding graphical model. The motivation for considering factorial HMM can be seen by noting that in order to represent, say, 10 bits of information at a given time step, a standard HMM would need K = 210 = 1024 latent states, whereas a factorial HMM could make use of 10 binary latent chains. The primary disadvantage of factorial HMMs, however, lies in the additional complexity of training them. The M step for the factorial HMM model is straightforward. However, observation of the x variables introduces dependencies between the latent chains, leading to dif\ufb01culties with the E step", "6e41f87b-fc4f-44fb-8040-6e2406e33052": "Deutsch  described a maze-solving machine based on his behavior theory  that has some properties in common with model-based reinforcement learning (Chapter 8). In his Ph.D. dissertation, Marvin Minsky  discussed computational models of reinforcement learning and described his construction of an analog machine composed of components he called SNARCs (Stochastic Neural-Analog Reinforcement Calculators) meant to resemble modi\ufb01able synaptic connections in the brain (Chapter 15). The web site cyberneticzoo.com contains a wealth of information on these and many other electro-mechanical learning machines. Building electro-mechanical learning machines gave way to programming digital computers to perform various types of learning, some of which implemented trial-and-error learning. Farley and Clark  described a digital simulation of a neural-network learning machine that learned by trial and error. But their interests soon shifted from trial-and-error learning to generalization and pattern recognition, that is, from reinforcement learning to supervised learning . This began a pattern of confusion about the relationship between these types of learning", "1ce68826-b634-4c3d-8ecf-ee54d57a4701": "These cells respond to features that are similar to those detected by simple cells, but complex cells are invariant to small shifts in the position of the feature. This inspires the pooling units of convolutional networks. Complex cells are also invariant to some changes in lighting that cannot be captured simply by pooling over spatial locations. These invariances have inspired some of the cross-channel pooling strategies in convolutional networks, such as maxout units .\n\nThough we know the most about V1, it is generally believed that the same basic principles apply to other areas of the visual system. In our cartoon view of the visual system, the basic strategy of detection followed by pooling is repeatedly applied as we move deeper into the brain. As we pass through multiple anatomical layers of the brain, we eventually find cells that respond to some specific concept and are invariant to many transformations of the input. These cells have been nicknamed \u201cgrandmother cells\u2019\u2014the idea is that a person could have a neuron that activates when seeing an image of their grandmother, regardless of whether she appears in the left or right side of the image, whether the image is a close-up of her face or zoomed-out shot of her entire body, whether she is brightly lit or in shadow, and so on", "1bca15ce-44ff-43d9-be5a-092d13827144": "As another example, a neural language model p\u03b8(t) over text sequence t is a generator with a decoder component (which is in turn implemented via a transformer or other neural architectures). The \u03b8 to be learned includes the neural network weights. It can be randomly initialized and then trained from scratch. On the other hand, with the prevalence of pretrained models, such as BERT  for text representation and GPT-3  for language modeling, we can often initialize \u03b8 with the appropriate pretrained model weights and \ufb01netune it to the downstream tasks of interest. Sometimes it is su\ufb03cient to \ufb01netune only a subset of the model parameters (e.g., the classi\ufb01cation head of the image classi\ufb01er) while keeping all other parts \ufb01xed.\n\nIn many downstream tasks, it is often di\ufb03cult to obtain a large number of supervised data instances to train/\ufb01netune \u03b8 with simple supervised MLE. In such cases, SE o\ufb00ers a more \ufb02exible framework that allows plugging in all other forms of experience (as those in Section 4) related to the downstream tasks for more e\ufb00ective learning. Prompts for pretrained models. Updating large pretrained models can be prohibitively expensive due to the massive amount of model parameters", "3e9910c4-d551-413c-a627-3792fa84354e": "Chapter 17  Monte Carlo Methods  Randomized algorithms fall into two rough categories: Las Vegas algorithms and Monte Carlo algorithms. Las Vegas algorithms always return precisely the correct answer (or report that they failed). These algorithms consume a random amount of resources, usually memory or time. In contrast, Monte Carlo algorithms return answers with a random amount of error. The amount of error can typically be reduced by expending more resources (usually running time and memory). For any fixed computational budget, a Monte Carlo algorithm can provide an approximate answer. Many problems in machine learning are so difficult that we can never expect to obtain precise answers to them. This excludes precise deterministic algorithms and Las Vegas algorithms. Instead, we must use deterministic approximate algorithms or Monte Carlo approximations. Both approaches are ubiquitous in machine learning. In this chapter, we focus on Monte Carlo methods. 17.1 Sampling and Monte Carlo Methods  Many important technologies used to accomplish machine learning goals are based on drawing samples from some probability distribution and using these samples to form a Monte Carlo estimate of some desired quantity. 17.1.1 Why Sampling? We may wish to draw samples from a probability distribution for many reasons", "75f0e550-ed9c-4bcd-995c-4b709d437409": "At time t + 1 they immediately form a target and make a useful update using the observed reward Rt+1 and the estimate V (St+1). The simplest TD method makes the update immediately on transition to St+1 and receiving Rt+1.\n\nIn e\u21b5ect, the target for the Monte Carlo update is Gt, whereas the target for the TD update is Rt+1 + \u03b3V (St+1). This TD method is called TD(0), or one-step TD, because it is a special case of the TD(\u03bb) and n-step TD methods developed in Chapter 12 and Chapter 7. The box below speci\ufb01es TD(0) completely in procedural form. Because TD(0) bases its update in part on an existing estimate, we say that it is a bootstrapping method, like DP. We know from Chapter 3 that Roughly speaking, Monte Carlo methods use an estimate of (6.3) as a target, whereas DP methods use an estimate of (6.4) as a target. The Monte Carlo target is an estimate because the expected value in (6.3) is not known; a sample return is used in place of the real expected return", "5dae1651-90a6-4804-9879-77200e899b4c": "A second result derived using calculus of variations is that  f= arg min \u00b0x.y~PastallY \u2014 f(@)II1 (6.16)  yields a function that predicts the median value of y for each a, as long as such a function may be described by the family of functions we optimize over.\n\nThis cost function is commonly called mean absolute error. Unfortunately, mean squared error and mean absolute error often lead to poor results when used with gradient-based optimization. Some output units that saturate produce very small gradients when combined with these cost functions. This is one reason that the cross-entropy cost function is more popular than mean squared error or mean absolute error, even when it is not necessary to estimate an entire distribution p(y | x). 6.2.2 Output Units  The choice of cost function is tightly coupled with the choice of output unit. Most of the time, we simply use the cross-entropy between the data distribution and the model distribution. The choice of how to represent the output then determines the form of the cross-entropy function. Any kind of neural network unit that may be used as an output can also be used as a hidden unit", "314b875f-b8a3-4722-a50a-952693fee1d4": "The experience can be encoded in the same form as the supervised data (Equation 4.2) but now with only the information of x\u2217: Applying the SE to this setting with proper speci\ufb01cations derives the unsupervised MLE algorithm. Unsupervised MLE.\n\nThe form of Equation 3.2 is reminiscent of the variational free energy objective in the standard EM for unsupervised MLE (Equation 2.9). We can indeed get exact correspondence by setting \u03b1 = \u03b2 = 1, and setting the auxiliary distribution q(x, y) = \u02dcpd(x)q(y|x). The reason for \u03b2 = 1, which di\ufb00ers from the speci\ufb01cation \u03b2 = \u03f5 in the supervised setting, is that the auxiliary distribution q cannot be determined fully by the unsupervised \u2018incomplete\u2019 data 4.1.4. Manipulated data instances. Data manipulation, such as reweighting data instances or augmenting an existing data set with new instances, is often a crucial step for e\ufb03cient learning, such as in a low data regime or in presence of low-quality data sets (e.g., imbalanced labels). We show that the rich data manipulation schemes can be treated as experience and be naturally encoded in the experience function", "96886c60-097b-4edb-8423-6ba12862943d": "8.16 (\u22c6 \u22c6) Consider the inference problem of evaluating p(xn|xN) for the graph shown in Figure 8.38, for all nodes n \u2208 {1, . , N \u2212 1}. Show that the message passing algorithm discussed in Section 8.4.1 can be used to solve this ef\ufb01ciently, and discuss which messages are modi\ufb01ed and in what way. 8.17 (\u22c6 \u22c6) Consider a graph of the form shown in Figure 8.38 having N = 5 nodes, in which nodes x3 and x5 are observed. Use d-separation to show that x2 \u22a5\u22a5 x5 | x3. Show that if the message passing algorithm of Section 8.4.1 is applied to the evaluation of p(x2|x3, x5), the result will be independent of the value of x5.\n\n8.18 (\u22c6 \u22c6) www Show that a distribution represented by a directed tree can trivially be written as an equivalent distribution over the corresponding undirected tree. Also show that a distribution expressed as an undirected tree can, by suitable normalization of the clique potentials, be written as a directed tree. Calculate the number of distinct directed trees that can be constructed from a given undirected tree", "6ac50ae6-c7e9-488d-8b3e-c68457bf8694": "Timing of updates Should updates be done as part of selecting actions, or only afterMemory for updates How long should updated values be retained? Should they be Of course, these dimensions are neither exhaustive nor mutually exclusive. Individual algorithms di\u21b5er in many other ways as well, and many algorithms lie in several places along several dimensions.\n\nFor example, Dyna methods use both real and simulated experience to a\u21b5ect the same value function. It is also perfectly sensible to maintain multiple value functions computed in di\u21b5erent ways or over di\u21b5erent state and action representations. These dimensions do, however, constitute a coherent set of ideas for describing and exploring a wide space of possible methods. The most important dimension not mentioned here, and not covered in Part I of this book, is that of function approximation. Function approximation can be viewed as an orthogonal spectrum of possibilities ranging from tabular methods at one extreme through state aggregation, a variety of linear methods, and then a diverse set of nonlinear methods. This dimension is explored in Part II", "ce1293fd-eb6b-43ed-8cb4-3dc77f2530ae": "13.30 (\u22c6 \u22c6) Starting from the result (13.65) for the pairwise posterior marginal in a state space model, derive the speci\ufb01c form (13.103) for the case of the Gaussian linear dynamical system. 13.31 (\u22c6 \u22c6) Starting from the result (13.103) and by substituting for \ufffd\u03b1(zn) using (13.84), verify the result (13.104) for the covariance between zn and zn\u22121. 13.33 (\u22c6 \u22c6) Verify the results (13.113) and (13.114) for the M-step equations for A and \u0393 in the linear dynamical system. 13.34 (\u22c6 \u22c6) Verify the results (13.115) and (13.116) for the M-step equations for C and \u03a3 in the linear dynamical system. In earlier chapters, we have explored a range of different models for solving classi\ufb01cation and regression problems. It is often found that improved performance can be obtained by combining multiple models together in some way, instead of just using a single model in isolation. For instance, we might train L different models and then make predictions using the average of the predictions made by each model. Such combinations of models are sometimes called committees", "6ca89066-97ad-40bc-8184-21e1c863394f": "In each case the red ellipse corresponds to unit Mahalanobis distance, with |C| taking the same value for both plots, while the dashed green circle shows the contrition arising from the noise term \u03b2\u22121. We see that any \ufb01nite value of \u03b1 reduces the probability of the observed data, and so for the most probable solution the basis vector is removed. the mechanism of sparsity in the context of the relevance vector machine.\n\nIn the process, we will arrive at a signi\ufb01cantly faster procedure for optimizing the hyperparameters compared to the direct techniques given above. Before proceeding with a mathematical analysis, we \ufb01rst give some informal insight into the origin of sparsity in Bayesian linear models. Consider a data set comprising N = 2 observations t1 and t2, together with a model having a single basis function \u03c6(x), with hyperparameter \u03b1, along with isotropic noise having precision \u03b2", "83554156-d7f7-4358-9ac8-7f9c5ab0d3b3": "Sparse representations  embed the dataset into a representation whose entries are mostly zeros for most inputs. The use of sparse representations typically requires increasing the dimensionality of the representation, so that the representation becoming mostly zeros does not discard too much information. This results in an overall structure of the representation that tends to distribute data along the axes of the representation space. Independent representations attempt to disentangle the sources of variation underlying the data distribution such that the dimensions of the representation are statistically independent. Of course these three criteria are certainly not mutually exclusive. Low- dimensional representations often yield elements that have fewer or weaker de- pendencies than the original high-dimensional data.\n\nThis is because one way to reduce the size of a representation is to find and remove redundancies. Identifying and removing more redundancy enables the dimensionality reduction algorithm to achieve more compression while discarding less information. The notion of representation is one of the central themes of deep learning and therefore one of the central themes in this book. In this section, we develop some simple examples of representation learning algorithms. Together, these example algorithms show how to operationalize all three of the criteria above", "5d70b2d3-d8b4-4d60-9f36-6059db594cee": "For any given value of x, the mixture model provides a general formalism for modelling an arbitrary conditional density function p(t|x). Provided we consider a suf\ufb01ciently \ufb02exible network, we then have a framework for approximating arbitrary conditional distributions.\n\nHere we shall develop the model explicitly for Gaussian components, so that This is an example of a heteroscedastic model since the noise variance on the data is a function of the input vector x. Instead of Gaussians, we can use other distributions for the components, such as Bernoulli distributions if the target variables are binary rather than continuous. We have also specialized to the case of isotropic covariances for the components, although the mixture density network can readily be extended to allow for general covariance matrices by representing the covariances using a Cholesky factorization . Even with isotropic components, the conditional distribution p(t|x) does not assume factorization with respect to the components of t (in contrast to the standard sum-of-squares regression model) as a consequence of the mixture distribution", "1f05d179-0c9a-4278-b32e-49fd9df2f782": "In other words, in some sense, no machine learning algorithm is universally any better than any other. The most sophisticated algorithm we can conceive of has the same average performance (over all possible tasks) as merely predicting that every point belongs to the same class. Fortunately, these results hold only when we average over all possible data- generating distributions. If we make assumptions about the kinds of probability distributions we encounter in real-world applications, then we can design learning algorithms that perform well on these distributions. This means that the goal of machine learning research is not to seek a universal learning algorithm or the absolute best learning algorithm.\n\nInstead, our goal is to understand what kinds of distributions are relevant to the \u201creal world\u201d that an AI  https://www.deeplearningbook.org/contents/ml.html    agent experiences, and what kinds of machine learning algorithms perform well on data drawn from the kinds of data-generating distributions we care about. 5.2.2 Regularization  The no free lunch theorem implies that we must design our machine learning algorithms to perform well on a specific task. We do so by building a set of preferences into the learning algorithm. When these preferences are aligned with the learning problems that we ask the algorithm to solve, it performs better", "917c5b38-9b66-4aff-9f49-2cd0185613fa": "Performing structure learning for many settings of \u03f5 is inexpensive, especially since the search needs to be performed only once before tuning the other hyperparameters.\n\nOn the large number of labeling functions in the Spouses task, structure learning for 25 values of \u03f5 takes 14 min. On CDR, with a smaller number of labeling functions, it takes 30 s. Further, if the search is started at a low value of \u03f5 and increased, it can often be terminated early, when the number of selected correlations reaches a low value. Selecting the elbow point itself is straightforward. We use the point with greatest absolute difference from its neighbors, but more sophisticated schemes can also be applied . Our full optimization algorithm for choosing a modeling strategy and (if necessary) correlations is shown in Algorithm 1. We evaluate Snorkel by drawing on deployments developed in collaboration with users. We report on two real-world deployments and four tasks on open-source data sets representative of other deployments", "1514c54e-3616-455e-8c35-cc983ca96c4d": "If every layer of the neural network consists of only  https://www.deeplearningbook.org/contents/mlp.html    linear transformations, then the network as a whole will be linear. However, it is acceptable for some \u2018layers of the neural network to be or ab). Consider a neural network layer with n inputs and p outputs, h a+b). We may  replace this with two layers, with one layer using weight matrix U and the other using weight matrix V. If the first layer has no activation function, then we have essentially factored the weight matrix of the original layer based on W. The factored approach is to compute h = g(V'U! a+ b). If U produces gq outputs, then U and V together contain only (n + p)q parameters, while W contains np parameters. For small q, this can be a considerable saving in parameters. It comes at the cost of constraining the linear transformation to be low rank, but  192  CHAPTER 6", "06b47aea-f7f8-4ab8-bd35-3cd3a0547f21": "Finally, any valid distribution over the real axis (such as a Gaussian) can be turned into a periodic distribution by mapping successive intervals of width 2\u03c0 onto the periodic variable (0, 2\u03c0), which corresponds to \u2018wrapping\u2019 the real axis around unit circle. Again, the resulting distribution is more complex to handle than the von Mises distribution. One limitation of the von Mises distribution is that it is unimodal. By forming mixtures of von Mises distributions, we obtain a \ufb02exible framework for modelling periodic variables that can handle multimodality. For an example of a machine learning application that makes use of von Mises distributions, see Lawrence et al. , and for extensions to modelling conditional densities for regression problems, see Bishop and Nabney . While the Gaussian distribution has some important analytical properties, it suffers from signi\ufb01cant limitations when it comes to modelling real data sets. Consider the example shown in Figure 2.21. This is known as the \u2018Old Faithful\u2019 data set, and comprises 272 measurements of the eruption of the Old Faithful geyser at Yellowstone National Park in the USA", "8e2c89f7-3c4b-4bdc-97f7-699a02c33186": "The value function of any such policy, v\u21e10(s), can be seen by inspection to be either \u22121, \u22122, or \u22123 at all states, s 2 S, whereas v\u21e1(s) is at most \u221214.\n\nThus, v\u21e10(s) \u2265 v\u21e1(s), for all s 2 S, illustrating policy improvement. Although in this case the new policy \u21e10 happens to be optimal, in general only an improvement is guaranteed. Once a policy, \u21e1, has been improved using v\u21e1 to yield a better policy, \u21e10, we can then compute v\u21e10 and improve it again to yield an even better \u21e100. We can thus obtain a sequence of monotonically improving policies and value functions: policy is guaranteed to be a strict improvement over the previous one (unless it is already optimal). Because a \ufb01nite MDP has only a \ufb01nite number of policies, this process must converge to an optimal policy and optimal value function in a \ufb01nite number of iterations. This way of \ufb01nding an optimal policy is called policy iteration. A complete algorithm is given in the box below", "7c352cfc-b2b9-46cd-8a1e-be2c195b2812": "Now assume that the prior is broad so that V\u22121 0 is small and the second term on the right-hand side above can be neglected. Furthermore, consider the case of independent, identically distributed data so that H is the sum of terms one for each data point. Show that the log model evidence can then be written approximately in the form of the BIC expression (4.139).\n\n4.24 (\u22c6 \u22c6) Use the results from Section 2.3.2 to derive the result (4.151) for the marginalization of the logistic regression model with respect to a Gaussian posterior distribution over the parameters w. 4.25 (\u22c6 \u22c6) Suppose we wish to approximate the logistic sigmoid \u03c3(a) de\ufb01ned by (4.59) by a scaled probit function \u03a6(\u03bba), where \u03a6(a) is de\ufb01ned by (4.114). Show that if \u03bb is chosen so that the derivatives of the two functions are equal at a = 0, then \u03bb2 = \u03c0/8. 4.26 (\u22c6 \u22c6) In this exercise, we prove the relation (4.152) for the convolution of a probit function with a Gaussian distribution", "f43f3da3-f355-41de-9cdd-d48be12867d8": "We would usually prefer to minimize the corresponding objective function where the expectation is taken across the data-generating distribution pgata rather than just over the finite training set:  J*(9) = Evey)~paatal(S(# 9), y)- (8.2) 8.1.1 Empirical Risk Minimization The goal of a machine learning algorithm is to reduce the expected generalization  https://www.deeplearningbook.org/contents/optimization.html    error piven by equation 8.2. This quantity is known as the risk. We emphasize here that the expectation is taken over the true underlying distribution pdata, If we knew the true distribution Pgata(#, y), risk minimization would be an optimization  272  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  task solvable by an optimization algorithm.\n\nWhen we do not know pdata(x, y) but only have a training set of samples, however, we have a machine learning problem. The simplest way to convert a machine learning problem back into an op- timization problem is to minimize the expected loss on the training set", "6fbaf65e-3536-4b57-929f-2637d6aa6fd4": "Using self- supervised learning at Facebook  At Facebook, we\u2019re not just advancing self-supervised learning techniques across many domains through  https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/  Self-supervised learning: The dark matter of intelligence   fundamental, open scientific research,  but we\u2019re also applying this leading- edge work in production to quickly improve the accuracy of content understanding systems in our products that keep people safe on our platforms. Self-supervision research, like our pretrained language model XLM, is accelerating important applications at Facebook today \u2014 including  proactive detection of hate speech.\n\nAnd we\u2019ve deployed XLM-R, a model that leverages our ROBERTa architecture, to improve our hate speech classifiers in multiple languages across Facebook and Instagram.This will enable hate speech detection even in languages for which there is very little training data. We\u2019re encouraged by the progress of self-supervision in recent years, though there\u2019s still a long way to go until this method can help us uncover the dark matter of Al intelligence. Self- supervision is one step on the path to human-level intelligence, but there are surely many steps that lie behind this one. Long-term progress will be cumulative", "71ce1e88-adcc-4ba3-8d64-2df606fd5bf9": "In the expectation step, or E step, we use the current values for the parameters to evaluate the posterior probabilities, or responsibilities, given by (9.13).\n\nWe then use these probabilities in the maximization step, or M step, to re-estimate the means, covariances, and mixing coef\ufb01cients using the results (9.17), (9.19), and (9.22). Note that in so doing we \ufb01rst evaluate the new means using (9.17) and then use these new values to \ufb01nd the covariances using (9.19), in keeping with the corresponding result for a single Gaussian distribution. We shall show that each update to the parameters resulting from an E step followed by an M step is guaranteed to increase the log likelihood function. In practice, the algorithm is deemed to have converged when the change Section 9.4 in the log likelihood function, or alternatively in the parameters, falls below some threshold. We illustrate the EM algorithm for a mixture of two Gaussians applied to the rescaled Old Faithful data set in Figure 9.8. Here a mixture of two Gaussians is used, with centres initialized using the same values as for the K-means algorithm in Figure 9.1, and with precision matrices initialized to be proportional to the unit matrix", "a079d2cd-180a-41aa-b928-5280c1dbd245": "If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be temporal-di\u21b5erence (TD) learning. TD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas. Like Monte Carlo methods, TD methods can learn directly from raw experience without a model of the environment\u2019s dynamics. Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a \ufb01nal outcome (they bootstrap). The relationship between TD, DP, and Monte Carlo methods is a recurring theme in the theory of reinforcement learning; this chapter is the beginning of our exploration of it. Before we are done, we will see that these ideas and methods blend into each other and can be combined in many ways. In particular, in Chapter 7 we introduce n-step algorithms, which provide a bridge from TD to Monte Carlo methods, and in Chapter 12 we introduce the TD(\u03bb) algorithm, which seamlessly uni\ufb01es them.\n\nAs usual, we start by focusing on the policy evaluation or prediction problem, the problem of estimating the value function v\u21e1 for a given policy \u21e1", "bdfec767-5fa8-4dea-b840-0dbc2bde1776": "DEEP FEEDFORWARD NETWORKS  O  Figure 6.9: A computational graph that results in repeated subexpressions when computing the gradient. Let w \u20ac R be the input to the graph. We use the same function f :R \u2014 R as the operation that we apply at every step of a chain: x = f(w), y= f(2), z= f(y). To compute 22. we apply equation 6.44 and obtain:  Oz  Fa (6.49) _ dz dy Oe 5 ~ Oy Ox Ow (6.50) =f\") f(a) fw) (6.51) FILO E La WNV EF aNV \u00a31 0.. (@ KAY  https://www.deeplearningbook.org/contents/mlp.html    SP SUM IIIS INET We \\u-vZ) Equation 6.51 suggests an implementation in which we compute the value of fw) only once and store it in the variable Z.\n\nThis is the approach taken by the back-propagation algorithm. An alternative approach is suggested by equation 6.52, where the subexpression f(w) appears more than once", "a86d94fc-1e9a-49b3-938d-4fdf03a15e45": "Note that the value of w is not observed, and so w is an example of a latent variable, also known as a hidden variable.\n\nSuch variables play a crucial role in many probabilistic models and will form the focus of Chapters 9 and 12. Having observed the values {tn} we can, if desired, evaluate the posterior distribution of the polynomial coef\ufb01cients w as discussed in Section 1.2.5. For the moment, we note that this involves a straightforward application of Bayes\u2019 theorem where again we have omitted the deterministic parameters in order to keep the notation uncluttered. In general, model parameters such as w are of little direct interest in themselves, because our ultimate goal is to make predictions for new input values. Suppose we are given a new input value \ufffdx and we wish to \ufb01nd the corresponding probability distribution for\ufffdt conditioned on the observed data", "cbe996a6-d239-4722-9e10-0578dc02e5ea": "These nested categories form a tree, with words at the leaves. In a balanced tree, the tree has depth O(log|Y]). The probability of choosing a word is given by the product of the probabilities of choosing the branch leading to that word at every node on a path from the root of the tree to the leaf containing the word. Figure 12.4 illustrates a simple example.\n\nMnih and Hinton  also describe how to use multiple paths to identify a single word in order to better model words that have multiple meanings. Computing the probability of a word then involves summation over all the paths that lead to that word. To predict the conditional probabilities required at each node of the tree, we typically use a logistic regression model at each node of the tree, and provide the same context C\u2019 as input to all these models. Because the correct output is encoded in the training set, we can use supervised learning to train the logistic regression models. This is typically done using a standard cross-entropy loss, corresponding to maximizing the log-likelihood of the correct sequence of decisions", "b098993a-d188-472f-97b6-e6a2dd4664bb": "If the player holds an ace that he could count as 11 without going bust, then the ace is said to be usable.\n\nIn this case it is always counted as 11 because counting it as 1 would make the sum 11 or less, in which case there is no decision to be made because, obviously, the player should always hit. Thus, the player makes decisions on the basis of three variables: his current sum (12\u201321), the dealer\u2019s one showing card (ace\u201310), and whether or not he holds a usable ace. This makes for a total of 200 states. Consider the policy that sticks if the player\u2019s sum is 20 or 21, and otherwise hits. To \ufb01nd the state-value function for this policy by a Monte Carlo approach, one simulates many blackjack games using the policy and averages the returns following each state. In this way, we obtained the estimates of the state-value function shown in Figure 5.1. The estimates for states with a usable ace are less certain and less regular because these states are less common. In any event, after 500,000 games the value function is very well approximated. Exercise 5.1 Consider the diagrams on the right in Figure 5.1", "25180d3c-be68-4963-8679-0d6b4d635829": "It is in part a game of chance, and it is a popular vehicle for waging signi\ufb01cant sums of money. There are probably more professional backgammon players than there are professional chess players. The game is played with 15 white and 15 black pieces on a board of 24 locations, called points. To the right on the next page is shown a typical position early in the game, seen from the perspective of the white player. White here has just rolled the dice and obtained a 5 and a 2. This means that he can move one of his pieces 5 steps and one (possibly the same piece) 2 steps. For example, he could move two pieces from the 12 point, one to the 17 point, and one to the 14 point. White\u2019s objective is to advance all of his pieces into the last quadrant (points 19\u201324) and then o\u21b5 the board. The \ufb01rst player to remove all his pieces wins. One complication is that the pieces interact as they pass each other going in di\u21b5erent directions", "a3337fa0-77f9-4c89-964f-9eb687c40110": "Methods based on changing w using the gradient of the error function cannot then be applied, because the gradient is zero almost everywhere. We therefore consider an alternative error function known as the perceptron criterion. To derive this, we note that we are seeking a weight vector w such that patterns xn in class C1 will have wT\u03c6(xn) > 0, whereas patterns xn in class C2 have wT\u03c6(xn) < 0. Using the t \u2208 {\u22121, +1} target coding scheme it follows that we would like all patterns to satisfy wT\u03c6(xn)tn > 0. The perceptron criterion associates zero error with any pattern that is correctly classi\ufb01ed, whereas for a misclassi\ufb01ed pattern xn it tries to minimize the quantity \u2212wT\u03c6(xn)tn. The perceptron criterion is therefore given by Rosenblatt\u2019s perceptron played an important role in the history of machine learning", "efa7a223-1c9e-450a-b5e1-10a513d884c5": "These two problems mean that, in the context of deep learning, we rarely use empirical risk minimization.\n\nInstead, we must use a slightly different approach, in which the quantity that we actually optimize is even more different from the quantity that we truly want to optimize. 8.1.2 Surrogate Loss Functions and Early Stopping  Sometimes, the loss function we actually care about (say, classification error) is not one that can be optimized efficiently. For example, exactly minimizing expected 0-1  https://www.deeplearningbook.org/contents/optimization.html    loss 1s typically intractable emer in the input dimension), even tor a linear classifier . In such situations, one typically optimizes a Surrogate loss function instead; which acts as a proxy but has advantages. For example, the negative log-likelihood of the correct class is typically used as a surrogate for the 0-1 loss. The negative log-likelihood allows the model to estimate the conditional probability of the classes, given the input, and if the model can do that well, then it can pick the classes that yield the least classification error in expectation. 273  CHAPTER 8", "1c5e758d-f65b-415a-87f2-eacc69d0078f": "youuey> v0 ~  I 9 41OMION   ainqoayyaie uoneyuaWBny Wes ay Jo UONeNSN||| OF \u2018B14  ajdwes [ aseqeiep sajdwes 7  uon22/95 woasaja5 wopuey wopuey xs xs exe sjouuey> sjauueys sjouuey> sjauueys ze ve oT ow og \u2014 sjauce2 omy visu \u2018or andr sajdues wopues om  T v OMEN  Shorten and Khoshgoftaar J Big Data  6:60   o 0 0 \u00a9 \u00a9 100 Fig. 31 On the gender recognition task, the image to the left is an example of an instance produced by Network-A in Smart Augmentation given the right images as input   the significant performance increase with the Smart Augmentation meta-learning strat- egy (Fig. 31)", "146a1ab7-17b1-4205-8ec1-ffcacd4c48bc": "https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Log  Learner and Meta-Learner Another popular view of meta-learning decomposes the model update into two stages:  A classifier f is the \u201clearner\u201d model, trained for operating a given task; In the meantime, a optimizer gg learns how to update the learner model's parameters via the support set S, 0\u2019 = g4(0,S)", "441d6f3e-b3f4-4073-b802-74319c7c8a23": "They can be completely determined by low-level sensations, such as direct sensor readings, or they can be more high-level and abstract, such as symbolic descriptions of objects in a room. Some of what makes up a state could be based on memory of past sensations or even be entirely mental or subjective. For example, an agent could be in the state of not being sure where an object is, or of having just been surprised in some clearly de\ufb01ned sense. Similarly, some actions might be totally mental or computational. For example, some actions might control what an agent chooses to think about, or where it focuses its attention. In general, actions can be any decisions we want to learn how to make, and the states can be anything we can know that might be useful in making them. In particular, the boundary between agent and environment is typically not the same as the physical boundary of a robot\u2019s or animal\u2019s body. Usually, the boundary is drawn closer to the agent than that.\n\nFor example, the motors and mechanical linkages of a robot and its sensing hardware should usually be considered parts of the environment rather than parts of the agent", "5c755c23-818c-4d57-b585-aa122b26d043": "Trained models are evaluated on the full binary-class test set. Results Table 3 shows the classi\ufb01cation results on SST-2 with varying imbalance ratios. We can see our data weighting performs best across all settings. In particular, the improvement over the base model increases as the data gets more imbalanced, ranging from around 6 accuracy points on 100:1000 to over 20 accuracy points on 20:1000. Our method is again consistently better than , validating that the parametric treatment is bene\ufb01cial. The proportion-based data weighting provides only limited improvement, showing the advantage of adaptive data weighting. The base model trained on the joint training-validation data for \ufb01xed steps fails to perform well, partly due to the lack of a proper mechanism for selecting steps. We also tested the text augmentation LM on the SST-2 imbalanced data. Interestingly, the augmentation tends to hinder model training and yields accuracy of around 50% (random guess)", "38cbef31-4a4b-43de-a92e-8ebe3093b5a6": "A little chemical memory and computation causes the frequency of \ufb02agellar reversal to decrease when the bacterium swims toward higher concentrations of molecules it needs to survive (attractants) and increase when the bacterium swims toward higher concentrations of molecules that are harmful (repellants). The result is that the bacterium tends to persist in swimming up attractant gradients and tends to avoid swimming up repellant gradients. The chemotactic behavior just described is called klinokinesis. It is a kind of trialand-error behavior, although it is unlikely that learning is involved: the bacterium needs a modicum of short-term memory to detect molecular concentration gradients, but it probably does not maintain long-term memories. Arti\ufb01cial intelligence pioneer Oliver Selfridge called this strategy \u201crun and twiddle,\u201d pointing out its utility as a basic adaptive strategy: \u201ckeep going in the same way if things are getting better, and otherwise move around\u201d .\n\nSimilarly, one might think of a neuron \u201cswimming\u201d (not literally of course) in a medium composed of the complex collection of feedback loops in which it is embedded, acting to obtain one type of input signal and to avoid others", "933090fe-38c1-432d-9651-f8093ddac6df": "AAAI Press, Menlo Park, CA. Maei, H. R. Gradient Temporal-Di\u21b5erence Learning Algorithms. Ph.D. thesis, University Maei, H. R. Convergent actor-critic algorithms under o\u21b5-policy training and function Maei, H. R., Sutton, R. S. GQ(\u03bb): A general gradient algorithm for temporal-di\u21b5erence prediction learning with eligibility traces. In Proceedings of the Third Conference on Arti\ufb01cial General Intelligence, pp. 91\u201396. Maei, H. R., Szepesv\u00b4ari, Cs., Bhatnagar, S., Precup, D., Silver, D., Sutton, R. S. Convergent temporal-di\u21b5erence learning with arbitrary smooth function approximation. In Advances in Neural Information Processing Systems 22 , pp. 1204\u20131212. Curran Associates, Inc. Maei, H", "234d5206-531b-42e2-aff0-9f2e8705f1ea": "The \ufb01rst of these is a \u2018Gaussian\u2019 kernel of the form (6.23), and the second is the exponential kernel given by k(x, x\u2032) = exp (\u2212\u03b8 |x \u2212 x\u2032|) (6.56) which corresponds to the Ornstein-Uhlenbeck process originally introduced by Uhlenbeck and Ornstein  to describe Brownian motion. In order to apply Gaussian process models to the problem of regression, we need to take account of the noise on the observed target values, which are given by where yn = y(xn), and \u03f5n is a random noise variable whose value is chosen independently for each observation n. Here we shall consider noise processes that have a Gaussian distribution, so that where \u03b2 is a hyperparameter representing the precision of the noise. Because the noise is independent for each data point, the joint distribution of the target values t = (t1, . , tN)T conditioned on the values of y = (y1,", "e7b19681-2a55-4e26-bd2a-2c3a3af057e4": "This two-stage optimization is then repeated until convergence. We shall see that these two stages of updating rnk and updating \u00b5k correspond respectively to the E (expectation) and M (maximization) steps of the EM algorithm, and to emphasize this we shall use the Section 9.4 terms E step and M step in the context of the K-means algorithm. Consider \ufb01rst the determination of the rnk. Because J in (9.1) is a linear function of rnk, this optimization can be performed easily to give a closed form solution. The terms involving different n are independent and so we can optimize for each n separately by choosing rnk to be 1 for whichever value of k gives the minimum value of \u2225xn \u2212 \u00b5k\u22252. In other words, we simply assign the nth data point to the closest cluster centre. More formally, this can be expressed as Now consider the optimization of the \u00b5k with the rnk held \ufb01xed", "7ce2ec39-cded-4693-9afd-26608fd5d828": "By contrast, basis functions that are localized to \ufb01nite regions of input space necessarily comprise a spectrum of different spatial frequencies. In many signal processing applications, it is of interest to consider basis functions that are localized in both space and frequency, leading to a class of functions known as wavelets. These are also de\ufb01ned to be mutually orthogonal, to simplify their application. Wavelets are most applicable when the input values live centre, and sigmoidal of the form (3.5) on the right. on a regular lattice, such as the successive time points in a temporal sequence, or the pixels in an image. Useful texts on wavelets include Ogden , Mallat , and Vidakovic .\n\nMost of the discussion in this chapter, however, is independent of the particular choice of basis function set, and so for most of our discussion we shall not specify the particular form of the basis functions, except for the purposes of numerical illustration. Indeed, much of our discussion will be equally applicable to the situation in which the vector \u03c6(x) of basis functions is simply the identity \u03c6(x) = x. Furthermore, in order to keep the notation simple, we shall focus on the case of a single target variable t", "3fd4ca5a-3a47-4882-a610-c834b3386812": "We want our states to be compact as well as Markov. There is a similar issue regarding how state is obtained and updated. We don\u2019t really want a function f that takes whole histories.\n\nInstead, for computational reasons we prefer to obtain the same e\u21b5ect as f with an incremental, recursive update that computes St+1 from St, incorporating the next increment of data, At and Ot+1: with the \ufb01rst state S0 given. The function u is called the state-update function. For example, if f were the identity (St =Ht), then u would merely extend St by appending At and Ot+1 to it. Given f, it is always possible to construct a corresponding u, but it may not be computationally convenient and, as in the identity example, it may not produce a compact state. The state-update function is a central part of any agent architecture that handles partial observability. It must be e\ufb03ciently computable, as no actions or predictions can be made until the state is available. An overall diagram of such an agent architecture is given in Figure 17.1", "0de9c0dd-57a1-473f-acb1-0cc7a6377c4b": "We write scalars in italics. We usually give scalars lowercase variable names.\n\nWhen we introduce them, we specify what kind of number they are. For  https://www.deeplearningbook.org/contents/linear_algebra.html    29  CHAPTER 2. LINEAR ALGEBRA  example, we might say \u201cLet s \u20ac R be the slope of the line,\u201d while defining a real-valued scalar, or \u201cLet n \u20ac N be the number of units,\u201d while defining a natural number scalar. e Vectors: A vector is an array of numbers. The numbers are arranged in order. We can identify each individual number by its index in that ordering. Typically we give vectors lowercase names in bold typeface, such as \u00ab. The elements of the vector are identified by writing its name in italic typeface, with a subscript. The first element of a is x, the second element is x2, and so on. We also need to say what kind of numbers are stored in the vector", "b458ef9d-fb89-467d-8361-d01e9b3719f5": "Correct choice requires taking into account indirect, delayed consequences of actions, and thus may require foresight or planning. At the same time, in all of these examples the e\u21b5ects of actions cannot be fully predicted; thus the agent must monitor its environment frequently and react appropriately. For example, Phil must watch the milk he pours into his cereal bowl to keep it from over\ufb02owing. All these examples involve goals that are explicit in the sense that the agent can judge progress toward its goal based on what it can sense directly. The chess player knows whether or not he wins, the re\ufb01nery controller knows how much petroleum is being produced, the gazelle calf knows when it falls, the mobile robot knows when its batteries run down, and Phil knows whether or not he is enjoying his breakfast. In all of these examples the agent can use its experience to improve its performance over time.\n\nThe chess player re\ufb01nes the intuition he uses to evaluate positions, thereby improving his play; the gazelle calf improves the e\ufb03ciency with which it can run; Phil learns to streamline making his breakfast", "72799e67-c411-442b-8792-5ce5c3b2c5a9": "This approach also allows us to estimate the ratio between the partition  622  CHAPTER 18. CONFRONTING THE PARTITION FUNCTION  functions as K  1 By (x(*)) Spe ell) 18.45 K do (x) s.t. : x Po ( ) This value can then be used directly to compare two models as described in equation 18.39. If the distribution po is close to p;, equation 18.44 can be an effective way of estimating the partition function . Unfortunately, most of the time py is both complicated (usually multimodal) and defined over a high-dimensional space. It is difficult to find a tractable pp that is simple enough to evaluate while still being close enough to p; to result in a high-quality approximation.\n\nIf po and p, are not close, most samples from po will have low probability under p; and therefore make (relatively) negligible contributions to the sum in equation 18.44.  https://www.deeplearningbook.org/contents/partition.html    Having few samples with significant weights in this sum will result in an estimator that is of poor quality because of high variance", "000a3c25-aae0-4a09-b390-026231d9a560": "The primary  https://www.deeplearningbook.org/contents/optimization.html    computational qiftiqult in applying Newton\u2019s update is the calculation of the inverse Hessian Th he approach adopted by quasi-Newton methods (of which the BFGS algorithm i is the most prominent) is to approximate the inverse with a matrix M, that is iteratively refined by low-rank updates to become a better  approximation of H~!. The specification and derivation of the BFGS approximation is given in many textbooks on optimization, including in Luenberger . Once the inverse Hessian approximation M, is updated, the direction of descent pz: is determined by p; = M;g;. A line search is performed in this direction to determine the size of the step, e*, taken in this direction. The final update to the parameters is given by  Or41= 6: + \u20ac pt. (8.33)  312  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  Like the method of conjugate gradients, the BFGS algorithm iterates a series of line searches with the direction incorporating second-order information", "4e97dbe6-3024-4f5f-abdd-b58c45b531e7": "Another form of GEM algorithm, known as the expectation conditional maximization, or ECM, algorithm, involves making several constrained optimizations within each M step .\n\nFor instance, the parameters might be partitioned into groups, and the M step is broken down into multiple steps each of which involves optimizing one of the subset with the remainder held \ufb01xed. We can similarly generalize the E step of the EM algorithm by performing a partial, rather than complete, optimization of L(q, \u03b8) with respect to q(Z) . As we have seen, for any given value of \u03b8 there is a unique maximum of L(q, \u03b8) with respect to q(Z) that corresponds to the posterior distribution q\u03b8(Z) = p(Z|X, \u03b8) and that for this choice of q(Z) the bound L(q, \u03b8) is equal to the log likelihood function ln p(X|\u03b8). It follows that any algorithm that converges to the global maximum of L(q, \u03b8) will \ufb01nd a value of \u03b8 that is also a global maximum of the log likelihood ln p(X|\u03b8)", "c9534e83-c83d-45a7-92f8-dbf50cc35800": "Due to the complex relationships between the object position or orientation and the pixel intensities, this manifold will be highly nonlinear.\n\nIf the goal is to learn a model that can take an input image and output the orientation of the object irrespective of its position, then there is only one degree of freedom of variability within the manifold that is signi\ufb01cant. We have seen in Section 1.2 how probability theory provides us with a consistent mathematical framework for quantifying and manipulating uncertainty. Here we turn to a discussion of decision theory that, when combined with probability theory, allows us to make optimal decisions in situations involving uncertainty such as those encountered in pattern recognition. Suppose we have an input vector x together with a corresponding vector t of target variables, and our goal is to predict t given a new value for x. For regression problems, t will comprise continuous variables, whereas for classi\ufb01cation problems t will represent class labels. The joint probability distribution p(x, t) provides a complete summary of the uncertainty associated with these variables. Determination of p(x, t) from a set of training data is an example of inference and is typically a very dif\ufb01cult problem whose solution forms the subject of much of this book", "db347247-c050-40f6-9481-ccbfccce6555": "See you in the next post! :)  Cited as:  @article{weng2018bandit,  title = \"A (Long) Peek into Reinforcement Learning\", author = \u201cWeng, Lilian\", journal = \u201clilianweng.github.io\", year = \"2018\", url = \"https://lilianweng. github. io/posts/2018-@2-19-rl-overview/\" + References   Yuxi Li. Deep reinforcement learning: An overview.\n\narXiv preprint arXiv:1701.07274. 2017. Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction; 2nd Edition. 2017. Volodymyr Mnih, et al. Asynchronous methods for deep reinforcement learning. ICML. 2016. Tim Salimans, et al. Evolution strategies as a scalable alternative to reinforcement learning. arXiv preprint arXiv:1703.03864 . David Silver, et al", "b1a8a1b3-7d6a-417a-86bd-5d2f235abeff": "(3.21)  See figure 3.1 for a plot of the normal distribution density function. The two parameters 4p \u20ac R and a \u20ac (0,00) control the normal distribution. The parameter j, gives the coordinate of the central peak. This is also the mean of the distribution: E = yu. The standard deviation of the distribution is given by o, and the variance by o?. When we evaluate the PDF, we need to square and invert 0. When we need to frequently evaluate the PDF with different parameter values, a more efficient way of parametrizing the distribution is to use a parameter 6 \u20ac (0, co) to control the precision, or inverse variance, of the distribution:  N (a; 1,87) ew ( 5 Bla Ww). (3.22)  Normal distributions are a sensible choice for many applications. In the absence  of prior knowledge about what form a distribution over the real numbers should take, the normal distribution is a good default choice for two major reasons", "05cc825d-086f-4c16-819b-1647bb52fbe0": "The external input gate unit g? is computed similarly to the forget gate (with a sigmoid unit to obtain a gating value between 0 and 1), but with its own parameters:  -1 gaa bet US ai) + Why yd, (10.42) J J  The output no of the LSM coDenn also be et off, via ih output gate 0, which also uses a sigmoid unit for gating:  https://www.deeplearningbook.org/contents/rnn.html    nf? = tanh (*\u201d) qh, (10.43)  (=o (oer Done + Oowye? j J  UJ ,  406  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  which has parameters 6\u00b0, U\u00b0, W\u00b0 for its biases, input weights and recurrent (t) i as an extra input (with its weight) into the three gates of the i-th unit, as shown in figure 10.16. This would require three additional parameters. weights, respectively", "34132554-b299-4cd0-a16f-04b801407766": "REGULARIZATION FOR DEEP LEARNING  approach\u2019s success. It is just a means of approximating the sum over all submodels. Wang and Manning  derived analytical approximations to this marginaliza- tion. Their approximation, known as fast dropout, resulted in faster convergence time due to the reduced stochasticity in the computation of the gradient. This method can also be applied at test time, as a more principled (but also more computationally expensive) approximation to the average over all sub-networks than the weight scaling approximation. Fast dropout has been used to nearly match the performance of standard dropout on small neural network problems, but has not yet yielded a significant improvement or been applied to a large problem.\n\nJust as stochasticity is not necessary to achieve the regularizing effect of dropout, it is also not sufficient. To demonstrate this, Warde-Farley et al. designed control experiments using a method called dropout boosting, which they designed to use exactly the same mask noise as traditional dropout but lack its regularizing effect. Dropout boosting trains the entire ensemble to jointly maximize the log-likelihood on the training set", "5df71be8-7828-4f1e-b561-1b160aac2d2c": "The fading-trace bootstrapping strategy of Sarsa(\u03bb) appears to result in more e\ufb03cient learning on this problem.\n\nThere is also an action-value version of our ideal TD method, the online \u03bb-return algorithm (Section 12.4) and its e\ufb03cient implementation as true online TD(\u03bb) (Section 12.5). Everything in Section 12.4 goes through without change other than to use the action-value form of the n-step return given at the beginning of the current section. The analyses in Sections 12.5 and 12.6 also carry through for action values, the only change being the use of state\u2013action feature vectors xt = x(St, At) instead of state feature vectors xt = x(St). Pseudocode for the resulting e\ufb03cient algorithm, called true online Sarsa(\u03bb) is given in the box on the next page. The \ufb01gure below compares the performance of various versions of Sarsa(\u03bb) on the Mountain Car example", "8f516dc5-9717-4c37-bf3f-07b0deded1b4": "Joglekar, M., Garcia-Molina, H., Parameswaran, A.: Comprehensive and reliable crowd assessment algorithms. In: International Conference on Data Engineering (ICDE)  26. Khandwala, N., Ratner, A., Dunnmon, J., Goldman, R., Lungren, M., Rubin, D., R\u00e9, C.: Cross-modal data programming for medical images. NIPS ML4H Workshop  27. Kingma, D., Ba, J.: Adam: A method for stochastic optimization  arXiv preprint arXiv:1412.6980 28. Ku, J.P., Hicks, J.L., Hastie, T., Leskovec, J., R\u00e9, C., Delp, S.L. : The Mobilize center: an NIH big data to knowledge center to advance human movement research and improve mobility. J. Am. Med. Inf. Assoc. 22(6), 1120\u20131125  29", "e0ada652-fc33-43f9-9332-9f0d4f25c5c0": "Parallel Augmentation  This category of approaches produce two noise versions of one anchor image and aim to learn representation such that these two augmented samples share the same embedding.\n\nSimCLR  SimCLR  proposed a simple framework for contrastive learning of visual representations. It learns representations for visual inputs by maximizing agreement between differently augmented views of the same sample via a contrastive loss in the latent space. https://lilianweng.github.io/posts/2021-05-31-contrastive/     Contrastive Representation Learning | Lil'Log   Maximize agreement  z+ - 2; a) Ja) hy <\u00ab\u2014 Representation \u2014> hy  Randomly sample a minibatch of N\u2019 samples and each sample is applied with two different data augmentation operations, resulting in 2V augmented samples in total. Ki =t(x), xj =t(x), tt oT  where two separate data augmentation operators, \u00a2 and t\u2019, are sampled from the same family of augmentations 7 . Data augmentation includes random crop, resize with random flip, color  distortions, and Gaussian blur", "f9529461-d533-4b03-9e0e-974efc5d08f1": "The last reference in particular gives a powerful forward view for o\u21b5-policy TD methods with general state-dependent \u03bb and \u03b3. The presentation here seems to be new. This section ends with an elegant Expected Sarsa(\u03bb) algorithm. Although it is a natural algorithm, to our knowledge it has not previously been described or tested in the literature. 12.10 Watkins\u2019s Q(\u03bb) is due to Watkins . The tabular, episodic, o\u270fine version has been proven convergent by Munos, Stepleton, Harutyunyan, and Bellemare . Alternative Q(\u03bb) algorithms were proposed by Peng and Williams  and by Sutton, Mahmood, Precup, and van Hasselt . Tree Backup(\u03bb) is due to Precup, Sutton, and Singh . Gradient-TD methods are by Yu . Emphatic TD(\u03bb) was introduced by Sutton, Mahmood, and White , who proved its stability. Yu  proved its convergence, and the algorithm was developed further by Hallak et al. In this chapter we consider something new", "0998b1a7-1c7f-448a-9a78-d2c0c8252340": "Thus, in spaces of high dimensionality, most of the volume of a sphere is concentrated in a thin shell near the surface!\n\nAs a further example, of direct relevance to pattern recognition, consider the behaviour of a Gaussian distribution in a high-dimensional space. If we transform from Cartesian to polar coordinates, and then integrate out the directional variables, we obtain an expression for the density p(r) as a function of radius r from the origin. Exercise 1.20 Thus p(r)\u03b4r is the probability mass inside a thin shell of thickness \u03b4r located at radius r. This distribution is plotted, for various values of D, in Figure 1.23, and we see that for large D the probability mass of the Gaussian is concentrated in a thin shell. The severe dif\ufb01culty that can arise in spaces of many dimensions is sometimes called the curse of dimensionality . In this book, we shall make extensive use of illustrative examples involving input spaces of one or two dimensions, because this makes it particularly easy to illustrate the techniques graphically. The reader should be warned, however, that not all intuitions developed in spaces of low dimensionality will generalize to spaces of many dimensions", "edfdadc1-5bfd-42ec-b490-017613c68f6a": "Instead, we\u2019ll introduce a method for learning the recognition model parameters \u03c6 jointly with the generative model parameters \u03b8. From a coding theory perspective, the unobserved variables z have an interpretation as a latent representation or code. In this paper we will therefore also refer to the recognition model q\u03c6(z|x) as a probabilistic encoder, since given a datapoint x it produces a distribution (e.g. a Gaussian) over the possible values of the code z from which the datapoint x could have been generated", "922f4b55-4b09-424a-8e71-2182661fae09": "14.2 Regularized Autoencoders  Undercomplete autoencoders, with code dimension less than the input dimension, can learn the most salient features of the data distribution. We have seen that these autoencoders fail to learn anything useful if the encoder and decoder are given too much capacity. A similar problem occurs if the hidden code is allowed to have dimension equal to the input, and in the overcomplete case in which the hidden code has dimension greater than the input. In these cases, even a linear encoder and a linear decoder can learn to copy the input to the output without learning anything useful about the data distribution. Ideally, one could train any architecture of autoencoder successfully, choosing the code dimension and the capacity of the encoder and decoder based on the complexity of distribution to be modeled. Regularized autoencoders provide the ability to do so", "48cc3e53-c249-4863-bba5-8991f3b1aac8": "On each transition from state St to state St+1, taking action At and receiving action Rt+1, that algorithm computes the TD error (\u03b4) and then updates the eligibility trace vectors (zw the parameters for the critic and actor (w and \u2713), according to where \u03b3 2  and \u03bbwa 2  are bootstrapping parameters for the critic and the actor respectively, and \u21b5w > 0 and \u21b5\u2713 > 0 are analogous step-size parameters. Think of the approximate value function \u02c6v as the output of a single linear neuron-like unit, called the critic unit and labeled V in Figure 15.5a. Then the value function is a linear function of the feature-vector representation of state s, x(s) = (x1(s), . , xn(s))>, parameterized by a weight vector w = (w1, . , wn)>: Each xi(s) is like the presynaptic signal to a neuron\u2019s synapse whose e\ufb03cacy is wi", "eb25aa2a-1cec-43ef-a5b1-1491e63e489b": "This is useful, for instance, when A is large and diagonal, and hence easy to invert, while B has many rows but few columns (and conversely for C) so that the right-hand side is much cheaper to evaluate than the left-hand side. A set of vectors {a1, . , aN} is said to be linearly independent if the relation \ufffd n \u03b1nan = 0 holds only if all \u03b1n = 0. This implies that none of the vectors can be expressed as a linear combination of the remainder. The rank of a matrix is the maximum number of linearly independent rows (or equivalently the maximum number of linearly independent columns). Trace and determinant apply to square matrices. The trace Tr(A) of a matrix A is de\ufb01ned as the sum of the elements on the leading diagonal", "2e6be908-4570-4259-8e70-279fc79d70dd": "When \ufb01ne-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100\u00d7 fewer labels.\n\n1 Learning effective visual representations without human supervision is a long-standing problem. Most mainstream approaches fall into one of two classes: generative or discriminative. Generative approaches learn to generate or otherwise model pixels in the input space . 1Google Research, Brain Team. Correspondence to: Ting Chen <iamtingchen@google.com>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). 1Code available at https://github.com/google-research/simclr. However, pixel-level generation is computationally expensive and may not be necessary for representation learning. Discriminative approaches learn representations using objective functions similar to those used for supervised learning, but train networks to perform pretext tasks where both the inputs and labels are derived from an unlabeled dataset. Many such approaches have relied on heuristics to design pretext tasks , which could limit the generality of the learned representations", "9d6f1c83-30ad-414a-a19d-ad8367d6a8c6": "It is easy to show that the representation on the righthand side of (8.5) is always correctly normalized provided the individual conditional distributions are normalized. Exercise 8.1 The directed graphs that we are considering are subject to an important restriction namely that there must be no directed cycles, in other words there are no closed paths within the graph such that we can move from node to node along links following the direction of the arrows and end up back at the starting node.\n\nSuch graphs are also called directed acyclic graphs, or DAGs. This is equivalent to the statement that Exercise 8.2 there exists an ordering of the nodes such that there are no links that go from any node to any lower numbered node. As an illustration of the use of directed graphs to describe probability distributions, we consider the Bayesian polynomial regression model introduced in Section 1.2.6. The random variables in this model are the vector of polynomial coef\ufb01cients w and the observed data t = (t1, . , tN)T. In addition, this model contains the input data x = (x1,", "e2904b45-fbc2-4311-b0c9-b22a8c1e5165": "One solution to this dilemma is to use cross-validation, which is illustrated in Figure 1.18. This allows a proportion (S \u2212 1)/S of the available data to be used for training while making use of all of the data to assess performance. When data is particularly scarce, it may be appropriate to consider the case S = N, where N is the total number of data points, which gives the leave-one-out technique.\n\nOne major drawback of cross-validation is that the number of training runs that must be performed is increased by a factor of S, and this can prove problematic for models in which the training is itself computationally expensive. A further problem with techniques such as cross-validation that use separate data to assess performance is that we might have multiple complexity parameters for a single model (for instance, there might be several regularization parameters). Exploring combinations of settings for such parameters could, in the worst case, require a number of training runs that is exponential in the number of parameters. Clearly, we need a better approach. Ideally, this should rely only on the training data and should allow multiple hyperparameters and model types to be compared in a single training run", "1c3859f8-6cea-412a-a2b6-c375c226af6e": "It provides a consistent estimator of probability distributions based on encouraging  https://www.deeplearningbook.org/contents/autoencoders.html    the model to have the same score as the data distribution at every training point z. In this context, the score is a particular gradient field:  Va log p(a). (14.15)  Score matching is discussed further in section 18.4. For the present discussion, regarding autoencoders, it is sufficient to understand that learning the gradient field of log paata is one way to learn the structure of pgata itself.\n\nA very important property of DAEs is that their training criterion (with conditionally Gaussian p(a | h)) makes the autoencoder learn a vector field (g( f(x)) \u2014 x) that estimates the score of the data distribution. This is illustrated in figure 14.4. Denoising training of a specific kind of autoencoder (sigmoidal hidden units, linear reconstruction units) using Gaussian noise and mean squared error as the reconstruction cost is equivalent  to training a specific kind of undirected probabilistic model called an RBM with Gaussian visible units", "5f381a04-f8da-4110-a915-6386942368bb": "Susskind, J., Anderson, A., and Hinton, G. E. The Toronto face dataset. Technical Report UTML TR 2010-001, U. Toronto. Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. J., and Fergus, R. Intriguing properties of neural networks. ICLR, abs/1312.6199. Tu, Z. Learning generative models via discriminative approaches. In Computer Vision and Pattern Recognition, 2007. CVPR\u201907. IEEE Conference on, pages 1\u20138. IEEE.", "158c2019-81eb-42d4-be41-57c75b7f91cd": "Gradient-based Reward Learning There is a rich line of research on learning the reward in reinforcement learning.\n\nOf particular interest to this work is  which learns a parametric intrinsic reward that additively transforms the original task reward (a.k.a extrinsic reward) to improve the policy optimization. For consistency of notations with above, formally, let p\u03b8(y|x) be a policy where y is an action and x is a state. Let Rin \u03c6 be the intrinsic reward with parameters \u03c6. In each iteration, the policy parameter \u03b8 is updated to maximize the joint rewards, through: where Lex+in is the expectation of the sum of extrinsic and intrinsic rewards; and \u03b3 is the step size. The equation shows \u03b8\u2032 depends on \u03c6, thus we can write as \u03b8\u2032 = \u03b8\u2032(\u03c6). The next step is to optimize the intrinsic reward parameters \u03c6. Recall that the ultimate measure of the performance of a policy is the value of extrinsic reward it achieves. Therefore, a good intrinsic reward is supposed to, when the policy is trained with it, increase the eventual extrinsic reward", "0c088506-b923-484a-94b2-0728eeee30c7": "13.10 (\u22c6 \u22c6 \u22c6) By applying the sum and product rules of probability, verify that the conditional independence properties (13.24)\u2013(13.31) are satis\ufb01ed by the joint distribution for the hidden Markov model de\ufb01ned by (13.6). 13.11 (\u22c6 \u22c6) Starting from the expression (8.72) for the marginal distribution over the variables of a factor in a factor graph, together with the results for the messages in the sum-product algorithm obtained in Section 13.2.3, derive the result (13.43) for the joint posterior distribution over two successive latent variables in a hidden Markov model. 13.12 (\u22c6 \u22c6) Suppose we wish to train a hidden Markov model by maximum likelihood using data that comprises R independent sequences of observations, which we denote by X(r) where r = 1, . , R. Show that in the E step of the EM algorithm, we simply evaluate posterior probabilities for the latent variables by running the \u03b1 and \u03b2 recursions independently for each of the sequences", "3d501073-231c-4cba-afee-8dd3e9b130a7": "Results are shown for n-step TD methods with a range of values for n and \u21b5.\n\nThe performance measure for each parameter setting, shown on the vertical axis, is the square-root of the average squared error between the predictions at the end of the episode for the 19 states and their true values, then averaged over the \ufb01rst 10 episodes and 100 repetitions of the whole experiment (the same sets of walks were used for all parameter settings). Note that methods with an intermediate value of n worked best. This illustrates how the generalization of TD and Monte Carlo methods to n-step methods can potentially perform better than either of the two extreme methods. Exercise 7.3 Why do you think a larger random walk task (19 states instead of 5) was used in the examples of this chapter? Would a smaller walk have shifted the advantage to a di\u21b5erent value of n? How about the change in left-side outcome from 0 to \u22121 made in the larger walk? Do you think that made any di\u21b5erence in the best value of n? \u21e4 How can n-step methods be used not just for prediction, but for control? In this section we show how n-step methods can be combined with Sarsa in a straightforward way to produce an on-policy TD control method", "25f0ccc9-757d-4366-9978-bdc163412fa6": "The argument in this case is that if a parametric transformation with k parameters can learn about r regions in input space, with k <r, and if obtaining such a representation was useful to the task of interest, then we could potentially generalize much better in this way than in a nondistributed setting, where we would need O(r) examples to obtain the same features and associated partitioning of the input space into r regions.\n\nUsing fewer parameters to represent the model means that we have fewer parameters to fit, and thus require far fewer training examples to generalize well. A further part of the argument for why models based on distributed represen- tations generalize well is that their capacity remains limited despite being able to distinctly encode so many different regions. For example, the VC dimension of a neural network of linear threshold units is only O(wlog w), where w is the number of weights . This limitation arises because, while we can assign very many unique codes to representation space, we cannot use absolutely all the code space, nor can we learn arbitrary functions mapping from the representation space h to the output y using a linear classifier", "1bf48a2d-1716-4a56-8685-71ec11f2e458": "The probability that x = x is denoted as P(x), with a probability of 1 indicating that x = x is certain and a probability of 0 indicating that x = x is impossible. Sometimes to disambiguate which PMF to use, we write the name of the random variable explicitly: P(x = x). Sometimes we define a variable first, then use ~ notation to specify which distribution it follows later: x ~ P(x). Probability mass functions can act on many variables at the same time. Such a probability distribution over many variables is known as a joint probability distribution. P(x = x,y = y) denotes the probability that x = x and y = y simultaneously. We may also write P(x, y) for brevity. To be a PMF on a random variable x, a function P must satisfy the following properties:  e The domain of P must be the set of all possible states of x.  e Vx \u20ac x,0 < P(x) < 1. An impossible event has probability 0, and no state can be less probable than that", "5ad2666e-a272-4822-ba62-daa83cbd9cb6": "Or  @article{weng2021contrastive,  title = \"Contrastive Representation Learning\", author = \"Weng, Lilian\", journal = \"lilianweng.github. io\", year = \"2021\", month = \"May\", url = \"https://lilianweng.github. io/posts/2021-05-31-contrastive/\" + References   Sumit Chopra, Raia Hadsell and Yann LeCun. \u201cLearning a similarity metric discriminatively, with application to face verification.\" CVPR 2005.  https://lilianweng.github.io/posts/2021-05-31-contrastive/  Contrastive Representation Learning | Lil'Log   2] Florian Schroff, Dmitry Kalenichenko and James Philbin. \u201cFaceNet: A Unified Embedding for  Face Recognition and Clustering.\" CVPR 2015. 3  Hyun Oh Song et al. \u201cDeep Metric Learning via Lifted Structured Feature Embedding.\" CVPR 2016. 4]  Ruslan Salakhutdinov and Geoff Hinton", "5c141785-48f6-421e-a760-cf2ee89938ab": "It remains unclear, however, how to exploit this observation to help better train and sample from deep generative models. Despite the difficulty of mixing, Monte Carlo techniques are useful and are often the best tool available. Indeed, they are the primary tool used to confront the intractable partition function of undirected models, discussed next. https://www.deeplearningbook.org/contents/monte_carlo.html    602  https://www.deeplearningbook.org/contents/monte_carlo.html", "850dcae9-cb88-48a3-81ab-405a6f3b518e": "For example, we can model the class-conditional densities p(y|Ck) using Gaussian distributions and then use the techniques of Section 1.2.4 to \ufb01nd the parameters of the Gaussian distributions by maximum likelihood. Having found Gaussian approximations to the projected classes, the formalism of Section 1.5.1 then gives an expression for the optimal threshold.\n\nSome justi\ufb01cation for the Gaussian assumption comes from the central limit theorem by noting that y = wTx is the sum of a set of random variables. The least-squares approach to the determination of a linear discriminant was based on the goal of making the model predictions as close as possible to a set of target values. By contrast, the Fisher criterion was derived by requiring maximum class separation in the output space. It is interesting to see the relationship between these two approaches. In particular, we shall show that, for the two-class problem, the Fisher criterion can be obtained as a special case of least squares. So far we have considered 1-of-K coding for the target values. If, however, we adopt a slightly different target coding scheme, then the least-squares solution for the weights becomes equivalent to the Fisher solution", "5c2767d1-89e7-4afe-a6a5-9911d55c51f9": "7  https://www.deeplearningbook.org/contents/prob.html    the real numbers. We can do this with a function U(X; a, 0), where a@ and 0 are the endpoints of the interval, with 6 > a. The \u201c;\u201d notation means \u201cparametrized by\u201d;  we consider Z to be the argument of the function, while @ and 0 are parameters that define the function. To ensure that there is no probability mass outside the  interval, we say u(a;a,b) = 0 for all a \u00a2 . Within , u(aja,b) = -4. We can see that this is non-negative everywhere. Additionally, it integrates to 1. We often denote that x follows the uniform distribution on  by writing x ~ U(a, b). 3.4 Marginal Probability  Sometimes we know the probability distribution over a set of variables and we want to know the probability distribution over just a subset of them. The probability distribution over the subset is known as the marginal probability distribution. For example, suppose we have discrete random variables x and y, and we know P(x,y)", "cf5ee3e4-566c-4951-a2c6-8506ef91202a": "This is most easily done by substituting the expressions for the Gaussian prior q(w) = N(w|m0, S0), together with the lower bound h(w, \u03be) on the likelihood function, into the integral (10.159) which de\ufb01nes L(\u03be). Next gather together the terms which depend on w in the exponential and complete the square to give a Gaussian integral, which can then be evaluated by invoking the standard result for the normalization coef\ufb01cient of a multivariate Gaussian. Finally take the logarithm to obtain (10.164). where Zj is the normalization constant de\ufb01ned by (10.197). By applying this result recursively, and initializing with p0(D) = 1, derive the result 10.37 (\u22c6) www Consider the expectation propagation algorithm from Section 10.7, and suppose that one of the factors f0(\u03b8) in the de\ufb01nition (10.188) has the same exponential family functional form as the approximating distribution q(\u03b8).\n\nShow that if the factor \ufffdf0(\u03b8) is initialized to be f0(\u03b8), then an EP update to re\ufb01ne \ufffdf0(\u03b8) leaves \ufffdf0(\u03b8) unchanged", "891f7653-0815-44fd-8b4a-3ae480e4737d": "In each of the 47 states there are four actions, up, down, right, and left, which take the agent deterministically to the corresponding neighboring states, except when movement is blocked by an obstacle or the edge of the maze, in which case the agent remains where it is. Reward is zero on all transitions, except those into the goal state, on which it is +1.\n\nAfter reaching the goal state (G), the agent returns to the start state (S) to begin a new episode. This is a discounted, episodic task with \u03b3 = 0.95. The main part of Figure 8.2 shows average learning curves from an experiment in which Dyna-Q agents were applied to the maze task. The initial action values were zero, the step-size parameter was \u21b5 = 0.1, and the exploration parameter was \" = 0.1. When selecting greedily among actions, ties were broken randomly. The agents varied in the number of planning steps, n, they performed per real step. For each n, the curves show the number of steps taken by the agent to reach the goal in each episode, averaged over 30 repetitions of the experiment. In each repetition, the initial seed for the random number generator was held constant across algorithms", "cd212a8f-e7cf-4d8c-944d-83bf4f2e3af3": "The conditional p(xi|pai) will depend on the parents of node xi, whereas the conditionals p(xk|pak) will depend on the children of xi as well as on the co-parents, in other words variables corresponding to parents of node xk other than node xi. The set of nodes comprising the parents, the children and the co-parents is called the Markov blanket and is illustrated in Figure 8.26. We can think of the Markov blanket of a node xi as being the minimal set of nodes that isolates xi from the rest of the graph. Note that it is not suf\ufb01cient to include only the parents and children of node xi because the phenomenon of explaining away means that observations of the child nodes will not block paths to the co-parents. We must therefore observe the co-parent nodes also.\n\nWe have seen that directed graphical models specify a factorization of the joint distribution over a set of variables into a product of local conditional distributions. They also de\ufb01ne a set of conditional independence properties that must be satis\ufb01ed by any distribution that factorizes according to the graph", "814f0d27-375d-4309-b6ca-98fa320391ac": "Evaluating f is computationally inexpensive compared to inferring \u201d via gradient descent. Because is a differentiable parametric function, PSD models may be stacked and used to initialize a deep network to be trained with another criterion. 14.9 Applications of Autoencoders  Autoencoders have been successfully applied to dimensionality reduction and infor- mation retrieval tasks. Dimensionality reduction was one of the first applications of representation learning and deep learning. It was one of the early motivations for studying autoencoders. For example, Hinton and Salakhutdinov  trained a stack of RBMs and then used their weights to initialize a deep autoencoder with gradually smaller hidden layers, culminating in a bottleneck of 30 units. The resulting code yielded less reconstruction error than PCA into 30 dimensions, and the learned representation was qualitatively easier to interpret and relate to the underlying categories, with these categories manifesting as well-separated clusters. Lower-dimensional representations can improve performance on many tasks, such as classification. Models of smaller spaces consume less memory and runtime", "fe66f8c5-78fb-49eb-a2ad-c4705733af39": "Then the two parts are treated as the input and output for the properly designed target model p\u03b8(x, y) for supervised MLE as above, by plugging in the slightly altered experience function: A key di\ufb00erence from the above standard supervised learning setting is that now the target variable y is not costly obtained labels or annotations, but rather part of the massively available data instances. The paradigm of treating part of observed instance as the prediction target is called \u2018self-supervised\u2019 learning  and has achieved great success in language and vision modeling. For example, in language modeling , the instance t is a piece of text, and the \u2018split\u2019 function usually selects from t one or few words to be the target y and the remaining words to be x. 4.1.3. Unsupervised data instances. In the unsupervised setting, for each instance t = (x, y), such as (image, cluster index), we only observe the x part. That is, we are given a data set D = {x\u2217} without the associated y\u2217. The data set de\ufb01nes the empirical distribution \u02dcpd(x)", "8a35500d-3d51-4d4c-ae29-181398cb87da": "The eigendecomposition of a matrix tells us many useful facts about the matrix. The matrix is singular if and only if any of the eigenvalues are zero.\n\nThe eigendecomposition of a real symmetric matrix can also be used to optimize quadratic expressions of the form f (a) = \u00ab! Aw subject to |{a||2 = 1. Whenever \u00ab is equal to an eigenvector of A, f takes on the value of the corresponding eigenvalue. The maximum value of f within the constraint region is the maximum eigenvalue and its minimum value within the constraint region is the minimum eigenvalue. A matrix whose eigenvalues are all positive is called positive definite. A matrix whose eigenvalues are all positive or zero valued is called positive semidefi- nite. Likewise, if all eigenvalues are negative, the matrix is negative definite, and if all eigenvalues are negative or zero valued, it is negative semidefinite. Positive semidefinite matrices are interesting because they guarantee that Va, a! Aa > 0", "b8e27c41-91b9-42f8-be1f-0d9f56dbb8ec": "Note that because of the factor x(St) in (14.2), only the associative strengths of CS components present on a trial are adjusted as a result of that trial. You can think of the prediction error as a measure of surprise, and the aggregate associative strength as the animal\u2019s expectation that is violated when it does not match the target US magnitude. From the perspective of machine learning, the Rescorla\u2013Wagner model is an errorcorrection supervised learning rule. It is essentially the same as the Least Mean Square (LMS), or Widrow-Ho\u21b5, learning rule  that \ufb01nds the weights\u2014 here the associative strengths\u2014that make the average of the squares of all the errors as close to zero as possible.\n\nIt is a \u201ccurve-\ufb01tting,\u201d or regression, algorithm that is widely used in engineering and scienti\ufb01c applications (see Section 9.4).3 The Rescorla\u2013Wagner model was very in\ufb02uential in the history of animal learning theory because it showed that a \u201cmechanistic\u201d theory could account for the main facts about blocking without resorting to more complex cognitive theories involving, for example, an animal\u2019s explicit recognition that another stimulus component had been added and then scanning its short-term memory backward to reassess the predictive relationships involving the US", "2807fc5a-f89b-46c9-85a5-233134a1750d": "O\u2019Reilly and Frank  and O\u2019Reilly, Frank, Hazy, and Watz  argued that phasic dopamine signals are RPEs but not TD errors.\n\nIn support of their theory they cited results with variable interstimulus intervals that do not match predictions of a simple TD model, as well as the observation that higher-order conditioning beyond second-order conditioning is rarely observed, while TD learning is not so limited. Dayan and Niv  discussed \u201cthe good, the bad, and the ugly\u201d of how reinforcement learning theory and the reward prediction error hypothesis align with experimental data. Glimcher  reviewed the empirical \ufb01ndings that support the reward prediction error hypothesis and emphasized the signi\ufb01cance of the hypothesis for contemporary neuroscience. 15.4 Graybiel  is a brief primer on the basal ganglia. The experiments mentioned that involve optogenetic activation of dopamine neurons were conducted by Tsai, Zhang, Adamantidis, Stuber, Bonci, de Lecea, and Deisseroth , Steinberg, Kei\ufb02in, Boivin, Witten, Deisseroth, and Janak , and Claridge-Chang, Roorda, Vrontou, Sjulson, Li, Hirsh, and Miesenb\u00a8ock", "b68f6daa-e6a4-4463-ba19-c93eba545e4f": "In practice, we usually have only a noisy or even biased estimate of these quantities. Nearly every deep learning algorithm relies on sampling-based estimates, at least insofar as using a minibatch of training examples to compute the gradient. In other cases, the objective function we want to minimize is actually intractable. When the objective function is intractable, typically its gradient is intractable as well.\n\nIn such cases we can only approximate the gradient. These issues mostly arise with the more advanced models we cover in part III. For example, contrastive divergence gives a technique for approximating the gradient of the intractable log-likelihood of a Boltzmann machine. Various neural network optimization algorithms are designed to account for imperfections in the gradient estimate. One can also avoid the problem by choosing a surrogate loss function that is easier to approximate than the true loss", "2902b340-7d87-4626-92af-e8ba1a335eb9": "Here we make use of the analogous law for the max operator which holds if a \u2a7e 0 (as will always be the case for the factors in a graphical model). This allows us to exchange products with maximizations. Consider \ufb01rst the simple example of a chain of nodes described by (8.49).\n\nThe evaluation of the probability maximum can be written as As with the calculation of marginals, we see that exchanging the max and product operators results in a much more ef\ufb01cient computation, and one that is easily interpreted in terms of messages passed from node xN backwards along the chain to node x1. We can readily generalize this result to arbitrary tree-structured factor graphs by substituting the expression (8.59) for the factor graph expansion into (8.89) and again exchanging maximizations with products. The structure of this calculation is identical to that of the sum-product algorithm, and so we can simply translate those results into the present context. In particular, suppose that we designate a particular variable node as the \u2018root\u2019 of the graph", "cc4c9b4f-08f5-47fe-84c9-ebe9c14c9fe4": "We strongly encourage the interested reader who is not afraid of the mathematics to go through it.\n\n4By a feedforward neural network we mean a function composed by a\ufb03ne transformations and pointwise nonlinearities which are smooth Lipschitz functions (such as the sigmoid, tanh, elu, softplus, etc). Note: the statement is also true for recti\ufb01er nonlinearities but the proof is more technical (even though very similar) so we omit it. Then assumption 1 is satis\ufb01ed and therefore W(Pr, P\u03b8) is continuous everywhere and di\ufb00erentiable almost everywhere. All this shows that EM is a much more sensible cost function for our problem than at least the Jensen-Shannon divergence. The following theorem describes the relative strength of the topologies induced by these distances and divergences, with KL the strongest, followed by JS and TV, and EM the weakest. Theorem 2. Let P be a distribution on a compact space X and (Pn)n\u2208N be a sequence of distributions on X"}