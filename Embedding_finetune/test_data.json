{"10387844-ad1d-4562-86bd-d5f0494f10ff": "On each step, after selecting action At and receiving the reward Rt, the action preferences are updated by: where \u21b5 > 0 is a step-size parameter, and \u00afRt 2 R is the average of all the rewards up to but not including time t, which can be computed incrementally as described in Section 2.4 (or Section 2.5 if the problem is nonstationary).The \u00afRt term serves as a baseline with which the reward is compared. If the reward is higher than the baseline, then the probability of taking At in the future is increased, and if the reward is below baseline, then the probability is decreased. The non-selected actions move in the opposite direction. armed testbed in which the true expected rewards were selected according to a normal distribution with a mean of +4 instead of zero (and with unit variance as before).\n\nThis shifting up of all the rewards has absolutely no e\u21b5ect on the gradient bandit algorithm because of the reward baseline term, which instantaneously adapts to the new level. But if the baseline were omitted (that is, if \u00afRt was taken to be constant zero in (2.12)), then performance would be signi\ufb01cantly degraded, as shown in the \ufb01gure", "fa24bdfa-3b04-4b76-971c-da95bd6de7be": "From (9.7) the log of the likelihood function is given by Before discussing how to maximize this function, it is worth emphasizing that there is a signi\ufb01cant problem associated with the maximum likelihood framework applied to Gaussian mixture models, due to the presence of singularities. For simplicity, consider a Gaussian mixture whose components have covariance matrices given by \u03a3k = \u03c32 kI, where I is the unit matrix, although the conclusions will hold for general covariance matrices. Suppose that one of the components of the mixture model, let us say the jth component, has its mean \u00b5j exactly equal to one of the data points so that \u00b5j = xn for some value of n. This data point will then contribute a term in the likelihood function of the form If we consider the limit \u03c3j \u2192 0, then we see that this term goes to in\ufb01nity and so the log likelihood function will also go to in\ufb01nity.\n\nThus the maximization of the log likelihood function is not a well posed problem because such singularities will always be present and will occur whenever one of the Gaussian components \u2018collapses\u2019 onto a speci\ufb01c data point. Recall that this problem did not arise in the case of a single Gaussian distribution", "f089f0d3-7a9c-45d4-9573-b42f89d01510": "Because \u03a3 is a real, symmetric matrix its eigenvalues will be real, and its eigenvectors can be chosen to form an orthonormal set, so that Exercise 2.18 where Iij is the i, j element of the identity matrix and satis\ufb01es and similarly the inverse covariance matrix \u03a3\u22121 can be expressed as Substituting (2.49) into (2.44), the quadratic form becomes We can interpret {yi} as a new coordinate system de\ufb01ned by the orthonormal vectors ui that are shifted and rotated with respect to the original xi coordinates. Forming the vector y = (y1, . , yD)T, we have where U is a matrix whose rows are given by uT an orthogonal matrix, i.e., it satis\ufb01es UUT = I, and hence also UTU = I, where I Appendix C The quadratic form, and hence the Gaussian density, will be constant on surfaces for which (2.51) is constant", "56caa28b-42f9-45b1-9d74-37e0eda89a5f": "Some domains, such as natural language processing, are known to benefit tremendously from unsuper- vised learning techniques, such as learning unsupervised word embeddings. In other domains, such as computer vision, current unsupervised learning techniques do not bring a benefit, except in the semi-supervised setting, when the number of labeled examples is very small . If your application is in a context where unsupervised learning is known to be important, then include it in your first end-to-end baseline. Otherwise, only use unsupervised learning in your first attempt if the task you want to solve is unsupervised. You can always try adding unsupervised learning later if you observe that your initial baseline overfits. 11.3.\n\nDetermining Whether to Gather More Data  https://www.deeplearningbook.org/contents/guidelines.html    After the first end-to-end system is established, it is time to measure the perfor- mance of the algorithm and determine how to improve it. Many machine learning novices are tempted to make improvements by trying out many different algorithms. Yet, it is often much better to gather more data than to improve the learning algorithm. How does one decide whether to gather more data? First, determine whether the performance on the training set is acceptable", "95670e5d-a831-4e8a-98c0-40aa339bd173": "The origins of the sparsity when optimizing with respect to hyperparameters is discussed in detail in the context of the relevance vector machine. Section 7.2.2 In deriving these variational update equations for the Gaussian mixture model, we assumed a particular factorization of the variational posterior distribution given by (10.42). However, the optimal solutions for the various factors exhibit additional factorizations. In particular, the solution for q\u22c6(\u00b5, \u039b) is given by the product of an independent distribution q\u22c6(\u00b5k, \u039bk) over each of the components k of the mixture, whereas the variational posterior distribution q\u22c6(Z) over the latent variables, given by (10.48), factorizes into an independent distribution q\u22c6(zn) for each observation n (note that it does not further factorize with respect to k because, for each value of n, the znk are constrained to sum to one over k).\n\nThese additional factorizations are a consequence of the interaction between the assumed factorization and the conditional independence properties of the true distribution, as characterized by the directed graph in Figure 10.5", "7138a8eb-4948-467f-860b-8dcee5638a47": "Int J Uncertain Fuzzin Know Based Syst. 1998;6(02):107-16. Jia S, Wang P, Jia P, Hu S. Research on data augmentation for image classification based on convolutional neural networks.\n\nIn: 2017 Chinese automation congress (CAC), 2017. p. 4165-70.  llija R, Piotr D, Ross G, Georgia G, Kaiming H. Data distillation: towards omni-supervised learning. In: CVPR '18; 2018. Guotai W, Michael A, Sebastien O, Wendi L, Jan D, Tom V. Test-time augmentation with uncertainty estimation for deep learning-based medical image segmentation. OpenReview.net. 2018. Fabio P, Christina V, Sandra A, Eduardo V. Data augmentation for skin lesion analysis. In: ISIC skin image analysis workshop and challenge @ MICCAI 2018. 2018. Karzuhisa M, Akira H, Akane M, Hiroshi K", "2e1767a0-5a47-453f-8efa-dc6ed2e65253": "These three operations\u2014convolution, backprop from output to weights, and backprop from output to inputs\u2014are sufficient to compute all the gradients needed to train any depth of feedforward convolutional network, as well as to train convolutional networks with reconstruction functions based on the transpose of convolution. See Goodfellow  for a full derivation of the equations in the fully general multidimensional, multiexample case. To give a sense of how these  350  https://www.deeplearningbook.org/contents/convnets.html    CHAPTER 9. CONVOLUTIONAL NETWORKS  equations work, we present the two-dimensional, single example version here.\n\nSuppose we want to train a convolutional network that incorporates strided convolution of kernel stack K applied to multichannel image V with stride s as defined by c(K, V,s), as in equation 9.8. Suppose we want to minimize some loss function J(V,K). During forward propagation, we will need to use c itself to output Z, which is then propagated through the rest of the network and used to compute the cost function J", "fa70da63-af24-4ebc-983d-e85d5235b655": "Many reinforcement learning methods perform bootstrapping, even those that do not require, as DP requires, a complete and accurate model of the environment. In the next chapter we explore reinforcement learning methods that do not require a model and do not bootstrap. In the chapter after that we explore methods that do not require a model but do bootstrap. These key features and properties are separable, yet can be mixed in interesting combinations. The term \u201cdynamic programming\u201d is due to Bellman , who showed how these methods could be applied to a wide range of problems. Extensive treatments of DP can be found in many texts, including Bertsekas , Bertsekas and Tsitsiklis , Dreyfus and Law , Ross , White , and Whittle . Our interest in DP is restricted to its use in solving MDPs, but DP also applies to other types of problems. Kumar and Kanal  provide a more general look at DP. To the best of our knowledge, the \ufb01rst connection between DP and reinforcement learning was made by Minsky  in commenting on Samuel\u2019s checkers player", "7ff6ff12-aa2f-4cda-848d-4c69bf53ef8b": ", zN) = p(z1) Using the d-separation criterion, we see that there is always a path connecting any two observed variables xn and xm via the latent variables, and that this path is never blocked. Thus the predictive distribution p(xn+1|x1, . , xn) for observation xn+1 given all previous observations does not exhibit any conditional independence properties, and so our predictions for xn+1 depends on all previous observations. The observed variables, however, do not satisfy the Markov property at any order. We shall discuss how to evaluate the predictive distribution in later sections of this chapter. There are two important models for sequential data that are described by this graph. If the latent variables are discrete, then we obtain the hidden Markov model, or HMM . Note that the observed variables in an HMM may Section 13.2 be discrete or continuous, and a variety of different conditional distributions can be used to model them.\n\nIf both the latent and the observed variables are Gaussian (with a linear-Gaussian dependence of the conditional distributions on their parents), then we obtain the linear dynamical system", "98dd8c9e-4059-4d6d-82a9-af7bb6367c11": "Technical Report, Department of Computer Science, University of Illinois at Urbana-Champaign. Ormoneit, D., Sen, \u00b4S. Kernel-based reinforcement learning. Machine Learning, 49(2Oudeyer, P.-Y., Kaplan, F. What is intrinsic motivation? A typology of computational Oudeyer, P.-Y., Kaplan, F., Hafner, V. V. .\n\nIntrinsic motivation systems for autonomous mental development. IEEE Transactions on Evolutionary Computation, 11(2):265\u2013286. Padoa-Schioppa, C., Assad, J. A. Neurons in the orbitofrontal cortex encode economic Page, C. V. Heuristics for signature table analysis as a pattern recognition technique. IEEE Transactions on Systems, Man, and Cybernetics, 7(2):77\u201386. striatum locked to errors of reward prediction. Nature Neuroscience, 5(2):97\u201398. predicted events during classical conditioning: Evidence for eligibility traces in the rewardlearning network", "01cdb862-9e2e-43cc-90fb-8bce1f8df8dd": "Research on learning automata had a more direct in\ufb02uence on the trial-and-error thread leading to modern reinforcement learning research.\n\nThese are methods for solving a nonassociative, purely selectional learning problem known as the k-armed bandit by analogy to a slot machine, or \u201cone-armed bandit,\u201d except with k levers (see Chapter 2). Learning automata are simple, low-memory machines for improving the probability of reward in these problems. Learning automata originated with work in the 1960s of the Russian mathematician and physicist M. L. Tsetlin and colleagues  and has been extensively developed since then within engineering . These developments included the study of stochastic learning automata, which are methods for updating action probabilities on the basis of reward signals. Although not developed in the tradition of stochastic learning automata, Harth and Tzanakou\u2019s  Alopex algorithm (for Algorithm of pattern extraction) is a stochastic method for detecting correlations between actions and reinforcement that in\ufb02uenced some of our early research", "a1ae2722-9567-43fc-ad94-4c4fcc5184c7": "R., Szepesv\u00b4ari, Cs., Bhatnagar, S., Sutton, R. S. Toward o\u21b5-policy learning control with function approximation. In Proceedings of the 27th International Conference on Machine Learning , pp. 719\u2013726). Mahadevan, S. .\n\nAverage reward reinforcement learning: Foundations, algorithms, and Mahadevan, S., Liu, B., Thomas, P., Dabney, W., Giguere, S., Jacek, N., Gemp, I., Liu, J. Proximal reinforcement learning: A new theory of sequential decision making in primal-dual spaces. ArXiv:1405.6757. Mahadevan, S., Connell, J. Automatic programming of behavior-based robots using reinforcement learning. Arti\ufb01cial Intelligence, 55(2-3):311\u2013365. Mahmood, A. R. Incremental O\u21b5-Policy Reinforcement Learning Algorithms. Ph.D. Mahmood, A", "da9c9658-c2f0-4710-9b8b-bfbb6afed6ac": "Many recom- mendation problems are most accurately described theoretically as contextual bandits . The issue is that when we use the recommendation system to collect data, we get a biased and incomplete view of the preferences of users: we see the responses of users only to the items recommended to them and not to the other items. In addition, in some cases we may not get any information on users for whom no recommendation has been made (for example, with ad auctions, it may be that the price proposed for an ad was below a minimum price threshold, or does not win the auction, so the ad is not shown at all). More importantly, we get no information about what outcome would have resulted from recommending any of the other items. This would be like training a classifier by picking one class g for each training example x (typically the class with the highest probability according to the model) and then only getting as feedback whether this was the correct class or not", "8fda65c5-d5a3-4550-b2a9-6d275527a25b": "You and your roommate might infect each other with a cold, and you and your work colleague might do the same, but assuming that your roommate and your colleague do not know each other, they can only infect each other indirectly via you. file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    defined on an undirected graph 9. For each clique C in the graph,\u201d a factor \u00a2(C) (also called a clique potential) measures the affinity of the variables in that clique for being in each of their possible joint states. The factors are constrained to be nonnegative. Together they define an unnormalized probability distribution  p(x) = Ieeg o(C). (16.3)  The unnormalized probability distribution is efficient to work with so long as all the cliques are small. It encodes the idea that states with higher affinity are more likely.\n\nHowever, unlike in a Bayesian network, there is little structure to the definition of the cliques, so there is nothing to guarantee that multiplying them together will yield a valid probability distribution. See figure 16.4 for an example of reading factorization information from an undirected graph", "9b30509e-6180-4008-851e-db0f953df19a": "Positive definite matrices additionally guarantee that x' Aw =0> 2=0. 2.8 Singular Value Decomposition  In section 2.7, we saw how to decompose a matrix into eigenvectors and eigenvalues. The singular value decomposition (SVD) provides another way to factorize a matrix, into singular vectors and singular values. The SVD enables us to discover some of the same kind of information as the eigendecomposition reveals;  https://www.deeplearningbook.org/contents/linear_algebra.html    however, the SV 1s more generally applicable. Kivery real matrix has a singular value decomposition, but the same is not true of the eigenvalue decomposition. 42  CHAPTER 2.\n\nLINEAR ALGEBRA  For example, if a matrix is not square, the eigendecomposition is not defined, and we must use a singular value decomposition instead", "c6ab13e7-9aa4-4f20-b08e-c477f3c9bc73": "This follows from the fact that when we sample from p(zi|{z\\i), the marginal distribution p(z\\i) is clearly invariant because the value of z\\i is unchanged. Also, each step by de\ufb01nition samples from the correct conditional distribution p(zi|z\\i). Because these conditional and marginal distributions together specify the joint distribution, we see that the joint distribution is itself invariant. The second requirement to be satis\ufb01ed in order that the Gibbs sampling procedure samples from the correct distribution is that it be ergodic. A suf\ufb01cient condition for ergodicity is that none of the conditional distributions be anywhere zero. If this is the case, then any point in z space can be reached from any other point in a \ufb01nite number of steps involving one update of each of the component variables. If this requirement is not satis\ufb01ed, so that some of the conditional distributions have zeros, then ergodicity, if it applies, must be proven explicitly.\n\nThe distribution of initial states must also be speci\ufb01ed in order to complete the algorithm, although samples drawn after many iterations will effectively become independent of this distribution", "39eb5df9-454c-4bc0-8a25-e9b4b4d8d0c3": "What happens when we are given a choice between two estimators, one with more bias and one with more variance? How do we choose between them?\n\nFor example, imagine that we are interested in approximating the function shown in figure 5.2 and we are only offered the choice between a model with large bias and one that suffers from large variance. How do we choose between them? The most common way to negotiate this trade-off is to use cross-validation. Empirically, cross-validation is highly successful on many real-world tasks. Alter- natively, we can also compare the mean squared error (MSE) of the estimates:  MSE = E (5.53)  = Bias(n)\u00b0 + Var(6m) (5.54)  The MSE measures the overall expected deviation\u2014in a squared error sense\u2014 between the estimator and the true value of the parameter 6. As is clear from equation 5.54, evaluating the MSE incorporates both the bias and the variance. Desirable estimators are those with small MSE and these are estimators that manage to keep both their bias and variance somewhat in check. The relationship between bias and variance is tightly linked to the machine learning concepts of capacity, underfitting and overfitting. When generalization  127  CHAPTER 5", "37b1a589-468a-4ca6-96f8-c9adbefe7d35": "\u201cSentence-BERT: Sentence embeddings using Siamese BERT-networks.\" EMNLP 2019. 29] Jianlin Su et al. \u201cWhitening sentence representations for better semantics and faster retrieval.\" arXiv preprint arXiv:2103.15316 . 30] Yan Zhang et al. \u201cAn unsupervised sentence embedding method by mutual information maximization.\" EMNLP 2020. 31] Bohan Li et al. \u201cOn the sentence embeddings from pre-trained language models.\" EMNLP 2020.\n\n32] Lajanugen Logeswaran and Honglak Lee. \u201cAn efficient framework for learning sentence representations.\" ICLR 2018. 33] Joshua Robinson, et al. \u201cContrastive Learning with Hard Negative Samples.\" ICLR 2021. 34] Ching-Yao Chuang et al. \u201cDebiased Contrastive Learning.\" NeuriPS 2020", "bf47f87d-14d0-4a12-810c-e9ba4f5a5bd6": "During Q-learning updates, samples are drawn at random from the replay memory and thus one sample could be  https://lilianweng.github.io/posts/2018-02-19-rl-overview/  A (Long) Peek into Reinforcement Learning | Lil'Log   used multiple times.\n\nExperience replay improves data efficiency, removes correlations in the observation sequences, and smooths over changes in the data distribution. Periodically Updated Target: Q is optimized towards target values that are only periodically updated. The Q network is cloned and kept frozen as the optimization target every C steps (C is a hyperparameter). This modification makes the training more stable as it overcomes the short- term oscillations. The loss function looks like this:  L(0) = E (6 a,r,s\")~U(D) . (I always get mixed feeling with parameter clipping, as many studies have shown that it works empirically but it makes the math much less pretty", "10b1d7c3-72db-4b93-a2fd-d1e14be6c3a4": "This approach uses extra parallel computation to eliminate latency.\n\nThe strategy of using only a single Markov chain to generate all samples and the strategy of using one Markov chain for each desired sample are two extremes; deep learning practitioners usually use a number of chains that is similar to the number of examples in a minibatch and then draw as many samples as are needed from this fixed set of Markov chains. A commonly used number of Markov chains is 100. Another difficulty is that we do not know in advance how many steps the Markov chain must run before reaching its equilibrium distribution. This length of time is called the mixing time. Testing whether a Markov chain has reached equilibrium is also difficult. We do not have a precise enough theory for guiding us in answering this question. Theory tells us that the chain will converge, but not much more. If we analyze the Markov chain from the point of view of a matrix A acting on a vector of probabilities v, then we know that the chain mixes when A\u2018 has effectively lost all the eigenvalues from A besides the unique eigenvalue of 1. This means that the magnitude of the second-largest eigenvalue will determine the mixing time", "086f9952-00e5-4077-9ab1-ce6b87f86738": "In doing so it is useful to note that on the right-hand side we only need to retain those terms that have some functional dependence on z1 because all other terms can be absorbed into the normalization constant.\n\nThus we have Next we observe that the right-hand side of this expression is a quadratic function of z1, and so we can identify q\u22c6(z1) as a Gaussian distribution. It is worth emphasizing that we did not assume that q(zi) is Gaussian, but rather we derived this result by variational optimization of the KL divergence over all possible distributions q(zi). Note also that we do not need to consider the additive constant in (10.9) explicitly because it represents the normalization constant that can be found at the end by inspection if required. Using the technique of completing the square, we can identify Section 2.3.1 the mean and precision of this Gaussian, giving Note that these solutions are coupled, so that q\u22c6(z1) depends on expectations computed with respect to q\u22c6(z2) and vice versa. In general, we address this by treating the variational solutions as re-estimation equations and cycling through the variables in turn updating them until some convergence criterion is satis\ufb01ed", "1115d656-a165-4225-b2de-e41ed110d40f": "The natural state-value learning algorithm for using n-step returns is thus while the values of all other states remain unchanged: Vt+n(s) = Vt+n\u22121(s), for all s6=St.\n\nWe call this algorithm n-step TD. Note that no changes at all are made during the \ufb01rst n \u2212 1 steps of each episode. To make up for that, an equal number of additional updates are made at the end of the episode, after termination and before starting the next episode. Complete pseudocode is given in the box on the next page. Exercise 7.1 In Chapter 6 we noted that the Monte Carlo error can be written as the sum of TD errors (6.6) if the value estimates don\u2019t change from step to step. Show that the n-step error used in (7.2) can also be written as a sum TD errors (again if the value estimates don\u2019t change) generalizing the earlier result", "77923ffd-f385-48a9-b5b6-863c2e92d470": "Let us begin by \ufb01nding an expression for the conditional distribution p(xa|xb).\n\nFrom the product rule of probability, we see that this conditional distribution can be evaluated from the joint distribution p(x) = p(xa, xb) simply by \ufb01xing xb to the observed value and normalizing the resulting expression to obtain a valid probability distribution over xa. Instead of performing this normalization explicitly, we can obtain the solution more ef\ufb01ciently by considering the quadratic form in the exponent of the Gaussian distribution given by (2.44) and then reinstating the normalization coef\ufb01cient at the end of the calculation. If we make use of the partitioning (2.65), (2.66), and (2.69), we obtain We see that as a function of xa, this is again a quadratic form, and hence the corresponding conditional distribution p(xa|xb) will be Gaussian. Because this distribution is completely characterized by its mean and its covariance, our goal will be to identify expressions for the mean and covariance of p(xa|xb) by inspection of (2.70)", "1eabf9ea-ddd5-43a9-b921-0488efdf7d16": "We also know that neighbouring pixels xi and xj in an image are strongly correlated. This prior knowledge can be captured using the Markov random \ufb01eld model whose undirected graph is shown in Figure 8.31. This graph has two types of cliques, each of which contains two variables. The cliques of the form {xi, yi} have an associated energy function that expresses the correlation between these variables. We choose a very simple energy function for these cliques of the form \u2212\u03b7xiyi where \u03b7 is a positive constant. This has the desired effect of giving a lower energy (thus encouraging a higher probability) when xi and yi have the same sign and a higher energy when they have the opposite sign. The remaining cliques comprise pairs of variables {xi, xj} where i and j are indices of neighbouring pixels. Again, we want the energy to be lower when the pixels have the same sign than when they have the opposite sign, and so we choose an energy given by \u2212\u03b2xixj where \u03b2 is a positive constant", "1f7169a0-d314-4590-a8cd-b6fbe1730150": "Thus, the data alone; knowledge of the MDP beyond what is revealed i Moreover, the two MDPs have di\u21b5erent minimal-BE value f the minimal-BE value function is the exact solution v\u03b8 = \u20d70 for 2. This is a critical observation, as it is possible for an error function t perfectly satisfactory for use in learning settings because the value th from data. For example, this is what happens with the VE. The VE policy together completely determine the probabilit Assume for the moment that the state, action, an for any \ufb01nite sequence \u03be = \u03c60, a0, r1, . , rk, \u03c6k, the sibly zero) of it occuring as the initial portion of P(\u03be) = Pr{\u03c6(S0) = \u03c60, A0 = a0, R1 = r1, . .\n\n, Rk = then is a complete characterization of a source of da everything about the statistics of the data, but it is particular, the VE and BE objectives are readily com Section 3, but these cannot be determined from P al the minimal-BE value function is the exact solution v 2", "0d5076d9-ec8b-406e-acfa-893a1633b706": "Sampling is also accelerated in the case of directed models, while the situation can be complicated with undirected models. The primary mechanism that allows all these operations to use less runtime and memory is choosing to not model certain interactions. Graphical models convey information by leaving edges out. Anywhere there is not an edge, the model specifies the assumption that we do not need to model a direct interaction. A less quantifiable benefit of using structured probabilistic models is that they allow us to explicitly separate representation of knowledge from learning of knowledge or inference given existing knowledge. This makes our models easier to develop and debug. We can design, analyze, and evaluate learning algorithms and inference algorithms that are applicable to broad classes of graphs.\n\nIndependently, we can design models that capture the relationships we believe are important in our data. We can then combine these different algorithms and structures and obtain a Cartesian product of different possibilities. It would be much more difficult to design end-to-end algorithms for every possible situation. 16.5 Learning about Dependencies  A good generative model needs to accurately capture the distribution over the observed, or \u201cvisible,\u201d variables v. Often the different elements of v are highly dependent on each other", "824fa87f-8009-4739-bbfa-6151ee6d7f2c": "Specifically, we use a minibatch of examples from the training set formatted as a design matrix X and a vector of associated class labels y. The network computes a layer of hidden features H = max{0,X WH). To simplify the presentation we do not use biases in this model. We assume that our graph language includes a relu operation that can compute max{0, Z} element- wise.\n\nThe predictions of the unnormalized log probabilities over classes are then given by HW), We assume that our graph language includes a cross_entropy operation that computes the cross-entropy between the targets y and the probability distribution defined by these unnormalized log probabilities. The resulting cross- entropy defines the cost J)yup. Minimizing this cross-entropy performs maximum likelihood estimation of the classifier. However, to make this example more realistic, we also include a regularization term. The total cost  1)\\? 2)\\? J=Iunt+0( 0 (we) +0 (we) (6.56) ij ij  consists of the cross-entropy and a weight decay term with coefficient \u2019", "a5171d5a-d29c-43aa-a3aa-8a2cb714fcf9": "What does this tell you about what happened on the \ufb01rst episode? Why was only the estimate for this one state changed? By exactly how much was it changed? \u21e4 Exercise 6.4 The speci\ufb01c results shown in the right graph of the random walk example are dependent on the value of the step-size parameter, \u21b5. Do you think the conclusions about which algorithm is better would be a\u21b5ected if a wider range of \u21b5 values were used? Is there a di\u21b5erent, \ufb01xed value of \u21b5 at which either algorithm would have performed signi\ufb01cantly better than shown? Why or why not?\n\n\u21e4 \u21e4Exercise 6.5 In the right graph of the random walk example, the RMS error of the TD method seems to go down and then up again, particularly at high \u21b5\u2019s. What could have caused this? Do you think this always occurs, or might it be a function of how the approximate value function was initialized? \u21e4 6, for states A through E. Describe at least two di\u21b5erent ways that Suppose there is available only a \ufb01nite amount of experience, say 10 episodes or 100 time steps. In this case, a common approach with incremental learning methods is to present the experience repeatedly until the method converges upon an answer", "23e88608-f132-48b3-b139-542f35e03424": "Also, while the (nonlinear) conjugate gradients algorithm has traditionally been cast as a batch method, minibatch versions have been used successfully for training neural networks . Adaptations of conjugate gradients specifically for neural networks have been proposed earlier, such as the scaled conjugate gradients algorithm . 8.6.3 BFGS  The Broyden\u2014Fletcher\u2014Goldfarb\u2014Shanno (BFGS) algorithm attempts to bring some of the advantages of Newton\u2019s method without the computational burden. In that respect, BFGS is similar to the conjugate gradient method. However, BFGS takes a more direct approach to the approximation of Newton\u2019s update.\n\nRecall that Newton\u2019s update is given by  0\u00b0 = 0) \u2014H 'VoJ(60), (8.32)  where H is the Hessian of J with respect to @ evaluated at 09", "85655d39-806a-4f48-834d-2ab47b9de2ad": "The right-hand plot shows the true conditional distribution p(t|x) from which the labels are generated, in which the green curve denotes the mean, and the shaded region spans one standard deviation on each side of the mean. red and blue. On the right is a plot of the true posterior probabilities, shown on a colour scale going from pure red denoting probability of the red class is 1 to pure blue denoting probability of the red class is 0. Because these probabilities are known, the optimal decision boundary for minimizing the misclassi\ufb01cation rate (which corresponds to the contour along which the posterior probabilities for each class equal 0.5) can be evaluated and is shown by the green curve. This decision boundary is also plotted on the left-hand \ufb01gure. In this appendix, we summarize the main properties of some of the most widely used probability distributions, and for each distribution we list some key statistics such as the expectation E, the variance (or covariance), the mode, and the entropy H", "02484d1b-ce1f-4c4a-ba0f-24e8b045cd28": "Olshausen and Field  showed that a simple unsupervised learning algorithm, sparse coding, learns features with receptive fields similar to those of simple cells. Since then, we have found that an extremely wide variety of statistical learning algorithms learn features with Gabor-like functions when applied to natural images. This includes most deep learning algorithms, which learn these features in their first layer. Figure 9.19 shows some examples. Because so many different learning algorithms learn edge detectors, it is difficult to conclude that any specific learning algorithm is the \u201cright\u201d model of the brain just based on the features it learns (though it can certainly be a bad sign if an algorithm does not learn some sort of edge detector when applied to natural images). These features are an important part of the statistical structure of natural images and can be recovered by many different approaches to statistical modeling. See Hyvarinen et al. for a review of the field of natural image statistics. https://www.deeplearningbook.org/contents/convnets.html    364  CHAPTER 9", "9e36e58b-c4e5-46cf-b7b8-e819037de2cb": "Together, these will motivate and illustrate the key concepts of d-separation.\n\nThe \ufb01rst of the three examples is shown in Figure 8.15, and the joint distribution corresponding to this graph is easily written down using the general result (8.5) to give If none of the variables are observed, then we can investigate whether a and b are independent by marginalizing both sides of (8.23) with respect to c to give In general, this does not factorize into the product p(a)p(b), and so where \u2205 denotes the empty set, and the symbol \u0338\u22a5\u22a5 means that the conditional independence property does not hold in general. Of course, it may hold for a particular distribution by virtue of the speci\ufb01c numerical values associated with the various conditional probabilities, but it does not follow in general from the structure of the graph. Now suppose we condition on the variable c, as represented by the graph of and so we obtain the conditional independence property We can provide a simple graphical interpretation of this result by considering the path from node a to node b via c", "3db9dbb7-8e94-40aa-bef6-f1d8630b1045": "This means that the direction of most curvature has five times more curvature than the direction of least curvature. In this case, the most curvature is in the direction ', and the least curvature is in the direction \". The red lines indicate the path followed by gradient descent. This very elongated quadratic \u2018unction resembles a long canyon. Gradient descent wastes time repeatedly descending canyon walls because they are the steepest feature. Since the step size is somewhat too arge, it has a tendency to overshoot the bottom of the function and thus needs to descend the opposite canyon wall on the next iteration.\n\nThe large positive eigenvalue of the Hessian corresponding to the eigenvector pointed in this direction indicates that this directional derivative is rapidly increasing, so an optimization algorithm based on the Hessian could predict that the steepest direction is not actually a promising search direction in this context. 89  https://www.deeplearningbook.org/contents/numerical.html    CHAPTER 4, NUMERICAL COMPUTATION  This issue can be resolved by using information from the Hessian matrix to guide the search. The simplest method for doing so is known as Newton\u2019s method", "75c5ba77-8121-4ca1-a419-62422e8d3f2a": "Because an inactive h\u00ae has negative value, then the solution to ming max, Maxq,o>0 L(#, A, a) will have a; = 0. We can thus observe that at the solution, a\u00a9 h(a) =0. In other words, for all 7, we know that at least one of the constraints a; > 0 or A\u00ae (x) < 0 must be active at the solution. To gain some intuition for this idea, we can say that either the solution is on the boundary imposed by the inequality and we must use its KKT multiplier to influence the solution to x, or the inequality has no influence on the solution and we represent this by zeroing out its KKT multiplier. A simple set of properties describe the optimal points of constrained opti-  https://www.deeplearningbook.org/contents/numerical.html    mization problems. These properties are called the Karush-Kuhn-Tucker (KKT) conditions . They are necessary conditions, but not always sufficient conditions, for a point to be optimal. The conditions are:  e The gradient of the generalized Lagrangian is zero", "91998ecf-f83a-4860-84e9-f2092086e460": "Wilson, R. C., Takahashi, Y. K., Schoenbaum, G., Niv, Y. Orbitofrontal cortex as a Witten, I. H. Learning to Control. University of Essex PhD thesis. Witten, I. H. .\n\nThe apparent con\ufb02ict between estimation and control\u2014A survey of the two-armed problem. Journal of the Franklin Institute, 301(1-2):161\u2013189. Witten, I. H. An adaptive optimal controller for discrete-time Markov environments. Witten, I. H., Corbin, M. J. Human operators and automatic adaptive controllers: A learning for trajectory generation. In 52nd Aerospace Sciences Meeting, p. 0990. Woodworth, R. S. Experimental Psychology. New York: Henry Holt and Company. Xie, X., Seung, H. S. Learning in neural networks by reinforcement of irregular spiking", "f9865213-2f72-4fea-a410-0ed1e489d426": "Formally, let Dv be the validation set of data examples. The update is then: where, since \u03b8\u2032 is a function of \u03c6, the gradient is backpropagated to \u03c6 through \u03b8\u2032(\u03c6).\n\nTaking data weighting for example where \u03c6 is the training sample weights (more details in section 4.2), the update is to optimize the weights of training samples so that the model performs best on the validation set. The resulting algorithm is summarized in Algorithm 1. Figure 1 illustrates the computation \ufb02ow. Learning the manipulation parameters effectively uses a held-out validation set. We show in our experiments that a very small set of validation examples (e.g., 2 labels per class) is enough to signi\ufb01cantly improve the model performance in low data regime. It is worth noting that some previous work has also leveraged validation examples, such as learning data augmentation with policy gradient  or inducing data weights with meta-learning . Our approach is inspired from a distinct paradigm of (intrinsic) reward learning. In contrast to  that treats data augmentation as a policy, we instead formulate manipulation as a reward function and enable ef\ufb01cient stochastic gradient updates. Our approach is also more broadly applicable to diverse data manipulation types than", "6eb5107b-1275-4a02-8cf2-ad90bcd7657b": "In this example, we evaluated the state in which the dealer is showing a deuce, the sum of the player\u2019s cards is 13, and the player has a usable ace (that is, the player holds an ace and a deuce, or equivalently three aces). The data was generated by starting in this state then choosing to hit or stick at random with equal probability (the behavior policy). The target policy was to stick only on a sum of 20 or 21, as in Example 5.1. The value of this state under the target policy is approximately \u22120.27726 (this was determined by separately generating one-hundred million episodes using the target policy and averaging their returns). Both o\u21b5-policy methods closely approximated this value after 1000 o\u21b5-policy episodes using the random policy. To make sure they did this reliably, we performed 100 independent runs, each starting from estimates of zero and learning for 10,000 episodes. Figure 5.3 shows the resultant learning curves\u2014the squared error of the estimates of each method as a function of number of episodes, averaged over the 100 runs.\n\nThe error approaches zero for both algorithms, but the weighted importance-sampling method has much lower error at the beginning, as is typical in practice", "c3957a94-4927-4fe6-b2c0-427de66b3d88": "G. A primer on reinforcement learning in the brain: Psychological, computational, and neural perspectives. In E. Alonso and E. Mondrag\u00b4on (Eds. ), Computational Neuroscience for Advancing Arti\ufb01cial Intelligence: Models, Methods and Applications, pp. 111\u201344.\n\nMedical Information Science Reference, Hershey PA. Ludvig, E. A., Sutton, R. S., Kehoe, E. J. Stimulus representation and the timing of Mackintosh, N. J. A theory of attention: Variations in the associability of stimuli with Mackintosh, N. J. Conditioning and Associative Learning. Clarendon Press, Oxford. Maclin, R., Shavlik, J. W. Incorporating advice into agents that learn from reinforcements. In Proceedings of the Twelfth National Conference on Arti\ufb01cial Intelligence (AAAI-94), pp. 694\u2013699", "b40088ed-95a0-4f4f-9223-88f8a9fae7cd": "This should be done subject to the constraints that UM+l be orthogonal to the existing vectors U1,\"\" UM, and also that it be normalized to unit length. Use Lagrange multipliAppendix E ers to enforce these constraints. Then make use of the orthonormality properties of the vectors U1,\"\" UM to show that the new vector UM+1 is an eigenvector of S. Finally, show that the variance is maximized if the eigenvector is chosen to be the one corresponding to eigenvector AM+1 where the eigenvalues have been ordered in decreasing value.\n\n12.2 (**) Show that the minimum value of the PCA distortion measure J given by (12.15) with respect to the Ui, subject to the orthonormality constraints (12.7), is obtained when the Ui are eigenvectors of the data covariance matrix S. To do this, introduce a matrix H of Lagrange multipliers, one for each constraint, so that the modified distortion measure, in matrix notation reads where Uis a m~trix of dimensio~D x (D - M) whose columns are gi:::..en b~ Ui", "b626c059-1cee-44c7-812a-9604b942e9d5": "For a D-dimensional vector x, the multivariate Gaussian distribution takes the form where \u00b5 is a D-dimensional mean vector, \u03a3 is a D \u00d7 D covariance matrix, and |\u03a3| denotes the determinant of \u03a3. The Gaussian distribution arises in many different contexts and can be motivated from a variety of different perspectives. For example, we have already seen that for Section 1.6 a single real variable, the distribution that maximizes the entropy is the Gaussian. This property applies also to the multivariate Gaussian. Exercise 2.14 Another situation in which the Gaussian distribution arises is when we consider the sum of multiple random variables. The central limit theorem (due to Laplace) tells us that, subject to certain mild conditions, the sum of a set of random variables, which is of course itself a random variable, has a distribution that becomes increasingly Gaussian as the number of terms in the sum increases . We can observe that as N increases, the distribution tends towards a Gaussian.\n\nillustrate this by considering N variables x1,", "7124ca67-9e67-422d-bc48-9d6378933dd1": "APPLICATIONS  Training n-gram models is straightforward because the maximum likelihood estimate can be computed simply by counting how many times each possible n-gram occurs in the training set. Models based on n-grams have been the core building block of statistical language modeling for many decades . For small values of n, models have particular names: unigram for n = 1, bigram for n = 2, and trigram for n = 3. These names derive from the Latin prefixes for the corresponding numbers and the Greek suffix \u201c-gram,\u201d denoting something that is written. Usually we train both an n-gram model and an n\u20141 gram model simultaneously.\n\nThis makes it easy to compute Pn(tt-n41, ne) %) Pr-1 (tt\u2014n41; re) %-1)  P(x | Tt-n4+1y+++ \u00bbt-1) (12.6)  simply by looking up two stored probabilities", "19a313ae-cacf-4290-8f66-e426fa862635": "We see that the mixture model gives a much better representation of the data distribution, and this is re\ufb02ected in the higher likelihood value. However, the mixture model also assigns signi\ufb01cant probability mass to regions where there is no data because its predictive distribution is bimodal for all values of x. This problem can be resolved by extending the model to allow the mixture coef\ufb01cients themselves to be functions of x, leading to models such as the mixture density networks discussed in Section 5.6, and hierarchical mixture of experts discussed in Section 14.5.3. target variable t, together with a mixture of two linear regression models whose mean functions y(x, wk), where k \u2208 {1, 2}, are shown by the blue and red lines.\n\nThe upper three plots show the initial con\ufb01guration (left), the result of running 30 iterations of EM (centre), and the result after 50 iterations of EM (right). Here \u03b2 was initialized to the reciprocal of the true variance of the set of target values. The lower three plots show the corresponding responsibilities plotted as a vertical line for each data point in which the length of the blue segment gives the posterior probability of the blue line for that data point (and similarly for the red segment)", "46afbbbc-8a5d-40aa-b623-e7386b1a65f2": "In many cases, x\u2019 can be so similar to z that a  + .007  https://www.deeplearningbook.org/contents/regularization.html    x sign(VaJ (0, x,y)) csign(V2J(0,2,y)) y =\u201cpanda\u201d \u201cnematode\u201d \u201csibbon\u201d w/ 57.7% w/ 8.2% w/ 99.3% confidence confidence confidence  Figure 7.8: A demonstration of adversarial example generation applied to GoogLeNet  on ImageNet.\n\nBy adding an imperceptibly small vector whose elements are equal to the sign of the elements of the gradient of the cost function with respect to the input, we can change GoogLeNet\u2019s classification of the image. Reproduced with permission from Goodfellow e\u00e9 al. 265  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  human observer cannot tell the difference between the original example and the adversarial example, but the network can make highly different predictions. See figure 7.8 for an example. Adversarial examples have many implications, for example, in computer security, hat are beyond the scope of this chapter", "edee76b5-8223-47d4-a0ab-2eeafba22254": "The agent\u2013environment boundary can be located at di\u21b5erent places for di\u21b5erent purposes.\n\nIn a complicated robot, many di\u21b5erent agents may be operating at once, each with its own boundary. For example, one agent may make high-level decisions which form part of the states faced by a lower-level agent that implements the high-level decisions. In practice, the agent\u2013environment boundary is determined once one has selected particular states, actions, and rewards, and thus has identi\ufb01ed a speci\ufb01c decision making task of interest. The MDP framework is a considerable abstraction of the problem of goal-directed learning from interaction. It proposes that whatever the details of the sensory, memory, and control apparatus, and whatever objective one is trying to achieve, any problem of learning goal-directed behavior can be reduced to three signals passing back and forth between an agent and its environment: one signal to represent the choices made by the agent (the actions), one signal to represent the basis on which the choices are made (the states), and one signal to de\ufb01ne the agent\u2019s goal (the rewards). This framework may not be su\ufb03cient to represent all decision-learning problems usefully, but it has proved to be widely useful and applicable", "4d598542-639f-4952-9ddc-ea862988ce41": "Modern reinforcement learning spans the spectrum from low-level, trial-and-error learning to high-level, deliberative planning. Reinforcement learning relies heavily on the concept of state\u2014as input to the policy and value function, and as both input to and output from the model. Informally, we can think of the state as a signal conveying to the agent some sense of \u201chow the environment is\u201d at a particular time. The formal de\ufb01nition of state as we use it here is given by the framework of Markov decision processes presented in Chapter 3.\n\nMore generally, however, we encourage the reader to follow the informal meaning and think of the state as whatever information is available to the agent about its environment. In e\u21b5ect, we assume that the state signal is produced by some preprocessing system that is nominally part of the agent\u2019s environment. We do not address the issues of constructing, changing, or learning the state signal in this book (other than brie\ufb02y in Section 17.3). We take this approach not because we consider state representation to be unimportant, but in order to focus fully on the decision-making issues. In other words, our concern in this book is not with designing the state signal, but with deciding what action to take as a function of whatever state signal is available", "155dd1ad-a67f-4fc3-9b5a-5c48e59ce63a": "Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run. For example, suppose a greedy action\u2019s value is known with certainty, while several other actions are estimated to be nearly as good but with substantial uncertainty. The uncertainty is such that at least one of these other actions probably is actually better than the greedy action, but you don\u2019t know which one. If you have many time steps ahead on which to make action selections, then it may be better to explore the nongreedy actions and discover which of them are better than the greedy action. Reward is lower in the short run, during exploration, but higher in the long run because after you have discovered the better actions, you can exploit them many times. Because it is not possible both to explore and to exploit with any single action selection, one often refers to the \u201ccon\ufb02ict\u201d between exploration and exploitation.\n\nIn any speci\ufb01c case, whether it is better to explore or exploit depends in a complex way on the precise values of the estimates, uncertainties, and the number of remaining steps. There are many sophisticated methods for balancing exploration and exploitation for particular mathematical formulations of the k-armed bandit and related problems", "ce50b152-6001-4782-8258-c89e51065a3c": "This network outputs two values:  (p, v) = fo(s)  gs: the game board configuration, 19 x 19 x 17 stacked feature planes; 17 features for each position, 8 past configurations (including current) for the current player + 8 past configurations for the opponent + 1 feature indicating the color (1=black, O=white). We need to code the color specifically because the network is playing with itself and the colors of current player and opponents are switching between steps. pp: the probability of selecting a move over 19%2 + 1 candidates (19%2 positions on the board, in addition to passing). v: the winning probability given the current setting. During self-play, MCTS further improves the action probability distribution 7 ~ p(.) and then the action a; is sampled from this improved policy.\n\nThe reward z; is a binary value indicating whether the current player eventually wins the game. Each move generates an episode tuple (s;, 7, 2,) and it is saved into the replay memory. The details on MCTS are skipped for the sake of space in this post; please read the original paper if you are interested", "c8d679f7-ba68-4e5e-914d-cdecc94b11ff": "Under modest assumptions\u2014 namely, that the average labeling function accuracy \u03b1\u2217 is greater than 50%\u2014it is known that the majority vote converges exponentially to an optimal solution as the average label density \u00afd increases, which serves as an upper bound for the expected optimal advantage as well: Proposition 2 (High-Density Upper Bound) Assume that P(\ufffdi, j \u0338= 0) = pl \u2200i, j, and that \u03b1\u2217 > 1 Medium Label Density In this middle regime, we expect that modeling the accuracies of the labeling functions will deliver the greatest gains in predictive performance because we will have many data points with a small number of disagreeing labeling functions. For such points, the estimated labeling function accuracies can heavily affect the predicted labels. We indeed see gains in the empirical results using an independent generative model that only includes accuracy factors \u03c6Acc i, j (Table 1).\n\nFurthermore, the guarantees in  establish that we can learn the optimal weights, and thus approach the optimal advantage. 3.1.2 Automatically choosing a modeling strategy The bounds in the previous subsection imply that there are settings in which we should be able to safely skip modeling the labeling function accuracies, simply taking the unweighted majority vote instead", "d3d745b1-a726-40b5-88fd-ac99c00cb651": "In fact, \u2212 ln x is a strictly convex function, so the equality will hold if, and only if, q(x) = p(x) for all x. Thus we can interpret the Kullback-Leibler divergence as a measure of the dissimilarity of the two distributions p(x) and q(x). We see that there is an intimate relationship between data compression and density estimation (i.e., the problem of modelling an unknown probability distribution) because the most ef\ufb01cient compression is achieved when we know the true distribution. If we use a distribution that is different from the true one, then we must necessarily have a less ef\ufb01cient coding, and on average the additional information that must be transmitted is (at least) equal to the Kullback-Leibler divergence between the two distributions. Suppose that data is being generated from an unknown distribution p(x) that we wish to model", "7fd2d2ff-bd85-44b1-ac67-a0d84a0a4793": "State B, on the other hand, is valued more than 5, its immediate reward, because from B the agent is taken to B\u2032, which has a positive value. From B\u2032 the expected penalty (negative reward) for possibly running into an edge is more Suppose the agent selects all four actions with equal probability in all states. Figure 3.2 (right) shows the value function, v\u21e1, for this policy, for the discounted reward case with \u03b3 = 0.9. This value function was computed by solving the system of linear equations (3.14). Notice the negative values near the lower edge; these are the result of the high probability of hitting the edge of the grid there under the random policy. State A is the best state to be in under this policy, but its expected return is less than 10, its immediate reward, because from A the agent is taken to A0, from which it is likely to run into the edge of the grid. State B, on the other hand, is valued more than 5, its immediate reward, because from B the agent is taken to B0, which has a positive value", "ba2cb73b-d9c5-430f-b3b7-716b723bc020": "in such a way that if such a signal occurs before the end of the decay time of the change the synaptic connexions between the cells are made more e\u21b5ective. Crow argued against previous proposals that reverberating neural circuits play this role by pointing out that the e\u21b5ect of a reward signal on such a circuit would \u201c...establish the synaptic connexions leading to the reverberation (that is to say, those involved in activity at the time of the reward signal) and not those on the path which led to the adaptive motor output.\u201d Crow further postulated that reward signals are delivered via a \u201cdistinct neural \ufb01ber system,\u201d presumably the one into which Olds and Milner  tapped, that would transform synaptic connections \u201cfrom a short into a long-term form.\u201d In another farsighted hypothesis, Miller proposed a Law-of-E\u21b5ect-like learning rule that includes synaptically-local contingent eligibility traces: ..", "d2ea6ee2-60ad-48da-bce3-b5cbb806e95b": "Each term in the sum in (5.48) requires one multiplication and one addition, leading to an overall computational cost that is O(W). An alternative approach to backpropagation for computing the derivatives of the error function is to use \ufb01nite differences. This can be done by perturbing each weight in turn, and approximating the derivatives by the expression where \u03f5 \u226a 1.\n\nIn a software simulation, the accuracy of the approximation to the derivatives can be improved by making \u03f5 smaller, until numerical roundoff problems arise. The accuracy of the \ufb01nite differences method can be improved signi\ufb01cantly by using symmetrical central differences of the form In this case, the O(\u03f5) corrections cancel, as can be veri\ufb01ed by Taylor expansion on Exercise 5.14 the right-hand side of (5.69), and so the residual corrections are O(\u03f52). The number of computational steps is, however, roughly doubled compared with (5.68). The main problem with numerical differentiation is that the highly desirable O(W) scaling has been lost. Each forward propagation requires O(W) steps, and there are W weights in the network each of which must be perturbed individually, so that the overall scaling is O(W 2)", "125acc8f-a14f-48e0-980e-c4af04249a40": "The log likelihood function for a Gaussian process regression model is easily evaluated using the standard form for a multivariate Gaussian distribution, giving For nonlinear optimization, we also need the gradient of the log likelihood function with respect to the parameter vector \u03b8. We shall assume that evaluation of the derivatives of CN is straightforward, as would be the case for the covariance functions considered in this chapter.\n\nMaking use of the result (C.21) for the derivative of C\u22121 N , together with the result (C.22) for the derivative of ln |CN|, we obtain Because ln p(t|\u03b8) will in general be a nonconvex function, it can have multiple maxima. It is straightforward to introduce a prior over \u03b8 and to maximize the log posterior using gradient-based methods. In a fully Bayesian treatment, we need to evaluate marginals over \u03b8 weighted by the product of the prior p(\u03b8) and the likelihood function p(t|\u03b8). In general, however, exact marginalization will be intractable, and we must resort to approximations. The Gaussian process regression model gives a predictive distribution whose mean and variance are functions of the input vector x", "5c556d31-8890-4f01-9af5-46e37bc22bbe": "If, as is often the case, p(z)f(z) is strongly varying and has a signi\ufb01cant proportion of its mass concentrated over relatively small regions of z space, then the set of importance weights {rl} may be dominated by a few weights having large values, with the remaining weights being relatively insigni\ufb01cant. Thus the effective sample size can be much smaller than the apparent sample size L. The problem is even more severe if none of the samples falls in the regions where p(z)f(z) is large. In that case, the apparent variances of rl and rlf(z(l)) may be small even though the estimate of the expectation may be severely wrong. Hence a major drawback of the importance sampling method is the potential to produce results that are arbitrarily in error and with no diagnostic indication. This also highlights a key requirement for the sampling distribution q(z), namely that it should not be small or zero in regions where p(z) may be signi\ufb01cant. For distributions de\ufb01ned in terms of a graphical model, we can apply the importance sampling technique in various ways.\n\nFor discrete variables, a simple approach is called uniform sampling", "360266b3-72ea-4745-b8fc-27a935cacfca": "We found that, although stronger data augmentation and longer training time do not bene\ufb01t accuracy on ImageNet, these models performed signi\ufb01cantly better than a supervised baseline trained for 90 epochs and ordinary data augmentation for linear evaluation on a subset of transfer datasets.\n\nThe supervised ResNet-50 baseline achieves 76.3% top-1 accuracy on ImageNet, vs. 69.3% for the self-supervised counterpart, while the ResNet-50 (4\u00d7) baseline achieves 78.3%, vs. 76.5% for the self-supervised model. Statistical Signi\ufb01cance Testing We test for the signi\ufb01cance of differences between model with a permutation test. Given predictions of two models, we generate 100,000 samples from the null distribution by randomly exchanging predictions for each example and computing the difference in accuracy after performing this randomization. We then compute the percentage of samples from the null distribution that are more extreme than the observed difference in predictions. For top-1 accuracy, this procedure yields the same result as the exact McNemar test. The assumption of exchangeability under the null hypothesis is also valid for mean per-class accuracy, but not when computing average precision curves", "9482d83e-2602-4c25-aec9-704b1997214b": "This form of synaptic plasticity, called reward-modulated STDP, is much like the actor learning rule discussed here. Synaptic changes that would be produced by regular STDP only occur if there is neuromodulatory input within a time window after a presynaptic spike is closely followed by a postsynaptic spike. Evidence is accumulating that reward-modulated STDP occurs at the spines of medium spiny neurons of the dorsal striatum, with dopamine providing the neuromodulatory factor\u2014 the sites where actor learning takes place in the hypothetical neural implementation of an actor\u2013critic algorithm illustrated in Figure 15.5b. Experiments have demonstrated reward-modulated STDP in which lasting changes in the e\ufb03cacies of corticostriatal synapses occur only if a neuromodulatory pulse arrives within a time window that can last up to 10 seconds after a presynaptic spike is closely followed by a postsynaptic spike . Although the evidence is indirect, these experiments point to the existence of contingent eligibility traces having prolonged time courses", "6a71596a-2572-49a3-ba28-2994ea9d8124": "Specifically, dropout trains the ensemble consisting of all subnetworks that can be formed by removing nonoutput units from an underlying base network, as illustrated in figure 7.6. In most modern neural networks, based on a series of affine transformations and nonlinearities, we can effectively remove a unit from a network by multiplying its output value by zero. This procedure requires some slight modification for models such as radial basis function networks, which take the difference between the unit\u2019s state and some reference value. Here, we present the dropout algorithm in terms of multiplication by zero for simplicity, but it can be trivially modified to work with other operations that remove a unit from the network. Recall that to learn with bagging, we define & different models, construct k  255  CHAPTER 7.\n\nREGULARIZATION FOR DEEP LEARNING  https://www.deeplearningbook.org/contents/regularization.html    Base network  Ensemble of subnetworks  Figure 7.6: Dropout trains an ensemble consisting of all subnetworks that can be con- structed by removing nonoutput units from an underlying base network. Here, we begin with a base network with two visible units and two hidden units", "7d7b6398-2773-4448-94b4-652522217859": "The result is like one step of the policy-iteration algorithm of dynamic programming discussed in Section 4.3 (though it is more like one step of asynchronous value iteration described in Section 4.5 because it changes the action for just the current state).\n\nIn other words, the aim of a rollout algorithm is to improve upon the rollout policy; not to \ufb01nd an optimal policy. Experience has shown that rollout algorithms can be surprisingly e\u21b5ective. For example, Tesauro and Galperin  were surprised by the dramatic improvements in backgammon playing ability produced by the rollout method. In some applications, a rollout algorithm can produce good performance even if the rollout policy is completely random. But the performance of the improved policy depends on properties of the rollout policy and the ranking of actions produced by the Monte Carlo value estimates. Intuition suggests that the better the rollout policy and the more accurate the value estimates, the better the policy produced by a rollout algorithm is likely be . This involves important tradeo\u21b5s because better rollout policies typically mean that more time is needed to simulate enough trajectories to obtain good value estimates. As decision-time planning methods, rollout algorithms usually have to meet strict time constraints", "c6386797-3ccc-4b6f-bee2-732906ab3d26": "Thus if (z, r) is the initial state and (z\u22c6, r\u22c6) is the state after the leapfrog integration, then this candidate state is accepted with probability min (1, exp{H(z, r) \u2212 H(z\u22c6, r\u22c6)}) . (11.67) If the leapfrog integration were to simulate the Hamiltonian dynamics perfectly, then every such candidate step would automatically be accepted because the value of H would be unchanged. Due to numerical errors, the value of H may sometimes decrease, and we would like the Metropolis criterion to remove any bias due to this effect and ensure that the resulting samples are indeed drawn from the required distribution. In order for this to be the case, we need to ensure that the update equations corresponding to the leapfrog integration satisfy detailed balance (11.40). This is easily achieved by modifying the leapfrog scheme as follows. Before the start of each leapfrog integration sequence, we choose at random, with equal probability, whether to integrate forwards in time (using step size \u03f5) or backwards in time (using step size \u2212\u03f5)", "5e0428f1-34f7-4946-9eab-6534e05837c8": "One way to prevent this is to explicitly model the noise on the labels. For example, we can assume that for some small constant \u00a2, the training set label y is correct with probability 1\u2014\u00ab, and otherwise any of the other possible labels might be correct. This assumption is easy to incorporate into the cost function analytically, rather than by explicitly drawing noise samples. For example, label smoothing regularizes a model based on a  https://www.deeplearningbook.org/contents/regularization.html    softmax with k ontput values by replacing the hard 0 and 1 classification targets with targets of k-1 and 1\u2014 \u20ac, respectively. The standard cross-entropy loss may then be used with these soft targets. Maximum likelihood learning with a softmax classifier and hard targets may actually never converge\u2014the softmax can never predict a probability of exactly 0 or exactly 1, so it will continue to learn larger and larger weights, making more extreme predictions forever. It is possible to prevent this scenario using other regularization strategies like weight decay.\n\nLabel smoothing has the advantage of preventing the pursuit of hard probabilities without discouraging correct classification. This strategy has been used since the 1980s  239  CHAPTER 7", "281a5e41-75f4-4d92-8452-bbf91ef75989": "This discussion is concerned with the status of faith for individuals or groups which may be members and members science: the chemical microscope is This essay discusses the mechanisms of reaction with a focus on the molecular structure of nucleite and of enzymes within the cytoskeleton, thus making it easier to understand the process of metabolism and other elements of cellular life", "d99aaaee-791c-4e7d-ac74-e12a8cb19e52": "Another simple way in which the learning of auxiliary tasks can improve performance is best explained by analogy to the psychological phenomena of classical conditioning (Section 14.2). One way of understanding classical conditioning is that evolution has built in a re\ufb02exive (non-learned) association to a particular action from the prediction of a particular signal. For example, humans and many other animals appear to have a built-in re\ufb02ex to blink whenever their prediction of being poked in the eye exceeds some threshold. The prediction is learned, but the association from prediction to eye closure is built in, and thus the animal is saved many unprotected pokes in its eye. Similarly, the association from fear to increased heart rate, or to freezing, can be built in.\n\nAgent designers can do something similar, connecting by design (without learning) predictions of speci\ufb01c events to predetermined actions. For example, a self-driving car that learns to predict whether going forward will produce a collision could be given a built-in re\ufb02ex to stop, or to turn away, whenever the prediction is above some threshold. Or consider a vacuum-cleaning robot that learned to predict whether it might run out of battery power before returning to the charger and that re\ufb02exively headed back to the charger whenever the prediction became non-zero", "6d462fda-d808-4f55-944f-4938ec5cc4bc": "The distribution of observed values x is therefore a mixture of Gaussians, which we take to be of the form where w is the proportion of background clutter and is assumed to be known. The prior over \u03b8 is taken to be Gaussian and Minka  chooses the parameter values a = 10, b = 100 and w = 0.5. The joint distribution of N observations D = {x1, . , xN} and \u03b8 is given by and so the posterior distribution comprises a mixture of 2N Gaussians. Thus the computational cost of solving this problem exactly would grow exponentially with the size of the data set, and so an exact solution is intractable for moderately large N. To apply EP to the clutter problem, we \ufb01rst identify the factors f0(\u03b8) = p(\u03b8) and fn(\u03b8) = p(xn|\u03b8). Next we select an approximating distribution from the exponential family, and for this example it is convenient to choose a spherical Gaussian where n = 1,", "5e720fe6-b1ab-42bd-b22d-a045ed8a8ab2": "Transition probabilities are all set to zero except for those that keep the state index k the same or that increment it by 1, and the model parameters are optimized using 25 iterations of EM. We can gain some insight into the resulting model by running it generatively, as shown in Figure 13.11. One of the most powerful properties of hidden Markov models is their ability to exhibit some degree of invariance to local warping (compression and stretching) of the time axis. To understand this, consider the way in which the digit \u20182\u2019 is written in the on-line handwritten digits example. A typical digit comprises two distinct sections joined at a cusp. The \ufb01rst part of the digit, which starts at the top left, has a sweeping arc down to the cusp or loop at the bottom left, followed by a second moreor-less straight sweep ending at the bottom right. Natural variations in writing style will cause the relative sizes of the two sections to vary, and hence the location of the cusp or loop within the temporal sequence will vary.\n\nFrom a generative perspective such variations can be accommodated by the hidden Markov model through changes in the number of transitions to the same state versus the number of transitions to the successive state", "877cc5b8-9193-41d0-9e76-9f4549fd6a29": "Because a potential function is an arbitrary, nonnegative function over a maximal clique, we can multiply it by any nonnegative functions of subsets of the clique, or equivalently we can add the corresponding energies. In this example, this allows us to add an extra term hxi for each pixel i in the noise-free image.\n\nSuch a term has the effect of biasing the model towards pixel values that have one particular sign in preference to the other. The complete energy function for the model then takes the form which de\ufb01nes a joint distribution over x and y given by We now \ufb01x the elements of y to the observed values given by the pixels of the noisy image, which implicitly de\ufb01nes a conditional distribution p(x|y) over noisefree images. This is an example of the Ising model, which has been widely studied in statistical physics. For the purposes of image restoration, we wish to \ufb01nd an image x having a high probability (ideally the maximum probability). To do this we shall use a simple iterative technique called iterated conditional modes, or ICM , which is simply an application of coordinate-wise gradient ascent", "baba5be0-6871-4235-9450-f828f714930e": "To write a sequential computer program to implement iterative policy evaluation as given by (4.5) you would have to use two arrays, one for the old values, vk(s), and one for the new values, vk+1(s). With two arrays, the new values can be computed one by one from the old values without the old values being changed. Of course it is easier to use one array and update the values \u201cin place,\u201d that is, with each new value immediately overwriting the old one. Then, depending on the order in which the states are updated, sometimes new values are used instead of old ones on the right-hand side of (4.5). This in-place algorithm also converges to v\u21e1; in fact, it usually converges faster than the two-array version, as you might expect, because it uses new data as soon as they are available. We think of the updates as being done in a sweep through the state space. For the in-place algorithm, the order in which states have their values updated during the sweep has a signi\ufb01cant in\ufb02uence on the rate of convergence. We usually have the in-place version in mind when we think of DP algorithms", "5cc881f6-e690-4ba9-903f-3a1a340d7029": "The input at time step \u00a2 is simply the output at time step t\u2014 1. The RNN then defines a directed graphical model over the y variables. We parametrize the joint distribution of these observations using the chain rule (equation 3.6) for conditional probabilities:  P(Y) =P(Y),....y\u00a5M) = P(y\u00a5O fy yO\", y\u2122), (10.31) t=1  where the righthand side of the Id is empty for t = 1, of course. Hence the (1) (7)  a a : c 1  https://www.deeplearningbook.org/contents/rnn.html   negative log-likelihood of a set of values {Y ,---,Y Ff according to such a model is t L= ye ) (10.32)  where L\u00ae = \u2014 log P(y = y | yl Dy) yD), (10.33)  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  Figure 10.7: Fully connected graphical model for a sequencey, f?),...,y,...", "5e4082b7-ca48-4f63-8fa1-a1b31a4a8234": "The above derivation of the backpropagation procedure allowed for general forms for the error function, the activation functions, and the network topology. In order to illustrate the application of this algorithm, we shall consider a particular example. This is chosen both for its simplicity and for its practical importance, because many applications of neural networks reported in the literature make use of this type of network. Speci\ufb01cally, we shall consider a two-layer network of the form illustrated in Figure 5.1, together with a sum-of-squares error, in which the output units have linear activation functions, so that yk = ak, while the hidden units have logistic sigmoid activation functions given by A useful feature of this function is that its derivative can be expressed in a particularly simple form: h\u2032(a) = 1 \u2212 h(a)2", "1947d75b-b2b6-4d51-9de2-d50f94c7fc87": "To summarize, principal component analysis involves evaluating the mean x and the covariance matrix S of the data set and then finding the M eigenvectors of S corresponding to the M largest eigenvalues. Algorithms for finding eigenvectors and eigenvalues, as well as additional theorems related to eigenvector decomposition, can be found in Golub and Van Loan . Note that the computational cost of computing the full eigenvector decomposition for a matrix of size D x Dis O(D3). If we plan to project our data onto the first M principal components, then we only need to find the first M eigenvalues and eigenvectors. This can be done with more efficient techniques, such as the power method , that scale like O(MD 2 ), or alternatively we can make use of the EM algorithm. We now discuss an alternative formulation of peA based on projection error minimization.\n\nTo do this, we introduce a complete orthonormal set of D-dimensional basis vectors {Ui} where i = 1, ..", "6f920f74-2971-4ab7-8a30-9adb5a19fbcf": "First, the large number of weights in a typical deep ANN makes it di\ufb03cult to avoid the problem of over\ufb01tting, that is, the problem of failing to generalize correctly to cases on which the network has not been trained. Second, backpropagation does not work well for deep ANNs because the partial derivatives computed by its backward passes either decay rapidly toward the input side of the network, making learning by deep layers extremely slow, or the partial derivatives grow rapidly toward the input side of the network, making learning unstable. Methods for dealing with these problems are largely responsible for many impressive recent results achieved by systems that use deep ANNs. Over\ufb01tting is a problem for any function approximation method that adjusts functions with many degrees of freedom on the basis of limited training data. It is less of a problem for online reinforcement learning that does not rely on limited training sets, but generalizing e\u21b5ectively is still an important issue.\n\nOver\ufb01tting is a problem for ANNs in general, but especially so for deep ANNs because they tend to have very large numbers of weights. Many methods have been developed for reducing over\ufb01tting", "da160142-a35e-4c88-8a22-1ce91a05141a": "We might imagine ourselves riding the stream of states, looking forward from each state to determine its update, as suggested by Figure 12.4. After looking forward from and updating one state, we move on to the next and never have to work with the preceding state again. Future states, on the other hand, are viewed and processed repeatedly, once from each vantage point preceding them. TD(\u03bb) is one of the oldest and most widely used algorithms in reinforcement learning. It was the \ufb01rst algorithm for which a formal relationship was shown between a more theoretical forward view and a more computationally-congenial backward view using eligibility traces.\n\nHere we will show empirically that it approximates the o\u270fine \u03bb-return algorithm presented in the previous section. TD(\u03bb) improves over the o\u270fine \u03bb-return algorithm in three ways. First it updates the weight vector on every step of an episode rather than only at the end, and thus its estimates may be better sooner. Second, its computations are equally distributed in time rather than all at the end of the episode. And third, it can be applied to continuing problems rather than just to episodic problems", "9423116b-f341-4116-b60d-e42d9e668a26": "In C. M. Bishop and B. Frey (Eds. ), Proceedings Ninth International Workshop on Arti\ufb01cial Intelligence and Statistics, Key West, Florida. Tong, S. and D. Koller . Restricted Bayes optimal classi\ufb01ers. In Proceedings 17th National Conference on Arti\ufb01cial Intelligence, pp. 658\u2013 664. AAAI. Tresp, V. .\n\nScaling kernel-based systems to large data sets. Data Mining and Knowledge Discovery 5(3), 197\u2013211. Valiant, L. G. A theory of the learnable. Communications of the Association for Computing Machinery 27, 1134\u20131142. Vapnik, V. N. Estimation of dependences based on empirical data. Springer. Vapnik, V. N. The nature of statistical learning theory. Springer. Veropoulos, K., C. Campbell, and N. Cristianini", "b123ac1d-3e8b-44e9-90ba-793fe78ad421": "However, sample averages are not a completely satisfactory solution because they may perform poorly on nonstationary problems. Is it possible to avoid the bias of constant step sizes while retaining their advantages on nonstationary problems? One way is to use a step size of to process the nth reward for a particular action, where \u21b5 > 0 is a conventional constant step size, and \u00afon is a trace of one that starts at 0: Exploration is needed because there is always uncertainty about the accuracy of the action-value estimates. The greedy actions are those that look best at present, but some of the other actions may actually be better. \"-greedy action selection forces the non-greedy actions to be tried, but indiscriminately, with no preference for those that are nearly greedy or particularly uncertain", "96f5b4a4-fb84-4ae1-8aca-fee76d70ea1b": "As an example of a zero-shot learning setting, consider the problem of having  536  https://www.deeplearningbook.org/contents/representation.html    CHAPTER 15. REPRESENTATION LEARNING  a learner read a large collection of text and then solve object recognition problems. It may be possible to recognize a specific object class even without having seen an image of that object if the text describes the object well enough. For example, having read that a cat has four legs and pointy ears, the learner might be able to guess that an image is a cat without having seen a cat before. Zero-data learning  and zero-shot learning  are only possible because additional information has been exploited during training.\n\nWe can think of the zero-data learning scenario as including three random variables: the traditional inputs zx, the traditional outputs or targets y, and an additional random variable describing the task, T. The model is trained to estimate the conditional distribution p(y | v,T), where T is a description of the task we wish the model to perform", "921f1dc1-f59b-45fd-ab18-89235e8447ec": "In the average-reward setting, the quality of a policy \u21e1 is de\ufb01ned as the average rate of reward, or simply average reward, while following that policy, which we denote as r(\u21e1): where the expectations are conditioned on the initial state, S0, and on the subsequent actions, A0, A1, . , At\u22121, being taken according to \u21e1. \u00b5\u21e1 is the steady-state distribution, \u00b5\u21e1(s) .= limt!1 Pr{St =s |A0:t\u22121 \u21e0\u21e1}, which is assumed to exist for any \u21e1 and to be independent of S0. This assumption about the MDP is known as ergodicity.\n\nIt means that where the MDP starts or any early decision made by the agent can have only a temporary e\u21b5ect; in the long run the expectation of being in a state depends only on the policy and the MDP transition probabilities. Ergodicity is su\ufb03cient to guarantee the existence of the limits in the equations above. There are subtle distinctions that can be drawn between di\u21b5erent kinds of optimality in the undiscounted continuing case", "b21dea30-8025-4db9-9c47-d2216ac7f968": "In policy gradient methods, the policy can be parameterized in any way, as long as \u21e1(a|s, \u2713) is di\u21b5erentiable with respect to its parameters, that is, as long as r\u21e1(a|s, \u2713) (the column vector of partial derivatives of \u21e1(a|s, \u2713) with respect to the components of \u2713) exists and is \ufb01nite for all s 2 S, a 2 A(s), and \u2713 2 Rd0. In practice, to ensure exploration we generally require that the policy never becomes deterministic (i.e., that \u21e1(a|s, \u2713) 2 (0, 1), for all s, a, \u2713). In this section we introduce the most common parameterization for discrete action spaces and point out the advantages it o\u21b5ers over action-value methods. Policy-based methods also o\u21b5er useful ways of dealing with continuous action spaces, as we describe later in Section 13.7", "1e51b269-ffbd-4eca-bc2d-7225a1671c14": "During training, however, the approximate inference network (or encoder) q(z | a) is used to obtain z, and Pmodei(x | Z) is then viewed as a decoder network. The key insight behind variational autoencoders is that they can be trained by maximizing the variational lower bound \u00a3(q) associated with data point x:  L\u00a3(q) = Rewq(z|a) log Pmodei(Z; x) + H(q(z | x)) (20.76) = Ee wq(z|a) log Pmodel(\u00ae | z) _ Dui (qz | L)||Pmode (z)) (20.77) < O\u00a3 Pmodel(Z).\n\n(20.78)  In equation 20.76, we recognize the first term as the joint log-likelihood of the visible and hidden variables under the approximate posterior over the latent variables (just  https://www.deeplearningbook.org/contents/generative_models.html    as with EM, except that we use an approximate rather than the exact posterior). We recognize also a second term, the entropy of the approximate posterior", "f2383460-7c76-4091-b56f-6197ceb0a44f": "The activity of cortical neurons conveys a wealth of information about sensory input, internal states, and motor activity. The axons of cortical neurons make synaptic contacts on the dendrites of the main input/output neurons of the striatum, called medium spiny neurons. Output from the striatum loops back via other basal ganglia nuclei and the thalamus to frontal areas of cortex, and to motor areas, making it possible for the striatum to in\ufb02uence movement, abstract decision processes, and reward processing.\n\nTwo main subdivisions of the striatum are important for reinforcement learning: the dorsal striatum, primarily implicated in in\ufb02uencing action selection, and the ventral striatum, thought to be critical for di\u21b5erent aspects of reward processing, including the assignment of a\u21b5ective value to sensations. The dendrites of medium spiny neurons are covered with spines on whose tips the axons of neurons in the cortex make synaptic contact. Also making synaptic contact with these spines\u2014in this case contacting the spine stems\u2014are axons of dopamine neurons (Figure 15.1)", "a9bfe953-d4fa-4455-a760-48adf2919b78": ", ym\u22121(x) are \ufb01xed, as are their coef\ufb01cients \u03b11, . , \u03b1m\u22121, and so we are minimizing only with respect to \u03b1m and ym(x).\n\nSeparating off the contribution from base classi\ufb01er ym(x), we can then write the error function in the form because we are optimizing only \u03b1m and ym(x). If we denote by Tm the set of data points that are correctly classi\ufb01ed by ym(x), and if we denote the remaining misclassi\ufb01ed points by Mm, then we can in turn rewrite the error function in the When we minimize this with respect to ym(x), we see that the second term is constant, and so this is equivalent to minimizing (14.15) because the overall multiplicative factor in front of the summation does not affect the location of the minimum. Similarly, minimizing with respect to \u03b1m, we obtain (14.17) in which \u03f5m is de\ufb01ned by (14.16)", "4a3fe2e7-94f0-4aac-ae7b-7bfecab838ad": "One can associate an image dest to a word Ytest, even if no image of that word was  https://www.deeplearningbook.org/contents/representation.html    ever presented, simply because word representations SY (ytest ) and image representations Fela est) can be related to each other via the maps between representation spaces. It works because, although that image and that word were never paired, their respective feature vectors fx(Ztest ) and fy(Ytest ) have been related to each other. Figure inspired  from suggestion by Hrant Khachatrian. 538  CHAPTER 15.\n\nREPRESENTATION LEARNING  explains how one can perform multimodal learning, capturing a representation in one modality, a representation in the other, and the relationship (in general a joint distribution) between pairs (a, y) consisting of one observation x in one modality and another observation y in the other modality . By learning all three sets of parameters (from \u00ab to its representation, from y to its representation, and the relationship between the two representations), concepts in one representation are anchored in the other, and vice versa, allowing one to meaningfully generalize to new pairs. The procedure is illustrated in figure 15.3", "81844d16-1c48-4a56-81b3-1ddb1a2e6529": "The use of momentum in combination with rescaling does not have a clear theoretical motivation. Second, Adam includes bias corrections to the estimates of both the first-order moments (the momentum erm) and the (uncentered) second-order moments to account for their initialization at the origin (see algorithm 8.7). RMSProp also incorporates an estimate of the  Algorithm 8.6 RMSProp algorithm with Nesterov momentum  Require: Global learning rate \u20ac, decay rate p, momentum coefficient a Require: Initial parameter 0, initial velocity v Initialize accumulation variable r = 0 while stopping criterion not met do Sample a minibatch of m examples from the training set {a),...,2(\u2122} with corresponding targets y @, Compute interim update: 0+ O+aw. Compute gradient: g + 1Vg >; L(f (a0), y). Accumulate gradient: r << pr +(1\u2014p)g Og. Compute velocity update: v + av Ti Og. ( Fi applied element-wise) Apply update: 0+ 6+ v", "d6d062fa-c7d4-4551-b056-ae9cce3a8f4a": "In this chapter we explore the convergence problems, take a closer look at the theory of linear function approximation, introduce a notion of learnability, and then discuss new algorithms with stronger convergence guarantees for the o\u21b5-policy case. In the end we will have improved methods, but the theoretical results will not be as strong, nor the empirical results as satisfying, as they are for on-policy learning. Along the way, we will gain a deeper understanding of approximation in reinforcement learning for on-policy learning as well as o\u21b5-policy learning. Recall that in o\u21b5-policy learning we seek to learn a value function for a target policy \u21e1, given data due to a di\u21b5erent behavior policy b.\n\nIn the prediction case, both policies are static and given, and we seek to learn either state values \u02c6v \u21e1 v\u21e1 or action values \u02c6q \u21e1 q\u21e1. In the control case, action values are learned, and both policies typically change during learning\u2014\u21e1 being the greedy policy with respect to \u02c6q, and b being something more exploratory such as the \"-greedy policy with respect to \u02c6q", "17f58820-1b6d-4348-bcd2-74e7a55225df": "Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. 2016. An actor-critic algorithm for sequence prediction. arXiv preprint arXiv:1607.07086. Samuel Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632\u2013642. Yen-Chun Chen and Mohit Bansal. 2018. Fast abstractive summarization with reinforce-selected sentence rewriting. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 675\u2013686. Leshem Choshen, Lior Fox, Zohar Aizenbud, and Omri Abend. 2020. On the weaknesses of reinforcement learning for neural machine translation. In International Conference on Learning Representations", "18742ad4-9695-47b7-bb88-381ef1c6c4a6": "The former kind of probability, related directly to the rates at which events occur, is known as frequentist probability, while the latter, related to  newalitadiven lavenln AL nantaintee fA lene 24 Daeernntianwn nnn LaLilti.--  https://www.deeplearningbook.org/contents/prob.html    YUALMLALIVE 1CVCID VL COL Lally, ib KLUWILI ad bDayvcsiall pL UuvaWViIlivy.\n\nIf we list several properties that we expect common sense reasoning about uncertainty to have, then the only way to satisfy those properties is to treat Bayesian probabilities as behaving exactly the same as frequentist probabilities. For example, if we want to compute the probability that a player will win a poker game given that she has a certain set of cards, we use exactly the same formulas as when we compute the probability that a patient has a disease given that she has certain symptoms. For more details about why a small set of common sense  53  CHAPTER 3. PROBABILITY AND INFORMATION THEORY  assumptions implies that the same axioms must control both kinds of probability, see Ramsey", "0b05a974-f1d5-4214-b702-d71911746ab7": "An estimator 6, is said to be asymptotically unbiased if  lim mann hinc(Am\\ A Wt tennltan ahd Ite min R/Am\\ a  https://www.deeplearningbook.org/contents/ml.html  quae PN aay ry) = UL WILE LIP Ues Ullal LL oN uy =O, Example: Bernoulli Distribution Consider a set of samples {a pene al\u201d) that are independently and identically distributed according to a Bernoulli distri-  122  CHAPTER 5", "318ee19b-52ef-45ff-89fe-07da238992cf": "If the sign of the covariance is positive, then both variables tend to take on relatively high values simultaneously. If the sign of the covariance is negative, then one variable tends to take on a relatively high value at the times that the other takes on a relatively low value and vice versa. Other measures such as correlation normalize the contribution of each variable in order to measure only how much the variables are related, rather than also being affected by the scale of the separate variables. The notions of covariance and dependence are related but distinct concepts. They are related because two variables that are independent have zero covariance, and two variables that have nonzero covariance are dependent. Independence, however, is a distinct property from covariance. For two variables to have zero covariance, there must be no linear dependence between them. Independence is a stronger requirement than zero covariance, because independence also excludes nonlinear relationships. It is possible for two variables to be dependent but have zero covariance. For example, suppose we first sample a real number x from a uniform distribution over the interval .\n\nWe next sample a random variable s.  59  CHAPTER 3", "52954f49-aba6-4543-b0c9-b3ba600cdbfa": "Global con\ufb01gurations that have a relatively high probability are those that \ufb01nd a good balance in satisfying the (possibly con\ufb02icting) in\ufb02uences of the clique potentials. We turn now to a speci\ufb01c example to illustrate the use of undirected graphs. We can illustrate the application of undirected graphs using an example of noise removal from a binary image . Although a very simple example, this is typical of more sophisticated applications. Let the observed noisy image be described by an array of binary pixel values yi \u2208 {\u22121, +1}, where the index i = 1, . , D runs over all pixels. We shall suppose that the image is obtained by taking an unknown noise-free image, described by binary pixel values xi \u2208 {\u22121, +1} and randomly \ufb02ipping the sign of pixels with some small probability. An example binary image, together with a noise corrupted image obtained by \ufb02ipping the sign of the pixels with probability 10%, is shown in Figure 8.30.\n\nGiven the noisy image, our goal is to recover the original noise-free image. Because the noise level is small, we know that there will be a strong correlation between xi and yi", "ebf766b6-89a5-4464-a568-cfc613425eac": "The design process consists of trial and error, intuiting that a kind of hidden unit may work well, and then training a network with that kind of hidden unit and evaluating its performance on a validation set. Some of the hidden units included in this list are not actually differentiable at all input points. For example, the rectified linear function g(z) = max{0, z} is not differentiable at z = 0. This may seem like it invalidates g for use with a gradient-based learning algorithm. In practice, gradient descent still performs well enough for these models to be used for machine learning tasks. This is in part because neural network training algorithms do not usually arrive at a local minimum of the cost function, but instead merely reduce its value significantly, as shown in figure 4.3. (These ideas are described further in chapter 8.) Because we do not expect training to actually reach a point where the gradient is 0, it is acceptable for the minima of the cost function to correspond to points with undefined gradient", "10e6d565-b922-4441-8de4-18c4b872a269": "Let's see how it works in an action-value actor-critic algorithm.\n\nInitialize s, 8, w at random; sample a ~ 7(a|s; 0). For t = 1... T: Sample reward r; ~ R(s, a) and next state s\u2019 ~ P(s'|s, a). Then sample the next action a\u2019 ~ 7(s\u2019, a\u2019; 8). Update policy parameters: 0 ~\u2014 6 + agQ(s, a; w) Vo ln x(als; 6). Compute the correction for action-value at time t: Gru =T + Q(s','; w) \u2014 Q(s, a; w) and use it to update value function parameters: we wt ay,Gir.1Vy~Q(s, a; w). https://lilianweng.github.io/posts/2018-02-19-rl-overview/  A (Long) Peek into Reinforcement Learning | Lil'Log  Update a + a\u2019 ands \u00a2 3\u2019", "d0e546f3-ae56-4b09-8bc4-785d790be14c": "NUMERICAL COMPUTATION  solution where the constraint is active. By differentiating the Lagrangian with respect to 2, we obtain the equation  A\u2019 Ax \u2014A\u2018b+2\\\" = 0. (4.25) This tells us that the solution will take the form x =(A'A+2\\1) 1A 'b. (4.26)  The magnitude of A must be chosen such that the result obeys the constraint. We can find this value by performing gradient ascent on 4. To do so, observe  (a)  _ Tt Hy L(#:A) aw \u00ab2-1. (4.27)  When the norm of x exceeds 1, this derivative is positive, so to follow the derivative uphill and increase the Lagrangian with respect to , we increase A. Because the coefficient on the \u00ab!' a penalty has increased, solving the linear equation for \u00ab will now yield a solution with a smaller norm. The process of solving the linear equation and adjusting A continues until a has the correct norm and the derivative on A is 0. This concludes the mathematical preliminaries that we use to develop machine learning algorithms. We are now ready to build and analyze some full-fledged learning systems", "3fda57ff-73c4-4fb7-ac11-30c092559610": "can resoh'e this problem as foIl\",\",'\" Fir;l. let us define X to be the (N \" DJ\u00b7 dimensional centred data matrix, whose nth row is given by (xn - X)T. The covariance matrix (12.3) can then be written as S = N- 1XTX, and the corresponding eigenvector equation becomes Now pre-multiply both sides by X to give which is an eigenvector equation for the N x N matrix N- 1XXT . We see that this has the same N -1 eigenvalues as the original covariance matrix (which itself has an additional D - N + 1 eigenvalues of value zero). Thus we can solve the eigenvector problem in spaces of lower dimensionality with computational cost O(N3 ) instead of O(D3 ). In order to determine the eigenvectors, we multiply both sides of (12.28) by X T to give from which we see that (XTVi) is an eigenvector of S with eigenvalue Ai", "30b7bdd9-e789-4d16-b43f-bd1c0219cea0": "For large D, the total number of parameters therefore grows quadratically with D, and the computational task of manipulating and inverting large matrices can become prohibitive. One way to address this problem is to use restricted forms of the covariance matrix. If we consider covariance matrices that are diagonal, so that \u03a3 = diag(\u03c32 pendent parameters in the density model.\n\nThe corresponding contours of constant density are given by axis-aligned ellipsoids. We could further restrict the covariance matrix to be proportional to the identity matrix, \u03a3 = \u03c32I, known as an isotropic covariance, giving D + 1 independent parameters in the model and spherical surfaces of constant density. The three possibilities of general, diagonal, and isotropic covariance matrices are illustrated in Figure 2.8. Unfortunately, whereas such approaches limit the number of degrees of freedom in the distribution and make inversion of the covariance matrix a much faster operation, they also greatly restrict the form of the probability density and limit its ability to capture interesting correlations in the data", "b463aee7-038d-4b1d-b677-8a0cba08d7cf": "Intuitively, we can think of the distribution p(x) as being defined by taking an isotropic Gaussian 'spray can' and moving it across the principal subspace spraying Gaussian ink with density determined by 0-2 and weighted by the prior distribution. The accumulated ink density gives rise to a 'pancake' shaped distribution representing the marginal density p(x).\n\nThe predictive distribution p(x) is governed by the parameters JL, W, and 0-2 \u2022 However, there is redundancy in this parameterization corresponding to rotations of the latent space coordinates. To see this, consider a matrix W = WR where R is an orthogonal matrix. Using the orthogonality property RRT = I, we see that the quantity WWT that appears in the covariance matrix C takes the form and hence is independent of R. Thus there is a whole family of matrices W all of which give rise to the same predictive distribution. This invariance can be understood in terms of rotations within the latent space. We shall return to a discussion of the number of independent parameters in this model later", "9ad85320-0bc0-4331-b1d6-f55fc09b7ce8": "This matrix can be singular whenever the data-generating distribution truly has no variance in some direction, or when no variance is observed in some direction because there are fewer examples (rows of X ) than input features (columns of X). In this case, many forms of regularization correspond to inverting X'X+al instead. This regularized matrix is guaranteed to be invertible. These linear problems have closed form solutions when the relevant matrix is invertible. It is also possible for a problem with no closed form solution to be underdetermined. An example is logistic regression applied to a problem where the classes are linearly separable. If a weight vector w is able to achieve perfect classification, then 2w will also achieve perfect classification and higher likelihood.\n\nAn iterative optimization procedure like stochastic gradient descent will continually increase the magnitude of w and, in theory, will never halt", "a45d0d74-d40e-4a6a-a0b8-56a954abbf23": "Now suppose that we observe the fuel gauge and discover that it reads empty, i.e., G = 0, corresponding to the middle graph in Figure 8.21. We can use Bayes\u2019 theorem to evaluate the posterior probability of the fuel tank being empty. First we evaluate the denominator for Bayes\u2019 theorem given by p(F = 0|G = 0) = p(G = 0|F = 0)p(F = 0) and so p(F = 0|G = 0) > p(F = 0). Thus observing that the gauge reads empty makes it more likely that the tank is indeed empty, as we would intuitively expect.\n\nNext suppose that we also check the state of the battery and \ufb01nd that it is \ufb02at, i.e., B = 0. We have now observed the states of both the fuel gauge and the battery, as shown by the right-hand graph in Figure 8.21. The posterior probability that the fuel tank is empty given the observations of both the fuel gauge and the battery state is then given by where the prior probability p(B = 0) has cancelled between numerator and denominator", "0a965551-d8cb-4410-95b7-f7bed3212532": "Note that the value of the blue curve at any point, such as that indicated by the vertical green line, corresponds to the slope of the red curve at the same point. Conversely, the value of the red curve at this point corresponds to the area under the blue curve indicated by the shaded green region. In the stochastic threshold model, the class label takes the value t = 1 if the value of a = wT\u03c6 exceeds a threshold, otherwise it takes the value t = 0. This is equivalent to an activation function given by the cumulative distribution function f(a). If the value of \u03b8 is drawn from a probability density p(\u03b8), then the corresponding activation function will be given by the cumulative distribution function As a speci\ufb01c example, suppose that the density p(\u03b8) is given by a zero mean, unit variance Gaussian.\n\nThe corresponding cumulative distribution function is given by which is known as the probit function. It has a sigmoidal shape and is compared with the logistic sigmoid function in Figure 4.9. Note that the use of a more general Gaussian distribution does not change the model because this is equivalent to a re-scaling of the linear coef\ufb01cients w", "b15ba92a-6663-42e6-803a-c54ea7347927": "The support set is part of the model input. The final optimization uses the mini-batch B* to compute the loss and update the model parameters through backpropagation, in the same way as how we use it in the supervised learning.\n\nYou may consider each pair of sampled dataset (S\u00a5, Bt) as one data point. The model is trained such that it can generalize to other datasets. Symbols in red are added for meta-learning in addition to the supervised learning objective. O= argmax Ercc|Estcp,ptcol SS P,(z,y,S\u201d)]]| (z,y)e BE  The idea is to some extent similar to using a pre-trained model in image classification (ImageNet) or language modeling (big text corpora) when only a limited set of task-specific data samples are available. Meta-learning takes this idea one step further, rather than fine-tuning according to one down-steam task, it optimizes the model to be good at many, if not all", "7e2b1c7d-af5b-4b9d-aca2-efdcc31adc0f": "Maayan F-A, Eyal K, Jacob G, Hayit G.GAN-based data augmentation for improved liver lesion classification. arXiv preprint. 2018. Joseph R, Santosh D, Ross G, Ali F. You only look once: unified, real-time object detection. In: CVPR'16. 2016. Ross G, Jeff D, Trevor D, Jitendra M. Rich feature hierarchies for accurate object detection and semantic segmenta- tion. In: CVPR'14. 2014. Ross G. Fast R-CNN. CoRR, abs/1504.08083. 2015. Shaoging R, Kaiming H, Ross G, Jian S. Faster R-CNN: towards real-time object detection with region proposal networks. In: NIPS, 2015. Jonathan L, Evan S, Trevor D. Fully convolutional networks for semantic segmentation. CoRR, abs/1411.4038. 2014", "8474afb4-c6cc-4377-a0da-55df44e78c0f": "\u201cLearning a Nonlinear Embedding by Preserving Class Neighbourhood Structure\u201d AISTATS 2007. 5. Michael Gutmann and Aapo Hyvarinen. \u201cNoise-contrastive estimation: A new estimation principle for unnormalized statistical models.\" AISTATS 2010. 6] Kihyuk Sohn et al. \u201cImproved Deep Metric Learning with Multi-class N-pair Loss Objective\" NIPS 2016. 7] Nicholas Frosst, Nicolas Papernot and Geoffrey Hinton. \u201cAnalyzing and Improving Representations with the Soft Nearest Neighbor Loss.\" ICML 2019  8] Tongzhou Wang and Phillip Isola.\n\n\u201cUnderstanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere.\" ICML 2020. 9] Zhirong Wu et al. \u201cUnsupervised feature learning via non-parametric instance-level discrimination.\" CVPR 2018. 10] Ekin D. Cubuk et al", "fc3a3d1b-f896-412b-9bbe-f825b5e0f1a3": "By considering any performance that scored at or above 75% of the human score to be comparable to, or better than, human-level play, Mnih et al. concluded that the levels of play DQN learned reached or exceeded human level on 29 of the 46 games. See Mnih et al. for a more detailed account of these results. For an arti\ufb01cial learning system to achieve these levels of play would be impressive enough, but what makes these results remarkable\u2014and what many at the time considered to be breakthrough results for arti\ufb01cial intelligence\u2014is that the very same learning system achieved these levels of play on widely varying games without relying on any game-speci\ufb01c modi\ufb01cations. A human playing any of these 49 Atari games sees 210\u21e5160 pixel image frames with 128 colors at 60Hz. In principle, exactly these images could have formed the raw input to DQN, but to reduce memory and processing requirements, Mnih et al. preprocessed each frame to produce an 84\u21e584 array of luminance values. Because the full states of many of the Atari games are not completely observable from the image frames, Mnih et al", "81c47ac6-cddb-4620-83fb-5052ed04d36f": ", xD)T. This is often simply known as linear regression. The key property of this model is that it is a linear function of the parameters w0, . , wD. It is also, however, a linear function of the input variables xi, and this imposes signi\ufb01cant limitations on the model. We therefore extend the class of models by considering linear combinations of \ufb01xed nonlinear functions of the input variables, of the form where \u03c6j(x) are known as basis functions. By denoting the maximum value of the index j by M \u2212 1, the total number of parameters in this model will be M. The parameter w0 allows for any \ufb01xed offset in the data and is sometimes called a bias parameter (not to be confused with \u2018bias\u2019 in a statistical sense). It is often convenient to de\ufb01ne an additional dummy \u2018basis function\u2019 \u03c60(x) = 1 so that where w = (w0, . , wM\u22121)T and \u03c6 = (\u03c60, . , \u03c6M\u22121)T", "75e9058a-193b-437d-af03-9f5e60270d57": "We can extend the Laplace method to approximate a distribution p(z) = f(z)/Z de\ufb01ned over an M-dimensional space z. At a stationary point z0 the gradient \u2207f(z) will vanish. Expanding around this stationary point we have where the M \u00d7 M Hessian matrix A is de\ufb01ned by and \u2207 is the gradient operator. Taking the exponential of both sides we obtain The distribution q(z) is proportional to f(z) and the appropriate normalization coef\ufb01cient can be found by inspection, using the standard result (2.43) for a normalized multivariate Gaussian, giving where |A| denotes the determinant of A. This Gaussian distribution will be well de\ufb01ned provided its precision matrix, given by A, is positive de\ufb01nite, which implies that the stationary point z0 must be a local maximum, not a minimum or a saddle point. In order to apply the Laplace approximation we \ufb01rst need to \ufb01nd the mode z0, and then evaluate the Hessian matrix at that mode", "c975c226-c1ad-46fb-8765-98a34b3d42d7": "Freund, Y., & Schapire, R. E. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of computer and system sciences, 55(1), 119\u2013139. Ganchev, K., Gillenwater, J., Taskar, B., et al. Posterior regularization for structured latent variable models. Journal of Machine Learning Research, 11(Jul), 2001\u20132049. Gebru, I. D., Alameda-Pineda, X., Forbes, F., & Horaud, R. EM algorithms for weighteddata clustering with application to audio-visual scene analysis. IEEE transactions on pattern analysis and machine intelligence, 38(12), 2402\u20132415. Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., & Bengio, Y", "aa3ddb8b-3615-4a7a-9ff2-93a8e4ba7541": "Fr\u00b4emaux, Sprekeler, and Gerstner  proposed theoretical conditions for successful learning by rules based on reward-modulated STDP. 15.9 Klopf\u2019s hedonistic neuron hypothesis  inspired our actor\u2013critic algorithm implemented as an ANN with a single neuron-like unit, called the actor unit, implementing a Law-of-E\u21b5ect-like learning rule . Ideas related to Klopf\u2019s synaptically-local eligibility have been proposed by others. Crow  proposed that changes in the synapses of cortical neurons are sensitive to the consequences of neural activity. Emphasizing the need to address the time delay between neural activity and its consequences in a reward-modulated form of synaptic plasticity, he proposed a contingent form of eligibility, but associated with entire neurons instead of individual synapses.\n\nAccording to his hypothesis, a wave of neuronal activity leads to a short-term change in the cells involved in the wave such that they are picked out from a background of cells not so activated. ... such cells are rendered sensitive by the short-term change to a reward signal ..", "5f8f2305-2d29-441b-9729-c324c5c5958c": "Chapter 7  Regularization for Deep Learning  A central problem in machine learning is how to make an algorithm that will perform well not just on the training data, but also on new inputs. Many strategies used in machine learning are explicitly designed to reduce the test error, possibly at the expense of increased training error. These strategies are known collectively as regularization. A great many forms of regularization are available to the deep learning practitioner. In fact, developing more effective regularization strategies has been one of the major research efforts in the field. Chapter 5 introduced the basic concepts of generalization, underfitting, overfit- ting, bias, variance and regularization. If you are not already familiar with these notions, please refer to that chapter before continuing with this one. In this chapter, we describe regularization in more detail, focusing on regular- ization strategies for deep models or models that may be used as building blocks to form deep models. Some sections of this chapter deal with standard concepts in machine learning. If you are already familiar with these concepts, feel free to skip the relevant sections", "1a4cf483-b010-4630-b12f-cbf07cdc92ff": "Thermal soaring, as this behavior is called, is a complex skill requiring responding to subtle environmental cues to increase altitude by exploiting a rising column of air for as long as possible. Reddy, Celani, Sejnowski, and Vergassola  used reinforcement learning to investigate thermal soaring policies that are e\u21b5ective in the strong atmospheric turbulence usually accompanying rising air currents.\n\nTheir primary goal was to provide insight into the cues birds sense and how they use them to achieve their impressive thermal soaring performance, but the results also contribute to technology relevant to autonomous gliders. Reinforcement learning had previously been applied to the problem of navigating e\ufb03ciently to the vicinity of a thermal updraft  but not to the more challenging problem of soaring within the turbulence of the updraft itself. Reddy et al. modeled the soaring problem as a continuing MDP with discounting. The agent interacted with a detailed model of a glider \ufb02ying in turbulent air. They devoted signi\ufb01cant e\u21b5ort toward making the model generate realistic thermal soaring conditions, including investigating several di\u21b5erent approaches to atmospheric modeling", "f43dec00-80fd-4041-85f2-c1a5f1bdf6b7": "Salakhutdinov and Larochelle  showed that a single pass in a  learned inference network could yield faster inference than iterating the mean field fixed-point equations in a DBM.\n\nThe training procedure is based on running the inference network, then applying one step of mean field to improve its estimates, and training the inference network to output this refined estimate instead of its original estimate. We have already seen in section 14.8 that the predictive sparse decomposition model trains a shallow encoder network to predict a sparse code for the input. This can be seen as a hybrid between an autoencoder and sparse coding. It is possible to devise probabilistic semantics for the model, under which the encoder may be viewed as performing learned approximate MAP inference. Due to its shallow encoder, PSD is not able to implement the kind of competition between units that we have seen in mean field inference. However, that problem can be remedied by training a deep encoder to perform learned approximate inference, as in the ISTA technique . Learned approximate inference has recently become one of the dominant approaches to generative modeling, in the form of the variational autoencoder", "e3fd4e0f-4ccf-4ccd-b1b2-5eb86093f51f": "Taking the logarithm, we obtain Comparison with the log likelihood function (9.14) for the incomplete data shows that the summation over k and the logarithm have been interchanged. The logarithm now acts directly on the Gaussian distribution, which itself is a member of the exponential family. Not surprisingly, this leads to a much simpler solution to the maximum likelihood problem, as we now show. Consider \ufb01rst the maximization with respect to the means and covariances. Because zn is a K-dimensional vector with all elements equal to 0 except for a single element having the value 1, the complete-data log likelihood function is simply a sum of K independent contributions, one for each mixture component. Thus the maximization with respect to a mean or a covariance is exactly as for a single Gaussian, except that it involves only the subset of data points that are \u2018assigned\u2019 to that component. For the maximization with respect to the mixing coef\ufb01cients, we note that these are coupled for different values of k by virtue of the summation constraint (9.9)", "c84e1dc2-17ff-4c4b-b11e-4dc62946bc10": "Contrastive learning needs stronger data augmentation than supervised learning To further demonstrate the importance of the color augmentation, we adjust the strength of color augmentation as 5Supervised models are trained for 90 epochs; longer training improves performance of stronger augmentation by \u223c 0.5%. shown in Table 1. Stronger color augmentation substantially improves the linear evaluation of the learned unsupervised models. In this context, AutoAugment , a sophisticated augmentation policy found using supervised learning, does not work better than simple cropping + (stronger) color distortion. When training supervised models with the same set of augmentations, we observe that stronger color augmentation does not improve or even hurts their performance. Thus, our experiments show that unsupervised contrastive learning bene\ufb01ts from stronger (color) data augmentation than supervised learning.\n\nAlthough previous work has reported that data augmentation is useful for self-supervised learning , we show that data augmentation that does not yield accuracy bene\ufb01ts for supervised learning can still help considerably with contrastive learning. 4. Architectures for Encoder and Head A Simple Framework for Contrastive Learning of Visual Representations 4.2", "22a900ee-ea74-418d-b11d-95b264b5d2a7": "Furthermore, let one of the basis functions be constant, say \u03c60(x) = 1.\n\nBy taking suitable linear combinations of these basis functions, we can construct a new basis set \u03c8j(x) spanning the same space but that are orthonormal, so that where Ijk is de\ufb01ned to be 1 if j = k and 0 otherwise, and we take \u03c80(x) = 1. Show that for \u03b1 = 0, the equivalent kernel can be written as k(x, x\u2032) = \u03c8(x)T\u03c8(x\u2032) where \u03c8 = (\u03c81, . , \u03c8M)T. Use this result to show that the kernel satis\ufb01es the summation constraint N \ufffd 3.15 (\u22c6) www Consider a linear basis function model for regression in which the parameters \u03b1 and \u03b2 are set using the evidence framework. Show that the function E(mN) de\ufb01ned by (3.82) satis\ufb01es the relation 2E(mN) = N. 3.16 (\u22c6 \u22c6) Derive the result (3.86) for the log evidence function p(t|\u03b1, \u03b2) of the linear regression model by making use of (2.115) to evaluate the integral (3.77) directly", "c74017dc-5d53-49a7-9ae3-6da4628161ea": "Adversarial search to add noise has been shown to improve performance on adversarial examples, but it is unclear if this is useful for the objective of reducing overfitting.\n\nFuture work seeks to expand on the relationship between resistance to adversarial attacks and actual  performance on test datasets. GAN-based Data Augmentation  Another exciting strategy for Data Augmentation is generative modeling. Genera- tive modeling refers to the practice of creating artificial instances from a dataset such that they retain similar characteristics to the original set. The principles of adversarial training discussed above have led to the very interesting and massively popular genera- tive modeling framework known as GANs. Bowles et al. describe GANs as a way  to \u201cunlock\u201d additional information from a dataset. GANs are not the only generative Shorten and Khoshgoftaar J Big Data  6:60   Generator G(z) Discriminator D(x)  content source  Fig. 16 Illustration of GAN concept provided by Mikolajczyk and Grochowski   modeling technique that exists; however they are dramatically leading the way in com- putation speed and quality of results", "b76ebd36-7cf6-49fd-a40a-7ad830e5d1b2": "Words represented by one-hot vectors are not very informative because every two distinct one-hot vectors are the same distance from each other (squared L? distance of 2). Learned word embeddings naturally encode similarity between words by their distance from each other. Because of this, unsupervised pretraining is especially useful when processing words. It is less useful when processing images, perhaps because images already lie in a rich vector space where distances provide a low-quality similarity metric.\n\nFrom the point of view of unsupervised pretraining as a regularizer, we can expect unsupervised pretraining to be most helpful when the number of labeled  https://www.deeplearningbook.org/contents/representation.html    examples 18 very small. Because the source ot intormation added by unsupervised pretraining is the unlabeled data, we may also expect unsupervised pretraining to perform best when the number of unlabeled examples is very large", "d9417b97-38aa-4682-a05b-8ee6a42b5ece": "Note that the same solution, w = 1, is optimal for both MRPs above (assuming \u00b5 is the same for the two indistinguishable states in the right MRP). Is this a coincidence, or could it be generally true that all MDPs with the same data distribution also have the same optimal parameter vector? If this is true\u2014and we will show next that it is\u2014then the VE remains a usable objective. The VE is not learnable, but the parameter that optimizes it is! To understand this, it is useful to bring in another natural objective function, this time one that is clearly learnable. One error that is always observable is that between the value estimate at each time and the return from that time. The Mean Square Return Error, denoted RE, is the expectation, under \u00b5, of the square of this error. In the on-policy case the RE can be written Thus, the two objectives are the same except for a variance term that does not depend on the parameter vector. The two objectives must therefore have the same optimal parameter value w\u21e4. The overall relationships are summarized in the left side of Figure 11.4", "a645154d-9b08-4598-a8aa-98bafe204cab": "When A has more rows than columns, it is possible for there to be no solution. In this case, using the pseudoinverse gives us the \u00ab for which Az is as close as possible to y in terms of Euclidean norm || Aa \u2014 y||2. 2.10 The Trace Operator  The trace operator gives the sum of all the diagonal entries of a matrix:  https://www.deeplearningbook.org/contents/linear_algebra.html   Tr(A) = S. Ai, (2.48)  The trace operator is useful for a variety of reasons. Some operations that are difficult to specify without resorting to summation notation can be specified using matrix products and the trace operator. For example, the trace operator provides an alternative way of writing the Frobenius norm of a matrix:  |Allp = Tr(AA\u2018). (2.49) 44  CHAPTER 2. LINEAR ALGEBRA  Writing an expression in terms of the trace operator opens up opportunities to manipulate the expression using many useful identities. For example, the trace operator is invariant to the transpose operator:  Tr(A) = Tr(A\u2018)", "a2d5d519-943b-4f95-adf3-be3597421d0a": "Derivatives with respect to vectors and matrices can also be de\ufb01ned, for instance \ufffd\u2202x The following is easily proven by writing out the components The derivative of the inverse of a matrix can be expressed as as can be shown by differentiating the equation A\u22121A = I using (C.20) and then right multiplying by A\u22121. Also which we shall prove later. If we choose x to be one of the elements of A, we have as can be seen by writing out the matrices using index notation. We can write this result more compactly in the form With this notation, we have the following properties which can again be proven by writing out the matrix indices.\n\nWe also have For a square matrix A of size M \u00d7 M, the eigenvector equation is de\ufb01ned by for i = 1, . , M, where ui is an eigenvector and \u03bbi is the corresponding eigenvalue. This can be viewed as a set of M simultaneous homogeneous linear equations, and the condition for a solution is that which is known as the characteristic equation", "9574f4e9-1801-49f5-aee5-239e32a6dfcf": "This equation says that the \u03bb-return is the \ufb01rst reward, undiscounted and una\u21b5ected by bootstrapping, plus possibly a second term to the extent that we are not discounting at the next state (that is, according to \u03b3t+1; recall that this is zero if the next state is terminal). To the extent that we aren\u2019t terminating at the next state, we have a second term which is itself divided into two cases depending on the degree of bootstrapping in the state. To the extent we are bootstrapping, this term is the estimated value at the state, whereas, to the extent that we not bootstrapping, the term is the \u03bb-return for the next time step. The action-based \u03bb-return is either the Sarsa form where (7.8) is generalized to function approximation as The \ufb01nal step is to incorporate importance sampling. For non-truncated \u03bb-returns, there is not a practical option in which the importance sampling is done outside the target return (as there is for n-step methods as explained in Section 7.3).\n\nInstead, we move directly to the bootstrapping generalization of per-decision importance sampling with control variates (Section 7.4)", "d81c7f89-d604-44f9-8570-e4062f8c197b": "Consider letting the constants be of the form P(z|X, \u03b8n). Since P(z|X, \u03b8n) is a probability measure, we have that P(z|X, \u03b8n) \u2265 0 and that \ufffd z P(z|X, \u03b8n) = 1 as required. Then starting with Equation (11) the constants P(z|X, \u03b8n) are introduced as, Equivalently we may write, and for convenience de\ufb01ne, so that the relationship in Equation (13) can be made explicit as, We have now a function, l(\u03b8|\u03b8n) which is bounded above by the likelihood function L(\u03b8). Additionally, observe that, so for \u03b8 = \u03b8n the functions l(\u03b8|\u03b8n) and L(\u03b8) are equal. Our objective is to choose a values of \u03b8 so that L(\u03b8) is maximized. We have shown that the function l(\u03b8|\u03b8n) is bounded above by the likelihood function L(\u03b8) and that the value of the functions l(\u03b8|\u03b8n) and L(\u03b8) are equal at the current estimate for \u03b8 = \u03b8n", "1109da20-a2ce-47da-bcb8-7d76d30d0ec3": "He developed the idea of \u201cgeneralized reinforcement,\u201d whereby every component (nominally, every neuron) views all of its inputs in reinforcement terms: excitatory inputs as rewards and inhibitory inputs as punishments.\n\nThis is not the same idea as what we now know as temporal-di\u21b5erence learning, and in retrospect it is farther from it than was Samuel\u2019s work. On the other hand, Klopf linked the idea with trial-and-error learning and related it to the massive empirical database of animal learning psychology. Sutton (1978a,b,c) developed Klopf\u2019s ideas further, particularly the links to animal learning theories, describing learning rules driven by changes in temporally successive predictions. He and Barto re\ufb01ned these ideas and developed a psychological model of classical conditioning based on temporal-di\u21b5erence learning . There followed several other in\ufb02uential psychological models of classical conditioning based on temporal-di\u21b5erence learning", "65d9931c-1c5e-4b29-ae7e-a4ee71852b4a": "For example, by marginal- izing out the slab variables s, the conditional distribution over the observations given the binary spike variables h is given by  1 ol Pss(a | h) Pik) Z [owt E(a,s,h)} ds (20.52) = N #03, W.imihi, CSp (20.53)  i where CS, = A+ {Bihi\u2014 50% \u201cay Wra \\, 1 The last equality holds only if the covariance matrix Ooh is positive definite. Gating by (the spike variables means that dhe true marginal distribution over  https://www.deeplearningbook.org/contents/generative_models.html    WS Is sparse. J is 1s different trom sparse coding, where samples trom the model 6, 1) ; + ; ; / \u2018almost never\u201d (in the measure theoretic sense) contain zeros in the code, and MAP inference is required to impose sparsity", "512a506c-0241-42f7-aad8-1b532f63ad01": "By using probabilistic PCA it is straightforward to define a fully probabilistic model simply by considering a mixture distribution in which the components are probabilistic PCA models . Such a model has both discrete latent variables, corresponding to the discrete mixture, as well as continuous latent variables, and the likelihood function can be maximized using the EM algorithm. A fully Bayesian treatment, based on variational inference , allows the number of components in the mixture, as well as the effective dimensionalities of the individual models, to be inferred from the data. There are many variants of this model in which parameters such as the W matrix or the noise variances are tied across components in the mixture, or in which the isotropic noise distributions are replaced by diagonal ones, giving rise to a mixture of factor analysers . The mixture of probabilistic PCA models can also be extended hierarchically to produce an interactive data visualization algorithm .\n\nAn alternative to considering a mixture of linear models is to consider a single nonlinear model. Recall that conventional PCA finds a linear subspace that passes close to the data in a least-squares sense. This concept can be extended to onedimensional nonlinear surfaces in the form of principal curves", "44636dea-a8c4-400e-865d-ea162fd45286": "By the time of Silver et al.\u2019s 2016 publication, AlphaGo had been shown to be decisively stronger than other current Go programs, and it had defeated the European Go champion Fan Hui 5 games to 0. These were the \ufb01rst victories of a Go program over a professional human Go player without handicap in full Go games.\n\nShortly thereafter, a similar version of AlphaGo won stunning victories over the 18-time world champion Lee Sedol, winning 4 out of a 5 games in a challenge match, making worldwide headline news. Arti\ufb01cial intelligence researchers thought that it would be many more years, perhaps decades, before a program reached this level of play. Here we describe AlphaGo and a successor program called AlphaGo Zero (Silver et al. 2017a). Where in addition to reinforcement learning, AlphaGo relied on supervised learning from a large database of expert human moves, AlphaGo Zero used only reinforcement learning and no human data or guidance beyond the basic rules of the game (hence the Zero in its name). We \ufb01rst describe AlphaGo in some detail in order to highlight the relative simplicity of AlphaGo Zero, which is both higher-performing and more of a pure reinforcement learning program", "597909b4-677f-4319-ace8-885111ba0110": "We formalize the problem of reinforcement learning using ideas from dynamical systems theory, speci\ufb01cally, as the optimal control of incompletely-known Markov decision processes.\n\nThe details of this formalization must wait until Chapter 3, but the basic idea is simply to capture the most important aspects of the real problem facing a learning agent interacting over time with its environment to achieve a goal. A learning agent must be able to sense the state of its environment to some extent and must be able to take actions that a\u21b5ect the state. The agent also must have a goal or goals relating to the state of the environment. Markov decision processes are intended to include just these three aspects\u2014sensation, action, and goal\u2014in their simplest possible forms without trivializing any of them. Any method that is well suited to solving such problems we consider to be a reinforcement learning method. Reinforcement learning is di\u21b5erent from supervised learning, the kind of learning studied in most current research in the \ufb01eld of machine learning. Supervised learning is learning from a training set of labeled examples provided by a knowledgable external supervisor", "585768f9-295a-418b-81d0-78b59c76a8ff": "Traditionally, people refer to regression, classification  p(y | x) =  and structured output problems as supervised learning. Density estimation in  https://www.deeplearningbook.org/contents/ml.html    support of other tasks 1s uSually considered unsupervised learning. Other variants of the learning paradigm are possible. For example, in semi- supervised learning, some examples include a supervision target but others do not. In multi-instance learning. an entire collection of examples is labeled as  containing or not containing an example of a class, but the individual members of the collection are not labeled. For a recent example of multi-instance learning with deep models, see Kotzias et al. Some machine learning algorithms do not just experience a fixed dataset. For example, reinforcement learning algorithms interact with an environment, so there is a feedback loop between the learning system and its experiences. Such algorithms are beyond the scope of this book. Please see Sutton and Barto  or Bertsekas and Tsitsiklis  for information about reinforcement learning, and Mnih et al. for the deep learning approach to reinforcement learning", "69236cb3-5abb-441d-9c3e-9c0b5dff6f0e": "In Bishop and James , statistical machine learning techniques were used to predict the volume fractions and also the geometrical con\ufb01guration of the phases shown in Figure A.2, from the 12-dimensional vector of measurements. The 12dimensional observation vectors can also be used to test data visualization algorithms. This data set has a rich and interesting structure, as follows. For any given con\ufb01guration there are two degrees of freedom corresponding to the fractions of oil and water, and so for in\ufb01nite integration time the data will locally live on a twodimensional manifold.\n\nFor a \ufb01nite integration time, the individual data points will be perturbed away from the manifold by the photon noise. In the homogeneous phase con\ufb01guration, the path lengths in oil and water are linearly related to the fractions of oil and water, and so the data points lie close to a linear manifold. For the annular con\ufb01guration, the relationship between phase fraction and path length is nonlinear and so the manifold will be nonlinear. In the case of the laminar con\ufb01guration the situation is even more complex because small variations in the phase fractions can cause one of the horizontal phase boundaries to move across one of the horizontal beam lines leading to a discontinuous jump in the 12-dimensional observation space", "604324ea-34b9-46ac-8b8b-9fb2752bac73": "For example, for rectifiers, how often are they off? Are there units that are always off? For tanh units, the average of the absolute value of the preactivations tells us how saturated the unit is. In a deep network where the propagated gradients quickly grow or quickly vanish, optimization may be hampered. Finally, it is useful to compare the magnitude of parameter gradients to the magnitude of the parameters themselves.\n\nAs suggested by Bottou , we would like the magnitude of parameter updates over a minibatch to represent something like 1 percent of the magnitude of the parameter, not 50 percent or 0.001 percent (which would make the parameters move too slowly). It may be that some groups of parameters are moving at a good pace while others are stalled. When the data is sparse (like in natural language),  434  CHAPTER 11. PRACTICAL METHODOLOGY  some parameters may be very rarely updated, and this should be kept in mind when monitoring their evolution. Finally, many deep learning algorithms provide some sort of guarantee about the results produced at each step. For example, in part III, we will see some approx- imate inference algorithms that work by using algebraic solutions to optimization problems", "537562b7-008f-4085-8bed-3fdf300dada1": "We see that (3.57) involves the convolution of two Gaussian distributions, and so making use of the result (2.115) from Section 8.1.4, we see that the predictive distribution takes the form Exercise 3.10 The \ufb01rst term in (3.59) represents the noise on the data whereas the second term re\ufb02ects the uncertainty associated with the parameters w. Because the noise process and the distribution of w are independent Gaussians, their variances are additive. Note that, as additional data points are observed, the posterior distribution becomes narrower. As a consequence it can be shown  that \u03c32 N+1(x) \u2a7d \u03c32 N(x). In the limit N \u2192 \u221e, the second term in (3.59) goes to zero, and the variance Exercise 3.11 of the predictive distribution arises solely from the additive noise governed by the parameter \u03b2. As an illustration of the predictive distribution for Bayesian linear regression models, let us return to the synthetic sinusoidal data set of Section 1.1.\n\nIn Figure 3.8, of the form (3.4) using the synthetic sinusoidal data set of Section 1.1. See the text for a detailed discussion", "c07dd5d7-13ac-4e46-99e5-9832dd12d984": "Advances in Neural Information Processing Systems, 33. Ziang Xie, Sida I. Wang, Jiwei Li, Daniel L\u00b4evy, Aiming Nie, Dan Jurafsky, and Andrew Y. Ng. 2017. Data noising as smoothing in neural network language models. CoRR, abs/1703.02573. Jingjing Xu, Xuancheng Ren, Junyang Lin, and Xu Sun. 2018. Dp-gan: Diversity-promoting generative adversarial network for generating informative and diversi\ufb01ed text. Weidi Xu, Haoze Sun, Chao Deng, and Ying Tan. 2017. Variational autoencoder for semi-supervised text classi\ufb01cation. In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence, volume 31. Diyi Yang, William Yang Wang. 2015. That\u2019s so annoying!!! : A lexical and frame-semantic embedding based data augmentation approach to automatic categorization of annoying behaviors using #petpeeve tweets", "833701a7-6d98-43ef-8092-1478507fe858": "Because the size of the vocabulary rarely exceeds a million words and logs( 10\u00b0) ~ 20, it is possible to reduce np to about 20, but np, is often much larger, around 10? or more.\n\nRather than carefully optimizing a tree with a branching factor of 2, one can instead define a tree with depth two and a branching factor of JV . Such a tree corresponds to simply defining a set of mutually exclusive word classes. The simple approach based on a tree of depth two captures most of the computational benefit of the hierarchical strategy. One question that remains somewhat open is how to best define these word classes, or how to define the word hierarchy in general. Early work used existing hierarchies , but the hierarchy can also be learned, ideally jointly with the neural language model. Learning the hierarchy is difficult. An exact optimization of the log-likelihood appears intractable because the choice of a word hierarchy is a discrete one, not amenable to gradient-based optimization. However, one could use discrete optimization to approximately optimize the partition of words into word classes", "f6552b5e-cea0-4e0b-8450-d0b246cbc1d9": "They clipped the error term Rt+1 + \u03b3 maxa \u02dcq(St+1, a, wt) \u2212 \u02c6q(St, At, wt) so that it remained in the interval . Mnih et al. conducted a large number of learning runs on 5 of the games to gain insight into the e\u21b5ect that various of DQN\u2019s design features had on its performance. They ran DQN with the four combinations of experience replay and the duplicate target network being included or not included. Although the results varied from game to game, each of these features alone signi\ufb01cantly improved performance, and very dramatically improved performance when used together. Mnih et al. also studied the role played by the deep convolutional ANN in DQN\u2019s learning ability by comparing the deep convolutional version of DQN with a version having a network of just one linear layer, both receiving the same stacked preprocessed video frames. Here, the improvement of the deep convolutional version over the linear version was particularly striking across all 5 of the test games.\n\nCreating arti\ufb01cial agents that excel over a diverse collection of challenging tasks has been an enduring goal of arti\ufb01cial intelligence", "0fb28a34-1034-4ff1-864d-9c0c576c1f5a": "Table 6 compares our results with previous approaches (Zhuang et al., 2019; He et al., 2019; Misra & van der Maaten, 2019; H\u00e9naff et al., 2019; Kolesnikov et al., 2019; Donahue & Simonyan, 2019; Bachman et al., A Simple Framework for Contrastive Learning of Visual Representations Food CIFAR10 CIFAR100 Birdsnap SUN397 Cars Aircraft VOC2007 DTD Pets Caltech-101 Flowers 2019; Tian et al., 2019) in the linear evaluation setting (see Appendix B.6). Table 1 shows more numerical comparisons among different methods. We are able to use standard networks to obtain substantially better results compared to previous methods that require speci\ufb01cally designed architectures. The best result obtained with our ResNet-50 (4\u00d7) can match the supervised pretrained ResNet-50. Semi-supervised learning. We follow Zhai et al.\n\nand sample 1% or 10% of the labeled ILSVRC-12 training datasets in a class-balanced way (\u223c12.8 and \u223c128 images per class respectively)", "3d30cca1-33e8-431f-8d6c-871b678bcb75": "Linear options. In Proceedings of the 9th International Conference on Sorg, J., Singh, S., Lewis, R. Internal rewards mitigate agent boundedness. In Proceedings of the 27th International Conference on Machine Learning , pp. 1007\u20131014. Spence, K. W. The role of secondary reinforcement in delayed reward learning. PsychoSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R. Dropout: A simple way to prevent neural networks from over\ufb01tting. Journal of Machine Learning Research, 15(1):1929\u20131958. Staddon, J. E. R. Adaptive Behavior and Learning. Cambridge University Press. Stan\ufb01ll, C., Waltz, D. Toward memory-based reasoning. Communications of the ACM, Steinberg, E. E., Kei\ufb02in, R., Boivin, J. R., Witten, I", "3929232a-d1eb-4bf7-a809-024c6dc11411": "is a V\u00b7dimensi\"nal ,ero-mean Gau..ian-distributed noi.., \"ariable witb co'-ariance ,,21. This generative process is illustrated in Figure 12.9. NOIe that this frame\".-orl< is based on a mapping from latent ,pace 10 data space.\n\nin contrast 10 the nl()l'(: C(\"\"'cnti,,,,\"1 \"iew \"f I'CA dis.cus\"'d alx\",e, 11Ie \",,'e= mapping, from data space to the latent space. ,,-ill he OOlained ,honly using Haycs\u00b7 lhwn:m. SUf!ll'OSC we wish 10 deten\"ine the \"alues ofll>o parameters \\V", "2c933da8-7cc8-4fc1-8b30-5131f7bfc1aa": "The forward recursions, analogous to the \u03b1 messages of the hidden Markov model, are known as the Kalman \ufb01lter equations , and the backward recursions, analogous to the \u03b2 messages, are known as the Kalman smoother equations, or the Rauch-Tung-Striebel (RTS) equations . The Kalman \ufb01lter is widely used in many real-time tracking applications. Because the linear dynamical system is a linear-Gaussian model, the joint distribution over all variables, as well as all marginals and conditionals, will be Gaussian. It follows that the sequence of individually most probable latent variable values is the same as the most probable latent sequence. There is thus no need to consider the Exercise 13.19 analogue of the Viterbi algorithm for the linear dynamical system. Because the model has linear-Gaussian conditional distributions, we can write the transition and emission distributions in the general form The initial latent variable also has a Gaussian distribution which we write as Note that in order to simplify the notation, we have omitted additive constant terms from the means of the Gaussians.\n\nIn fact, it is straightforward to include them if desired", "6923f87d-173b-4292-9cc3-994eb460258a": "Slight rotations such as between 1 and 20 or \u20141 to \u201420 could be useful on digit recognition tasks such as MNIST, but as the rotation  degree increases, the label of the data is no longer preserved post-transformation. Translation  Shifting images left, right, up, or down can be a very useful transformation to avoid positional bias in the data.\n\nFor example, if all the images in a dataset are centered, which is common in face recognition datasets, this would require the model to be tested on perfectly centered images as well. As the original image is translated in a direction, the remaining space can be filled with either a constant value such as 0 s or 255 s, or it can be filled with random or Gaussian noise. This padding preserves the  spatial dimensions of the image post-augmentation. Noise injection Noise injection consists of injecting a matrix of random values usually drawn from a Gaussian distribution. Noise injection is tested by Moreno-Barea et al. on nine datasets from the UCI repository . Adding noise to images can help CNNs learn more robust features. Geometric transformations are very good solutions for positional biases present in  the training data", "ff40943b-61fe-44a0-83b7-b6d970b7744c": "Because the logistic regression model de\ufb01nes a conditional distribution for the target variable, given the input vector, it is straightforward to use it as the component distribution in a mixture model, thereby giving rise to a richer family of conditional distributions compared to a single logistic regression model. This example involves a straightforward combination of ideas encountered in earlier sections of the book and will help consolidate these for the reader. The conditional distribution of the target variable, for a probabilistic mixture of K logistic regression models, is given by denotes the adjustable parameters namely {\u03c0k} and {wk}. Now suppose we are given a data set {\u03c6n, tn}. The corresponding likelihood where ynk = \u03c3(wT k \u03c6n) and t = (t1, . , tN)T. We can maximize this likelihood function iteratively by making use of the EM algorithm.\n\nThis involves introducing latent variables znk that correspond to a 1-of-K coded binary indicator variable for each data point n. The complete-data likelihood function is then given by where Z is the matrix of latent variables with elements znk", "0b61e802-6c81-4e4e-82f1-4123d02f214a": "could convincingly argue on the basis of their simulation results that a memory controller that learns online via reinforcement learning has the potential to improve performance to levels that would otherwise require more complex and more expensive memory systems, while removing from human designers some of the burden required to manually design e\ufb03cient scheduling policies. Mukundan and Mart\u00b4\u0131nez  took this project forward by investigating learning controllers with additional actions, other performance criteria, and more complex reward functions derived using genetic algorithms. They considered additional performance criteria related to energy e\ufb03ciency. The results of these studies surpassed the earlier results described above and signi\ufb01cantly surpassed the 2012 state-of-the-art for all of the performance criteria they considered. The approach is especially promising for developing sophisticated power-aware DRAM interfaces. One of the greatest challenges in applying reinforcement learning to real-world problems is deciding how to represent and store value functions and/or policies. Unless the state set is \ufb01nite and small enough to allow exhaustive representation by a lookup table\u2014as in many of our illustrative examples\u2014one must use a parameterized function approximation scheme.\n\nWhether linear or nonlinear, function approximation relies on features that have to be readily accessible to the learning system and able to convey the information necessary for skilled performance", "d9192a04-c5a9-4a97-b0dd-821fc6fc12cb": "The agent always learns to maximize its reward. If we want it to do something for us, we must provide rewards to it in such a way that in maximizing them the agent will also achieve our goals. It is thus critical that the rewards we set up truly indicate what we want accomplished. In particular, the reward signal is not the place to impart to the agent prior knowledge about how to achieve what we want it to do.5 For example, a chess-playing agent should be rewarded only for actually winning, not for achieving subgoals such as taking its opponent\u2019s pieces or gaining control of the center of the board. If achieving these sorts of subgoals were rewarded, then the agent might \ufb01nd a way to achieve them without achieving the real goal. For example, it might \ufb01nd a way to take the opponent\u2019s pieces even at the cost of losing the game. The reward signal is your way of communicating to the robot what you want it to achieve, not how you want it achieved.6 So far we have discussed the objective of learning informally. We have said that the agent\u2019s goal is to maximize the cumulative reward it receives in the long run", "800fc4a3-2bc0-42ad-825b-0659410fcdb5": "We see that the only factors in q(x) that change when we update \ufffdfb(x2, x3) are those that involve the variables in fb namely x2 and x3.\n\nTo obtain the re\ufb01ned factor \ufffdfb(x2, x3) = \ufffdfb2(x2)\ufffdfb3(x3) we simply divide qnew(x) by q\\b(x), which gives These are precisely the messages obtained using belief propagation in which mesSection 8.4.4 sages from variable nodes to factor nodes have been folded into the messages from factor nodes to variable nodes. In particular, \ufffdfb2(x2) corresponds to the message \u00b5fb\u2192x2(x2) sent by factor node fb to variable node x2 and is given by (8.81). Similarly, if we substitute (8.78) into (8.79), we obtain (10.235) in which \ufffdfa2(x2) corresponds to \u00b5fa\u2192x2(x2) and \ufffdfc2(x2) corresponds to \u00b5fc\u2192x2(x2), giving the message \ufffdfb3(x3) which corresponds to \u00b5fb\u2192x3(x3)", "0ba27b5a-a679-4ec4-b388-d480a3d04586": "Consider a general directed graph in which A, B, and C are arbitrary nonintersecting sets of nodes (whose union may be smaller than the complete set of nodes in the graph).\n\nWe wish to ascertain whether a particular conditional independence statement A \u22a5\u22a5 B | C is implied by a given directed acyclic graph. To do so, we consider all possible paths from any node in A to any node in B. Any such path is said to be blocked if it includes a node such that either (a) the arrows on the path meet either head-to-tail or tail-to-tail at the node, and the node is in the set C, or (b) the arrows meet head-to-head at the node, and neither the node, nor any of its descendants, is in the set C. If all paths are blocked, then A is said to be d-separated from B by C, and the joint distribution over all of the variables in the graph will satisfy A \u22a5\u22a5 B | C. The concept of d-separation is illustrated in Figure 8.22", "460e6d84-f210-47c5-8122-8b16fef74699": "arXiv preprint arXiv:1611.03852, 2016. Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of computer and system sciences, 55(1):119\u2013139, 1997. K. Ganchev, J. Gillenwater, B. Taskar, et al. Posterior regularization for structured latent variable models. Journal of Machine Learning Research, 11(Jul):2001\u20132049, 2010. P. K. B. Giridhara, M. Chinmaya, R. K. M. Venkataramana, S. S. Bukhari, and A. Dengel. A study of various text augmentation techniques for relation classi\ufb01cation in free text. In ICPRAM, 2019.\n\nI. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D", "f783a300-e78c-419f-9e09-a1eeeb7d69bb": "Pseudolikelihood has a much greater cost per gradient step than SML, due to its explicit computation of all the conditionals. But generalized pseudolikelihood and similar criteria can still perform well if only one randomly selected condi- tional is computed per example , thereby bringing the computational cost down to match that of SML. Though the pseudolikelihood estimator does not explicitly minimize log Z, it can still be thought of as having something resembling a negative phase. The denominators of each conditional distribution result in the learning algorithm suppressing the probability of all states that have only one variable differing from a training example. See Marlin and de Freitas  for a theoretical analysis of the asymptotic efficiency of pseudolikelihood. 18.4 Score Matching and Ratio Matching  Score matching  provides another consistent means of training a model without estimating Z or its derivatives", "a35ae79a-920d-4943-a091-084b393135e6": "For each task, we simply plug in the taskspeci\ufb01c inputs and outputs into BERT and \ufb01netune all the parameters end-to-end. At the input, sentence A and sentence B from pre-training are analogous to (1) sentence pairs in paraphrasing, (2) hypothesis-premise pairs in entailment, (3) question-passage pairs in question answering, and (4) a degenerate text-\u2205 pair in text classi\ufb01cation or sequence tagging. At the output, the token representations are fed into an output layer for tokenlevel tasks, such as sequence tagging or question answering, and the  representation is fed into an output layer for classi\ufb01cation, such as entailment or sentiment analysis. Compared to pre-training, \ufb01ne-tuning is relatively inexpensive. All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model.7 We describe the task-speci\ufb01c details in the corresponding subsections of Section 4. More details can be found in Appendix A.5", "e8a65afc-3f76-4e35-a19e-f031b1525895": "2.44 (\u22c6 \u22c6) Consider a univariate Gaussian distribution N(x|\u00b5, \u03c4 \u22121) having conjugate Gaussian-gamma prior given by (2.154), and a data set x = {x1, . , xN} of i.i.d. observations.\n\nShow that the posterior distribution is also a Gaussian-gamma distribution of the same functional form as the prior, and write down expressions for the parameters of this posterior distribution. 2.48 (\u22c6) By following analogous steps to those used to derive the univariate Student\u2019s t-distribution (2.159), verify the result (2.162) for the multivariate form of the Student\u2019s t-distribution, by marginalizing over the variable \u03b7 in (2.161). Using the de\ufb01nition (2.161), show by exchanging integration variables that the multivariate t-distribution is correctly normalized", "5a3cb206-e6c0-407a-b34e-1f0f69364714": "AUTOENCODERS  A one-dimensional example is illustrated in figure 14.7, showing that, by making the reconstruction function insensitive to perturbations of the input around the data points, we cause the autoencoder to recover the manifold structure. To understand why autoencoders are useful for manifold learning, it is in- structive to compare them to other approaches. What is most commonly learned o characterize a manifold is a representation of the data points on (or near) he manifold.\n\nSuch a representation for a particular example is also called its embedding. It is typically given by a low-dimensional vector, with fewer dimensions han the \u201cambient\u201d space of which the manifold is a low-dimensional subset. Some algorithms (nonparametric manifold learning algorithms, discussed below) directly earn an embedding for each training example, while others learn a more general mapping, sometimes called an encoder, or representation function, that maps any point in the ambient space (the input space) to its embedding", "1d214e8f-0bc7-4a6b-a5c0-6b49b091f710": "Once this message propagation is complete, we can then propagate messages from the root node out to the leaf nodes, and these are given by From the leaf nodes x1 and x4 towards the root node x3. (b) From the root node towards the leaf nodes. One message has now passed in each direction across each link, and we can now evaluate the marginals. As a simple check, let us verify that the marginal p(x2) is given by the correct expression. Using (8.63) and substituting for the messages using the above results, we have So far, we have assumed that all of the variables in the graph are hidden.\n\nIn most practical applications, a subset of the variables will be observed, and we wish to calculate posterior distributions conditioned on these observations. Observed nodes are easily handled within the sum-product algorithm as follows. Suppose we partition x into hidden variables h and observed variables v, and that the observed value of v is denoted \ufffdv. Then we simply multiply the joint distribution p(x) by \ufffd where I(v,\ufffdv) = 1 if v = \ufffdv and I(v,\ufffdv) = 0 otherwise", "6bd1d71d-1622-4737-b0bd-a512810cec9d": "To \ufb01nd an invariant measure of the mean, we note that the observations can be viewed as points on the unit circle and can therefore be described instead by two-dimensional unit vectors x1, . , xN where \u2225xn\u2225 = 1 for n = 1, . , N, as illustrated in Figure 2.17. We can average the vectors {xn} and then \ufb01nd the corresponding angle \u03b8 of this average. Clearly, this de\ufb01nition will ensure that the location of the mean is independent of the origin of the angular coordinate. Note that x will typically lie inside the unit circle. The Cartesian coordinates of the observations are given by xn = (cos \u03b8n, sin \u03b8n), and we can write the Cartesian coordinates of the sample mean in the form x = (r cos \u03b8, r sin \u03b8).\n\nSubstituting into (2.167) and equating the x1 and x2 components then gives Taking the ratio, and using the identity tan \u03b8 = sin \u03b8/ cos \u03b8, we can solve for \u03b8 to give Shortly, we shall see how this result arises naturally as the maximum likelihood estimator for an appropriately de\ufb01ned distribution over a periodic variable", "bf87b07e-cc7a-44f1-a081-938186e1d16a": "Chapter 6  Deep Feedforward Networks  Deep feedforward networks, also called feedforward neural networks, or multilayer perceptrons (MLPs), are the quintessential deep learning models. The goal of a feedforward network is to approximate some function f*. For example, for a classifier, y = f*(a) maps an input x to a category y. A feedforward network defines a mapping y = f(a; @) and learns the value of the parameters @ that result in the best function approximation. These models are called feedforward because information flows through the function being evaluated from a, through the intermediate computations used to define f, and finally to the output y. There are no feedback connections in which outputs of the model are fed back into itself. When feedforward neural networks are extended to include feedback connections, they are called recurrent neural networks, as presented in chapter 10. Feedforward networks are of extreme importance to machine learning practi- tioners. They form the basis of many important commercial applications. For example, the convolutional networks used for object recognition from photos are a specialized kind of feedforward network", "062f5c56-ed20-4ed2-95f7-9c820bc9e67f": "The images should be standardized so that their pixels all lie in the same reasonable range, like  or . Mixing images that lie in  with images that lie in  will usually result in failure. Formatting images to have the same scale is the only kind of preprocessing that is strictly necessary.\n\nMany computer vision architectures require images of a standard size, so images must be cropped or scaled to fit that size. Even this rescaling is not always strictly necessary. Some convolutional models accept variably sized inputs and dynamically adjust the size of their pooling regions to keep the output size constant . Other convolutional models have variably sized output that automatically scales in size with the input, such as models that denoise or label each pixel in an image . Dataset augmentation may be seen as a way of preprocessing the training set only. Dataset augmentation is an excellent way to reduce the generalization error of most computer vision models. A related idea applicable at test time is to show the model many different versions of the same input (for example, the same image cropped at slightly different locations) and have the different instantiations of the model vote to determine the output", "8fa4efca-3f2b-4b83-8d69-9f43a45f6cb2": "It tells us the functional form that the optimal solution will take, whether we arrive there by fixed-point equations or not. This means we can take the functional form from that equation but regard some of the values that appear in it as parameters, which we can optimize with any optimization algorithm we like. As an example, consider a simple probabilistic model, with latent variables h \u00a2 R and just one visible variable, v. Suppose that p(h) = N(h;0,I) and p(v | h) = N(v;w'h;1).\n\nWe could actually simplify this model by integrating out h; the result is just a Gaussian distribution over v. The model itself is not interesting; we have constructed it only to provide a simple demonstration of how calculus of variations can be applied to probabilistic modeling. The true posterior is given, up to a normalizing constant, by p(h |v) (19.57) 646  CHAPTER 19. APPROXIMATE INFERENCE  op(h, v) (19.58) = p(hi)p(ha)p(v | h) (19.59) exp (-3  )", "fa079311-6621-47c9-be4e-6a115bd0662e": "To achieve this, any of a broad range of existing methods for supervised-learning function approximation can be used simply by treating each update as a training example. Perhaps the most suitable supervised learning methods are those using parameterized function approximation, in which the policy is parameterized by a weight vector w. Although the weight vector has many components, the state space is much larger still, and we must settle for an approximate solution.\n\nWe de\ufb01ned the mean squared value error, VE(w), as a measure of the error in the values v\u21e1w(s) for a weight vector w under the on-policy distribution, \u00b5. The VE gives us a clear way to rank di\u21b5erent value-function approximations in the on-policy case. To \ufb01nd a good weight vector, the most popular methods are variations of stochastic gradient descent (SGD). In this chapter we have focused on the on-policy case with a \ufb01xed policy, also known as policy evaluation or prediction; a natural learning algorithm for this case is n-step semi-gradient TD, which includes gradient Monte Carlo and semi-gradient TD(0) algorithms as the special cases when n=1 and n=1 respectively", "bc650f08-ec32-4048-9473-a31701a0a135": "Note that the max-sum algorithm works with log probabilities and so there is no need to use re-scaled variables as was done with the forward-backward algorithm. Figure 13.16 shows a fragment of the hidden Markov model expanded as lattice diagram. As we have already noted, the number of possible paths through the lattice grows exponentially with the length of the chain. The Viterbi algorithm searches this space of paths ef\ufb01ciently to \ufb01nd the most probable path with a computational cost that grows only linearly with the length of the chain. As with the sum-product algorithm, we \ufb01rst represent the hidden Markov model as a factor graph, as shown in Figure 13.15. Again, we treat the variable node zN as the root, and pass messages to the root starting with the leaf nodes. Using the results (8.93) and (8.94), we see that the messages passed in the max-sum algorithm are given by showing two possible paths.\n\nThe Viterbi algorithm ef\ufb01ciently determines the most probable path from amongst the exponentially many possibilities", "3ee9ec50-bee4-4e15-90c6-1115888904fd": "First note that, for any episode ending with the right action, the importance sampling ratio is zero, because the target policy would never take this action; these episodes thus contribute nothing to the expectation (the quantity in parenthesis will be zero) and can be ignored. We need only consider episodes that involve some number (possibly zero) of left actions that transition back to the nonterminal state, followed by a left action transitioning to termination.\n\nAll of these episodes have a return of 1, so the G0 factor can be ignored. To get the expected square we need only consider each length of episode, multiplying the probability of the episode\u2019s occurrence by the square of its importance-sampling ratio, and add these up: Exercise 5.7 In learning curves such as those shown in Figure 5.3 error generally decreases with training, as indeed happened for the ordinary importance-sampling method. But for the weighted importance-sampling method error \ufb01rst increased and then decreased. Why do you think this happened? \u21e4 Exercise 5.8 The results with Example 5.5 and shown in Figure 5.4 used a \ufb01rst-visit MC method. Suppose that instead an every-visit MC method was used on the same problem", "a7ef4409-cc9f-4dc5-9634-a84d368c68b0": "Formally, a matrix D is diagonal if and only if D;,; = 0 for all i 4 7.\n\nWe have already seen one example of a diagonal matrix: the identity  38  CHAPTER 2. LINEAR ALGEBRA  matrix, where all the diagonal entries are 1. We write diag(v) to denote a square diagonal matrix whose diagonal entries are given by the entries of the vector v. Diagonal matrices are of interest in part because multiplying by a diagonal matrix is computationally efficient. To compute diag(v)a, we only need to scale each  https://www.deeplearningbook.org/contents/linear_algebra.html    element 2 by v'. In other words, diag(v)# = v \u00a9 a. Inverting a square diagonal matrix is also efficient. The, inverse exists only if every diagonal entry is nonzero, and in that case, diag(v) *\u00b0 = diag({L/v1,..\u00b0, L/onl)", "1f359a21-f3b8-40e9-91cd-46bd73b2492d": "Although the resulting integral over w is no longer analytically tractable, it might be thought that approximating this integral, for example using the Laplace approximation discussed (Section 4.4) which is based on a local Gaussian approximation centred on the mode of the posterior distribution, might provide a practical alternative to the evidence framework .\n\nHowever, the integrand as a function of w typically has a strongly skewed mode so that the Laplace approximation fails to capture the bulk of the probability mass, leading to poorer results than those obtained by maximizing the evidence . Returning to the evidence framework, we note that there are two approaches that we can take to the maximization of the log evidence. We can evaluate the evidence function analytically and then set its derivative equal to zero to obtain re-estimation equations for \u03b1 and \u03b2, which we shall do in Section 3.5.2. Alternatively we use a technique called the expectation maximization (EM) algorithm, which will be discussed in Section 9.3.4 where we shall also show that these two approaches converge to the same solution", "54caf014-de61-4acc-a33d-2c319b7e7c5c": "Examples include:  e aand b are d-separated given the empty set  e aande are d-separated given c  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    e dand e are d-separated given c We can also see that some variables are no longer d-separated when we observe some variables:  e aand b are not d-separated given c  e aand b are not d-separated given d  572  CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  Figure 16.10: Examples of complete graphs, which can describe any probability distribution. Here we show examples with four random variables. (Left) The complete undirected graph. In the undirected case, the complete graph is unique. (Right) A complete directed graph. In the directed case, there is not a unique complete graph.\n\nWe choose an ordering of the variables and draw an arc from each variable to every variable that comes after it in the ordering", "83935891-30f2-4dd8-b8db-d4ab61ef3f52": "Threads are divided into small groups called warps. Each thread in a warp executes the same instruction during each cycle, so if different threads within the same warp need to execute different code paths, these different code paths must be traversed sequentially rather than in parallel. Because of the difficulty of writing high-performance GPU code, researchers should structure their workflow to avoid needing to write new GPU code to test new models or algorithms. Typically, one can do this by building a software library of high-performance operations like convolution and matrix multiplication, then specifying models in terms of calls to this library of operations. For example, the machine learning library Pylearn2  specifies all its  https://www.deeplearningbook.org/contents/applications.html    machine learning algorithms in terms of calls to Theano  and cuda-convnet , which provide these high-performance operations. This factored approach can also ease support for multiple kinds of hardware. For example, the same Theano program can run on either CPU or GPU, without needing to change any of the calls to Theano itself. Other libraries like TensorFlow  and Torch (Collobert et ai.,  201 1b) provide similar features", "ba165fe0-b6c6-4b32-879a-f64c6170803c": "APV-MCTS\u2019s rollouts in AlphaGo were simulated games with both players using a fast rollout policy provided by a simple linear network, also trained by supervised learning before play. Throughout its execution, APV-MCTS kept track of how many simulations passed through each edge of the search tree, and when its execution completed, the most-visited edge from the root node was selected as the action to take, here the move AlphaGo actually made in a game. The value network had the same structure as the deep convolutional SL policy network except that it had a single output unit that gave estimated values of game positions instead of the SL policy network\u2019s probability distributions over legal actions. Ideally, the value network would output optimal state values, and it might have been possible to approximate the optimal value function along the lines of TD-Gammon described above: self-play with nonlinear TD(\u03bb) coupled to a deep convolutional ANN. But the DeepMind team took a di\u21b5erent approach that held more promise for a game as complex as Go. They divided the process of training the value network into two stages.\n\nIn the \ufb01rst stage, they created the best policy they could by using reinforcement learning to train an RL policy network", "4a4db636-a2fc-4b35-b4a9-6cf47ca1541e": "Casella . Monte Carlo Statistical Methods. Springer. Roth, V. and V. Steinhage . Nonlinear discriminant analysis using kernel functions. In S. A. Solla, T. K. Leen, and K. R. M\u00a8uller (Eds. ), Advances in Neural Information Processing Systems, Volume 12. MIT Press. Roweis, S. EM algorithms for PCA and SPCA. In M. I. Jordan, M. J. Kearns, and S. A. Solla (Eds. ), Advances in Neural Information Processing Systems, Volume 10, pp. 626\u2013632. MIT Press. Roweis, S. and Z. Ghahramani . A unifying review of linear Gaussian models. Neural Computation 11(2), 305\u2013345. Roweis, S", "28bc466e-27d5-4422-9aff-57d5c2adf297": "Verify that this reduces to the reject criterion discussed in Section 1.5.3 when the loss matrix is given by Lkj = 1 \u2212 Ikj.\n\nWhat is the relationship between \u03bb and the rejection threshold \u03b8? Using the calculus of variations, show that the function y(x) for which this expected loss is minimized is given by y(x) = Et. Show that this result reduces to (1.89) for the case of a single target variable t. 1.26 (\u22c6) By expansion of the square in (1.151), derive a result analogous to (1.90) and hence show that the function y(x) that minimizes the expected squared loss for the case of a vector t of target variables is again given by the conditional expectation of t. 1.27 (\u22c6 \u22c6) www Consider the expected loss for regression problems under the Lq loss function given by (1.91). Write down the condition that y(x) must satisfy in order to minimize E. Show that, for q = 1, this solution represents the conditional median, i.e., the function y(x) such that the probability mass for t < y(x) is the same as for t \u2a7e y(x)", "0bf9459b-b14c-4b2a-9d79-9df855d3b1d1": "The total number of parameters that must be speci\ufb01ed in the joint distribution is therefore (K \u2212 1) + K(K \u2212 1) = K2 \u2212 1 as before. Now suppose that the variables x1 and x2 were independent, corresponding to the graphical model shown in Figure 8.9(b). Each variable is then described by having K states, requires the speci\ufb01cation of K \u2212 1 + (M \u2212 1)K(K \u2212 1) parameters, which grows linearly with the length M of the chain. In contrast, a fully connected graph of M nodes would have KM \u2212 1 parameters, which grows exponentially with M. a separate multinomial distribution, and the total number of parameters would be 2(K \u2212 1). For a distribution over M independent discrete variables, each having K states, the total number of parameters would be M(K \u2212 1), which therefore grows linearly with the number of variables.\n\nFrom a graphical perspective, we have reduced the number of parameters by dropping links in the graph, at the expense of having a restricted class of distributions. More generally, if we have M discrete variables x1,", "b69c55ec-e9fe-4a2b-9538-3e6e60693347": "Because of this, all the results obtained for the parameterized case apply to partial observability without change. In this sense, the case of parameterized function approximation includes the case of partial observability. Nevertheless, there are many issues that cannot be investigated without a more explicit treatment of partial observability. Although we cannot give them a full treatment here, we can outline the changes that would be needed to do so. There are four steps. First, we would change the problem. The environment would emit not its states, but only observations\u2014signals that depend on its state but, like a robot\u2019s sensors, provide only partial information about it. For convenience, without loss of generality, we assume that the reward is a direct, known function of the observation (perhaps the observation is a vector, and the reward is one of is components). The environmental interaction would then have no explicit states or rewards, but would simply be an alternating sequence of actions At 2 A and observations Ot 2 O: Second, we can recover the idea of state as used in this book from the sequence of observations and actions", "3fc4630c-23d3-4a59-9d7d-643b24fdbb14": "Tangent propagation and dataset augmentation using manually specified transformations both require that the model be invariant to certain specified directions of change in the input. Double backprop and adversarial training both require that the model should be invariant to all directions of change in the input as long as the change is small. Just as dataset augmentation is the non-infinitesimal version of tangent propagation, adversarial training is the non-infinitesimal version of double backprop.\n\nThe manifold tangent classifier , eliminates the need to know the tangent vectors a priori. As we will see in chapter 14, autoencoders can estimate the manifold tangent vectors. The manifold tangent classifier makes use of this technique to avoid needing user-specified tangent vectors. As illustrated in figure 14.10, these estimated tangent vectors go beyond the classical invariants that arise out of the geometry of images (such as translation, rotation, and scaling) and include factors that must be learned because they are object-specific (such as  269  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  moving body parts)", "3228ab67-85a8-49c4-bd74-0ca44dc45262": ", xM, we can model the joint distribution using a directed graph with one variable corresponding to each node. The conditional distribution at each node is given by a set of nonnegative parameters subject to the usual normalization constraint. If the graph is fully connected then we have a completely general distribution having KM \u2212 1 parameters, whereas if there are no links in the graph the joint distribution factorizes into the product of the marginals, and the total number of parameters is M(K \u2212 1). Graphs having intermediate levels of connectivity allow for more general distributions than the fully factorized one while requiring fewer parameters than the general joint distribution. As an illustration, consider the chain of nodes shown in Figure 8.10. The marginal distribution p(x1) requires K \u2212 1 parameters, whereas each of the M \u2212 1 conditional distributions p(xi|xi\u22121), for i = 2, . , M, requires K(K \u2212 1) parameters. This gives a total parameter count of K \u22121+(M \u22121)K(K \u22121), which is quadratic in K and which grows linearly (rather than exponentially) with the length M of the chain", "30f50f0d-9a9a-4f5e-96bb-0ac70bea64c8": "This approach is complicated by the fact that no scaling factor can change the contrast of a zero-contrast image (one whose pixels all have equal intensity). Images with very low but nonzero contrast often have little information content. Dividing by the true standard deviation usually accomplishes nothing more than amplifying sensor noise or compression artifacts in such cases. This  449  https://www.deeplearningbook.org/contents/applications.html    Raw input GCN, A =0 GCN, A = 10-? me CFay,  ({ ye:  NL || ees  \u20141.5 0.0 1.5 \u20141.5 0.0 1.5 \u20141.5 0.0 1.5  xO zo xO  Figure 12.1: GCN maps examples onto a sphere. (Left)Raw input data may have any norm. (Center)GCN with \\ = 0 maps all nonzero examples perfectly onto a sphere. Here we use s =1ande=10-\u00ae. Because we use GCN based on normalizing the standard deviation rather than the L? norm, the resulting sphere is not the unit sphere", "69edb928-b9d6-427d-a0b6-2d384a537502": "Humans can bene\ufb01t from various experience\u2014by observing examples through reading and hearing, studying abstract de\ufb01nitions and grammar, making mistakes and getting correction from teachers, interacting with others and observing implicit feedback, and so on. Knowledge of a prior language can also accelerate the acquisition of a new one. How can we build arti\ufb01cial intelligence (AI) agents that are similarly capable of learning from all types of experience?\n\nWe refer to the capability of \ufb02exibly integrating all available experience in learning as panoramic learning. In handling di\ufb00erent experience ranging from data instances, knowledge, constraints, to rewards, adversaries, and lifelong interplay in an ever-growing spectrum of tasks, contemporary ML and AI research has resulted in a large multitude of learning paradigms (e.g., supervised, unsupervised, active, reinforcement, adversarial learning), models, optimization techniques, not mentioning countless approximation heuristics and tuning tricks, plus combinations of all the above. While pushing the \ufb01eld forward rapidly, these results also make mastering existing ML techniques very di\ufb03cult, and fall short of reusable, repeatable, and composable development of ML approaches to diverse problems with distinct available experience", "5602250b-12de-4871-8296-3a5b288341d6": "Of course there are also double versions of Sarsa and Expected Sarsa.\n\nAlgorithm parameters: step size \u21b5 2 (0, 1], small \" > 0 Initialize Q1(s, a) and Q2(s, a), for all s 2 S+, a 2 A(s), such that Q(terminal, \u00b7) = 0 In this book we try to present a uniform approach to a wide class of tasks, but of course there are always exceptional tasks that are better treated in a specialized way. For example, our general approach involves learning an action-value function, but in Chapter 1 we presented a TD method for learning to play tic-tac-toe that learned something much more like a state-value function. If we look closely at that example, it becomes apparent that the function learned there is neither an action-value function nor a state-value function in the usual sense. A conventional state-value function evaluates states in which the agent has the option of selecting an action, but the state-value function used in tic-tac-toe evaluates board positions after the agent has made its move. Let us call these afterstates, and value functions over these, afterstate value functions", "a95e0710-80f3-4096-855a-39933c32b8bc": "For the purposes of this data set, two speci\ufb01c idealizations are considered. In the annular con\ufb01guration the oil, water, and gas form concentric cylinders with the water around the outside and the gas in the centre, whereas in the homogeneous con\ufb01guration the oil, water and gas are assumed to be intimately mixed as might occur at high \ufb02ow velocities under turbulent conditions. These con\ufb01gurations are also illustrated in Figure A.2. We have seen that a single dual-energy beam gives the oil and water fractions measured along the path length, whereas we are interested in the volume fractions of oil and water. This can be addressed by using multiple dual-energy gamma densitometers whose beams pass through different regions of the pipe. For this particular data set, there are six such beams, and their spatial arrangement is shown in Figure A.3. A single observation is therefore represented by a 12-dimensional vector comprising the fractions of oil and water measured along the paths of each of the beams. We are, however, interested in obtaining the overall volume fractions of the three phases in the pipe", "1440bfa9-260c-482c-a3aa-064aa55a1246": "843-52.\n\nEsteva A, Kuprel B, Novoa RA, Ko J, Swetter SM, Blau HM, Thrun S. Dermatologist-level classification of skin cancer with deep neural networks. Nature. 2017;542:115-8. Geert L, Thijs K, Babak EB, Arnaud AAS, Francesco C, Mohsen G, Jeroen AWM, van Bram G, Clara IS. A survey on deep learning in medical image analysis. Med Image Anal. 2017;42:60-88. Joffrey LL, Taghi MK, Richard AB, Naeem S. A survey on addressing high-class imbalance in big data. Springer J Big Data. 2018;5:42. LeCun Y, Bottou L, Bengio Y, Haffner P. Gradient-based learning applied to document recognition. Proc IEEE. 1998;86(1 1):2278-324. Nitesh VC, Kevin WB, Lawrence OH, Kegelmeyer W", "9785ad1a-1d8b-47a9-8eb5-9301bc777fd6": "Because the logarithm function is monotonically increasing, the inequality A \u2a7e B implies ln A \u2a7e ln B.\n\nThis gives a lower bound on the log of the joint distribution of t and w of the form Substituting for the prior p(w), the right-hand side of this inequality becomes, as a function of w This is a quadratic function of w, and so we can obtain the corresponding variational approximation to the posterior distribution by identifying the linear and quadratic terms in w, giving a Gaussian variational posterior of the form As with the Laplace framework, we have again obtained a Gaussian approximation to the posterior distribution. However, the additional \ufb02exibility provided by the variational parameters {\u03ben} leads to improved accuracy in the approximation . Here we have considered a batch learning context in which all of the training data is available at once. However, Bayesian methods are intrinsically well suited to sequential learning in which the data points are processed one at a time and then discarded. The formulation of this variational approach for the sequential case is straightforward", "d9042201-7ccf-4a00-a0b7-497dc796d071": "One of the earliest related works is by Aleksandrov, Sysoyev, and Shemeneva . Thomas  \ufb01rst realized that the factor of \u03b3t, as speci\ufb01ed in the boxed algorithms of this chapter, was needed in the case of discounted episodic problems. 13.2 The policy gradient theorem here and on page 334 was \ufb01rst obtained by Marbach and Tsitsiklis  and then independently by Sutton et al. A similar expression was obtained by Cao and Chen .\n\nOther early results are due to Konda and Tsitsiklis , Baxter and Bartlett , and Baxter, Bartlett, and Weaver . Some additional results are developed by Sutton, Singh, and McAllester . The all-actions algorithm was \ufb01rst presented in an unpublished but widely circulated incomplete paper  and then developed further by Ciosek and Whiteson , who termed it \u201cexpected policy gradients,\u201d and by Asadi, Allen, Roderick, Mohamed, Konidaris, and Littman  who called it \u201cmean actor critic.\u201d 13.4 The baseline was introduced in Williams\u2019s  original work. Greensmith, Bartlett, and Baxter  analyzed an arguably better baseline", "13844515-1a92-42ce-9b36-6373ac840719": "thesis, University of Alberta, Hallak, A., Tamar, A., Mannor, S. Emphatic TD Bellman operator is a contraction. Hallak, A., Tamar, A., Munos, R., Mannor, S. Generalized emphatic temporal di\u21b5erence Arti\ufb01cial Intelligence (AAAI-16), pp. 1631\u20131637. AAAI Press, Menlo Park, CA. Hammer, M. The neural basis of associative reward learning in honeybees. Trends in Hampson, S. E. Connectionist Problem Solving: Computational Aspects of Biological Hare, T. A., O\u2019Doherty, J., Camerer, C. F., Schultz, W., Rangel, A. Dissociating the role of the orbitofrontal cortex and the striatum in the computation of goal values and prediction errors. The Journal of Neuroscience, 28(22):5623\u20135630. Harth, E., Tzanakou, E", "fd63d66d-6718-4e35-a198-fc105db68dd5": "MACHINE LEARNING BASICS  Training error  Underfitting zone} Overfitting zone . 6 8 \u2014 Generalization error  Error  0 Optimal Capacity Capacity  Figure 5.3: Typical relationship between capacity and error. Training and test error behave differently. At the left end of the graph, training error and generalization error are both high. This is the underfitting regime. As we increase capacity, training error decreases, but the gap between training and generalization error increases. Eventually, the size of this gap outweighs the decrease in training error, and we enter the overfitting regime, where capacity is too large, above the optimal capacity. generalization error has a U-shaped curve as a function of model capacity. This is illustrated in figure 5.3. To reach the most extreme case of arbitrarily high capacity, we introduce the concept of nonparametric models. So far, we have seen only parametric  https://www.deeplearningbook.org/contents/ml.html    models, such as linear regression.\n\nParametric models learn a function described by a parameter vector whose size is finite and fixed before any data is observed. onparametric models have no such limitation", "e9810546-b463-4043-b47a-e9923e315352": "APPLICATIONS  Input image GCN LCN  Figure 12.2: A comparison of global and local contrast normalization.\n\nVisually, the effects of global contrast normalization are subtle. It places all images on roughly the same scale, which reduces the burden on the learning algorithm to handle multiple scales. Local contrast normalization modifies the image much more, discarding all regions of constant intensity. This allows the model to focus on just the edges. Regions of fine texture, such as the houses in the second row, may lose some detail due to the bandwidth of the normalization kernel being too high. normalize each pixel . Local contrast normalization can usually be implemented efficiently by using separable convolution (see section 9.8) to compute feature maps of local means and local standard deviations, then using element-wise subtraction and element-wise division on different feature maps. Local contrast normalization is a differentiable operation and can also be used as a nonlinearity applied to the hidden layers of a network, as well as a preprocessing operation applied to the input. As with global contrast normalization, we typically need to regularize local contrast normalization to avoid division by zero", "817537d0-affa-4da2-b664-65fb0610d409": "n--n A mn nnn bine 2A wn nnn btn 4 Lee de (6-42 TNOEN AA LA  https://www.deeplearningbook.org/contents/regularization.html    JIU PLOVEU BCUCLAUZALIOM ALLU BECLICL alZavlou CLLUL VUULLUS (DaAALCL, 1ygg9V) Call VE achieved because of the shared parameters, for which statistical strength can be greatly improved (in proportion with the increased number of examples for the shared parameters, compared to the scenario of single-task models).\n\nOf course this  will happen only if some assumptions about the statistical relationship between the different tasks are valid, meaning that there is something shared across some of the tasks. From the point of view of deep learning, the underlying prior belief is the following: among the factors that explain the variations observed in the data associated with the different tasks, some are shared across two or more tasks. 7.8 Early Stopping  When training large models with sufficient representational capacity to overfit the task, we often observe that training error decreases steadily over time, but  241  CHAPTER 7", "b4fd2b81-3a4e-4b89-89d9-9f0e3aa3722c": "Thomas and Brunskill  argue that an action-dependent baseline can be used without incurring bias. 13.5\u20136 Actor\u2013critic methods were among the earliest to be investigated in reinforcement learning . The algorithms presented here are based on the work of Degris, White, and Sutton . Actor\u2013critic methods are sometimes referred to as advantage actor\u2013critic (\u201cA2C\u201d) methods in the literature. In this last part of the book we look beyond the standard reinforcement learning ideas presented in the \ufb01rst two parts of the book to brie\ufb02y survey their relationships with psychology and neuroscience, a sampling of reinforcement learning applications, and some of the active frontiers for future reinforcement learning research. In previous chapters we developed ideas for algorithms based on computational considerations alone.\n\nIn this chapter we look at some of these algorithms from another perspective: the perspective of psychology and its study of how animals learn. The goals of this chapter are, \ufb01rst, to discuss ways that reinforcement learning ideas and algorithms correspond to what psychologists have discovered about animal learning, and second, to explain the in\ufb02uence reinforcement learning is having on the study of animal learning", "110d07c3-5b11-4a7b-a7df-e30d9db648a8": "There is one important result that is common to all such criteria, which is worth emphasizing.\n\nWe \ufb01rst note from (4.46) that SB is composed of the sum of K matrices, each of which is an outer product of two vectors and therefore of rank 1. In addition, only (K \u2212 1) of these matrices are independent as a result of the constraint (4.44). Thus, SB has rank at most equal to (K \u2212 1) and so there are at most (K \u2212 1) nonzero eigenvalues. This shows that the projection onto the (K \u2212 1)-dimensional subspace spanned by the eigenvectors of SB does not alter the value of J(w), and so we are therefore unable to \ufb01nd more than (K \u2212 1) linear \u2018features\u2019 by this means . Another example of a linear discriminant model is the perceptron of Rosenblatt , which occupies an important place in the history of pattern recognition algorithms", "46bc3b0e-cfaf-4f56-89d0-11e6af2bdf71": "SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  sub-RNN that moves forward through time and g\u00ae standing for the state of the sub-RNN that moves backward through time.\n\nThis allows the output units o)  https://www.deeplearningbook.org/contents/rnn.html    to compute a representation that depends on both the past and the future but is most sensitive to the input values around time \u00a2, without, having to specify a fixed-size window around \u00a3 (as one would have to do with a feedforward network, a convolutional network, or a regular RNN with a fixed-size look-ahead buffer). This idea can be naturally extended to two-dimensional input, such as images, by having four RNNs, each one going in one of the four directions: up, down, left, right. At each point (i,j) of a 2-D grid, an output O;, could then compute a representation that would capture mostly local information but could also depend on long-range inputs, if the RNN is able to learn to carry that information", "32001c5c-a0fa-4eef-b4f5-320d6abeb5da": "Chapter 13  Linear Factor Models  Many of the research frontiers in deep learning involve building a probabilistic model of the input, Pmoge(Z). Such a model can, in principle, use probabilis- tic inference to predict any of the variables in its environment given any of the other variables. Many of these models also have latent variables h, with Pmodel(Z) = Enpmodei(x | h). These latent variables provide another means of rep- resenting the data. Distributed representations based on latent variables can obtain all the advantages of representation learning that we have seen with deep  feedforward and recurrent networks. In this chapter, we describe some of the simplest probabilistic models with latent variables: linear factor models. These models are sometimes used as building blocks of mixture models  or of larger, deep probabilistic models . They also show many of the basic approaches necessary to building generative models that the more advanced deep models will extend further. A linear factor model is defined by the use of a stochastic linear decoder function that generates a by adding noise to a linear transformation of h.  These models are interesting because they allow us to discover explanatory factors that have a simple joint distribution", "6516db42-b841-4728-808b-e9a47cc87761": "For example, let T(x\u2217; \u03b8) be a feature vector of data instance x\u2217 \u2208 D, the constraint posterior set Q can be de\ufb01ned as: Max-margin constraint is another expectation constraint that has shown to be widely e\ufb00ective in classi\ufb01cation and regression . The maximum entropy discrimination (MED) by Jaakkola et al. regularizes linear regression models with the max-margin constraints, which is latter generalized to more complex models p(x|\u03b8), such as Markov networks  and latent variable models . Formally, let y\u2217 \u2208 R be the observed label associated with x\u2217. The margin-based constraint says that a classi\ufb01cation/regression function h(x; \u03b8) should make at most \u03f5 deviation from the true label y\u2217", "8ba04799-100a-47fc-99c1-d97ec8e75d70": "To produce each successive approximation, vk+1 from vk, iterative policy evaluation applies the same operation to each state s: it replaces the old value of s with a new value obtained from the old values of the successor states of s, and the expected immediate rewards, along all the one-step transitions possible under the policy being evaluated. We call this kind of operation an expected update. Each iteration of iterative policy evaluation updates the value of every state once to produce the new approximate value function vk+1. There are several di\u21b5erent kinds of expected updates, depending on whether a state (as here) or a state\u2013action pair is being updated, and depending on the precise way the estimated values of the successor states are combined. All the updates done in DP algorithms are called expected updates because they are based on an expectation over all possible next states rather than on a sample next state.\n\nThe nature of an update can be expressed in an equation, as above, or in a backup diagram like those introduced in Chapter 3. For example, the backup diagram corresponding to the expected update used in iterative policy evaluation is shown on page 59", "6819bf63-d901-4994-81af-0b700beaf9b5": "Recall that in the continuing case J(\u2713) = r(\u21e1) (13.15) and that v\u21e1 and q\u21e1 denote values with respect to the di\u21b5erential return (13.17).\n\nThe gradient of the state-value function can be written, for any s 2 S, as Notice that the left-hand side can be written rJ(\u2713), and that it does not depend on s. Thus the right-hand side does not depend on s either, and we can safely sum it over all s 2 S, weighted by \u00b5(s), without changing it (because P Policy-based methods o\u21b5er practical ways of dealing with large actions spaces, even continuous spaces with an in\ufb01nite number of actions. Instead of computing learned probabilities for each of the many actions, we instead learn statistics of the probability distribution. For example, the action set might be the real numbers, with actions chosen from a normal (Gaussian) distribution. The probability density function for the normal distribution is conventionally written To produce a policy parameterization, the policy can be de\ufb01ned as the normal probability density over a real-valued scalar action, with mean and standard deviation given by parametric function approximators that depend on the state", "038f4227-73fc-4faf-88b7-46f1ccceb781": "After these gradients have been computed, the gradient descent algorithm, or another optimization algorithm, uses these gradients to update the parameters.\n\nhttps://www.deeplearningbook.org/contents/mlp.html    For the MLP, the computational cost is dominated by the cost of matrix multiplication. During the forward propagation stage, we multiply by each weight matrix, resulting in O(w) multiply-adds, where w is the number of weights. During  the backward propagation stage, we multiply by the transpose of each weight matrix, which has the same computational cost. The main memory cost of the algorithm is that we need to store the input to the nonlinearity of the hidden layer. 216  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  This value is stored from the time it is computed until the backward pass has returned to the same point. The memory cost is thus O(mmnp,), where m is the number of examples in the minibatch and np is the number of hidden units. 6.5.8 Complications  Our description of the back-propagation algorithm here is simpler than the imple- mentations actually used in practice", "2014405c-e7f4-4a0b-8d0a-44050453594e": "If it did, then the value function would eventually converge to the value function for that policy, and that in turn would cause the policy to change. Stability is achieved only when both the policy and the value function are optimal. Convergence to this optimal \ufb01xed point seems inevitable as the changes to the action-value function decrease over time, but has not yet been formally proved.\n\nIn our opinion, this is one of the most fundamental open theoretical questions in reinforcement learning . Example 5.3: Solving Blackjack It is straightforward to apply Monte Carlo ES to blackjack. Because the episodes are all simulated games, it is easy to arrange for exploring starts that include all possibilities. In this case one simply picks the dealer\u2019s cards, the player\u2019s sum, and whether or not the player has a usable ace, all at random with equal probability. As the initial policy we use the policy evaluated in the previous blackjack example, that which sticks only on 20 or 21. The initial action-value function can be zero for all state\u2013action pairs. Figure 5.2 shows the optimal policy for blackjack found by Monte Carlo ES", "6ddc9fbc-b921-4ce4-a720-9d6da3cf2f61": "By the same token, when learning is complete, a transition from the latest reward-predicting state to the rewarding state produces a zero TD error because the latest reward-predicting state\u2019s value, being correct, cancels the reward. This parallels the observation that fewer dopamine neurons generate a phasic response to a fully predicted reward than to an unpredicted reward. After learning, if the reward is suddenly omitted, the TD error goes negative at the usual time of reward because the value of the latest reward-predicting state is then too high: \u03b4t\u22121 = Rt + Vt \u2212 Vt\u22121 = 0 + 0 \u2212 R? = \u2212R?, as shown at the right end of the \u2018R omitted\u2019 graph of \u03b4 in Figure 15.4. This is like dopamine neuron activity decreasing below baseline at the time an expected reward is omitted as seen in the experiment of Schultz et al. described above and shown in Figure 15.3. The idea of an earliest reward-predicting state deserves more attention.\n\nIn the scenario described above, because experience is divided into trials, and we assumed that predictions are con\ufb01ned to individual trials, the earliest reward-predicting state is always the \ufb01rst state of a trial", "d9a87059-39b5-411e-99db-ae1ab3874c9d": "We see that the mean \u00b5k for the kth Gaussian component is obtained by taking a weighted mean of all of the points in the data set, in which the weighting factor for data point xn is given by the posterior probability \u03b3(znk) that component k was responsible for generating xn. If we set the derivative of ln p(X|\u03c0, \u00b5, \u03a3) with respect to \u03a3k to zero, and follow a similar line of reasoning, making use of the result for the maximum likelihood solution for the covariance matrix of a single Gaussian, we obtain Section 2.3.4 which has the same form as the corresponding result for a single Gaussian \ufb01tted to the data set, but again with each data point weighted by the corresponding posterior probability and with the denominator given by the effective number of points associated with the corresponding component. Finally, we maximize ln p(X|\u03c0, \u00b5, \u03a3) with respect to the mixing coef\ufb01cients \u03c0k. Here we must take account of the constraint (9.9), which requires the mixing coef\ufb01cients to sum to one.\n\nThis can be achieved using a Lagrange multiplier and Appendix E where again we see the appearance of the responsibilities", "12301043-d247-4e2f-a989-37f32872e888": "Diagnostic tests for convergence of Markov chain Monte Carlo algorithms are summarized in Robert and Casella , and some practical guidance on the use of sampling methods in the context of machine learning is given in Bishop and Nabney . In this section, we consider some simple strategies for generating random samples from a given distribution. Because the samples will be generated by a computer algorithm they will in fact be pseudo-random numbers, that is, they will be deterministically calculated, but must nevertheless pass appropriate tests for randomness. Generating such numbers raises several subtleties  that lie outside the scope of this book. Here we shall assume that an algorithm has been provided that generates pseudo-random numbers distributed uniformly over (0, 1), and indeed most software environments have such a facility built in. We \ufb01rst consider how to generate random numbers from simple nonuniform distributions, assuming that we already have available a source of uniformly distributed random numbers. Suppose that z is uniformly distributed over the interval (0, 1), and that we transform the values of z using some function f(\u00b7) so that y = f(z)", "26b62163-fd1c-44cb-b001-912b131566ce": "Fast Weights  https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Log  The rapid generalization of MetaNet relies on \u201cfast weights\u201d. There are a handful of papers on this  topic, but | haven't read all of them in detail and | failed to find a very concrete definition, only a  vague agreement on the concept. Normally weights in the neural networks are updated by  stochastic gradient descent in an objective function and this process is known to be slow. One  faster way to learn is to utilize one neural network to predict the parameters of another neural  network and the generated weights are called fast weights. In comparison, the ordinary SGD-based  weights are named s/ow weights.\n\nIn MetaNet, loss gradients are used as meta information to populate models that learn fast weights. Slow and fast weights are combined to make predictions in neural networks. Input  Fast weight layer  ReLU  Slow weight layer  ReLU  Fast weight layer  ReLU  Slow weight layer  ReLU  Fast weight layer Slow weight layer  Softmax  Output  Model Components  Disclaimer: Below you will find my annotations are different from those in the paper", "50edd1e6-d948-4c88-b547-2dd2db9e41d9": "To transition from Rescorla and Wagner\u2019s model to the TD model of classical conditioning (which we just call the TD model), we \ufb01rst recast their model in terms of the concepts that we are using throughout this book.\n\nSpeci\ufb01cally, we match the notation we use for learning with linear function approximation (Section 9.4), and we think of the conditioning process as one of learning to predict the \u201cmagnitude of the US\u201d on a trial on the basis of the compound CS presented on that trial, where the magnitude of a US Y is the RY of the Rescorla\u2013Wagner model as given above. We also introduce states. Because the Rescorla\u2013Wagner model is a trial-level model, meaning that it deals with how associative strengths change from trial to trial without considering any details about what happens within and between trials, we do not have to consider how states change during a trial until we present the full TD model in the following section. Instead, here we simply think of a state as a way of labeling a trial in terms of the collection of component CSs that are present on the trial", "bf2be6cd-b2c3-4a7f-9cd4-4f8bb8908c59": "Because the KL divergence is non-negative and measures the difference between two distributions, it is often conceptualized as measuring some sort of distance between these distributions. It is not a true distance measure because it is not symmetric: Dxi(P||Q) 4 Dki(Q||P) for some P and Q. This asymmetry means that there are important consequences to the choice of whether to use Dy (P||Q) or D xp (Q||P). See figure 3.6 for more detail. 72  CHAPTER 3.\n\nPROBABILITY AND INFORMATION THEORY  0.7  0.6  0.5  0.4  entropy in nats  https://www.deeplearningbook.org/contents/prob.html    Shannon  0.0 0.0 0.2 0.4 0.6 0.8 1.0  Figure 3.5: Shannon entropy of a binary random variable. This plot shows how distri- butions that are closer to deterministic have low Shannon entropy while distributions that are close to uniform have high Shannon entropy. On the horizontal axis, we plot p, the probability of a binary random variable being equal to 1", "a388b53f-28ab-4c21-a85e-2b7be23af15b": "The weights of the network were set initially to small random values. The initial evaluations were thus entirely arbitrary. Since the moves were selected on the basis of these evaluations, the initial moves were inevitably poor, and the initial games often lasted hundreds or thousands of moves before one side or the other won, almost by accident. After a few dozen games however, After playing about 300,000 games against itself, TD-Gammon 0.0 as described above learned to play approximately as well as the best previous backgammon computer programs. This was a striking result because all the previous high-performance computer programs had used extensive backgammon knowledge. For example, the reigning champion program at the time was, arguably, Neurogammon, another program written by Tesauro that used a neural network but not TD learning.\n\nNeurogammon\u2019s network was trained on a large training corpus of exemplary moves provided by backgammon experts, and, in addition, started with a set of features specially crafted for TD-Gammon 0.0, backgammon positions were represented to the network in a relatively direct way that involved little backgammon knowledge. It did, however, involve substantial knowledge of how ANNs work and how information is best presented to them", "82632d28-918b-4788-b171-7941c4a4d64a": "We would like to compute the log-probability of the observed data, log p(v; @). Sometimes it is too difficult to compute log p(v; @) if it is costly to marginalize out h. Instead, we can compute a lower bound L(v, 6, q) on log p(v; 8). This bound is called the evidence lower bound (ELBO). Another commonly used name for this lower bound is the negative variational free energy.\n\nSpecifically, the evidence lower bound is defined to be  L(v, 6,4) = log p(v; 0) \u2014 Dx (q(h | v)||p(h | v; 4), (19.1)  where q is an arbitrary probability distribution over h.  Because the difference between log p(v) and L(v,0,q) is given by the KL divergence, and because the KL divergence is always nonnegative, we can see that \u00a3 always has at most the same value as the desired log-probability. The two are equal if and only if qg is the same distribution as p(h | v)", "348eb488-17b9-4651-9a94-3e40765a6fe4": "Vasudevan, and Q. V. Le. Autoaugment: Learning augmentation policies from data. In CVPR, 2019. P. Dayan and G. E. Hinton. Using expectation-maximization for reinforcement learning. Neural Computation, 9(2):271\u2013278, 1997. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019. Y. Fan, F. Tian, T. Qin, X.-Y. Li, and T.-Y. Liu. Learning to teach. In ICLR, 2018. C. Finn, P. Christiano, P. Abbeel, and S. Levine. A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models", "997e57b4-0e6b-4a1e-943f-a988aa254a38": "Consider a D-dimensional variable x with components x1, . , xD. The constraint equation g(x) = 0 then represents a (D\u22121)-dimensional surface in x-space as indicated in Figure E.1.\n\nWe \ufb01rst note that at any point on the constraint surface the gradient \u2207g(x) of the constraint function will be orthogonal to the surface. To see this, consider a point x that lies on the constraint surface, and consider a nearby point x + \u03f5 that also lies on the surface. If we make a Taylor expansion around x, we have Because both x and x+\u03f5 lie on the constraint surface, we have g(x) = g(x+\u03f5) and hence \u03f5T\u2207g(x) \u2243 0. In the limit \u2225\u03f5\u2225 \u2192 0 we have \u03f5T\u2207g(x) = 0, and because \u03f5 is then parallel to the constraint surface g(x) = 0, we see that the vector \u2207g is normal to the surface. Next we seek a point x\u22c6 on the constraint surface such that f(x) is maximized", "bb6999c8-4b30-425d-b4ad-1c69c9be40cd": "Instead, we must search for them by maximizing the log-likelihood.\n\nWe can do this by minimizing the negative log-likelihood using gradient descent. This same strategy can be applied to essentially any supervised learning problem, by writing down a parametric family of conditional probability distributions over the right kind of input and output variables. 138  CHAPTER 5. MACHINE LEARNING BASICS  5.7.2. Support Vector Machines  One of the most influential approaches to supervised learning is the support vector machine . This model is similar to logistic regression in that it is driven by a linear function wx +b. Unlike logistic regression, the support vector machine does not provide probabilities, but only outputs a class identity. The SVM predicts that the positive class is present when w!x-+b is positive. Likewise, it predicts that the negative class is present when  w \u00ab+b is negative. One key innovation associated with support vector machines is the kernel trick. The kernel trick consists of observing that many machine learning algorithms can be written exclusively in terms of dot products between examples", "b1c6f38b-6a0c-45fc-ac62-d02f11b88c16": "This produced TD-Gammon 1.0. TD-Gammon 1.0 was clearly substantially better than all previous backgammon programs and found serious competition only among human experts. Later versions of the program, TD-Gammon 2.0 (40 hidden units) and TD-Gammon 2.1 (80 hidden units), were augmented with a selective two-ply search procedure.\n\nTo select moves, these programs looked ahead not just to the positions that would immediately result, but also to the opponent\u2019s possible dice rolls and moves. Assuming the opponent always took the move that appeared immediately best for him, the expected value of each candidate move was computed and the best was selected. To save computer time, the second ply of search was conducted only for candidate moves that were ranked highly after the \ufb01rst ply, about four or \ufb01ve moves on average. Two-ply search a\u21b5ected only the moves selected; the learning process proceeded exactly as before. The \ufb01nal versions of the program, TD-Gammon 3.0 and 3.1, used 160 hidden units and a selective three-ply search. TD-Gammon illustrates the combination of learned value functions and decision-time search as in heuristic search and MCTS methods", "17e3b1bc-e5b8-4130-b3ab-e4411898f166": "We will want to find some encoding function that produces the code for an input, f(a) = c, and a decoding function that produces the reconstructed input given its code, x \u00a5 g(f(x)). PCA is defined by our choice of the decoding function. Specifically, to make the decoder very simple, we choose to use matrix multiplication to map the code back into R\". Let g(c) = De, where D \u20ac R\"*! is the matrix defining the decoding. Computing the optimal code for this decoder could be a difficult problem. To keep the encoding problem easy, PCA constrains the columns of D to be orthogonal to each other. (Note that D is still not technically \u201can orthogonal matrix\u201d unless l=n.) With the problem as described so far, many solutions are possible, because we can increase the scale of D,; if we decrease c; proportionally for all points. To give the problem a unique solution, we constrain all the columns of D to have unit norm", "2e7b3dc7-ff91-4e5b-ade2-a5ad24e43b55": "For a graphical depiction ot how parameter sharing works, see figure 9.5. As an example of both of these first two principles in action, figure 9.6 shows how sparse connectivity and parameter sharing can dramatically improve the  \u00b0 \u00a9 \u00a9  OOO  Figure 9.5: Parameter sharing. Black arrows indicate the connections that use a particular parameter in two different models. (Top)The black arrows indicate uses of the central element of a 3-element kernel in a convolutional model. Because of parameter sharing, this single parameter is used at all input locations. (Bottom)The single black arrow indicates the use of the central element of the weight matrix in a fully connected model. This model has no parameter sharing, so the parameter is used only once. \u00a9 O-O OQ O-O\u00a9  OO O-O OO OO \u00a9)  333  CHAPTER 9. CONVOLUTIONAL NETWORKS  Figure 9.6: Efficiency of edge detection", "ba543b74-5ac6-40d3-a2b3-38426a61758c": "Compositional generalization and natural language variation: Can a semantic parsing approach handle both? Dinghan Shen, Mingzhi Zheng, Yelong Shen, Yanru Qu, and Weizhu Chen. 2020. A simple but toughto-beat data augmentation approach for natural language understanding and generation. arXiv preprint arXiv:2009.13818. Sam Shleifer. 2019. Low resource text classi\ufb01cation with ulm\ufb01t and backtranslation. Patrice Y. Simard, David Steinkraus, and John C. Platt. 2003.\n\nBest practices for convolutional neural networks applied to visual document analysis. Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings., pages 958\u2013963. Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. 2020. Fixmatch: Simplifying semi-supervised learning with consistency and con\ufb01dence. Advances in Neural Information Processing Systems, 33", "d158600e-bd7a-4075-bbdf-98d7b316ad2c": "Exercise 11.3 The generalization to multiple variables is straightforward and involves the Jacobian of the change of variables, so that As a \ufb01nal example of the transformation method we consider the Box-Muller method for generating samples from a Gaussian distribution. First, suppose we generate pairs of uniformly distributed random numbers z1, z2 \u2208 (\u22121, 1), which we can do by transforming a variable distributed uniformly over (0, 1) using z \u2192 2z \u2212 1.\n\nNext we discard each pair unless it satis\ufb01es z2 distribution of points inside the unit circle with p(z1, z2) = 1/\u03c0, as illustrated in Figure 11.3. Then, for each pair z1, z2 we evaluate the quantities and so y1 and y2 are independent and each has a Gaussian distribution with zero mean and unit variance. If y has a Gaussian distribution with zero mean and unit variance, then \u03c3y + \u00b5 will have a Gaussian distribution with mean \u00b5 and variance \u03c32. To generate vectorvalued variables having a multivariate Gaussian distribution with mean \u00b5 and covariance \u03a3, we can make use of the Cholesky decomposition, which takes the form \u03a3 = LLT", "61fa5b8e-9ff7-400d-a281-d10b042bb8a3": "The distribution given by (13.119) is a mixture distribution, and samples can be drawn by choosing a component l with probability given by the mixing coef\ufb01cients w(l) and then drawing a sample from the corresponding component. In summary, we can view each step of the particle \ufb01lter algorithm as comprising two stages. At time step n, we have a sample representation of the posterior distribution p(zn|Xn) expressed as samples {z(l) n } with corresponding weights {w(l) n }. This can be viewed as a mixture representation of the form (13.119). To obtain the corresponding representation for the next time step, we \ufb01rst draw L samples from the mixture distribution (13.119), and then for each sample we use the new observation xn+1 to evaluate the corresponding weights w(l) n+1 \u221d p(xn+1|z(l) n+1). This is illustrated, for the case of a single variable z, in Figure 13.23", "800ed08d-1eb3-48ca-b8c3-d94bb508a6d9": "To do this, we \ufb01rst de\ufb01ne the kinetic energy by The total energy of the system is then the sum of its potential and kinetic energies where H is the Hamiltonian function.\n\nUsing (11.53), (11.55), (11.56), and (11.57), we can now express the dynamics of the system in terms of the Hamiltonian equations given by Exercise 11.15 William Rowan Hamilton was an Irish mathematician and physicist, and child prodigy, who was appointed Professor of Astronomy at Trinity College, Dublin, in 1827, before he had even graduated. One of Hamilton\u2019s most important contributions was a new formulation of dynamics, which played a signi\ufb01cant role in the later development of quantum mechanics. His other great achievement was the development of quaternions, which generalize the concept of complex numbers by introducing three distinct square roots of minus one, which satisfy i2 = j2 = k2 = ijk = \u22121. It is said that these equations occurred to him while walking along the Royal Canal in Dublin with his wife, on 16 October 1843, and he promptly carved the equations into the side of Broome bridge", "6878e708-fdc6-4fa0-b4bf-41cd28fefb16": "Show that this cannot be represented by a single-level mixture of 3 components having mixing coef\ufb01cients determined by a linear-softmax model. In this appendix, we give a brief introduction to the data sets used to illustrate some of the algorithms described in this book.\n\nDetailed information on \ufb01le formats for these data sets, as well as the data \ufb01les themselves, can be obtained from the book web site: http://research.microsoft.com/\u223ccmbishop/PRML The digits data used in this book is taken from the MNIST data set , which itself was constructed by modifying a subset of the much larger data set produced by NIST (the National Institute of Standards and Technology). It comprises a training set of 60, 000 examples and a test set of 10, 000 examples. Some of the data was collected from Census Bureau employees and the rest was collected from high-school children, and care was taken to ensure that the test examples were written by different individuals to the training examples. The original NIST data had binary (black or white) pixels. To create MNIST, these images were size normalized to \ufb01t in a 20\u00d720 pixel box while preserving their aspect ratio", "c67414dc-f631-4291-b7f8-1e0903b78f46": "Therefore, any \u03b8 which increases l(\u03b8|\u03b8n) in turn increase the L(\u03b8). In order to achieve the greatest possible increase in the value of L(\u03b8), the EM algorithm calls for selecting \u03b8 such that l(\u03b8|\u03b8n) is maximized. We denote this updated value as \u03b8n+1. This process is illustrated in Figure (2). Formally we have, Now drop terms which are constant w.r.t. \u03b8 In Equation (15) the expectation and maximization steps are apparent. The EM algorithm thus consists of iterating the: 1. E-step: Determine the conditional expectation EZ|X,\u03b8n{ln P(X, z|\u03b8)} 2.\n\nM-step: Maximize this expression with respect to \u03b8. At this point it is fair to ask what has been gained given that we have simply traded the maximization of L(\u03b8) for the maximization of l(\u03b8|\u03b8n). The answer lies in the fact that l(\u03b8|\u03b8n) takes into account the unobserved or missing data Z. In the case where we wish to estimate these variables the EM algorithms provides a framework for doing so", "cc2e63ae-679e-4329-aed7-8aaeb7c56831": "With a true SGD method such divergence would not be possible. The appeal of SGD is so strong that great e\u21b5ort has gone into \ufb01nding a practical way of harnessing it for reinforcement learning. The starting place of all such e\u21b5orts is the choice of an error or objective function to optimize. In this and the next section we explore the origins and limits of the most popular proposed objective function, that based on the Bellman error introduced in the previous section. Although this has been a popular and in\ufb02uential approach, the conclusion that we reach here is that it is a misstep and yields no good learning algorithms.\n\nOn the other hand, this approach fails in an interesting way that o\u21b5ers insight into what might constitute a good approach. To begin, let us consider not the Bellman error, but something more immediate and naive. Temporal di\u21b5erence learning is driven by the TD error", "4afb48d1-ec2e-4ec5-9018-d674c44aba23": "Similarly, we can de\ufb01ne UF to be the set of such distributions that can be expressed as a factorization of the form (8.39) with respect to the maximal cliques of the graph. The Hammersley-Clifford theorem  states that the sets UI and UF are identical. Because we are restricted to potential functions which are strictly positive it is convenient to express them as exponentials, so that where E(xC) is called an energy function, and the exponential representation is called the Boltzmann distribution. The joint distribution is de\ufb01ned as the product of potentials, and so the total energy is obtained by adding the energies of each of the maximal cliques.\n\nIn contrast to the factors in the joint distribution for a directed graph, the potentials in an undirected graph do not have a speci\ufb01c probabilistic interpretation. Although this gives greater \ufb02exibility in choosing the potential functions, because there is no normalization constraint, it does raise the question of how to motivate a choice of potential function for a particular application. This can be done by viewing the potential function as expressing which con\ufb01gurations of the local variables are preferred to others", "77838fce-9af9-44d3-8dfb-133dda73d880": "From these two relationships, it is easily shown that h(x) must be given by the logarithm of p(x) and so we have Exercise 1.28 where the negative sign ensures that information is positive or zero. Note that low probability events x correspond to high information content. The choice of basis for the logarithm is arbitrary, and for the moment we shall adopt the convention prevalent in information theory of using logarithms to the base of 2. In this case, as we shall see shortly, the units of h(x) are bits (\u2018binary digits\u2019). Now suppose that a sender wishes to transmit the value of a random variable to a receiver. The average amount of information that they transmit in the process is obtained by taking the expectation of (1.92) with respect to the distribution p(x) and is given by This important quantity is called the entropy of the random variable x.\n\nNote that limp\u21920 p ln p = 0 and so we shall take p(x) ln p(x) = 0 whenever we encounter a value for x such that p(x) = 0", "878f36a2-a7a0-4d18-a7b1-770ac6e6495c": "The behavior of populations of reinforcement learning agents is deeply relevant to the study of social and economic systems, and if anything like Klopf\u2019s hedonistic neuron hypothesis is correct, to neuroscience as well.\n\nThe hypothesis described above about how an actor\u2013critic algorithm might be implemented in the brain only narrowly addresses the implications of the fact that the dorsal and ventral subdivisions of the striatum, the respective locations of the actor and the critic according to the hypothesis, each contain millions of medium spiny neurons whose synapses undergo change modulated by phasic bursts of dopamine neuron activity. The actor in Figure 15.5a is a single-layer network of k actor units. The actions produced by this network are vectors (A1, A2, \u00b7 \u00b7 \u00b7 , Ak)> presumed to drive the animal\u2019s behavior. Changes in the e\ufb03cacies of the synapses of all of these units depend on the reinforcement signal \u03b4. Because actor units attempt to make \u03b4 as large as possible, \u03b4 e\u21b5ectively acts as a reward signal for them (so in this case reinforcement is the same as reward)", "dfce28ab-2568-4479-a850-03cdfb7670f0": "Because of the complexity of the learning task, however, contingent eligibility is merely a preliminary step in the credit assignment process: the relationship between a single team member\u2019s action and changes in the team\u2019s reward signal is a statistical correlation that has to be estimated over many trials. Contingent eligibility is an essential but preliminary step in this process. Learning with non-contingent eligibility traces does not work at all in the team setting because it does not provide a way to correlate actions with consequent changes in the reward signal. Non-contingent eligibility traces are adequate for learning to predict, as the critic component of the actor\u2013critic algorithm does, but they do not support learning to control, as the actor component must do.\n\nThe members of a population of critic-like agents may still receive a common reinforcement signal, but they would all learn to predict the same quantity (which in the case of an actor\u2013critic method, would be the expected return for the current policy). How successful each member of the population would be in learning to predict the expected return would depend on the information it receives, which could be very di\u21b5erent for di\u21b5erent members of the population. There would be no need for the population to produce di\u21b5erentiated patterns of activity", "a52154a9-d7b2-49de-b5f5-0e24b3ec9710": "The latter noise level should correspond to the mean squared error of reconstructions, whereas the injected noise is a hyperparameter that controls the mixing speed as well as the extent to which the estimator smooths the empirical distribution . In the example illustrated here, only the C and p conditionals are stochastic steps (f and g are deterministic computations), although noise can also be injected inside the autoencoder, as in generative stochastic networks . 708  CHAPTER 20. DEEP GENERATIVE MODELS  2. Encode # into h = f(z). 3. Decode h to obtain the parameters w = g(h) of p(x | w = g(h)) = p(x| @). 4. Sample the next state x from p(x | w = g(h)) = p(x | #). Bengio et al", "73ed83a1-00f1-4619-ab63-c32180076f54": "Theano  is an example of a software package that automatically detects and stabilizes many common numerically unstable expressions that arise in the context of deep learning. 4.2 Poor Conditioning  Conditioning refers to how rapidly a function changes with respect to small changes in its inputs. Functions that change rapidly when their inputs are perturbed slightly can be problematic for scientific computation because rounding errors in the inputs can result in large changes in the output. Consider the function f(a) = Ata. When A \u20ac R\u201d*\u201d has an eigenvalue decomposition, its condition number is  i  max (4.2)  ij  This is the ratio of the magnitude of the largest and smallest eigenvalue. When this number is large, matrix inversion is particularly sensitive to error in the input. This sensitivity is an intrinsic property of the matrix itself, not the result of rounding error during matrix inversion. Poorly conditioned matrices amplify pre-existing errors when we multiply by the true matrix inverse. In practice, the error will be compounded further by numerical errors in the inversion process itself. 4.3", "a238b989-6749-401f-a164-f6d819188f37": "Many NLP applications are based on language models that define a probability distribution over sequences of words, characters, or bytes in a natural language. As with the other applications discussed in this chapter, very generic neural network techniques can be successfully applied to natural language processing. To achieve excellent performance and to scale well to large applications, however, some domain-specific strategies become important. To build an efficient model of natural language, we must usually use techniques that are specialized for processing sequential data. In many cases, we choose to regard natural language as a sequence of words, rather than a sequence of individual characters or bytes. Because the total number of possible words is so large, word-based language models must operate on  Powaow : 1 1 4 a tou 4 : 1  https://www.deeplearningbook.org/contents/applications.html    all CXLICILIELY LIS U-CAMNEISLONAL ALL Sparse AISCreLe Space. O\u20acVELal SLIALERICS Lave been developed to make models of such a space efficient, in both a computational and a statistical sense", "1f9fec28-be1c-418c-89dd-004adcf82c27": "Erhan ef al.\n\nmade  a linear projection to high-dimensional space by concatenating the y for many specific x  points. They then made a further nonlinear projection to 2-D by Isomap . Color indicates time. All networks are initialized near the center of the plot (corresponding to the region of functions that produce approximately uniform distributions over the class y for most inputs). Over time, learning moves the function outward, to points that make strong predictions. Training consistently terminates in one region when using pretraining and in another, nonoverlapping region when not using pretraining. Isomap tries to preserve global relative distances (and hence volumes) so the small region corresponding to pretrained models may indicate that the pretraining-based estimator has reduced variance. 532  CHAPTER 15. REPRESENTATION LEARNING  many hyperparameters, whose effect may be measured after the fact but is often difficult to predict ahead of time. When we perform unsupervised and supervised learning simultaneously, instead of using the pretraining strategy, there is a single hyperparameter, usually a coefficient attached to the unsupervised cost, that determines how strongly the unsupervised objective will regularize the supervised model", "0f36c8a4-1c71-4bfd-923e-d099c4a0b828": "As As SARSA  Q-learning  Deep Q-Network  Theoretically, we can memorize Q..(. ) for all state-action pairs in Q-learning, like in a gigantic table. However, it quickly becomes computationally infeasible when the state and action space are large. Thus people use functions (i.e. a machine learning model) to approximate Q values and this is called function approximation. For example, if we use a function with parameter 0 to calculate Q values, we can label Q value function as Q(s, a; 9). Unfortunately Q-learning may suffer from instability and divergence when combined with an nonlinear Q-value function approximation and bootstrapping (See Problems #2). Deep Q-Network  aims to greatly improve and stabilize the training procedure of Q-learning by two innovative mechanisms:  Experience Replay: All the episode steps e, = (.5;, A;, Ri, Sti1) are stored in one replay memory D; = {e,,..., e;}. D; has experience tuples over many episodes", "6d4e12a9-2dcc-4191-9e68-7f0b038dddf4": "Each basis function \u03c6j(xn), evaluated at the N data points, can also be represented as a vector in the same space, denoted by \u03d5j, as illustrated in Figure 3.2. Note that \u03d5j corresponds to the jth column of \u03a6, whereas \u03c6(xn) corresponds to the nth row of \u03a6. If the number M of basis functions is smaller than the number N of data points, then the M vectors \u03c6j(xn) will span a linear subspace S of dimensionality M. We de\ufb01ne y to be an N-dimensional vector whose nth element is given by y(xn, w), where n = 1, . , N. Because y is an arbitrary linear combination of the vectors \u03d5j, it can live anywhere in the M-dimensional subspace.\n\nThe sum-of-squares error (3.12) is then equal (up to a factor of 1/2) to the squared Euclidean distance between y and t. Thus the least-squares solution for w corresponds to that choice of y that lies in subspace S and that is closest to t", "4913902f-4fee-4bde-aac0-2699deac00c2": "Of course, there are many MS representations depending on the nature of the microstimuli, and a number of examples of MS representations have been studied in the literature, in some cases along with proposals for how an animal\u2019s brain might generate them (see the Bibliographic and Historical Comments at the end of this chapter). MS representations are more realistic than the presence or CSC representations as hypotheses about neural representations of stimuli, and they allow the behavior of the TD model to be related to a broader collection of phenomena observed in animal experiments. In particular, by assuming that cascades of microstimuli are initiated by USs as well as by CSs, and by studying the signi\ufb01cant e\u21b5ects on learning of interactions between microstimuli, eligibility traces, and discounting, the TD model is helping to frame hypotheses to account for many of the subtle phenomena of classical conditioning and how an animal\u2019s brain might produce them.\n\nWe say more about this below, particularly in Chapter 15 where we discuss reinforcement learning and neuroscience. Even with the simple presence representation, however, the TD model produces all the basic properties of classical conditioning that are accounted for by the Rescorla\u2013Wagner model, plus features of conditioning that are beyond the scope of trial-level models", "72849358-35e8-4a8b-af03-2ffd3f32c7c8": "All of this makes optimization difficult, especially when the input to the function is multidimensional. We therefore usually settle for finding a value of f that is very  low but not necessarily minimal in any formal sense. See figure 4.3 for an example. We often minimize functions that have multiple inputs: f : R\u201d \u2014 R. For the concept of \u201cminimization\u201d to make sense, there must still be only one (scalar) output. For functions with multiple inputs, we must make use of the concept of partial derivatives.\n\nThe partial derivative ro f(z) measures how f changes as only the variable x; increases at point a. The gradient generalizes the notion of derivative to the case where the derivative is with respect to a vector: the gradient of f is the vector containing all the partial derivatives, denoted V_ f(a). Element 7 of the gradient is the partial derivative of f with respect to z;. In multiple dimensions,  82  CHAPTER 4. NUMERICAL COMPUTATION  This local minimum performs nearly as well as the global one,  so it is an acceptable halting point", "01c57e3f-745d-48b2-b0f7-8b73d6b71acc": "For each bandit problem, such as the one shown in Figure 2.1, the action values, q\u21e4(a), a = 1, . , 10, were selected according to a normal (Gaussian) distribution with mean 0 and variance 1. Then, when a learning method applied to that problem selected action At at time step t, the actual reward, Rt, was selected from a normal distribution with mean q\u21e4(At) and variance 1. These distributions are shown in gray in Figure 2.1. We call this suite of test tasks the 10-armed testbed.\n\nFor any learning method, we can measure its performance and behavior as it improves with experience over 1000 time steps when applied to one of the bandit problems. This makes up one run. Repeating this for 2000 independent runs, each with a di\u21b5erent bandit problem, we obtained measures of the learning algorithm\u2019s average behavior. as described above, on the 10-armed testbed. All the methods formed their action-value estimates using the sample-average technique. The upper graph shows the increase in expected reward with experience. The greedy method improved slightly faster than the other methods at the very beginning, but then leveled o\u21b5 at a lower level", "908cf3e9-f017-4f5f-8ef8-229339f7252b": "For example, the  274  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  most commonly used property is the gradient:  VoJ(@) = Ux,y~Paata V log Dmodel(, YS 0). (8.6)  Computing this expectation exactly is very expensive because it requires evaluating the model on every example in the entire dataset. In practice, we can compute these expectations by randomly sampling a small number of examples from the dataset, then taking the average over only those examples. Recall that the standard error of the mean (equation 5.46) estimated from n samples is given by a/./n, where o is the true standard deviation of the value of the samples.\n\nThe denominator of ,/n shows that there are less than linear returns to using more examples to estimate the gradient. Compare two hypothetical estimates of the gradient, one based on 100 examples and another based on 10,000 examples. The latter requires 100 times more computation than the former but reduces the standard error of the mean only by a factor of 10", "fe860d96-3b18-4b67-93fd-bb75b00ea75f": "Peters, J., B\u00a8uchel, C. Neural representations of subjective reward value. Behavioral Peters, J., Schaal, S. Natural actor\u2013critic. Neurocomputing, 71(7):1180\u20131190. Peters, J., Vijayakumar, S., Schaal, S. Natural actor\u2013critic. In European Conference on Machine Learning, pp. 280\u2013291. Springer Berlin Heidelberg. Pezzulo, G., van der Meer, M. A. A., Lansink, C. S., Pennartz, C. M. A. Internally generated sequences in learning and executing goal-directed behavior. Trends in Cognitive Science, 18(12):647\u2013657. Pfei\u21b5er, B. E., Foster, D. J. Hippocampal place-cell sequences depict future paths to Phansalkar, V. V., Thathachar, M. A. L", "7b65980a-37e1-466b-8610-7852bf7b0aa6": "But what happens over the entire course of training? We will further simplify the analysis by making a quadratic approximation to the objective function in the neighborhood of the value of the weights that obtains minimal unregularized training cost, w* = arg min, J(w). If the objective function is truly quadratic, as in the case of fitting a linear regression model with  \u2018More generally, we could regularize the parameters to be near any specific point in space and, surprisingly, still get a regularization effect, but better results will be obtained for a value closer to the true one, with zero being a default value that makes sense when we do not know if the correct value should be positive or negative. Since it is far more common to regularize the model parameters toward zero, we will focus on this special case in our exposition. 227  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  mean squared error, then the approximation is perfect", "87d6fd7d-97c9-4a3b-8113-64db5717b5f7": "Hint: Start by writing out \u03b4a Exercise 12.11 The truncated version of the general o\u21b5-policy return is denoted G\u03bba Using steps entirely analogous to those for the state case, one can write a forward-view update based on (12.27), transform the sum of the updates using the summation rule, and \ufb01nally derive the following form for the eligibility trace for action values: This eligibility trace, together with the expectation-based TD error (12.28) and the usual semi-gradient parameter-update rule (12.7), forms an elegant, e\ufb03cient Expected Sarsa(\u03bb) algorithm that can be applied to either on-policy or o\u21b5-policy data. It is probably the best algorithm of this type at the current time (though of course it is not guaranteed to be stable until combined in some way with one of the methods presented in the following sections). In the on-policy case with constant \u03bb and \u03b3, and the usual state-action TD error (12.16), the algorithm would be identical to the Sarsa(\u03bb) algorithm presented in Section 12.7", "8bd4f7ec-02d8-4f6d-954c-84f03ec63c51": "Applying the analysis again, we will be able to obtain a special case of the same results, but with the solution now phrased in terms of the training data. For linear regression, the cost function is the sum of squared errors:  (Xw \u2014 y)\"(Xw\u2014y). (7.14)  When we add L? regularization, the objective function changes to  https://www.deeplearningbook.org/contents/regularization.html    (Xw-\u2014y)' (Xw\u2014y)+ jaw! w. 7.15)  This changes the normal equations for the solution from w=(X'X)txTy 7.16)  to  w=(X'X+oar)txly. 7.17) The matrix X 'X in equation 7.16 is proportional to the covariance matrix 4 XTX. Using L? regularization replaces this matrix with (XTX + al) ' in equation 7.17. The new matrix is the same as the original one, but with the addition of a to the  diagonal.\n\nThe diagonal entries of this matrix correspond to the variance of each input feature", "2a2a11ab-6642-4ca3-bc3b-68ad18091921": "The relationship that we are looking for is that the forward-view update, summed over time, is approximately equal to a backward-view update, summed over time (this relationship is only approximate because again we ignore changes in the value function).\n\nThe sum of the forward-view update over time is which would be in the form of the sum of a backward-view TD update if the entire expression from the second sum left could be written and updated incrementally as an eligibility trace, which we now show can be done. That is, we show that if this expression was the trace at time k, then we could update it from its value at time k \u2212 1 by: This eligibility trace, together with the usual semi-gradient parameter-update rule for TD(\u03bb) (12.7), forms a general TD(\u03bb) algorithm that can be applied to either on-policy or o\u21b5-policy data. In the on-policy case, the algorithm is exactly TD(\u03bb) because \u21e2t is alway 1 and (12.25) becomes the usual accumulating trace (12.5) (extended to variable \u03bb and \u03b3). In the o\u21b5-policy case, the algorithm often works well but, as a semi-gradient method, is not guaranteed to be stable", "df1881ce-68ee-4399-935e-20c4efa97d36": "Kearns, M., Singh, S. Near-optimal reinforcement learning in polynomial time. Machine Keerthi, S. S., Ravindran, B. Reinforcement learning. In E. Fieslerm and R. Beale (Eds.\n\n), Handbook of Neural Computation, C3. Oxford University Press, New York. Kehoe, E. J. Conditioning with serial compound stimuli: Theoretical and empirical Kehoe, E. J., Schreurs, B. G., Graham, P. Temporal primacy overrides prior training Kei\ufb02in, R., Janak, P. H. Dopamine prediction errors in reward learning and addiction: Ffrom theory to neural circuitry. Neuron, 88(2):247\u2013 263. Kimble, G. A. Hilgard and Marquis\u2019 Conditioning and Learning. Appleton-CenturyKimble, G. A. Foundations of Conditioning and Learning", "cb3b224b-888a-4e84-9334-682d93eb6d28": "It follows that the samples from the previous model\u2019s distribution will be very close to being fair samples from the current model\u2019s distribution, so a Markov chain initialized with these samples will not require much time to mix. Because each Markov chain is continually updated throughout the learning process, rather than restarted at each gradient step, the chains are free to wander far enough to find all the model\u2019s modes. SML is thus considerably more resistant to forming models with spurious modes than CD is. Moreover, because it is possible to store the state of all the sampled variables, whether visible or latent, SML provides an initialization point for both the hidden and the visible units. CD is only able to provide an initialization for the visible units, and therefore requires  610  CHAPTER 18. CONFRONTING THE PARTITION FUNCTION  Algorithm 18.3 The stochastic maximum likelihood / persistent contrastive divergence algorithm using gradient ascent as the optimization procedure  Set \u20ac, the step size, to a small positive number. Set k, the number of Gibbs steps, high enough to allow a Markov chain sampling from p(x;@ + eg) to burn in, starting from samples from p(x;@)", "5d7f79e6-8f00-4280-ac0d-204678b34dd0": "In the case of binary variables, we wished to produce a single number \u00a7=Ply=1|2).\n\n(6.27)  Because this number needed to lie between 0 and 1, and because we wanted the logarithm of the number to be well behaved for gradient-based optimization of the log-likelihood, we chose to instead predict a number z = log P(y = 1 | \u00ab). Exponentiating and normalizing gave us a Bernoulli distribution controlled by the sigmoid function. To generalize to the case of a discrete variable with n values, we now need to produce a vector y, with 7; = P(y =i| a). We require not only that each element of 4; be between 0 and 1, but also that the entire vector sums to 1 so that it represents a valid probability distribution. The same approach that worked for the Bernoulli distribution generalizes to the multinoulli distribution. First, a linear layer predicts unnormalized log probabilities:  z=W'h+b, (6.28)  where 2 = log P(y = %| a). The softmax function can then exponentiate and normalize z to obtain the desired y", "281392b1-29a4-47a7-bde5-1a45dfec28ee": "For the purposes of this simple illustration, we have \ufb01xed the parameters to be \u03b2 = 1.0, \u03b7 = 2.1 and h = 0. Note that leaving h = 0 simply means that the prior probabilities of the two states of xi are equal. Starting with the observed noisy image as the initial con\ufb01guration, we run ICM until convergence, leading to the de-noised image shown in the lower left panel of Figure 8.30. Note that if we set \u03b2 = 0, which effectively removes the links between neighbouring pixels, then the global most probable solution is given by xi = yi for all i, corresponding to the observed noisy image. Exercise 8.14 Later we shall discuss a more effective algorithm for \ufb01nding high probability solutions called the max-product algorithm, which typically leads to better solutions, Section 8.4 although this is still not guaranteed to \ufb01nd the global maximum of the posterior distribution. However, for certain classes of model, including the one given by (8.42), there exist ef\ufb01cient algorithms based on graph cuts that are guaranteed to \ufb01nd the global maximum .\n\nThe lower right panel of Figure 8.30 shows the result of applying a graph-cut algorithm to the de-noising problem", "d99f09d4-bf22-4068-a4d2-e9a3f6816f7a": "Linear SFA modules may then be composed to learn deep nonlinear slow feature extractors by repeatedly learning a linear SFA feature extractor, applying  https://www.deeplearningbook.org/contents/linear_factors.html    a nonlinear basis expansion to its output, and then learning another linear SFA eature extractor on top of that expansion.\n\nWhen trained on small spatial patches of videos of natural scenes, SFA with quadratic basis expansions learns features that share many characteristics with hose of complex cells in V1 cortex . When trained on videos of random motion within 3-D computer-rendered environments, deep SFA learns features that share many characteristics with the features represented by neurons in rat brains that are used for navigation . SFA chus seems to be a reasonably biologically plausible model. A major advantage of SFA is that it is possible to theoretically predict which eatures SFA will learn, even in the deep nonlinear setting. To make such theoretical predictions, one must know about the dynamics of the environment in terms of configuration space (e.g., in the case of random motion in the 3-D rendered  environment, the theoretical analysis proceeds from knowledge of the probability distribution over position and velocity of the camera)", "e17dfd00-21de-41ff-9b15-aee7552f6954": "Section 10.4.1 the complexity penalty arises from components whose parameters are pushed away from their prior values. Components that take essentially no responsibility for explaining the data points have rnk \u2243 0 and hence Nk \u2243 0. From (10.58), we see that \u03b1k \u2243 \u03b10 and from (10.60)\u2013(10.63) we see that the other parameters revert to their prior values. In principle such components are \ufb01tted slightly to the data points, but for broad priors this effect is too small to be seen numerically. For the variational Gaussian mixture model the expected values of the mixing coef\ufb01cients in the posterior distribution are given by Exercise 10.15 Consider a component for which Nk \u2243 0 and \u03b1k \u2243 \u03b10.\n\nIf the prior is broad so that \u03b10 \u2192 0, then E \u2192 0 and the component plays no role in the model, whereas if the prior tightly constrains the mixing coef\ufb01cients so that \u03b10 \u2192 \u221e, then E \u2192 1/K. In Figure 10.6, the prior over the mixing coef\ufb01cients is a Dirichlet of the form (10.39)", "619b4022-e684-4e54-a2ae-7c5ac339ccb6": "Generalized polynomial approximations in Markovian decision processes. Journal of Mathematical Analysis and Applications, 110(2):568\u2013582. Selfridge, O. G. Tracking and trailing: Adaptation in movement strategies. Technical report, Bolt Beranek and Newman, Inc. Unpublished report. Selfridge, O. G. Some themes and primitives in ill-de\ufb01ned systems. In O. G. Selfridge, E. L. Rissland, and M. A. Arbib (Eds. ), Adaptive Control of Ill-De\ufb01ned Systems, pp. 21\u201326. Plenum Press, NY. Proceedings of the NATO Advanced Research Institute on Adaptive Control of Ill-de\ufb01ned Systems, NATO Conference Series II, Systems Science, Vol. 16.\n\nSelfridge, O. J., Sutton, R. S., Barto, A. G. Training and tracking in robotics. In A", "d05eeb23-0cf8-4f07-96b6-6f56a5592c0b": "This example suggests that search might be usefully focused by working backward from goal states. Of course, we do not really want to use any methods speci\ufb01c to the idea of \u201cgoal state.\u201d We want methods that work for general reward functions. Goal states are just a special case, convenient for stimulating intuition. In general, we want to work back not just from goal states but from any state whose value has changed. Suppose that the values are initially correct given the model, as they were in the maze example prior to discovering the goal. Suppose now that the agent discovers a change in the environment and changes its estimated value of one state, either up or down.\n\nTypically, this will imply that the values of many other states should also be changed, but the only useful one-step updates are those of actions that lead directly into the one state whose value has been changed. If the values of these actions are updated, then the values of the predecessor states may change in turn. If so, then actions leading into them need to be updated, and then their predecessor states may have changed. In this way one can work backward from arbitrary states that have changed in value, either performing useful updates or terminating the propagation", "0d64fe3a-d541-4f57-8f58-84dfc8213c57": "We begin by generalizing the n-step return (7.4) to its di\u21b5erential form, with function approximation: where \u00afR is an estimate of r(\u21e1), n \u2265 1, and t + n < T. If t + n \u2265 T, then we de\ufb01ne Gt:t+n .= Gt as usual. The n-step TD error is then after which we can apply our usual semi-gradient Sarsa update (10.12). Pseudocode for the complete algorithm is given in the box", "61274911-56d4-40b6-8e41-06aaca6a8b4e": ", xN) of the observed data for the state space model represented by the directed graph in Figure 13.5 does not satisfy any conditional independence properties and hence does not exhibit the Markov property at any \ufb01nite order. represented by a parametric model p(x|z, w), such as a linear regression model or a neural network, in which w is a vector of adaptive parameters. Describe how the parameters w can be learned from data using maximum likelihood.\n\n13.5 (\u22c6 \u22c6) Verify the M-step equations (13.18) and (13.19) for the initial state probabilities and transition probability parameters of the hidden Markov model by maximization of the expected complete-data log likelihood function (13.17), using appropriate Lagrange multipliers to enforce the summation constraints on the components of \u03c0 and A. 13.6 (\u22c6) Show that if any elements of the parameters \u03c0 or A for a hidden Markov model are initially set to zero, then those elements will remain zero in all subsequent updates of the EM algorithm. 13.7 (\u22c6) Consider a hidden Markov model with Gaussian emission densities", "23efc706-9e75-46d0-a4a5-a361ca10814d": "But contrastive methods have a major issue: They are very inefficient to train. In high-dimensional spaces such as images, there are many ways one image can be different from another. Finding a set of contrastive images that cover all the ways they can differ from a given image is a nearly impossible task.\n\nTo paraphrase Leo Tolstoy\u2019s Anna Karenina: \u201cHappy families are all alike; every unhappy family is unhappy in its own way.\u201d This applies to any family of high-dimensional objects, it seems. What if it were possible to make sure the energy of incompatible pairs is higher than that of compatible pairs without explicitly pushing up on the energy of many incompatible pairs? Non-contrastive energy- based SSL  Non-contrastive methods applied to joint embedding architectures is possibly the hottest topic in SSL for  https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/  Self-supervised learning: The dark matter of intelligence   vision at the moment. The domain is still largely unexplored, but it seems very promising", "d731604e-e872-456b-b186-c44d7e8c4dab": "In the latter case, it is not clear that the hard assignment to the nearest cluster is the most appropriate. We shall see in the next section that by adopting a probabilistic approach, we obtain \u2018soft\u2019 assignments of data points to clusters in a way that re\ufb02ects the level of uncertainty over the most appropriate assignment. This probabilistic formulation brings with it numerous bene\ufb01ts. As an illustration of the application of the K-means algorithm, we consider the related problems of image segmentation and image compression.\n\nThe goal of segmentation is to partition an image into regions each of which has a reasonably homogeneous visual appearance or which corresponds to objects or parts of objects . Each pixel in an image is a point in a 3-dimensional space comprising the intensities of the red, blue, and green channels, and our segmentation algorithm simply treats each pixel in the image as a separate data point. Note that strictly this space is not Euclidean because the channel intensities are bounded by the interval . Nevertheless, we can apply the K-means algorithm without dif\ufb01culty", "e5a137c7-bdfe-4b39-9392-2cf5c1b6fcc9": "If you are already familiar with probability theory and information theory, you may wish to skip this chapter except for section 3.14, which describes the graphs we use to describe structured probabilistic models for machine learning.\n\nIf you have absolutely no prior experience with these subjects, this chapter should be sufficient to successfully carry out deep learning research projects, but we do suggest that you consult an additional resource, such as Jaynes . https://www.deeplearningbook.org/contents/prob.html    51  CHAPTER 3. PROBABILITY AND INFORMATION THEORY  3.1 Why Probability? Many branches of computer science deal mostly with entities that are entirely deterministic and certain. A programmer can usually safely assume that a CPU will execute each machine instruction flawlessly. Errors in hardware do occur but are rare enough that most software applications do not need to be designed to account for them. Given that many computer scientists and software engineers work in a relatively clean and certain environment, it can be surprising that machine learning makes heavy use of probability theory. Machine learning must always deal with uncertain quantities and sometimes stochastic (nondeterministic) quantities. Uncertainty and stochasticity can arise from many sources. Researchers have made compelling arguments for quantifying uncertainty using probability since at least the 1980s", "cbc329fd-9b52-4f8b-a9f5-4982e4a9552d": "In a general feed-forward network, each unit computes a weighted sum of its inputs of the form aj = \ufffd where zi is the activation of a unit, or input, that sends a connection to unit j, and wji is the weight associated with that connection. In Section 5.1, we saw that biases can be included in this sum by introducing an extra unit, or input, with activation \ufb01xed at +1. We therefore do not need to deal with biases explicitly. The sum in (5.48) is transformed by a nonlinear activation function h(\u00b7) to give the activation zj of unit j in the form zj = h(aj). (5.49) Note that one or more of the variables zi in the sum in (5.48) could be an input, and similarly, the unit j in (5.49) could be an output. For each pattern in the training set, we shall suppose that we have supplied the corresponding input vector to the network and calculated the activations of all of the hidden and output units in the network by successive application of (5.48) and (5.49). This process is often called forward propagation because it can be regarded as a forward \ufb02ow of information through the network", "e52781e9-216f-45eb-a189-4690dacf0024": ", but the system has improved substantially after publication.\n\ntuning data, we only lose 0.1-0.4 F1, still outperforming all existing systems by a wide margin.12 The SQuAD 2.0 task extends the SQuAD 1.1 problem de\ufb01nition by allowing for the possibility that no short answer exists in the provided paragraph, making the problem more realistic. We use a simple approach to extend the SQuAD v1.1 BERT model for this task. We treat questions that do not have an answer as having an answer span with start and end at the  token. The probability space for the start and end answer span positions is extended to include the position of the  token. For prediction, we compare the score of the no-answer span: snull = S\u00b7C + E\u00b7C to the score of the best non-null span 12The TriviaQA data we used consists of paragraphs from TriviaQA-Wiki formed of the \ufb01rst 400 tokens in documents, that contain at least one of the provided possible answers. \u02c6 si,j = maxj\u2265iS\u00b7Ti + E\u00b7Tj", "80e29c94-3f32-492c-ac67-a7210792ef95": "5.1 Learning Algorithms  A machine learning algorithm is an algorithm that is able to learn from data. But what do we mean by learning?\n\nMitchell  provides a succinct definition: \u201cA computer program is said to learn from experience F with respect to some class of tasks T and performance measure P, if its performance at tasks in T\u2019, as measured by P, improves with experience EF.\u201d One can imagine a wide variety of experiences E, tasks T\u2019, and performance measures P, and we do not attempt in this book to formally define what may be used for each of these entities. Instead, in the following sections, we provide intuitive descriptions and examples of the different kinds of tasks, performance measures, and experiences that can be used to construct machine learning algorithms. 5.1.1 The Task, T  Machine learning enables us to tackle tasks that are too difficult to solve with fixed programs written and designed by human beings. From a scientific and philosophical point of view, machine learning is interesting because developing our understanding of it entails developing our understanding of the principles that underlie intelligence. In this relatively formal definition of the word \u201ctask,\u201d the process of learning itself is not the task. Learning is our means of attaining the ability to perform the task", "c8697001-4d7a-4ce6-878e-1276e7cd8bdd": "Dropout thus regularizes each hidden unit to be not merely a good feature but a feature that is good in many contexts. Warde-Farley ef al. compared dropout training to training of large ensembles and concluded that dropout offers additional improvements to generalization error beyond those obtained by ensembles of independent models. It is important to understand that a large portion of the power of dropout arises from the fact that the masking noise is applied to the hidden units.\n\nThis can be seen as a form of highly intelligent, adaptive destruction of the information content of the input rather than destruction of the raw values of the input. For example, if the model learns a hidden unit h; that detects a face by finding the nose, then dropping h; corresponds to erasing the information that there is a nose in the image. The model must learn another h;, that either redundantly encodes  https://www.deeplearningbook.org/contents/regularization.html    the presence ot a nose or detects the tace by another feature, such as the mouth", "bd4df9df-3cec-46c9-bab3-86e6d940345a": "For example, in card games with imperfect information the optimal play is often to do two di\u21b5erent things with speci\ufb01c probabilities, such as when blu\ufb03ng in Poker. Action-value methods have no natural way of \ufb01nding stochastic optimal policies, whereas policy approximating methods can, as shown in Example 13.1. Consider the small corridor gridworld shown inset in the graph below. The reward is \u22121 per step, as usual. In each of the three nonterminal states there are only two actions, right and left.\n\nThese actions have their usual consequences in the \ufb01rst and third states (left causes no movement in the \ufb01rst state), but in the second state they are reversed, so that right moves to the left and left moves to the right. The problem is di\ufb03cult because all the states appear identical under the function approximation. In particular, we de\ufb01ne x(s, right) = > and x(s, left) = >, for all s. An action-value method with \"-greedy action selection is forced to choose between just two policies: choosing right with high probability 1 \u2212 \"/2 on all steps or choosing left with the same high probability on all time steps", "f458fdcb-db6e-4dfa-a400-9ae82a0dd0bc": "Some singular values remain above 1, however, because the reconstruction error penalty encourages the CAE to encode the directions with the most local variance. The directions corresponding to the largest singular values are interpreted as the tangent directions that the contractive autoencoder has learned. Ideally, these tangent directions should correspond to real variations in the data. For example, a CAE applied to images should learn tangent vectors that show how the image changes as objects in the image gradually change pose, as shown in figure 14.6. Visualizations of the experimentally obtained singular vectors do seem to correspond to meaningful transformations of the input image, as shown in figure 14.10. One practical issue with the CAE regularization criterion is that although it is cheap to compute in the case of a single hidden layer autoencoder, it becomes much more expensive in the case of deeper autoencoders. The strategy followed by Rifai et al. (201 1a) is to separately train a series of single-layer autoencoders, each trained to reconstruct the previous autoencoder\u2019s hidden layer", "bf8be646-6fe9-461d-b64a-eec5dab53397": "PROBABILITY AND INFORMATION THEORY  1.0 0.8 0.6 0.4 0.2 0.0  \u201410 \u20145 0 5 10  Figure 3.3: The logistic sigmoid function. 10  https://www.deeplearningbook.org/contents/prob.html    \u201410 \u20145 0 5 10  Figure 3.4: The softplus function. 67  CHAPTER 3.\n\nPROBABILITY AND INFORMATION THEORY  them: o(a) = ETO (3.33) \u00a3 a(x) = 0(2)(1 ~o(2)) (3.34) 1 = o(x) = o(\u2014\u00ab) 3.35 log a(\u00ab) = \u2014\u00a2(\u20142) (3.36) car) = 0(0) (3.37) Vx \u20ac (0,1), 0 '(e) = tox (>=) (3.38) Va > 0, \u00a2~1(x) = log (exp(a) \u2014 1) (3.39) cn) =f oway (3.40) ((@) \u2014\u00a2(\u20142) =x (3.41)  The function g~! (z) is called the logit in statistics, but this term is rarely used in machine learning", "2ee0030d-650c-4d21-ab7b-f9339e8538a8": "(Though in this discussion, we will use the term trial instead of episode to relate better to the experiments.) As usual, we also need to make an assumption about how states are represented as inputs to the learning algorithm, an assumption that in\ufb02uences how closely the TD error corresponds to dopamine neuron activity. We discuss this issue later, but for now we assume the same CSC representation used by Montague et al. in which there is a separate internal stimulus for each state visited at each time step in a trial. This reduces the process to the tabular case covered in the \ufb01rst part of this book.\n\nFinally, we assume that the agent uses TD(0) to learn a value function, V , stored in a lookup table initialized to be zero for all the states. We also assume that this is a deterministic task and that the discount factor, \u03b3, is very nearly one so that we can ignore it. policy-evaluation task. The time axes represent the time interval over which a sequence of states is visited in a trial (where for clarity we omit showing individual states)", "4b07b2ee-66e5-4097-ad84-c3871c9fe7fd": "Using parallel computers or special purpose hardware is one approach; another is the use of special multi-dimensional data structures to store the training data. One data structure studied for this application is the k-d tree (short for k-dimensional tree), which recursively splits a k-dimensional space into regions arranged as nodes of a binary tree. Depending on the amount of data and how it is distributed over the state space, nearest-neighbor search using k-d trees can quickly eliminate large regions of the space in the search for neighbors, making the searches feasible in some problems where naive searches would take too long.\n\nLocally weighted regression additionally requires fast ways to do the local regression computations which have to be repeated to answer each query. Researchers have developed many ways to address these problems, including methods for forgetting entries in order to keep the size of the database within bounds. The Bibliographic and Historical Comments section at the end of this chapter points to some of the relevant literature, including a selection of papers describing applications of memory-based learning to reinforcement learning", "f603b1d0-22bf-4a33-866e-b7fba5cfab3e": "Training with a supervised criterion naturally leads to the representation at every hidden layer (but more so near the top hidden layer) taking on properties that make the classification ask easier.\n\nFor example, classes that were not linearly separable in the input features may become linearly separable in the last hidden layer. In principle, the last layer could be another kind of model, such as a nearest neighbor classifier Salakhutdinov and Hinton, 2007a). The features in the penultimate layer should earn different properties depending on the type of the last layer. Supervised training of feedforward networks does not involve explicitly imposing any condition on the learned intermediate features. Other kinds of representation learning algorithms are often explicitly designed to shape the representation in some particular way. For example, suppose we want to learn a representation that makes density estimation easier. Distributions with more independences are easier to model, so we could design an objective function that encourages the elements of the representation vector h to be independent. Just like supervised networks, unsupervised deep learning algorithms have a main training objective but also learn a representation as a side effect. Regardless of how a representation was obtained, it can be used for another task", "dadd184b-15e4-4718-b5c8-462439a18056": "Classical DP methods operate in sweeps through the state set, performing an expected update operation on each state. Each such operation updates the value of one state based on the values of all possible successor states and their probabilities of occurring. Expected updates are closely related to Bellman equations: they are little more than these equations turned into assignment statements. When the updates no longer result in any changes in value, convergence has occurred to values that satisfy the corresponding Bellman equation. Just as there are four primary value functions (v\u21e1, v\u21e4, q\u21e1, and q\u21e4), there are four corresponding Bellman equations and four corresponding expected updates. An intuitive view of the operation of DP updates is given by their backup diagrams.\n\nInsight into DP methods and, in fact, into almost all reinforcement learning methods, can be gained by viewing them as generalized policy iteration (GPI). GPI is the general idea of two interacting processes revolving around an approximate policy and an approximate value function. One process takes the policy as given and performs some form of policy evaluation, changing the value function to be more like the true value function for the policy", "f2184569-4dbc-487d-878d-e723965dc118": "The distribution of y will be governed by where, in this case, p(z) = 1.\n\nOur goal is to choose the function f(z) such that the resulting values of y have some speci\ufb01c desired distribution p(y). Integrating (11.5) we obtain which is the inde\ufb01nite integral of p(y). Thus, y = h\u22121(z), and so we have to Exercise 11.2 transform the uniformly distributed random numbers using a function which is the inverse of the inde\ufb01nite integral of the desired distribution. This is illustrated in Figure 11.2. Consider for example the exponential distribution where 0 \u2a7d y < \u221e. In this case the lower limit of the integral in (11.6) is 0, and so h(y) = 1 \u2212 exp(\u2212\u03bby). Thus, if we transform our uniformly distributed variable z using y = \u2212\u03bb\u22121 ln(1 \u2212 z), then y will have an exponential distribution. Another example of a distribution to which the transformation method can be applied is given by the Cauchy distribution In this case, the inverse of the inde\ufb01nite integral can be expressed in terms of the \u2018tan\u2019 function", "cf290279-6aeb-4795-b848-d9f13c7bcfca": "In this way, the two-dimensional nonlinear manifold for the laminar con\ufb01guration is broken into six distinct segments. Note also that some of the manifolds for different phase con\ufb01gurations meet at speci\ufb01c points, for example if the pipe is \ufb01lled entirely with oil, it corresponds to speci\ufb01c instances of the laminar, annular, and homogeneous con\ufb01gurations. Old Faithful, shown in Figure A.4, is a hydrothermal geyser in Yellowstone National Park in the state of Wyoming, U.S.A., and is a popular tourist attraction. Its name stems from the supposed regularity of its eruptions.\n\nThe data set comprises 272 observations, each of which represents a single eruption and contains two variables corresponding to the duration in minutes of the eruption, and the time until the next eruption, also in minutes. Figure A.5 shows a plot of the time to the next eruption versus the duration of the eruptions. It can be seen that the time to the next eruption varies considerably, although knowledge of the duration of the current eruption allows it to be predicted more accurately. Note that there exist several other data sets relating to the eruptions of Old Faithful", "26db7921-4e60-457d-bdee-c42353af18e5": "Related are also encoder-decoder architectures such as the predictive sparse decomposition (PSD) , from which we drew some inspiration.\n\nAlso relevant are the recently introduced Generative Stochastic Networks  where noisy auto-encoders learn the transition operator of a Markov chain that samples from the data distribution. In  a recognition model was employed for ef\ufb01cient learning with Deep Boltzmann Machines. These methods are targeted at either unnormalized models (i.e. undirected models like Boltzmann machines) or limited to sparse coding models, in contrast to our proposed algorithm for learning a general class of directed probabilistic models. The recently proposed DARN method , also learns a directed probabilistic model using an auto-encoding structure, however their method applies to binary latent variables. Even more recently,  also make the connection between auto-encoders, directed proabilistic models and stochastic variational inference using the reparameterization trick we describe in this paper. Their work was developed independently of ours and provides an additional perspective on AEVB", "f04e2b2c-a2fc-4453-b168-8292f85c3795": "If the action space is discrete and not too large, then a natural and common kind of parameterization is to form parameterized numerical preferences h(s, a, \u2713) 2 R for each state\u2013action pair.\n\nThe actions with the highest preferences in each state are given the highest probabilities of being selected, for example, according to an exponential soft-max distribution: where e \u21e1 2.71828 is the base of the natural logarithm. Note that the denominator here is just what is required so that the action probabilities in each state sum to one. We call this kind of policy parameterization soft-max in action preferences. The action preferences themselves can be parameterized arbitrarily. For example, they might be computed by a deep arti\ufb01cial neural network (ANN), where \u2713 is the vector of all the connection weights of the network (as in the AlphaGo system described in Section 16.6). Or the preferences could simply be linear in features, One advantage of parameterizing policies according to the soft-max in action preferences is that the approximate policy can approach a deterministic policy, whereas with \"-greedy action selection over action values there is always an \" probability of selecting a random action", "f36e9d8f-50bd-4dab-926b-ec68f8aa0d8f": "Klopf\u2019s idea of eligibility is that synapses are temporarily marked as eligible for modi\ufb01cation if they participated in the neuron\u2019s \ufb01ring (making this the contingent form of eligibility trace). A synapse\u2019s e\ufb03cacy is modi\ufb01ed if a reinforcing signal arrives while the synapse is eligible. We alluded to the chemotactic behavior of a bacterium as an example of a single cell that directs its movements in order to seek some molecules and to avoid others. A conspicuous feature of the dopamine system is that \ufb01bers releasing dopamine project widely to multiple parts of the brain. Although it is likely that only some populations of dopamine neurons broadcast the same reinforcement signal, if this signal reaches the synapses of many neurons involved in actor-type learning, then the situation can be modeled as a team problem", "e452ff0a-c551-476d-ad9c-4f7fb0023367": "(19.37)  Oh We can then iteratively apply the solution to the equation for i = 1,...,m,  and repeat the cycle until we satisfy a convergence criterion.\n\nCommon convergence criteria include stopping when a full cycle of updates does not improve \u00a3 by more than some tolerance amount, or when the cycle does not change h by more than some amount. Iterating mean field fixed-point equations is a general technique that can provide fast variational inference in a broad variety of models. To make this more concrete, we show how to derive the updates for the binary sparse coding model in particular. First, we must write an expression for the derivatives with respect to hj. To do so, we substitute equation 19.36 into the left side of equation 19.37:  a", "99082d1f-72f9-4629-a159-ad8896f95633": "Appendix B If we substitute (10.64), (10.65), and (10.66) into (10.46) and make use of (10.49), we obtain the following result for the responsibilities Notice the similarity to the corresponding result for the responsibilities in maximum likelihood EM, which from (9.13) can be written in the form where we have used the precision in place of the covariance to highlight the similarity to (10.67).\n\nThus the optimization of the variational posterior distribution involves cycling between two stages analogous to the E and M steps of the maximum likelihood EM algorithm. In the variational equivalent of the E step, we use the current distributions over the model parameters to evaluate the moments in (10.64), (10.65), and (10.66) and hence evaluate E = rnk. Then in the subsequent variational equivalent of the M step, we keep these responsibilities \ufb01xed and use them to re-compute the variational distribution over the parameters using (10.57) and (10.59). In each case, we see that the variational posterior distribution has the same functional form as the corresponding factor in the joint distribution (10.41). This is a general result and is a consequence of the choice of conjugate distributions", "c8537ed5-fb1a-437e-8f27-a3b9718bc579": "Daw and Shohamy  proposed that while dopamine signaling connects well to habitual, or model-free, behavior, other processes are involved in goal-directed, or model-based, behavior. Data from experiments by Bromberg-Martin, Matsumoto, Hong, and Hikosaka  indicate that dopamine signals contain information pertinent to both habitual and goal-directed behavior. Doll, Simon, and Daw  argued that there may not a clear separation in the brain between mechanisms that subserve habitual and goal-directed learning and choice. 15.12 Kei\ufb02in and Janak  reviewed connections between TD errors and addiction. Nutt, Lingford-Hughes, Erritzoe, and Stokes  critically evaluated the hypothesis that addiction is due to a disorder of the dopamine system.\n\nMontague, Dolan, Friston, and Dayan  outlined the goals and early e\u21b5orts in the \ufb01eld of computational psychiatry, and Adams, Huys, and Roiser  reviewed more recent progress. In this chapter we present a few case studies of reinforcement learning. Several of these are substantial applications of potential economic signi\ufb01cance. One, Samuel\u2019s checkers player, is primarily of historical interest", "273cdcaa-be49-4b15-8a4d-9ec0b3595175": "Rather than outputtingY in a single shot, the recurrent network iteratively refines its estimate Y by using a previous estimate of Y as input for creating a new estimate. The same parameters are used for each updated estimate, and the estimate can be refined as many times as we wish. The tensor of convolution kernels U is used on each step to compute the hidden representation given the input image. The kernel tensor V is used to produce an estimate of the labels given the hidden values.\n\nOn all but the first step, the kernels W are convolved over Y to provide input to the hidden layer. On the first time step, this term is replaced by zero. Because the same parameters are used on each step, this is an example of a recurrent network, as described in chapter 10.  kind of recurrent network . Figure 9.17 shows the architecture of such a recurrent convolutional network. Once a prediction for each pixel is made, various methods can be used to further process these predictions to obtain a segmentation of the image into regions . The general idea is to assume that large groups of contiguous pixels tend to be associated with the same label", "12638450-fa07-423a-a44a-b5360d1217c3": "\u2022 In Section 2, we provide a comprehensive theoretical analysis of how the Earth Mover (EM) distance behaves in comparison to popular probability distances and divergences used in the context of learning distributions. \u2022 In Section 3, we de\ufb01ne a form of GAN called Wasserstein-GAN that minimizes a reasonable and e\ufb03cient approximation of the EM distance, and we theoretically show that the corresponding optimization problem is sound.\n\nIn Section 4, we empirically show that WGANs cure the main training problems of GANs. In particular, training WGANs does not require maintaining a careful balance in training of the discriminator and the generator, and does not require a careful design of the network architecture either. The mode dropping phenomenon that is typical in GANs is also drastically reduced. One of the most compelling practical bene\ufb01ts of WGANs is the ability to continuously estimate the EM distance by training the discriminator to optimality. Plotting these learning curves is not only useful for debugging and hyperparameter searches, but also correlate remarkably well with the observed sample quality. We now introduce our notation", "115fdbe5-ee1c-46e5-8d0d-7f3ccccb2a22": "We refer the reader to  for more details on these tricks.\n\nTo provide a quantitative comparison of the DA methods we have surveyed, we experiment with 10 of the most commonly used and model-agnostic augmentation techniques from different levels in Table 1, including: (i) Token-level augmentation: Synonym Replacement (SR) (Kolomiyets et al., 2011; Yang, 2015), Word Replacement based on Language Model , Random Insertion (RI) , Random Deletion (RD) , Random Swapping (RS) , and Word Replacement (WR) based on TF-IDF in Vocabulary Set ; (ii) Sentence-level augmentation: Roundtrip Translation (RT) ; (iii) Hidden-space Augmentation: Adversarial training (ADV) , Cutoff , and Mixup in the embedding space . Most aforementioned techniques are not label-dependent (except mixup), thus can be applied directly to unlabeled data", "dd71b353-ea22-4b4b-8add-7e12eb3a7e5d": "Taking the transpose of the second of these equations, and using AT = A, we see that the left-hand sides of the two equations are equal, and hence that \u03bb\u22c6 i = \u03bbi and so \u03bbi must be real.\n\nThe eigenvectors ui of a real symmetric matrix can be chosen to be orthonormal (i.e., orthogonal and of unit length) so that and hence, by exchange of indices, we have We now take the transpose of the second equation and make use of the symmetry property AT = A, and then subtract the two equations to give Hence, for \u03bbi \u0338= \u03bbj, we have uT i uj = 0, and hence ui and uj are orthogonal. If the two eigenvalues are equal, then any linear combination \u03b1ui + \u03b2uj is also an eigenvector with the same eigenvalue, so we can select one linear combination arbitrarily, and then choose the second to be orthogonal to the \ufb01rst (it can be shown that the degenerate eigenvectors are never linearly dependent)", "57ce51ce-170d-438d-b169-f911f015da86": "Fortunately, GPI does not require that the policy be taken all the way to a greedy policy, only that it be moved toward a greedy policy. In our on-policy method we will move it only to an \"-greedy policy. For any \"-soft policy, \u21e1, any \"-greedy policy with respect to q\u21e1 is guaranteed to be better than or equal to \u21e1. The complete algorithm is given in the box below. On-policy \ufb01rst-visit MC control (for \"-soft policies), estimates \u21e1 \u21e1 \u21e1\u21e4 That any \"-greedy policy with respect to q\u21e1 is an improvement over any \"-soft policy \u21e1 is assured by the policy improvement theorem. Let \u21e10 be the \"-greedy policy. The conditions of the policy improvement theorem apply because for any s 2 S: (the sum is a weighted average with nonnegative weights summing to 1, and as such it must be less than or equal to the largest number averaged) Thus, by the policy improvement theorem, \u21e10 \u2265 \u21e1 (i.e., v\u21e10(s) \u2265 v\u21e1(s), for all s 2 S)", "4c0190eb-b028-49c2-8ec3-2021f2cedfef": "This is sometimes desirable, but it is problematic if the target policy \u21e1 changes as it does in reinforcement learning and GPI. In control applications, LSTD typically has to be combined with some other mechanism to induce forgetting, mooting any initial advantage of not requiring a step-size parameter. Choose and take action A \u21e0 \u21e1(\u00b7|S), observe R, S0; x0  x(S0) So far we have discussed the parametric approach to approximating value functions. In this approach, a learning algorithm adjusts the parameters of a functional form intended to approximate the value function over a problem\u2019s entire state space. Each update, s 7! g, is a training example used by the learning algorithm to change the parameters with the aim of reducing the approximation error. After the update, the training example can be discarded (although it might be saved to be used again).\n\nWhen an approximate value of a state (which we will call the query state) is needed, the function is simply evaluated at that state using the latest parameters produced by the learning algorithm. Memory-based function approximation methods are very di\u21b5erent", "60212576-b568-411b-b82b-b51e175a56b8": "Semi-supervised learning methods further reduce the dependency on labeled data and enhance the models when there is only limited labeled data available. These methods use large amounts of unlabeled data in the training process, as unlabeled data is usually cheap and easy to obtain compared to labeled data.\n\nIn this paper, we focus on consistency regularization, while there are also other widely-used methods for NLP including self-training , generative methods , and cotraining . unlabeled data as additional information, few-shot learning leverages various kinds of prior knowledge such as pre-trained models or supervised data from other domains and modalities . While most work on few-shot focuses on computer vision, few-shot learning has recently seen increasing adoption in NLP . To better leverage pre-trained models, PET (Schick and Sch\u00a8utze, 2021a,b) converts the text and label in an example into a \ufb02uent sentence, and then uses the probability of generating the label text as the class logit, outperforming GPT3 for few shot learning . How to better model and incorporate prior knowledge to handle few-shot learning for NLP remains an open challenge and has the potential to signi\ufb01cantly improve model performance with less labeled data", "ba890434-287e-45eb-aa14-2f7c825ed792": "Speci\ufb01cally, we always \ufb01rst randomly crop images and resize them to the same resolution, and we then apply the targeted transformation(s) only to one branch of the framework in Figure 2, while leaving the other branch as the identity (i.e. t(xi) = xi). Note that this asymmetA Simple Framework for Contrastive Learning of Visual Representations ric data augmentation hurts the performance. Nonetheless, this setup should not substantively change the impact of individual data augmentations or their compositions. One composition of augmentations stands out: random cropping and random color distortion.\n\nWe conjecture that one serious issue when using only random cropping as data augmentation is that most patches from an image share a similar color distribution. Figure 6 shows that color histograms alone suf\ufb01ce to distinguish images. Neural nets may exploit this shortcut to solve the predictive task. Therefore, it is critical to compose cropping with color distortion in order to learn generalizable features. 3.2", "7bdaec8d-94d2-4751-99e4-d9fceec25c76": "By selecting the order in which small updates are done it is possible to greatly improve planning e\ufb03ciency beyond that possible with prioritized sweeping. We have suggested in this chapter that all kinds of state-space planning can be viewed as sequences of value updates, varying only in the type of update, expected or sample, large or small, and in the order in which the updates are done.\n\nIn this section we have emphasized backward focusing, but this is just one strategy. For example, another would be to focus on states according to how easily they can be reached from the states that are visited frequently under the current policy, which might be called forward focusing. Peng and Williams  and Barto, Bradtke and Singh  have explored versions of forward focusing, and the methods introduced in the next few sections take it to an extreme form. The examples in the previous sections give some idea of the range of possibilities for combining methods of learning and planning. In the rest of this chapter, we analyze some of the component ideas involved, starting with the relative advantages of expected and sample updates. Much of this book has been about di\u21b5erent kinds of value-function updates, and we have considered a great many varieties. Focusing for the moment on one-step updates, they vary primarily along three binary dimensions", "da9561e1-c988-4713-90f8-8cf46b166a2d": "Until recently, most work on deep generative models focused on models that provided a parametric speci\ufb01cation of a probability distribution function. The model can then be trained by maximizing the log likelihood. In this family of model, perhaps the most succesful is the deep Boltzmann machine . Such models generally have intractable likelihood functions and therefore require numerous approximations to the likelihood gradient. These dif\ufb01culties motivated the development of \u201cgenerative machines\u201d\u2013models that do not explicitly represent the likelihood, yet are able to generate samples from the desired distribution.\n\nGenerative stochastic networks  are an example of a generative machine that can be trained with exact backpropagation rather than the numerous approximations required for Boltzmann machines. This work extends the idea of a generative machine by eliminating the Markov chains used in generative stochastic networks. Our work backpropagates derivatives through generative processes by using the observation that We were unaware at the time we developed this work that Kingma and Welling  and Rezende et al. had developed more general stochastic backpropagation rules, allowing one to backpropagate through Gaussian distributions with \ufb01nite variance, and to backpropagate to the covariance parameter as well as the mean", "5defdd3f-fd34-46cb-a6e0-65f67263e60f": "For now, we seek merely to illustrate the ideas and stimulate your intuition. Within a planning agent, there are at least two roles for real experience: it can be used to improve the model (to make it more accurately match the real environment) and it can be used to directly improve the value function and policy using the kinds of Both direct and indirect methods have advantages and disadvantages. Indirect methods often make fuller use of a limited amount of experience and thus achieve a better policy with fewer environmental interactions. On the other hand, direct methods are much simpler and are not a\u21b5ected by biases in the design of the model. Some have argued that indirect methods are always superior to direct ones, while others have argued that direct methods are responsible for most human and animal learning.\n\nRelated debates in psychology and arti\ufb01cial intelligence concern the relative importance of cognition as opposed to trial-and-error learning, and of deliberative planning as opposed to reactive decision making (see Chapter 14 for discussion of some of these issues from the perspective of psychology). Our view is that the contrast between the alternatives in all these debates has been exaggerated, that more insight can be gained by recognizing the similarities between these two sides than by opposing them", "31313ad0-aa55-448d-b1dc-cf579958faa8": "If the denominator is zero, then we instead de\ufb01ne Qt(a) as some default value, such as 0. As the denominator goes to in\ufb01nity, by the law of large numbers, Qt(a) converges to q\u21e4(a). We call this the sample-average method for estimating action values because each estimate is an average of the sample of relevant rewards. Of course this is just one way to estimate action values, and not necessarily the best one. Nevertheless, for now let us stay with this simple estimation method and turn to the question of how the estimates might be used to select actions. The simplest action selection rule is to select one of the actions with the highest estimated value, that is, one of the greedy actions as de\ufb01ned in the previous section. If there is more than one greedy action, then a selection is made among them in some arbitrary way, perhaps randomly. We write this greedy action selection method as where argmaxa denotes the action a for which the expression that follows is maximized (again, with ties broken arbitrarily)", "bbcf5737-d4b6-4298-8f23-d29ea2750f2d": "Animal experiments showed that if conditions favor the development of conditioned reinforcement during a delay period, learning does not decrease with increased delay as much as it does under conditions that obstruct secondary reinforcement. Conditioned reinforcement is favored if there are stimuli that regularly occur during the delay interval. Then it is as if reward is not actually delayed because there is more immediate conditioned reinforcement. Hull therefore envisioned that there is a primary gradient based on the delay of the primary reinforcement mediated by stimulus traces, and that this is progressively modi\ufb01ed, and lengthened, by conditioned reinforcement.\n\nAlgorithms presented in this book that use both eligibility traces and value functions to enable learning with delayed reinforcement correspond to Hull\u2019s hypothesis about how animals are able to learn under these conditions. The actor\u2013critic architecture discussed in Sections 13.5, 15.7, and 15.8 illustrates this correspondence most clearly. The critic uses a TD algorithm to learn a value function associated with the system\u2019s current behavior, that is, to predict the current policy\u2019s return. The actor updates the current policy based on the critic\u2019s predictions, or more exactly, on changes in the critic\u2019s predictions", "4c12e185-0b06-483f-a77b-6c977a6362b4": "\u21e4 Exercise 7.2 (programming) With an n-step method, the value estimates do change from step to step, so an algorithm that used the sum of TD errors (see previous exercise) in The n-step return uses the value function Vt+n\u22121 to correct for the missing rewards beyond Rt+n. An important property of n-step returns is that their expectation is guaranteed to be a better estimate of v\u21e1 than Vt+n\u22121 is, in a worst-state sense. That is, the worst error of the expected n-step return is guaranteed to be less than or equal to \u03b3n for all n \u2265 1. This is called the error reduction property of n-step returns. Because of the error reduction property, one can show formally that all n-step TD methods converge to the correct predictions under appropriate technical conditions.\n\nThe n-step TD methods thus form a family of sound methods, with one-step TD methods and Monte Carlo methods as extreme members. TD methods on the 5-state random walk task described in Example 6.2 (page 125)", "631ea8dc-1d2e-4e43-93dd-eab7d7f806d1": "In this case, we select aq that has low probability where p has low probability. When p has multiple modes that are sufficiently widely separated, as in this figure, the KL divergence is minimized by choosing a single mode, to avoid putting probability mass in the low-probability areas between modes ofp. Here, we illustrate the outcome when q is chosen to emphasize the left mode. We could also have achieved an equal value of the KL divergence by choosing the right mode. If the modes are not separated by a sufficiently strong low-probability region, then this direction of the KL divergence can still choose to blur the modes. 74  CHAPTER 3. PROBABILITY AND INFORMATION THEORY  https://www.deeplearningbook.org/contents/prob.html    Instead of using a single function to represent a probability distribution, we can split a probability distribution into many factors that we multiply together. For example, suppose we have three random variables: a, b and c", "a7d191d4-2117-4584-8187-7da15bf132b6": "To maintain the expressive  power of the network, it is common to replace the batch of hidden unit activations H with yH\u2019 + @ rather than simply the normalized H\u2019. The variables y and 3 are learned parameters that allow the new variable to have any mean and standard deviation. At first glance, this may seem useless\u2014why did we set the mean to 0, and then introduce a parameter that allows it to be set back to any arbitrary value B?\n\nThe answer is that the new parametrization can represent the same family of functions of the input as the old parametrization, but the new parametrization has different learning dynamics. In the old parametrization, the mean of H was determined by a complicated interaction between the parameters in the layers below H. In the new parametrization, the mean of yH'+ @ is determined solely by @. The new parametrization is much easier to learn with gradient descent. Most neural network layers take the form of \u00a2(XW +b), where \u00a2 is some fixed nonlinear activation function such as the rectified linear transformation. It is natural to wonder whether we should apply batch normalization to the input X, or to the transformed value XW +", "6b26aede-a99d-41f3-8141-bd424dab19e8": "When comparing networks having different numbers of hidden units, this can be taken into account by multiplying the evidence by a factor of M!2M. So far, we have used the Laplace approximation to develop a Bayesian treatment of neural network regression models. We now discuss the modi\ufb01cations to this framework that arise when it is applied to classi\ufb01cation. Here we shall consider a network having a single logistic sigmoid output corresponding to a two-class classi\ufb01cation problem.\n\nThe extension to networks with multiclass softmax outputs is straightforward. We shall build extensively on the analogous results for linear Exercise 5.40 classi\ufb01cation models discussed in Section 4.5, and so we encourage the reader to familiarize themselves with that material before studying this section. The log likelihood function for this model is given by where tn \u2208 {0, 1} are the target values, and yn \u2261 y(xn, w). Note that there is no hyperparameter \u03b2, because the data points are assumed to be correctly labelled. As before, the prior is taken to be an isotropic Gaussian of the form (5.162)", "8747de2e-8151-44df-8bc0-e23e88a48c2e": "Regardless of which of these problems are most significant, all of them might be avoided if there exists a region of space connected reasonably directly to a solution by a path that local descent can follow, and if we are able to initialize learning within that well-behaved region. This last view suggests research into choosing good initial points for traditional optimization algorithms to use. 8.2.8 Theoretical Limits of Optimization  Several theoretical results show that there are limits on the performance of any optimization algorithm we might design for neural networks . Typically these results have little bearing on the use of neural networks in practice. 289  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  Some theoretical results apply only when the units of a neural network output discrete values. Most neural network units output smoothly increasing values that make optimization via local search feasible. Some theoretical results show that there exist problem classes that are intractable, but it can be difficult to tell whether a particular problem falls into that class.\n\nOther results show that finding a solution for a network of a given size is intractable, but in practice we can find a solution easily by using a larger network for which many more parameter settings correspond to an acceptable solution", "6590be85-1ceb-4b22-91aa-91fe02c76683": "e Karma point: Any other act that improves the class, which the TA or instructor notices and deems worthy 1%. Homework Assignments  There will be 2 homework assignments over the course of the quarter.\n\nThe students are required to typeset homework solutions using I{T\u2014:X the provided template. DSC291-Winter2023  Logistics Lectures Homework Project  sure you personally unaerstana tne SOIUUON arising Irom SUCN COllabOralion. YOU alSO MUST indicate on each homework with whom you have collaborated. Late Policy  You will be allowed 3 total homework late days without penalty for the entire quarter. You may be late by up to 3 days on any homework assignment. Once those days are used, you will be penalized according to the following policy:  e Homework is worth full credit at the due time on the due date. e The allowed late days are counted by day (i.e., each new late day starts at 12:00 am PT). e The homeword is worth 80% credit 0-24 hours after exceeding the late day limit. e The homework is worth 50% credit 24-48 hours after exceeding the late day limit", "2d622efe-16a7-4dc8-ae23-c34bdecaaaad": "We may concatenate all the eigenvectors to form a matrix V with one eigenvector per column: V = fo, weey ov], Likewise, we can concatenate the eigenvalues to form a vector A = [A\\y,...,  40  https://www.deeplearningbook.org/contents/linear_algebra.html    CHAPTER 2, LINEAR ALGEBRA  An] '. The eigendecomposition of A is then given by A=Vdiag(A)V 1. (2.40)  We have seen that constructing matrices with specific eigenvalues and eigen- vectors enables us to stretch space in desired directions. Yet we often want to decompose matrices into their eigenvalues and eigenvectors. Doing so can help us analyze certain properties of the matrix, much as decomposing an integer into its prime factors can help us understand the behavior of that integer. Not every matrix can be decomposed into eigenvalues and eigenvectors. In some cases, the decomposition exists but involves complex rather than real numbers", "0ae9c115-f23d-497a-a3f1-054cf96fe60e": "In: Meeting of the Association for Computational Linguistics (ACL)  4.\n\nBach, S., Rodriguez, D., Liu, Y., Luo, C., Shao, H., Xia, C., Sen, S., Ratner, A., Hancock, B., Alborzi, H., Kuchhal, R., R\u00e9 C, Snorkel, Malkin, R.: drybell: A case study in deploying weak supervision at industrial scale. Arxiv  5. Bach, S.H., He, B., Ratner, A., R\u00e9, C.: Learning the structure of generative models without labeled data. In: International Conference on Machine Learning (ICML)  6. Blum, A., Mitchell, T.: Combining labeled and unlabeled data with co-training. In: Workshop on Computational Learning Theory (COLT)  7. Bunescu, R.C., Mooney, R.J.: Learning to extract relations from the Web using minimal supervision. In: Meeting of the Association for Computational Linguistics (ACL)  8", "411033cd-a977-415d-9f0b-a386a53fc7ac": "distribution by multiplying by the likelihood function for the new observation and then normalizing to obtain the new, revised posterior distribution.\n\nAt each stage, the posterior is a beta distribution with some total number of (prior and actual) observed values for x = 1 and x = 0 given by the parameters a and b. Incorporation of an additional observation of x = 1 simply corresponds to incrementing the value of a by 1, whereas for an observation of x = 0 we increment b by 1. Figure 2.3 illustrates one step in this process. We see that this sequential approach to learning arises naturally when we adopt a Bayesian viewpoint. It is independent of the choice of prior and of the likelihood function and depends only on the assumption of i.i.d. data. Sequential methods make use of observations one at a time, or in small batches, and then discard them before the next observations are used. They can be used, for example, in real-time learning scenarios where a steady stream of data is arriving, and predictions must be made before all of the data is seen. Because they do not require the whole data set to be stored or loaded into memory, sequential methods are also useful for large data sets", "4df8b67f-e1ff-4f43-861d-7cd208793eea": "There are L = 100 data sets, each having N = 25 data points, and there are 24 Gaussian basis functions in the model so that the total number of parameters is M = 25 including the bias parameter. The left column shows the result of \ufb01tting the model to the data sets for various values of ln \u03bb (for clarity, only 20 of the 100 \ufb01ts are shown). The right column shows the corresponding average of the 100 \ufb01ts (red) along with the sinusoidal function from which the data sets were generated (green).\n\n\ufb01t a model with 24 Gaussian basis functions by minimizing the regularized error function (3.27) to give a prediction function y(l)(x) as shown in Figure 3.5. The top row corresponds to a large value of the regularization coef\ufb01cient \u03bb that gives low variance (because the red curves in the left plot look similar) but high bias (because the two curves in the right plot are very different). Conversely on the bottom row, for which \u03bb is small, there is large variance (shown by the high variability between the red curves in the left plot) but low bias (shown by the good \ufb01t between the average model \ufb01t and the original sinusoidal function)", "e773572d-8494-4112-bc42-6174cb53ef1b": "The active supervision experience function is then de\ufb01ned as: where the \ufb01rst term is essentially the same as the supervised data experience function (Equation 4.2) with the only di\ufb00erence that now the label y\u2217 is from the oracle rather than pre-given in D; \u03bb > 0 is a trade-o\ufb00 parameter. The formulation of the active supervision is interesting as it is simply a combination of the common supervision experience and the informativeness measure in an additive manner.\n\nWe plug factive into the SE and obtain the algorithm to carry out learning. The result turns out to recover classical active learning algorithms. If the pool D is large, the update can be carried out by the following procedure: we \ufb01rst pick a random subset Dsub from D, and select a sample from Dsub according to the informativeness distribution proportional to exp{\u03bbu(x)} over Dsub. The sample is then labeled by the oracle, which is \ufb01nally used to update the target model. By setting \u03bb to a very large value (i.e., a near-zero 4.2. Knowledge-Based Experience. Many aspects of problem structures and human knowledge are di\ufb03cult if not impossible to be expressed through individual data instances", "a24e918f-785c-4ee3-8200-0d38f3ecb0d6": "More speci\ufb01cally, if we denote the probability C\u03c6(t) = exp f\u03c6(t), then the equation reduces to the familiar GAN objective in Equation 5.6. The results can be extended to the more general case of f-GAN : if we set D to an f-divergence and do not restrict the form (e.g., classi\ufb01er) of the experience function f\u03c6, then with mild conditions, the equation recovers the f-GAN algorithm. Now consider D as the \ufb01rst-order Wasserstein distance and suppose the f\u03c6-space F is a convex subset of 1-Lipschitz functions. It can be shown that Equation 6.4 reduces to the Wasserstein GAN algorithm as shown in Equation 5.8 where \u03d5 now corresponds to f\u03c6", "22039ae4-a4d3-42a2-afd6-9838b7c4f009": "+ xN, and for each observation the mean and variance are given by (2.3) and (2.4), respectively, we have We have seen in (2.8) that the maximum likelihood setting for the parameter \u00b5 in the Bernoulli distribution, and hence in the binomial distribution, is given by the fraction of the observations in the data set having x = 1. As we have already noted, this can give severely over-\ufb01tted results for small data sets. In order to develop a Bayesian treatment for this problem, we need to introduce a prior distribution p(\u00b5) over the parameter \u00b5. Here we consider a form of prior distribution that has a simple interpretation as well as some useful analytical properties. To motivate this prior, we note that the likelihood function takes the form of the product of factors of the form \u00b5x(1 \u2212 \u00b5)1\u2212x.\n\nIf we choose a prior to be proportional to powers of \u00b5 and (1 \u2212 \u00b5), then the posterior distribution, which is proportional to the product of the prior and the likelihood function, will have the same functional form as the prior. This property is called conjugacy and we will see several examples of it later in this chapter", "ebb9d934-7d4f-4044-b3b8-2408f7fb7f80": "Initially we restrict attention to on-policy training, treating in Chapter 9 the prediction case, in which the policy is given and only its value function is approximated, and then in Chapter 10 the control case, in which an approximation to the optimal policy is found. The challenging problem of o\u21b5-policy learning with function approximation is treated in Chapter 11. In each of these three chapters we will have to return to \ufb01rst principles and re-examine the objectives of the learning to take into account function approximation. Chapter 12 introduces and analyzes the algorithmic mechanism of eligibility traces, which dramatically improves the computational properties of multi-step reinforcement learning methods in many cases.\n\nThe \ufb01nal chapter of this part explores a di\u21b5erent approach to control, policy-gradient methods, which approximate the optimal policy directly and need never form an approximate value function (although they may be much more e\ufb03cient if they do approximate a value function as well the policy). In this chapter, we begin our study of function approximation in reinforcement learning by considering its use in estimating the state-value function from on-policy data, that is, in approximating v\u21e1 from experience generated using a known policy \u21e1", "c0310d2c-cc0a-4e8e-886e-15f1770bfe9e": "\u21e4 Exercise 4.7 (programming) Write a program for policy iteration and re-solve Jack\u2019s car rental problem with the following changes.\n\nOne of Jack\u2019s employees at the \ufb01rst location rides a bus home each night and lives near the second location. She is happy to shuttle one car to the second location for free. Each additional car still costs $2, as do all cars moved in the other direction. In addition, Jack has limited parking space at each location. If more than 10 cars are kept overnight at a location (after any moving of cars), then an additional cost of $4 must be incurred to use a second parking lot (independent of how many cars are kept there). These sorts of nonlinearities and arbitrary dynamics often occur in real problems and cannot easily be handled by optimization methods other than dynamic programming. To check your program, \ufb01rst replicate the results given for the original problem. \u21e4 One drawback to policy iteration is that each of its iterations involves policy evaluation, which may itself be a protracted iterative computation requiring multiple sweeps through the state set. If policy evaluation is done iteratively, then convergence exactly to v\u21e1 occurs only in the limit", "b657a56b-363d-4c81-ad06-83ea03ae2e5b": "These factors are just functions, not  75  https://www.deeplearningbook.org/contents/prob.html    CHAPTER 3. PROBABILITY AND INFORMATION THEORY  3.0 @)  O  Figure 3.7: A directed graphical model over random variablesa, b, c, dand e. This graph corresponds to probability distributions that can be factored as  pla, b,c, d,e) = p(a)p(b | a)p(c | a, b)p(d | b)p(e | c). (3.54)  This graphical model enables us to quickly see some properties of the distribution. For example, a and c interact directly, but a and e interact only indirectly via c.  probability distributions. The output of each factor must be non-negative, but here is no constraint that the factor must sum or integrate to 1 like a probability distribution.\n\nThe probability of a configuration of random variables is proportional to the product of all these factors\u2014assignments that result in larger factor values are more likely. Of course, there is no guarantee that this product will sum to 1", "3d37e183-bcf7-4986-8b89-97f8124cce76": "Compute a loss for representation learning; i.e., cross entropy for the verification task:  Lemb = 1 yy) log Py + (1 \u2014 1,y) log(1 \u2014 P:), where P; = o(W| fo(X(1)) \u2014 fo(X,2))|)  Compute the task-level fast weights: 0+ = F,,(VoLs\u2122, Lee La)  Next go through examples in the support set S and compute the example-level fast weights. Meanwhile, update the memory with learned representations. fori =1,...,K: a.\n\nThe base learner outputs a probability distribution: P(g;|x;) = g\u00a2(x;) and the loss can be cross-entropy or MSE: Lik = y' log gg(x/) + (1 \u2014 yj) log(1 \u2014 go(x})) b. Extract meta information (loss gradients) of the task and compute the example-level fast weights: \u00a27 = G,(V L's)  Then store \u00a2;\" into j-th location of the \u201cvalue\u201d memory M.  d", "924e4028-9619-45eb-8692-4d38d3cfa838": "We study on two tasks, E2E  and CommonGEN , and use the respective datasets pre-processed by  which allow sequence-tosequence modeling with standard transformers.\n\nWe run four sets of methods: the standard MLE training (MLE); PG training from scratch (PG); joint MLE and PG training, with MLE initialization (MLE+PG); and our SQL training from scratch with both off-policy and on-policy updates (SQL). We use the standard BLEU as reward. We additionally investigate the training stability and sensitivity w.r.t hyperparameters, in particular the scale of reward. To this end, for MLE+PG and SQL, we vary the reward scale in {1, 10, 50, 100, 500, 1000} and evaluate the respective performance under different scales. the proposed SQL that trains models from scratch achieves competitive results with the common MLE and MLE+PG. In contrast, the PG algorithm alone without MLE fails the training. Figure A.2 (left) shows the respective training curves (on the validation set), demonstrating that SQL converges in an ef\ufb01cient and stable way as MLE", "9823a2c3-55be-4182-a3c8-c565870417ca": "Compared to a convolutional network, RNNs applied to images are typically more expensive but allow for long-range lateral interactions between features in the same feature map . Indeed, the forward propagation equations for such RNNs may be written in a form that shows they use a convolution that computes the bottom-up input to each layer, prior to the recurrent propagation across the feature map that incorporates the lateral interactions. 10.4 Encoder-Decoder Sequence-to-Sequence Architectures  We have seen in figure 10.5 how an RNN can map an input sequence to a fixed-size vector. We have seen in figure 10.9 how an RNN can map a fixed-size vector to a sequence.\n\nWe have seen in figures 10.3, 10.4, 10.10 and 10.11 how an RNN can map an input sequence to an output sequence of the same length. Here we discuss how an RNN can be trained to map an input sequence to an output sequence which is not necessarily of the same length. This comes up in many applications, such as speech recognition, machine translation and question answering, where the input and output sequences in the training set are generally not of the same length (although their lengths might be related)", "43743173-5205-496c-821d-978b319a46cc": "Local and global optimization algorithms for generalized learning automata. Neural Computation, 7(5):950\u2013973. Poggio, T., Girosi, F. A theory of networks for approximation and learning. A.I. Memo Polyak, B. T. New stochastic approximation type procedures. Automat. i Telemekh, Polyak, B. T., Juditsky, A. B. Acceleration of stochastic approximation by averaging. SIAM Journal on Control and Optimization, 30(4):838\u2013855. Powell, M. J. D. Radial basis functions for multivariate interpolation: A review. In Powell, W. B. Approximate Dynamic Programming: Solving the Curses of Dimensionality, Powers, W. T. Behavior: The Control of Perception. Aldine de Gruyter, Chicago. 2nd Precup, D. Temporal Abstraction in Reinforcement Learning", "96277654-4aaf-4e20-b089-ca44ae1c33da": "Visualisation of high-dimensional data If we choose a low-dimensional latent space (e.g.\n\n2D), we can use the learned encoders (recognition model) to project high-dimensional data to a lowdimensional manifold. See appendix A for visualisations of the 2D latent manifolds for the MNIST and Frey Face datasets. We have introduced a novel estimator of the variational lower bound, Stochastic Gradient VB (SGVB), for ef\ufb01cient approximate inference with continuous latent variables. The proposed estimator can be straightforwardly differentiated and optimized using standard stochastic gradient methods. For the case of i.i.d. datasets and continuous latent variables per datapoint we introduce an ef\ufb01cient algorithm for ef\ufb01cient inference and learning, Auto-Encoding VB (AEVB), that learns an approximate inference model using the SGVB estimator. The theoretical advantages are re\ufb02ected in experimental results. Since the SGVB estimator and the AEVB algorithm can be applied to almost any inference and learning problem with continuous latent variables, there are plenty of future directions: (i) learning hierarchical generative architectures with deep neural networks (e.g", "8d20f9fb-d843-47de-8540-4e15ca1804a9": "This is true for RTDP as well: for episodic tasks with exploring starts, RTDP is an asynchronous value-iteration algorithm that converges to optimal policies for discounted \ufb01nite MDPs (and for the undiscounted case under certain conditions). Unlike the situation for a prediction problem, it is generally not possible to stop updating any state or state\u2013action pair if convergence to an optimal policy is important.\n\nThe most interesting result for RTDP is that for certain types of problems satisfying reasonable conditions, RTDP is guaranteed to \ufb01nd a policy that is optimal on the relevant states without visiting every state in\ufb01nitely often, or even without visiting some states at all. Indeed, in some problems, only a small fraction of the states need to be visited. This can be a great advantage for problems with very large state sets, where even a single sweep may not be feasible. The tasks for which this result holds are undiscounted episodic tasks for MDPs with absorbing goal states that generate zero rewards, as described in Section 3.4. At every step of a real or simulated trajectory, RTDP selects a greedy action (breaking ties randomly) and applies the expected value-iteration update operation to the current state", "f5cee849-6773-494d-abe6-4c1fe711b520": "At one extreme we have a fully connected graph that exhibits no conditional independence properties at all, and which can represent any possible joint probability distribution over the given variables. The set DF will contain all possible distributions p(x). At the other extreme, we have the fully disconnected graph, i.e., one having no links at all. This corresponds to joint distributions which factorize into the product of the marginal distributions over the variables comprising the nodes of the graph. Note that for any given graph, the set of distributions DF will include any distributions that have additional independence properties beyond those described by the graph. For instance, a fully factorized distribution will always be passed through the \ufb01lter implied by any graph over the corresponding set of variables. We end our discussion of conditional independence properties by exploring the concept of a Markov blanket or Markov boundary. Consider a joint distribution p(x1,", "2177dbb1-2b5f-44dc-bb2f-7fd22f456f18": "The unsupervised numbers on SentEval with IS-BERT outperforms most of the unsupervised baselines , but unsurprisingly weaker than supervised runs. When using labelled NLI datasets, IS-BERT produces results comparable with SBERT (See Fig. 25 & 30). Model MR CR_ SUBJ MPQA SST TREC MRPC | Avg. Using unlabeled data (unsupervised methods)  Unigram-TFIDFt 73.7 79.2 90.3 82.4 - 85.0 73.6 - SDAEt 74.6 78.0 90.8 86.9 - 78.4 73.7 - ParagraphVec DBOWt 60.2 669 763 70.7 - 59.4 72.9 - SkipThought? 76.5 80.1 93.6 87.1 82.0 92.2 73.0 | 83.50 FastSentt 70.8 78.4 88.7 80.6 76.8 72.2  Avg. GloVe embeddings? | 77.25 78.30 91.17 87.85 80.18 83.0 72.87 | 81.52 Avg", "ea4e2d1c-89ca-4870-b86e-5a543c581d61": "2.49 (\u22c6 \u22c6) By using the de\ufb01nition (2.161) of the multivariate Student\u2019s t-distribution as a convolution of a Gaussian with a gamma distribution, verify the properties (2.164), (2.165), and (2.166) for the multivariate t-distribution de\ufb01ned by (2.162). 2.50 (\u22c6) Show that in the limit \u03bd \u2192 \u221e, the multivariate Student\u2019s t-distribution (2.162) reduces to a Gaussian with mean \u00b5 and precision \u039b. in which i is the square root of minus one. By considering the identity prove the result (2.177). Similarly, using the identity where \u211c denotes the real part, prove (2.178). Finally, by using sin(A \u2212 B) = \u2111 exp{i(A \u2212 B)}, where \u2111 denotes the imaginary part, prove the result (2.183)", "9758205d-c2cd-4ec4-a1e6-4d1c0d6c2fa3": "The remaining \ufb01gures show histogram estimates of the marginal distributions p(X) and p(Y ), as well as the conditional distribution p(X|Y = 1) corresponding to the bottom row in the top left \ufb01gure. Again, note that these probabilities are normalized so that We can now use the sum and product rules of probability to evaluate the overall from which it follows, using the sum rule, that p(F = o) = 1 \u2212 11/20 = 9/20. Suppose instead we are told that a piece of fruit has been selected and it is an orange, and we would like to know which box it came from. This requires that we evaluate the probability distribution over boxes conditioned on the identity of the fruit, whereas the probabilities in (1.16)\u2013(1.19) give the probability distribution over the fruit conditioned on the identity of the box. We can solve the problem of reversing the conditional probability by using Bayes\u2019 theorem to give p(B = r|F = o) = p(F = o|B = r)p(B = r) From the sum rule, it then follows that p(B = b|F = o) = 1 \u2212 2/3 = 1/3", "3fd07d49-cec3-4aa2-8ca8-39c1bdb475cb": "The individual conditionals are simple to compute as well.\n\nFor the binary RBM we obtain  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    P(hi=1|v)=0 (v W:,i + ci (16.13)  P(hi=0|v)=1-0 (VWs + c) (16.14) Together these properties allow for efficient block Gibbs sampling, which alter- nates between sampling all of h simultaneously and sampling all of v simultane- ously. Samples generated by Gibbs sampling from an RBM model are shown in figure 16.15. Since the energy function itself is just a linear function of the parameters, it is easy to take its derivatives. For example,  E(v,h) = \u2014vihyj. (16.15)  fe) OwWi5  These two properties\u2014efficient Gibbs sampling and efficient derivatives\u2014make training convenient. In chapter 18, we will see that undirected models may be trained by computing such derivatives applied to samples from the model. Training the model induces a representation h of the data v", "0f2b9afb-bbce-44dc-be79-8422a4707c7d": "While almost any function thus qualifies as an estimator,  https://www.deeplearningbook.org/contents/ml.html    121  CHAPTER 5. MACHINE LEARNING BASICS  a good estimator is a function whose output is close to the true underlying @ that generated the training data.\n\nFor now, we take the frequentist perspective on statistics. That is, we assume that the true parameter value @ is fixed but unknown, while the point estimate 6 is a function of the data. Since the data is drawn from a random process, any function of the data is random. Therefore @ is a random variable. Point estimation can also refer to the estimation of the relationship between input and target variables. We refer to these types of point estimates as function estimators. Function Estimation \u2018Sometimes we are interested in performing function estimation (or function approximation). Here, we are trying to predict a variable y given an input vector 2. We assume that there is a function f(a) that describes the approximate relationship between y and x", "f754c7de-9d5b-438b-b309-43c32118344c": "This can be done using prior beliefs about the nature of the function to be approximated, and some automated selection methods developed for polynomial regression can be adapted to deal with the incremental and nonstationary nature of reinforcement learning. Exercise 9.3 What n and ci,j produce the feature vectors x(s) = (1, s1, s2, s1s2, s2 Another linear function approximation method is based on the time-honored Fourier series, which expresses periodic functions as weighted sums of sine and cosine basis functions (features) of di\u21b5erent frequencies. (A function f is periodic if f(x) = f(x + \u2327) for all x and some period \u2327.)\n\nThe Fourier series and the more general Fourier transform are widely used in applied sciences in part because if a function to be approximated is known, then the basis function weights are given by simple formulae and, further, with enough basis functions essentially any function can be approximated as accurately as desired. In reinforcement learning, where the functions to be approximated are unknown, Fourier basis functions are of interest because they are easy to use and can perform well in a range of reinforcement learning problems. First consider the one-dimensional case", "81e3531d-e2ff-4f86-bf69-34687ed34f4d": "First bring the integral over y inside the integrand of the integral over x, next make the change of variable t = y + x where x is \ufb01xed, then interchange the order of the x and t integrations, and \ufb01nally make the change of variable x = t\u00b5 where t is \ufb01xed.\n\n2.6 (\u22c6) Make use of the result (2.265) to show that the mean, variance, and mode of the beta distribution (2.13) are given respectively by 2.7 (\u22c6 \u22c6) Consider a binomial random variable x given by (2.9), with prior distribution for \u00b5 given by the beta distribution (2.13), and suppose we have observed m occurrences of x = 1 and l occurrences of x = 0. Show that the posterior mean value of x lies between the prior mean and the maximum likelihood estimate for \u00b5. To do this, show that the posterior mean can be written as \u03bb times the prior mean plus (1 \u2212 \u03bb) times the maximum likelihood estimate, where 0 \u2a7d \u03bb \u2a7d 1. This illustrates the concept of the posterior distribution being a compromise between the prior distribution and the maximum likelihood solution", "87cac61a-4885-4805-9980-932150a2ab7f": "We then run the mean field inference process, with arrows indicating which variables influence which other variables in the process. In practical applications, we unroll mean field for several steps. In this illustration, we unroll for only two steps. Dashed arrows indicate how the process could be unrolled for more steps. The data variables that were not used as inputs to the inference process become targets, shaded in gray. We can view the inference process for each example as a recurrent network. We use gradient descent and back-propagation to train these recurrent networks to produce the correct targets given their inputs. This trains the mean field process for the MP-DBM to produce accurate estimates. Figure adapted from Goodfellow et al. 672  CHAPTER 20. DEEP GENERATIVE MODELS  each graph being whether it includes or excludes each unit. The MP-DBM also shares parameters across many computational graphs.\n\nIn the case of the MP-DBM, the difference between the graphs is whether each input unit is observed or not. When a unit is not observed, the MP-DBM does not delete it entirely as dropout does", "b68ce2a0-e627-48f6-b678-016d11182cd5": "Shorten and Khoshgoftaar J Big Data  6:60   Signs of Overfitting (Training Error and Testing Error) Desired convergence of Training and Testing Error \u2018= Tiaring or = Testing Exor = Trang Eo = Testhg ror 10 a Fy 0 | i a & 5 ba i 20 2 i gz 0 2 0 \u00b0 2 4 6 8 10 \u00b0 2 4 6 8 10 epoch Epoch Fig. 1 The plot on the left shows an inflection point where the validation error starts to increase as the training rate continues to decrease. The increased training has caused the model to overfit to the training data and perform poorly on the testing set relative to the training set. In contrast, the plot on the right shows a model with the desired relationship between training and testing error  There are many branches of study that hope to improve current benchmarks by apply- ing deep convolutional networks to Computer Vision tasks. Improving the generaliza- tion ability of these models is one of the most difficult challenges.\n\nGeneralizability refers to the performance difference of a model when evaluated on previously seen data (train- ing data) versus data it has never seen before (testing data)", "0f41d7d6-7784-4925-adca-871ef6c9fffb": "In graph (a), the path from a to b is not blocked by node f because it is a tail-to-tail node for this path and is not observed, nor is it blocked by node e because, although the latter is a head-to-head node, it has a descendant c because is in the conditioning set. Thus the conditional independence statement a \u22a5\u22a5 b | c does not follow from this graph. In graph (b), the path from a to b is blocked by node f because this is a tail-to-tail node that is observed, and so the conditional independence property a \u22a5\u22a5 b | f will be satis\ufb01ed by any distribution that factorizes according to this graph. Note that this path is also blocked by node e because e is a head-to-head node and neither it nor its descendant are in the conditioning set.\n\nFor the purposes of d-separation, parameters such as \u03b1 and \u03c32 in Figure 8.5, indicated by small \ufb01lled circles, behave in the same was as observed nodes. However, there are no marginal distributions associated with such nodes", "7f78f912-6c2b-453a-a4c0-e75af3278008": "9.14 (\u22c6) Consider the joint distribution of latent and observed variables for the Bernoulli distribution obtained by forming the product of p(x|z, \u00b5) given by (9.52) and p(z|\u03c0) given by (9.53). Show that if we marginalize this joint distribution with respect to z, then we obtain (9.47).\n\n9.15 (\u22c6) www Show that if we maximize the expected complete-data log likelihood function (9.55) for a mixture of Bernoulli distributions with respect to \u00b5k, we obtain the M step equation (9.59). 9.16 (\u22c6) Show that if we maximize the expected complete-data log likelihood function (9.55) for a mixture of Bernoulli distributions with respect to the mixing coef\ufb01cients \u03c0k, using a Lagrange multiplier to enforce the summation constraint, we obtain the M step equation (9.60). 9.17 (\u22c6) www Show that as a consequence of the constraint 0 \u2a7d p(xn|\u00b5k) \u2a7d 1 for the discrete variable xn, the incomplete-data log likelihood function for a mixture of Bernoulli distributions is bounded above, and hence that there are no singularities for which the likelihood goes to in\ufb01nity", "5ec71f48-bf88-40e6-8488-3186b9b0a163": "Make use of the matrix identity (Appendix C) 3.12 (\u22c6 \u22c6) We saw in Section 2.3.6 that the conjugate prior for a Gaussian distribution with unknown mean and unknown precision (inverse variance) is a normal-gamma distribution. This property also holds for the case of the conditional Gaussian distribution p(t|x, w, \u03b2) of the linear regression model. If we consider the likelihood function (3.10), then the conjugate prior for w and \u03b2 is given by and \ufb01nd expressions for the posterior parameters mN, SN, aN, and bN. 3.13 (\u22c6 \u22c6) Show that the predictive distribution p(t|x, t) for the model discussed in Exercise 3.12 is given by a Student\u2019s t-distribution of the form and obtain expressions for \u00b5, \u03bb and \u03bd. 3.14 (\u22c6 \u22c6) In this exercise, we explore in more detail the properties of the equivalent kernel de\ufb01ned by (3.62), where SN is de\ufb01ned by (3.54). Suppose that the basis functions \u03c6j(x) are linearly independent and that the number N of data points is greater than the number M of basis functions", "2e7b0a2c-f599-45a0-9e28-addcf144bdd3": "It is rare to learn a covariance or precision matrix with richer structure than diagonal. If the covariance is full and conditional, then a parametrization must be chosen that guarantees positive definiteness of the predicted covariance matrix. This can be achieved by writing E(x) = B(x) B' (x), where B is an unconstrained square matrix. One practical issue if the matrix is full rank is that computing the likelihood is expensive, with a d x d matrix requiring O(d 3) computation for the determinant and inverse of (a) (or equivalently, and more commonly done, its eigendecomposition or that of B(a)).\n\nWe often want to perform multimodal regression, that is, to predict real values from a conditional distribution p(y | 2) that can have several different peaks in y space for the same value of x. In this case, a Gaussian mixture is a natural representation for the output . Neural networks with Gaussian mixtures as their output are often called mixture density networks", "605248df-7e8d-4097-9c5d-1e9e7fd4326d": "For example, as we have already mentioned, a conspicuous feature of classical conditioning is that the US generally must begin after the onset of a neutral stimulus for conditioning to occur, and that after conditioning, the CR begins before the appearance of the US. In other words, conditioning generally requires a positive ISI, and the CR generally anticipates the US. How the strength of conditioning (e.g., the percentage of CRs elicited by a CS) depends on the ISI varies substantially across species and response systems, but it typically has the following properties: it is negligible for a zero or negative ISI, i.e., when the US onset occurs simultaneously with, or earlier than, the CS onset (although research has found that associative strengths sometimes increase slightly or become negative with 5In our formalism, for each CS component CSi present on a trial, and for each time step t during a i(St0) = 1 if t = t0 for any t0 at which CSi is present, and equals 0 otherwise.\n\nThis is di\u21b5erent from the CSC representation in Sutton and Barto  in which there are the same distinct features for each time step but no reference to external stimuli; hence the name complete serial compound", "7d28b9f6-5260-44b0-8dac-c87fee943e1a": "Similarly, maximizing (2.181) with respect to m, and making use of I\u2032 I1(m) , we have where we have substituted for the maximum likelihood solution for \u03b8ML that we are performing a joint optimization over \u03b8 and m), and we have de\ufb01ned The function A(m) is plotted in Figure 2.20. Making use of the trigonometric identity (2.178), we can write (2.185) in the form The right-hand side of (2.187) is easily evaluated, and the function A(m) can be inverted numerically. For completeness, we mention brie\ufb02y some alternative techniques for the construction of periodic distributions. The simplest approach is to use a histogram of observations in which the angular coordinate is divided into \ufb01xed bins. This has the virtue of simplicity and \ufb02exibility but also suffers from signi\ufb01cant limitations, as we shall see when we discuss histogram methods in more detail in Section 2.5. Another approach starts, like the von Mises distribution, from a Gaussian distribution over a Euclidean space but now marginalizes onto the unit circle rather than conditioning .\n\nHowever, this leads to more complex forms of distribution and will not be discussed further", "9f76e3ae-5c4f-4ac3-9bbc-f662b8294ce6": "Sometimes local information provides us no guide, such as when the function has a wide flat region, or if we manage to land exactly on a critical point (usually this  Tadd canennntan 21 Lanne be tn de thd 2A n nee ata lee fa Aetetand 7 ata. https://www.deeplearningbook.org/contents/optimization.html    AALLEL SCCMALIO VILLY HapPels LU HICLILOUS Lilal SULVE CXAPLICILLyY LOL CLILICAL PULLLLS, such as Newton\u2019s method). In these cases, local descent does not define a path to a solution at all.\n\nIn other cases, local moves can be too greedy and lead us along a path that moves downhill but away from any solution, as in figure 8.4, or along an  unnecessarily long trajectory to the solution, as in figure 8.2. Currently, we do not understand which of these problems are most relevant to making neural network optimization difficult, and this is an active area of research", "ecb0e8b1-5380-4f02-a396-b144d0cb26bd": "), Intrinsically Motivated Learning in Natural and Arti\ufb01cial Systems, pp. 17\u201347. Springer-Verlag, Berlin Heidelberg. Barto, A. G., Anandan, P. Pattern recognizing stochastic learning automata. IEEE Transactions on Systems, Man, and Cybernetics, 15(3):360\u2013375. Barto, A. G., Anderson, C. W. Structural learning in connectionist systems. In Program of the Seventh Annual Conference of the Cognitive Science Society, pp. 43\u201354. Barto, A. G., Anderson, C. W., Sutton, R. S. Synthesis of nonlinear control surfaces by a layered associative search network. Biological Cybernetics, 43(3):175\u2013185. asynchronous dynamic programming. Technical Report 91-57. Department of Computer and Information Science, University of Massachusetts, Amherst. Barto, A. G., Du\u21b5, M", "6b3c702f-56f3-4bbf-9ca9-ff6cec65c96c": "We show that many of the wellknown algorithms of the above paradigms are all instantiations of the general formulation.\n\nMore concretely, the SE, based on the maximum entropy and variational principles, consists of three principled terms, including the experience term that o\ufb00ers a uni\ufb01ed language to express arbitrary relevant information to supervise the learning, the divergence term that measures the \ufb01tness of the target model to be learned, and the uncertainty term that regularizes the complexity of the system. The single succinct formula re-derives the objective functions of a large diversity of learning algorithms, reducing them to di\ufb00erent choices of the components. The formulation thus shed new light on the fundamental relationships between the diverse algorithms that were each originally designed to deal with a speci\ufb01c type of experience. The modularity and generality of the framework is particularly appealing not only from the theoretical point of view, but also because it o\ufb00ers guiding principles for designing algorithmic approaches to new problems in a mechanical way. Speci\ufb01cally, the SE by its nature allows combining together all di\ufb00erent experience to learn a model of interest", "977f058a-7931-4c27-96fa-80c100084c2f": "Watch Soap Operas R=0.5  Read Twitter R=2  Go Back to Bed R=5  Surf Some More R=1  Go Home and  Call in Sick Get Back Go to Bed R=0 to Work Surf R=5 R=-1 Facebook  R=1  Wake Up R=-1  Go to Work R=-2  Work Hard on Project R=-4  R=-1  Hit Snooze Get Back R=1 to Work R=-1  Get Termination | Promotion State R=50  Ask for Promotion R=-5  https://lilianweng.github.io/posts/2018-02-19-rl-overview/  A (Long) Peek into Reinforcement Learning | Lil'Log   Bellman Equations  Bellman equations refer to a set of equations that decompose the value function into the immediate reward plus the discounted future values. V(s) = E  e(Risi + yRive 4 7 Res3 +..", "04b2671b-2109-4077-a7c1-e8c170649091": "A related idea is the use of convolution across a 1-D temporal sequence. This convolutional approach is the basis for time-delay neural networks . The convolution operation allows a network to share parameters across time but is shallow. The output of convolution is a sequence where each member of the output is a function of a small number of neighboring members of the input. The idea of parameter sharing manifests in the application of the same convolution kernel at each time step.\n\nRecurrent networks share parameters in a different way. Each member of the output is a function of the previous members of the output. Each member of the output is produced using the same update rule applied to the previous outputs. This recurrent formulation results in the sharing of parameters through a very deep computational graph. For the simplicity of exposition, we refer to RNNs as operating on a sequence that contains vectors \u00a2 with the time step index \u00a2 ranging from 1 to 7. In practice, recurrent networks usually operate on minibatches of such sequences, with a different sequence length + for each member of the minibatch. We have omitted the minibatch indices to simplify notation", "b47aeefc-ecb7-4732-ad0b-4c1185474c7c": "Let's say, our classifier f with parameter 6 outputs a probability of a data point belonging to the class y given the feature vector x, Ps (y|x). The optimal parameters should maximize the probability of true labels across multiple training batches B C D:  = arg maxgE(xy)eD  6*  arg max,Excp| > Po(y|x)] ; trained with mini-batches. (x,y)\u20acB  In few-shot classification, the goal is to reduce the prediction error on data samples with unknown labels given a small support set for \u201cfast learning\u201d (think of how \u201cfine-tuning\u201d works). To make the training process mimics what happens during inference, we would like to \u201cfake\u201d datasets with a subset of labels to avoid exposing all the labels to the model and modify the optimization procedure accordingly to encourage fast learning:  Sample a subset of labels, L Cc \u00a3#>e!,  Sample a support set S\u2019 CDanda training batch BY\" CD. Both of them only contain data points with labels belonging to the sampled label set L, y \u20ac L, V(x, y) \u20ac S*, BY", "098ec315-29e6-43d5-8d4a-8bc03cb6994a": "from which we \ufb01nd that all of the p(xi) are equal and are given by p(xi) = 1/M where M is the total number of states xi. The corresponding value of the entropy is then H = ln M. This result can also be derived from Jensen\u2019s inequality (to be discussed shortly). To verify that the stationary point is indeed a maximum, we can Exercise 1.29 evaluate the second derivative of the entropy, which gives where Iij are the elements of the identity matrix. We can extend the de\ufb01nition of entropy to include distributions p(x) over continuous variables x as follows. First divide x into bins of width \u2206.\n\nThen, assuming p(x) is continuous, the mean value theorem  tells us that, for each such bin, there must exist a value xi such that We can now quantize the continuous variable x by assigning any value x to the value xi whenever x falls in the ith bin. The probability of observing the value xi is then p(xi)\u2206", "d89fe513-5306-441b-8b4b-7763c162f9f2": "If we inspect the P(v) represented by the Boltzmann machine learned by the MP-DBM, it tends to be somewhat defective,  in the sense that Gibbs sampling yields poor samples. Back-propagation through the inference graph has two main advantages. First, it trains the model as it is really used\u2014with approximate inference.\n\nThis means that approximate inference, for example, to fill in missing inputs or to perform classification despite the presence of missing inputs, is more accurate in the MP- DBM than in the original DBM. The original DBM does not make an accurate classifier on its own; the best classification results with the original DBM were based on training a separate classifier to use features extracted by the DBM, rather than by using inference in the DBM to compute the distribution over the class labels. Mean field inference in the MP-DBM performs well as a classifier without special modifications. The other advantage of back-propagating through approximate inference is that back-propagation computes the exact gradient of he loss. This is better for optimization than the approximate gradients of SML raining, which suffer from both bias and variance", "f461acdf-ae32-48b0-9589-de27e62a6ca0": "(See our comments at the end of this chapter that explain how our terminology sometimes di\u21b5ers, as it does here, from terminology used in psychology.) Conditioned reinforcement is a key phenomenon that explains, for instance, why we work for the conditioned reinforcer money, whose worth derives solely from what is predicted by having it. In actor\u2013critic methods described in Section 13.5 (and discussed in the context of neuroscience in Sections 15.7 and 15.8), the critic uses a TD method to evaluate the actor\u2019s policy, and its value estimates provide conditioned reinforcement to the actor, allowing the actor to improve its policy. This analog of higher-order instrumental conditioning helps address the credit-assignment problem mentioned in Section 1.7 because the critic gives moment-by-moment reinforcement to the actor when the primary reward signal is delayed.\n\nWe discuss this more below in Section 14.4. Rescorla and Wagner created their model mainly to account for blocking. The core idea of the Rescorla\u2013Wagner model is that an animal only learns when events violate its expectations, in other words, only when the animal is surprised (although without necessarily implying any conscious expectation or emotion)", "cbb549e4-b846-4e2c-a5cd-c55d88169db7": "We shall provide some intuition to support these de\ufb01nitions shortly when we consider a simple example. So far, we have considered a single input value x.\n\nIf we substitute this expansion back into (3.37), we obtain the following decomposition of the expected squared loss and the bias and variance terms now refer to integrated quantities. Our goal is to minimize the expected loss, which we have decomposed into the sum of a (squared) bias, a variance, and a constant noise term. As we shall see, there is a trade-off between bias and variance, with very \ufb02exible models having low bias and high variance, and relatively rigid models having high bias and low variance. The model with the optimal predictive capability is the one that leads to the best balance between bias and variance. This is illustrated by considering the sinusoidal data set from Chapter 1. Here we generate 100 data sets, each containing N = 25 Appendix A data points, independently from the sinusoidal curve h(x) = sin(2\u03c0x). The data sets are indexed by l = 1, . , L, where L = 100, and for each data set D(l) we tion parameter \u03bb, using the sinusoidal data set from Chapter 1", "4b5ac405-c3d3-447d-aa5a-4c6ccd0214a0": "So far we have assumed that the projected data set given by \u00a2(xn ) has zero mean, which in general will not be the case. We cannot simply compute and then subtract off the mean, since we wish to avoid working directly in feature space, and so again, we formulate the algorithm purely in-!erms of the kernel function. The projected data points after centralizing, denoted \u00a2(xn ), are given by and the corresponding elements of the Gram matrix are given by This can be expressed in matrix notation as where IN denotes the N x N matrix in which every element takes the value l/N. ~ ~ Thus we can evaluate K using only the kernel function and then use K to determine the eigenvalues and eigenvectors. Note that the standard PCA algorithm is recovered as a special case if we use a linear kernel k(x, x') = xTx/. Figure 12.17 shows an example of kernel PCA applied to a synthetic data set", "d444d342-fac1-478d-8e37-f38833728651": "Note that each of the messages comprises a set of K values, one for each choice of xn, and so the product of two messages should be interpreted as the point-wise multiplication of the elements of the two messages to give another set of K values. The message \u00b5\u03b1(xn) can be evaluated recursively because and then apply (8.55) repeatedly until we reach the desired node. Note carefully the structure of the message passing equation.\n\nThe outgoing message \u00b5\u03b1(xn) in (8.55) is obtained by multiplying the incoming message \u00b5\u03b1(xn\u22121) by the local potential involving the node variable and the outgoing variable and then summing over the node variable. Similarly, the message \u00b5\u03b2(xn) can be evaluated recursively by starting with This recursive message passing is illustrated in Figure 8.38. The normalization constant Z is easily evaluated by summing the right-hand side of (8.54) over all states of xn, an operation that requires only O(K) computation. Graphs of the form shown in Figure 8.38 are called Markov chains, and the corresponding message passing equations represent an example of the ChapmanKolmogorov equations for Markov processes", "2a9a8c7a-79db-41b2-869b-e7e95b611bb9": "The overall architecture of Dyna agents, of which the Dyna-Q algorithm is one example, is shown in Figure 8.1. The central column represents the basic interaction between agent and environment, giving rise to a trajectory of real experience. The arrow on the left of the \ufb01gure represents direct reinforcement learning operating on real experience to improve the value function and the policy. On the right are model-based processes. The model is learned from real experience and gives rise to simulated experience. We use the term search control to refer to the process that selects the starting states and actions for the simulated experiences generated by the model. Finally, planning is achieved by applying reinforcement learning methods to the simulated experiences just as if they had really happened. Typically, as in Dyna-Q, the same reinforcement learning method is used both for learning from real experience and for planning from simulated experience. The reinforcement learning method is thus the \u201c\ufb01nal common path\u201d for both learning and planning. Learning and planning are deeply integrated in the sense that they share almost all the same machinery, di\u21b5ering only in the source of their experience.\n\nConceptually, planning, acting, model-learning, and direct RL occur simultaneously and in parallel in Dyna agents", "1bcd107b-e3b3-4361-b6d1-5e766495c595": "In fact, this feature of the TD algorithm is one of the major reasons for its development, which we now understand through its connection to dynamic programming as described in Chapter 6. Bootstrapping values is intimately related to second-order, and higher-order, conditioning.\n\nIn the examples of the TD model\u2019s behavior described above, we examined only the changes in the associative strengths of the CS components; we did not look at what the model predicts about properties of an animal\u2019s conditioned responses (CRs): their timing, shape, and how they develop over conditioning trials. These properties depend on the species, the response system being observed, and parameters of the conditioning trials, but in many experiments with di\u21b5erent animals and di\u21b5erent response systems, the magnitude of the CR, or the probability of a CR, increases as the expected time of the US approaches. For example, in classical conditioning of a rabbit\u2019s nictitating membrane response that we mentioned above, over conditioning trials the delay from CS onset to when the nictitating membrane begins to move across the eye decreases over trials, and the amplitude of this anticipatory closure gradually increases over the interval between the CS and the US until the membrane reaches maximal closure at the expected time of the US", "4f6f046c-010e-415e-863f-7d9a82485d52": "It consists of a linear string of sample actions and states, just as in n-step Sarsa, except that its last element is a branch over all action possibilities weighted, as always, by their probability under \u21e1.\n\nThis algorithm can be described by the same equation as n-step Sarsa (above) except with the n-step return rede\ufb01ned as (with Gt:t+n .=Gt for t + n \u2265 T) where \u00afVt(s) is the expected approximate value of state s, using the estimated action values at time t, under the target policy: Expected approximate values are used in developing many of the action-value methods in the rest of this book. If s is terminal, then its expected approximate value is de\ufb01ned to be 0. Recall that o\u21b5-policy learning is learning the value function for one policy, \u21e1, while following another policy, b. Often, \u21e1 is the greedy policy for the current action-valuefunction estimate, and b is a more exploratory policy, perhaps \"-greedy", "5b6acea8-e920-437b-ac4a-79ef20f67551": "Compared to many of the divergence metrics (e.g., KL divergence), Wasserstein distance has the desirable properties as a distance metric, such as symmetry and the triangle inequality.\n\nBased on the Kantorovich duality (Santambrogio, 2015, Section 1.2), the \ufb01rstorder Wasserstein distance between the two distributions q and p can be written as: where \u2225\u03d5\u2225L \u2264 1 is the constraint of \u03d5 : T \u2192 R being a 1-Lipschitz function. Wasserstein GAN. Setting the divergence function in Equation 5.1 to the Wasserstein distance W1, we thus recover the Wasserstein GAN algorithm : The standard equation Equation 3.1 so far has played the role of the ultimate learning objective that fully de\ufb01nes the learning problem in an analytical form. As seen in Sections 4 and 5, many of the known algorithms are special cases of the SE objective. On the other hand, in a dynamic or online setting, the learning objective itself may be evolving over time", "6adbe0ba-1a1d-406c-bede-f47025670f7f": "Contours of the true posterior distribution p(\u00b5, \u03c4|D) are shown in green. (a) Contours of the initial factorized approximation q\u00b5(\u00b5)q\u03c4(\u03c4) are shown in blue. (b) After re-estimating the factor q\u00b5(\u00b5). (c) After re-estimating the factor q\u03c4(\u03c4). (d) Contours of the optimal factorized approximation, to which the iterative scheme converges, are shown in red. In general, we will need to use an iterative approach such as this in order to solve for the optimal factorized posterior distribution.\n\nFor the very simple example we are considering here, however, we can \ufb01nd an explicit solution by solving the simultaneous equations for the optimal factors q\u00b5(\u00b5) and q\u03c4(\u03c4). Before doing this, we can simplify these expressions by considering broad, noninformative priors in which \u00b50 = a0 = b0 = \u03bb0 = 0. Although these parameter settings correspond to improper priors, we see that the posterior distribution is still well de\ufb01ned", "04db78d2-fa8d-4861-93f2-02ead663f3c0": "The composition of these autoencoders then forms a deep autoencoder.\n\nBecause each layer was separately trained to be locally contractive, the deep autoencoder is contractive as well. The result is not the same as what would be obtained by jointly training the entire architecture with a penalty on the Jacobian of the deep model, but it  520  CHAPTER 14. AUTOENCODERS  captures many of the desirable qualitative characteristics. Another practical issue is that the contraction penalty can obtain useless results if we do not impose some sort of scale on the decoder. For example, the encoder could consist of multiplying the input by a small constant \u20ac, and the decoder could consist of dividing the code by \u00a2\u00ab. As \u20ac approaches 0, the encoder drives the  we othy. toa ad od . 1 1 sae 1 aout  https://www.deeplearningbook.org/contents/autoencoders.html    contractive penalty \\4(14) to approach U Without naving learned anything about the ecoder m:  distribution, Meanwhile, the aintains perfect reconstruction. In Rifai  et al", "2424e668-a431-40f0-bde3-552c25898ff6": "If we train the model with a unimodal approximate posterior, we will obtain a model with a true posterior that is far closer to unimodal than we would have obtained by training the model with exact inference.\n\nhttps://www.deeplearningbook.org/contents/inference.html    Computing the true amount of harm imposed on a model by a variational approximation is thus very difficult. There exist several methods for estimating log p(v). We often estimate log p(v; @) after training the model and find that  the gap with \u00a3(v, 6,q) is small. From this, we can conclude that our variational approximation is accurate for the specific value of @ that we obtained from the learning process. We should not conclude that our variational approximation is accurate in general or that the variational approximation did little harm to the learning process. To measure the true amount of harm induced by the variational approximation, we would need to know @* = maxg log p(v;@). It is possible for L(v,0,q) \u00a9 log p(v; 8) and logp(v;@) < log p(v;6*) to hold simultaneously", "00786ac7-8090-4fa2-9002-7a6619a01d55": "Given the knowledge of how the underlying factors actually change, it is possible to analytically solve for the optimal functions expressing these factors. In practice, experiments with deep SFA applied to simulated data seem to recover the theoretically predicted functions. This is in comparison to other learning algorithms, where the cost function depends highly on specific pixel values, making it much more difficult to determine what features the model will learn. 491  CHAPTER 13.\n\nLINEAR FACTOR MODELS  Deep SFA has also been used to learn features for object recognition and pose estimation . So far, the slowness principle has not become the basis for any state-of-the-art applications. It is unclear what factor has limited its performance. We speculate that perhaps the slowness prior is too strong, and that, rather than imposing a prior that features should be approximately constant, it would be better to impose a prior that features should be easy to predict from one time step to the next. The position of an object is a useful feature regardless of whether the object\u2019s velocity is high or low, but the slowness principle encourages the model to ignore the position of objects that have high velocity", "e9832108-1cbd-475b-bce5-f7f73cec601d": "Formally, a Markov chain is defined by a random state x and a transition distribution T(a\u2019 | x) specifying the probability that a random update will go to state a\u2019 if it starts in state a.\n\nRunning the Markov chain means repeatedly updating the state a to a value a\u2019 sampled from T(x\u2019 | a). To gain some theoretical understanding of how MCMC methods work, it is useful to reparametrize the problem. First, we restrict our attention to the case where the random variable x has countably many states. We can then represent the state as just a positive integer x. Different integer values of x map back to different states a in the original problem. Consider what happens when we run infinitely many Markov chains in parallel. All the states of the different Markov chains are drawn from some distribution  ay. https://www.deeplearningbook.org/contents/monte_carlo.html    gq\u2019? (x), wher, | t indicates the number of time steps that have elapsed. At the beginning, \u00a2\u2019 is some (lstribution that we used to arbitrarily initialize for each Markov chain", "9352657a-b7d8-4467-aaec-b5e7a4daf95f": "One of the most striking results found in this competition is that as an architecture makes use of deeper and deeper representations (learned in a purely unsupervised way from data collected in the first setting, P,), the learning curve on the new categories of the second (transfer) setting P: becomes much better. For deep representations, fewer labeled examples of the transfer tasks are necessary to achieve the apparently asymptotic generalization performance. Two extreme forms of transfer learning are one-shot learning and zero-shot learning, sometimes also called zero-data learning. Only one labeled example of the transfer task is given for one-shot learning, while no labeled examples are given at all for the zero-shot learning task.\n\nOne-shot learning  is possible because the representation learns to cleanly separate the underlying classes during the first stage. During the transfer learning stage, only one labeled example is needed to infer the label of many possible test examples that all cluster around the same point in representation space. This works to the extent that the factors of variation corresponding to these invariances have been cleanly separated from the other factors, in the learned representation space, and that we have somehow learned which factors do and do not matter when discriminating objects of certain categories", "676f11f8-d07e-47d4-9da1-4e13111d2c25": "Note that the approach of \ufb01tting Exercise 4.10 Gaussian distributions to the classes is not robust to outliers, because the maximum likelihood estimation of a Gaussian is not robust. Section 2.3.7 Let us now consider the case of discrete feature values xi. For simplicity, we begin by looking at binary feature values xi \u2208 {0, 1} and discuss the extension to more general discrete features shortly. If there are D inputs, then a general distribution would correspond to a table of 2D numbers for each class, containing 2D \u2212 1 independent variables (due to the summation constraint). Because this grows exponentially with the number of features, we might seek a more restricted representation. Here we will make the naive Bayes assumption in which the feature values are Section 8.2.2 treated as independent, conditioned on the class Ck.\n\nThus we have class-conditional distributions of the form which contain D independent parameters for each class. Substituting into (4.63) then gives which again are linear functions of the input values xi. For the case of K = 2 classes, we can alternatively consider the logistic sigmoid formulation given by (4.57)", "da637e6c-f0a1-4f83-a8ee-78bcbbe87d53": "A hierarchical latent variable model for data visualization. IEEE Transactions on Pattern Analysis and Machine Intelligence 20(3), 281\u2013293. Blei, D. M., M. I. Jordan, and A. Y. Ng . Hierarchical Bayesian models for applications in information retrieval. In J. M. B. et al. (Ed. ), Bayesian Statistics, 7, pp. 25\u201343.\n\nOxford University Press. Blum, J. A. Multidimensional stochastic approximation methods. Annals of Mathematical Statistics 25, 737\u2013744. Bodlaender, H. A tourist guide through treewidth. Acta Cybernetica 11, 1\u201321. Boser, B. E., I. M. Guyon, and V. N. Vapnik . A training algorithm for optimal margin classi\ufb01ers. In D", "c0dbf3d5-363b-4e0f-9685-02279a49fc30": "However, most of this chapter is concerned with the extension of these basic concepts to the particular case of neural networks.\n\nIn section 5.2.2, we defined regularization as \u201cany modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error.\u201d There are many regularization strategies. Some put extra con- straints on a machine learning model, such as adding restrictions on the parameter values. Some add extra terms in the objective function that can be thought of as corresponding to a soft constraint on the parameter values. If chosen carefully, these extra constraints and penalties can lead to improved performance on the  https://www.deeplearningbook.org/contents/regularization.html    224  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  test set. Sometimes these constraints and penalties are designed to encode specific kinds of prior knowledge. Other times, these constraints and penalties are designed to express a generic preference for a simpler model class in order to promote generalization. Sometimes penalties and constraints are necessary to make an underdetermined problem determined. Other forms of regularization, known as ensemble methods, combine multiple hypotheses that explain the training data. In the context of deep learning, most regularization strategies are based on regularizing estimators", "d765a83b-b81b-4ddc-b3af-770e08e332c3": "Note that our aim in this example is to illustrate the K-means algorithm.\n\nIf we had been aiming to produce a good image compressor, then it would be more fruitful to consider small blocks of adjacent pixels, for instance 5\u00d75, and thereby exploit the correlations that exist in natural images between nearby pixels. In Section 2.3.9 we motivated the Gaussian mixture model as a simple linear superposition of Gaussian components, aimed at providing a richer class of density models than the single Gaussian. We now turn to a formulation of Gaussian mixtures in terms of discrete latent variables. This will provide us with a deeper insight into this important distribution, and will also serve to motivate the expectation-maximization algorithm. Recall from (2.188) that the Gaussian mixture distribution can be written as a linear superposition of Gaussians in the form Let us introduce a K-dimensional binary random variable z having a 1-of-K representation in which a particular element zk is equal to 1 and all other elements are equal to 0", "122d4688-6114-4a80-8dc3-60805c91972f": "where L is a D x D diagonal matrix with elements Ai, and U is a D x D orthogonal matrix with columns given by Ui. Then we define, for each data point X n , a transformed value given by where x is the sample mean defined by (12.1). Clearly, the set {Yn} has zero mean, and its covariance is given by the identity matrix because This operation is known as whitening or sphereing the data and is illustrated for the Old Faithful data set in Figure 12.6. It is interesting to compare PCA with the Fisher linear discriminant which was discussed in Section 4.1.4. Both methods can be viewed as techniques for linear dimensionality reduction. However, PCA is unsupervised and depends only on the values X n whereas Fisher linear discriminant also uses class-label information. This difference is highlighted by the example in Figure 12.7.\n\nAnother common application of principal component analysis is to data visualization. Here each data point is projected onto a two-dimensional (M = 2) principal subspace, so that a data point X n is plotted at Cartesian coordinates given by x'J. U1 and x'J", "75e217a3-06c4-4cd2-9750-df39d7379477": "Barto, A. G., Sutton, R. S. Landmark learning: An illustration of associative search. Barto, A. G., Sutton, R. S. Simulation of anticipatory responses in classical conditioning by a neuron-like adaptive element. Behavioural Brain Research, 4(3):221\u2013235. di\ufb03cult learning control problems. IEEE Transactions on Systems, Man, and Cybernetics, 13(5):835\u2013846. Reprinted in J. A. Anderson and E. Rosenfeld (Eds. ), Neurocomputing: Foundations of Research, pp. 535\u2013549. MIT Press, Cambridge, MA, 1988. Barto, A. G., Sutton, R. S., Brouwer, P. S. Associative search network: A reinforcement learning associative memory. Biological Cybernetics, 40(3):201\u2013211. Barto, A. G., Sutton, R", "66bff2ea-9361-413b-a860-2a30b147acb2": "These can be addressed through a closely related algorithm called max-sum, which can be viewed as an application of dynamic programming in the context of graphical models . A simple approach to \ufb01nding latent variable values having high probability would be to run the sum-product algorithm to obtain the marginals p(xi) for every variable, and then, for each marginal in turn, to \ufb01nd the value x\u22c6 i that maximizes that marginal. However, this would give the set of values that are individually the most probable. In practice, we typically wish to \ufb01nd the set of values that jointly have the largest probability, in other words the vector xmax that maximizes the joint distribution, so that xmax = arg max x p(x) (8.87) for which the corresponding value of the joint probability will be given by In general, xmax is not the same as the set of x\u22c6 i values, as we can easily show using a simple example. Consider the joint distribution p(x, y) over two binary variables x, y \u2208 {0, 1} given in Table 8.1", "17f5b19b-9a46-4cc5-adc8-62cb4ed6f33c": "=35 Lr) = 551 =o (20.65)  which means that  By | (Sy) \u2014 O(w)) 282 w) = Exy) [yyy | \u2014 bw) E yy) [Peetu  Ow Ow Ow (20.66) = Exy) [yyy ere | (20.67)  Furthermore, we can obtain the optimal 6(w) by computing the variance of (J(y) \u2014 b(w)) 2egrw) under p(y) and minimizing with respect to b(w).\n\nWhat we find is that this optimal baseline b*(w), is different for each element w,; of the vector w:  Egy) [J (y) ge\")  PY Ow;  Enty)  687  [| |  bY (w); = (20.68)  Alog p(y) ? Ow;  https://www.deeplearningbook.org/contents/generative_models.html    CGHAPFER-20\u2014DEEP-GENERAFIVE MODELS:  The gradient estimator with respect to w; then becomes  J log p(y) (J(y) \u2014 b(w) 8), (20.69) ar where b(w); estimates the above b*(w);", "6c0abb3b-07f5-4991-aea4-4c248268c676": "It is similar to MAML in many ways, given that both rely on meta-optimization through gradient descent and both are model-agnostic. The Reptile works by repeatedly: sampling a task, training on it by multiple gradient descent steps, and then moving the model weights towards the new parameters. See the algorithm below: SGD(ZL,,,, 9, k) performs stochastic gradient update for k steps on the loss \u00a3,, starting with initial parameter @ and returns the final parameter vector. The batch version samples multiple tasks instead of one within each iteration.\n\nThe reptile gradient is defined as  (9 \u2014 W)/a, where a is the stepsize used by the SGD operation. Algorithm 2 Reptile, batched version  Initialize 6 for iteration = 1,2,... do Sample tasks 71, 72,...,T for i= 1,2,...,n do Compute W; = SGD(L,,, 6, k)  end for 12 Update 0+ 0+ B\u2014 ) (W; \u2014 6) n\u00a2 end for i=1  At a glance, the algorithm looks a lot like an ordinary SGD. However, because the task-specific optimization can take more than one step", "98da1bc6-9fea-4744-94fe-1c20059494eb": "We have observed this occurring, for example, in the early stages of application development.\n\nWe see that with non-adversarial labeling functions (w\u2217 > 0), even an optimal generative model (WMV*) can only disagree with MV when there are disagreeing labels, which will occur infrequently. We see that the expected optimal advantage will have an upper bound that falls quadratically with label density: High Label Density In this setting, the majority of the data points have a large number of labels. For example, we might be working in an extremely high-volume crowdsourcing setting, or an application with many high-coverage knowledge bases as distant supervision", "68574c95-5806-4b72-ac1f-a6fc43be0828": "On the other hand, such information can be estimated from experience, in this case by playing many games against the opponent. About the best one can do on this problem is \ufb01rst to learn a model of the opponent\u2019s behavior, up to some level of con\ufb01dence, and then apply dynamic programming to compute an optimal solution given the approximate opponent model. In the end, this is not that di\u21b5erent from some of the reinforcement learning methods we examine later in this book. An evolutionary method applied to this problem would directly search the space of possible policies for one with a high probability of winning against the opponent. Here, a policy is a rule that tells the player what move to make for every state of the game\u2014every possible con\ufb01guration of Xs and Os on the three-by-three board.\n\nFor each policy considered, an estimate of its winning probability would be obtained by playing some number of games against the opponent. This evaluation would then direct which policy or policies were considered next. A typical evolutionary method would hill-climb in policy space, successively generating and evaluating policies in an attempt to obtain incremental improvements. Or, perhaps, a genetic-style algorithm could be used that would maintain and evaluate a population of policies. Literally hundreds of di\u21b5erent optimization methods could be applied", "2b4fe9b3-a882-48dd-9d11-8fe7d3a5094d": "Tasks with episodes of this kind are called episodic tasks. In episodic tasks we sometimes need to distinguish the set of all nonterminal states, denoted S, from the set of all states plus the terminal state, denoted S+. The time of termination, T, is a random variable that normally varies from episode to episode.\n\nOn the other hand, in many cases the agent\u2013environment interaction does not break naturally into identi\ufb01able episodes, but goes on continually without limit. For example, this would be the natural way to formulate an on-going process-control task, or an application to a robot with a long life span. We call these continuing tasks. The return formulation (3.7) is problematic for continuing tasks because the \ufb01nal time step would 5Better places for imparting this kind of prior knowledge are the initial policy or initial value function, 6Section 17.4 delves further into the issue of designing e\u21b5ective reward signals. 7Episodes are sometimes called \u201ctrials\u201d in the literature. be T = 1, and the return, which is what we are trying to maximize, could itself easily be in\ufb01nite", "a85d41b2-9ca1-4d88-9f72-dfd3d3deaf16": "Even when planning is only done at decision time, we can still view it, as we did in Section 8.1, as proceeding from simulated experience to updates and values, and ultimately to a policy. It is just that now the values and policy are speci\ufb01c to the current state and the action choices available there, so much so that the values and policy created by the planning process are typically discarded after being used to select the current action. In many applications this is not a great loss because there are very many states and we are unlikely to return to the same state for a long time. In general, one may want to do a mix of both: focus planning on the current state and store the results of planning so as to be that much farther along should one return to the same state later.\n\nDecision-time planning is most useful in applications in which fast responses are not required. In chess playing programs, for example, one may be permitted seconds or minutes of computation for each move, and strong programs may plan dozens of moves ahead within this time. On the other hand, if low latency action selection is the priority, then one is generally better o\u21b5 doing planning in the background to compute a policy that can then be rapidly applied to each newly encountered state", "69869dea-5435-4f55-bd38-036ef84d54b5": "The new variables h also provide an alternative representation for v. For example, as discussed in section 3.9.6, the mixture of Gaussians model learns a latent variable that corresponds to the category of examples the input was drawn from. This means that the latent variable in a mixture of Gaussians model can be used to do classification. In chapter 14 we saw how simple probabilistic models like sparse coding learn latent variables that can be used as input features for a classifier, or as coordinates along a manifold. Other models can be used in this same way, but deeper models and models with different kinds of interactions can create even richer descriptions of the input. Many approaches accomplish feature learning by learning latent variables. Often, given some model of v and h, experimental observations show that E or argmax,p(h, v) is a good feature mapping for v.  16.6 Inference and Approximate Inference  One of the main ways we can use a probabilistic model is to ask questions about how variables are related to each other. Given a set of medical tests, we can ask what disease a patient might have", "b50efe77-7963-4912-a02a-1ae1aa09e204": "Rawlik, K., Toussaint, M., & Vijayakumar, S. On stochastic optimal control and reinforcement learning by approximate inference. Proceedings of Robotics: Science and Systems VIII. Real, E., Liang, C., So, D., & Le, Q. AutoML-zero: Evolving machine learning algorithms from scratch. International Conference on Machine Learning, 8007\u20138019. Richardson, M., & Domingos, P. Markov logic networks. Machine learning, 62(1), 107\u2013136. Roth, D. Incidental supervision: Moving beyond supervised learning. Thirty-First AAAI Conference on Arti\ufb01cial Intelligence.\n\nSamdani, R., Chang, M.-W., & Roth, D. Uni\ufb01ed expectation maximization. Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 688\u2013698. Schmidhuber, J. Formal theory of creativity, fun, and intrinsic motivation . IEEE Transactions on Autonomous Mental Development", "97667c73-0dc9-4f2d-8ddc-d0ebbd8a7e28": "The critic and actor learning rules share with Hebb\u2019s proposal the idea that changes in a synapse\u2019s e\ufb03cacy depend on the interaction of several factors. In the critic learning rule the interaction is between the reinforcement signal \u03b4 and eligibility traces that depend only on presynaptic signals. Neuroscientists call this a two-factor learning rule because the interaction is between two signals or quantities. The actor learning rule, on the other hand, is a three-factor learning rule because, in addition to depending on \u03b4, its eligibility traces depend on both presynaptic and postsynaptic activity. Unlike Hebb\u2019s proposal, however, the relative timing of the factors is critical to how synaptic e\ufb03cacies change, with eligibility traces intervening to allow the reinforcement signal to a\u21b5ect synapses that were active in the recent past. Some subtleties about signal timing for the actor and critic learning rules deserve closer attention", "9401b770-7090-49e2-92eb-f120b20acc1b": "4.6 (N@is = Feag f\u00ae) (4.6) Equivalently, the Hessian is the Jacobian of the gradient. Anywhere that the second partial derivatives are continuous, the differential operators are commutative; that is, their order can be swapped: O? &  0x4, Ox; 7) = Ox ;0x; v)-  (4.7)  This implies that H;; = Hj;, so the Hessian matrix is symmetric at such points. Most of the functions we encounter in the context of deep learning have a symmetric Hessian almost everywhere. Because the Hessian matrix is real and symmetric, we can decompose it into a set of real eigenvalues and an orthogonal basis of  85  CHAPTER 4. NUMERICAL COMPUTATION  eigenvectors. The second derivative in a specific direction represented by a unit vector d is given by d' Hd", "399ec477-dbdd-4f3b-bc46-09b8e7d32c72": "This is called a heteroscedastic model.\n\nIn the heteroscedastic case, we simply make the specification of the variance be one of he values output by f(x;0@). A typical way to do this is to formulate the Gaussian distribution using precision, rather than variance, as described in equation 3.22. In the multivariate case, it is most common to use a diagonal precision matrix diag(@). (6.34) This formulation works well with gradient descent because the formula for the  log-likelihood of the Gaussian distribution parametrized by @ involves only mul- tiplication by 8; and addition of log B;. The gradient of multiplication, addition,  184  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  and logarithm operations is well behaved. By comparison, if we parametrized the output in terms of variance, we would need to use division. The division function becomes arbitrarily steep near zero. While large gradients can help learning, arbi- trarily large gradients usually result in instability", "3aa99f42-1957-4090-ba96-cc8464ec58ce": "., ak) )  p(@| 2, ...,a0\") (5.67)  133  https://www.deeplearningbook.org/contents/ml.html    CHAPTER 5. MACHINE LEARNING BASICS  In the scenarios where Bayesian estimation is typically used, the prior begins as a relatively uniform or Gaussian distribution with high entropy, and the observation of the data usually causes the posterior to lose entropy and concentrate around a few highly likely values of the parameters. Relative to maximum likelihood estimation, Bayesian estimation offers two important differences. First, unlike the maximum likelihood approach that makes predictions using a point estimate of 8, the Bayesian approach is to make predictions using a full distribution over @. For example, after observing m examples, the predicted distribution over the next data sample, 2(\u2122+)), is given by  pia) | o\u2122 2 al\u2122)) = / pal) | A)p(O | aM ,...,a0) dd.\n\n(5.68)  Here each value of @ with positive probability density contributes to the prediction of the next example, with the contribution weighted by the posterior density itself", "d35792fd-fca8-43dc-ba04-d69f09216547": "Let us denote a clique by C and the set of variables in that clique by xC.\n\nThen the joint distribution is written as a product of potential functions \u03c8C(xC) over the maximal cliques of the graph which ensures that the distribution p(x) given by (8.39) is correctly normalized. By considering only potential functions which satisfy \u03c8C(xC) \u2a7e 0 we ensure that p(x) \u2a7e 0. In (8.40) we have assumed that x comprises discrete variables, but the framework is equally applicable to continuous variables, or a combination of the two, in which the summation is replaced by the appropriate combination of summation and integration. Note that we do not restrict the choice of potential functions to those that have a speci\ufb01c probabilistic interpretation as marginal or conditional distributions. This is in contrast to directed graphs in which each factor represents the conditional distribution of the corresponding variable, conditioned on the state of its parents. However, in special cases, for instance where the undirected graph is constructed by starting with a directed graph, the potential functions may indeed have such an interpretation, as we shall see shortly", "fa7b008f-d85f-4df0-9840-b1067fe8ed82": "When the norm of reconstruction error (shown by the length of the arrows) is large, probability can be significantly increased by moving in the direction of the arrow, and that is mostly the case in places of low probability. The autoencoder maps these low probability points to higher probability reconstructions. Where probability is maximal, the arrows shrink because the reconstruction becomes more accurate. Figure reproduced with permission from Alain and Bengio . https://www.deeplearningbook.org/contents/autoencoders.html    512  CHAPTER 14.\n\nAUTOENCODERS  14.6 Learning Manifolds with Autoencoders  Like many other machine learning algorithms, autoencoders exploit the idea that data concentrates around a low-dimensional manifold or a small set of such manifolds, as described in section 5.11.3. Some machine learning algorithms exploit this idea only insofar as they learn a function that behaves correctly on the manifold but that may have unusual behavior if given an input that is off the manifold. Autoencoders take this idea further and aim to learn the structure of the manifold. To understand how autoencoders do this, we must present some important characteristics of manifolds", "c88525b5-a569-4211-8a22-73fd0e9b35b2": "In Chapter 7 we described a variety of tabular o\u21b5-policy algorithms.\n\nTo convert them to semi-gradient form, we simply replace the update to an array (V or Q) to an update to a weight vector (w), using the approximate value function (\u02c6v or \u02c6q) and its gradient. Many of these algorithms use the per-step importance sampling ratio: For example, the one-step, state-value algorithm is semi-gradient o\u21b5-policy TD(0), which is just like the corresponding on-policy algorithm (page 203) except for the addition of \u21e2t: where \u03b4t is de\ufb01ned appropriately depending on whether the problem is episodic and discounted, or continuing and undiscounted using average reward: For action values, the one-step algorithm is semi-gradient Expected Sarsa: Note that this algorithm does not use importance sampling. In the tabular case it is clear that this is appropriate because the only sample action is At, and in learning its value we do not have to consider any other actions. With function approximation it is less clear because we might want to weight di\u21b5erent state\u2013action pairs di\u21b5erently once they all contribute to the same overall approximation", "dad090db-4058-46aa-9e70-813419f5fb7e": "Gibbs block 2: ~() ~() ( ~ (@) }) ~ Q) Wi fc ceeded fee DIE 1) \u2014 a Pars ot. war 2)T \\  https://www.deeplearningbook.org/contents/generative_models.html    Ves Jp t4bJ SALUIPICU LOL D (446s Ay TUL ergy 1 ee OTe . end for det wll \\ ) Awa + Awa \u20144V' AY Aw.) \u2014 Awe) _ FAO! AH) Wh) \u2014_w) + ceAw) (this is a cartoon illustration, in practice use a more effective algorithm, such as momentum with a decaying learning rate) WA \u2014-_ WO + Awe  end while  RBMs have been trained in this way, they can be combined to form a DBM. The DBM may then be trained with PCD. Typically PCD training will make only a small change in the model\u2019s parameters and in its performance as measured by the  667  CHAPTER 20", "3a1dda36-29b5-45bf-bff1-5479c1ad2ae1": "3.19 (\u22c6 \u22c6) Show that the integration over w in the Bayesian linear regression model gives the result (3.85). Hence show that the log marginal likelihood is given by (3.86). 3.20 (\u22c6 \u22c6) www Starting from (3.86) verify all of the steps needed to show that maximization of the log marginal likelihood function (3.86) with respect to \u03b1 leads to the re-estimation equation (3.92).\n\n3.21 (\u22c6 \u22c6) An alternative way to derive the result (3.92) for the optimal value of \u03b1 in the evidence framework is to make use of the identity Prove this identity by considering the eigenvalue expansion of a real, symmetric matrix A, and making use of the standard results for the determinant and trace of A expressed in terms of its eigenvalues (Appendix C). Then make use of (3.117) to derive (3.92) starting from (3.86). 3.22 (\u22c6 \u22c6) Starting from (3.86) verify all of the steps needed to show that maximization of the log marginal likelihood function (3.86) with respect to \u03b2 leads to the re-estimation equation (3.95). by \ufb01rst marginalizing with respect to w and then with respect to \u03b2", "146915fd-35ed-4643-8766-d77548fed2e5": "This was simply the idea of a learning system that wants something, that adapts its behavior in order to maximize a special signal from its environment.\n\nThis was the idea of a \u201chedonistic\u201d learning system, or, as we would say now, the idea of reinforcement learning. Like others, we had a sense that reinforcement learning had been thoroughly explored in the early days of cybernetics and arti\ufb01cial intelligence. On closer inspection, though, we found that it had been explored only slightly. While reinforcement learning had clearly motivated some of the earliest computational studies of learning, most of these researchers had gone on to other things, such as pattern classi\ufb01cation, supervised learning, and adaptive control, or they had abandoned the study of learning altogether. As a result, the special issues involved in learning how to get something from the environment received relatively little attention. In retrospect, focusing on this idea was the critical step that set this branch of research in motion. Little progress could be made in the computational study of reinforcement learning until it was recognized that such a fundamental idea had not yet been thoroughly explored. The \ufb01eld has come a long way since then, evolving and maturing in several directions. Reinforcement learning has gradually become one of the most active research areas in machine learning, arti\ufb01cial intelligence, and neural network research", "62c93d57-4053-4bc1-9b00-baf34bdfa304": "The n-step methods of Chapter 7 also enabled this, but eligibility trace methods are more general, often faster to learn, and o\u21b5er di\u21b5erent computational complexity tradeo\u21b5s. This chapter has o\u21b5ered an introduction to the elegant, emerging theoretical understanding of eligibility traces for onand o\u21b5-policy learning and for variable bootstrapping and discounting. One aspect of this elegant theory is true online methods, which exactly reproduce the behavior of expensive ideal methods while retaining the computational congeniality of conventional TD methods. Another aspect is the possibility of derivations that automatically convert from intuitive forward-view methods to more e\ufb03cient incremental backward-view algorithms. We illustrated this general idea in a derivation that started with a classical, expensive Monte Carlo algorithm and ended with a cheap incremental non-TD implementation using the same novel eligibility trace used in true online TD methods. As we mentioned in Chapter 5, Monte Carlo methods may have advantages in nonMarkov tasks because they do not bootstrap.\n\nBecause eligibility traces make TD methods more like Monte Carlo methods, they also can have advantages in these cases. If one wants to use TD methods because of their other advantages, but the task is at least partially non-Markov, then the use of an eligibility trace method is indicated", "3b4a9ed6-4ec0-439e-91de-be431ec5cdbf": "To do this requires methods that are able to learn e\ufb03ciently from incrementally acquired data. In addition, reinforcement learning generally requires function approximation methods able to handle nonstationary target functions (target functions that change over time). For example, in control methods based on GPI (generalized policy iteration) we often seek to learn q\u21e1 while \u21e1 changes. Even if the policy remains the same, the target values of training examples are nonstationary if they are generated by bootstrapping methods (DP and TD learning). Methods that cannot easily handle such nonstationarity are less suitable for reinforcement learning. Up to now we have not speci\ufb01ed an explicit objective for prediction.\n\nIn the tabular case a continuous measure of prediction quality was not necessary because the learned value function could come to equal the true value function exactly. Moreover, the learned values at each state were decoupled\u2014an update at one state a\u21b5ected no other. But with genuine approximation, an update at one state a\u21b5ects many others, and it is not possible to get the values of all states exactly correct. By assumption we have far more states than weights, so making one state\u2019s estimate more accurate invariably means making others\u2019 less accurate", "9687d209-738c-435c-8796-f06b7ff4d610": "18.7.1 Annealed Importance Sampling  In situations where Dxz(pol|p1) is large (i-e., where there is little overlap between po and p;), a strategy called annealed importance sampling (AIS) attempts to bridge the gap by introducing intermediate distributions .\n\nConsider a sequence of distributions py,...,Py,, With O = no <m <-+-< Nn-1 <n = 1 so that the first and last distributions in the sequence are py) and Pp, respectively. 623  CHAPTER 18. CONFRONTING THE PARTITION FUNCTION  This approach enables us to estimate the partition function of a multimodal distribution defined over a high-dimensional space (such as the distribution defined by a trained RBM). We begin with a simpler model with a known partition function (such as an RBM with zeros for weights) and estimate the ratio between the two model\u2019s partition functions. The estimate of this ratio is based on the estimate of the ratios of a sequence of many similar distributions, such as the sequence of RBMs with weights interpolating between zero and the learned weights", "03a6b7a0-5439-4cdc-915b-1730dda12bf1": "We can conveniently group these together using vector notation so that where \ufffd W is a matrix whose kth column comprises the D + 1-dimensional vector \ufffdwk = (wk0, wT k )T and \ufffdx is the corresponding augmented input vector (1, xT)T with a dummy input x0 = 1. This representation was discussed in detail in Section 3.1. A new input x is then assigned to the class for which the output yk = \ufffdwT k \ufffdx is largest. We now determine the parameter matrix \ufffd W by minimizing a sum-of-squares error function, as we did for regression in Chapter 3. Consider a training data set {xn, tn} where n = 1, . , N, and de\ufb01ne a matrix T whose nth row is the vector tT n, together with a matrix \ufffdX whose nth row is \ufffdxT n. The sum-of-squares error function can then be written as where \ufffdX\u2020 is the pseudo-inverse of the matrix \ufffdX, as discussed in Section 3.1.1", "467ceee0-037b-4a7a-8cde-d32014f06ffa": "The graph structure allows us to represent complicated, high-dimensional distributions with a reasonable number of parameters, but the graphs used for deep learning are usually not restrictive enough to also allow efficient inference. It is straightforward to see that computing the marginal probability of a general graphical model is ##P hard. The complexity class #P is a generalization of the complexity class NP. Problems in NP require determining only whether a problem has a solution and finding a solution if one exists. Problems in #P require counting the number of solutions. To construct a worst-case graphical model, imagine that we define a graphical model over the binary variables in a 3-SAT problem. We can impose a uniform distribution over these variables.\n\nWe can then add one binary latent variable per clause that indicates whether each clause is satisfied. We can then add another latent variable indicating whether all the clauses are satisfied. This can be done without making a large clique, by building a reduction tree of latent variables, with each node in the tree reporting whether two other variables are satisfied. The leaves of this tree are the variables for each clause. The root of the tree reports whether the entire problem is satisfied", "739a8ee1-3a93-464c-8ea0-766f65e73bfc": "This can be achieved by transforming a uniform random variable y using z = b tan y + c, which gives random numbers distributed according to. Exercise 11.7 The minimum reject rate is obtained by setting c = a \u2212 1, b2 = 2a \u2212 1 and choosing the constant k to be as small as possible while still satisfying the requirement kq(z) \u2a7e \ufffdp(z).\n\nThe resulting comparison function is also illustrated in Figure 11.5. In many instances where we might wish to apply rejection sampling, it proves dif\ufb01cult to determine a suitable analytic form for the envelope distribution q(z). An alternative approach is to construct the envelope function on the \ufb02y based on measured values of the distribution p(z) . Construction of an envelope function is particularly straightforward for cases in which p(z) is log concave, in other words when ln p(z) has derivatives that are nonincreasing functions of z. The construction of a suitable envelope function is illustrated graphically in Figure 11.6. The function ln p(z) and its gradient are evaluated at some initial set of grid points, and the intersections of the resulting tangent lines are used to construct the envelope function", "0fb8daf5-c4a1-4d8e-81c0-32fe560b5993": "The challenge of o\u21b5-policy learning can be divided into two parts, one that arises in the tabular case and one that arises only with function approximation. The \ufb01rst part of the challenge has to do with the target of the update (not to be confused with the target policy), and the second part has to do with the distribution of the updates. The techniques related to importance sampling developed in Chapters 5 and 7 deal with the \ufb01rst part; these may increase variance but are needed in all successful algorithms, tabular and approximate. The extension of these techniques to function approximation are quickly dealt with in the \ufb01rst section of this chapter. Something more is needed for the second part of the challenge of o\u21b5-policy learning with function approximation because the distribution of updates in the o\u21b5-policy case is not according to the on-policy distribution. The on-policy distribution is important to the stability of semi-gradient methods. Two general approaches have been explored to deal with this.\n\nOne is to use importance sampling methods again, this time to warp the update distribution back to the on-policy distribution, so that semi-gradient methods are guaranteed to converge (in the linear case)", "b8a37319-817b-4410-9663-431b17c26279": "A commonly used variation of REINFORCE is to subtract a baseline value from the return G; to reduce the variance of gradient estimation while keeping the bias unchanged. For example, a common baseline is state-value, and if applied, we would use A(s,a) = Q(s,a) \u2014 V(s) inthe gradient ascent update. Initialize 8 at random Generate one episode S,, A,, Ry, 5, A5,...,S7 For t=1, 2, ..., T: Estimate the the return G_t since the time step t. 6< 0+ ay'GiV In7(Ai|S:, 9). Actor-Critic  If the value function is learned in addition to the policy, we would get Actor-Critic algorithm. Critic: updates value function parameters w and depending on the algorithm it could be action- value Q(a|s; w) or state-value V(s; w). Actor: updates policy parameters 6, in the direction suggested by the critic, m(a|s; )", "8586ae15-2837-4855-b471-98cbedd68be3": "Our goal is to choose the regions Rj in order to minimize the expected loss (1.80), which implies that for each x we should minimize \ufffd k Lkjp(x, Ck).\n\nAs before, we can use the product rule p(x, Ck) = p(Ck|x)p(x) to eliminate the common factor of p(x). Thus the decision rule that minimizes the expected loss is the one that assigns each new x to the class j for which the quantity is a minimum. This is clearly trivial to do, once we know the posterior class probabilities p(Ck|x). We have seen that classi\ufb01cation errors arise from the regions of input space where the largest of the posterior probabilities p(Ck|x) is signi\ufb01cantly less than unity, or equivalently where the joint distributions p(x, Ck) have comparable values. These are the regions where we are relatively uncertain about class membership. In some applications, it will be appropriate to avoid making decisions on the dif\ufb01cult cases in anticipation of a lower error rate on those examples for which a classi\ufb01cation decision is made. This is known as the reject option", "338bdb62-3d7e-4c69-86fa-566ae242c46d": "Such generalization makes the learning potentially more powerful but also potentially more di\ufb03cult to manage and understand. Perhaps surprisingly, extending reinforcement learning to function approximation also makes it applicable to partially observable problems, in which the full state is not available to the agent. If the parameterized function form for \u02c6v does not allow the estimated value to depend on certain aspects of the state, then it is just as if those aspects are unobservable. In fact, all the theoretical results for methods using function approximation presented in this part of the book apply equally well to cases of partial observability. What function approximation can\u2019t do, however, is augment the state representation with memories of past observations. Some such possible further extensions are discussed brie\ufb02y in Section 17.3. All of the prediction methods covered in this book have been described as updates to an estimated value function that shift its value at particular states toward a \u201cbacked-up value,\u201d or update target, for that state. Let us refer to an individual update by the notation s 7! u, where s is the state updated and u is the update target that s\u2019s estimated value is shifted toward", "e0869097-20ed-4b51-8f7e-1ad080ef589b": "This suggests a different approach to solving the optimization problem for the RVM, in which we make explicit all of the dependence of the marginal likelihood (7.85) on a particular \u03b1i and then determine its stationary points explicitly . To do this, we \ufb01rst pull out the contribution from \u03b1i in the matrix C de\ufb01ned by (7.86) to give where \u03d5i denotes the ith column of \u03a6, in other words the N-dimensional vector with elements (\u03c6i(x1), . , \u03c6i(xN)), in contrast to \u03c6n, which denotes the nth row of \u03a6.\n\nThe matrix C\u2212i represents the matrix C with the contribution from basis function i removed. Using the matrix identities (C.7) and (C.15), the determinant and inverse of C can then be written where L(\u03b1\u2212i) is simply the log marginal likelihood with basis function \u03d5i omitted, and the quantity \u03bb(\u03b1i) is de\ufb01ned by and contains all of the dependence on \u03b1i", "96b74f3e-6766-41bf-960f-64cf0e7706a0": "Now the question becomes what is the best n? Which Gg) gives us the best return approximation?\n\nA common yet smart solution is to apply a weighted sum of all possible n-step TD targets rather than to pick a single best n. The weights decay by a factor A with n, A\u201d~! ; the intuition is similar to why we want to discount future rewards  https://lilianweng.github.io/posts/2018-02-19-rl-overview/  A (Long) Peek into Reinforcement Learning | Lil'Log   when computing the return: the more future we look into the less confident we would be. To make all the weight (n > co) sum up to 1, we multiply every weight by (1-A), because:  let S=1+A+A? 4+... S=14+A1+A+4A? +...) S=1+A8 S=1/(1-A)  This weighted sum of many n-step returns is called A-return GS = (1 \u2014 A) S3\u00b0\u00b0, dr-ig), TD learning that adopts A-return for value updating is labeled as TD(A)", "5240da88-6461-4346-bea8-af0b4055a8f0": "These assumptions are that the examples in each dataset are independent from each other, and that the training set and test set are identically distributed , drawn from the same probability distribution as each other. This assumption enables us to describe the data-generating process with a probability distribution over a single example. The same distribution is then used to generate every train example and every test example. We call that shared underlying distribution the data-generating dis- tribution, denoted pgata. This probabilistic framework and the i.i.d.\n\nassumptions enables us to mathematically study the relationship between training error and test error. One immediate connection we can observe between training error and test error is that the expected training error of a randomly selected model is equal to the expected test error of that model. Suppose we have a probability distribution p(x, y) and we sample from it repeatedly to generate the training set and the test set. For some fixed value w, the expected training set error is exactly the same as the expected test set error, because both expectations are formed using the same dataset sampling process. The only difference between the two conditions is the name we assign to the dataset we sample", "2d76053e-5304-4eab-bfbe-6509911a5e37": "(20.37)  Typically y is a hyperparameter fixed at the beginning of training. It is usu- ally chosen to make sure that # \u2014 w & O when the model is initialized. This reparametrization does not change the set of probability distributions that the model can represent, but it does change the dynamics of stochastic gradient descent applied to the likelihood. Specifically, in many cases, this reparametrization results in a Hessian matrix that is better conditioned. Melchior et al. experimentally confirmed that the conditioning of the Hessian matrix improves, and observed that the centering trick is equivalent to another Boltzmann machine learning technique, the enhanced gradient . The improved conditioning of the Hessian matrix enables learning to succeed, even in difficult cases like training a deep Boltzmann machine with multiple layers. The other approach to jointly training deep Boltzmann machines is the multi- prediction deep Boltzmann machine (MP-DBM), which works by viewing the mean  670  CHAPTER 20.\n\nDEEP GENERATIVE MODELS  field equations as defining a family of recurrent networks for approximately solving every possible inference problem", "2598605c-b132-4831-8a00-13e1122a1757": "\u2022 GPT uses a sentence separator () and classi\ufb01er token () which are only introduced at \ufb01ne-tuning time; BERT learns ,  and sentence A/B embeddings during pre-training. \u2022 GPT was trained for 1M steps with a batch size of 32,000 words; BERT was trained for 1M steps with a batch size of 128,000 words.\n\nGPT used the same learning rate of 5e-5 for all \ufb01ne-tuning experiments; BERT chooses a task-speci\ufb01c \ufb01ne-tuning learning rate which performs the best on the development set. To isolate the effect of these differences, we perform ablation experiments in Section 5.1 which demonstrate that the majority of the improvements are in fact coming from the two pre-training tasks and the bidirectionality they enable. The illustration of \ufb01ne-tuning BERT on different tasks can be seen in Figure 4. Our task-speci\ufb01c models are formed by incorporating BERT with one additional output layer, so a minimal number of parameters need to be learned from scratch. Among the tasks, (a) and (b) are sequence-level tasks while (c) and (d) are token-level tasks", "65fa1d50-39e9-4714-b178-92d15f74b3d4": "Because the logarithm is a monotonically increasing function of its argument, maximization of the log of a function is equivalent to maximization of the function itself. Taking the log not only simpli\ufb01es the subsequent mathematical analysis, but it also helps numerically because the product of a large number of small probabilities can easily under\ufb02ow the numerical precision of the computer, and this is resolved by computing instead the sum of the log probabilities.\n\nFrom (1.46) and (1.53), the log likelihood which is the sample mean, i.e., the mean of the observed values {xn}. Similarly, maximizing (1.54) with respect to \u03c32, we obtain the maximum likelihood solution for the variance in the form which is the sample variance measured with respect to the sample mean \u00b5ML. Note that we are performing a joint maximization of (1.54) with respect to \u00b5 and \u03c32, but in the case of the Gaussian distribution the solution for \u00b5 decouples from that for \u03c32 so that we can \ufb01rst evaluate (1.55) and then subsequently use this result to evaluate (1.56). Later in this chapter, and also in subsequent chapters, we shall highlight the signi\ufb01cant limitations of the maximum likelihood approach", "795fe842-b274-4078-8f22-9378f0a23d1b": "Since ratio matching applies specifically to binary data, this means that it acts on all fantasy states within Hamming distance 1 of the data. Ratio matching can also be useful as the basis for dealing with high-dimensional sparse data, such as word count vectors.\n\nThis kind of data poses a challenge for MCMC-based methods because the data is extremely expensive to represent in dense format, yet the MCMC sampler does not yield sparse values until the model has learned to represent the sparsity in the data distribution. Dauphin and Bengio  overcame this issue by designing an unbiased stochastic approximation to ratio matching. The approximation evaluates only a randomly selected subset of the terms of the objective and does not require the model to generate complete fantasy samples. See Marlin and de Freitas  for a theoretical analysis of the asymptotic efficiency of ratio matching", "57ba9a13-0619-4e49-914b-82b03f231302": "We have chosen this loss function to simplify the math for this example as much as possible. In practical applications, MSE is usually not an  167  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  appropriate cost function for modeling binary data. More appropriate approaches are described in section 6.2.2.2. Evaluated on our whole training set, the MSE loss function is  J(8) = 5 0 (Fw) ~ Fas)? - (6.1)  weEX  Now we must choose the form of our model, f(a;0). Suppose that we choose a linear model, with 6 consisting of w and b. Our model is defined to be  f(a; w,b) =a 'w +b. (6.2)  We can minimize J(@) in closed form with respect to w and 6 using the normal equations. After solving the normal equations, we obtain w = 0 and b = 3. The linear model simply outputs 0.5 everywhere. Why does this happen? Figure 6.1 shows how a linear model is not able to represent the XOR function", "cdeb9605-f2a1-40e7-a88f-db726d2b2891": "valL>ll$ Ewrrise /2,/7 subspace to minimize !he squared reoonslruCtioo error in 'oIhich the proje<:tion, are C.,N. We ean gh'e a ,imple physical analogy for this EM algorithm. which is easily visualized for D = 2 and M = 1. Coo,ider a collectioo nf data point' ,n tWI) dimension', aod let tile u\"\"'-dimensiunal principal subspace be represented by a <ohd rod. Now atlaCh each data point to the nxI via a ,pring oo<:)\"ing HooI;:e', law (\"umJ energy i, propol1ional 10 ,lie square of lile spring\". length)", "432ee0ab-2539-4800-90ba-a39b220492a1": "E4879, 2016, Reddy, Celani, Sejnowski, and Vergassola, Learning to Soar in Turbulent Environments. The interface between the agent and the environment required de\ufb01ning the agent\u2019s actions, the state information the agent receives from the environment, and the reward signal. By experimenting with various possibilities, Reddy et al. decided that three actions each for the angle of attack and the bank angle were enough for their purposes: increment or decrement the current bank angle and angle of attack by 5\u25e6 and 2.5\u25e6, respectively, or leave them unchanged. This resulted in 32 possible actions. The bank angle was bounded to remain between \u221215\u25e6 and +15\u25e6", "383c1cbb-a675-499e-b8de-f529f678a017": "A fourth issue that needs to be addressed in future research is that of automating the choice of tasks on which an agent works and uses to structure its developing competence. It is usual in machine learning for human designers to set the tasks that the learning agent is expected to master. Because these tasks are known in advance and remain \ufb01xed, they can be built into the learning algorithm code. However, looking ahead, we will want the agent to make its own choices about what tasks it should try to master. These might be subtasks of a speci\ufb01c overall task that is already known, or they might be intended to create building blocks that permit more e\ufb03cient learning of many di\u21b5erent tasks that the agent is likely to face in the future but which are currently unknown. These tasks may be like the auxiliary tasks or the GVFs discussed in Section 17.1, or tasks solved by options as discussed in Section 17.2", "1fcecc21-ec0b-4c8a-9783-e359e121b0fb": "As an example of how we can control a model\u2019s tendency to overfit or underfit via weight decay, we can train a high-degree polynomial regression model with different values of A. See figure 5.5 for the results. More generally, we can regularize a model that learns a function f(a; 0) by adding a penalty called a regularizer to the cost function. In the case of weight decay, the regularizer is Q(w) = w !w. In chapter 7, we will see that many other regularizers are possible. Expressing preferences for one function over another is a more general way of controlling a model\u2019s capacity than including or excluding members from the hypothesis space. We can think of excluding a function from a hypothesis space as expressing an infinitely strong preference against that function. In our weight decay example, we expressed our preference for linear functions defined with smaller weights explicitly, via an extra term in the criterion we minimize. There are many other ways of expressing preferences for different solutions, both implicitly and explicitly. Together, these different approaches are known as regularization", "6a1dca82-9095-4188-bd65-6e9771ccda98": "After applying batch normalization, we obtain the normalized hy that restores the zero mean and unit variance properties. For almost any update to the lower layers, hia will remain a unit Gaussian.\n\nThe output y may then be learned as a simple linear function y = whit. Learning in this model is now very simple because the parameters at the lower layers do not have an effect in most cases; their output is always renormalized to a unit Gaussian. In some  315  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  corner cases, the lower layers can have an effect. Changing one of the lower layer weights to 0 can make the output become degenerate, and changing the sign of one of the lower weights can flip the relationship between hy, and y. These situations are very rare. Without normalization, nearly every update would have an extreme effect on the statistics of hj_;. Batch normalization has thus made this model significantly easier to learn. In this example, the ease of learning of course came at the cost of making the lower layers useless", "5d78ec00-70c4-4332-ae1c-ebd2ea6a7aff": "RNNs are useful when we believe that the distribution over y may depend on a value of y  from the distant past in a way that is not captured by the effect of y on y ed),  One way to interpret an RNN as a graphical model is to view the RNN as defining a graphical model whose structure is the complete graph, able to represent direct dependencies between any pair of y values. The graphical model over the y  https://www.deeplearningbook.org/contents/rnn.html    values with the complete graphs structure is shown in figure 10.7. The cone graph interpretation of the is based on ignoring the hidden units h marginalizing them out of the model. It is more interesting to consider the graphical model structure of RNNs that results from regarding the hidden units h as random variables.! Including the  'The conditional distribution over these variables given their parents is deterministic. This is perfectly legitimate, though it is somewhat rare to design a graphical model with such deterministic hidden units", "cc679bed-9216-4d51-8033-768edad4c3ae": "A kernel two-sample test. J. Mach. Learn. Res., 13:723\u2013 773, 2012. Ferenc Huszar. How (not) to train your generative model: Scheduled sampling, likelihood, adversary? CoRR, abs/1511.05101, 2015. Shizuo Kakutani. Concrete representation of abstract (m)-spaces (a characterization of the space of continuous functions). Annals of Mathematics, 42(4):994\u2013 1024, 1941. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. CoRR, abs/1312.6114, 2013. Yujia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 1718\u20131727", "533ac279-7f6e-46d6-85e0-80f8394ce836": "Images are high dimensional and include  236  https://www.deeplearningbook.org/contents/regularization.html    CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  an enormous range of factors of variation, many of which can be easily simulated.\n\nOperations like translating the training images a few pixels in each direction can often greatly improve generalization, even if the model has already been designed to be partially translation invariant by using the convolution and pooling techniques described in chapter 9. Many other operations, such as rotating the image or scaling the image, have also proved quite effective. One must be careful not to apply transformations that would change the correct class. For example, optical character recognition tasks require recognizing the difference between \u201cb\u201d and \u201cd\u201d and the difference between \u201c6\u201d and \u201c9,\u201d so horizontal flips and 180\u00b0 rotations are not appropriate ways of augmenting datasets for these tasks. There are also transformations that we would like our classifiers to be invariant to but that are not easy to perform. For example, out-of-plane rotation cannot be implemented as a simple geometric operation on the input pixels. Dataset augmentation is effective for speech recognition tasks as well", "ddbec8f0-ad79-474a-81b3-272368802ff1": ".\n\nReinforcement learning in continuous state and action spaces. In M. Wiering and M. van Otterlo (Eds. ), Reinforcement Learning: State-of-the-Art, pp. 207\u2013251. Springer-Verlag Berlin Heidelberg. van Hasselt, H., Sutton, R. S. Learning to predict independent of span. ArXiv:1508.04582. Van Roy, B., Bertsekas, D. P., Lee, Y., Tsitsiklis, J. N. A neuro-dynamic programming approach to retailer inventory management. In Proceedings of the 36th IEEE Conference on Decision and Control, Vol. 4, pp. 4052\u20134057. van Seijen, H. Reinforcement Learning under Space and Time Constraints. University of van Seijen, H. E\u21b5ective multi-step temporal-di\u21b5erence learning for non-linear function van Seijen, H., Sutton, R. S", "89e24a0d-aedd-474a-ae0b-d6484ea29b01": "One can show, as Williams  did, that a team of Bernoulli-logistic REINFORCE units implements a policy gradient algorithm as a whole with respect to average rate of the team\u2019s common reward signal, where the actions are the collective actions of the team. Further, Williams  showed that a team of Bernoulli-logistic units using REINFORCE ascends the average reward gradient when the units in the team are interconnected to form a multilayer ANN. In this case, the reward signal is broadcast to all the units in the network, though reward may depend only on the collective actions of the network\u2019s output units. This means that a multilayer team of Bernoulli-logistic REINFORCE units learns like a multilayer network trained by the widely-used error backpropagation method, but in this case the backpropagation process is replaced by the broadcasted reward signal", "69d5ff44-a184-413e-aa3d-e3604a7dd99d": "\u21e4 In the maze example presented in the previous section, the changes in the model were relatively modest. The model started out empty, and was then \ufb01lled only with exactly correct information. In general, we cannot expect to be so fortunate. Models may be incorrect because the environment is stochastic and only a limited number of samples have been observed, or because the model was learned using function approximation that has generalized imperfectly, or simply because the environment has changed and its new behavior has not yet been observed. When the model is incorrect, the planning process is likely to compute a suboptimal policy. In some cases, the suboptimal policy computed by planning quickly leads to the discovery and correction of the modeling error. This tends to happen when the model is optimistic in the sense of predicting greater reward or better state transitions than are actually possible. The planned policy attempts to exploit these opportunities and in doing so discovers that they do not exist. kind of modeling error and recovery from it is shown in Figure 8.4.\n\nInitially, there is a short path from start to goal, to the right of the barrier, as shown in the upper left of the \ufb01gure", "21b0e836-5d14-4ec6-bc0b-dfbb6a66a006": "If we believe that the directions of sensitivity are somewhat axis aligned, it can make sense to use a separate learning rate for each parameter and automatically adapt these learning rates throughout the course of earning. The delta-bar-delta algorithm  is an early heuristic approach o adapting individual learning rates for model parameters during training. The approach is based on a simple idea: if the partial derivative of the loss, with respect o a given model parameter, remains the same sign, then the learning rate should increase. If that partial derivative changes sign, then the learning rate should decrease. Of course, this kind of rule can only be applied to full batch optimization.\n\nMore recently, a number of incremental (or mini batch-based) methods have  https://www.deeplearningbook.org/contents/optimization.html    been introduced that adapt the learning rates of model parameters. In this section, we briefly review a few of these algorithms. 8.5.1 AdaGrad  The AdaGrad algorithm, shown in algorithm 8.4, individually adapts the learning rates of all model parameters by scaling them inversely proportional to the square root of the sum of all the historical squared values of the gradient", "076cc6b7-0c4b-4cee-8bfa-9e9398ce54f0": "The corresponding log likelihood function is given, from (12.35), by Setting the derivative of the log likelihood with respect to JL equal to zero gives the expected result JL =xwhere xis the data mean defined by (12.1). Back-substituting we can then write the log likelihood function in the form where S is the data covariance matrix defined by (12.3). Because the log likelihood is a quadratic function of JL, this solution represents the unique maximum, as can be confirmed by computing second derivatives. Maximization with respect to W and 0'2 is more complex but nonetheless has an exact closed-form solution. It was shown by Tipping and Bishop  that all of the stationary points of the log likelihood function can be written as where U M is a D x M matrix whose columns are given by any subset (of size M) of the eigenvectors of the data covariance matrix S, the M x M diagonal matrix L M has elements given by the corresponding eigenvalues ..\\, and R is an arbitrary M x M orthogonal matrix", "a7e19b4d-98a8-4f6a-925b-cceb6fe6d115": "Such a function is called a discriminant function. In fact, we can identify three distinct approaches to solving decision problems, all of which have been used in practical applications. These are given, in decreasing order of complexity, by: (a) First solve the inference problem of determining the class-conditional densities p(x|Ck) for each class Ck individually. Also separately infer the prior class probabilities p(Ck). Then use Bayes\u2019 theorem in the form Equivalently, we can model the joint distribution p(x, Ck) directly and then normalize to obtain the posterior probabilities. Having found the posterior probabilities, we use decision theory to determine class membership for each new input x.\n\nApproaches that explicitly or implicitly model the distribution of inputs as well as outputs are known as generative models, because by sampling from them it is possible to generate synthetic data points in the input space. (b) First solve the inference problem of determining the posterior class probabilities p(Ck|x), and then subsequently use decision theory to assign each new x to one of the classes. Approaches that model the posterior probabilities directly are called discriminative models", "345f0157-089b-40fc-994c-663633439696": "The clear formalism provided by reinforcement learning that systemizes tasks, returns, and algorithms is proving to be enormously useful in making sense of experimental data, in suggesting new kinds of experiments, and in pointing to factors that may be critical to manipulate and to measure. The idea of optimizing return over the long term that is at the core of reinforcement learning is contributing to our understanding of otherwise puzzling features of animal learning and behavior. Some of the correspondences between reinforcement learning and psychological theories are not surprising because the development of reinforcement learning drew inspiration from psychological learning theories. However, as developed in this book, reinforcement learning explores idealized situations from the perspective of an arti\ufb01cial intelligence researcher or engineer, with the goal of solving computational problems with e\ufb03cient algorithms, rather than to replicate or explain in detail how animals learn.\n\nAs a result, some of the correspondences we describe connect ideas that arose independently in their respective \ufb01elds. We believe these points of contact are specially meaningful because they expose computational principles important to learning, whether it is learning by arti\ufb01cial or by natural systems. For the most part, we describe correspondences between reinforcement learning and learning theories developed to explain how animals like rats, pigeons, and rabbits learn in controlled laboratory experiments. Thousands of these experiments were conducted throughout the 20th century, and many are still being conducted today", "857f46a2-6f45-43f3-8f2e-b6a1495f7ab9": "We can represent the unfolded recurrence after \u00a2 steps with a function g: hn) =g (a), 2) al) ae?) aD) (10.6) =f(h\u2014) a; 8).\n\n(10.7) The function g\u00ae takes the whole past sequence (a), x) a9)... ea)  as input and produces the current state, but the unfolded recurrent structure allows us to factorize g into repeated application of a function f. The unfolding  a1 toa 1 4 + 1  https://www.deeplearningbook.org/contents/rnn.html    process LLLUS LWILFOGUCES LWO IlajJOL AAVALLLAXeS:  1. Regardless of the sequence length, the learned model always has the same input size, because it is specified in terms of transition from one state to another state, rather than specified in terms of a variable-length history of states. 2. It is possible to use the same transition function f with the same parameters at every time step", "cd43c722-8dd7-43ce-8009-ea3e4012152b": "This approach is based on using sigmoid output units combined with maximum likelihood. A sigmoid output unit is defined by j=o (wh + b) (6.19)  where g is the logistic sigmoid function described in section 3.10. We can think of the sigmoid output unit as having two components. First, it  uses a linear layer to compute z =w!h +b. Next, it uses the sigmoid activation function to convert z into a probability.\n\nWe omit the dependence on \u00abx for the moment to discuss how to define a probability distribution over y using the value z. The sigmoid can be motivated by constructing an unnormalized probability distribution P(y), which does not sum to 1. We can then divide by an appropriate constant to obtain a valid probability distribution. If we begin with the assumption that the unnormalized log probabilities are linear in y and z, we can exponentiate to obtain the unnormalized probabilities", "27ad1581-358c-43ed-9d75-ac8b0014cdf5": "During training, forward propagation can continue onward until it produces a scalar cost J(@). The back-propagation algorithm , often simply called backprop, allows the information from the cost to then flow backward through the network in order to compute the gradient. Computing an analytical expression for the gradient is straightforward, but numerically evaluating such an expression can be computationally expensive. The back-propagation algorithm does so using a simple and inexpensive procedure.\n\nThe term back-propagation is often misunderstood as meaning the whole learning algorithm for multi layer neural networks. Actually, back-propagation refers only to the method for computing the gradient, while another algorithm, such as stochastic gradient descent, is used to perform learning using this gradient. Furthermore, back-propagation is often misunderstood as being specific to multi- layer neural networks, but in principle it can compute derivatives of any function (for some functions, the correct response is to report that the derivative of the function is undefined)", "44c91198-f242-48b5-ae8b-152947949786": "This is achieved by expressing f(x) as a linear combination of radial basis functions, one centred on every data point The values of the coef\ufb01cients {wn} are found by least squares, and because there are the same number of coef\ufb01cients as there are constraints, the result is a function that \ufb01ts every target value exactly. In pattern recognition applications, however, the target values are generally noisy, and exact interpolation is undesirable because this corresponds to an over-\ufb01tted solution. Expansions in radial basis functions also arise from regularization theory .\n\nFor a sum-of-squares error function with a regularizer de\ufb01ned in terms of a differential operator, the optimal solution is given by an expansion in the Green\u2019s functions of the operator (which are analogous to the eigenvectors of a discrete matrix), again with one basis function centred on each data point. If the differential operator is isotropic then the Green\u2019s functions depend only on the radial distance from the corresponding data point. Due to the presence of the regularizer, the solution no longer interpolates the training data exactly. Another motivation for radial basis functions comes from a consideration of the interpolation problem when the input (rather than the target) variables are noisy", "8c35e32a-3706-448f-9ae4-791146781d4f": "The stationary point need not, however, be a local maximum. In  it is shown that it is possible for the algorithm to converge to local minima or saddle points in unusual cases.\n\nIn the formulation of the EM algorithm described above, \u03b8n+1 was chosen as the value of \u03b8 for which \u2206(\u03b8|\u03b8n) was maximized. While this ensures the greatest increase in L(\u03b8), it is however possible to relax the requirement of maximization to one of simply increasing \u2206(\u03b8|\u03b8n) so that \u2206(\u03b8n+1|\u03b8n) \u2265 \u2206(\u03b8n|\u03b8n). This approach, to simply increase and not necessarily maximize \u2206(\u03b8n+1|\u03b8n) is known as the Generalized Expectation Maximization (GEM) algorithm and is often useful in cases where the maximization is di\ufb03cult. The convergence of the GEM algorithm can be argued as above. A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society: Series B, 39(1):1\u201338, November 1977. R", "f120c9b9-2446-4116-b19e-8b36a07d0c16": "E., Berthier, N. E., Blazis, E. J., Sutton, R. S., Barto, A. G. Simulation of the classically conditioned nictitating membrane response by a neuron-like adaptive element: I. Response topography, neuronal \ufb01ring, and interstimulus intervals. Behavioural Brain Research, 21(2):143\u2013154. Moore, J. W., Marks, J. S., Castagna, V. E., Polewan, R. J. Parameter stability in the TD model of complex CR topographies. In Society for Neuroscience Abstracts, 27:642. Moore, J. W., Schmajuk, N. A. Kamin blocking. Scholarpedia, 3(5):3542. Moore, J. W., Stickney, K. J. Formation of attentional-associative networks in real Mukundan, J., Mart\u00b4\u0131nez, J. F", "4a8a91c3-b917-447d-8e75-946746720713": "In this chapter we show how DP can be used to compute the value functions de\ufb01ned in Chapter 3.\n\nAs discussed there, we can easily obtain optimal policies once we have found the optimal value functions, v\u21e4 or q\u21e4, which satisfy the Bellman optimality equations: for all s 2 S, a 2 A(s), and s0 2 S+. As we shall see, DP algorithms are obtained by turning Bellman equations such as these into assignments, that is, into update rules for improving approximations of the desired value functions. First we consider how to compute the state-value function v\u21e1 for an arbitrary policy \u21e1. This is called policy evaluation in the DP literature. We also refer to it as the prediction problem. Recall from Chapter 3 that, for all s 2 S, where \u21e1(a|s) is the probability of taking action a in state s under policy \u21e1, and the expectations are subscripted by \u21e1 to indicate that they are conditional on \u21e1 being followed. The existence and uniqueness of v\u21e1 are guaranteed as long as either \u03b3 < 1 or eventual termination is guaranteed from all states under the policy \u21e1", "d8e80cb6-9e64-4661-a162-1d7cf0c00b33": "In other words, J is contractive if its image of the unit sphere is completely encapsulated by the unit sphere. We can think of the CAE as penalizing the Frobenius norm of the local linear approximation of f(a) at every training point x in order to encourage each of these local linear operators to become a contraction. As described in section 14.6, regularized autoencoders learn manifolds by balancing two opposing forces. In the case of the CAE, these two forces are reconstruction error and the contractive penalty Q(h). Reconstruction error alone would encourage the CAE to learn an identity function.\n\nThe contractive penalty alone would encourage the CAE to learn features that are constant with respect to x. The compromise between these two forces yields an autoencoder whose derivatives Sis) are mostly tiny. Only a small number of hidden units, corresponding to a small number of directions in the input, may have significant derivatives. The goal of the CAE is to learn the manifold structure of the data. Directions x with large Jz rapidly change h, so these are likely to be directions that approximate the tangent planes of the manifold", "b872ff7d-d625-4cb1-a334-f723273c1505": "This is the conjugate prior distribution for a multivariate Gaussian N(x|\u00b5, \u039b) in which both the mean \u00b5 and the precision \u039b are unknown, and is also called the normal-Wishart distribution. It comprises the product of a Gaussian distribution for \u00b5, whose precision is proportional to \u039b, and a Wishart distribution over \u039b. For the particular case of a scalar x, this is equivalent to the Gaussian-gamma distribution. If we generalize the Bernoulli distribution to an K-dimensional binary variable x with components xk \u2208 {0, 1} such that \ufffd where Ijk is the j, k element of the identity matrix. Because p(xk = 1) = \u00b5k, the parameters must satisfy 0 \u2a7d \u00b5k \u2a7d 1 and \ufffd k \u00b5k = 1.\n\nThe multinomial distribution is a multivariate generalization of the binomial and gives the distribution over counts mk for a K-state discrete variable to be in state k given a total number of observations N. gives the number of ways of taking N identical objects and assigning mk of them to bin k for k = 1,", "e0826407-6dc5-4988-a2ab-33a76b028d02": "(1) In the next section, we present a theoretical analysis of adversarial nets, essentially showing that the training criterion allows one to recover the data generating distribution as G and D are given enough capacity, i.e., in the non-parametric limit. See Figure 1 for a less formal, more pedagogical explanation of the approach. In practice, we must implement the game using an iterative, numerical approach. Optimizing D to completion in the inner loop of training is computationally prohibitive, and on \ufb01nite datasets would result in over\ufb01tting. Instead, we alternate between k steps of optimizing D and one step of optimizing G. This results in D being maintained near its optimal solution, so long as G changes slowly enough.\n\nThe procedure is formally presented in Algorithm 1. In practice, equation 1 may not provide suf\ufb01cient gradient for G to learn well. Early in learning, when G is poor, D can reject samples with high con\ufb01dence because they are clearly different from the training data. In this case, log(1 \u2212 D(G(z))) saturates", "a20e8cf1-2012-46c9-b873-ce24867cfe54": "|t does demand access to supervised dataset in which we know which text matches which image. It is trained on 400 million (text, image) pairs, collected from the Internet. The query list contains all the words occurring at least 100 times in the English version of Wikipedia. Interestingly, they found that Transformer-based language models are 3x slower than a bag-of-words (BoW) text encoder at zero-shot ImageNet classification.\n\nUsing contrastive objective instead of trying to predict the exact words associated with images (i.e. a method commonly adopted by image caption prediction tasks)  can further improve the data efficiency another 4x. Nn wwe uo ons  Zero-Shot ImageNet Accuracy N o  \u2014\u00ae Bag of Words Contrastive (CLIP)  5 \u2014\u00ae- Bag of Words Prediction \u2014\u00ae- Transformer Language Model  i) 2M 33M 67M 134M 268M # of images processed  400M  CLIP produces good visual representation that can non-trivially transfer to many CV benchmark datasets, achieving results competitive with supervised baseline", "f020d922-82f9-4449-9283-41b051327e20": "), Learning and Computational Neuroscience: Foundations of Adaptive Networks, pp. 497\u2013537. MIT Press, Cambridge, MA.\n\nSutton, R. S., Maei, H. R., Precup, D., Bhatnagar, S., Silver, D., Szepesv\u00b4ari, Cs., Wiewiora, E. Fast gradient-descent methods for temporal-di\u21b5erence learning with linear function approximation. In Proceedings of the 26th International Conference on Machine Learning , pp. 993\u20131000. ACM, New York. Sutton, R. S., Szepesv\u00b4ari, Cs., Maei, H. R. A convergent O(d2) temporal-di\u21b5erence algorithm for o\u21b5-policy learning with linear function approximation. In Advances in Neural Information Processing Systems 21 , pp. 1609\u20131616. Curran Associates, Inc. Sutton, R", "b6595be5-fc03-43e4-8172-6e3a598a49dc": "For example, if you learn to predict and control your sensors over short time scales, say seconds, then you might plausibly come up with part of the idea of objects, which would then greatly help with the prediction and control of long-term reward. We might imagine an arti\ufb01cial neural network (ANN) in which the last layer is split into multiple parts, or heads, each working on a di\u21b5erent task. One head might produce the approximate value function for the main task (with reward as its cumulant) whereas the others would produce solutions to various auxiliary tasks.\n\nAll heads could propagate errors by stochastic gradient descent into the same body\u2014the shared preceding part of the network\u2014which would then try to form representations, in its next-to-last layer, to support all the heads. Researchers have experimented with auxiliary tasks such as predicting change in pixels, predicting the next time step\u2019s reward, and predicting the distribution of the return. In many cases this approach has been shown to greatly accelerate learning on the main task . Multiple predictions have similarly been repeatedly proposed as a way of directing the construction of state estimates (see Section 17.3)", "ebfbb0b6-3847-4574-bfad-121181d933c2": "In many situations this will not be the case, for instance if a sensor fails to return a value whenever the quantity it is measuring exceeds some threshold. We now consider the application of this latent variable view of EM to the speci\ufb01c case of a Gaussian mixture model. Recall that our goal is to maximize the log likelihood function (9.14), which is computed using the observed data set X, and we saw that this was more dif\ufb01cult than for the case of a single Gaussian distribution due to the presence of the summation over k that occurs inside the logarithm. Suppose then that in addition to the observed data set X, we were also given the values of the corresponding discrete variables Z. Recall that Figure 9.5(a) shows a \u2018complete\u2019 data set (i.e., one that includes labels showing which component generated each data point) while Figure 9.5(b) shows the corresponding \u2018incomplete\u2019 data set. The graphical model for the complete data is shown in Figure 9.9.\n\nNow consider the problem of maximizing the likelihood for the complete data set {X, Z}. From (9.10) and (9.11), this likelihood function takes the form where znk denotes the kth component of zn", "537c7d75-0459-4ab1-8d65-2f4a0c0d1d28": "Brie\ufb02y, the objective of \u03c6 is to maximize the ultimate measure of the performance of model p\u03b8(y|x), which, in the context of supervised learning, is the model performance on a held-out validation set. The algorithm optimizes \u03b8 and \u03c6 alternatingly, corresponding to Eq. (5) and Eq. (6), respectively. More concretely, in each iteration, we \ufb01rst update the model parameters \u03b8 in analogue to Eq. (5) which optimizes intrinsic reward-enriched objective. Here, we optimize the log-likelihood of the training set enriched with data manipulation. That is, we replace R\u03b4 with R\u03c6 in Eq. (4), and obtain the augmented M-step: By noticing that the new \u03b8\u2032 depends on \u03c6, we can write \u03b8\u2032 as a function of \u03c6, namely, \u03b8\u2032 = \u03b8\u2032(\u03c6). The practical implementation of the above update depends on the actual parameterization of manipulation R\u03c6, which we discuss in more details in the next section. The next step is to optimize \u03c6 in terms of the model validation performance, in analogue to Eq.(6)", "e1de5628-6838-4d90-b918-6fefc33f5fae": "For a particular starting pair, s, a, let b be the branching factor (i.e., the number of possible next states, s0, for which \u02c6p(s0|s, a) > 0). Then an expected update of this pair requires roughly b times as much computation as a sample update. If there is enough time to complete an expected update, then the resulting estimate is generally better than that of b sample updates because of the absence of sampling error. But if there is insu\ufb03cient time to complete an expected update, then sample updates are always preferable because they at least make some improvement in the value estimate with fewer than b updates. In a large problem with many state\u2013action pairs, we are often in the latter situation. With so many state\u2013action pairs, expected updates of all of them would take a very long time. Before that we may be much better o\u21b5 with a few sample updates at many state\u2013action pairs than with expected updates at a few pairs", "8e614565-757f-4b9d-8774-89d3fa8aaac9": "The RNN, when used as a Turing machine, takes a binary sequence as input, and its outputs must be discretized to provide a binary output. It is possible to compute all functions in this setting using a single specific RNN of finite size (Siegelmann and Sontag|1995] use 886 units). The \u201cinput\u201d of the Turing machine is  372  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  \u00a9)  2 994 \u00a9) w) &) &) C) oe) &) &)  \u2014_>> V Unfold Vv Vv Vv Ww -- -- \u00a2 N w w, N fo G)k () (+1) } Bet 6) 1 Ny 7] NN 4  https://www.deeplearningbook.org/contents/rnn.html    Figure 10.3: The computational graph to compute the training loss of a recurrent network that maps an input sequence of a values to a corresponding sequence of output o values. A loss [ measures how far each o is from the corresponding training target y. When using softmax outputs, we assume o is the unnormalized log probabilities", "3d5f6b26-3039-449d-bad8-5d5e916da4f0": "618  CHAPTER 18. CONFRONTING THE PARTITION FUNCTION  NCE works by reducing the unsupervised learning problem of estimating p(x) to that of learning a probabilistic binary classifier in which one of the categories corresponds to the data generated by the model. This supervised learning problem is constructed in such a way that maximum likelihood estimation defines an asymptotically consistent estimator of the original problem. Specifically, we introduce a second distribution, the noise distribution pyoise(X). The noise distribution should be tractable to evaluate and to sample from. We can now construct a model over both x and a new, binary class variable y. In the new joint model, we specify that  Pjoint(Y = 1) = 9?\n\n(18.29) Pjoint(X | y = 1) = Pmodel(X); (18.30)  and Pioint(X | y = 0) = Pnoise (X).- (18.31)  In other words, y is a switch variable that determines whether we will generate x from the model or from the noise distribution. We can construct a similar joint model of training data", "6da49a4b-afeb-4bd5-bad3-3f1c5354152d": "In the example of the world\u2019s surface as a manifold, one can walk north, south, east, or west.\n\nAlthough there is a formal mathematical meaning to the term \u201cmanifold,\u201d in machine learning it tends to be used more loosely to designate a connected set of points that can be approximated well by considering only a small number of  157  FARR OA FA DATA DOL  https://www.deeplearningbook.org/contents/ml.html    UAL LEU J. IVA LH AULT DALY  \u201c0.5 1.0 15 2.0 2.5 3.0 3.5 4.0  Figure 5.11: Data sampled from a distribution in a two-dimensional space that is actually concentrated near a one-dimensional manifold, like a twisted string. The solid line indicates the underlying manifold that the learner should infer. degrees of freedom, or dimensions, embedded in a higher-dimensional space. Each dimension corresponds to a local direction of variation. See figure 5.11 for an example of training data lying near a one-dimensional manifold embedded in two- dimensional space. In the context of machine learning, we allow the dimensionality of the manifold to vary from one point to another", "1b19cf6f-eabf-4359-8d62-31597d361b5f": "In a few minutes, the ball was caroming o\u21b5 the walls of the box as if the pigeon had been a champion squash player. (Skinner, 1958, p. 94) Not only did the pigeon learn a behavior that is unusual for pigeons, it learned quickly through an interactive process in which its behavior and the reinforcement contingencies changed in response to each other. Skinner compared the process of altering reinforcement contingencies to the work of a sculptor shaping clay into a desired form. Shaping is a powerful technique for computational reinforcement learning systems as well. When it is di\ufb03cult for an agent to receive any non-zero reward signal at all, either due to sparseness of rewarding situations or their inaccessibility given initial behavior, starting with an easier problem and incrementally increasing its di\ufb03culty as the agent learns can be an e\u21b5ective, and sometimes indispensable, strategy. A concept from psychology that is especially relevant in the context of instrumental conditioning is motivation, which refers to processes that in\ufb02uence the direction and strength, or vigor, of behavior.\n\nThorndike\u2019s cats, for example, were motivated to escape from puzzle boxes because they wanted the food that was sitting just outside", "04c0847e-8fcc-41ad-a180-8934d6987132": "Supervised learning provides a very strong clue: a label y, presented with each a, that usually specifies the value of at least one of the factors of variation directly. More generally, to make use of abundant unlabeled data, representation learning makes use of other, less direct hints about the underlying factors. These hints take the form of implicit prior beliefs that we, the designers of the learning algorithm, impose in order to guide the learner. Results such as the no free lunch theorem show that regularization strategies are necessary to obtain good generalization. While it is impossible to find a universally superior regularization strategy, one goal of deep learning is to find a set of fairly generic regularization strategies that are applicable  https://www.deeplearningbook.org/contents/representation.html    to a wide variety of AI tasks, similar to the tasks that people and animals are able  and has been partially expanded here. e Smoothness: This is the assumption that f(a + ed) \u00a9 f(a) for unit d and small \u00ab. This assumption allows the learner to generalize from training  552  CHAPTER 15", "a0938144-e325-4271-8988-695fd213b1b7": "The reduced CIFAR-10 results demonstrate the usefulness of the SamplePairing technique in limited data applications (Fig. 8). Another detail found in the study is that better results were obtained when mixing images from the entire training set rather than from instances exclusively belonging to the same class. Starting from a training set of size N, SamplePairing produces a dataset of size N\u2019-+N. In addition, Sample Pairing can be stacked on top of other  augmentation techniques", "8d481e2a-736d-455b-b1a1-b42e29f094cd": "A context vectorc is formed by taking a weighted average of feature vectors h) with weights a\u00ae .\n\nIn some applications, the feature vectorsh are hidden units of a neural network, but they may also be raw input to the model. The weights a are produced by the model itself. They are usually values in the interval  and are intended to concentrate around just one h\u00ae so that the weighted average approximates reading that one specific time step precisely. The weights a) are usually produced by applying a softmax function to relevance scores emitted by another portion of the model. The attention mechanism is more expensive computationally than directly indexing the desired h), but direct indexing cannot be trained with gradient descent. The attention mechanism based on weighted averages is a smooth, differentiable approximation that can be trained with existing optimization algorithms. 471  CHAPTER 12. APPLICATIONS  rates than traditional approaches based on the frequency counts in the phrase table. There is even earlier work on learning cross-lingual word vectors . Many extensions to this approach are possible. For example, more efficient cross-lingual alignment  allows training on larger datasets", "f503eaad-f37b-43ee-80ee-6bee2a8af2db": "Furthermore, the terms involving \u00b5 and \u039b themselves comprise a sum over k of terms involving \u00b5k and \u039bk leading to the further factorization Identifying the terms on the right-hand side of (10.54) that depend on \u03c0, we have where we have used (10.50). Taking the exponential of both sides, we recognize q\u22c6(\u03c0) as a Dirichlet distribution Finally, the variational posterior distribution q\u22c6(\u00b5k, \u039bk) does not factorize into the product of the marginals, but we can always use the product rule to write it in the form q\u22c6(\u00b5k, \u039bk) = q\u22c6(\u00b5k|\u039bk)q\u22c6(\u039bk). The two factors can be found by inspecting (10.54) and reading off those terms that involve \u00b5k and \u039bk.\n\nThe result, as expected, is a Gaussian-Wishart distribution and is given by Exercise 10.13 These update equations are analogous to the M-step equations of the EM algorithm for the maximum likelihood solution of the mixture of Gaussians", "1f212391-5d84-487b-8d06-07267c6af3c3": "After playing about 300,000 games against itself, TD-Gammon 0.0 as described above learned to play approximately as well as the best previous backgammon computer programs.\n\nThis was a striking result because all the previous high-performance computer programs had used extensive backgammon knowledge. For example, the reigning champion program at the time was, arguably, Neurogammon, another program written by Tesauro that used an ANN but not TD learning. Neurogammon\u2019s network was trained on a large training corpus of exemplary moves provided by backgammon experts, and, in addition, started with a set of features specially crafted for backgammon. Neurogammon was a highly tuned, highly e\u21b5ective backgammon program that decisively won the World Backgammon Olympiad in 1989. TD-Gammon 0.0, on the other hand, was constructed with essentially zero backgammon knowledge. That it was able to do as well as Neurogammon and all other approaches is striking testimony to the potential of self-play learning methods. The tournament success of TD-Gammon 0.0 with zero expert backgammon knowledge suggested an obvious modi\ufb01cation: add the specialized backgammon features but keep the self-play TD learning method", "75bd1499-3845-481b-a014-581a4c6ef223": "In principle, this approach could fail due to optimization difficulties, but for many  425  CHAPTER 11.\n\nPRACTICAL METHODOLOGY  Hyperparameter | Increases | Reason Caveats  capacity  when...  increased | Increasing the number of | Increasing the number  Number of hid-  https://www.deeplearningbook.org/contents/guidelines.html  den units  hidden units increases the representational capacity of the model. ot hidden, units increases both the time and memory cost of essentially every op- eration on the model. Learning rate  tuned op- timally  An improper learning rate, whether too high or too low, results in a model with low effective capac- ity due to optimization fail- ure. Convolution ker- nel width  increased  Increasing the kernel width increases the number of pa- rameters in the model. A wider kernel results in a narrower output di- mension, reducing model capacity unless you use implicit zero padding to reduce this effect. Wider kernels require more mem- ory for parameter storage and increase runtime, but a narrower output reduces memory cost. Implicit zero padding  increased  Adding implicit zeros be- fore convolution keeps the representation size large", "e2c8959c-4195-4b77-b08c-1509daef8fe5": "Curriculum learning, a term originally coined by Bengio et al. , is an applicable concept for all Deep Learning models, not just those constrained with limited data. Plot- ting out training accuracy over time across different initial training subsets could help reveal patterns in the data that dramatically speed up training time. Data Augmentation constructs massively inflated training from combinations such as flipping, translating, and randomly erasing. It is highly likely that a subset exists in this set such that training will be faster and more accurate. Resolution impact Another interesting discussion about Data Augmentation in images is the impact of res- olution.\n\nHigher resolution images such as HD (1920 x 1080 x 3) or 4 K (3840 x 2160 x 3) require much more processing and memory to train deep CNNs. However, it seems intuitive that next-generation models would be trained on higher resolution images. Many current models downsample images from their original resolution to make the classification problem computationally more feasible. However, sometimes this down- sampling causes information loss within the image, making image recognition more dif- ficult (Table 10)", "b0be4c36-b1b9-4011-8f93-75c873b31f3b": "Scienti\ufb01c Articles (Chem) With modern online repositories of scienti\ufb01c literature, such as PubMed13 for biomedical articles, research results are more accessible than ever before. However, actually extracting \ufb01ne-grained pieces of information in a structured format and using this data to answer speci\ufb01c questions at scale remains a signi\ufb01cant open challenge for researchers.\n\nTo address this challenge in the context of drug safety research, Stanford and US Food and Drug Administration (FDA) collaborators used Snorkel to develop a system for extracting chemical reagent and reaction product relations from PubMed abstracts. The goal was to build a database of chemical reactions that researchers at the FDA can use to predict unknown drug interactions. We used the chemical reactions described in the Metacyc database  for distant supervision. Electronic Health Records (EHR) As patients\u2019 clinical records increasingly become digitized, researchers hope to inform clinical decision making by retrospectively analyzing large patient cohorts, rather than conducting expensive randomized controlled studies. However, much of the valuable information in electronic health records (EHRs)\u2014such as \ufb01ne-grained clinical details, practitioner notes, etc.\u2014is not contained in standardized medical coding systems and is thus locked away in the unstructured text notes sections", "d85b2510-ec38-4ddc-bb4e-d31d2b1b8543": "Nevertheless, there are still reasons to think that the general idea outlined in this section applies to the approximate case. The general idea is that a state that is good for some predictions is also good for others (in particular, that a Markov state, su\ufb03cient for one-step predictions, is also su\ufb03cient for all others). If we step back from that speci\ufb01c result for the Markov case, the general idea is similar to what we discussed in Section 17.1 with multi-headed learning and auxiliary tasks. We discussed how representations that were good for the auxiliary tasks were often also good for the main task.\n\nTaken together, these suggest an approach to both partial observability and representation learning in which multiple predictions are pursued and used to direct the construction of state features. The guarantee provided by the perfect-but-impractical Markov property is replaced by the heuristic that what\u2019s good for some predictions may be good for others. This approach scales well with computational resources. With a large machine one could experiment with large numbers of predictions, perhaps favoring those that are most similar to the ones of ultimate interest or that are easiest to learn reliably, or according to some other criteria. It is important here to move beyond selecting the predictions manually. The agent should do it", "6725234b-c486-49fc-8e17-a3d3dbf50f6f": "Unfortunately, in most practical settings, this gradient is unavailable, either because of its high computation and memory cost, or because of hyperparameters that have intrinsically nondifferentiable interactions with the validation set error, as in the case of discrete-valued hyperparameters. To compensate for this lack of a gradient, we can build a model of the validation set error, then propose new hyperparameter guesses by performing optimization within this model. Most model-based algorithms for hyperparameter search use a Bayesian regression model to estimate both the expected value of the validation set error for each hyperparameter and the uncertainty around this expectation. Opti- mization thus involves a trade-off between exploration (proposing hyperparameters for that there is high uncertainty, which may lead to a large improvement but may also perform poorly) and exploitation (proposing hyperparameters that the model is confident will perform as well as any hyperparameters it has seen so far\u2014usually hyperparameters that are very similar to ones it has seen before).\n\nContemporary approaches to hyperparameter optimization include Spearmint , TPE  and SMAC", "6d272bd5-c641-4064-96d1-9c76052babbc": "Whether or not this is so depends critically on how the states are represented in terms of features, which we investigate in this large section. Choosing features appropriate to the task is an important way of adding prior domain knowledge to reinforcement learning systems. Intuitively, the features should correspond to the aspects of the state space along which generalization may be appropriate. If we are valuing geometric objects, for example, we might want to have features for each possible shape, color, size, or function. If we are valuing states of a mobile robot, then we might want to have features for locations, degrees of remaining battery power, recent sonar readings, and so on.\n\nA limitation of the linear form is that it cannot take into account any interactions between features, such as the presence of feature i being good only in the absence of feature j. For example, in the pole-balancing task (Example 3.4) high angular velocity can be either good or bad depending on the angle. If the angle is high, then high angular velocity means an imminent danger of falling\u2014a bad state\u2014whereas if the angle is low, then high angular velocity means the pole is righting itself\u2014a good state", "49e470fb-c9c4-488c-b14f-d6b73fe99393": "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshop on Generative-Model Based Vision, 2004. Gidaris, S., Singh, P., and Komodakis, N. Unsupervised representation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., WardeFarley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial nets. In Advances in neural information processing systems, pp. 2672\u20132680, 2014. A Simple Framework for Contrastive Learning of Visual Representations Goyal, P., Doll\u00e1r, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y., and He, K", "88bdab00-ef4c-4db1-890a-9ba58b1e1651": "The PFD approach  shows that minimizing J(p) w.r.t p can equivalently be done by solving the following saddle-point problem: where \u03d5 : T \u2192 R is a continuous function, and J\u2217(\u03d5) = suph Eh \u2212J(h) is the convex conjugate of J. In the case of J(p) being the JS divergence with pd, if we approximate the above optimization by parameterizing \u03d5(t) as \u03d5\u03c6 = 1 2 log 2 where C\u03c6 : T \u2192  is a binary classi\ufb01er (a.k.a., discriminator) and p as p\u03b8 (i.e., the target model), Equation 5.5 recovers the original GAN algorithm : 5.3. Wasserstein Distance. Another distance measure that is receiving increasingly interest is the Wasserstein distance, a member of the optimal transport distance family", "6fdbca83-7683-4bb0-8abe-4e6af041acaf": "MACHINE LEARNING BASICS  bution with mean @:  P(x: 6) = 9? (1 \u2014 ot), (5.21)  A common estimator for the @ parameter of this distribution is the mean of the  - 1a, m= \u2014 Sov, (5.22) m i=l  To determine whether this estimator is biased, we can substitute equation 5.22 into equation 5.20:  training samples:  bias(@m) = E \u2014 9 (5.23 _ |= se) _9 (5.24 i=l -1y  -0 (5.25 , , i=1 \u00a2=0  ll S|r Ms => S Ye | S ~ or i) a  Since bias(8) = 0, we say that our estimator is unbiased", "3909a7a2-1e6a-496d-b05c-22ea76f6b5b0": "Model averaging is an extremely powerful and reliable method for reducing generalization error. Its use is usually discouraged when benchmarking algorithms for scientific papers, because any machine learning algorithm can benefit substan- tially from model averaging at the price of increased computation and memory. 4 When both the original and the resampled dataset containm examples, the exact proportion of examples missing from the new dataset is (1 \u2014 3)\".\n\nThis is the chance that a particular example is not chosen among the m possible source examples for allm draws used to create the  new dataset. As m approaches infinity this quantity converges to Z, which is slightly larger than 1  3\u00b0  254  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  For this reason, benchmark comparisons are usually made using a single model. Machine learning contests are usually won by methods using model averag-  ing over dozens of models. A recent prominent example is the Netflix Grand Prize . Not all techniques for constructing ensembles are designed to make the ensemble more regularized than the individual models. For example, a technique called boosting (Freund and Schapire, 1996b,a) constructs an ensemble with higher capacity than the individual models", "7d5207e9-6dbf-4d86-b1aa-e0f92d00d591": "In the special unconstrained form, the interplay between the exogenous experience, divergence, and the endogenous uncertainty become more explicit. Optimization: Teacher-student mechanism.\n\nThe introduction of the auxiliary distribution q relaxes the learning problem of p\u03b8, originally only over \u03b8, to be now alternating between q and \u03b8. Here q acts as a conduit between the exogenous experience and the target model: it on the one hand subsumes the experience (by maximizing the expected f value), and on the other hand passes it incrementally to the target model (by minimizing the divergence D). The following \ufb01xed point iteration between q and \u03b8 illustrates this optimization strategy under the SE. Let us plug into Equation 3.2 the popular cross entropy (CE) as the divergence function, that is, D(q, p\u03b8) = \u2212Eq, and Shannon entropy as the uncertainty measure, that is, H(q) = \u2212Eq. We further assume the experience f is independent of the model parameters \u03b8 (the assumption is indeed not necessary for the teacher step). We have, at iteration n: where Z is the normalization factor", "420398dd-6b31-488d-8394-50f31e465a0f": "Otherwise, no estimator can recover Pdata-  e The true distribution p gata must correspond to exactly one value of 8. Oth- erwise, maximum likelihood can recover the correct pgata but will not be able to determine which value of 8 was used by the data-generating process.\n\nThere are other inductive principles besides the maximum likelihood estimator, many of which share the property of being consistent estimators. Consistent estimators can differ, however, in their statistical efficiency, meaning that one consistent estimator may obtain lower generalization error for a fixed number of samples m, or equivalently, may require fewer examples to obtain a fixed level of generalization error. Statistical efficiency is typically studied in the parametric case (as in linear regression), where our goal is to estimate the value of a parameter (assuming it is possible to identify the true parameter), not the value of a function. A way to measure how close we are to the true parameter is by the expected mean squared error, computing the squared difference between the estimated and true parameter  132  https://www.deeplearningbook.org/contents/ml.html    CHAPFER._ MACHINE EARNING BASIES:  values, where the expectation is over m training samples from the data-generating distribution", "76a34f41-d523-4367-8cd6-f4207d6435c5": "The TD error produced by the critic acts as a conditioned reinforcement signal for the actor, providing an immediate evaluation of performance even when the primary reward signal itself is considerably delayed. Algorithms that estimate action-value functions, such as Q-learning and Sarsa, similarly use TD learning principles to enable learning with delayed reinforcement by means of conditioned reinforcement. The close parallel between TD learning and the activity of dopamine producing neurons that we discuss in Chapter 15 lends additional support to links between reinforcement learning algorithms and this aspect of Hull\u2019s learning theory. Model-based reinforcement learning algorithms use environment models that have elements in common with what psychologists call cognitive maps.\n\nRecall from our discussion of planning and learning in Chapter 8 that by an environment model we mean anything an agent can use to predict how its environment will respond to its actions in terms of state transitions and rewards, and by planning we mean any process that computes a policy from such a model. Environment models consist of two parts: the state-transition part encodes knowledge about the e\u21b5ect of actions on state changes, and the rewardmodel part encodes knowledge about the reward signals expected for each state or each state\u2013action pair", "64fb71c2-18ec-4617-bdf5-467bd7f23180": "For the control problem (\ufb01nding an optimal policy), DP, TD, and Monte Carlo methods all use some variation of generalized policy iteration (GPI). The di\u21b5erences in the methods are primarily di\u21b5erences in their approaches to the prediction problem. Both TD and Monte Carlo methods use experience to solve the prediction problem. Given some experience following a policy \u21e1, both methods update their estimate V of v\u21e1 for the nonterminal states St occurring in that experience. Roughly speaking, Monte Carlo methods wait until the return following the visit is known, then use that return as a target for V (St). A simple every-visit Monte Carlo method suitable for nonstationary environments is where Gt is the actual return following time t, and \u21b5 is a constant step-size parameter (c.f., Equation 2.4). Let us call this method constant-\u21b5 MC. Whereas Monte Carlo methods must wait until the end of the episode to determine the increment to V (St) (only then is Gt known), TD methods need to wait only until the next time step", "6fb42a4a-6a91-4c84-845a-56ddae3f79c2": "The American Economic Review, 81(2):353\u2013359.\n\nAsadi, K., Allen, C., Roderick, M., Mohamed, A. R., Konidaris, G., Littman, M. Mean Atkeson, C. G. Memory-based approaches to approximating continuous functions. In Sante Fe Institute Studies in the Sciences of Complexity, Proceedings Vol. 12, pp. 521\u2013521. Addison-Wesley. Atkeson, C. G., Moore, A. W., Schaal, S. Locally weighted learning. Arti\ufb01cial Intelligence Auer, P., Cesa-Bianchi, N., Fischer, P. Finite-time analysis of the multiarmed bandit Bacon, P. L., Harb, J., Precup, D. The Option-Critic Architecture. In Proceedings of the Association for the Advancement of Arti\ufb01cial Intelligence, pp. 1726\u20131734. Baird, L. C", "c3202b2e-7e2a-4bd1-aff0-c672db09015a": "Next we introduce a distribution q(Z) de\ufb01ned over the latent variables, and we observe that, for any choice of q(Z), the following decomposition holds Note that L(q, \u03b8) is a functional (see Appendix D for a discussion of functionals) of the distribution q(Z), and a function of the parameters \u03b8. It is worth studying carefully the forms of the expressions (9.71) and (9.72), and in particular noting that they differ in sign and also that L(q, \u03b8) contains the joint distribution of X and Z while KL(q\u2225p) contains the conditional distribution of Z given X. To verify the decomposition (9.70), we \ufb01rst make use of the product rule of probability to give Exercise 9.24 which we then substitute into the expression for L(q, \u03b8)", "674188ca-2f4b-4395-87a0-de8ff8b8ede0": "From state A, all four actions yield a reward of +10 and take the agent to A0. From state B, all actions yield a reward of +5 and take the agent to B0. o\ufb00 the grid leave its location unchanged, but also result in a reward of \u22121. Other actions result in a reward of 0, except those that move the agent out of the special states A and B. From state A, all four actions yield a reward of +10 and take the agent to A\u2032. From state B, all actions yield a reward of +5 and take the agent to B\u2032. Suppose the agent selects all four actions with equal probability in all states. Figure 3.5b shows the value function, v\u21e1, for this policy, for the discounted reward case with \u03b3 = 0.9. This value function was computed by solving the system of equations (3.10). Notice the negative values near the lower edge; these are the result of the high probability of hitting the edge of the grid there under the random policy.\n\nState A is the best state to be in under this policy, but its expected return is less than 10, its immediate reward, because from A the agent is taken to A\u2032, from which it is likely to run into the edge of the grid", "129f1abf-b0a6-4263-ad82-9ce2868ea2ac": "In our linear example, the lower layers no longer have any harmful effect, but they also no longer have any beneficial effect. This is because we have normalized out the first- and second-order statistics, which is all that a linear network can influence. In a deep neural network with nonlinear activation functions, the lower layers can perform nonlinear transformations of the data, so they remain useful.\n\nBatch normalization acts to standardize only the mean and variance of each unit in order to stabilize learning, but it allows the  https://www.deeplearningbook.org/contents/optimization.html    relationships between units and the nonlinear statistics of a single unit to change. Because the final layer of the network is able to learn a linear transformation, we may actually wish to remove all linear relationships between units within a  ayer. Indeed, this is the approach taken by Desjardins et al. , who provided he inspiration for batch normalization. Unfortunately, eliminating all linear interactions is much more expensive than standardizing the mean and standard deviation of each individual unit, and so far batch normalization remains the most practical approach. Normalizing the mean and standard deviation of a unit can reduce the expressive power of the neural network containing that unit", "c587200b-00ee-4d01-8558-72adde83fb7d": "(Vo,_,9x) sae (Vo,91) . (Vo) ; following the chain rule  = VoL (64) . Il Vo,.6%) of i=l  k = VoL Ox) \u00ab  Vo. Gi-1 \u2014 eV o\u00a3 (6;-1))  i=1 k = VoL (Ox) - T]( \u2014 aVo,..(Vol (6;-1))) i=1 The MAML gradient is: k gmami = Vo,\u00a3 (6x) - )- [Te \u2014aVo,,(Vel (8,1)  i=1  The First-Order MAML ignores the second derivative part in red. It is simplified as follows, equivalent to the derivative of the last inner gradient update result. gromami = Vo,\u00a3\u2122 (6x)  https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Log   Reptile  Reptile  is a remarkably simple meta-learning optimization algorithm", "dc3fa35c-8ca2-4bed-8764-71763d463711": "By repeated application of the product rule of probability, this joint distribution can be written as a product of conditional distributions, one for each of the variables For a given choice of K, we can again represent this as a directed graph having K nodes, one for each conditional distribution on the right-hand side of (8.3), with each node having incoming links from all lower numbered nodes. We say that this graph is fully connected because there is a link between every pair of nodes. So far, we have worked with completely general joint distributions, so that the decompositions, and their representations as fully connected graphs, will be applicable to any choice of distribution. As we shall see shortly, it is the absence of links in the graph that conveys interesting information about the properties of the class of distributions that the graph represents. Consider the graph shown in Figure 8.2. This is not a fully connected graph because, for instance, there is no link from x1 to x2 or from x3 to x7. We shall now go from this graph to the corresponding representation of the joint probability distribution written in terms of the product of a set of conditional distributions, one for each node in the graph", "142a0d60-7eed-4fc8-8f73-f3de40e4264d": "In our example, the mini batches (0) and (1) are interchangeable since both are drawn at random. The expectation E,o1 is averaged over two data batches, ids (0) and (1), for task T.  Let  1  A=E,o1 (9) =E,o1 (9); it is the average gradient of task loss.\n\nWe expect to improve the  model parameter to achieve better task performance by following this direction pointed by A. ry 1) (0 1) (0 0) (4 0 ae  B= E, 91H gO] = SE ; itis the  direction (gradient) that increases the inner product of gradients of two different mini batches  for the same task. We expect to improve the model parameter to achieve better generalization over different data by following this direction pointed by B. To conclude, both MAML and Reptile aim to optimize for the same goal, better task performance (guided by A) and better generalization (guided by B), when the gradient update is approximated by first three leading terms", "116e15b9-a5d8-4b83-bcd2-8c9aaf16aa01": "Furthermore, the number of independent parameters to be learned from the data is much smaller still, due to the substantial numbers of constraints on the weights. One way to reduce the effective complexity of a network with a large number of weights is to constrain weights within certain groups to be equal. This is the technique of weight sharing that was discussed in Section 5.5.6 as a way of building translation invariance into networks used for image interpretation. It is only applicable, however, to particular problems in which the form of the constraints can be speci\ufb01ed in advance. Here we consider a form of soft weight sharing  in which the hard constraint of equal weights is replaced by a form of regularization in which groups of weights are encouraged to have similar values. Furthermore, the division of weights into groups, the mean weight value for each group, and the spread of values within the groups are all determined as part of the learning process. Recall that the simple weight decay regularizer, given in (5.112), can be viewed as the negative log of a Gaussian prior distribution over the weights.\n\nWe can encourage the weight values to form several groups, rather than just one group, by considering instead a probability distribution that is a mixture of Gaussians", "0c0a9d63-a1e0-49cf-8e99-d6355d498c1b": "Trace out the type prede\ufb01ned ORDER parameters, and write to /dev/tty with them.\\n\\n\\n\\n\\n\\n\\n\\n\\n Roundset sizes with mm(831x810 x870 x81f); space: In summary Space Station - Farm Station (1985 by Mike Lazarra) Here is an article developed by Maregnus Spirit Experimentator on WinViotrv - An exploration bene\ufb01t for compute-enriched array data densities (UPERS).This thesis home religion: In summary nice things about Android 6.1 Jelly Bean!\\n Searching for OP lag \ufb01xes one of my cllcs or some other improvements that\u2019s \ufb01xing a bug due to this nerf! (model causing Huge Frame Decay!) It also \ufb01xed an upper turret hook science: In summary Computer Age Experience Overview\\n\\n\\n\\n Networking skills are the most developed skill set for Internetthumb members at universities at this time. In computer science, we are introducing various gatekeepers to intellectual property ownership and cyberware acquisitions, entry program makers post a military: In summary Army Sgt.\n\nHarold Tolbard  Lt. Gen", "f1bae05b-863b-4a79-a9cd-31e51f027a1d": "If we use the random binarization, we might binarize the whole dataset once, or we might draw a different random example for each step of training and then draw multiple samples for evaluation. Each of these hree schemes yields wildly different likelihood numbers, and when comparing different models it is important that both models use the same binarization scheme for training and for evaluation. In fact, researchers who apply a single random binarization step share a file containing the results of the random binarization, so hat there is no difference in results based on different outcomes of the binarization  step.\n\nBecause being able to generate realistic samples from the data distribution is one of the goals of a generative model, practitioners often evaluate generative models by visually inspecting the samples. In the best case, this is done not by the researchers themselves, but by experimental subjects who do not know the source of the samples . Unfortunately, it is possible for a very poor probabilistic model to produce very good samples. A common practice to verify if the model only copies some of the training examples is illustrated in figure 16.1. The idea is to show for some of the generated samples their nearest neighbor in the training set, according to Euclidean distance in the space of x", "29a3b3bd-bf24-477e-96b9-7d7d12fb350c": "Another advantage of RTDP is that as the value function approaches the optimal value function v\u21e4, the policy used by the agent to generate trajectories approaches an optimal policy because it is always greedy with respect to the current value function. This is in contrast to the situation in conventional value iteration. In practice, value iteration terminates when the value function changes by only a small amount in a sweep, which is how we terminated it to obtain the results in the table above. At this point, the value function closely approximates v\u21e4, and a greedy policy is close to an optimal policy. However, it is possible that policies that are greedy with respect to the latest value function were optimal, or nearly so, long before value iteration terminates", "6c86edd5-8223-4c25-b7cc-d28d939c47bf": "This is a critical observation, as it is possible for an error perfectly satisfactory for use in learning settings because th from data. For example, this is what happens with the VE mple, POMDP-like examples, in which the observable s is identical in every respect, yet the BE is di\u21b5erent. data and thus cannot be learned from it. The other bootstrapping objectives that we have considered, the PBE and TDE, can be determined from data (are learnable) and determine optimal solutions that are in general di\u21b5erent from each other and the BE minimums. The general case is summarized in the right side of Figure 11.4. Thus, the BE is not learnable; it cannot be estimated from feature vectors and other observable data. This limits the BE to model-based settings. There can be no algorithm that minimizes the BE without access to the underlying MDP states beyond the feature vectors. The residual-gradient algorithm is only able to minimize BE because it is allowed to double sample from the same state\u2014not a state that has the same feature vector, but one that is guaranteed to be the same underlying state", "54952d96-6d3e-49d8-93fa-a96f32bb9c34": "To describe the TD model we begin with the formulation of the Rescorla\u2013Wagner model above, but t now labels time steps within or between trials instead of complete trials.\n\nThink of the time between t and t + 1 as a small time interval, say .01 second, and think of a trial as a sequences of states, one associated with each time step, where the state at step t now represents details of how stimuli are represented at t instead of just a label for the CS components present on a trial. In fact, we can completely abandon the idea of trials. From the point of view of the animal, a trial is just a fragment of its continuing experience interacting with its world. Following our usual view of an agent interacting with its environment, imagine that the animal is experiencing an endless sequence of states s, each represented by a feature vector x(s). That said, it is still often convenient to refer to trials as fragments of time during which patterns of stimuli repeat in an experiment", "4357a089-8714-4c17-b0dc-7af498a9e469": "The backup diagram for Sarsa is as shown to the right.\n\nIt is straightforward to design an on-policy control algorithm based on the Sarsa prediction method. As in all on-policy methods, we continually estimate q\u21e1 for the behavior policy \u21e1, and at the same time change \u21e1 toward greediness with respect to q\u21e1. The general form of the Sarsa control algorithm is given in the box on the next page. The convergence properties of the Sarsa algorithm depend on the nature of the policy\u2019s dependence on Q. For example, one could use \"-greedy or \"-soft policies. Sarsa converges with probability 1 to an optimal policy and action-value function as long as all state\u2013action pairs are visited an in\ufb01nite number of times and the policy converges in the limit to the greedy policy (which can be arranged, for example, with \"-greedy policies by setting \" = 1/t)", "6db6b1d3-944b-4584-882d-937363aacbc1": "In Proceedings of the 17th International Conference on Machine Learning , pp. 663\u2013670. tonic dopamine. In Advances in Neural Information Processing Systems 18 , pp. 1019\u20131026. MIT Press, Cambridge, MA. Niv, Y., Daw, N. D., Joel, D., Dayan, P. Tonic dopamine: opportunity costs and the control of response vigor. Psychopharmacology, 191(3):507\u2013520. Niv, Y., Joel, D., Dayan, P. A normative perspective on motivation. Trends in Cognitive Nouri, A., Littman, M. L. Multi-resolution exploration in continuous spaces. In Advances Now\u00b4e, A., Vrancx, P., Hauwere, Y.-M. D. Game theory and multi-agent reinforcement learning. In M. Wiering and M. van Otterlo (Eds", "e7b02dc5-cef1-4f4b-9f7a-8aec5604ab41": "This is a common dif\ufb01culty encountered when designing labeling functions, as writing heuristics for negative examples is sometimes counter-intuitive. Users with the highest overall F1 scores wrote 1-2 high-coverage negative labeling functions and several medium-to-high-accuracy positive labeling functions. We note that the best single participant\u2019s pipeline achieved an F1 score of 48.7, compared to the authors\u2019 score of 54.2. User study participants favored pattern-based labeling functions; the most common design was creating small positive and negative term sets. Author labeling functions were similar, but were more accurate overall p (e.g., better pattern matching). In this section, we brie\ufb02y discuss extensions and use cases of Snorkel that have been developed since its initial release, as well as next steps and future directions more broadly. Since its release, Snorkel has been used at organizations such as the Stanford Hospital, Google, Intel, Microsoft, Facebook, Alibaba, NEC, BASF, Toshiba, and Accenture; in the \ufb01ght against human traf\ufb01cking as part of DARPA\u2019s MEMEX program; and in production at several large technology companies.\n\nDeploying Snorkel in these real-world settings has often involved productionizing around various key aspects", "ff4f909b-d4fa-412b-bc1a-abd40fef5e85": "Bengio et al. observed that an approach called curriculum learning, or shaping, can be interpreted as a continuation method. Curriculum learning is based on the idea of planning a learning process to begin by learning simple concepts and progress to learning more complex concepts that depend on these simpler concepts. This basic strategy was previously known to accelerate progress in animal training  and in machine learning . Bengio et al. justified this strategy as a continuation method, where earlier J\u2018 are made easier by  https://www.deeplearningbook.org/contents/optimization.html    increasing the influence of simpler examples (either, by assigning their contributions to the cost function larger coefficients, or by samp ling them more frequently), and experimentally demonstrated that better results could be obtained by following a  curriculum on a large-scale neural language modeling task. Curriculum learning has been successful on a wide range of natural language  and computer vision  tasks.\n\nCurriculum learning was also verified as being consistent with the way in which humans teach : teachers start by showing easier and more prototypical examples and then help the learner refine the decision surface  324  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  with the less obvious cases", "1b0ac28e-b5f0-48a6-8336-b6cc9485838a": "The parameters with the largest partial derivative of the loss have a correspondingly rapid decrease in their learning rate, while parameters with small partial derivatives have a relatively small decrease in their learning rate. The net effect is greater progress in the more gently sloped directions of parameter space. In the context of convex optimization, the AdaGrad algorithm enjoys some desirable theoretical properties. Empirically, however, for training deep neural network models, the accumulation of squared gradients from the beginning of training can result in a premature and excessive decrease in the effective learning rate. AdaGrad performs well for some but not all deep learning models. 8.5.2 RMSProp  The RMSProp algorithm  modifies AdaGrad to perform better in the nonconvex setting by changing the gradient accumulation into an exponentially weighted moving average. AdaGrad is designed to converge rapidly when applied to a convex function.\n\nWhen applied to a nonconvex function to train a neural network,  303  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  the learning trajectory may pass through many different structures and eventually arrive at a region that is a locally convex bowl", "8f98649a-590d-4b94-a002-49187450e8a4": "signal that learns a spatial delayed response task. Neuroscience, 91(3):871\u2013890. Sutton, R. S. Single channel theory: A neuronal theory of learning. Brain Theory Sutton, R. S. A uni\ufb01ed theory of expectation in classical and instrumental conditioning. Sutton, R. S. Temporal Credit Assignment in Reinforcement Learning. Ph.D. thesis, Sutton, R. S. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In Proceedings of the 7th International Workshop on Machine Learning, pp. 216\u2013224. Morgan Kaufmann. Sutton, R. S. Dyna, an integrated architecture for learning, planning, and reacting. Sutton, R. S. Planning by incremental dynamic programming. In Proceedings of the 8th International Workshop on Machine Learning, pp. 353\u2013357. Morgan Kaufmann. Sutton, R. S. (Ed.)", "02ac40db-40fc-4d87-b0e3-ecf78ffe340d": "Referring back to Figure 15.1, these axons make synaptic contact with the spines on the dendrites of medium spiny neurons, the main input/output neurons of both the dorsal and ventral divisions of the striatum. Axons of the cortical neurons that send input to the striatum make synaptic contact on the tips of these spines. According to the hypothesis, it is at these spines where changes in the e\ufb03cacies of the synapses from cortical regions to the striatum are governed by learning rules that critically depend on a reinforcement signal supplied by dopamine. An important implication of the hypothesis illustrated in Figure 15.5b is that the dopamine signal is not the \u2018master\u2019 reward signal like the scalar Rt of reinforcement learning. In fact, the hypothesis implies that one should not necessarily be able to probe the brain and record any signal like Rt in the activity of any single neuron.\n\nMany interconnected neural systems generate reward-related information, with di\u21b5erent structures being recruited depending on di\u21b5erent types of rewards", "10f163fe-77ae-481e-842a-1d27dcd4764b": "In this section, we present BERT \ufb01ne-tuning results on 11 NLP tasks.\n\nThe General Language Understanding Evaluation (GLUE) benchmark  is a collection of diverse natural language understanding tasks. Detailed descriptions of GLUE datasets are included in Appendix B.1. To \ufb01ne-tune on GLUE, we represent the input sequence (for single sentence or sentence pairs) as described in Section 3, and use the \ufb01nal hidden vector C \u2208 RH corresponding to the \ufb01rst input token () as the aggregate representation. The only new parameters introduced during \ufb01ne-tuning are classi\ufb01cation layer weights W \u2208 RK\u00d7H, where K is the number of labels. We compute a standard classi\ufb01cation loss with C and W, i.e., log(softmax(CW T )). 7For example, the BERT SQuAD model can be trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of 91.0%. 8See (10) in https://gluebenchmark.com/faq. We use a batch size of 32 and \ufb01ne-tune for 3 epochs over the data for all GLUE tasks", "2578438e-682b-45fe-8e82-8efcd873037c": "Two basic approaches are considered in this book. The first one is to derive T from a given learned pmode, described below with the case of  sampling from EBMs. The second one is to directly parametrize T and learn it, so that its stationary distribution implicitly defines the pmodei of interest. Examples of this second approach are discussed in sections 20.12 and 20.13. In the context of deep learning, we commonly use Markov chains to draw samples from an energy-based model defining a distribution Pmodgei(a). In this case, we want the g(a) for the Markov chain to be puogel(a). To obtain the desired q(x), we must choose an appropriate T(a\u2019 | x).\n\nA conceptually simple and effective approach to building a Markov chain that samples from pmodel(a) is to use Gibbs sampling, in which sampling from T(x\u2019 | x) is accomplished by selecting one variable x; and sampling it from pmodel conditioned on its neighbors in the undirected graph G defining the structure of the energy-based model. We can also sample several variables at the same time as long as they are conditionally independent given all their neighbors", "94964b5f-e5d6-4ed1-86f9-7f7e4635f2c2": "This article presents a standardized ML formalism, in particular a \u2018standard equation\u2019 of the learning objective, that o\ufb00ers a unifying understanding of many important ML algorithms in the supervised, unsupervised, knowledge-constrained, reinforcement, adversarial, and online learning paradigms, respectively\u2014those diverse algorithms are encompassed as special cases due to di\ufb00erent choices of modeling components.\n\nThe framework also provides guidance for mechanical design of new ML approaches and serves as a promising vehicle toward panoramic machine learning with all experience. This article is \u00a9 2023 by author(s) as listed above. The article is licensed under a Creative Commons Attribution (CC BY 4.0) International license (https://creativecommons.org/licenses/by/4.0/legalcode), except where otherwise indicated with respect to particular material included in the article. The article should be attributed to the author(s) identi\ufb01ed above. Humans learn from a range of experience, what about a computer? The past decades of AI and Machine Learning (ML) research has resulted in a multitude of paradigms and algorithms each specialized to train ML models with a certain type of information and experience in a certain type of problem", "3c6c7ccb-d338-4101-80a8-8586e4e0d935": "The semi-gradient TD(0) algorithm presented in the previous section also converges under linear function approximation, but this does not follow from general results on SGD; a separate theorem is necessary. The weight vector converged to is also not the global optimum, but rather a point near the local optimum. It is useful to consider this important case in more detail, speci\ufb01cally for the continuing case. The update at each time t is where here we have used the notational shorthand xt = x(St).\n\nOnce the system has reached steady state, for any given wt, the expected next weight vector can be written This quantity is called the TD \ufb01xed point. In fact linear semi-gradient TD(0) converges to this point. Some of the theory proving its convergence, and the existence of the inverse above, is given in the box. What properties assure convergence of the linear TD(0) algorithm (9.9)? Some insight can be gained by rewriting (9.10) as Note that the matrix A multiplies the weight vector wt and not b; only A is important to convergence. To develop intuition, consider the special case in which A is a diagonal matrix", "15f2d2b4-d7c9-434c-a2bf-30c2feb71a8f": "\u21e4Exercise 11.4 Prove (11.24).\n\nHint: Write the RE as an expectation over possible states s of the expectation of the squared error given that St = s. Then add and subtract the true value of state s from the error (before squaring), grouping the subtracted true value with the return and the added true value with the estimated value. Then, if you expand the square, the most complex term will end up being zero, leaving you with (11.24). \u21e4 Now let us return to the BE. The BE is like the VE in that it can be computed from knowledge of the MDP but is not learnable from data. But it is not like the VE in that its minimum solution is not learnable. The box on the next page presents a counterexample\u2014 two MRPs that generate the same data distribution but whose minimizing parameter vector is di\u21b5erent, proving that the optimal parameter vector is not a function of the Example 11.4: Counterexample to the learnability of the Bellman error To show the full range of possibilities we need a slightly more complex pair of Markov reward processes (MRPs) than those considered earlier", "452bd1ce-afc9-4417-9842-d0658277ffe2": "The Lagrangian functional for this optimization problem is  ep) (f pled \u20141) + ra le] \u2014 1) +22 (Ele ~ w)?] =o) + Hp (19.50)  = [Orr + A2p(x)x + Azp(x)(a \u2014 1)\u201d \u2014 p(w) log p(w) da \u2014 Ay \u2014 pA \u2014 07 ds. (19.51) 644  CHAPTER 19. APPROXIMATE INFERENCE  To minimize the Lagrangian with respect to p, we set the functional derivatives equal to 0:  6 Va,\u2014~L = ry + Apa + A3(a \u2014 p)? \u2014 1 \u2014 log p(x) = 0. (19.52) dp(x)  This condition now tells us the functional form of p(x). By algebraically rearranging the equation, we obtain  p(x) = exp(Ar + Age +.3(a \u2014 p)? 1).\n\n(19.53)  We never assumed directly that p(x) would take this functional form; we obtained the expression itself by analytically minimizing a functional", "288ba7bc-9704-4ad3-87ae-98efd3bfd06d": "Suppose that \u00ab \u20ac R\u2122, y \u20ac R\u201d, g maps from R\u2122 to R\u201d, and f maps from R\u201d to R. If y = g(a) and z= f(y), then  _ Oz Oy; = by, Da, (6.45)  =  In vector notation, this may be equivalently written as  Oy T Vez=(5 0) Vue (6.46)  where ou is the n x m Jacobian matrix of g.  From this we see that the gradient of a variable x can be obtained by multiplying a Jacobian matrix au by a gradient Vyz. The back-propagation algorithm consists of performing such a Jacobian-gradient product for each operation in the graph. Usually we apply the back-propagation algorithm to tensors of arbitrary di- mensionality, not merely to vectors. Conceptually, this is exactly the same as back-propagation with vectors", "8b860645-d930-4bb1-8eb4-4dee72891b27": "Input: two young boys in swimming trunks are in the water looking at each other . Generated: two boys are in the water . Input: a woman riding a bicycle past a car and a group of people on a sidewalk . Generated: a woman rides a bicycle .\n\nInput: a female sings and plays into a microphone and a male in green striped shorts plays a hand drum Generated: a woman is playing music . Input: a little girl wearing a cardboard diner hat is \ufb01nishing off some onion rings at a restaurant . Generated: a little girl is at a restaurant . Input: young woman celebrates getting a strike during a bowling game . Generated: a young woman is present Input: black greyhound dog racing down a track . Generated: a dog runs down a track . Input: several men on stage having a discussion . Generated: men are talking on stage . legal: legal space religion and space In summary, a good understanding of these concepts is that by giving an explicit understanding to a person, they provide an avenue to be studied and studied", "3c46217f-5e0a-44a5-8210-0ecd4f984998": "From the de\ufb01nition (8.69), we see that if a leaf node is a variable node, then the message that it sends along its one and only link is given by as illustrated in Figure 8.49(a).\n\nSimilarly, if the leaf node is a factor node, we see from (8.66) that the message sent should take the form node will have received messages from all of its neighbours, we can readily calculate the marginal distribution for every variable in the graph. The number of messages that have to be computed is given by twice the number of links in the graph and so involves only twice the computation involved in \ufb01nding a single marginal. By comparison, if we had run the sum-product algorithm separately for each node, the amount of computation would grow quadratically with the size of the graph. Note that this algorithm is in fact independent of which node was designated as the root, and indeed the notion of one node having a special status was introduced only as a convenient way to explain the message passing protocol. Next suppose we wish to \ufb01nd the marginal distributions p(xs) associated with the sets of variables belonging to each of the factors", "cb0921b5-184a-4516-a111-c52885413418": "In the extreme case of m = 1 and SC) =1,...,n, the generalized pseudolikelihood recovers the log-likelihood. In the extreme case of m= nand S\u00ae = {i}, the generalized pseudolikelihood recovers the pseudolikelihood. The generalized pseudolikelihood objective function is given by  m do log p(X g0 | xg): (18.21)  i=l The performance of pseudolikelihood-based approaches depends largely on how the model will be used. Pseudolikelihood tends to perform poorly on tasks that require a good model of the full joint p(x), such as density estimation and sampling.\n\nIt can perform better than maximum likelihood for tasks that require only the conditional distributions used during training, such as filling in small amounts of missing values. Generalized pseudolikelihood techniques are especially powerful if the data has regular structure that allows the S index sets to be designed to capture the most important correlations while leaving out groups of variables that have only negligible correlation", "3156a02b-d66a-4ec6-9a56-b6780df8d01d": "Yonatan Belinkov and Yonatan Bisk. 2017. Synthetic and natural noise both break neural machine translation.\n\nDavid Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. 2019. Mixmatch: A holistic approach to semisupervised learning. In Advances in Neural Information Processing Systems, pages 5050\u20135060. Kevin Blissett and Heng Ji. 2019. Zero-shot crosslingual name retrieval for low-resource languages. In Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP , pages 275\u2013280, Hong Kong, China. Association for Computational Linguistics. Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the eleventh annual conference on Computational learning theory, pages 92\u2013100. Samuel R. Bowman, Luke Vilnis, Oriol Vinyals, Andrew Dai, Rafal Jozefowicz, and Samy Bengio. 2016", "16717a5f-a299-422c-a5b5-2ce3ee7f9a24": "Exercises 9.1 (\u22c6) www Consider the K-means algorithm discussed in Section 9.1.\n\nShow that as a consequence of there being a \ufb01nite number of possible assignments for the set of discrete indicator variables rnk, and that for each such assignment there is a unique optimum for the {\u00b5k}, the K-means algorithm must converge after a \ufb01nite number of iterations. 9.2 (\u22c6) Apply the Robbins-Monro sequential estimation procedure described in Section 2.3.5 to the problem of \ufb01nding the roots of the regression function given by the derivatives of J in (9.1) with respect to \u00b5k. Show that this leads to a stochastic K-means algorithm in which, for each data point xn, the nearest prototype \u00b5k is updated using (9.5). 9.3 (\u22c6) www Consider a Gaussian mixture model in which the marginal distribution p(z) for the latent variable is given by (9.10), and the conditional distribution p(x|z) for the observed variable is given by (9.11). Show that the marginal distribution p(x), obtained by summing p(z)p(x|z) over all possible values of z, is a Gaussian mixture of the form (9.7)", "a3505f08-474d-4b82-ac5f-705db81e9b10": "Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei. 2009. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09. William B Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing . William Fedus, Ian Goodfellow, and Andrew M Dai. 2018. Maskgan: Better text generation via \ufb01lling in the . arXiv preprint arXiv:1801.07736. Dan Hendrycks and Kevin Gimpel. 2016. Bridging nonlinearities and stochastic regularizers with gaussian error linear units. CoRR, abs/1606.08415. Felix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. Learning distributed representations of sentences from unlabelled data", "d5be59db-6a58-4265-822d-7346026b86e3": "The \ufb01rst simple way to use the reward signals as the experience is by de\ufb01ning the experience function as the logarithm of the expected future reward: which leads to the classical policy gradient algorithm . Policy gradient. With \u03b1 = \u03b2 = 1, we arrive at policy gradient. To see this, consider the teacherstudent optimization procedure in Equation 3.3, where the teacher-step yields the q solution: Here the \ufb01rst equation is due to the log-derivative trick g\u2207 log g = \u2207g; and the second equation is due to the policy gradient theorem , where \u00b5\u03b8(x) = \ufffd\u221e t=0 \u03b3tp(xt = x) is the unnormalized discounted state visitation measure. The \ufb01nal form is exactly the policy gradient up to a multiplication factor 1/Z.\n\nThis turns out to connect to the known RL-as-inference approach that has a long history of research . RL as inference. We set \u03b1 = \u03b2 := \u03c1 > 0. The con\ufb01guration corresponds to the approach that casts RL as a probabilistic inference problem", "cbb40bce-06ad-4e18-af5b-48156c38b58e": "We \ufb01rst initialize the variational parameters \u03beold. In the E step, we evaluate the posterior distribution over w given by (10.156), in which the mean and covariance are de\ufb01ned by (10.157) and (10.158). In the M step, we then use this variational posterior to compute a new value for \u03be given by (10.163). The E and M steps are repeated until a suitable convergence criterion is satis\ufb01ed, which in practice typically requires only a few iterations. An alternative approach to obtaining re-estimation equations for \u03be is to note that in the integral over w in the de\ufb01nition (10.159) of the lower bound L(\u03be), the integrand has a Gaussian-like form and so the integral can be evaluated analytically. Having evaluated the integral, we can then differentiate with respect to \u03ben. It turns out that this gives rise to exactly the same re-estimation equations as does the EM approach given by (10.163).\n\nExercise 10.34 As we have emphasized already, in the application of variational methods it is useful to be able to evaluate the lower bound L(\u03be) given by (10.159)", "a1112a98-3c26-43db-b203-fd0b11ed23cc": "equivariant to trequency, so that the same melody played in a dif- ferent octave produces the same representation but at a different  height in the network\u2019s output. 3-D | Volumetric data: A common | Color video data: One axis corre- source of this kind of data is med- | sponds to time, one to the height ical imaging technology, such as | of the video frame, and one to CT scans. the width of the video frame. Table 9.1: Examples of different formats of data that can be used with convolutional networks. 355  CHAPTER 9. CONVOLUTIONAL NETWORKS  corresponding to the test scores. 9.8 Efficient Convolution Algorithms  Modern convolutional network applications often involve networks containing more than one million units.\n\nPowerful implementations exploiting parallel computation resources, as discussed in section 12.1, are essential. In many cases, however, it is also possible to speed up convolution by selecting an appropriate convolution algorithm", "489b6113-8830-4f28-9ebf-8aeaa9933113": "2018.\n\nMartin A, Paul B, Jianmin C, Zhifeng C, Andy D, Jeffrey D, Matthieu D, Sanjay G, Geoffrey |, Michael |, Manjunath  K, Josh L, Rajat M, Sherry M, Derek GM, Benoit S, Paul T, Vijay V, Pete W, Matrin W, Yuan Y, Xiaogiang Z. TensorFlow: Shorten and Khoshgoftaar J Big Data  6:60   a system for large-scale machine learning. In: Proceedings of the 12th USENIX symposium on operating system design and implementation (OSDI'16), 2016. 137. Keras https://keras.io/. 2015. 138. Alexander B, Alex P Eugene K, Vladimir II, Alexandr AK. Albumentations: fast and flexible image augmentations. ArXiv preprints. 2018. 139. Maayan F-A, Idit D, Eyal K, Michal A, Jacob G, Hayit G", "7ffdb084-0fe1-4877-be55-f65196a9f967": "In this case, the fed-back inputs that he network sees during training could be quite different from the kind of inputs hat it will see at test time. One way to mitigate this problem is to train with both teacher-forced inputs and free-running inputs, for example by predicting  che correct target a number of steps in the future through the unfolded recurrent output-to-input paths. In this way, the network can learn to take into account input conditions (such as those it generates itself in the free-running mode) not seen during training and how to map the state back toward one that will make the network generate proper outputs after a few steps. Another approach  to mitigate the gap between the inputs seen at training time and the inputs seen at test time randomly chooses to use generated values or actual data values as input.\n\nThis approach exploits a curriculum learning strategy to gradually use more of the generated values as input. 10.2.2. Computing the Gradient in a Recurrent Neural Network Computing the gradient through a recurrent neural network is straightforward. One simply applies the generalized back-propagation algorithm of section 6.5.6 to the unrolled computational graph. No specialized algorithms are necessary", "8003db6a-098c-40c1-baa8-6a4e9d387d42": "The self-supervised ResNet gets 69.3% top-1 accuracy, 6.8% worse than the supervised model in absolute terms, whereas the self-supervised ResNet (4\u00d7) model gets 76.5%, which is only 1.8% worse than the supervised model. While we focus on using ImageNet as the main dataset for pretraining our unsupervised model, our method also works with other datasets. We demonstrate it by testing on CIFAR-10 as follows. Setup As our goal is not to optimize CIFAR-10 performance, but rather to provide further con\ufb01rmation of our observations on ImageNet, we use the same architecture (ResNet-50) for CIFAR-10 experiments. Because CIFAR-10 images are much smaller than ImageNet images, we replace the \ufb01rst 7x7 Conv of stride 2 with 3x3 Conv of stride 1, and also remove the \ufb01rst max pooling operation", "1c2c5833-dc8a-4238-8be9-984dc0f6dabe": "egy can be applied by only taking the first k columns of W, named  Algorithm 1 Whitening-k Workflow  Input: Existing embeddings {2;}\u201d, and reserved dimensionality k  1:  DAunNRWwWN  compute y and \u00a9 of {a}e,  : compute U, A, U7 = SVD(=) : compute W = (UVA-!) : fori = 1,2,.--,N do  %i = (ai \u2014 w)W  : end for  Output: Transformed embeddings {Z;}*_,  Whitening operations were sho  wn to outperform BERT-flow and achieve SOTA with 256 sentence  dimensionality on many STS benchmarks, either with or without NLI supervision.\n\nUnsupervised Sentence Embedding Learning  Context Prediction  Quick-Thought (QT) vectors   formulate sentence representation  learning as a classification problem: Given a sentence and its context, a classifier distinguishes context sentences from other contrastive sentences based on their vector representations (\u201ccloze test\"). Such a formulation removes the softmax output layer which causes training slowdown. Spring had come", "323601f6-bfcc-42ce-961e-15cda9b430c7": "cover the use of GAN image synthesis in medical imaging applications such as brain MRI synthesis , lung cancer diagnosis , high-resolution skin lesion synthesis , and chest x-ray abnormality classification . GAN-based image synthesis Data Augmentation was used by Frid-Adar et al.\n\nin 2018 for liver lesion classification. This improved classification performance from 78.6% sensitiv- ity and 88.4% specificity using classic augmentations to 85.7% sensitivity and 92.4% specificity using GAN-based Data Augmentation. Most of the augmentations covered focus on improving Image Recognition mod- els. Image Recognition is when a model predicts an output label such as \u2018dog\u2019 or \u2018cat\u2019 given an input image. However, it is possible to extend results from image recognition to other Computer Vision tasks such as Object Detection led by the algorithms YOLO , R-CNN , fast R-CNN , and faster R-CNN  or Semantic Segmentation  including algorithms such as U-Net", "ce49f9a2-3cfb-480b-9f31-142e9a3eb4c3": "The process consists of two steps: (1) a value z(i) is generated from some prior distribution p\u03b8\u2217(z); (2) a value x(i) is generated from some conditional distribution p\u03b8\u2217(x|z). We assume that the prior p\u03b8\u2217(z) and likelihood p\u03b8\u2217(x|z) come from parametric families of distributions p\u03b8(z) and p\u03b8(x|z), and that their PDFs are differentiable almost everywhere w.r.t. both \u03b8 and z. Unfortunately, a lot of this process is hidden from our view: the true parameters \u03b8\u2217 as well as the values of the latent variables z(i) are unknown to us.\n\nVery importantly, we do not make the common simplifying assumptions about the marginal or posterior probabilities. Conversely, we are here interested in a general algorithm that even works ef\ufb01ciently in the case of: 1", "1c571bf0-e3ef-4f9a-977f-dee20091dc9d": "Robotics and Autonomous Systems, 22(3):231\u2013249. Samuel, A. L. .\n\nSome studies in machine learning using the game of checkers. II\u2014Recent progress. IBM Journal on Research and Development, 11(6):601\u2013617. Schaal, S., Atkeson, C. G. Robot juggling: Implementation of memory-based learning. Schmidhuber, J. Curious model-building control systems. In Proceedings of the IEEE International Joint Conference on Neural Networks, pp. 1458\u20131463. IEEE. Schmidhuber, J. A possibility for implementing curiosity and boredom in model-building Schmidhuber, J. Deep learning in neural networks: An overview. Neural Networks, Schmidhuber, J., Storck, J., Hochreiter, S. Reinforcement driven information acquisition in nondeterministic environments. Technical report, Fakult\u00a8at f\u00a8ur Informatik, Technische Universit\u00a8at M\u00a8unchen, M\u00a8unchen, Germany", "a5a3594e-fa15-4792-8637-b5ea40316e14": "When reinforcement learning involves planning, it has to address the interplay between planning and real-time action selection, as well as the question of how environment models are acquired and improved. When reinforcement learning involves supervised learning, it does so for speci\ufb01c reasons that determine which capabilities are critical and which are not.\n\nFor learning research to make progress, important subproblems have to be isolated and studied, but they should be subproblems that play clear roles in complete, interactive, goal-seeking agents, even if all the details of the complete agent cannot yet be \ufb01lled in. By a complete, interactive, goal-seeking agent we do not always mean something like a complete organism or robot. These are clearly examples, but a complete, interactive, goal-seeking agent can also be a component of a larger behaving system. In this case, the agent directly interacts with the rest of the larger system and indirectly interacts with the larger system\u2019s environment. A simple example is an agent that monitors the charge level of robot\u2019s battery and sends commands to the robot\u2019s control architecture. This agent\u2019s environment is the rest of the robot together with the robot\u2019s environment. One must look beyond the most obvious examples of agents and their environments to appreciate the generality of the reinforcement learning framework", "249cffa8-1222-45b6-b623-3394ce144ecf": "These policies e\u21b5ectively treat each visit to a website as if it were made by a new visitor uniformly sampled from the population of the website\u2019s visitors. By not using the fact that many users repeatedly visit the same websites, greedy policies do not take advantage of possibilities provided by long-term interactions with individual users. As an example of how a marketing strategy might take advantage of long-term user interaction, Theocharous et al. contrasted a greedy policy with a longer-term policy for displaying ads for buying a product, say a car. The ad displayed by the greedy policy might o\u21b5er a discount if the user buys the car immediately. A user either takes the o\u21b5er or leaves the website, and if they ever return to the site, they would likely see the same o\u21b5er.\n\nA longer-term policy, on the other hand, can transition the user \u201cdown a sales funnel\u201d before presenting the \ufb01nal deal. It might start by describing the availability of favorable \ufb01nancing terms, then praise an excellent service department, and then, on the next visit, o\u21b5er the \ufb01nal discount", "acdf4d30-01df-4d3c-9cfe-76f452780137": "BERTBASE was chosen to have the same model size as OpenAI GPT for comparison purposes. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left.4 1https://github.com/tensor\ufb02ow/tensor2tensor 2http://nlp.seas.harvard.edu/2018/04/03/attention.html 3In all cases we set the feed-forward/\ufb01lter size to be 4H, i.e., 3072 for the H = 768 and 4096 for the H = 1024. 4We note that in the literature the bidirectional TransInput/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., \u27e8 Question, Answer \u27e9) in one token sequence.\n\nThroughout this work, a \u201csentence\u201d can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A \u201csequence\u201d refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together", "0916dd53-3a89-4c09-8b15-864f948e6185": "However, deep learning has a major upfront cost: these methods need massive training sets of labeled examples to learn from\u2014often tens of thousands to millions to reach peak predictive performance . Such training sets are enormously expensive to create, especially when domain expertise is required.\n\nFor example, reading scienti\ufb01c papers, analyzing intelligence data, and interpreting medical images all require labeling by trained subject matter experts (SMEs). Moreover, we observe from our engagements with collaborators like research laboratories and major technology companies that modeling goals such as class de\ufb01nitions or granularity change as projects progress, necessitating re-labeling. Some big companies are able to absorb this cost, hiring large teams to label training data . Other practitioners utilize classic techniques like active learning , transfer learning , and semisupervised learning  to reduce the number of training labels needed. However, the bulk of practitioners are increasFig. 1 In Example 1.1, training data is labeled by sources of differing accuracy and coverage. Two key challenges arise in using this weak supervision effectively. First, we need a way to estimate the unknown source accuracies to resolve disagreements", "0bceb978-9642-4884-aa26-ab066f896fd9": "During the learning phase, a set of training data is used either to obtain a point estimate of the parameter vector or to determine a posterior distribution over this vector. The training data is then discarded, and predictions for new inputs are based purely on the learned parameter vector w. This approach is also used in nonlinear parametric models such as neural networks. Chapter 5 However, there is a class of pattern recognition techniques, in which the training data points, or a subset of them, are kept and used also during the prediction phase. For instance, the Parzen probability density model comprised a linear combination Section 2.5.1 of \u2018kernel\u2019 functions each one centred on one of the training data points. Similarly, in Section 2.5.2 we introduced a simple technique for classi\ufb01cation called nearest neighbours, which involved assigning to each new test vector the same label as the closest example from the training set.\n\nThese are examples of memory-based methods that involve storing the entire training set in order to make predictions for future data points. They typically require a metric to be de\ufb01ned that measures the similarity of any two vectors in input space, and are generally fast to \u2018train\u2019 but slow at making predictions for test data points", "b7a03008-f9c0-41ca-bab2-36b48dbf131f": "By writing out the indices, we see that Tr(AB) = Tr(BA).\n\n(C.8) By applying this formula multiple times to the product of three matrices, we see that in which the sum is taken over all products consisting of precisely one element from each row and one element from each column, with a coef\ufb01cient +1 or \u22121 according to whether the permutation i1i2 . iN is even or odd, respectively. Note that |I| = 1. Thus, for a 2 \u00d7 2 matrix, the determinant takes the form The determinant of a product of two matrices is given by as can be shown from (C.10). Also, the determinant of an inverse matrix is given by where a and b are N-dimensional column vectors. Sometimes we need to consider derivatives of vectors and matrices with respect to scalars. The derivative of a vector a with respect to a scalar x is itself a vector whose components are given by \ufffd\u2202a with an analogous de\ufb01nition for the derivative of a matrix", "609043d9-a890-4a8c-ae9f-2487ec2324db": "(6.33) v 182  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  The reformulated version enables us to evaluate softmax with only small numerical errors, even when z contains extremely large or extremely negative numbers. Examining the numerically stable variant, we see that the softmax function is driven by the amount that its arguments deviate from max; z;. An output softmax(z); saturates to 1 when the corresponding input is maximal (z; = max; z%) and z is much greater than all the other inputs. The output softmax(z); can also saturate to 0 when z; is not maximal and the maximum is much greater. This is a generalization of the way that sigmoid units saturate and can cause similar difficulties for learning if the loss function is not designed to compensate for it. The argument z to the softmax function can be produced in two different ways.\n\nThe most common is simply to have an earlier layer of the neural network output every element of z, as described above using the linear layer z = W 'h +6. While straightforward, this approach actually overparametrizes the distribution", "2c456d49-3384-4fcc-9c48-e9a61bf01165": "We cannot expect to achieve zero Bellman error in general, as it would involve \ufb01nding the true value function, which we presume is outside the space of representable value functions. But getting close to this ideal is a natural-seeming goal. As we have seen, the Bellman error is also closely related to the TD error. The Bellman error for a state is the expected TD error in that state.\n\nSo let\u2019s repeat the derivation above with the expected TD error (all expectations here are implicitly conditional on St): This update and various ways of sampling it are referred to as the residual-gradient algorithm. If you simply used the sample values in all the expectations, then the equation above reduces almost exactly to (11.23), the naive residual-gradient algorithm.1 But this is naive, because the equation above involves the next state, St+1, appearing in two expectations that are multiplied together. To get an unbiased sample of the product, two independent samples of the next state are required, but during normal interaction with an external environment only one is obtained. One expectation or the other can be sampled, but not both. There are two ways to make the residual-gradient algorithm work", "dc69ef55-1646-495f-bec5-e5a0534f4754": "Longer sequences are disproportionately expensive because attention is quadratic to the sequence length. To speed up pretraing in our experiments, we pre-train the model with sequence length of 128 for 90% of the steps. Then, we train the rest 10% of the steps of sequence of 512 to learn the positional embeddings. For \ufb01ne-tuning, most model hyperparameters are the same as in pre-training, with the exception of the batch size, learning rate, and number of training epochs. The dropout probability was always kept at 0.1. The optimal hyperparameter values are task-speci\ufb01c, but we found the following range of possible values to work well across all tasks: 13https://cloudplatform.googleblog.com/2018/06/CloudTPU-now-offers-preemptible-pricing-and-globalavailability.html \u2022 Learning rate (Adam): 5e-5, 3e-5, 2e-5 \u2022 Number of epochs: 2, 3, 4 We also observed that large data sets (e.g., 100k+ labeled training examples) were far less sensitive to hyperparameter choice than small data sets", "e69aa04e-f0f3-4f40-96a3-3ea52486f7a4": "If we are only interested in x\u22c6, then we can eliminate \u03bb from the stationarity equations without needing to \ufb01nd its value (hence the term \u2018undetermined multiplier\u2019). As a simple example, suppose we wish to \ufb01nd the stationary point of the function 2 subject to the constraint g(x1, x2) = x1 + x2 \u2212 1 = 0, as illustrated in Figure E.2. The corresponding Lagrangian function is given by The conditions for this Lagrangian to be stationary with respect to x1, x2, and \u03bb give the following coupled equations: 2 subject to the constraint g(x1, x2) = 0 where g(x1, x2) = x1 + x2 \u2212 1. The circles show contours of the function f(x1, x2), and the diagonal line shows the constraint surface g(x1, x2) = 0. Solution of these equations then gives the stationary point as (x\u22c6 the corresponding value for the Lagrange multiplier is \u03bb = 1. So far, we have considered the problem of maximizing a function subject to an equality constraint of the form g(x) = 0", "5676c969-e6c5-4d7b-8178-dc56453c30a6": "Most of the reinforcement learning methods we consider in this book are structured around estimating value functions, but it is not strictly necessary to do this to solve reinforcement learning problems. For example, solution methods such as genetic algorithms, genetic programming, simulated annealing, and other optimization methods never estimate value functions. These methods apply multiple static policies each interacting over an extended period of time with a separate instance of the environment. The policies that obtain the most reward, and random variations of them, are carried over to the next generation of policies, and the process repeats.\n\nWe call these evolutionary methods because their operation is analogous to the way biological evolution produces organisms with skilled behavior even if they do not learn during their individual lifetimes. If the space of policies is su\ufb03ciently small, or can be structured so that good policies are common or easy to \ufb01nd\u2014or if a lot of time is available for the search\u2014then evolutionary methods can be e\u21b5ective. In addition, evolutionary methods have advantages on problems in which the learning agent cannot sense the complete state of its environment. Our focus is on reinforcement learning methods that learn while interacting with the environment, which evolutionary methods do not do. Methods able to take advantage of the details of individual behavioral interactions can be much more e\ufb03cient than evolutionary methods in many cases", "039641dd-c6c2-4ddf-8c34-17dca8c5400e": "These connections add the input to the block to its output, and no additional weights are needed. He et al. evaluated this method using deep convolutional networks with skip connections around every pair of adjacent layers, \ufb01nding substantial improvement over networks without the skip connections on benchmark image classi\ufb01cation tasks. Both batch normalization and deep residual learning were used in the reinforcement learning application to the game of Go that we describe in Chapter 16.\n\nA type of deep ANN that has proven to be very successful in applications, including impressive reinforcement learning applications (Chapter 16), is the deep convolutional network. This type of network is specialized for processing high-dimensional data arranged in spatial arrays, such as images. It was inspired by how early visual processing works in the brain . Because of its special architecture, a deep convolutional network can be trained by backpropagation without resorting to methods like those described above to train the deep layers. from LeCun et al. , was designed to recognize hand-written characters. It consists of alternating convolutional and subsampling layers, followed by several fully connected \ufb01nal layers. Each convolutional layer produces a number of feature maps", "cbec7a47-02e9-4b8b-95f0-10a5bb6aaba9": "A weight roughly corresponds to the e\ufb03cacy of a synaptic connection in a real neural network (see Section 15.1). If an ANN has at least one loop in its connections, it is a recurrent rather than a feedforward ANN. Although both feedforward and recurrent ANNs have been used in reinforcement learning, here we look only at the simpler feedforward case. The units (the circles in Figure 9.14) are typically semi-linear units, meaning that they compute a weighted sum of their input signals and then apply to the result a nonlinear function, called the activation function, to produce the unit\u2019s output, or activation.\n\nDi\u21b5erent activation functions are used, but they are typically S-shaped, or sigmoid, functions such as the logistic function f(x) = 1/(1 + e\u2212x), though sometimes the recti\ufb01er nonlinearity f(x) = max(0, x) is used. A step function like f(x) = 1 if x \u2265 \u2713, and 0 otherwise, results in a binary unit with threshold \u2713", "adf61f64-ac8a-47af-b849-1e27876a1dfc": "Dein value is defingd as usual:  https://www.deeplearningbook.org/contents/rnn.html    wb? =o (\" +S Ubi! + 5 Wis \") (10.46)  Oso [op + So Uz a + owen |. (10.47)  j j The reset and update gates can individually \u201cignore\u201d parts of the state vector. The update gates act like conditional leaky integrators that can linearly gate any  407  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  dimension, thus choosing to copy it (at one extreme of the sigmoid) or completely ignore it (at the other extreme) by replacing it with the new \u201ctarget state\u201d value (toward which the leaky integrator wants to converge). The reset gates control which parts of the state get used to compute the next target state, introducing an additional nonlinear effect in the relationship between past state and future state.\n\nMany more variants around this theme can be designed. For example the reset gate (or forget gate) output could be shared across multiple hidden units", "ce46e4df-f46e-45c3-afc3-923bb4982a70": "DEEP GENERATIVE MODELS  y is a waveform, and the entire waveform must sound like a coherent utterance. A natural way to represent the relationships between the entries in y is to use a probability distribution p(y | 2). Boltzmann machines, extended to model conditional distributions, can supply this probabilistic model. The same tool of conditional modeling with a Boltzmann machine can be used not just for structured output tasks, but also for sequence modeling. In the latter case, rather than mapping an input a to an output y, the model must estimate a probability distribution over a sequence of variables, p(x), ...,x(7). Conditional Boltzmann machines can represent factors of the form p(x | x@,... xD) in order to accomplish this task. An important sequence modeling task for the video game and film industry is modeling sequences of joint angles of skeletons used to render 3-D characters. These sequences are often collected using motion capture systems to record the movements of actors. A probabilistic model of a character\u2019s movement allows the generation of new, previously unseen, but realistic animations", "b32c82fc-361c-4358-af93-34d859ac83a8": "From (5.165), this is given by where H is the Hessian matrix comprising the second derivatives of the sum-ofsquares error function with respect to the components of w. Algorithms for computing and approximating the Hessian were discussed in Section 5.4.\n\nThe corresponding Gaussian approximation to the posterior is then given from (4.134) by Similarly, the predictive distribution is obtained by marginalizing with respect to this posterior distribution However, even with the Gaussian approximation to the posterior, this integration is still analytically intractable due to the nonlinearity of the network function y(x, w) as a function of w. To make progress, we now assume that the posterior distribution has small variance compared with the characteristic scales of w over which y(x, w) is varying", "79b3ddd4-c4d6-4fb5-9a0c-123777864d57": "In: International conference on artificial neural networks. Berlin: Springer; 2017. P. 626-34.\n\nCamilo B, Andrew JP, Larry TD, Allen TN, Susan MR, Bennett AL. Learning implicit brain MRI manifolds with deep learning. Int Soc Opt Photonics. 2018;10574:105741. Maria JMC, Sarfaraz H, Jeremy B, Ulas B. How to fool radiologists with generative adversarial networks? A visual turing test for lung cancer diagnosis. arXiv preprint. 2017. Baur C, Albarqouni S, Navab N. MelanoGANs: high resolution skin lesion synthesis with GANs. arXiv preprint. 2018. Madani A, Moradi M, Karargyris A, Syeda-Mahmood T. Chest x-ray generation and data augmentation for cardio- vascular abnormality classification. In: Medical imaging 2018. Image Processing 2018;10574:105741", "cdfeeb8d-abeb-48a9-b53d-7f627b86ff95": "In the case of the cross-entropy error function for a network with logistic sigmoid output-unit activation functions, the corresponding approximation is given by Exercise 5.19 An analogous result can be obtained for multiclass networks having softmax outputunit activation functions.\n\nExercise 5.20 We can use the outer-product approximation to develop a computationally ef\ufb01cient procedure for approximating the inverse of the Hessian . First we write the outer-product approximation in matrix notation as where bn \u2261 \u2207wan is the contribution to the gradient of the output unit activation arising from data point n. We now derive a sequential procedure for building up the Hessian by including data points one at a time. Suppose we have already obtained the inverse Hessian using the \ufb01rst L data points. By separating off the contribution from data point L + 1, we obtain In order to evaluate the inverse of the Hessian, we now consider the matrix identity where I is the unit matrix, which is simply a special case of the Woodbury identity (C.7)", "4a37aec6-799f-4752-8345-cb00e30840a4": "The feedforward classifier attempts to recognize all samples from the generative model as being fake and all samples from the training set as being real. In this framework, any structured pattern that the feedforward network can recognize is highly salient. The generative adversarial network is described in more detail in section 20.10.4. For the purposes of the present discussion, it is sufficient to understand that the networks learn how to determine what is salient. Lotter ef al. showed that models trained to generate images of human heads will often neglect to generate the ears when trained with mean squared error, but will successfully generate the ears when trained with the adversarial framework. Because the ears are not extremely bright or dark compared to the surrounding skin, they are not especially salient according to mean squared error loss, but their highly recognizable shape  542  CHAPTER 15. REPRESENTATION LEARNING  Ground Truth MSE Adversarial  Figure 15.6: Predictive generative networks provide an example of the importance of learning which features are salient. In this example, the predictive generative network has been trained to predict the appearance of a 3-D model of a human head at a specific viewing angle", "99f0d99b-46e6-443d-9922-3f1fadbb8730": "To compare the performance of one machine learning algorithm to another, it is necessary to perform controlled experiments. When comparing machine learning algorithm A and machine learning algorithm B, make  https://www.deeplearningbook.org/contents/regularization.html    sure that both algorithms are evaluated using the same hand-designed dataset augmentation schemes. Suppose that algorithm A performs poorly with no dataset  237  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  augmentation, and algorithm B performs well when combined with numerous syn- chetic transformations of the input.\n\nIn such a case the synthetic transformations ikely caused the improved performance, rather than the use of machine learning algorithm B. Sometimes deciding whether an experiment has been properly con- rolled requires subjective judgment. For example, machine learning algorithms hat inject noise into the input are performing a form of dataset augmentation. Usually, operations that are generally applicable (such as adding Gaussian noise to he input) are considered part of the machine learning algorithm, while operations hat are specific to one application domain (such as randomly cropping an image) are considered to be separate preprocessing steps. 7.5 Noise Robustness  Section 7.4 has motivated the use of noise applied to the inputs as a dataset augmentation strategy", "c0bbc343-3d73-4f4d-85ca-82d9823c9c4f": "Convolutional networks offer us the opportunity to take the pretraining strategy one step further than is possible with multilayer perceptrons. Instead of training an entire convolutional layer at a time, we can train a model of asmall patch, as Coates ei al. do with k-means. We can then use the parameters from this patch-based model to define the kernels of a convolutional layer. This means that it is possible to use unsupervised learning to train a convolutional network without ever using convolution during the training  357  CHAPTER 9. CONVOLUTIONAL NETWORKS  process. Using this approach, we can train very large models and incur a high computational cost only at inference time . This approach was popular from roughly 2007 to 2013, when labeled datasets were small and computational power was more limited.\n\nToday, most convolutional networks are trained in a purely supervised fashion, using full forward and back-propagation through the entire network on each training iteration. As with other approaches to unsupervised pretraining, it remains difficult to tease apart the cause of some of the benefits seen with this approach", "84ebba43-c534-4246-a682-d846f9703030": "From (7.85), the marginal likelihood is given by p(t|\u03b1, \u03b2) = N(t|0, C) in which the covariance matrix takes the form where \u03d5 denotes the N-dimensional vector (\u03c6(x1), \u03c6(x2))T, and similarly t = (t1, t2)T. Notice that this is just a zero-mean Gaussian process model over t with covariance C. Given a particular observation for t, our goal is to \ufb01nd \u03b1\u22c6 and \u03b2\u22c6 by maximizing the marginal likelihood. We see from Figure 7.10 that, if there is a poor alignment between the direction of \u03d5 and that of the training data vector t, then the corresponding hyperparameter \u03b1 will be driven to \u221e, and the basis vector will be pruned from the model", "2add74fe-ea54-4a9c-8b9e-2829d2bca0d0": "The other process takes the value function as given and performs some form of policy improvement, changing the policy to make it better, assuming that the value function is its value function. Although each process changes the basis for the other, overall they work together to \ufb01nd a joint solution: a policy and value function that are unchanged by either process and, consequently, are optimal. In some cases, GPI can be proved to converge, most notably for the classical DP methods that we have presented in this chapter. In other cases convergence has not been proved, but still the idea of GPI improves our understanding of the methods. It is not necessary to perform DP methods in complete sweeps through the state set. Asynchronous DP methods are in-place iterative methods that update states in an arbitrary order, perhaps stochastically determined and using out-of-date information. Many of these methods can be viewed as \ufb01ne-grained forms of GPI. Finally, we note one last special property of DP methods.\n\nAll of them update estimates of the values of states based on estimates of the values of successor states. That is, they update estimates on the basis of other estimates. We call this general idea bootstrapping", "003f487b-1835-4b5e-a5cf-c3e8e8b50d42": "2018. 14. Yosinski J, Clune J, Bengio Y, Lipson H. How transferable are features in deep neural networks? Adv Neural Inf Process Syst.\n\n2014;27:3320-8.  a Shorten and Khoshgoftaar J Big Data  6:60  Erhan D, Bengio Y, Courville A, Manzagol PA, Vincent P. Why does unsupervised pre-training help deep learning? J Mach Learn Res. 2010;11:625-60. Mark P, Dean P, Geoffrey H, Tom MM. Zero-shot learning with semantic output codes. In: NIPS; 2009. Yongqin X, Christoph HL, Bernt S, Zeynep A. Zero-shot learning\u2014a comprehensive evaluation of the good, the bad and the ugly. arXiv preprint. 2018. Yaniv T, Ming Y, Marc\u2019 AR, Lior W. DeepFace: closing the gap to human-level performance in face verification. In: CVPR \u201814; 2014", "8b11addc-c42a-46f6-a977-d69291ef17f4": "In section 15.4, we saw an example of a generative model that learned about  550  CHAPTER 15. REPRESENTATION LEARNING  the explanatory factors underlying images of faces, including the person\u2019s gender and whether they are wearing glasses. The generative model that accomplished this task was based on a deep neural network. It would not be reasonable to expect a shallow network, such as a linear network, to learn the complicated relationship between these abstract explanatory factors and the pixels in the image. In this and other AI tasks, the factors that can be chosen almost independently from each other yet still correspond to meaningful inputs are more likely to be very high level and related in highly nonlinear ways to the input. We argue that this demands deep distributed representations, where the higher level features (seen as unctions of the input) or factors (seen as generative causes) are obtained through che composition of many nonlinearities", "24f9d4ad-0d71-4bf4-a26e-96ae6f9257f0": "This \ufb01ts well with several properties of the neural circuitry: axons of dopamine neurons target both the dorsal and ventral subdivisions of the striatum; dopamine appears to be critical for modulating synaptic plasticity in both structures; and how a neuromodulator such as dopamine acts on a target structure depends on properties of the target structure and not just on properties of the neuromodulator. Section 13.5 presents actor\u2013critic algorithms as policy gradient methods, but the actor\u2013 critic algorithm of Barto, Sutton, and Anderson  was simpler and was presented as an arti\ufb01cial neural network (ANN). Here we describe an ANN implementation something like that of Barto et al., and we follow Takahashi, Schoenbaum, and Niv  in giving a schematic proposal for how this ANN might be implemented by real neural networks in the brain. We postpone discussion of the actor and critic learning rules until Section 15.8, where we present them as special cases of the policy-gradient formulation and discuss what they suggest about how dopamine might modulate synaptic plasticity", "60d9b981-3b91-4b80-a747-32018746568f": "Black-Box methods are usually model-agnostic since they do not require information from a model or its parameters and usually focus on task-speci\ufb01c heuristics for creating adversarial examples. For example, by enumerating feasible substitutions on the basis of word similarity and language models, Ren et al. and Garg and Ramakrishnan  select adversarial word replacements which severely in\ufb02uence the predictions from the text classi\ufb01cation model. To attack reading comprehension systems, Jia and Liang  1For more detailed discussion on textual adversarial examples, please refer to recent comprehensive surveys .\n\nand Wang and Bansal  insert distracting but meaningless sentences at different locations in paragraphs and Ribeiro et al. leverage rule-based paraphrasing to produce semanticallyequivalent adversarial examples. Likewise, for multi-hop question answering, Jiang and Bansal  insert shortcut reasoning sentences and Trivedi et al. constructed disconnected reasoning example by removing certain supporting facts. For machine translation, Belinkov and Bisk  attacks character-based models by natural or synthesized typos and Tan et al. further adopt subword morphology level attacks", "22460b7e-d7ee-47e3-9f0c-a66d1f52b576": "Batch mode means that the entire data set for learning is available from the start, as opposed to the online mode of the algorithms we focus on in this book in which data are acquired sequentially while the learning algorithm executes.\n\nBatch-mode reinforcement learning algorithms are sometimes necessary when online learning is not practical, and they can use any batch-mode supervised learning regression algorithm, including algorithms known to scale well to high-dimensional spaces. The convergence of FQI depends on properties of the function approximation algorithm . For their application to LTV optimization, Theocharous et al. used the same RF algorithm they used for the greedy optimization approach. Because in this case FQI convergence is not monotonic, Theocharous et al. kept track of the best FQI policy by o\u21b5-policy evaluation using a validation training set. The \ufb01nal policy for testing the LTV approach was the \"-greedy policy based on the best policy produced by FQI with the initial action-value function set to the mapping produced by the RF for the greedy optimization approach. To measure the performance of the policies produced by the greedy and LTV approaches, Theocharous et al", "d3e2b704-df8b-4deb-973b-d8cd1872b9e6": "Thus, pz is a Gaussian density function and fs : Z \u2014 Uis an invertible transformation:  z~pz(z) u=fg(z) z= f;(u)  A flow-based generative model learns the invertible mapping function by maximizing the likelihood of U's marginal:  max Hu\u2014BERT(s),s~D logpz(f, '(u)) +log det  where s is a sentence sampled from the text corpus D. Only the flow parameters \u00a2 are optimized while parameters in the pretrained BERT stay unchanged. BERT-flow was shown to improve the performance on most STS tasks either with or without supervision from NLI datasets. Because learning normalizing flows for calibration does not require labels, it can utilize the entire dataset including validation and test sets. Whitening Operation  Su et al.\n\napplied whitening operation to im and also to reduce the dimensionality of sentence  prove the isotropy of the learned representation embedding. They transform the mean value of the sentence vectors to 0 and the covariance matrix to the  identity matrix", "3d7fd595-55f0-4f0f-9c99-8983a3e507ec": "This term is a negative reconstruction error in auto-encoder parlance.\n\nIn order to solve our problem we invoked an alternative method for generating samples from q\u03c6(z|x). The essential parameterization trick is quite simple. Let z be a continuous random variable, and z \u223c q\u03c6(z|x) be some conditional distribution. It is then often possible to express the random variable z as a deterministic variable z = g\u03c6(\u03f5, x), where \u03f5 is an auxiliary variable with independent marginal p(\u03f5), and g\u03c6(.) is some vector-valued function parameterized by \u03c6. This reparameterization is useful for our case since it can be used to rewrite an expectation w.r.t q\u03c6(z|x) such that the Monte Carlo estimate of the expectation is differentiable w.r.t. \u03c6. A proof is as follows", "d819f19d-8080-48f0-8773-52f52fe509c4": "Almost any temporal-di\u21b5erence (TD) method, such as Q-learning or Sarsa, can be combined with eligibility traces to obtain a more general method that may learn more e\ufb03ciently. Eligibility traces unify and generalize TD and Monte Carlo methods. When TD methods are augmented with eligibility traces, they produce a family of methods spanning a spectrum that has Monte Carlo methods at one end (\u03bb=1) and one-step TD methods at the other (\u03bb = 0). In between are intermediate methods that are often better than either extreme method.\n\nEligibility traces also provide a way of implementing Monte Carlo methods online and on continuing problems without episodes. Of course, we have already seen one way of unifying TD and Monte Carlo methods: the n-step TD methods of Chapter 7. What eligibility traces o\u21b5er beyond these is an elegant algorithmic mechanism with signi\ufb01cant computational advantages. The mechanism is a short-term memory vector, the eligibility trace zt 2 Rd, that parallels the long-term weight vector wt 2 Rd", "475f93c4-9e76-432a-ba04-63b3dfa3daad": "We also need to evaluate the gradient of ln p(tN|\u03b8) with respect to the parameter vector \u03b8. Note that changes in \u03b8 will cause changes in a\u22c6 N, leading to additional terms in the gradient. Thus, when we differentiate (6.90) with respect to \u03b8, we obtain two sets of terms, the \ufb01rst arising from the dependence of the covariance matrix CN on \u03b8, and the rest arising from dependence of a\u22c6 N on \u03b8.\n\nThe terms arising from the explicit dependence on \u03b8 can be found by using (6.80) together with the results (C.21) and (C.22), and are given by To compute the terms arising from the dependence of a\u22c6 N on \u03b8, we note that the Laplace approximation has been constructed such that \u03a8(aN) has zero gradient at aN = a\u22c6 N, and so \u03a8(a\u22c6 N) gives no contribution to the gradient as a result of its dependence on a\u22c6 N", "5b844a2f-3ea9-4fe0-a54e-e434894ff66f": "CONFRONTING THE PARTITION FUNCTION  https://www.deeplearningbook.org/contents/partition.html    of feature x? given all the other features x \u2014? : n  log p(x; | @-).\n\n(18.20)  t=  If each random variable has k different values, this requires only k xn evaluations of p to compute, as opposed to the k\u201d evaluations needed to compute the partition function. This may look like an unprincipled hack, but it can be proved that estimation by maximizing the pseudolikelihood is asymptotically consistent . Of course, in the case of datasets that do not approach the large sample limit, pseudolikelihood may display different behavior from the maximum likelihood estimator. It is possible to trade computational complexity for deviation from maxi- mum likelihood behavior by using the generalized pseudolikelihood estima- tor . The generalized pseudolikelihood estimator uses m different sets S\u00ae,i = 1,...,m of indices of variables that appear together on the left side of the conditioning bar", "75497485-d19b-4108-a218-1bc19433b321": "Suppose we have two random variables, x and y, such that y = g(x), where g is an invertible, con- tinuous, differentiable transformation. One might expect that p,(y) = pa(g (y))- This is actually not the case. As a simple example, suppose we have scalar random variables x and y. Suppose y = 3 andx ~ U(0,1). If we use the rule p,(y) = pe(2y) then py will be 0 everywhere except the interval , and it will be 1 on this interval. This means  (3.43)  https://www.deeplearningbook.org/contents/prob.html  which violates the denmition of a probability distribution. his 1s a common mistake. The problem with this approach is that it fails to account for the distortion of space introduced by the Ri nection g", "a3c4d3ee-d1d9-43b8-b24b-becde039c7b7": "\u2018I'he specialized approach has the drawback ot requiring the library developer to define the PPT\u00b0P methods for every operation and limiting the user of the library to only those operations that have been defined. Yet the specialized approach also has the benefit of allowing customized back-propagation rules to be developed for each operation, enabling the developer to improve speed or stability in nonobvious ways that an automatic procedure would presumably be  unable to replicate. Back-propagation is therefore not the only way or the optimal way of computing the gradient, but it is a practical method that continues to serve the deep learning community well. In the future, differentiation technology for deep networks may improve as deep learning practitioners become more aware of advances in the broader field of automatic differentiation. 6.5.10 Higher-Order Derivatives  Some software frameworks support the use of higher-order derivatives.\n\nAmong the deep learning software frameworks, this includes at least Theano and TensorFlow. 219  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  These libraries use the same kind of data structure to describe the expressions for derivatives as they use to describe the original function being differentiated. This means that the symbolic differentiation machinery can be applied to derivatives", "41458faf-15eb-4af4-a34a-d97ec3ec7f3a": "The use of a support vector machine to solve a regression problem is illustrated using the sinusoidal data set in Figure 7.8. Here the parameters \u03bd and C have been Appendix A chosen by hand. In practice, their values would typically be determined by crossvalidation. Historically, support vector machines have largely been motivated and analysed using a theoretical framework known as computational learning theory, also sometimes called statistical learning theory .\n\nThis has its origins with Valiant  who formulated the probably approximately correct, or PAC, learning framework. The goal of the PAC framework is to understand how large a data set needs to be in order to give good generalization. It also gives bounds for the computational cost of learning, although we do not consider these here. Suppose that a data set D of size N is drawn from some joint distribution p(x, t) where x is the input variable and t represents the class label, and that we restrict attention to \u2018noise free\u2019 situations in which the class labels are determined by some (unknown) deterministic function t = g(x)", "3ccdcf3b-9f87-4428-b7a9-9c46f655bd6f": "Although the curse of dimensionality certainly raises important issues for pattern recognition applications, it does not prevent us from \ufb01nding effective techniques applicable to high-dimensional spaces. The reasons for this are twofold.\n\nFirst, real data will often be con\ufb01ned to a region of the space having lower effective dimensionality, and in particular the directions over which important variations in the target variables occur may be so con\ufb01ned. Second, real data will typically exhibit some smoothness properties (at least locally) so that for the most part small changes in the input variables will produce small changes in the target variables, and so we can exploit local interpolation-like techniques to allow us to make predictions of the target variables for new values of the input variables. Successful pattern recognition techniques exploit one or both of these properties. Consider, for example, an application in manufacturing in which images are captured of identical planar objects on a conveyor belt, in which the goal is to determine their orientation. Each image is a point in a high-dimensional space whose dimensionality is determined by the number of pixels. Because the objects can occur at different positions within the image and in different orientations, there are three degrees of freedom of variability between images, and a set of images will live on a three dimensional manifold embedded within the high-dimensional space", "b822c109-ba8d-4dcd-81f4-8b42397c2315": "In general, simultaneous gradient descent on two players\u2019 costs is not guaranteed to reach an equilibrium.\n\nConsider, for example, the value function v (a,b) = ab, where one player controls qg and incurs cost ab, while the other player controls b and receives a cost \u2014ab. If we model each player as making infinitesimally small gradient steps, each player reducing their own cost at the expense of the other player, then a and 6 go into a stable, circular orbit, rather than arriving at the equilibrium point at the origin. Note that the equilibria for a minimax game are not local minima of v. Instead, they are points that are simultaneously minima for both players\u2019 costs. This means that they are saddle points of v that are local minima with respect to the first player\u2019s parameters and local maxima with respect  identified an alternative formulation of the payoffs, in which  he game is no longer zero-sum, that has the same expected gradient as maximum likelihood learning whenever the discriminator is optimal. Because maximum  697  CHAPTER 20. DEEP GENERATIVE MODELS  likelihood training converges, this reformulation of the GAN game should also converge, given enough samples", "e2b4799f-3cbc-428b-8a8c-ee9a08216052": "APPROXIMATE INFERENCE  19.4.4 Interactions between Learning and Inference  Using approximate inference as part of a learning algorithm affects the learning process, and this in turn affects the accuracy of the inference algorithm. Specifically, the training algorithm tends to adapt the model in a way that makes the approximating assumptions underlying the approximate inference algorithm become more true. When training the parameters, variational learning increases  Enxg log p(v, h). (19.68)  For a specific v, this increases p(h | v) for values of h that have high probability under q(h | v) and decreases p(h | v) for values of h that have low probability under g(h | v). This behavior causes our approximating assumptions to become self-fulfilling prophecies", "9f6fa392-49ad-4ad8-ab97-7cf6976a905e": "Initialize the means \u00b5k, covariances \u03a3k and mixing coef\ufb01cients \u03c0k, and evaluate the initial value of the log likelihood. 2. E step. Evaluate the responsibilities using the current parameter values 3. M step. Re-estimate the parameters using the current responsibilities and check for convergence of either the parameters or the log likelihood. If the convergence criterion is not satis\ufb01ed return to step 2. In this section, we present a complementary view of the EM algorithm that recognizes the key role played by latent variables.\n\nWe discuss this approach \ufb01rst of all in an abstract setting, and then for illustration we consider once again the case of Gaussian mixtures. The goal of the EM algorithm is to \ufb01nd maximum likelihood solutions for models having latent variables. We denote the set of all observed data by X, in which the nth row represents xT n, and similarly we denote the set of all latent variables by Z, with a corresponding row zT n. The set of all model parameters is denoted by \u03b8, and so the log likelihood function is given by Note that our discussion will apply equally well to continuous latent variables simply by replacing the sum over Z with an integral", "1886c613-8da9-482a-9fad-e82a2e27d872": "On an intuitive level, the norm of a vector a measures the distance from the origin to the point wz.\n\nMore rigorously, a norm is any function f that satisfies the following properties:  e f(x) =0>2=0 e f(xt+y) < f(x)+ f(y) (the triangle inequality) e Va ER, f(ax) = |al f(x)  The L? norm, with p = 2, is known as the Euclidean norm, which is simply the Euclidean distance from the origin to the point identified by \u00ab The L? norm is used so frequently in machine learning that it is often denoted simply as ||a\\|, with the subscript 2 omitted. It is also common to measure the size of a vector using the squared L? norm, which can be calculated simply as \u00ab ! x. The squared L? norm is more convenient to work with mathematically and computationally than the L? norm itself. For example, each derivative of the squared L? norm with respect to each element of 2 depends only on the corre- sponding element of x, while all the derivatives of the L? norm depend on the entire vector", "df24fd52-12b6-4ba6-bfeb-28ca4c73fa30": "Furthermore, it may be possible that some kernels used in practice are unsuitable for capturing very complex distances in high dimensional sample spaces such as natural images. This is properly justi\ufb01ed by the fact that  shows that for the typical Gaussian MMD test to be reliable (as in it\u2019s power as a statistical test approaching 1), we need the number of samples to grow linearly with the number of dimensions. Since the MMD computational cost grows quadratically with the number of samples in the batch used to estimate equation (4), this makes the cost of having a reliable estimator grow quadratically with the number of dimensions, which makes it very inapplicable for high dimensional problems.\n\nIndeed, for something as standard as 64x64 images, we would need minibatches of size at least 4096 (without taking into account the constants in the bounds of  which would make this number substantially larger) and a total cost per iteration of 40962, over 5 orders of magnitude more than a GAN iteration when using the standard batch size of 64. That being said, these numbers can be a bit unfair to the MMD, in the sense that we are comparing empirical sample complexity of GANs with the theoretical sample complexity of MMDs, which tends to be worse", "03c6874c-ca9f-4965-8866-2c39cbbf55e8": "1.19 (\u22c6 \u22c6) Consider a sphere of radius a in D-dimensions together with the concentric hypercube of side 2a, so that the sphere touches the hypercube at the centres of each of its sides.\n\nBy using the results of Exercise 1.18, show that the ratio of the volume of the sphere to the volume of the cube is given by Now make use of Stirling\u2019s formula in the form which is valid for x \u226b 1, to show that, as D \u2192 \u221e, the ratio (1.145) goes to zero. Show also that the ratio of the distance from the centre of the hypercube to one of the corners, divided by the perpendicular distance to one of the sides, is \u221a D, which therefore goes to \u221e as D \u2192 \u221e. From these results we see that, in a space of high dimensionality, most of the volume of a cube is concentrated in the large number of corners, which themselves become very long \u2018spikes\u2019! We wish to \ufb01nd the density with respect to radius in polar coordinates in which the direction variables have been integrated out", "afe5f33e-e4e0-4d12-bf28-5b837d17a0e9": "(20.24)  E(v,hM An?) n@: 0) = \u2014v WOn \u2014-AOTWAn\u00ae \u2014 Ane wep), (20.25)  Figure 20.2: The graphical model for a deep Boltzmann machine with one visible layer (bottom) and two hidden layers. Connections are only between units in neighboring layers. There are no intralayer connections. 660  CHAPTER 20. DEEP GENERATIVE MODELS  In comparison to the RBM energy function (equation 20.5), the DBM energy function includes connections between the hidden units (latent variables) in the form of the weight matrices (W@) and W\u00ae)).\n\nAs we will see, these connections have significant consequences for the model behavior as well as how we go about  https://www.deeplearningbook.org/contents/generative_models.html    performing inference in the model. In comparison to fully connected Boltzmann machines (with every unit con- nected to every other unit), the DBM offers some advantages that are similar  to those offered by the RBM", "456a23b8-75a6-4e3e-8a7a-68352aa32598": "Example 6.3: Random walk under batch updating Batch-updating versions of TD(0) and constant-\u21b5 MC were applied as follows to the random walk prediction example (Example 6.2). After each new episode, all episodes seen so far were treated as a batch. They were repeatedly presented to the algorithm, either TD(0) or constant-\u21b5 MC, with \u21b5 su\ufb03ciently small that the value function converged. The resulting value function was then compared with v\u21e1, and the average root mean-squared error across the \ufb01ve states (and across 100 independent repetitions of the whole experiment) was plotted to obtain the learning curves shown in Figure 6.2. Note that the batch TD method was consistently better than the batch Monte Carlo method. This means that the \ufb01rst episode started in state A, transitioned to B with a reward of 0, and then terminated from B with a reward of 0. The other seven episodes were even shorter, starting from B and terminating immediately", "efb36fe6-ae60-4b52-ab88-f8e4e94a82ff": "It is important to recognize that the two stages are distinct. Thus, the \ufb01rst stage, namely the propagation of errors backwards through the network in order to evaluate derivatives, can be applied to many other kinds of network and not just the multilayer perceptron. It can also be applied to error functions other that just the simple sum-of-squares, and to the evaluation of other derivatives such as the Jacobian and Hessian matrices, as we shall see later in this chapter. Similarly, the second stage of weight adjustment using the calculated derivatives can be tackled using a variety of optimization schemes, many of which are substantially more powerful than simple gradient descent.\n\nWe now derive the backpropagation algorithm for a general network having arbitrary feed-forward topology, arbitrary differentiable nonlinear activation functions, and a broad class of error function. The resulting formulae will then be illustrated using a simple layered network structure having a single layer of sigmoidal hidden units together with a sum-of-squares error. Many error functions of practical interest, for instance those de\ufb01ned by maximum likelihood for a set of i.i.d", "9c702393-df6d-4355-a473-b2aa3dba8edf": "As in Chapters 9 and 12, we shall see that complex models can thereby be constructed from simpler components (in particular, from distributions belonging to the exponential family) and can be readily characterized using the framework of probabilistic graphical models. Here we focus on the two most important examples of state space models, namely the hidden Markov model, in which the latent variables are discrete, and linear dynamical systems, in which the latent variables are Gaussian.\n\nBoth models are described by directed graphs having a tree structure (no loops) for which inference can be performed ef\ufb01ciently using the sum-product algorithm. The easiest way to treat sequential data would be simply to ignore the sequential aspects and treat the observations as i.i.d., corresponding to the graph in Figure 13.2. Such an approach, however, would fail to exploit the sequential patterns in the data, such as correlations between observations that are close in the sequence. Suppose, for instance, that we observe a binary variable denoting whether on a particular day it rained or not. Given a time series of recent observations of this variable, we wish to predict whether it will rain on the next day", "09cc9d87-60be-4e2d-b769-f00ec2369282": "This can also be seen from the result (2.16) for the variance of the beta distribution, in which we see that the variance goes to zero for a \u2192 \u221e or b \u2192 \u221e.\n\nIn fact, we might wonder whether it is a general property of Bayesian learning that, as we observe more and more data, the uncertainty represented by the posterior distribution will steadily decrease. To address this, we can take a frequentist view of Bayesian learning and show that, on average, such a property does indeed hold. Consider a general Bayesian inference problem for a parameter \u03b8 for which we have observed a data set D, described by the joint distribution p(\u03b8, D). The following result Exercise 2.8 says that the posterior mean of \u03b8, averaged over the distribution generating the data, is equal to the prior mean of \u03b8. Similarly, we can show that The term on the left-hand side of (2.24) is the prior variance of \u03b8. On the righthand side, the \ufb01rst term is the average posterior variance of \u03b8, and the second term measures the variance in the posterior mean of \u03b8. Because this variance is a positive quantity, this result shows that, on average, the posterior variance of \u03b8 is smaller than the prior variance", "70dd0d5e-1b3d-42d5-b31d-c959dd00d875": "Equivalence between Data and Reward The recent work  introduced a unifying perspective of reformulating maximum likelihood supervised learning as a special instance of a policy optimization framework. In this perspective, data examples providing supervision signals are equivalent to a specialized reward function. Since the original framework  was derived for sequence generation problems, here we present a slightly adapted formulation for our context of data manipulation. To connect the maximum likelihood supervised learning with policy optimization, consider the model p\u03b8(y|x) as a policy that takes \u201caction\u201d y given the \u201cstate\u201d x.\n\nLet R(x, y|D) \u2208 R denote a reward function, and p(x) be the empirical data distribution which is known given D. Further assume a variational distribution q(x, y) that factorizes as q(x, y) = p(x)q(y|x). A variational policy optimization objective is then written as: where KL(\u00b7\u2225\u00b7) is the Kullback\u2013Leibler divergence; H(\u00b7) is the Shannon entropy; and \u03b1, \u03b2 > 0 are balancing weights", "fbaf60c6-ae22-44bb-bf00-96f59789d4da": "There are close connections between score matching, denoising autoencoders, and contractive autoencoders. These connections demonstrate that some kinds of autoencoders learn the data distribution in some way. We have not yet seen how . This manifold diffusion technique is a kind of Markov chain. There is also a more general Markov chain that can sample from any denoising autoencoder. 707  CHAPTER 20.\n\nDEEP GENERATIVE MODELS  20.11.1 Markov Chain Associated with Any Denoising Autoen- coder  The above discussion left open the question of what noise to inject and where to obtain a Markov chain that would generate from the distribution estimated by the autoencoder. Bengio et al. showed how to construct such a Markov chain for generalized denoising autoencoders. Generalized denoising autoencoders are specified by a denoising distribution for sampling an estimate of the clean input given the corrupted input. Each step of the Markov chain that generates from the estimated distribution consists of the following substeps, illustrated in figure 20.11:  1", "14dcc0a0-0d8d-4189-9933-f91651a62abd": "Distributed representations are powerful because they can use n features with k values to describe k\u201d different concepts. As we have seen throughout this book, neural networks with multiple hidden units and probabilistic models with multiple latent variables both make use of the strategy of distributed representation. We now introduce an additional observation.\n\nMany deep learning algorithms are motivated by the assumption that the hidden units  https://www.deeplearningbook.org/contents/representation.html    can learn to represent the underlying causal factors that explain the data, as discussed in section 15.3. Distributed representations are natural for this approach, because each direction in representation space can correspond to the value of a  different underlying configuration variable. An example of a distributed representation is a vector of n binary features, which can take 2\u201d configurations, each potentially corresponding to a different region in input space, as illustrated in figure 15.7. This can be compared with a symbolic representation, where the input is associated with a single symbol or category. If there are n symbols in the dictionary, one can imagine n feature detectors, each corresponding to the detection of the presence of the associated category", "6b814013-0a82-4878-95c6-e97f9284de03": "As a consequence of the anti-aliasing used to change the resolution of the images, the resulting MNIST digits are grey scale. These images were then centred in a 28 \u00d7 28 box. Examples of the MNIST digits are shown in Figure A.1. Error rates for classifying the digits range from 12% for a simple linear classi\ufb01er, through 0.56% for a carefully designed support vector machine, to 0.4% for a convolutional neural network . This is a synthetic data set that arose out of a project aimed at measuring noninvasively the proportions of oil, water, and gas in North Sea oil transfer pipelines .\n\nIt is based on the principle of dual-energy gamma densitometry. The ideas is that if a narrow beam of gamma rays is passed through the pipe, the attenuation in the intensity of the beam provides information about the density of material along its path. Thus, for instance, the beam will be attenuated more strongly by oil than by gas", "7062dbd1-55c5-454c-aa06-c70c944bbbe2": "This corresponds to a nonlinear decision boundary in the original input space, shown by the black curve in the left-hand plot. Bayes\u2019 theorem, represents an example of generative modelling, because we could take such a model and generate synthetic data by drawing values of x from the marginal distribution p(x). In the direct approach, we are maximizing a likelihood function de\ufb01ned through the conditional distribution p(Ck|x), which represents a form of discriminative training.\n\nOne advantage of the discriminative approach is that there will typically be fewer adaptive parameters to be determined, as we shall see shortly. It may also lead to improved predictive performance, particularly when the class-conditional density assumptions give a poor approximation to the true distributions. So far in this chapter, we have considered classi\ufb01cation models that work directly with the original input vector x. However, all of the algorithms are equally applicable if we \ufb01rst make a \ufb01xed nonlinear transformation of the inputs using a vector of basis functions \u03c6(x). The resulting decision boundaries will be linear in the feature space \u03c6, and these correspond to nonlinear decision boundaries in the original x space, as illustrated in Figure 4.12", "fa73b382-4b44-4938-8261-1539c3ab4681": "fi= o(W; .\n\n+ by) ; how much to forget the old value of parameters. Ut = o(W; . + b;) ; corresponding to the learning rate at time step t. 0, = \u2014Vo, Li  0 = frOn 1 +i OO  Model Setup (Xi, Yi) : mini-batches sampled from De\u00ae. D\u00ae, (Xi, Yi) (Xo, Yo) (Xs, Ys) y v \u00a5  = Ma = Ma, Me Learner. % % (Vi, \u00a31) (V2, L2) B 5 eee iB < 4 A, Meta-learner : outputs the learner\u2019s parameter Repeats T steps; t=1,...,D  Repeats D steps; d = 1, ...,D  The training process mimics what happens during test, since it has been proved to be beneficial in Matching Networks. During each training epoch, we first sample a dataset  D = (Drains Pest) \u20ac Dmeta-train and then sample mini-batches out of Di,ai, to update @ for T rounds", "02e1fc44-4752-48cd-9b1d-85d30c83554b": "Greedy action selection always exploits current knowledge to maximize immediate reward; it spends no time at all sampling apparently inferior actions to see if they might really be better.\n\nA simple alternative is to behave greedily most of the time, but every once in a while, say with small probability \", instead select randomly from among all the actions with equal probability, independently of the action-value estimates. We call methods using this near-greedy action selection rule \"-greedy methods. An advantage of these methods is that, in the limit as the number of steps increases, every action will be sampled an in\ufb01nite number of times, thus ensuring that all the Qt(a) converge to q\u21e4(a). This of course implies that the probability of selecting the optimal action converges to greater than 1 \u2212 \", that is, to near certainty. These are just asymptotic guarantees, however, and say little about the practical e\u21b5ectiveness of the methods. To roughly assess the relative e\u21b5ectiveness of the greedy and \"-greedy action-value methods, we compared them numerically on a suite of test problems. This was a set of 2000 randomly generated k-armed bandit problems with k = 10", "191cc692-1edb-4b32-a06c-5017eb4364cc": "8.2.7 Poor Correspondence between Local and Global Structure  Many of the problems we have discussed so far correspond to properties of the  https://www.deeplearningbook.org/contents/optimization.html    oss function at a single point\u2014it can be difficult to make a single step if J (8) is poorly conditioned at the current point 9, or if @ lies on a cliff, or if 8 is a saddle point hiding the opportunity to make progress downhill from the gradient. It is possible to overcome all these problems at a single point and still perform poorly if the direction that results in the most improvement locally does not point \u2018oward distant regions of much lower cost. Goodfellow et al. argue that much of the runtime of training is due to he length of the trajectory needed to arrive at the solution. Figure 8.2 shows that he learning trajectory spends most of its time tracing out a wide arc around a mountain-shaped structure.\n\nMuch of research into the difficulties of optimization has focused on whether  287  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  )  Figure 8.4: Optimization based on local downhill moves can fail if the local surface does not point toward the global solution", "3b01463e-0285-43a0-b987-bc1e715b5c29": ";--whitening 69.65 7757 74.66 82.27 78.39 79.52 76.91 77.00 * SimCSE-BERT\u00bbase 75.30 84.67 80.19 85.40 80.82 84.25 80.39 81.57 SROBERTazase\u2122 71.54 72.49 70.80 78.74 73.69 77.77 74.46 74.21 SRoBERTayas-whitening 70.46 77.07 74.46 81.64 76.43 79.49 76.65 76.60 * SimCSE-RoBERTapase 76.53 85.21 80.95 86.03 82.57 85.83 80.50 82.52 + SimCSE-RoBERTaiarge 7746 87.27 82.36 =: 86.66 = 83.93 86.70 81.95 83.76  Supervision from NLI  The pre-trained BERT sentence embedding without any fine-tuning has been found to have poor performance for semantic similarity tasks", "65e643c6-a3a3-4d01-8880-fa6f72f145b7": "i L(v, 0, h) (19.38) FE (log o(bj) \u2014 logh,) + (1 \u2014 hy)(log o(\u2014b,) \u2014 log(1 \u2014 hy))| (19.39)  hy j=l  Sd log 8; | v3 \u201420;W;, hey W2 he + SLWy Wy rhelr | ) )  j=l l~k (19.40  =log o(bj) \u2014 log hy \u2014 1 + log(1 \u2014 hy) + 1 \u2014 log o(\u2014b;) (19.41  n 1 . + (ww, \u2014 3 Via \u2014 WW, fn! (19.42) j=l ki . 1 . \u2014p. T . T . T -}. =b; zy ee \u2014h)+v BW - i (s - a W. BW. ih", "e0de5fef-8b95-4bef-9f39-9e214efd8503": "We can use (6.13) and (6.17) to extend this class of kernels by considering sums over products of different probability distributions, with positive weighting coef\ufb01cients p(i), of the form This is equivalent, up to an overall multiplicative constant, to a mixture distribution in which the components factorize, with the index i playing the role of a \u2018latent\u2019 variable. Two inputs x and x\u2032 will give a large value for the kernel function, and Section 9.2 hence appear similar, if they have signi\ufb01cant probability under a range of different components.\n\nTaking the limit of an in\ufb01nite sum, we can also consider kernels of the form where z is a continuous latent variable. Now suppose that our data consists of ordered sequences of length L so that an observation is given by X = {x1, . , xL}. A popular generative model for sequences is the hidden Markov model, which expresses the distribution p(X) as a Section 13.2 marginalization over a corresponding sequence of hidden states Z = {z1, . , zL}", "36fbd1aa-e08d-41a2-be83-80cb0325b42b": "Chapter 5  Machine Learning Basics  Deep learning is a specific kind of machine learning. To understand deep learning well, one must have a solid understanding of the basic principles of machine learning. This chapter provides a brief course in the most important general principles that are applied throughout the rest of the book. Novice readers or those who want a wider perspective are encouraged to consider machine learning textbooks with a more comprehensive coverage of the fundamentals, such as Murphy  or Bishop . If you are already familiar with machine learning basics, feel free to skip ahead to section 5.11. That section covers some perspectives on traditional machine learning techniques that have strongly influenced the development of deep learning algorithms. We begin with a definition of what a learning algorithm is and present an example: the linear regression algorithm. We then proceed to describe how the challenge of fitting the training data differs from the challenge of finding patterns that generalize to new data", "8c2682a9-c14e-406d-b916-88df20cba091": "Careful design of reward signals is essential if an agent is to act in the real world with no opportunity for human vetting of its actions or means to easily interrupt its behavior.\n\nDespite the possibility of unintended negative consequences, optimization has been used for hundreds of years by engineers, architects, and others whose designs have positively impacted the world. We owe much that is good in our environment to the application of optimization methods. Many approaches have been developed to mitigate the risk of optimization, such as adding hard and soft constraints, restricting optimization to robust and risk-sensitive policies, and optimizing with multiple objective functions. Some of these approaches have been adapted to reinforcement learning, and more research is needed to address these concerns. The problem of ensuring that a reinforcement learning agent\u2019s goal is attuned to our own remains a challenge. Another challenge if reinforcement learning agents are to act and learn in the real world is not just about what they might learn eventually, but about how they will behave while they are learning", "aa19b6a0-c3d0-4ddf-9e39-3794d9254570": "component networks implementing the actor and the critic.\n\nThe critic consists of a single neuron-like unit, V , whose output activity represents state values, and a component shown as the diamond labeled TD that computes TD errors by combining V \u2019s output with reward signals and with previous state values (as suggested by the loop from the TD diamond to itself). The actor network has a single layer of k actor units labeled Ai, i = 1, . , k. The output of each actor unit is a component of a k-dimensional action vector. An alternative is that there are k separate actions, one commanded by each actor unit, that compete with one another to be executed, but here we will think of the entire A-vector as an action. Both the critic and actor networks receive input consisting of multiple features representing the state of the agent\u2019s environment. (Recall from Chapter 1 that the environment of a reinforcement learning agent includes components both inside and outside of the \u2018organism\u2019 containing the agent.) The \ufb01gure shows these features as the circles labeled x1, x2, . , xn, shown twice just to keep the \ufb01gure simple", "1ece072e-2b62-4e42-b1ac-f23ace9b5b3d": "If maxyL (vu, 6 ,q) < log pv; 6*), because 6* induces too complicated of a posterior distribution for our q family to capture, then the learning process will never approach @*. Such a problem is very difficult to detect, because we can only know for sure that it happened if we have a superior learning algorithm that can find 6* for comparison. 19.5 Learned Approximate Inference  We have seen that inference can be thought of as an optimization procedure that increases the value of a function \u00a3.\n\nExplicitly performing optimization via iterative procedures such as fixed-point equations or gradient-based optimization is often very expensive and time consuming. Many approaches to inference avoid  648  CHAPTER 19. APPROXIMATE INFERENCE  this expense by learning to perform approximate inference. Specifically, we can think of the optimization process as a function f that maps an input v to an approximate distribution q* = arg max, L(v, q). Once we think of the multistep iterative optimization process as just being a function, we can approximate it with  a neural network that implements an approximation f(v; 6)", "516df69b-cc66-4b9e-8c1d-42cb8dda5e37": "Because we now have a Gaussian approximation for the posterior distribution over w, and a model for a that is a linear function of w, we can now appeal to the results of Section 4.5.2.\n\nThe distribution of output unit activation values, induced by the distribution over network weights, is given by where q(w|D) is the Gaussian approximation to the posterior distribution given by (5.167). From Section 4.5.2, we see that this distribution is Gaussian with mean aMAP \u2261 a(x, wMAP), and variance Finally, to obtain the predictive distribution, we must marginalize over a using with \u2018tanh\u2019 activation functions and a single logistic-sigmoid output unit. The weight parameters were found using scaled conjugate gradients, and the hyperparameter \u03b1 was optimized using the evidence framework. On the left is the result of using the simple approximation (5.185) based on a point estimate wMAP of the parameters, in which the green curve shows the y = 0.5 decision boundary, and the other contours correspond to output probabilities of y = 0.1, 0.3, 0.7, and 0.9. On the right is the corresponding result obtained using (5.190)", "5383ebb2-84ba-484c-af8b-65aef7c7f48c": "The method is based on the observation that  lw + ie) = fla) + ief'(w) +02), (11) MEI) _ Fe) 4+0(2), (119)  real(f(x + ie)) = f(x) +O(e*), imag(  https://www.deeplearningbook.org/contents/guidelines.html    c  where 7 = ,/-1. Unlike in the real-valued case above, there is no cancellation effect because we take the difference between the value of f at different ppoints. This allows the use of tiny values of \u20ac, like e= 10-159, which make the O(e ?) error insignificant for all practical purposes. Monitor histograms of activations and gradient: It is often useful to visualize statistics of neural network activations and gradients, collected over a large amount of training iterations (maybe one epoch). The preactivation value of hidden units can tell us if the units saturate, or how often they do", "474dda88-e2e2-4cab-9161-633ad21bce60": "Advancing self- supervised learning for vision  Most recently, we\u2019ve created and open sourced a new billion-  parameter self-supervised CV model called SEER that\u2019s proven to work efficiently with complex, high-  https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/  Self-supervised learning: The dark matter of intelligence   dimensional image data.\n\nIt is based on the SwAV method applied to a convolutional network architecture (ConvNet) and can be trained from a vast number of random images without any metadata or annotations. The ConvNet is large enough to capture and learn every visual concept from this large and complex data. After pretraining ona billion random, unlabeled and uncurated public Instagram images, and supervised fine- tuning on ImageNet, SEER outperformed the most advanced,  state-of-the-art self-supervised systems, reaching 84.2 percent top-1  accuracy on ImageNet. These results show that we can bring the self-supervised learning paradigm shift to computer vision", "bca97bc2-757f-4c08-9786-9d6192371dab": "Summarizing the situation, Doll, Simon, and Daw  wrote that \u201cmodel-based in\ufb02uences appear ubiquitous more or less wherever the brain processes reward information,\u201d and this is true even in the regions thought to be critical for model-free learning. This includes the dopamine signals themselves, which can exhibit the in\ufb02uence of model-based information in addition to the reward prediction errors thought to be the basis of model-free processes. Continuing neuroscience research informed by reinforcement learning\u2019s model-free and model-based distinction has the potential to sharpen our understanding of habitual and goal-directed processes in the brain. A better grasp of these neural mechanisms may lead to algorithms combining model-free and model-based methods in ways that have not yet been explored in computational reinforcement learning.\n\nUnderstanding the neural basis of drug abuse is a high-priority goal of neuroscience with the potential to produce new treatments for this serious public health problem. One view is that drug craving is the result of the same motivation and learning processes that lead us to seek natural rewarding experiences that serve our biological needs. Addictive substances, by being intensely reinforcing, e\u21b5ectively co-opt our natural mechanisms of learning and decision making", "1e1fffab-2d52-40a2-b270-de8552c83e48": "Perhaps more importantly, the fact that we can train the critic till optimality makes it impossible to collapse modes when we do.\n\nThis is due to the fact that mode collapse comes from the fact that the optimal generator for a \ufb01xed discriminator is a sum of deltas on the points the discriminator assigns the highest values, as observed by  and highlighted in . In the following section we display the practical bene\ufb01ts of our new algorithm, and we provide an in-depth comparison of its behaviour and that of traditional GANs. We run experiments on image generation using our Wasserstein-GAN algorithm and show that there are signi\ufb01cant practical bene\ufb01ts to using it over the formulation used in standard GANs. \u2022 a meaningful loss metric that correlates with the generator\u2019s convergence and sample quality \u2022 improved stability of the optimization process We run experiments on image generation. The target distribution to learn is the LSUN-Bedrooms dataset  \u2013 a collection of natural images of indoor bedrooms. Our baseline comparison is DCGAN , a GAN with a convolutional architecture trained with the standard GAN procedure using the \u2212 log D trick . The generated samples are 3-channel images of 64x64 pixels in size", "1aace2fc-93e2-49bb-97f7-d464fe5d1763": "Simard, P., Y. Le Cun, and J. Denker . Ef\ufb01cient pattern recognition using a new transformation distance. In S. J. Hanson, J. D. Cowan, and Simard, P., B. Victorri, Y. Le Cun, and J. Denker . Tangent prop \u2013 a formalism for specifying selected invariances in an adaptive network. In J. E. Moody, S. J. Hanson, and R. P. Lippmann (Eds. ), Advances in Neural Information Processing Systems, Volume 4, pp. 895\u2013903. Morgan Kaufmann. Simard, P. Y., D. Steinkraus, and J. Platt . Best practice for convolutional neural networks applied to visual document analysis. In Proceedings International Conference on Document Analysis and Recognition (ICDAR), pp. 958\u2013 962", "d69c1214-2977-4e26-9f59-4837d2247e42": "Deep learning does not always involve especially deep graphical models. In the context of graphical models, we can define the depth of a model in terms of the graphical model graph rather than the computational graph. We can think of a latent variable h; as being at depth 7 if the shortest path from h; to an observed variable is 7 steps. We usually describe the depth of the model as being the greatest depth of any such h;. This kind of depth is different from the depth induced by the computational graph. Many generative models used for deep learning have no latent variables or only one layer of latent variables but use deep computational graphs to define the conditional distributions within a model. Deep learning essentially always makes use of the idea of distributed represen- tations. Even shallow models used for deep learning purposes (such as pretraining shallow models that will later be composed to form deep ones) nearly always have a single large layer of latent variables. Deep learning models typically have more latent variables than observed variables.\n\nComplicated nonlinear interactions between variables are accomplished via indirect connections that flow through multiple latent variables", "26ebc4f1-7eb5-4fc0-93a3-2cecb6317cc1": "We can model the cost function J with a quadratic approximation in the neighborhood of the empirically optimal value of the weights w\u201d*:  J(0) = J(w*) + S(w \u2014w*)' H(w \u2014 w*), (7.33)  where H is the Hessian matrix of J with respect to w evaluated at w*.\n\nGiven the assumption that w* is a minimum of J(w), we know that H is positive semidefinite. 247  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  https://www.deeplearningbook.org/contents/regularization.html    Under a local Taylor series approximation, the gradient is given by  Vw (w) = H(w \u2014w\u2019%). (7.34)  We are going to study the trajectory followed by the parameter vector during training. For simplicity, let us set the initial parameter vector to the origin,\u00ae w) = 0", "b2368064-17f9-4663-9681-8290428b9841": "When some of the inputs may be missing, rather than providing a single classification function, the learning algorithm must learn a set of functions. Each function corresponds to classifying x with a different subset of its inputs missing. This kind of situation arises frequently in medical diagnosis, because many kinds of medical tests are expensive or invasive. One way to efficiently define such a large set of functions is to learn a probability distribution over all the relevant variables, then solve the  https://www.deeplearningbook.org/contents/ml.html    classification task by marginalizing out the missing variables.\n\nWith \u2122 input variables, we can now obtain all 2\u00b0 different classification functions needed for each possible set of missing inputs, but the computer program needs to learn only a single function describing the joint probability distribution. See Goodfellow et al. for an example of a deep probabilistic model applied to such a task in this way. Many of the other tasks described in this section can also be generalized to work with missing inputs; classification with missing inputs is just one example of what machine learning can do. 98  CHAPTER 5", "d1953b92-a7e6-4a0d-9534-e74ec76a0e77": "There might be a target right at the end (as depicted here), or the gradient on the output o) can be obtained by back-propagating from further downstream modules. of the unrolled graph in figure 10.3, followed by a backward propagation pass moving right to left through the graph. The runtime is O(r) and cannot be reduced by parallelization because the forward propagation graph is inherently sequential; each time step may be computed only after the previous one. States computed in the forward pass must be stored until they are reused during the backward pass, so the memory cost is also O(7). The back-propagation algorithm applied to the unrolled graph with O(7) cost is called back-propagation through time (BPTT) and is discussed further in section 10.2.2. The network with recurrence between hidden units is thus very powerful but also expensive to train. Is there an alternative? 10.2.1 Teacher Forcing and Networks with Output Recurrence  The network with recurrent connections only from the output at one time step to the hidden units at the next time step (shown in figure 10.4) is strictly less powerful because it lacks hidden-to-hidden recurrent connections.\n\nFor example, it cannot simulate a universal Turing machine", "7a8253a6-ac81-4b1a-a970-45251fcb3338": "The reinforcement learning problem is deeply indebted to the idea of Markov decision processes (MDPs) from the \ufb01eld of optimal control. These historical in\ufb02uences and other major in\ufb02uences from psychology are described in the brief history given in Chapter 1.\n\nReinforcement learning adds to MDPs a focus on approximation and incomplete information for realistically large problems. MDPs and the reinforcement learning problem are only weakly linked to traditional learning and decision-making problems in arti\ufb01cial intelligence. However, arti\ufb01cial intelligence is now vigorously exploring MDP formulations for planning and decision making from a variety of perspectives. MDPs are more general than previous formulations used in arti\ufb01cial intelligence in that they permit more general kinds of goals and uncertainty. The theory of MDPs is treated by, for example, Bertsekas , White , Whittle , and Puterman . A particularly compact treatment of the \ufb01nite case is given by Ross . MDPs are also studied under the heading of stochastic optimal control, where adaptive optimal control methods are most closely related to reinforcement learning . The theory of MDPs evolved from e\u21b5orts to understand the problem of making sequences of decisions under uncertainty, where each decision can depend on the previous decisions and their outcomes", "34a13da9-47fb-4b5c-bb71-625ffea09c0d": "8.19 (\u22c6 \u22c6) Apply the sum-product algorithm derived in Section 8.4.4 to the chain-ofnodes model discussed in Section 8.4.1 and show that the results (8.54), (8.55), and (8.57) are recovered as a special case. 8.20 (\u22c6) www Consider the message passing protocol for the sum-product algorithm on a tree-structured factor graph in which messages are \ufb01rst propagated from the leaves to an arbitrarily chosen root node and then from the root node out to the leaves. Use proof by induction to show that the messages can be passed in such an order that at every step, each node that must send a message has received all of the incoming messages necessary to construct its outgoing messages", "488a0d01-b50c-48fc-b1dd-7ccd2cf2640a": "When we evaluate the predictive distribution, we require C- 1, which involves the inversion of a D x D matrix. The computation required to do this can be reduced by making use of the matrix inversion identity (C.7) to give C- 1 = 0-- 11 - 0--2WM- 1W T (12.40) where the M x M matrix M is defined by Because we invert M rather than inverting C directly, the cost of evaluating C- 1 is reduced from O(D3 ) to O(M3 ). As well as the predictive distribution p(x), we will also require the posterior distributionp(zlx), which can again be written down directly using the result (2.116) for linear-Gaussian models to give Note that the posterior mean depends on x, whereas the posterior covariance is independent of x. We next consider the determination of the model parameters using maximum likelihood.\n\nGiven a data set X = {xn } of observed data points, the probabilistic peA model can be expressed as a directed graph, as shown in Figure 12.10", "af013f2f-f745-47e9-8c26-4a571fcbe415": "2.17 (\u22c6) www Consider the multivariate Gaussian distribution given by (2.43).\n\nBy writing the precision matrix (inverse covariance matrix) \u03a3\u22121 as the sum of a symmetric and an anti-symmetric matrix, show that the anti-symmetric term does not appear in the exponent of the Gaussian, and hence that the precision matrix may be taken to be symmetric without loss of generality. Because the inverse of a symmetric matrix is also symmetric (see Exercise 2.22), it follows that the covariance matrix may also be chosen to be symmetric without loss of generality. 2.18 (\u22c6 \u22c6 \u22c6) Consider a real, symmetric matrix \u03a3 whose eigenvalue equation is given by (2.45). By taking the complex conjugate of this equation and subtracting the original equation, and then forming the inner product with eigenvector ui, show that the eigenvalues \u03bbi are real. Similarly, use the symmetry property of \u03a3 to show that two eigenvectors ui and uj will be orthogonal provided \u03bbj \u0338= \u03bbi", "f3006f59-7429-4b68-851e-d69076655709": "The other way to use planning is to begin and complete it after encountering each new state St, as a computation whose output is the selection of a single action At; on the next step planning begins anew with St+1 to produce At+1, and so on. The simplest, and almost degenerate, example of this use of planning is when only state values are available, and an action is selected by comparing the values of model-predicted next states for each action (or by comparing the values of afterstates as in the tic-tac-toe example in Chapter 1).\n\nMore generally, planning used in this way can look much deeper than one-step-ahead and evaluate action choices leading to many di\u21b5erent predicted state and reward trajectories. Unlike the \ufb01rst use of planning, here planning focuses on a particular state. We call this decision-time planning. These two ways of thinking about planning\u2014using simulated experience to gradually improve a policy or value function, or using simulated experience to select an action for the current state\u2014can blend together in natural and interesting ways, but they have tended to be studied separately, and that is a good way to \ufb01rst understand them. Let us now take a closer look at decision-time planning", "0623becd-dabc-4948-813e-954e3539006f": "The idea behind continuation methods is to construct a series of objective functions over the same parameters. To minimize a cost function J(@), we construct new cost functions {JO, oe KY) }. These cost functions are designed to be increasingly difficult, with J) being fairly easy to minimize, and J(\u2122), the most difficult, being J (9), the true cost function motivating the entire process. When we say that J\u00ae is easier than J+), we mean that it is well behaved over more of 8 space. A random initialization is more likely to land in the region where local descent can minimize the cost function successfully because this region is larger. The series of cost functions are designed so that a solution to one is a good initial point of the next.\n\nWe thus begin by solving an easy problem, then refine the solution to solve incrementally harder problems until we arrive at a solution to the true underlying problem. Traditional continuation methods (predating the use of continuation methods for neural network training) are usually based on smoothing the objective function. See Wu  for an example of such a method and a review of some related methods. Continuation methods are also closely related to simulated annealing, which adds noise to the parameters", "e888ecbc-b2e7-4745-8429-351ccb2cd302": "We \ufb01ne-tune the model for 3 epochs with a learning rate of 2e-5 and a batch size of 16. Results are presented in Table 4. BERTLARGE outperforms the authors\u2019 baseline ESIM+ELMo system by +27.1% and OpenAI GPT by 8.3%. In this section, we perform ablation experiments over a number of facets of BERT in order to better understand their relative importance. Additional ablation studies can be found in Appendix C. We demonstrate the importance of the deep bidirectionality of BERT by evaluating two pretraining objectives using exactly the same pretraining data, \ufb01ne-tuning scheme, and hyperparameters as BERTBASE: No NSP: A bidirectional model which is trained using the \u201cmasked LM\u201d (MLM) but without the \u201cnext sentence prediction\u201d (NSP) task. LTR & No NSP: A left-context-only model which is trained using a standard Left-to-Right (LTR) LM, rather than an MLM.\n\nThe left-only constraint was also applied at \ufb01ne-tuning, because removing it introduced a pre-train/\ufb01ne-tune mismatch that degraded downstream performance", "37e6042d-a7bd-491e-973c-39e6ea620e34": "All the changes in notation are summarized in a table on page xix. The second edition is signi\ufb01cantly expanded, and its top-level organization has been changed. After the introductory \ufb01rst chapter, the second edition is divided into three new parts. The \ufb01rst part (Chapters 2\u20138) treats as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. We cover both learning and planning methods for the tabular case, as well as their uni\ufb01cation in n-step methods and in Dyna. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, Double learning, tree-backup, Q(\u03c3), RTDP, and MCTS. Doing the tabular case \ufb01rst, and thoroughly, enables core ideas to be developed in the simplest possible setting. The second part of the book (Chapters 9\u201313) is then devoted to extending the ideas to function approximation", "fce3eb66-a788-41d3-8d01-9d7196ff30dc": "In a single dimension, it is easy to obtain a local minimum by tossing a coin and getting heads once. In n-dimensional space, it is exponentially unlikely that all n coin tosses will be heads. See Dauphin ef al. for a review of the relevant theoretical work. An amazing property of many random functions is that the eigenvalues of the Hessian become more likely to be positive as we reach regions of lower cost. In our coin tossing analogy, this means we are more likely to have our coin come up heads n times if we are at a critical point with low cost.\n\nIt also means that local minima are much more likely to have low cost than high cost. Critical points with high cost are far more likely to be saddle points. Critical points with extremely high cost are more likely to be local maxima. This happens for many classes of random functions. Does it happen for neural networks? Baldi and Hornik  showed theoretically that shallow autoencoders feedforward networks trained to copy their input to their output, described in chapter 14) with no nonlinearities have global minima and saddle points but no ocal minima with higher cost than the global minimum", "9511628f-efb9-44de-bb4d-6c6c87df64d3": "\u2020 Hal\u0131c\u0131o\u011flu Data Science Institute, University of California San Diego, San Diego, USA \u2021 Machine Learning Department, Carnegie Mellon University, Pittsburgh, USA \u266e Mohamed bin Zayed University of Arti\ufb01cial Intelligence, Abu Dhabi, UAE Abstract. Machine learning (ML) is about computational methods that enable machines to learn concepts from experience. In handling a wide variety of experience ranging from data instances, knowledge, constraints, to rewards, adversaries, and lifelong interaction in an ever-growing spectrum of tasks, contemporary ML/AI (arti\ufb01cial intelligence) research has resulted in a multitude of learning paradigms and methodologies. Despite the continual progresses on all di\ufb00erent fronts, the disparate narrowly focused methods also make standardized, composable, and reusable development of ML approaches di\ufb03cult, and preclude the opportunity to build AI agents that panoramically learn from all types of experience", "0b80dd5a-b667-4c00-b6a4-cbd1850e2ebc": "Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. Hadsell, R., Chopra, S., and LeCun, Y. Dimensionality reduction by learning an invariant mapping.\n\nIn 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201906), volume 2, pp. 1735\u20131742. IEEE, 2006. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016. He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Momentum contrast for unsupervised visual representation learning. arXiv preprint arXiv:1911.05722, 2019. H\u00e9naff, O. J., Razavi, A., Doersch, C., Eslami, S., and Oord, A. v", "cee28fdb-737e-4c65-a744-65c1fc2d4cdc": "LINEAR FACTOR MODELS  https://www.deeplearningbook.org/contents/linear_factors.html    13.59 Manitold Interpretation of PCA  Linear factor models including PCA and factor analysis can be interpreted as learning a manifold .\n\nWe can view probabilistic PCA as defining a thin pancake-shaped region of high probability\u2014a Gaussian distribution that is very narrow along some axes, just as a pancake is very flat along its vertical axis, but is elongated along other axes, just as a pancake is wide along its horizontal axes. This is illustrated in figure 13.3. PCA can be interpreted as aligning this pancake with a linear manifold in a higher-dimensional space. This interpretation applies not just to traditional PCA but also to any linear autoencoder that learns matrices W and V with the goal of making the reconstruction of x lie as close to x as possible. Let the encoder be h= f(x) =W'(a\u2014 p). (13.19)  The encoder computes a low-dimensional representation of h", "a5eee44b-c2dc-4f8b-8f2c-7a6988b1e2e8": "The ratio \u21e2t:T \u22121 transforms the returns to have the right expected value: Now we are ready to give a Monte Carlo algorithm that averages returns from a batch of observed episodes following policy b to estimate v\u21e1(s). It is convenient here to number time steps in a way that increases across episode boundaries. That is, if the \ufb01rst episode of the batch ends in a terminal state at time 100, then the next episode begins at time t = 101. This enables us to use time-step numbers to refer to particular steps in particular episodes. In particular, we can de\ufb01ne the set of all time steps in which state s is visited, denoted T(s). This is for an every-visit method; for a \ufb01rst-visit method, T(s) would only include time steps that were \ufb01rst visits to s within their episodes. Also, let T(t) denote the \ufb01rst time of termination following time t, and Gt denote the return after t up through T(t). Then {Gt}t2T(s) are the returns that pertain to state s, and the corresponding importance-sampling ratios", "b3516d30-2d57-46e9-8479-b60255164825": "First consider a computational graph describing how to compute a single scalar u\u2122 (say, the loss on a training example).\n\nThis scalar is the quantity whose gradient we want to obtain, with respect to the n; input nodes u to ul), In other words, we wish to compute ou for all 1,2,...,n; . In the application of back-propagation to computing gradients foregyadient descent over parameters,  a  (m\\ ay (m \\  https://www.deeplearningbook.org/contents/mlp.html    u\u201d will be the cost associated with an example or a minibatch, while u\u2018\u201d\u2019 to u\u201d\u2019  correspond to the parameters of the model. We will assume that the nodes of the graph have been ordered in such a way that we can compute their output one after the other, starting at u(t) and going up to u\\\u201d). As defined in algorithm 6.1, each node u is associated with an operation f\u00ae and is computed by evaluating the function  u = F(A), (6.48) where A is the set of all nodes that are parents of u@. 204  CHAPTER 6", "13bab0b7-dbbc-4314-871d-59535c191600": "We see that the factor involving the derivative of the logistic sigmoid has cancelled, leading to a simpli\ufb01ed form for the gradient of the log likelihood.\n\nIn particular, the contribution to the gradient from data point n is given by the \u2018error\u2019 yn \u2212 tn between the target value and the prediction of the model, times the basis function vector \u03c6n. Furthermore, comparison with (3.13) shows that this takes precisely the same form as the gradient of the sum-of-squares error function for the linear regression model. Section 3.1.1 If desired, we could make use of the result (4.91) to give a sequential algorithm in which patterns are presented one at a time, in which each of the weight vectors is updated using (3.22) in which \u2207En is the nth term in (4.91). It is worth noting that maximum likelihood can exhibit severe over-\ufb01tting for data sets that are linearly separable. This arises because the maximum likelihood solution occurs when the hyperplane corresponding to \u03c3 = 0.5, equivalent to wT\u03c6 = 0, separates the two classes and the magnitude of w goes to in\ufb01nity", "8c294bdc-0beb-4a9a-85a1-424804203d38": "We consider a generative model in which there are two latent variables corresponding to the unobserved speech signal amplitudes, and there are two observed variables given by the signal values at the microphones. The latent variables have a joint distribution that factorizes as above, and the observed variables are given by a linear combination of the latent variables. There is no need to include a noise distribution because the number of latent variables equals the number of observed variables, and therefore the marginal distribution of the observed variables will not in general be singular, so the observed variables are simply deterministic linear combinations of the latent variables. Given a data set of observations, the likelihood function for this model is a function of the coefficients in the linear combination. The log likelihood can be maximized using gradient-based optimization giving rise to a particular version of independent component analysis. The success of this approach requires that the latent variables have non-Gaussian distributions.\n\nTo see this, recall that in probabilistic PCA (and in factor analysis) the latent-space distribution is given by a zero-mean isotropic Gaussian. The model therefore cannot distinguish between two different choices for the latent variables where these differ simply by a rotation in latent space", "e15ff6c3-a69f-46e4-a585-5d3886e36bf0": "The vanishing and exploding gradient problem for RNNs was independently discovered by separate researchers . One may hope that the problem can be avoided simply by staying in a region of parameter space where the gradients do not vanish or explode. Unfortunately, in order to store memories in a way that is robust to small perturbations, the RNN must enter a region of parameter space where gradients vanish . Specifically, whenever the model is able to represent long-term dependencies, che gradient of a long-term interaction has exponentially smaller magnitude than he gradient of a short-term interaction.\n\nThis means not that it is impossible to learn, but that it might take a very long time to learn long-term dependencies, because the signal about these dependencies will tend to be hidden by the smallest fluctuations arising from short-term dependencies. In practice, the experiments in Bengio et al. show that as we increase the span of the dependencies that  398  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  need to be captured, gradient-based optimization becomes increasingly difficult, with the probability of successful training of a traditional RNN via SGD rapidly reaching 0 for sequences of only length 10 or 20", "c0ff8e09-ccbb-4c34-af3c-c15752a31bfe": "Now minimize J with respect to U and show that the s~ution satisfies SU = UH. Clearly, one possible solution is that the columns of U are eigenvectors of S, in which case H is a diagonal matrix containing the corresponding eigenvalues. To obtain the general solution, show that H can be assumed to be a symmetr~ ma~ix, and by using its eigenvect\u00a3r expansion show that the general solution to SU =~UH gives the same value for J as the specific solution in which the columns of U are the eigenvectors of S. Because these solutions are all equivalent, it is convenient to choose the eigenvector solution. 12.4 (*) Imm Suppose we replace the zero-mean, unit-covariance latent space distribution (12.31) in the probabilistic PCA model by a general Gaussian distribution of the formN(zlm, ~)", "80e08af2-489d-4eba-9765-4f23e9619511": "Note that for both viewpoints, the matrix inversion must be performed once for the given training set.\n\nFor each new test point, both methods require a vector-matrix multiply, which has cost O(N 2) in the Gaussian process case and O(M 2) for the linear basis function model. If the number M of basis functions is smaller than the number N of data points, it will be computationally more ef\ufb01cient to work in the basis function framework. However, an advantage of a Gaussian processes viewpoint is that we can consider covariance functions that can only be expressed in terms of an in\ufb01nite number of basis functions. For large training data sets, however, the direct application of Gaussian process methods can become infeasible, and so a range of approximation schemes have been developed that have better scaling with training set size than the exact approach . Practical issues in the application of Gaussian processes are discussed in Bishop and Nabney . We have introduced Gaussian process regression for the case of a single target variable. The extension of this formalism to multiple target variables, known as co-kriging , is straightforward", "28093c36-7037-4e0d-8bb9-457cc849684e": "Models of characters and environments are specified via lists of 3-D coordinates of vertices. Graphics cards must perform matrix multiplication and division on many  https://www.deeplearningbook.org/contents/applications.html    vertices in parallel to convert these 3-D coordinates into 2-D on-screen coordinates. The graphics card must then perform many computations at each pixel in parallel to determine the color of each pixel.\n\nIn both cases, the computations are fairly simple  439  CHAPTER 12. APPLICATIONS  and do not involve much branching compared to the computational workload that a CPU usually encounters. For example, each vertex in the same rigid object will be multiplied by the same matrix; there is no need to evaluate an if statement per vertex to determine which matrix to multiply by. The computations are also entirely independent of each other, and thus may be parallelized easily. The computations also involve processing massive buffers of memory, containing bitmaps describing the texture (color pattern) of each object to be rendered. Together, this results in graphics cards having been designed to have a high degree of parallelism and high memory bandwidth, at the cost of having a lower clock speed and less branching capability relative to traditional CPUs", "a049b527-6112-47fb-96db-bd8e5eb22679": ", xN} consisting of N observations of a random D-dimensional Euclidean variable x. Our goal is to partition the data set into some number K of clusters, where we shall suppose for the moment that the value of K is given. Intuitively, we might think of a cluster as comprising a group of data points whose inter-point distances are small compared with the distances to points outside of the cluster.\n\nWe can formalize this notion by \ufb01rst introducing a set of D-dimensional vectors \u00b5k, where k = 1, . , K, in which \u00b5k is a prototype associated with the kth cluster. As we shall see shortly, we can think of the \u00b5k as representing the centres of the clusters. Our goal is then to \ufb01nd an assignment of data points to clusters, as well as a set of vectors {\u00b5k}, such that the sum of the squares of the distances of each data point to its closest vector \u00b5k, is a minimum. It is convenient at this point to de\ufb01ne some notation to describe the assignment of data points to clusters", "9f2d7a6d-8689-44f2-8edb-2d0c501ba065": "Thus if we write then the value of the normalization constant ZE, also known as the partition function, is not needed in order to draw samples from p(z).\n\nHowever, knowledge of the value of ZE can be useful for Bayesian model comparison since it represents the model evidence (i.e., the probability of the observed data given the model), and so it is of interest to consider how its value might be obtained. We assume that direct evaluation by summing, or integrating, the function exp(\u2212E(z)) over the state space of z is intractable. For model comparison, it is actually the ratio of the partition functions for two models that is required. Multiplication of this ratio by the ratio of prior probabilities gives the ratio of posterior probabilities, which can then be used for model selection or model averaging. One way to estimate a ratio of partition functions is to use importance sampling from a distribution with energy function G(z) where {z(l)} are samples drawn from the distribution de\ufb01ned by pG(z). If the distribution pG is one for which the partition function can be evaluated analytically, for example a Gaussian, then the absolute value of ZE can be obtained", "7c36dcf2-73e5-485d-8df7-d34b9c29043a": "Yet the unifying way of thinking would be incredibly valuable, to continuously unleash the extensive power of current vibrant research, to produce more principled understanding, and to build more versatile AI solutions. Abdolmaleki, A., Springenberg, J. T., Tassa, Y., Munos, R., Heess, N., & Riedmiller, M. Maximum a posteriori policy optimisation. International Conference on Learning Representations. Altun, Y., & Smola, A. .\n\nUnifying divergence minimization and statistical inference via convex duality. International Conference on Computational Learning Theory, 139\u2013153. Anderson, P. W. More is di\ufb00erent: Broken symmetry and the nature of the hierarchical structure of science. Science, 177, 393\u2013396. Arjovsky, M., Chintala, S., & Bottou, L. Wasserstein generative adversarial networks. International Conference on Machine Learning, 214\u2013223", "8c4d9e4c-0b64-4902-8ac3-48faa2b5bb3a": "Of course, successive samples from the Markov chain will be highly correlated, and so to obtain samples that are nearly independent it will be necessary to subsample the sequence. We can obtain the Gibbs sampling procedure as a particular instance of the Metropolis-Hastings algorithm as follows. Consider a Metropolis-Hastings sampling step involving the variable zk in which the remaining variables z\\k remain \ufb01xed, and for which the transition probability from z to z\u22c6 is given by qk(z\u22c6|z) = p(z\u22c6 k|z\\k). We note that z\u22c6 \\k = z\\k because these components are unchanged by the sampling step. Also, p(z) = p(zk|z\\k)p(z\\k). Thus the factor that determines the acceptance probability in the Metropolis-Hastings (11.44) is given by where we have used z\u22c6 \\k = z\\k. Thus the Metropolis-Hastings steps are always accepted", "0d5edf12-2420-4037-b1ee-b1c031545d85": "One can view an energy-based model with multiple terms in its energy function as being a product of experts . Each term in the energy function corresponds to another factor in the probability distribution. Each term of the energy function can  5For some models, we may still need to use constrained optimization to make sure Z exists. 567  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  Figure 16.5: This graph implies that E(a,b,c,d,e,f) can be written as Fa(a,b) + Ep ,c(b, \u00a2) + Ea,a(a, d) + Ep,e(b, e) + Ee,s(e, f) for an appropriate choice of the per-clique  energy functions. Note that we can obtain the \u00a2 functions in figure 16.4 by setting each \u00a2 to the exponential of the corresponding negative energy, e.g.,da,p(a, b) = exp (\u2014E(a, b))", "53e56719-aa3d-4f78-9c78-e00a02f382b4": "APPLICATIONS  personalization, it is possible to train a model once, then deploy it to be used by billions of users. In many cases, the end user is more resource constrained than the developer.\n\nFor example, one might train a speech recognition network with a powerful computer cluster, then deploy it on mobile phones. A key strategy for reducing the cost of inference is model compression . The basic idea of model compression is to replace the original, expensive model with a smaller model that requires less memory and runtime to store and evaluate. Model compression is applicable when the size of the original model is driven primarily by a need to prevent overfitting. In most cases, the model with the lowest generalization error is an ensemble of several independently trained models. Evaluating all n ensemble members is expensive. Sometimes, even a single model generalizes better if it is large (for example, if it is regularized with dropout). These large models learn some function f(a), but do so using many more parameters than are necessary for the task. Their size is necessary only because of the limited number of training examples", "71666151-070e-4dd3-844d-abe0ff2b00d4": "For instance, the extensive astronomical observations of Tycho Brahe in the 16th century allowed Johannes Kepler to discover the empirical laws of planetary motion, which in turn provided a springboard for the development of classical mechanics. Similarly, the discovery of regularities in atomic spectra played a key role in the development and veri\ufb01cation of quantum physics in the early twentieth century. The \ufb01eld of pattern recognition is concerned with the automatic discovery of regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories. Consider the example of recognizing handwritten digits, illustrated in Figure 1.1. Each digit corresponds to a 28\u00d728 pixel image and so can be represented by a vector x comprising 784 real numbers.\n\nThe goal is to build a machine that will take such a vector x as input and that will produce the identity of the digit 0, . , 9 as the output. This is a nontrivial problem due to the wide variability of handwriting", "30e51ad7-c8e0-46ea-8e3f-c7f10fb99796": "Using the sum rule, the denominator in Bayes\u2019 theorem can be expressed in terms of the quantities appearing in the numerator We can view the denominator in Bayes\u2019 theorem as being the normalization constant required to ensure that the sum of the conditional probability on the left-hand side of (1.12) over all values of Y equals one.\n\nIn Figure 1.11, we show a simple example involving a joint distribution over two variables to illustrate the concept of marginal and conditional distributions. Here a \ufb01nite sample of N = 60 data points has been drawn from the joint distribution and is shown in the top left. In the top right is a histogram of the fractions of data points having each of the two values of Y . From the de\ufb01nition of probability, these fractions would equal the corresponding probabilities p(Y ) in the limit N \u2192 \u221e. We can view the histogram as a simple way to model a probability distribution given only a \ufb01nite number of points drawn from that distribution. Modelling distributions from data lies at the heart of statistical pattern recognition and will be explored in great detail in this book", "3a7d57e8-94eb-4322-b452-3ececc9f55e2": "For example, it is common to use the term \u201cbatch size\u201d to describe the size of a minibatch. Optimization algorithms that use only a single example at a time are sometimes called stochastic and sometimes online methods. The term \u201conline\u201d is usually reserved for when the examples are drawn from a stream of continually created examples rather than from a fixed-size training set over which several passes are made. Most algorithms used for deep learning fall somewhere in between, using more  275  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  than one but fewer than all the training examples. These were traditionally called minibatch or minibatch stochastic methods, and it is now common to call them simply stochastic methods. The canonical example of a stochastic method is stochastic gradient descent, presented in detail in section 8.3.1. Minibatch sizes are generally driven by the following factors:  Larger batches provide a more accurate estimate of the gradient, but with less than linear returns. Multicore architectures are usually underutilized by extremely small batches.\n\nThis motivates using some absolute minimum batch size, below which there is no reduction in the time to process a minibatch", "89ebcb91-4e52-475d-af62-135572109387": "Consider an uncertain event, for example whether the moon was once in its own orbit around the sun, or whether the Arctic ice cap will have disappeared by the end of the century. These are not events that can be repeated numerous times in order to de\ufb01ne a notion of probability as we did earlier in the context of boxes of fruit. Nevertheless, we will generally have some idea, for example, of how quickly we think the polar ice is melting. If we now obtain fresh evidence, for instance from a new Earth observation satellite gathering novel forms of diagnostic information, we may revise our opinion on the rate of ice loss. Our assessment of such matters will affect the actions we take, for instance the extent to which we endeavour to reduce the emission of greenhouse gasses. In such circumstances, we would like to be able to quantify our expression of uncertainty and make precise revisions of uncertainty in the light of new evidence, as well as subsequently to be able to take optimal actions or decisions as a consequence. This can all be achieved through the elegant, and very general, Bayesian interpretation of probability", "e5503ad4-b67d-4f79-a54a-0efffff8520e": "No set of m-dimensional vectors can have more than m mutually linearly independent columns, but a matrix with more than m columns may have more than one such set.\n\nFor the matrix to have an inverse, we additionally need to ensure that equa- tion 2.11 has at most one solution for each value of b. To do so, we need to make certain that the matrix has at most m columns. Otherwise there is more than one way of parametrizing each solution. Together, this means that the matrix must be square, that is, we require that m =n and that all the columns be linearly independent. A square matrix with linearly dependent columns is known as singular. If A is not square or is square but singular, solving the equation is still possible, but we cannot use the method of matrix inversion to find the solution. 36  CHAPTER 2. LINEAR ALGEBRA  So far we have discussed matrix inverses as being multiplied on the left. It is also possible to define an inverse that is multiplied on the right:  AA =I. (2.29)  For square matrices, the left inverse and right inverse are equal", "9a0d119c-97dc-468e-a6af-f8df50b50247": "Evolutionary methods ignore much of the useful structure of the reinforcement learning problem: they do not use the fact that the policy they are searching for is a function from states to actions; they do not notice which states an individual passes through during its lifetime, or which actions it selects. In some cases this information can be misleading (e.g., when states are misperceived), but more often it should enable more e\ufb03cient search.\n\nAlthough evolution and learning share many features and naturally work together, we do not consider evolutionary methods by themselves to be especially well suited to reinforcement learning problems and, accordingly, we do not cover them in this book. To illustrate the general idea of reinforcement learning and contrast it with other approaches, we next consider a single example in more detail. Consider the familiar child\u2019s game of tic-tac-toe. Two players take turns playing on a three-by-three board. One player plays Xs and the other Os until one player wins by placing three marks in a row, horizontally, vertically, or diagonally, as the X player has in the game shown to the right. If the board \ufb01lls up with neither player getting three in a row, then the game is a draw", "117d2474-eb20-4ad8-b413-e468f28500ed": "The \ufb01rst two dimensions are whether they update state values or action values and whether they estimate the value for the optimal policy or for an arbitrary given policy. These two dimensions give rise to four classes of updates for approximating the four value functions, q\u21e4, v\u21e4, q\u21e1, and v\u21e1. The updates certainly yield a better estimate because they are uncorrupted by sampling error, but they also require more computation, and computation is often the limiting resource in planning.\n\nTo properly assess the relative merits of expected and sample updates for planning we must control for their di\u21b5erent computational requirements. For concreteness, consider the expected and sample updates for approximating q\u21e4, and the special case of discrete states and actions, a table-lookup representation of the approximate value function, Q, and a model in the form of estimated dynamics, \u02c6p(s0, r|s, a)", "31558fb3-451c-4e21-9535-1d1472a0ae8b": "Here the quantities ak are de\ufb01ned by The normalized exponential is also known as the softmax function, as it represents a smoothed version of the \u2018max\u2019 function because, if ak \u226b aj for all j \u0338= k, then p(Ck|x) \u2243 1, and p(Cj|x) \u2243 0. We now investigate the consequences of choosing speci\ufb01c forms for the classconditional densities, looking \ufb01rst at continuous input variables x and then discussing brie\ufb02y the case of discrete inputs. Let us assume that the class-conditional densities are Gaussian and then explore the resulting form for the posterior probabilities. To start with, we shall assume that all classes share the same covariance matrix. Thus the density for class Ck is given by Consider \ufb01rst the case of two classes.\n\nFrom (4.57) and (4.58), we have We see that the quadratic terms in x from the exponents of the Gaussian densities have cancelled (due to the assumption of common covariance matrices) leading to a linear function of x in the argument of the logistic sigmoid. This result is illustrated for the case of a two-dimensional input space x in Figure 4.10", "7708d4c2-6fb7-47be-bb18-05a8eb9016c5": "20.10.9 Neural Auto-Regressive Networks  Neural auto-regressive networks (Bengio and Bengio, 2000a,b) have the same left-to-right graphical model as logistic auto-regressive networks (figure 20.8) but  https://www.deeplearningbook.org/contents/generative_models.html    employ a GULerent paramevrizavion OL LE COUGLUONAL GISLLIDULIOUS WILL Lat graphical model structure. The new parametrization is more powerful in the sense that its capacity can be increased as much as needed, allowing approximation of any joint distribution. The new parametrization can also improve generalization by introducing a parameter sharing and feature sharing principle common to deep learning in general.\n\nThe models were motivated by the objective of avoiding the curse of dimensionality arising out of traditional tabular graphical models, sharing  703  CHAPTER 20. DEEP GENERATIVE MODELS  the same structure as figure 20.8. In tabular discrete probabilistic models, each conditional distribution is represented by a table of probabilities, with one entry and one parameter for each possible configuration of the variables involved. By using a neural network instead, two advantages are obtained:  1", "3c43e889-a7d7-40fa-801e-f2161e559cc8": "Increases time and mem- ory cost of most opera- tions. Weight decay co-| decreased | Decreasing the weight de-  efficient cay coefficient frees the model parameters to be- come larger. Dropout rate decreased | Dropping units less often  gives the units more oppor- tunities to \u201cconspire\u201d with each other to fit the train- ing set. Table 11.1: The effect of various hyperparameters on model capacity.\n\nproblems optimization does not seem to be a significant barrier, provided that the  model is chosen appropriately. CHAPTER 11.  https://www.deeplearningbook.org/contents/guidelines.html  426  PRACTICAL METHODOLOGY       11.4.2. Automatic Hyperparameter Uptimization Algorithms The ideal learning algorithm just takes a dataset and outputs a function, without requiring hand tuning of hyperparameters. The popularity of several learning algorithms such as logistic regression and SVMs stems in part from their ability to perform well with only one or two tuned hyperparameters. Neural networks can sometimes perform well with only a small number of tuned hyperparameters, but often benefit significantly from tuning of forty or more", "4d9b1001-22cf-47b5-9897-413126c4ee29": "It finds a low-dimensional projection of the data such as to preserve, as closely as possible, the pairwise distances between data points, and involves finding the eigenvectors of the distance matrix. In the case where the distances are Euclidean, it gives equivalent results to PCA. The MDS concept can be extended to a wide variety of data types specified in terms of a similarity matrix, giving nonmetric MDS.\n\nTwo other nonprobabilistic methods for dimensionality reduction and data visualization are worthy of mention. Locally linear embedding, or LLE  first computes the set of coefficients that best reconstructs each data point from its neighbours. These coefficients are arranged to be invariant to rotations, translations, and scalings of that data point and its neighbours, and hence they characterize the local geometrical properties of the neighbourhood. LLE then maps the high-dimensional data points down to a lower dimensional space while preserving these neighbourhood coefficients. If the local neighbourhood for a particular data point can be considered linear, then the transformation can be achieved using a combination of translation, rotation, and scaling, such as to preserve the angles formed between the data points and their neighbours", "23a23fa8-4457-49dc-8db6-4a4f26d087b1": "For example, in 2011, the best CPUs available could run neural network workloads faster when using fixed-point arithmetic rather than floating-point arithmetic. By creating a carefully tuned fixed- point implementation, Vanhoucke e? al. obtained a threefold speedup over a strong floating-point system. Each new model of CPU has different performance characteristics, so sometimes floating-point implementations can be faster too.\n\nThe important principle is that careful specialization of numerical computation routines can yield a large payoff. Other strategies, besides choosing whether to use fixed or floating point, include optimizing data structures to avoid cache misses and using vector instructions. Many machine learning researchers neglect these implementation details, but when the performance of an implementation restricts the size of the model, the accuracy of the model suffers. 12.1.2 GPU Implementations  Most modern neural network implementations are based on graphics processing units. Graphics processing units (GPUs) are specialized hardware components hat were originally developed for graphics applications. The consumer market for video gaming systems spurred development of graphics processing hardware. The performance characteristics needed for good video gaming systems turn out to be beneficial for neural networks as well. Video game rendering requires performing many operations in parallel quickly", "9b07df47-652f-4262-853a-d2f4dab4f019": "The joint distribution is maximized by setting x = 1 and y = 0, corresponding the value 0.4.\n\nHowever, the marginal for p(x), obtained by summing over both values of y, is given by p(x = 0) = 0.6 and p(x = 1) = 0.4, and similarly the marginal for y is given by p(y = 0) = 0.7 and p(y = 1) = 0.3, and so the marginals are maximized by x = 0 and y = 0, which corresponds to a value of 0.3 for the joint distribution. In fact, it is not dif\ufb01cult to construct examples for which the set of individually most probable values has probability zero under the joint distribution. Exercise 8.27 We therefore seek an ef\ufb01cient algorithm for \ufb01nding the value of x that maximizes the joint distribution p(x) and that will allow us to obtain the value of the joint distribution at its maximum. To address the second of these problems, we shall simply write out the max operator in terms of its components where M is the total number of variables, and then substitute for p(x) using its expansion in terms of a product of factors. In deriving the sum-product algorithm, we made use of the distributive law (8.53) for multiplication", "9233b5ea-7415-4c22-bbd6-ce6b35a80a8b": "PRACTICAL METHODOLOGY  estimate by zooming in and running a grid search over {\u20140.1,0,0.1}.\n\nThe obvious problem with grid search is that its computational cost grows exponentially with the number of hyperparameters. If there are m hyperparameters, each taking at most n values, then the number of training and evaluation trials required grows as O(n\u2122). The trials may be run in parallel and exploit loose parallelism (with almost no need for communication between different machines carrying out the search). Unfortunately, because of the exponential cost of grid search, even parallelization may not provide a satisfactory size of search. 11.4.4 Random Search  Fortunately, there is an alternative to grid search that is as simple to program, more convenient to use, and converges much faster to good values of the hyperparameters: random search . A random search proceeds as follows. First we define a marginal distribution for each hyperparameter, for example, a Bernoulli or multinoulli for binary or discrete hyperparameters, or a uniform distribution on a log-scale for positive real-valued hyperparameters", "9f9b59ef-076f-4987-a934-b2e3c6457891": "All the methods we have discussed so far in this chapter have required computation per time step proportional to the number of parameters. With more computation, however, one can do better. In this section we present a method for linear function approximation that is arguably the best that can be done for this case. As we established in Section 9.4 TD(0) with linear function approximation converges asymptotically (for appropriately decreasing step sizes) to the TD \ufb01xed point: Why, one might ask, must we compute this solution iteratively? This is wasteful of data! Could one not do better by computing estimates of A and b, and then directly computing the TD \ufb01xed point? The Least-Squares TD algorithm, commonly known as LSTD, does exactly this. It forms the natural estimates where I is the identity matrix, and \"I, for some small \" > 0, ensures that bAt is always invertible. It might seem that these estimates should both be divided by t, and indeed they should; as de\ufb01ned here, these are really estimates of t times A and t times b", "5c173e79-7b9e-401c-8467-d1be4a2aa121": "The one-hot code still confers some statistical advantages (it naturally conveys the idea that all examples in the same cluster are similar to each other), and it confers the computational advantage that the entire representation  147  CHAPTER 5. MACHINE LEARNING BASICS  may be captured by a single integer. The k-means algorithm works by initializing k different centroids {4 Me, pl} to different values, then alternating between two different steps until convergence. In one step, each training example is assigned to cluster 7, where 7 is the index of the nearest centroid pb, In the other step, each centroid po is updated to the mean of all training examples \u00ab) assigned to cluster i. One difficulty pertaining to clustering is that the clustering problem is inherently ill posed, in the sense that there is no single criterion that measures how well a clustering of the data corresponds to the real world.\n\nWe can measure properties of the clustering, such as the average Euclidean distance from a cluster centroid to the members of the cluster. This enables us to tell how well we are able to reconstruct the training data from the cluster assignments. We do not know how well the cluster assignments correspond to properties of the real world", "192e55d1-59c5-4f26-a509-bb6527cbf791": "We see that the computations that must be performed in order to update the variational posterior distribution over the model parameters involve evaluation of the same sums over the data set, as arose in the maximum likelihood treatment. In order to perform this variational M step, we need the expectations E = rnk representing the responsibilities. These are obtained by normalizing the \u03c1nk that are given by (10.46). We see that this expression involves expectations with respect to the variational distributions of the parameters, and these are easily evaluated to give Exercise 10.14 where we have introduced de\ufb01nitions of \ufffd\u039bk and \ufffd\u03c0k, and \u03c8(\u00b7) is the digamma function de\ufb01ned by (B.25), with \ufffd\u03b1 = \ufffd k \u03b1k. The results (10.65) and (10.66) follow from the standard properties of the Wishart and Dirichlet distributions", "4ef9539f-503e-450a-b5fa-44ff52a351f9": "Figure 5.12 shows how, instead, uniformly sampled points look like the patterns of static that appear on analog television sets when no signal is available. Similarly, if you generate a document by picking letters uniformly at random, what is the probability that you will get a meaningful English-language text? Almost zero, again, because most of the long sequences of letters do not correspond to a natural language sequence: the distribution of natural language sequences occupies a very little volume in the total space of sequences of letters. Of course, concentrated probability distributions are not sufficient to show that the data lies on a reasonably small number of manifolds.\n\nWe must also establish that the examples we encounter are connected to each other by other examples, with each example surrounded by other highly similar examples that can be reached by applying transformations to traverse the manifold. The second argument in favor of the manifold hypothesis is that we can imagine such neighborhoods and transformations, at least informally. In the case of images, we can certainly think of many possible transformations that allow us to trace out a manifold in image space: we can gradually dim or brighten the lights, gradually move or rotate objects in the image, gradually alter the colors on the surfaces of objects, and so forth", "a55660d7-11ef-4f60-a79d-28781bea7bc3": "Foundations and Trends in Machine Bengio, Y., Courville, A. C., Vincent, P. Unsupervised feature learning and deep learning: A review and new perspectives. CoRR 1, ArXiv:1206.5538. Berg, H. C. Chemotaxis in bacteria. Annual review of biophysics and bioengineering, Berns, G. S., McClure, S. M., Pagnoni, G., Montague, P. R. .\n\nPredictability modulates human brain response to reward. The journal of neuroscience, 21(8):2793\u20132798. Berridge, K. C., Kringelbach, M. L. A\u21b5ective neuroscience of pleasure: reward in humans impact, reward learning, or incentive salience? Brain Research Reviews, 28(3):309\u2013369. Berry, D. A., Fristedt, B. Bandit Problems. Chapman and Hall, London. Bertsekas, D. P. Distributed dynamic programming", "c66df00c-cfcc-4fe2-9608-1ed1217400a3": "Sample updates di\u21b5er from the expected updates of DP methods in that they are based on a single sample successor rather than on a complete distribution of all possible successors. Finally, note that the quantity in brackets in the TD(0) update is a sort of error, measuring the di\u21b5erence between the estimated value of St and the better estimate Rt+1 + \u03b3V (St+1). This quantity, called the TD error, arises in various forms throughout reinforcement learning: Notice that the TD error at each time is the error in the estimate made at that time. Because the TD error depends on the next state and next reward, it is not actually available until one time step later", "5c0d2f28-c115-4c22-8793-feb9db5b5810": "His inspiration apparently came from Claude Shannon\u2019s  suggestion that a computer could be programmed to use an evaluation function to play chess, and that it might be able to improve its play by modifying this function online. (It is possible that these ideas of Shannon\u2019s also in\ufb02uenced Bellman, but we know of no evidence for this.) Minsky  extensively discussed Samuel\u2019s work in his \u201cSteps\u201d paper, suggesting the connection to secondary reinforcement theories, both natural and arti\ufb01cial. As we have discussed, in the decade following the work of Minsky and Samuel, little computational work was done on trial-and-error learning, and apparently no computational work at all was done on temporal-di\u21b5erence learning. In 1972, Klopf brought trial-anderror learning together with an important component of temporal-di\u21b5erence learning. Klopf was interested in principles that would scale to learning in large systems, and thus was intrigued by notions of local reinforcement, whereby subcomponents of an overall learning system could reinforce one another", "e6015919-3560-4d44-a319-d7eac98bdbd8": "Shorten and Khoshgoftaar J Big Data  6:60   Image Data Augmentation techniques  The earliest demonstrations showing the effectiveness of Data Augmentations come from simple transformations such as horizontal flipping, color space augmentations, and random cropping. These transformations encode many of the invariances discussed ear- lier that present challenges to image recognition tasks. The augmentations listed in this survey are geometric transformations, color space transformations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, GAN-based augmentation, neural style transfer, and meta-learning schemes. This section will explain how each augmentation algorithm works, report experimental results, and discuss dis-  advantages of the augmentation technique.\n\nData Augmentations based on basic image manipulations  Geometric transformations  This section describes different augmentations based on geometric transformations and many other image processing functions. The class of augmentations discussed below could be characterized by their ease of implementation. Understanding these trans- formations will provide a useful base for further investigation into Data Augmentation techniques. We will also describe the different geometric augmentations in the context of their \u2018safety\u2019 of application", "0810007e-1361-4f71-88f4-324afb27626b": "Each Candidate is a tuple of Context objects, which are part of a hierarchy representing the local context of the Candidate Snorkel pipeline is to synthesize this label matrix \ufffd\u2014which may contain overlapping and con\ufb02icting labels for each data point\u2014into a single vector of probabilistic training labels \u02dcY = ( \u02dcy1, . , \u02dcym), where \u02dcyi \u2208 . These training labels can then be used to train a discriminative model. Next, we introduce the running example of a text relation extraction task as a proxy for many real-world knowledge base construction and data analysis tasks: Example 2.1 Consider the task of extracting mentions of adverse chemical\u2013disease relations from the biomedical literature (see CDR task, Sect. 4.1).\n\nGiven documents with mentions of chemicals and diseases tagged, we refer to each co-occurring (chemical, disease) mention pair as a candidate extraction, which we view as a data point to be classi\ufb01ed as either true or false. For example, in Fig", "c03aa591-48de-4dd4-a76a-4760cd72a7d1": "These di\ufb03culties notwithstanding, Abbeel and Ng  argue that the inverse reinforcement learning approach can sometimes be more e\u21b5ective than supervised learning for bene\ufb01ting from the behavior of an expert. Another approach to \ufb01nding a good reward signal is to automate the trial-and-error search for a good signal that we mentioned above. From an application perspective, the reward signal is a parameter of the learning algorithm.\n\nAs is true for other algorithm parameters, the search for a good reward signal can be automated by de\ufb01ning a space of feasible candidates and applying an optimization algorithm. The optimization algorithm evaluates each candidate reward signal by running the reinforcement learning system with that signal for some number of steps, and then scoring the overall result by a \u201chigh-level\u201d objective function intended to encode the designer\u2019s true goal, ignoring the limitations of the agent. Reward signals can even be improved via online gradient ascent, where the gradient is that of the high-level objective function . Relating this approach to the natural world, the algorithm for optimizing the high-level objective function is analogous to evolution, where the high-level objective function is an animal\u2019s evolutionary \ufb01tness determined by the number of its o\u21b5spring that survive to reproductive age", "0930fa77-cb29-4523-88b5-224d851510a6": "In this case, the methods can often \ufb01nd exact solutions, that is, they can often \ufb01nd exactly the optimal value function and the optimal policy.\n\nThis contrasts with the approximate methods described in the next part of the book, which only \ufb01nd approximate solutions, but which in return can be applied e\u21b5ectively to much larger problems. The \ufb01rst chapter of this part of the book describes solution methods for the special case of the reinforcement learning problem in which there is only a single state, called bandit problems. The second chapter describes the general problem formulation that we treat throughout the rest of the book\u2014\ufb01nite Markov decision processes\u2014and its main ideas including Bellman equations and value functions. The next three chapters describe three fundamental classes of methods for solving \ufb01nite Markov decision problems: dynamic programming, Monte Carlo methods, and temporaldi\u21b5erence learning. Each class of methods has its strengths and weaknesses. Dynamic programming methods are well developed mathematically, but require a complete and accurate model of the environment. Monte Carlo methods don\u2019t require a model and are conceptually simple, but are not well suited for step-by-step incremental computation. Finally, temporal-di\u21b5erence methods require no model and are fully incremental, but are more complex to analyze", "d6a31112-51d7-4b1a-8421-a8e4398a9360": "9.6 Structured Outputs  Convolutional networks can be used to output a high-dimensional structured object, rather than just predicting a class label for a classification task or a real value for a regression task. Typically this object is just a tensor, emitted by a standard convolutional layer. For example, the model might emit a tensor S, where S; jx is the probability that pixel (j,k) of the input to the network belongs to class i. This allows the model to label every pixel in an image and draw precise masks that follow the outlines of individual objects. One issue that often comes up is that the output plane can be smaller than the input plane, as shown in figure 9.13. In the kinds of architectures typically used for classification of a single object in an image, the greatest reduction in the spatial dimensions of the network comes from using pooling layers with large stride. To produce an output map of similar size as the input, one can avoid pooling altogether . Another strategy is to simply emit a lower-resolution grid of labels", "37312786-1237-4e90-90bb-9a6617977575": "This approach to deep learning was based on training undirected probabilistic models called restricted Boltzmann machines (RBMs) to model the input data. RBMs are described in part III.\n\nTo solve speech recognition tasks, unsupervised pretraining was used to build deep feedforward networks whose layers were each initialized by training an RBM. These networks take spectral acoustic representations in a fixed-size input window (around a center frame) and predict the conditional probabilities of HMM states for that center frame. Training such deep networks helped to significantly improve the recognition rate on TIMIT , bringing down the  https://www.deeplearningbook.org/contents/applications.html    2012b) for an analysis of reasons for the success of these models. Extensions to the asic phone recognition pipeline included the addition of speaker-adaptive features   that further reduced the error rate. This was quickly followed up by work to expand the architecture from phoneme recognition (which is what TIMIT is focused on) to large-vocabulary speech recognition , which involves not just recognizing phonemes but also recognizing sequences  poset error rate from about 26 percent to 20.7 percent. See Mohamed ef al. of words from a large vocabulary", "42fc802f-ebc2-4b84-aafd-d1337947d00a": "9.2 Motivation  Convolution leverages three important ideas that can help improve a machine learning system: sparse interactions, parameter sharing and equivariant  329  CHAPTER 9. CONVOLUTIONAL NETWORKS  Input  Output  Figure 9.1: An example of 2-D convolution without kernel flipping. We restrict the output to only positions where the kernel lies entirely within the image, called \u201cvalid\u201d convolution in some contexts. We draw boxes with arrows to indicate how the upper-left element of the output tensor is formed by applying the kernel to the corresponding upper-left region of the input tensor. https://www.deeplearningbook.org/contents/convnets.html    representations, Moreover, convolution provides a means for working with inputs of variable size. We now describe each of these ideas in turn. Traditional neural network layers use matrix multiplication by a matrix of parameters with a separate parameter describing the interaction between each input unit and each output unit.\n\nThis means that every output unit interacts with every input unit. Convolutional networks, however, typically have sparse interactions (also referred to as sparse connectivity or sparse weights)", "25270a42-e9e6-4ca0-b674-713e191fae3f": "Using this notation, the gradient on the remaining parameters is given by  dol) \\ | Vel = > ( ao VowL = Yo VowL, 10.22 t t  ah) >\u00bb | oem  2 Vol Val = Sodiag (1 \u2014 (2) ) Vw L,(10.23 t t  OL VE = od (2) Vl? => (Vow Lh, 0.24 7 los  t a 0 {) t DL wwe = E(2) own 025 t a a  2 = Yang (1 - (2) ) nye) al, 0.26  VuL = >y/( oF Vy wht? 0.27 7 ( 2 = Yang (1 - (0) ) not)\u201d, 10.28 t  We do not need to compute the gradient with respect to x for training because it does not have any parameters as ancestors in the computational graph defining the loss. 10.2.3", "d84735ed-ba9f-47df-8f5b-b8da29bdbaf5": "We therefore minimize where the parameter C > 0 controls the trade-off between the slack variable penalty and the margin. Because any point that is misclassi\ufb01ed has \u03ben > 1, it follows that \ufffd n \u03ben is an upper bound on the number of misclassi\ufb01ed points. The parameter C is therefore analogous to (the inverse of) a regularization coef\ufb01cient because it controls the trade-off between minimizing training errors and controlling model complexity. In the limit C \u2192 \u221e, we will recover the earlier support vector machine for separable data.\n\nWe now wish to minimize (7.21) subject to the constraints (7.20) together with \u03ben \u2a7e 0. The corresponding Lagrangian is given by where {an \u2a7e 0} and {\u00b5n \u2a7e 0} are Lagrange multipliers. The corresponding set of KKT conditions are given by Appendix E Using these results to eliminate w, b, and {\u03ben} from the Lagrangian, we obtain the dual Lagrangian in the form which is identical to the separable case, except that the constraints are somewhat different. To see what these constraints are, we note that an \u2a7e 0 is required because these are Lagrange multipliers", "8c82b587-bbbb-4667-8fea-979e6070f875": "From the table it can be seen that \ufb01ne-tuning is surprisingly robust to different masking strategies. However, as expected, using only the MASK strategy was problematic when applying the featurebased approach to NER. Interestingly, using only the RND strategy performs much worse than our strategy as well.", "a9548141-d11e-450c-ab42-d1f95924a725": "We also de\ufb01ned in Chapter 7 an algorithm that uni\ufb01es all action-value algorithms: n-step Q(\u03c3). We leave the semigradient form of that algorithm, and also of the n-step state-value algorithm, as exercises for the reader. Exercise 11.1 Convert the equation of n-step o\u21b5-policy TD (7.9) to semi-gradient form. Give accompanying de\ufb01nitions of the return for both the episodic and continuing cases. \u21e4 \u21e4Exercise 11.2 Convert the equations of n-step Q(\u03c3) (7.11 and 7.17) to semi-gradient form. Give de\ufb01nitions that cover both the episodic and continuing cases. \u21e4 In this section we begin to discuss the second part of the challenge of o\u21b5-policy learning with function approximation\u2014that the distribution of updates does not match the onpolicy distribution.\n\nWe describe some instructive counterexamples to o\u21b5-policy learning\u2014 cases where semi-gradient and other simple algorithms are unstable and diverge. To establish intuitions, it is best to consider \ufb01rst a very simple example", "4856a7d7-3fd7-42cf-8719-5e46ce6ab070": "Dalvi, N., Dasgupta, A., Kumar, R., Rastogi, V.: Aggregating crowdsourced binary ratings. In: International World Wide Web Conference (WWW)  12.\n\nDavis, P.A. et al. : A CTD\u2013P\ufb01zer collaboration: Manual curation of 88,000 scienti\ufb01c articles text mined for drug\u2013disease and drug\u2013 phenotype interactions. em Database  13. Dawid, A.P., Skene, A.M.: Maximum likelihood estimation of observer error-rates using the EM algorithm. J. R. Stat. Soc. C 28(1), 20\u201328  14. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  15. Dong, X.L., Srivastava, D.: Big Data Integration. Synthesis Lectures on Data Management", "47816987-8464-428d-a654-ffb70ffb0c98": "The derivatives of the feedforward network can also approximate the derivatives of the function arbitrarily well .\n\nThe concept of Borel measurability is beyond the scope of this book; for our purposes it suffices to say that any continuous function on a closed and bounded subset of R\u201d is Borel measurable and therefore may be approximated by a neural network. A neural network may also approximate any function mapping from any finite dimensional discrete space  194  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  to another. While the original theorems were first stated in terms of units with activation functions that saturate for both very negative and very positive argu- ments, universal approximation theorems have also been proved for a wider class of activation functions, which includes the now commonly used rectified linear unit . The universal approximation theorem means that regardless of what function we are trying to learn, we know that a large MLP will be able to represent this function. We are not guaranteed, however, that the training algorithm will be able to learn that function. Even if the MLP is able to represent the function, earning can fail for two different reasons", "9e1ac94e-812d-43cd-ac6c-8430f15fd078": "378  https://www.deeplearningbook.org/contents/rnn.html    CHAPTER 10, SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS. Gradients obtained by back-propagation may then be used with any general-purpose gradient-based techniques to train an RNN. To gain some intuition for how the BPTT algorithm behaves, we provide an example of how to compute gradients by BPTT for the RNN equations above (equation 10.8 and equation 10.12). The nodes of our computational graph include the parameters U, V,W, b and c as well as the sequence of nodes indexed by t for 2, h\u00ae, o and L\u00ae. For each node N we need to compute the gradient VnL recursively, based on the gradient computed at nodes that follow it in the graph. We start the recursion with the nodes immediately preceding the final loss:  OL  ane = (10.17)  In this derivation we assume that the outputs o) are used as the argument to the softmax function to obtain the vector y of probabilities over the output", "b7eb9ce4-0891-401e-8326-ff1897c9b1d1": "If several leapfrog steps are applied in succession, it can be seen that half-step updates to the momentum variables can be combined into full-step updates with step size \u03f5. The successive updates to position and momentum variables then leapfrog over each other. In order to advance the dynamics by a time interval \u03c4, we need to take \u03c4/\u03f5 steps. The error involved in the discretized approximation to the continuous time dynamics will go to zero, assuming a smooth function E(z), in the limit \u03f5 \u2192 0. However, for a nonzero \u03f5 as used in practice, some residual error will remain. We shall see in Section 11.5.2 how the effects of such errors can be eliminated in the hybrid Monte Carlo algorithm. In summary then, the Hamiltonian dynamical approach involves alternating between a series of leapfrog updates and a resampling of the momentum variables from their marginal distribution.\n\nNote that the Hamiltonian dynamics method, unlike the basic Metropolis algorithm, is able to make use of information about the gradient of the log probability distribution as well as about the distribution itself. An analogous situation is familiar from the domain of function optimization. In most cases where gradient information is available, it is highly advantageous to make use of it", "5488a103-92a0-4ef2-a6dc-d0d7f91b1a56": "They found that sensitivity to temperature was of little help. They also found that controlling the angle of attack is not helpful in staying within a particular thermal, being useful instead for traveling between thermals when covering large distances, as in cross-country gliding and bird migration. Due to the fact that soaring in di\u21b5erent levels of turbulence requires di\u21b5erent policies, training was done in conditions ranging from weak to strong turbulence. In strong turbulence the rapidly changing wind and glider velocities allowed less time for the controller to react. This reduced the amount of control possible compared to what was possible for maneuvering when \ufb02uctuations were weak. Reddy et al. examined the policies Sarsa learned under these di\u21b5erent conditions. Common to policies learned in all regimes were these features: when sensing negative wind acceleration, bank sharply in the direction of the wing with the higher lift; when sensing large positive wind acceleration and no torque, do nothing.\n\nHowever, di\u21b5erent levels of turbulence led to policy di\u21b5erences", "3f6ae810-6df3-4e1b-8c6f-2270352bfdf2": "With the autoencoder view, we have a decoder computing the reconstruction  &=g(h)=b+Vh. (13.20) The choices of linear encoder and decoder that minimize reconstruction error  Ell|a\u2014 2\\)3 (13.21)  correspond to V = W, w= b = Ea] and the columns of W form an orthonormal basis, which spans the same subspace as the principal eigenvectors of the covariance  matrix  C = E(w p)(w \u2014 p)\"). (13.22)  In the case of PCA, the columns of W are these eigenvectors, ordered by the magnitude of the corresponding eigenvalues (which are all real and non-negative). One can also show that eigenvalue \u00bb; of C corresponds to the variance of x in the direction of eigenvector vu. If z \u20ac R?\n\nand h \u20ac R\u00a2 with d < D, then the optimal reconstruction error (choosing p, b, V and W as above) is  D min Ele \u2014 2||? ]= S> Xs", "ff467f2e-a51f-421b-b860-33e3df2e1904": "(For example, suppose the agent receives a reward of +1 at each time step.) Thus, in this book we usually use a de\ufb01nition of return that is slightly more complex conceptually but much simpler mathematically. The additional concept that we need is that of discounting. According to this approach, the agent tries to select actions so that the sum of the discounted rewards it receives over the future is maximized. In particular, it chooses At to maximize the expected discounted return: where \u03b3 is a parameter, 0 \uf8ff \u03b3 \uf8ff 1, called the discount rate.\n\nThe discount rate determines the present value of future rewards: a reward received k time steps in the future is worth only \u03b3k\u22121 times what it would be worth if it were received immediately. If \u03b3 < 1, the in\ufb01nite sum in (3.8) has a \ufb01nite value as long as the reward sequence {Rk} is bounded. If \u03b3 = 0, the agent is \u201cmyopic\u201d in being concerned only with maximizing immediate rewards: its objective in this case is to learn how to choose At so as to maximize only Rt+1", "3fe6b330-505a-414a-95be-7173ffc87169": "Labeled or unlabeled examples of x allow one to learn a representation function f, and similarly with examples of y to learn fy.\n\nEach application of the f, and fy functions appears as an upward arrow, with the style of the arrows indicating which function is applied. Distance in h, space provides a similarity metric between any pair of points in x space that may be more meaningful than distance ina space. Likewise, distance inh, space provides a similarity metric between any pair of points iny space. Both of these similarity functions are indicated with dotted bidirectional arrows. Labeled examples (dashed horizontal lines) are pairs (a, y) that allow one to learn a one-way or two-way map (solid bidirectional arrow) between the representations f,,( a) and the representations f,(y) and to anchor these representations to each other. Zero-data learning is then enabled as follows", "7c8501ff-608a-4493-905f-f082215ecbc9": "The L-BFGS strategy with no storage described here can be generalized to include more information about the Hessian by storing some of the vectors used to update M at each time step, which costs only O(n) per step. orn\u201d MM 42---2 at nee Oana ete 7 A NAAR A TNH  https://www.deeplearningbook.org/contents/optimization.html    O.f UPULILIZALLULL OLFALERIES ALLU LVLELA-ALYULLULLLLLS Many optimization techniques are not exactly algorithms but rather general tem- plates that can be specialized to yield algorithms, or subroutines that can be incorporated into many different algorithms.\n\n8.7.1 Batch Normalization  Batch normalization  is one of the most exciting recent innovations in optimizing deep neural networks, and it is actually not an opti- mization algorithm at all. Instead, it is a method of adaptive reparametrization, motivated by the difficulty of training very deep models. Very deep models involve the composition of several functions, or layers. The gradient tells how to update each parameter, under the assumption that the other layers do not change. In practice, we update all the layers simultaneously", "ffb3c46f-0942-4a74-ad85-4e1f92eabe19": ", \u00b5K)T, and the parameters \u00b5k are constrained to satisfy \u00b5k \u2a7e 0 and \ufffd k \u00b5k = 1, because they represent probabilities. The distribution (2.26) can be regarded as a generalization of the Bernoulli distribution to more than two outcomes. It is easily seen that the distribution is normalized Now consider a data set D of N independent observations x1, . , xN. The corresponding likelihood function takes the form which represent the number of observations of xk = 1. These are called the suf\ufb01cient statistics for this distribution. Section 2.4 In order to \ufb01nd the maximum likelihood solution for \u00b5, we need to maximize ln p(D|\u00b5) with respect to \u00b5k taking account of the constraint that the \u00b5k must sum to one.\n\nThis can be achieved using a Lagrange multiplier \u03bb and maximizing Appendix E Setting the derivative of (2.31) with respect to \u00b5k to zero, we obtain We can solve for the Lagrange multiplier \u03bb by substituting (2.32) into the constraint \ufffd which is the fraction of the N observations for which xk = 1. We can consider the joint distribution of the quantities m1,", "62334984-9fc3-4db4-b7cf-d9b5c4fbee00": "This policy iteration process works and always converges to the optimality, but why this is the case? Say, we have a policy 7 and then generate an improved version 7\u2019 by greedily taking actions, m'(s) = argmax,<,4 Q,,(, a).\n\nThe value of this improved 7\u2019 is guaranteed to be better because:  Q,(s,7'(s)) = @,(s, argmax Q,(s, 4) \u2014 max Q,(s, a) = Q;(s, 7(s)) = V,(s)  Monte-Carlo Methods  First, let's recall that V(s) = E. Monte-Carlo (MC) methods uses a simple idea: It learns from episodes of raw experience without modeling the environmental dynamics and  computes the observed mean return as an approximation of the expected return. To compute the  empirical return G;, MC methods need to learn from complete episodes S$), A), Ry,..., S57 to compute G, = = Recker and all the episodes must eventually terminate", "e2c329b4-a970-4a6f-a21b-5a3475036a1b": "The performance boost found from mixing images is very difficult to understand or explain. One possible explanation for this is that the increased dataset size results in more robust representations of low-level characteristics such as lines and edges. Testing the performance of this in comparisons to transfer learning and pretrain- ing methods is an interesting area for future work. Transfer learning and pretraining are  other techniques that learn low-level characteristics in CNNs. Additionally, it will be Shorten and Khoshgoftaar J Big Data  6:60   < 2 8 = B 3 rs z \u00a3  input image | Random Erasing Fig. 11 Example of random erasing on image recognition tasks   interesting to see how the performance changes if we partition the training data such that the first 100 epochs are trained with original and mixed images and the last 50 with original images only. These kinds of strategies are discussed further in Design Consid- erations of Data Augmentation with respect to curriculum learning .\n\nAdditionally, the paper will cover a meta-learning technique developed by Lemley et al. that uses  a neural network to learn an optimal mixing of images. Random erasing  Random erasing  is another interesting Data Augmentation technique developed by Zhong et al", "2ba2785a-d298-4146-92d4-0eb754454207": "Xinyue Z, Yifan L, Zengchang Q, Jiahong L. Emotion classification with data augmentation using generative adver- sarial networks. CoRR, vol. abs/1711.00648. 2017. Goodfellow UJ, Erhan D, Carrier PL, Courville A, Mirza M, Hamner B, Cukierski W, Tang Y, Thaler D, Lee DH, et al. Chal- lenges in representation learning: A report on three machine learning contests. In: NIPS. Berlin: Springer; 2013 p.117-24  Mehdi M, Simon O. Conditional generative adversarial nets. arXiv preprint. 2014. Mario L, Karol K, Marcin M, Olivier B, Sylvain G. Are GANs created equal? A large-scale study. arXiv preprint. 2018. Shorten and Khoshgoftaar J Big Data  6:60   129. 135. 136", "2f84b89a-9e1f-4e41-b11d-3ab66c95743a": "468  CHAPTER 12. APPLICATIONS  Output object (English sentence)  Decoder  Intermediate, semantic representation  Encoder  Source object (French sentence or image)  Figure 12.5: The encoder-decoder architecture to map back and forth between a surface  representation (such as a sequence of words or an image) and a semantic representation. By using the output of an encoder of data from one modality (such as the encoder mapping from French sentences to hidden representations capturing the meaning of sentences) as the input to a decoder for another modality (such as the decoder mapping from hidden representations capturing the meaning of sentences to English), we can train systems that translate from one modality to another.\n\nThis idea has been applied successfully not just  https://www.deeplearningbook.org/contents/applications.html      to machine translation but also to caption generation from images. A drawback of the MLP-based approach is that it requires the sequences to be preprocessed to be of fixed length. To make the translation more flexible, we would like to use a model that can accommodate variable length inputs and variable length outputs. An RNN provides this ability", "97deb17a-1a0e-4ccd-bda1-037a86e5cf2c": "This held true even when the Monte Carlo approximation was allowed to sample up to 1,000 subnetworks. Gal and Ghahramani  found that some models obtain better classification accuracy using twenty samples and the Monte Carlo approximation. It appears that the optimal choice of inference approximation is problem dependent. Srivastava et al. showed that dropout is more effective than other standard computationally inexpensive regularizers, such as weight decay, filter  261  https://www.deeplearningbook.org/contents/regularization.html    CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  norm constraints, and sparse activity regularization. Dropout may also be combined with other forms of regularization to yield a further improvement. One advantage of dropout is that it is very computationally cheap. Using dropout during training requires only O(n) computation per example per update, to generate n random binary numbers and multiply them by the state. Depending on the implementation, it may also require O(n) memory to store these binary numbers until the back-propagation stage", "701336f0-2ae7-41f7-8dab-066c91671661": "OPTIMIZATION FOR TRAINING DEEP MODELS  In some cases, a surrogate loss function actually results in being able to learn more. For example, the test set 0-1 loss often continues to decrease for a long time after the training set 0-1 loss has reached zero, when training using the log-likelihood surrogate.\n\nThis is because even when the expected 0-1 loss is zero, one can improve the robustness of the classifier by further pushing the classes apart from each other, obtaining a more confident and reliable classifier, thus extracting more information from the training data than would have been possible by simply minimizing the average 0-1 loss on the training set. A very important difference between optimization in general and optimization as we use it for training algorithms is that training algorithms do not usually halt at a local minimum. Instead, a machine learning algorithm usually minimizes a surrogate loss function but halts when a convergence criterion based on early stopping (section 7.8) is satisfied. Typically the early stopping criterion is based on the true underlying loss function, such as 0-1 loss measured on a validation set, and is designed to cause the algorithm to halt whenever overfitting begins to occur", "0b32aaa1-fd6c-40f0-9f24-7daf832e8889": "We \ufb01rst remove this factor from the approximating distribution to give and we then multiply this by the exact factor fb(x2, x3) to give \ufffdp(x) = q\\b(x)fb(x2, x3) = \ufffdfa1(x1)\ufffdfa2(x2)\ufffdfc2(x2)\ufffdfc4(x4)fb(x2, x3). (10.229) We now \ufb01nd qnew(x) by minimizing the Kullback-Leibler divergence KL(\ufffdp\u2225qnew). The result, as noted above, is that qnew(z) comprises the product of factors, one for each variable xi, in which each factor is given by the corresponding marginal of \ufffdp(x). These four marginals are given by and qnew(x) is obtained by multiplying these marginals together", "07321723-3b50-4006-9189-fe853271f130": "For example, in Q-learning the target policy is the greedy policy given the current action values, which is de\ufb01ned with a max, and in Sarsa the policy is often \"-greedy, which also involves a maximization operation. In these algorithms, a maximum over estimated values is used implicitly as an estimate of the maximum value, which can lead to a signi\ufb01cant positive bias. To see why, consider a single state s where there are many actions a whose true values, q(s, a), are all zero but whose estimated values, Q(s, a), are uncertain and thus distributed some above and some below zero. The maximum of the true values is zero, but the maximum of the estimates is positive, a positive bias.\n\nWe call this maximization bias. mistake. Nevertheless, our control methods may favor left because of maximization bias making B appear to have a positive value. Figure 6.5 shows that Q-learning with \"-greedy action selection initially learns to strongly favor the left action on this example. Even at asymptote, Q-learning takes the left action about 5% more often than is optimal at our parameter settings (\" = 0.1, \u21b5 = 0.1, and \u03b3 = 1)", "0ec262c0-37cc-48a0-a675-77c6ef651722": "10.30 (\u22c6) By evaluating the second derivative, show that the log logistic function f(x) = \u2212 ln(1 + e\u2212x) is concave.\n\nDerive the variational upper bound (10.137) directly by making a second order Taylor expansion of the log logistic function around a point x = \u03be. 10.31 (\u22c6 \u22c6) By \ufb01nding the second derivative with respect to x, show that the function f(x) = \u2212 ln(ex/2 + e\u2212x/2) is a concave function of x. Now consider the second derivatives with respect to the variable x2 and hence show that it is a convex function of x2. Plot graphs of f(x) against x and against x2. Derive the lower bound (10.144) on the logistic sigmoid function directly by making a \ufb01rst order Taylor series expansion of the function f(x) in the variable x2 centred on the value \u03be2. 10.32 (\u22c6 \u22c6) www Consider the variational treatment of logistic regression with sequential learning in which data points are arriving one at a time and each must be processed and discarded before the next data point arrives", "69f405cf-45d4-4b92-b5ea-c4b4eb90ca28": "SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  Without clipping With clipping = = 8 3 5 SN Ww Ww b b  Figure 10.17: Example of the effect of gradient clipping in a recurrent network with two parameters w and b. Gradient clipping can make gradient descent perform more reasonably in the vicinity of extremely steep cliffs. These steep cliffs commonly occur in recurrent networks near where a recurrent network behaves approximately linearly.\n\nThe cliff is exponentially steep in the number of time steps because the weight matrix is multiplied by itself once for each time step. (Left) Gradient descent without gradient clipping overshoots the bottom of this small ravine, then receives a very large gradient from the cliff face. The large gradient catastrophically propels the parameters outside the axes of the plot. (Right) Gradient descent with gradient clipping has a more moderate reaction to the cliff. While it does ascend the cliff face, the step size is restricted so that it cannot be propelled away from the steep region near the solution. Figure adapted with permission from Pascanu et al", "9b30ecf7-6e63-471f-9435-ee1d1ffd9260": "https://www.deeplearningbook.org/contents/convnets.html    366  https://www.deeplearningbook.org/contents/convnets.html", "5b347eae-28df-4cce-9fd9-b483a568f68c": "We can see now that there is no way around this.\n\nMinimizing the BE requires some such access to the nominal, underlying MDP. This is an important limitation of the BE beyond that identi\ufb01ed in the A-presplit example on page 273. All this directs more attention toward the PBE. We now consider SGD methods for minimizing the PBE. As true SGD methods, these Gradient-TD methods have robust convergence properties even under o\u21b5-policy training and nonlinear function approximation. Remember that in the linear case there is always an exact solution, the TD \ufb01xed point wTD, at which the PBE is zero. This solution could be found by least-squares methods (Section 9.8), but only by methods of quadratic O(d2) complexity in the number of parameters. We seek instead an SGD method, which should be O(d) and have robust convergence properties. Gradient-TD methods come close to achieving these goals, at the cost of a rough doubling of computational complexity", "742f8a5f-b650-437b-b64e-4e153de29b4e": "The end goal in Snorkel is to train a model that generalizes beyond the information expressed in the labeling functions.\n\nWe train a discriminative model h\u03b8 on our probabilistic labels \u02dcY by minimizing a noise-aware variant of the loss l(h\u03b8(xi), y), i.e., the expected loss with respect to \u02dcY: A formal analysis shows that as we increase the amount of unlabeled data, the generalization error of discriminative models trained with Snorkel will decrease at the same asymptotic rate as traditional supervised learning models do with additional hand-labeled data , allowing us to increase predictive performance by adding more unlabeled data. Intuitively, this property holds because as more data is provided, the discriminative model sees more features that co-occur with the heuristics encoded in the labeling functions. Example 2.5 The CDR data contains the sentence, \u201cMyasthenia gravis presenting as weakness after magnesium administration.\u201d None of the 33 labeling functions we developed vote on the corresponding Causes(magnesium, myasthenia gravis) candidate, i.e., they all abstain", "99bd0456-40ca-47b4-974e-c92058cc86cd": "Reinforcement learning (RL) (Sutton and Barto, 2018) offers an alternative principled framework for learning from arbitrary reward functions. However, RL by far has made limited success for training text generation, primarily due to the key challenges of sparse reward (i.e., a single reward signal is received only after the whole text sequence is generated) and large action space (i.e., a vocabulary of millions of words).\n\nFor instance, a popular family of RL algorithms studied extensively for text generation is the policy-based  or actor-critic based  algorithms, with policy gradient (PG) being the most prevalent example . Those algorithms train the model with on-policy updates, i.e., the text samples used for estimating policy gradients are from the target model itself. Due to the exponentially large space of sequences, on-policy updates often suffer from extremely high variance and low data ef\ufb01ciency (e.g., most model samples are not useful for learning). Thus directly training with PG from scratch is usually impossible. In practice, the model has to be initialized by MLE training, followed by PG as \ufb01netuning, which often leads to limited improvement . Another set of work has resorted to off-policy RL", "16f82bf4-a668-4ca4-88b2-46fec1536b9c": "propose an approximation in which we consider the back-propagated vectors V1) L as if they were constants (for the purpose of this regularizer, so that there is no need to back-propagate through them). The experiments with this regularizer suggest that, if combined with the norm clipping heuristic (which handles gradient explosion), the regularizer can considerably increase the span of the dependencies that an RNN can learn. Because it keeps the RNN dynamics on the edge of explosive gradients, the gradient clipping is particularly important. Without gradient clipping, gradient explosion prevents learning from succeeding. A key weakness of this approach is that it is not as effective as the LSTM for tasks where data is abundant, such as language modeling. 411  CHAPTER 10.\n\nSEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  10.12 Explicit Memory  Intelligence requires knowledge, and acquiring knowledge can be done via learning, which has motivated the development of large-scale deep architectures. However, there are different kinds of knowledge. Some knowledge can be implicit, sub- conscious, and difficult to verbalize\u2014such as how to walk, or how a dog looks different from a cat", "93cc7619-b002-4ac4-9e8e-751c045f46b8": "Although it has been a common belief that dopamine neurons do act together like this, modern evidence is pointing to the more complicated picture that di\u21b5erent subpopulations of dopamine neurons respond to input di\u21b5erently depending on the structures to which they send their signals and the di\u21b5erent ways these signals act on their target structures. Dopamine has functions other than signaling RPEs, and even for dopamine neurons that do signal RPEs, it can make sense to send di\u21b5erent RPEs to di\u21b5erent structures depending on the roles these structures play in producing reinforced behavior", "4e73493c-2ee0-482b-aa25-12a1867cdd9a": "Similarly in speech recognition, small levels of nonlinear warping along the time axis, which preserve temporal ordering, should not change the interpretation of the signal. If suf\ufb01ciently large numbers of training patterns are available, then an adaptive model such as a neural network can learn the invariance, at least approximately. This involves including within the training set a suf\ufb01ciently large number of examples of the effects of the various transformations. Thus, for translation invariance in an image, the training set should include examples of objects at many different positions. This approach may be impractical, however, if the number of training examples is limited, or if there are several invariants (because the number of combinations of transformations grows exponentially with the number of such transformations). We therefore seek alternative approaches for encouraging an adaptive model to exhibit the required invariances. These can broadly be divided into four categories: 1.\n\nThe training set is augmented using replicas of the training patterns, transformed according to the desired invariances. For instance, in our digit recognition example, we could make multiple copies of each example in which the digit is shifted to a different position in each image. 2", "c471d892-eb96-4764-b55b-7c6473c32d1c": "From this point of view, training a neural network is not much different from training any other model. Computing the gradient is slightly more complicated for a neural network but can still be done efficiently and exactly. In Section 6.5 we describe how to obtain the gradient using the back-propagation algorithm and modern generalizations of the back-propagation algorithm. As with other machine learning models, to apply gradient-based learning we must choose a cost function, and we must choose how to represent the output of the model.\n\nWe now revisit these design considerations with special emphasis on the neural networks scenario. 6.2.1 Cost Functions  An important aspect of the design of a deep neural network is the choice of the cost function. Fortunately, the cost functions for neural networks are more or less  173  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  the same as those for other parametric models, such as linear models. In most cases, our parametric model defines a distribution p( 30) and we simply use the principle of maximum likelihood", "6679f71d-6cfe-42e3-a843-366ab4673122": "This is done by extending the data-instance experience function (Equation 4.2), in particular by enriching the similarity metric in di\ufb00erent ways. The discussion here generally applies to data instance t of any structures, for example, t = (x, y) or t = x. Data reweighting. Rather than assuming the same importance of all data instances, we can associate each instance t\u2217 with an importance weight w(t\u2217) \u2208 R, so that the learning pays more attention to those high-quality instances, while low-quality ones (e.g., with noisy labels) are downplayed.\n\nThis can be done by scaling the above 0/1 indicator function (e.g., Equation 4.2) with the weight (Figure 2): Plugging fdata-w into the SE (Equation 3.2) with the same other speci\ufb01cation of supervised MLE (\u03b1 = 1, \u03b2 = \u03f5), we get the update rule of model parameters \u03b8 in the student-step (Equation 3.3): which is the familiar weighted supervised MLE. The weights w can be speci\ufb01ed a priori based on heuristics, for example, using inverse class frequency", "e7fb9733-66d7-4ea8-9859-68f289d6a3e9": "This policy is built by the planning process while the agent is still wandering near the start state. By the end of the third episode a complete optimal policy will have been found and perfect performance attained. In Dyna-Q, learning and planning are accomplished by exactly the same algorithm, operating on real experience for learning and on simulated experience for planning. Because planning proceeds incrementally, it is trivial to intermix planning and acting. Both proceed as fast as they can. The agent is always reactive and always deliberative, responding instantly to the latest sensory information and yet always planning in the background. Also ongoing in the background is the model-learning process. As new information is gained, the model is updated to better match reality. As the model changes, the ongoing planning process will gradually compute a di\u21b5erent way of behaving to match the new model.\n\nExercise 8.1 The nonplanning method looks particularly poor in Figure 8.3 because it is a one-step method; a method using multi-step bootstrapping would do better. Do you think one of the multi-step bootstrapping methods from Chapter 7 could do as well as the Dyna method? Explain why or why not", "a74c57a5-8567-401a-a675-c93f73829a9b": "e The homework is worth 0% credit 48 hours after exceeding the late day limit. You must turn in all assignments, even if for zero credit, in order to pass the course. http://zhiting.ucsd.edu/teaching/dsc291winter2023/logistics.html  DSC 291 Machine Learning with Few Labels   Paper Presentation  Each student will give an oral presentation on a research paper that is broadly related to machine learning. Each talk is 10 mins in length, including 8 mins for presentation and 2 mins for QA/discussion. Each student will need to sign up for the presentation in a google sheet (TBA).\n\nCourse Project  The course project will be carried out in groups of 3 or 4 people, and has four main parts: a proposal, a midway report, a final report, and an oral presentation. Please see the project page for more information about the final project. Machine Learning with Few Labels  \u00a9 Copyright 2023 UC San Diego. Powered by Jeky!! with al-folio theme. http://zhiting.ucsd.edu/teaching/dsc291winter2023/logistics.html", "bbfad7e3-4092-4a25-915f-88aec7cb367d": "5.19 (\u22c6) www Derive the expression (5.85) for the outer product approximation to the Hessian matrix for a network having a single output with a logistic sigmoid output-unit activation function and a cross-entropy error function, corresponding to the result (5.84) for the sum-of-squares error function.\n\n5.20 (\u22c6) Derive an expression for the outer product approximation to the Hessian matrix for a network having K outputs with a softmax output-unit activation function and a cross-entropy error function, corresponding to the result (5.84) for the sum-ofsquares error function. 5.21 (\u22c6 \u22c6 \u22c6) Extend the expression (5.86) for the outer product approximation of the Hessian matrix to the case of K > 1 output units. Hence, derive a recursive expression analogous to (5.87) for incrementing the number N of patterns and a similar expression for incrementing the number K of outputs. Use these results, together with the identity (5.88), to \ufb01nd sequential update expressions analogous to (5.89) for \ufb01nding the inverse of the Hessian by incrementally including both extra patterns and extra outputs", "d9bb1ff0-d04b-47eb-92f2-d4a471de9f56": "These results suggest that the hippocampus is critical for the statetransition part of an animal\u2019s environment model, and that it is part of a system that uses the model to simulate possible future state sequences to assess the consequences of possible courses of action: a form of planning. The results described above add to a voluminous literature on neural mechanisms underlying goal-directed, or model-based, learning and decision making, but many questions remain unanswered. For example, how can areas as structurally similar as the DLS and DMS be essential components of modes of learning and behavior that are as di\u21b5erent as model-free and model-based algorithms? Are separate structures responsible for (what we call) the transition and reward components of an environment model?\n\nIs all planning conducted at decision time via simulations of possible future courses of action as the forward sweeping activity in the hippocampus suggests? In other words, is all planning something like a rollout algorithm (Section 8.10)? Or are models sometimes engaged in the background to re\ufb01ne or recompute value information as illustrated by the Dyna architecture (Section 8.2)? How does the brain arbitrate between the use of the habit and goal-directed systems? Is there, in fact, a clear separation between the neural The evidence is not pointing to a positive answer to this last question", "69a0fa07-4307-43ec-b7f6-018db58a8570": "How can each reinforcement learning agent in a team learn to \u201cdo the right thing\u201d so that the collective action of the team is highly rewarded? An interesting result is that if each agent can learn e\u21b5ectively despite its reward signal being corrupted by a large amount of noise, and despite its lack of access to complete state information, then the population as a whole will learn to produce collective actions that improve as evaluated by the common reward signal, even when the agents cannot communicate with one another. Each agent faces its own reinforcement learning task in which its in\ufb02uence on the reward signal is deeply buried in the noise created by the in\ufb02uences of other agents.\n\nIn fact, for any agent, all the other agents are part of its environment because its input, both the part conveying state information and the reward part, depends on how all the other agents are behaving. Furthermore, lacking access to the actions of the other agents, indeed lacking access to the parameters determining their policies, each agent can only partially observe the state of its environment. This makes each team member\u2019s learning task very di\ufb03cult, but if each uses a reinforcement learning algorithm able to increase a reward signal even under these di\ufb03cult conditions, teams of reinforcement learning agents can learn to produce collective actions that improve over time as evaluated by the team\u2019s common reward signal", "7a56bcc8-8880-4d06-b437-a4e14722c345": "Thus the density takes the same form in the new variable as in the original one, and so the density is independent of the choice of origin.\n\nWe would like to choose a prior distribution that re\ufb02ects this translation invariance property, and so we choose a prior that assigns equal probability mass to an interval A \u2a7d \u00b5 \u2a7d B as to the shifted interval A \u2212 c \u2a7d \u00b5 \u2a7d B \u2212 c. This implies \ufffd B and because this must hold for all choices of A and B, we have which implies that p(\u00b5) is constant. An example of a location parameter would be the mean \u00b5 of a Gaussian distribution. As we have seen, the conjugate prior distribution for \u00b5 in this case is a Gaussian p(\u00b5|\u00b50, \u03c32 0) = N(\u00b5|\u00b50, \u03c32 0), and we obtain a noninformative prior by taking the limit \u03c32 0 \u2192 \u221e. Indeed, from (2.141) and (2.142) we see that this gives a posterior distribution over \u00b5 in which the contributions from the prior vanish. As a second example, consider a density of the form where \u03c3 > 0", "d6871485-4134-4a67-8081-62a4a31c22af": "382  CHAPTER 10.\n\nSEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  eee Se  rea  N  \u2018lus  Figure 10.8: Introducing the state variable in the graphical model of the RNN, even though it is a deterministic function of its inputs, helps to see how we can obtain a very efficient parametrization, based on equation 10.5. Every stage in the sequence (forh and y) involves the same structure (the same number of inputs for each node) and can share the same parameters with the other stages. hidden units in the graphical model reveals that the RNN provides an efficient parametrization of the joint distribution over the observations. Suppose that we represented an arbitrary joint distribution over discrete values with a tabular representation\u2014an array containing a separate entry for each possible assignment of values, with the value of that entry giving the probability of that assignment occurring. If y can take on k different values, the tabular representation would have O(k7) parameters. By comparison, because of parameter sharing, the number of parameters in the RNN is O(1) as a function of sequence length. The number of parameters in the RNN may be adjusted to control model capacity but is not forced to scale with sequence length", "201a163c-ee6e-4629-82a4-cd034f845fd6": ", D that satisfy Because this basis is complete, each data point can be represented exactly by a linear combination of the basis vectors where the coefficients ani will be different for different data points. This simply corresponds to a rotation of the coordinate system to a new system defined by the {Ui}, and the original D components {Xnl' ... , XnD} are replaced by an equivalent set {anl' ... ,anD}. Taking the inner product with Uj, and making use of the orthonormality property, we obtain anj = x;Uj, and so without loss of generality we can write D Our goal, however, is to approximate this data point using a representation involving a restricted number M < D of variables corresponding to a projection onto a lower-dimensional subspace. The M -dimensional linear subspace can be represented, without loss of generality, by the first M of the basis vectors, and so we approximate each data point X n by where the {Zni} depend on the particular data point, whereas the {bd are constants that are the same for all data points", "1ea9e24d-29f1-48c2-93dc-d5bac1fe7dbf": "APPLICATIONS  https://www.deeplearningbook.org/contents/applications.html    VUYS PLOUUCL, USECL SPCLLUS SULLIC GINOULLL UL LLOLE ULL LUE PLUUUCL, USCL SPCLLUS time visiting a page for the product, and so fort n). _ This often ends up being either a regression problem (predicting some conditional expected value) or a probabilistic classification pro jlem (predicting the conditional probability of some discrete event). The early work on recommender systems relied on minimal information as inputs for these predictions: the user ID and the item ID. In this context, the only way to generalize is to rely on the similarity between the patterns of values of the target variable for different users or for different items. Suppose that user 1 and user 2 both like items A, B and C. From this, we may infer that user 1 and user 2 have similar tastes. If user 1 likes item D, then this should be a strong cue that user 2 will also like D. Algorithms based on this principle come under the name of collaborative filtering", "8997739f-89a8-4a17-8fd0-bc210dfe3253": "This is an example of a Hebbian learning rule  often summarized with the mnemonic \u201cfire together, wire together.\u201d Hebbian learning rules are among the oldest hypothesized explanations for learning in biological systems and remain relevant today . Other learning algorithms that use more information than local statistics seem to require us to hypothesize the existence of more machinery than this. For example, for the brain to implement back-propagation in a multilayer perceptron, it seems necessary for the brain to maintain a secondary communication network for transmitting gradient information backward through the network. Proposals for biologically plausible implementations (and approximations) of back-propagation have been made  but remain to be validated, and Bengio  links back-propagation of gradients to inference in energy-based models similar to the Boltzmann machine (but with continuous latent variables).\n\nThe negative phase of Boltzmann machine learning is somewhat harder to explain from a biological point of view. As argued in section 18.2, dream sleep may be a form of negative phase sampling. This idea is more speculative though. 20.2", "dd923d8d-d935-4cb1-bf4b-ba5176b66d7e": "Nonetheless, they are useful learning algorithms when computational resources are constrained. We can also build intuition for more sophisticated learning algorithms by thinking about the similarities and differences between sophisticated algorithms and k-nearest neighbors or decision tree baselines. See Murphy , Bishop , Hastie et al. or other machine learning textbooks for more material on traditional supervised learning algorithms. 5.8 Unsupervised Learning Algorithms  Recall from section 5.1.3 that unsupervised algorithms are those that experience only \u201cfeatures\u201d but not a supervision signal. The distinction between supervised and unsupervised algorithms is not formally and rigidly defined because there is no objective test for distinguishing whether a value is a feature or a target provided by a supervisor. Informally, unsupervised learning refers to most attempts to extract information from a distribution that do not require human labor to annotate  142 CHAPTER 5", "7bfcf17c-cdbe-4f39-bfd3-c24fc3408ef5": "If we train a neural network on a new classification task and it achieves 5 percent test error, we have no straightforward way of knowing if this is the expected behavior or suboptimal behavior. A further difficulty is that most machine learning models have multiple parts that are each adaptive. If one part is broken, the other parts can adapt and still  https://www.deeplearningbook.org/contents/guidelines.html    achieve roughly acceptable performance. For example suppose that we are training a neural net with several layers parametrized by weights and biases 6.\n\nSuppose further that we have manually implemented the gradient descent rule for each  parameter separately, and we made an error in the update for the biases: b+ b-a, (11.4)  where ais the learning rate. This erroneous update does not use the gradient at all. It causes the biases to constantly become negative throughout learning, which  431  CHAPTER 11. PRACTICAL METHODOLOGY  is clearly not a correct implementation of any reasonable learning algorithm. The bug may not be apparent just from examining the output of the model though. Depending on the distribution of the input, the weights may be able to adapt to compensate for the negative biases", "24a8ee7e-2a24-4258-b282-046988074869": "Each such conditional distribution will be conditioned only on the parents of the corresponding node in the graph.\n\nFor instance, x5 will be conditioned on x1 and x3. The joint distribution of all 7 variables The reader should take a moment to study carefully the correspondence between (8.4) and Figure 8.2. We can now state in general terms the relationship between a given directed graph and the corresponding distribution over the variables. The joint distribution de\ufb01ned by a graph is given by the product, over all of the nodes of the graph, of a conditional distribution for each node conditioned on the variables corresponding to the parents of that node in the graph. Thus, for a graph with K nodes, the joint distribution is given by where pak denotes the set of parents of xk, and x = {x1, . , xK}. This key equation expresses the factorization properties of the joint distribution for a directed graphical model. Although we have considered each node to correspond to a single variable, we can equally well associate sets of variables and vector-valued variables with the nodes of a graph", "7fdcd7ef-2713-4187-ad69-013ba4a2af27": "2.\n\nAnalogous to the Gaussian example, for any \u201dlocation-scale\u201d family of distributions we can choose the standard distribution (with location = 0, scale = 1) as the auxiliary variable \u03f5, and let g(.) = location + scale \u00b7 \u03f5. Examples: Laplace, Elliptical, Student\u2019s t, Logistic, Uniform, Triangular and Gaussian distributions. 3. Composition: It is often possible to express random variables as different transformations of auxiliary variables. Examples: Log-Normal (exponentiation of normally distributed variable), Gamma (a sum over exponentially distributed variables), Dirichlet (weighted sum of Gamma variates), Beta, Chi-Squared, and F distributions. When all three approaches fail, good approximations to the inverse CDF exist requiring computations with time complexity comparable to the PDF (see e.g. for some methods)", "9551d6f7-0d74-40ff-9917-02a3f257740c": "Unlike the bacterium, however, the neuron\u2019s synaptic strengths retain information about its past trial-and-error behavior. If this view of the behavior of a neuron (or just one type of neuron) is plausible, then the closed-loop nature of how the neuron interacts with its environment is important for understanding its behavior, where the neuron\u2019s environment consists of the rest of the animal together with the environment with which the animal as a whole interacts. Klopf\u2019s hedonistic neuron hypothesis extended beyond the idea that individual neurons are reinforcement learning agents. He argued that many aspects of intelligent behavior can be understood as the result of the collective behavior of a population of self-interested hedonistic neurons interacting with one another in an immense society or economic system making up an animal\u2019s nervous system. Whether or not this view of nervous systems is useful, the collective behavior of reinforcement learning agents has implications for neuroscience. We take up this subject next", "7185a808-6eec-4cfd-aeb9-9317d67baabc": "9.7 (\u22c6) www Verify that maximization of the complete-data log likelihood (9.36) for a Gaussian mixture model leads to the result that the means and covariances of each component are \ufb01tted independently to the corresponding group of data points, and the mixing coef\ufb01cients are given by the fractions of points in each group. 9.8 (\u22c6) www Show that if we maximize (9.40) with respect to \u00b5k while keeping the responsibilities \u03b3(znk) \ufb01xed, we obtain the closed form solution given by (9.17). and suppose that we partition the vector x into two parts so that x = (xa, xb). Show that the conditional density p(xb|xa) is itself a mixture distribution and \ufb01nd expressions for the mixing coef\ufb01cients and for the component densities. 9.11 (\u22c6) In Section 9.3.2, we obtained a relationship between K means and EM for Gaussian mixtures by considering a mixture model in which all components have covariance \u03f5I.\n\nShow that in the limit \u03f5 \u2192 0, maximizing the expected completedata log likelihood for this model, given by (9.40), is equivalent to minimizing the distortion measure J for the K-means algorithm given by (9.1)", "80d20b01-cc61-42a1-91c7-6fde72fdfdc4": "We now consider the problem of maximizing f(x) subject to an inequality constraint of the form g(x) \u2a7e 0, as illustrated in Figure E.3.\n\nThere are now two kinds of solution possible, according to whether the constrained stationary point lies in the region where g(x) > 0, in which case the constraint is inactive, or whether it lies on the boundary g(x) = 0, in which case the constraint is said to be active. In the former case, the function g(x) plays no role and so the stationary condition is simply \u2207f(x) = 0. This again corresponds to a stationary point of the Lagrange function (E.4) but this time with \u03bb = 0. The latter case, where the solution lies on the boundary, is analogous to the equality constraint discussed previously and corresponds to a stationary point of the Lagrange function (E.4) with \u03bb \u0338= 0. Now, however, the sign of the Lagrange multiplier is crucial, because the function f(x) will only be at a maximum if its gradient is oriented away from the region g(x) > 0, as illustrated in Figure E.3", "8e262d70-e17e-4777-8b0b-a2889c008a1b": "REGULARIZATION FOR DEEP LEARNING  >@) Cr@r\u00ae) \u00a9)  pshared  https://www.deeplearningbook.org/contents/regularization.html    Figure 7.2: Multitask learning can be cast in several ways in deep learning frameworks, and this figure illustrates the common situation where the tasks share a common input but involve different target random variables. The lower layers of a deep network (whether it is supervised and feedforward or includes a generative component with downward arrows)  can be shared across such tasks, while task-specific parameters (associated respectively with the weights into and from h\u2122 and h)) can be learned on top of those yielding a shared representation h&\"\u00b0Y, The underlying assumption is that there exists a common pool of factors that explain the variations in the input x, while each task is associated with a subset of these factors.\n\nIn this example, it is additionally assumed that top-level hidden units h and A) are specialized to each task (respectively predicting y and y \u2018)), while some intermediate-level representation h \"*\"\u00b09) is shared across all tasks", "70f27561-062c-4e40-87a1-748f23cb39b5": "A popular choice is the squared loss function, for which the optimal prediction is given by the conditional expectation, which we denote by h(x) and which is given by At this point, it is worth distinguishing between the squared loss function arising from decision theory and the sum-of-squares error function that arose in the maximum likelihood estimation of model parameters. We might use more sophisticated techniques than least squares, for example regularization or a fully Bayesian approach, to determine the conditional distribution p(t|x). These can all be combined with the squared loss function for the purpose of making predictions.\n\nWe showed in Section 1.5.5 that the expected squared loss can be written in the form Recall that the second term, which is independent of y(x), arises from the intrinsic noise on the data and represents the minimum achievable value of the expected loss. The \ufb01rst term depends on our choice for the function y(x), and we will seek a solution for y(x) which makes this term a minimum. Because it is nonnegative, the smallest that we can hope to make this term is zero", "fed9c065-86d7-44ed-9812-fd0eae720683": "The max-sum algorithm is more complex because the messages are functions of node variables x and hence comprise a set of K values for each possible state of x. Unlike max-sum, however, ICM is not guaranteed to \ufb01nd a global maximum even for tree-structured graphs. The sum-product and max-sum algorithms provide ef\ufb01cient and exact solutions to inference problems in tree-structured graphs. For many practical applications, however, we have to deal with graphs having loops. The message passing framework can be generalized to arbitrary graph topologies, giving an exact inference procedure known as the junction tree algorithm . Here we give a brief outline of the key steps involved. This is not intended to convey a detailed understanding of the algorithm, but rather to give a \ufb02avour of the various stages involved. If the starting point is a directed graph, it is \ufb01rst converted to an undirected graph by moralization, whereas if starting from an undirected graph this step is not required", "b62ad49f-5557-4bef-9ac3-e8c4215bbf00": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1455\u20131460, Hong Kong, China.\n\nAssociation for Computational Linguistics. Javid Ebrahimi, Daniel Lowd, and Dejing Dou. 2018a. On adversarial examples for character-level neural machine translation. Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. 2018b. Hot\ufb02ip: White-box adversarial examples for text classi\ufb01cation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 31\u201336. Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. 2018. Understanding back-translation at scale. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 489\u2013500, Brussels, Belgium. Association for Computational Linguistics. Marzieh Fadaee, Arianna Bisazza, and Christof Monz. 2017", "0689da87-6c32-47b8-9f11-18a563c13c0f": "Various works on GAN exten- sions such as DCGANs, CycleGANs and Progressively-Growing GANs  were pub- lished in 2015, 2017, and 2017, respectively. Neural Style Transfer was sped up with the development of Perceptual Losses by Johnson et al. in 2016. Applying meta- learning concepts from NAS to Data Augmentation has become increasingly popular with works such as Neural Augmentation , Smart Augmentation , and Auto- Augment  published in 2017, 2017, and 2018, respectively. Applying Deep Learning to medical imaging has been a popular application for CNNs since they became so popular in 2012. Deep Learning and medical imaging became increasingly popular with the demonstration of dermatologist-level skin can- cer detection by Esteva et al. in 2017. The use of GANs in medical imaging is well documented in a survey by Yi et al. This survey covers the use of GANs in reconstruction such as CT denoising , accelerated magnetic resonance imaging , PET denoising , and the applica- tion of super-resolution GANs in retinal vasculature segmentation . Additionally, Yi et al", "ed14331c-5423-4448-b38a-e580c455086e": "If we had separate parameters for each value of the time index, we could not generalize to sequence lengths not seen during training, nor share statistical strength across different sequence lengths and across different positions in time.\n\nSuch sharing is particularly important when a specific piece of information can occur at multiple positions within the sequence. For example, consider the two sentences \u201cI went to Nepal in 2009\u201d and \u201cIn 2009, I went to Nepal.\u201d If we ask a machine learning model to read each sentence and extract the year in which the narrator went to Nepal, we would like it to recognize the year 2009 as the relevant piece of information, whether it appears in the sixth  https://www.deeplearningbook.org/contents/rnn.html    367  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  word or in the second word of the sentence. Suppose that we trained a feedforward network that processes sentences of fixed length. A traditional fully connected feedforward network would have separate parameters for each input feature, so it would need to learn all the rules of the language separately at each position in the sentence. By comparison, a recurrent neural network shares the same weights across several time steps", "cb35782f-e62e-41f8-ac48-c600f4c39f6f": "If we perturb each possible pair of weights in turn, we obtain Again, by using a symmetrical central differences formulation, we ensure that the residual errors are O(\u03f52) rather than O(\u03f5). Because there are W 2 elements in the Hessian matrix, and because the evaluation of each element requires four forward propagations each needing O(W) operations (per pattern), we see that this approach will require O(W 3) operations to evaluate the complete Hessian. It therefore has poor scaling properties, although in practice it is very useful as a check on the software implementation of backpropagation methods. A more ef\ufb01cient version of numerical differentiation can be found by applying central differences to the \ufb01rst derivatives of the error function, which are themselves calculated using backpropagation. This gives Because there are now only W weights to be perturbed, and because the gradients can be evaluated in O(W) steps, we see that this method gives the Hessian in O(W 2) operations", "e26480b1-5812-4745-aba2-1eb686c18e80": "As with maximum likelihood, this leads to a two-pass forward-backward recursion to compute posterior probabilities. The directed graph that represents the hidden Markov model, shown in Figure 13.5, is a tree and so we can solve the problem of \ufb01nding local marginals for the hidden variables using the sum-product algorithm. Not surprisingly, this turns out to Section 8.4.4 be equivalent to the forward-backward algorithm considered in the previous section, and so the sum-product algorithm therefore provides us with a simple way to derive the alpha-beta recursion formulae. We begin by transforming the directed graph of Figure 13.5 into a factor graph, of which a representative fragment is shown in Figure 13.14.\n\nThis form of the factor graph shows all variables, both latent and observed, explicitly. However, for the purpose of solving the inference problem, we shall always be conditioning on the variables x1, . , xN, and so we can simplify the factor graph by absorbing the emission probabilities into the transition probability factors", "29973a23-f814-4162-858c-8b6b87751088": "5.7.3 Other Simple Supervised Learning Algorithms  We have already briefly encountered another nonprobabilistic supervised learning algorithm, nearest neighbor regression. More generally, k-nearest neighbors is a family of techniques that can be used for classification or regression. As a nonparametric learning algorithm, k-nearest neighbors is not restricted to a fixed number of parameters. We usually think of the k-nearest neighbors algorithm  https://www.deeplearningbook.org/contents/ml.html    as not having any parameters but rather implementing a simple function of the training data. In fact, there is not even really a training stage or learning process. Instead, at test time, when we want to produce an output \u00a5 for a new test input 2,  we find the k-nearest neighbors to x in the training data X. We then return the average of the corresponding y values in the training set. This works for essentially any kind of supervised learning where we can define an average over y values. In the case of classification, we can average over one-hot code vectors \u00a2 with cy = 1 and c; = 0 for all other values of 7", "d380507b-e6e5-487a-8306-d49c1156eec9": "This arrangement brings together presynaptic activity of cortical neurons, postsynaptic activity of medium spiny neurons, and input from dopamine neurons. What actually occurs at these spines is complex and not completely understood. Figure 15.1 hints at the complexity by showing two types of receptors for dopamine, receptors for glutamate\u2014the neurotransmitter of the cortical inputs\u2014and multiple ways that the various signals can interact. But evidence is mounting that changes in the e\ufb03cacies of the synapses on the pathway from the cortex to the striatum, which neuroscientists call corticostriatal synapses, depend critically on appropriately-timed dopamine signals. Dopamine neurons respond with bursts of activity to intense, novel, or unexpected visual and auditory stimuli that trigger eye and body movements, but very little of their activity is related to the movements themselves.\n\nThis is surprising because degeneration of dopamine neurons is a cause of Parkinson\u2019s disease, whose symptoms include motor disorders, particularly de\ufb01cits in self-initiated movement", "fd90eea0-8755-4025-9451-b234a9546b1a": "DEEP GENERATIVE MODELS  og-likelihood it assigns to the data, or its ability to classify inputs. See figure 20.4 for an illustration of the training procedure. This greedy layer-wise training procedure is not just coordinate ascent.\n\nIt bears some passing resemblance to coordinate ascent because we optimize one subset of he parameters at each step. The two methods differ because the greedy layer-wise raining procedure uses a different objective function at each step. Greedy layer-wise pretraining of a DBM differs from greedy layer-wise pre- raining of a DBN. The parameters of each individual RBM may be copied to he corresponding DBN directly. In the case of the DBM, the RBM parameters must be modified before inclusion in the DBM. A layer in the middle of the stack of RBMs is trained with only bottom-up input, but after the stack is combined  o form the DBM, the layer will have both bottom-up and top-down input. To account for this effect, Salakhutdinov and Hinton  advocate dividing the weights of all but the top and bottom RBM in half before inserting them into the DBM", "288ab57c-8d66-4c72-9937-0c0b7547d057": "The quantities \u03f5m represent weighted measures of the error rates of each of the base classi\ufb01ers on the data set. We therefore see that the weighting coef\ufb01cients \u03b1m de\ufb01ned by (14.17) give greater weight to the more accurate classi\ufb01ers when computing the overall output given by (14.19). The AdaBoost algorithm is illustrated in Figure 14.2, using a subset of 30 data points taken from the toy classi\ufb01cation data set shown in Figure A.7. Here each base learners consists of a threshold on one of the input variables. This simple classi\ufb01er corresponds to a form of decision tree known as a \u2018decision stumps\u2019, i.e., a deciSection 14.4 sion tree with a single node. Thus each base learner classi\ufb01es an input according to whether one of the input features exceeds some threshold and therefore simply partitions the space into two regions separated by a linear decision surface that is parallel to one of the axes. Boosting was originally motivated using statistical learning theory, leading to upper bounds on the generalization error.\n\nHowever, these bounds turn out to be too loose to have practical value, and the actual performance of boosting is much better than the bounds alone would suggest. Friedman et al", "7a653d6b-bc33-4a5f-acb4-6af5307f1d46": "Input: a function F(s, a) returning the set of (indices of) active features for s, a Input: a policy \u21e1 (if estimating q\u21e1) Algorithm parameters: step size \u21b5 > 0, trace decay rate \u03bb 2  Initialize: w = (w1, . , wd)> 2 Rd (e.g., w = 0), z = (z1, . , zd)> 2 Rd Example 12.2: Sarsa(\u03bb) on Mountain Car Figure 12.10 (left) on the next page shows results with Sarsa(\u03bb) on the Mountain Car task introduced in Example 10.1. The function approximation, action selection, and environmental details were exactly as in Chapter 10, and thus it is appropriate to numerically compare these results with the Chapter 10 results for n-step Sarsa (right side of the \ufb01gure). The earlier results varied the update length n whereas here for Sarsa(\u03bb) we vary the trace parameter \u03bb, which plays a similar role", "5a746a12-5b56-4544-8d18-8f732f7e87d8": "LINEAR ALGEBRA by the definition of the L? norm, equation 2.30) =a! x\u2014a'g(c) \u2014g(c)'x + g(c)' g(c) (2.57) by the distributive property) =a! x \u20142a'g(c) + g(c) 'g(c) (2.58) because the scalar g(c)! a is equal to the transpose of itself). We can now change the function being minimized again, to omit the first term, since this term does not depend on c:  c= arg min\u20142a ' 9(\u00a2) + g(e) 'g(c). (2.59)  To make further progress, we must substitute in the definition of g(c):  c =argmin\u201422'De+c! 'D' De (2.60) c  = argmin \u2122De+e'he (2.61) c \u20142x  https://www.deeplearningbook.org/contents/linear_algebra.html     (by the orthogonality and unit norm constraints on D)  = arg min \u20142%'De+ce'e", "4822c774-39f5-468c-91c7-77c846e8a74e": "One of the key factors responsible for the improvement in neural network\u2019s accuracy and the improvement of the complexity of tasks they can solve between the 1980s and today is the dramatic increase in the size of the networks we use.\n\nAs we saw in section 1.2.3, network sizes have grown exponentially for the past three decades, yet artificial neural networks are only as large as the nervous systems of insects. Because the size of neural networks is critical, deep learning requires high  https://www.deeplearningbook.org/contents/applications.html    438  CHAPTER 12. APPLICATIONS  performance hardware and software infrastructure. 12.1.1 Fast CPU Implementations  Traditionally, neural networks were trained using the CPU of a single machine. Today, this approach is generally considered insufficient. We now mostly use GPU computing or the CPUs of many machines networked together. Before moving to these expensive setups, researchers worked hard to demonstrate that CPUs could not manage the high computational workload required by neural networks. A description of how to implement efficient numerical CPU code is beyond the scope of this book, but we emphasize here that careful implementation for specific CPU families can yield large improvements", "ecef4b52-44fd-4171-a493-ae84f14eb2a5": "What if one has no idea what the rewards should be but there is another agent, perhaps a person, who is already expert at the task and whose behavior can be observed? In this case one can use methods known variously as \u201cimitation learning,\u201d \u201clearning from demonstration,\u201d and \u201capprenticeship learning.\u201d The idea here is to bene\ufb01t from the expert agent but leave open the possibility of eventually performing better.\n\nLearning from an expert\u2019s behavior can be done either by learning directly by supervised learning or by extracting a reward signal using what is known as \u201cinverse reinforcement learning\u201d and then using a reinforcement learning algorithm with that reward signal to learn a policy. The task of inverse reinforcement learning as explored by Ng and Russell  is to try to recover the expert\u2019s reward signal from the expert\u2019s behavior alone. This cannot be done exactly because a policy can be optimal with respect to many di\u21b5erent reward signals (for example, any reward signal that gives the same reward for all states and actions), but it is possible to \ufb01nd plausible reward signal candidates. Unfortunately, strong assumptions are required, including knowledge of the environment\u2019s dynamics and of the feature vectors in which the reward signal is linear. The method also requires completely solving the problem (e.g., by dynamic programming methods) multiple times", "87613d59-bf7b-4916-a095-b7e36f6af7cb": "6.1 Example: Learning XOR  To make the idea of a feedforward network more concrete, we begin with an example of a fully functioning feedforward network on a very simple task: learning the XOR function. The XOR function (\u201cexclusive or\u201d) is an operation on two binary values, 21 and x2. When exactly one of these binary values is equal to 1, the XOR function returns 1. Otherwise, it returns 0. The XOR function provides the target function  https://www.deeplearningbook.org/contents/mlp.html    y = J7(#) that we want to learn. Our model proyides a function y= f(#@), and pur earning algorithm will adapt the parameters 9 to make / as similar as possible tof. In this simple example, we will not be concerned with statistical generalization. We want our network to perform correctly on the four points X = {\", and \"}.\n\nWe will train the network on all four of these points. The only challenge is to fit the training set. We can treat this problem as a regression problem and use a mean squared error loss function", "b1da756e-80d6-4937-b2f6-cae9260d14f7": "Suppose, perhaps as part of a larger MDP, there are two states whose estimated values are of the functional form w and 2w, where the parameter vector w consists of only a single component w. This occurs under linear function approximation if the feature vectors for the two states are each simple numbers (single-component vectors), in this case 1 and 2. In the \ufb01rst state, only one action is available, and it results deterministically in a transition to the second state with a reward of 0: where the expressions inside the two circles indicate the two state\u2019s values. Suppose initially w = 10. The transition will then be from a state of estimated value 10 to a state of estimated value 20. It will look like a good transition, and w will be increased to raise the \ufb01rst state\u2019s estimated value. If \u03b3 is nearly 1, then the TD error will be nearly 10, and, if \u21b5 = 0.1, then w will be increased to nearly 11 in trying to reduce the TD error. However, the second state\u2019s estimated value will also be increased, to nearly 22", "c46e64f9-cddf-4ddc-97a3-950a57c51387": "If we have n different microphones placed in different locations, ICA can detect the changes in the volume between each speaker as heard by each microphone and separate the signals so that each h; contains only one person speaking clearly. This is commonly used in neuroscience for electroencephalography, a technology for recording electrical signals originating in the brain. Multiple electrode sensors placed on the subject\u2019s head are used to measure many electrical signals coming from the body. The experimenter is typically only interested in signals from the brain, but signals from the subject\u2019s heart and eyes are strong enough to confound measurements taken at the subject\u2019s scalp.\n\nThe signals arrive at the electrodes mixed together, so ICA is necessary to separate the electrical signature of the heart from the signals originating in the brain, and to separate signals in different brain regions from each other. As mentioned before, many variants of ICA are possible. Some add some noise in the generation of x rather than using a deterministic decoder. Most do not use the maximum likelihood criterion, but instead aim to make the elements of h =W\u2014'a independent from each other. Many criteria that accomplish this goal are possible. Equation 3.47 requires taking the determinant of W, which can be an expensive and numerically unstable operation", "c4a124f8-cc37-4f69-a76d-5ca953d7f69c": "For example, suppose the states in the examples are the states generated by interaction (or simulated interaction) with the environment using policy \u21e1. Because the true value of a state is the expected value of the return following it, the Monte Carlo target Ut .= Gt is by de\ufb01nition an unbiased estimate of v\u21e1(St). With this choice, the general SGD method (9.7) converges to a locally optimal approximation to v\u21e1(St). Thus, the gradient-descent version of Monte Carlo state-value prediction is guaranteed to \ufb01nd a locally optimal solution. Pseudocode for a complete algorithm is shown in the box below. Gradient Monte Carlo Algorithm for Estimating \u02c6v \u21e1 v\u21e1 Generate an episode S0, A0, R1, S1, A1, . , RT , ST using \u21e1 Loop for each step of episode, t = 0, 1,", "41a1830a-cac6-47bf-9897-80cd345b77bc": "S., Watkins, C. J. C. H. Learning and sequential decision making. In M. Gabriel and J. Moore (Eds. ), Learning and Computational Neuroscience: Foundations of Adaptive Networks, pp. 539\u2013602.\n\nMIT Press, Cambridge, MA. Baxter, J., Bartlett, P. L. In\ufb01nite-horizon policy-gradient estimation. Journal of Arti\ufb01cial Baxter, J., Bartlett, P. L., Weaver, L. Experiments with in\ufb01nite-horizon, policy-gradient estimation. Journal of Arti\ufb01cial Intelligence Research, 15:351\u2013381. Bellemare, M. G., Dabney, W., Munos, R. A distributional perspective on reinforcement Bellemare, M. G., Naddaf, Y., Veness, J., Bowling, M. The arcade learning environment: Bellemare, M", "fa386ba8-aa4d-4834-a7e7-4fba1b4b7d59": "The deep learning approach is often to figure out what the minimum amount of information we absolutely need is, and then to figure out how to get a reasonable approximation of that information as quickly as possible. 583  CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  Figure 16.14: An RBM drawn as a Markov network. 16.7.1 Example: The Restricted Boltzmann Machine  The restricted Boltzmann machine (RBM) , or harmo- nium, is the quintessential example of how graphical models are used for deep learning. The RBM is not itself a deep model. Instead, it has a single layer of latent variables that may be used to learn a representation for the input", "a4a7ff2f-672a-41b9-be02-0bc7244ba89d": "However, there are not many comparative studies that show the performance differences of these different augmentations. One such study was conducted by Shijie et al. which compared GANs, WGANs, flipping, cropping, shifting, PCA jittering, color jittering, adding noise, rotation, and some combinations on the CIFAR-10 and ImageNet datasets. Additionally, the comparative study ranged across dataset sizes with the small set consisting of 2 k samples with 200 in each class, tthe medium set consisting of 10 k samples with 1 k in each class, and the large set consist- ing of 50 k samples with 5 k in each class. They also tested with 3 levels of augmentation, no augmentation, original plus same size of generated samples, and original plus double size of generated samples. They found that cropping, flipping, WGAN, and rotation gen- erally performed better than others. The combinations of flipping + cropping and flip- ping + WGAN were the best overall, improving classification performance on CIFAR-10 by + 3% and + 3.5%, respectively", "3e8e881c-598e-4471-8116-9d6337895f03": "In particular, we shall take the targets for class C1 to be N/N1, where N1 is the number of patterns in class C1, and N is the total number of patterns. This target value approximates the reciprocal of the prior probability for class C1. For class C2, we shall take the targets to be \u2212N/N2, where N2 is the number of patterns in class C2.\n\nThe sum-of-squares error function can be written Setting the derivatives of E with respect to w0 and w to zero, we obtain respectively From (4.32), and making use of our choice of target coding scheme for the tn, we obtain an expression for the bias in the form and where m is the mean of the total data set and is given by After some straightforward algebra, and again making use of the choice of tn, the second equation (4.33) becomes Exercise 4.6 where SW is de\ufb01ned by (4.28), SB is de\ufb01ned by (4.27), and we have substituted for the bias using (4.34). Using (4.27), we note that SBw is always in the direction of (m2 \u2212 m1). Thus we can write where we have ignored irrelevant scale factors. Thus the weight vector coincides with that found from the Fisher criterion", "a5617531-296b-43f6-bd9e-6bfe70f4f368": "Snorkel: rapid training data creation with weak supervision Alexander Ratner1 \u00b7 Stephen H. Bach1,2 \u00b7 Henry Ehrenberg1 \u00b7 Jason Fries1 \u00b7 Sen Wu1 \u00b7 Christopher R\u00e91 Received: 16 December 2018 / Revised: 15 May 2019 / Accepted: 25 June 2019 / Published online: 15 July 2019 \u00a9 The Author(s) 2019 Abstract Labeling training data is increasingly the largest bottleneck in deploying machine learning systems. We present Snorkel, a \ufb01rst-of-its-kind system that enables users to train state-of-the-art models without hand labeling any training data. Instead, users write labeling functions that express arbitrary heuristics, which can have unknown accuracies and correlations. Snorkel denoises their outputs without access to ground truth by incorporating the \ufb01rst end-to-end implementation of our recently proposed machine learning paradigm, data programming. We present a \ufb02exible interface layer for writing labeling functions based on our experience over the past year collaborating with companies, agencies, and research laboratories. In a user study, subject matter experts build models 2.8\u00d7 faster and increase predictive performance an average 45.5% versus seven hours of hand labeling", "59476af1-d334-4536-9c19-9703c57c36c1": "Typically we do not allow the precision matrix to be nondiagonal in this context, because some operations on the Gaussian distribution require inverting the matrix, and a diagonal matrix can be inverted trivially.\n\nIn the sections ahead, we will see that other forms of Boltzmann machines permit modeling the covariance structure, using various techniques to avoid inverting the precision matrix. 20.5.2 Undirected Models of Conditional Covariance  While the Gaussian RBM has been the canonical energy model for real-valued data, Ranzato et al. argue that the Gaussian RBM inductive bias is not well suited to the statistical variations present in some types of real-valued data, especially natural images. The problem is that much of the information content present in natural images is embedded in the covariance between pixels rather than in the raw pixel values. In other words, it is the relationships between pixels and not their absolute values where most of the useful information in images resides. Since the Gaussian RBM only models the conditional mean of the input given the hidden units, it cannot capture conditional covariance information. In response to these criticisms, alternative models have been proposed that attempt to better account for the covariance of real-valued data", "4e87b5de-35dc-4a41-bdb1-d2f03a862b77": "IEEE Computer Society. Sirovich, L. Turbulence and the dynamics of coherent structures. Quarterly Applied Mathematics 45(3), 561\u2013590. Smola, A. J. and P. Bartlett . Sparse greedy Gaussian process regression. In T. K. Leen, T. G. Dietterich, and V. Tresp (Eds. ), Advances in Neural Information Processing Systems, Volume 13, pp. 619\u2013625. MIT Press. Spiegelhalter, D. and S. Lauritzen . Sequential updating of conditional probabilities on directed graphical structures. Networks 20, 579\u2013605. Stinchecombe, M. and H. White . Universal approximation using feed-forward networks with non-sigmoid hidden layer activation functions. In International Joint Conference on Neural Networks, Volume 1, pp. 613\u2013618. IEEE", "1d8b8618-ba2a-41ea-a9d6-e224b446e22b": "We note that, if a patient who does not have cancer is incorrectly diagnosed as having cancer, the consequences may be some patient distress plus the need for further investigations. Conversely, if a patient with cancer is diagnosed as healthy, the result may be premature death due to lack of treatment. Thus the consequences of these two types of mistake can be dramatically different. It would clearly be better to make fewer mistakes of the second kind, even if this was at the expense of making more mistakes of the \ufb01rst kind. We can formalize such issues through the introduction of a loss function, also called a cost function, which is a single, overall measure of loss incurred in taking any of the available decisions or actions. Our goal is then to minimize the total loss incurred.\n\nNote that some authors consider instead a utility function, whose value they aim to maximize. These are equivalent concepts if we take the utility to be simply the negative of the loss, and throughout this text we shall use the loss function convention. Suppose that, for a new value of x, the true class is Ck and that we assign x to class Cj (where j may or may not be equal to k)", "7e898adb-6e32-467e-9c71-385a1c98a91e": "When training with a penalty on the norm of the weights, these configurations can be locally optimal, even if it is possible to significantly reduce J by making the weights larger. Explicit constraints implemented by reprojection can work much better in these cases because they do not encourage the weights to approach the origin.\n\nExplicit constraints implemented by reprojection have an effect only when the weights become large and attempt to leave the constraint region. 234  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  https://www.deeplearningbook.org/contents/regularization.html    Finally, explicit constraints with reprojection can be useful because they impose some stability on the optimization procedure. When using high learning rates, it is possible to encounter a positive feedback loop in which large weights induce large gradients, which then induce a large update to the weights. If these updates  consistently increase the size of the weights, then @ rapidly moves away from the origin until numerical overflow occurs. Explicit constraints with reprojection prevent this feedback loop from continuing to increase the magnitude of the weights without bound. Hinton et al. recommend using constraints combined with a high learning rate to enable rapid exploration of parameter space while maintaining some stability", "39acd09f-2a95-4079-b13b-7373f64993ea": "However, all these local minima arising from nonidentifiability are equivalent to each other in cost function value. As a result, these local minima are not a problematic form of nonconvexity. Local minima can be problematic if they have high cost in comparison to the global minimum. One can construct small neural networks, even without hidden units, that have local minima with higher cost than the global minimum (Sontag  281  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  https://www.deeplearningbook.org/contents/optimization.html    and Sussman, 1989; Brady et al., 1989; Gori and Tesi, 1992). If local minima with high cost are common, this could pose a serious problem for gradient-based optimization algorithms. Whether networks of practical interest have many local minima of high cost and whether optimization algorithms encounter them remain open questions. For many years, most practitioners believed that local minima were a common problem plaguing neural network optimization", "6a7278c3-38f5-488b-92e6-ff5191b997e3": "Academic Press, New York. Calabresi, P., Picconi, B., Tozzi, A., Filippo, M. D. Dopamine-mediated regulation of corticostriatal synaptic plasticity. Trends in Neuroscience, 30(5):211\u2013219. Camerer, C. Behavioral Game Theory: Experiments in Strategic Interaction. Princeton Campbell, D. T. Blind variation and selective survival as a general strategy in knowledgeprocesses. In M. C. Yovits and S. Cameron (Eds. ), Self-Organizing Systems, pp. 205\u2013231.\n\nPergamon, New York. Cao, X. R. Stochastic learning and optimization\u2014A sensitivity-based approach. Annual Cao, X. R., Chen, H. F. Perturbation realization, potentials, and sensitivity analysis of Markov processes. IEEE Transactions on Automatic Control, 42(10):1382\u20131393", "bd1118ce-7f67-4b1c-b05a-0dcd5d5336fe": "Our goal is to extend this model by making the basis functions \u03c6j(x) depend on parameters and then to allow these parameters to be adjusted, along with the coef\ufb01cients {wj}, during training. There are, of course, many ways to construct parametric nonlinear basis functions. Neural networks use basis functions that follow the same form as (5.1), so that each basis function is itself a nonlinear function of a linear combination of the inputs, where the coef\ufb01cients in the linear combination are adaptive parameters. This leads to the basic neural network model, which can be described a series of functional transformations. First we construct M linear combinations of the input variables x1, . , xD in the form where j = 1, . , M, and the superscript (1) indicates that the corresponding parameters are in the \ufb01rst \u2018layer\u2019 of the network. We shall refer to the parameters w(1) ji as weights and the parameters w(1) j0 as biases, following the nomenclature of Chapter 3", "f59bb3e7-f4b1-44d2-ae3b-4ccdf5bc75a1": "These are based on analytical approximations to the posterior distribution, for example by assuming that it factorizes in a particular way or that it has a speci\ufb01c parametric form such as a Gaussian. As such, they can never generate exact results, and so their strengths and weaknesses are complementary to those of sampling methods. In Section 4.4, we discussed the Laplace approximation, which is based on a local Gaussian approximation to a mode (i.e., a maximum) of the distribution. Here we turn to a family of approximation techniques called variational inference or variational Bayes, which use more global criteria and which have been widely applied. We conclude with a brief introduction to an alternative variational framework known as expectation propagation. Variational methods have their origins in the 18th century with the work of Euler, Lagrange, and others on the calculus of variations.\n\nStandard calculus is concerned with \ufb01nding derivatives of functions. We can think of a function as a mapping that takes the value of a variable as the input and returns the value of the function as the output", "364832b6-adc6-4eb8-aed0-a74836920e3f": "Shimansky  proposed a synaptic learning rule somewhat similar to Seung\u2019s mentioned above in which each synapse individually acts like a chemotactic bacterium. In this case a collection of synapses \u201cswims\u201d toward attractants in the high-dimensional space of synaptic weight values. Montague, Dayan, Person, and Sejnowski  proposed a chemotactic-like model of the bee\u2019s foraging behavior involving the neuromodulator octopamine. 15.10 Research on the behavior of reinforcement learning agents in team and game problems has a long history roughly occurring in three phases. To the best of our knowledge, the \ufb01rst phase began with investigations by the Russian mathematician and physicist M. L. Tsetlin. A collection of his work was published as Tsetlin  after his death in 1966. Our Sections 1.7 and 4.8 refer to his study of learning automata in connection to bandit problems", "7e385adb-d731-4eb3-a454-4acdfd511945": "Most implementations of minibatch stochastic gradient descent shuffle the dataset once and then pass through it multiple times. On the first pass, each minibatch is used to compute an unbiased estimate of the true generalization error.\n\nOn the second pass, the estimate becomes biased because it is formed by resampling values that have already been used, rather than obtaining new fair samples from the data-generating distribution. The fact that stochastic gradient descent minimizes generalization error is easiest to see in online learning, where examples or minibatches are drawn from a stream of data. In other words, instead of receiving a fixed-size training set, the learner is similar to a living being who sees a new example at each instant, with every example (x, y) coming from the data-generating distribution pdata (x, y). In this scenario, examples are never repeated; every experience is a fair sample from Pdata:  The equivalence is easiest to derive when both a and y are discrete", "d5642361-ac01-4b8d-a8f4-845488f3bc8c": "Monte Carlo methods sample and average returns for each state\u2013action pair much like the bandit methods we explored in Chapter 2 sample and average rewards for each action. The main di\u21b5erence is that now there are multiple states, each acting like a di\u21b5erent bandit problem (like an associative-search or contextual bandit) and the di\u21b5erent bandit problems are interrelated. That is, the return after taking an action in one state depends on the actions taken in later states in the same episode. Because all the action selections are undergoing learning, the problem becomes nonstationary from the point of view of the earlier state. To handle the nonstationarity, we adapt the idea of general policy iteration (GPI) developed in Chapter 4 for DP. Whereas there we computed value functions from knowledge of the MDP, here we learn value functions from sample returns with the MDP. The value functions and corresponding policies still interact to attain optimality in essentially the same way (GPI).\n\nAs in the DP chapter, \ufb01rst we consider the prediction problem (the computation of v\u21e1 and q\u21e1 for a \ufb01xed arbitrary policy \u21e1) then policy improvement, and, \ufb01nally, the control problem and its solution by GPI", "007486b6-b960-4ee6-8e1b-4e3354f4973a": "The main reason that random search finds good solutions faster than grid  https://www.deeplearningbook.org/contents/guidelines.html    search is that it has no wasted experimental runs, unlike in the case of grid search, when two values of a hyperparameter (given values of the other hyperparameters)  429  CHAPTER 11. PRACTICAL METHODOLOGY  would give the same result. In the case of grid search, the other hyperparameters would have the same values for these two runs, whereas with random search, they would usually have different values. Hence if the change between these two values does not marginally make much difference in terms of validation set error, grid search will unnecessarily repeat two equivalent experiments while random search will still give two independent explorations of the other hyperparameters. 11.4.5 Model-Based Hyperparameter Optimization  The search for good hyperparameters can be cast as an optimization problem.\n\nThe decision variables are the hyperparameters. The cost to be optimized is the validation set error that results from training using these hyperparameters. In simplified settings where it is feasible to compute the gradient of some differentiable error measure on the validation set with respect to the hyperparameters, we can simply follow this gradient", "0cd8cd19-4bbf-4e66-a261-7c9b9bf182b0": "Although this can be a very useful and informative quantity, in the end we must decide either to give treatment to the patient or not, and we would like this choice to be optimal in some appropriate sense . This is the decision step, and it is the subject of decision theory to tell us how to make optimal decisions given the appropriate probabilities. We shall see that the decision stage is generally very simple, even trivial, once we have solved the inference problem. Here we give an introduction to the key ideas of decision theory as required for the rest of the book. Further background, as well as more detailed accounts, can be found in Berger  and Bather .\n\nBefore giving a more detailed analysis, let us \ufb01rst consider informally how we might expect probabilities to play a role in making decisions. When we obtain the X-ray image x for a new patient, our goal is to decide which of the two classes to assign to the image. We are interested in the probabilities of the two classes given the image, which are given by p(Ck|x)", "e8d28c5a-94d0-41d1-ac16-4150cccf9c21": "APPROXIMATE INFERENCE  approximations of p(h | v), the lower bound \u00a3 will be tighter, in other words, closer to log p(v).\n\nWhen q(h | v) = p(h | v), the approximation is perfect, and L(v, 0, \u00a2) = log p(v; 8). We can thus think of inference as the procedure for finding the g that maximizes L. Exact inference maximizes \u00a3 perfectly by searching over a family of functions q that includes p(h | v). Throughout this chapter, we show how to derive different forms of approximate inference by using approximate optimization to find g. We can make the optimization procedure less expensive but approximate by restricting the family of distributions qg that the optimization is allowed to search over or by using an imperfect optimization procedure that may not completely maximize \u00a3 but may merely increase it by a significant amount. No matter what choice of q we use, \u00a3 is a lower bound. We can get tighter or looser bounds that are cheaper or more expensive to compute depending on how we choose to approach this optimization problem", "d580631d-5475-4cda-8c0b-8b0eb3903ee3": "But when reward arrives earlier than expected, dopamine neurons do not do what the TD error does\u2014at least with the CSC representation used by Montague et al. and by us in our example. Dopamine neurons do respond to the early reward, which is consistent with a positive TD error because the reward is not predicted to occur then.\n\nHowever, at the later time when the reward is expected but omitted, the TD error is negative whereas, in contrast to this prediction, dopamine neuron activity does not drop below baseline in the way the TD model predicts . Something more complicated is going on in the animal\u2019s brain than simply TD learning with a CSC representation. Some of the mismatches between the TD error and dopamine neuron activity can be addressed by selecting suitable parameter values for the TD algorithm and by using stimulus representations other than the CSC representation. For instance, to address the early-reward mismatch just described, Suri and Schultz  proposed a CSC representation in which the sequences of internal signals initiated by earlier stimuli are cancelled by the occurrence of a reward", "5d800940-8468-49a2-bb1a-781d848032d8": "To finish computing the value of h for each example, we apply the rectified linear transformation:  (6.10)  NrRrRO a  This transformation has changed the relationship between the examples. They no  https://www.deeplearningbook.org/contents/mlp.html    longer lie on a single line. As shown In figure 0.1, they now he 1n a space where a linear model can solve the problem. We finish with multiplying by the weight vector w:  (6.11)  The neural network has obtained the correct answer for every example in the batch. In this example, we simply specified the solution, then showed that it obtained zero error. In a real situation, there might be billions of model parameters and billions of training examples, so one cannot simply guess the solution as we did here. Instead, a gradient-based optimization algorithm can find parameters that produce very little error. The solution we described to the XOR problem is at a global minimum of the loss function, so gradient descent could converge to this point. There are other equivalent solutions to the XOR problem that gradient descent could also find", "e82c510e-b2e0-4a82-9e9d-382678e76466": "This result is unsurprising, since both principal component analysis and the neural network are using linear dimensionality reduction and are minimizing the same sum-of-squares error function.\n\nIt might be thought that the limitations of a linear dimensionality reduction could be overcome by using nonlinear (sigmoidal) activation functions for the hidden units in the network in Figure 12.18. However, even with nonlinear hidden units, the minimum error solution is again given by the projection onto the principal component subspace . There is therefore no advantage in using twolayer neural networks to perform dimensionality reduction. Standard techniques for principal component analysis (based on singular value decomposition) are guaranteed to give the correct solution in finite time, and they also generate an ordered set of eigenvalues with corresponding orthonormal eigenvectors. The situation is different, however. if additional hidden layers are pcrmillcd in the network. Consider the four-layer autoassociativc network shown in Figure 12.19. Again the output units are linear, and the M units in the second hidden layer can also be linear. however, the first and third hidden layers have sigmoidal nonlinear activation functions", "f49c7fa3-19a0-4a83-9a57-1ab844646e76": "The computation performed by each node does not have to be the traditional artificial  neuron computation (affine transformation of all inputs followed by a monotone nonlinearity). For example, Socher ef al. propose using tensor operations and bilinear forms, which have previously been found useful to model relationships between concepts  when the concepts are represented by continuous vectors (embeddings). 10.7 The Challenge of Long-Term Dependencies  The mathematical challenge of learning long-term dependencies in recurrent net- works is introduced in section 8.2.5. The basic problem is that gradients propagated over many stages tend to either vanish (most of the time) or explode (rarely, but with much damage to the optimization).\n\nEven if we assume that the parameters are such that the recurrent network is stable (can store memories, with gradients not  https://www.deeplearningbook.org/contents/rnn.html    exploding), the difficulty with long-term dependencies arises from the exponentially smaller weights given to long-term interactions (involving the multiplication of many Jacobians) compared to short-term ones. Many other sources provide a deeper treatment . In this section, we describe the problem in more detail", "4e42af57-e783-401a-ac71-7c5829b09927": "Consider, for instance, the case of a Gaussian mixture, and suppose we perform an update for data point m in which the corresponding old and new values of the responsibilities are denoted \u03b3old(zmk) and \u03b3new(zmk). In the M step, the required suf\ufb01cient statistics can be updated incrementally. For instance, for the means the suf\ufb01cient statistics are de\ufb01ned by (9.17) and (9.18) from which we obtain Exercise 9.26 The corresponding results for the covariances and the mixing coef\ufb01cients are analogous. Thus both the E step and the M step take \ufb01xed time that is independent of the total number of data points. Because the parameters are revised after each data point, rather than waiting until after the whole data set is processed, this incremental version can converge faster than the batch version. Each E or M step in this incremental algorithm is increasing the value of L(q, \u03b8) and, as we have shown above, if the algorithm converges to a local (or global) maximum of L(q, \u03b8), this will correspond to a local (or global) maximum of the log likelihood function ln p(X|\u03b8)", "874ac031-ba3e-4dcf-bd60-bfdd961892e5": "For a polynomial of order M, the growth in the number of coef\ufb01cients is like DM. Although this is now a power law growth, Exercise 1.16 rather than an exponential growth, it still points to the method becoming rapidly unwieldy and of limited practical utility. Our geometrical intuitions, formed through a life spent in a space of three dimensions, can fail badly when we consider spaces of higher dimensionality. As a simple example, consider a sphere of radius r = 1 in a space of D dimensions, and ask what is the fraction of the volume of the sphere that lies between radius r = 1\u2212\u03f5 and r = 1. We can evaluate this fraction by noting that the volume of a sphere of radius r in D dimensions must scale as rD, and so we write where the constant KD depends only on D. Thus the required fraction is given by Exercise 1.18 which is plotted as a function of \u03f5 for various values of D in Figure 1.22. We see that, for large D, this fraction tends to 1 even for small values of \u03f5", "34aad8ab-7329-40a4-8b59-0c28aa480992": "Intractability: the case where the integral of the marginal likelihood p\u03b8(x) = \ufffd p\u03b8(z)p\u03b8(x|z) dz is intractable (so we cannot evaluate or differentiate the marginal likelihood), where the true posterior density p\u03b8(z|x) = p\u03b8(x|z)p\u03b8(z)/p\u03b8(x) is intractable (so the EM algorithm cannot be used), and where the required integrals for any reasonable mean-\ufb01eld VB algorithm are also intractable. These intractabilities are quite common and appear in cases of moderately complicated likelihood functions p\u03b8(x|z), e.g. a neural network with a nonlinear hidden layer. 2. A large dataset: we have so much data that batch optimization is too costly; we would like to make parameter updates using small minibatches or even single datapoints. Samplingbased solutions, e.g. Monte Carlo EM, would in general be too slow, since it involves a typically expensive sampling loop per datapoint. We are interested in, and propose a solution to, three related problems in the above scenario: 1", "b492dc5c-1b2a-485d-b140-6e26236a8b48": "It will not, however, ensure that edges within the dark region stand out. This motivates local contrast normalization. Local contrast normalization ensures that the contrast is normalized across each small window, rather than over the image as a whole. See figure 12.2 for a comparison of global and local contrast normalization. Various definitions of local contrast normalization are possible. In all cases, one modifies each pixel by subtracting a mean of nearby pixels and dividing by a standard deviation of nearby pixels. In some cases, this is literally the mean and standard deviation of all pixels in a rectangular window centered on the pixel to be modified . In other cases, this is a weighted mean and weighted standard deviation using Gaussian weights centered on the pixel to  1 rn re c 1 . \u2018 ' . wer : 1  https://www.deeplearningbook.org/contents/applications.html    pe moained. in the case or color images, some strategies process aliferent color channels separately while others combine information from different channels to  451  CHAPTER 12", "ee3c29a9-b5d5-4951-9c58-724bb83c8fcb": "The fully variational Bayesian method for inferring a posterior over the parameters is given in the appendix.\n\nUnder certain mild conditions outlined in section 2.4 for a chosen approximate posterior q\u03c6(z|x) we can reparameterize the random variable \ufffdz \u223c q\u03c6(z|x) using a differentiable transformation g\u03c6(\u03f5, x) of an (auxiliary) noise variable \u03f5: \ufffdz = g\u03c6(\u03f5, x) with \u03f5 \u223c p(\u03f5) (4) See section 2.4 for general strategies for chosing such an approriate distribution p(\u03f5) and function g\u03c6(\u03f5, x). We can now form Monte Carlo estimates of expectations of some function f(z) w.r.t. q\u03c6(z|x) as follows: We apply this technique to the variational lower bound (eq. (2)), yielding our generic Stochastic Gradient Variational Bayes (SGVB) estimator \ufffdLA(\u03b8, \u03c6; x(i)) \u2243 L(\u03b8, \u03c6; x(i)): Algorithm 1 Minibatch version of the Auto-Encoding VB (AEVB) algorithm", "d4297648-59a3-495b-a9d0-55952e9fb37b": "Suppose the \ufb01rst episode progressed directly from the center state, C, to the right, through D and E, and then terminated on the right with a return of 1. Recall that the estimated values of all the states started at an intermediate value, V (s) = 0.5. As a result of this experience, a one-step method would change only the estimate for the last state, V (E), which would be incremented toward 1, the observed return. A two-step method, on the other hand, would increment the values of the two states preceding termination: V (D) and V (E) both would be incremented toward 1. A three-step method, or any n-step method for n > 2, would increment the values of all three of the visited states toward 1, all by the same amount. Which value of n is better? Figure 7.2 shows the results of a simple empirical test for a larger random walk process, with 19 states instead of 5 (and with a \u22121 outcome on the left, all values initialized to 0), which we use as a running example in this chapter", "861c4af5-01fb-4261-b5f8-9413adc1a711": "\u02d9Ipek, Mutlu, Mart\u00b4\u0131nez, and Caruana   designed a reinforcement learning memory controller and demonstrated that it can signi\ufb01cantly improve the speed of program execution over what was possible with conventional controllers at the time of their research. They were motivated by limitations of existing state-of-the-art controllers that used policies that did not take advantage of past scheduling experience and did not account for long-term consequences of scheduling decisions. \u02d9Ipek et al.\u2019s project was carried out by means of simulation, but they designed the controller at the detailed level of the hardware needed to implement it\u2014including the learning algorithm\u2014directly on a processor chip. Accessing DRAM involves a number of steps that have to be done according to strict time constraints. DRAM systems consist of multiple DRAM chips, each containing multiple rectangular arrays of storage cells arranged in rows and columns. Each cell stores a bit as the charge on a capacitor", "87110c23-8d68-4293-a253-712be534d5c9": "However, this too runs into the problem of ambiguous regions, as illustrated in the right-hand diagram of Figure 4.2.\n\nWe can avoid these dif\ufb01culties by considering a single K-class discriminant comprising K linear functions of the form and then assigning a point x to class Ck if yk(x) > yj(x) for all j \u0338= k. The decision boundary between class Ck and class Cj is therefore given by yk(x) = yj(x) and hence corresponds to a (D \u2212 1)-dimensional hyperplane de\ufb01ned by This has the same form as the decision boundary for the two-class case discussed in Section 4.1.1, and so analogous geometrical properties apply. The decision regions of such a discriminant are always singly connected and convex. To see this, consider two points xA and xB both of which lie inside decision region Rk, as illustrated in Figure 4.3", "9c3dba8e-1264-4ba8-8fef-70d41d2ca2ed": "Left: before learning: the agent selects actions randomly and the glider descends. Right: after learning: the glider gains altitude by following a spiral trajectory. Adapted with permission from PNAS vol. 113(22), p. E4879, 2016, Reddy, Celani, Sejnowski, and Vergassola, Learning to Soar in Turbulent Environments. within the rising column of air. Although Reddy et al. found that performance varied widely over di\u21b5erent simulated periods of air \ufb02ow, the number of times the glider touched the ground consistently decreased to nearly zero as learning progressed. After experimenting with di\u21b5erent sets of features available to the learning agent, it turned out that the combination of just vertical wind acceleration and torques worked best.\n\nThe authors conjectured that because these features give information about the gradient of vertical wind velocity in two di\u21b5erent directions, they allow the controller to select between turning by changing the bank angle or continuing along the same course by leaving the bank angle alone. This allows the glider to stay within a rising column of air. Vertical wind velocity is indicative of the strength of the thermal but does not help in staying within the \ufb02ow", "b7bdaf80-2ae3-4d84-b32d-fbcc61c25e7e": "This meang, Wepuse the  https://www.deeplearningbook.org/contents/mlp.html    CIrOSS-eENLLOpy DELWeeL LE Lally Gala ald Ue LOGE $8 PLedICLIOUs aS ULE COSL function. Sometimes, we take a simpler approach, where rather than predicting a complete probability distribution over y, we merely predict some statistic of y conditioned  on x. Specialized loss functions enable us to train a predictor of these estimates. The total cost function used to train a neural network will often combine one of the primary cost functions described here with a regularization term. We have already seen some simple examples of regularization applied to linear models in section 5.2.2. The weight decay approach used for linear models is also directly applicable to deep neural networks and is among the most popular regulariza- tion strategies. More advanced regularization strategies for neural networks are described in chapter 7.\n\n6.2.1.1 Learning Conditional Distributions with Maximum Likelihood  Most modern neural networks are trained using maximum likelihood. This means that the cost function is simply the negative log-likelihood, equivalently described as the cross-entropy between the training data and the model distribution", "f890f8cf-c063-41f0-828c-892def48136b": "For all values of \u03b1 we have D\u03b1(p\u2225q) \u2a7e 0, with equality if, and only if, Exercise 10.6 p(x) = q(x). Suppose p(x) is a \ufb01xed distribution, and we minimize D\u03b1(p\u2225q) with respect to some set of distributions q(x). Then for \u03b1 \u2a7d \u22121 the divergence is zero forcing, so that any values of x for which p(x) = 0 will have q(x) = 0, and typically q(x) will under-estimate the support of p(x) and will tend to seek the mode with the largest mass. Conversely for \u03b1 \u2a7e 1 the divergence is zero-avoiding, so that values of x for which p(x) > 0 will have q(x) > 0, and typically q(x) will stretch to cover all of p(x), and will over-estimate the support of p(x).\n\nWhen \u03b1 = 0 we obtain a symmetric divergence that is linearly related to the Hellinger distance given by The square root of the Hellinger distance is a valid distance metric", "c83280a7-a568-4c03-a70e-5f5632f3bd8a": "Now suppose we wish to evaluate the marginals p(xn) for every node n \u2208 {1, . , N} in the chain. Simply applying the above procedure separately for each node will have computational cost that is O(N 2M 2). However, such an approach would be very wasteful of computation. For instance, to \ufb01nd p(x1) we need to propagate a message \u00b5\u03b2(\u00b7) from node xN back to node x2. Similarly, to evaluate p(x2) we need to propagate a messages \u00b5\u03b2(\u00b7) from node xN back to node x3. This will involve much duplicated computation because most of the messages will be identical in the two cases.\n\nSuppose instead we \ufb01rst launch a message \u00b5\u03b2(xN\u22121) starting from node xN and propagate corresponding messages all the way back to node x1, and suppose we similarly launch a message \u00b5\u03b1(x2) starting from node x1 and propagate the corresponding messages all the way forward to node xN. Provided we store all of the intermediate messages along the way, then any node can evaluate its marginal simply by applying (8.54)", "099c7c9f-c55e-44d4-831e-8a5f51233e45": "Boosting has been applied to build ensembles of neural networks  by incrementally adding neural networks to the ensemble. Boosting has also been applied interpreting an individual neural network as an ensemble , incrementally adding hidden units to the network. 7.12 Dropout  Dropout  provides a computationally inexpensive but powerful method of regularizing a broad family of models. To a first approximation,  https://www.deeplearningbook.org/contents/regularization.html    dropout can be thought of as a method of making bagging practical for ensembles of very many large neural networks. Bagging involves training multiple models and evaluating multiple models on each test example.\n\nThis seems impractical  when each model is a large neural network, since training and evaluating such networks is costly in terms of runtime and memory. It is common to use ensembles of five to ten neural networks\u2014Szegedy et al. used six to win the ILSVRC\u2014 but more than this rapidly becomes unwieldy. Dropout provides an inexpensive approximation to training and evaluating a bagged ensemble of exponentially many neural networks", "d47636e5-c7ef-4b3e-bc8e-b7a266457954": "For a two-dimensional space, we say that each tiling is o\u21b5set by the displacement vector (1, 1), meaning that it is o\u21b5set from the previous tiling by w the asymmetrically o\u21b5set tilings shown in the lower part of Figure 9.11 are o\u21b5set by a displacement vector of (1, 3).\n\nExtensive studies have been made of the e\u21b5ect of di\u21b5erent displacement vectors on the generalization of tile coding (Parks and Militzer, 1991; An, 1991; An, Miller and Parks, 1991; Miller, An, Glanz and Carter, 1990), assessing their homegeneity and tendency toward diagonal artifacts like those seen for the (1, 1) displacement vectors. Based on this work, Miller and Glanz  recommend using displacement vectors consisting of the \ufb01rst odd integers. In particular, for a continuous space of dimension k, a good choice is to use the \ufb01rst odd integers (1, 3, 5, 7, . , 2k \u2212 1), with n (the number of tilings) set to an integer power of 2 greater than or equal to 4k", "f9feb219-a80b-40b5-9602-99ef54e33773": "A., Hudspeth, A. J. (Eds.) . Principles of Neural Science, Fifth Edition. McGraw-Hill Companies, Inc. Karampatziakis, N., Langford, J. Online importance weight aware updates. ArXiv:1011.1576. Kashyap, R. L., Blaydon, C. C., Fu, K. S. Stochastic approximation. In J. M. Mendel and K. S. Fu (Eds. ), Adaptive, Learning, and Pattern Recognition Systems: Theory and Applications, pp. 329\u2013355. Academic Press, New York. Kearney, A., Veeriah, V, Travnik, J, Sutton, R. S., Pilarski, P. M. (in preparation). TIDBD: Adapting Temporal-di\u21b5erence Step-sizes Through Stochastic Meta-descent", "c0755003-e2de-441e-9ec0-c94ed3318f5e": "(2) A decoder or writer or output RNN is conditioned on that fixed-length vector (just as in figure 10.9) to generate the output sequence Y = (y,...,y). The innovation of this kind of architecture over those presented in earlier sections of this chapter is that the lengths n, and n, can vary from each other, while previous architectures constrained nz = ny = 7. na sequence-to-sequence architecture, the two RNNs are trained jointly to maximize the average of log P(y\u2122,..., y@) | a, ..., a) over all the pairs of # and y sequences in the training set.\n\nThe last state hn, of the encoder RNN is typically used as a representation C' of the input sequence that is provided as input to the decoder RNN. If the context C' is a vector, then the decoder RNN is simply a vector-to- sequence RNN, as described in section 10.2.4. As we have seen, there are at least two ways for a vector-to-sequence RNN to receive input", "78a0fb66-11bb-47c5-8036-82506eee73e3": "The state-value update shown is for n-step TD with importance sampling, and the action-value update is for n-step Q(\u03c3), which generalizes Expected Sarsa and Q-learning. All n-step methods involve a delay of n time steps before updating, as only then are all the required future events known. A further drawback is that they involve more computation per time step than previous methods. Compared to one-step methods, n-step methods also require more memory to record the states, actions, rewards, and sometimes other variables over the last n time steps.\n\nEventually, in Chapter 12, we will see how multi-step TD methods can be implemented with minimal memory and computational complexity using eligibility traces, but there will always be some additional computation beyond one-step methods. Such costs can be well worth paying to escape the tyranny of the single time step. Although n-step methods are more complex than those using eligibility traces, they have the great bene\ufb01t of being conceptually clear. We have sought to take advantage of this by developing two approaches to o\u21b5-policy learning in the n-step case. One, based on importance sampling is conceptually simple but can be of high variance", "12a0cc0d-e962-437e-86ac-17ced33a2f7d": "This implies that h(px) = xh(p) where x is a positive rational number, and hence by continuity when it is a positive real number. Finally, show that this implies h(p) must take the form h(p) \u221d ln p. with equality if, and only if, x and y are statistically independent. 1.32 (\u22c6) Consider a vector x of continuous variables with distribution p(x) and corresponding entropy H. Suppose that we make a nonsingular linear transformation of x to obtain a new variable y = Ax. Show that the corresponding entropy is given by H = H + ln |A| where |A| denotes the determinant of A. 1.33 (\u22c6 \u22c6) Suppose that the conditional entropy H between two discrete random variables x and y is zero. Show that, for all values of x such that p(x) > 0, the variable y must be a function of x, in other words for each x there is only one value of y such that p(y|x) \u0338= 0", "9ca0fd72-f3f4-4a75-9330-4fddee8a9745": "Notice that here each state is a list, or vector, of sensor readings and symbolic inputs, and each action is a vector consisting of a target temperature and a stirring rate. It is typical of reinforcement learning tasks to have states and actions with such structured representations. Rewards, on the other hand, are always single numbers. control the motion of a robot arm in a repetitive pick-and-place task.\n\nIf we want to learn movements that are fast and smooth, the learning agent will have to control the motors directly and have low-latency information about the current positions and velocities of the mechanical linkages. The actions in this case might be the voltages applied to each motor at each joint, and the states might be the latest readings of joint angles and velocities. The reward might be +1 for each object successfully picked up and placed. To encourage smooth movements, on each time step a small, negative reward can be given as a function of the moment-to-moment \u201cjerkiness\u201d of the motion. Exercise 3.1 Devise three example tasks of your own that \ufb01t into the MDP framework, identifying for each its states, actions, and rewards. Make the three examples as di\u21b5erent from each other as possible", "7868b7b5-62c7-4e96-8d3b-8beec18e0299": "Each entry in the matrix Ci; is the cosine similarity between network output vector dimension at index i, 7 and batch index b, Zi; and Zi with a value between -1 (i.e. perfect anti-  correlation) and 1 (i.e. perfect correlation). https://lilianweng.github.io/posts/2021-05-31-contrastive/     Contrastive Representation Learning | Lil'Log  Ler=>o(1-\u00a2;,)? +A SOC,  i i dj invariance term redundancy reduction term  Do ei2h Zoli)? (Ze)? where C;; =  Barlow Twins is competitive with SOTA methods for self-supervised learning. It naturally avoids trivial constants (i.e. collapsed representations), and is robust to different training batch sizes. Algorithm 1 PyTorch-style pseudocode for Barlow Twins", "c748a84e-6ca3-4c69-b64c-3ee0cabcd621": "The idea of autoencoders has been part of the historical landscape of neural networks for decades . Traditionally, autoencoders were used for dimensionality reduction or feature learning. Recently, theoretical connections between autoencoders and latent variable models have brought autoencoders to the forefront of generative modeling, as we will see in chapter 20.\n\nAutoencoders may be thought of as being a special case of feedforward networks and may be trained with all the same techniques, typically minibatch gradient descent following gradients computed by back-propagation. Unlike general feedforward networks, autoencoders may also be trained using recirculation , a learning algorithm based on comparing the activations of the network on the original input  https://www.deeplearningbook.org/contents/autoencoders.html    499  CHAPTER 14. AUTOENCODERS  Figure 14.1: The general structure of an autoencoder, mapping an input x to an output (called reconstruction) r through an internal representation or code h. The autoencoder has two components: the encoder f (mapping x to h) and the decoder g (mapping h tor)", "bdfbfa20-87ca-40b6-82bf-21f2bf5bf7ee": "2018. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471. Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transferable are features in deep neural networks? In Advances in neural information processing systems, pages 3320\u20133328. Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. 2018. QANet: Combining local convolution with global self-attention for reading comprehension. In ICLR. Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. Swag: A large-scale adversarial dataset for grounded commonsense inference. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP). Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015", "c90e4ce5-fa53-45fe-97cc-d51f60cdab57": "into the MAML gradients with one step inner gradient update:  ) (1) _  = gy) \u2014 aH\" 9\u201d + O(a\u201d) dL aH, gl? )_ aT g\\\u00b0) + O(a?) \u2014 aH\u201d) (g\\? \u2014 aT) gl) + O(a?)) gH g \u2014 Hg\u201d 4 aH Hg + O(a2)  pL aH) g(\u00ae \u2014 ag + O(a\u2019) mes: JReptile = g + gv = g) + gs) - aH) g(\u00ae + O(a?) So far we have the formula of three types of gradients:  https://lilianweng.github.io/posts/2018-11-30-|  meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Log   JFOMAML = g) \u2014 aH |\") g) + O(a\u2019)  JMAML = g\\ _ aH |\") g\\) _ aH g? + O(a?) JReptile = g + gi\u201d \u2014 ag) + O(a\u2019)  During training, we often average over multiple data batches", "609a749e-89b6-41d8-af94-a78d525468bf": "Actor\u2013Critic with Eligibility Traces (episodic), for estimating \u21e1\u2713 \u21e1 \u21e1\u21e4 Input: a di\u21b5erentiable policy parameterization \u21e1(a|s, \u2713) Input: a di\u21b5erentiable state-value function parameterization \u02c6v(s,w) Parameters: trace-decay rates \u03bb\u2713 2 , \u03bbw 2 ; step sizes \u21b5\u2713 > 0, \u21b5w > 0 Initialize policy parameter \u2713 2 Rd0 and state-value weights w 2 Rd (e.g., to 0) Loop forever (for each episode): As discussed in Section 10.3, for continuing problems without episode boundaries we need to de\ufb01ne performance in terms of the average rate of reward per time step: where \u00b5 is the steady-state distribution under \u21e1, \u00b5(s) .= limt!1 Pr{St =s|A0:t \u21e0\u21e1}, which is assumed to exist and to be independent of S0 (an ergodicity assumption).\n\nRemember that this is the special distribution under which, if you select actions according to \u21e1, you remain in the same distribution: Complete pseudocode for the actor\u2013critic algorithm in the continuing case (backward view) is given in the box below", "6f145f00-0b7a-4a35-9b22-4860b84af276": "An elegant aspect of this technique is that the equations for evaluating vTH mirror closely those for standard forward and backward propagation, and so the extension of existing software to compute this product is typically straightforward.\n\nIf desired, the technique can be used to evaluate the full Hessian matrix by choosing the vector v to be given successively by a series of unit vectors of the form (0, 0, . , 1, . , 0) each of which picks out one column of the Hessian. This leads to a formalism that is analytically equivalent to the backpropagation procedure of Bishop , as described in Section 5.4.5, though with some loss of ef\ufb01ciency due to redundant calculations. 5.5. Regularization in Neural Networks The number of input and outputs units in a neural network is generally determined by the dimensionality of the data set, whereas the number M of hidden units is a free parameter that can be adjusted to give the best predictive performance. Note that M controls the number of parameters (weights and biases) in the network, and so we might expect that in a maximum likelihood setting there will be an optimum value of M that gives the best generalization performance, corresponding to the optimum balance between under-\ufb01tting and over-\ufb01tting", "2cb2cfd1-5e9d-41ca-9126-dba96a6b519f": "Chapter 10  Sequence Modeling: Recurrent and Recursive Nets  Recurrent neural networks, or RNNs , are a family of neural networks for processing sequential data. Much as a convolutional network is a neural network that is specialized for processing a grid of values X such as an image, a recurrent neural network is a neural network that is specialized for processing a sequence of values w@),...,a(7. Just as convolutional networks can readily scale to images with large width and height, and some convolutional networks can process images of variable size, recurrent networks can scale to much longer sequences than would be practical for networks without sequence-based specialization. Most recurrent networks can also process sequences of variable length. To go from multilayer networks to recurrent networks, we need to take advantage of one of the early ideas found in machine learning and statistical models of the 1980s: sharing parameters across different parts of a model. Parameter sharing makes it possible to extend and apply the model to examples of different forms (different lengths, here) and generalize across them", "2f055389-4f16-4866-8772-36a039ddea0b": "In that way (and by choosing \u03b3(\u00b7) as a constant in all other states) we can recover the classical episodic setting as a special case. State dependent termination includes other prediction cases such as pseudo termination, in which we seek to predict a quantity without altering the \ufb02ow of the Markov process. Discounted returns can be thought of as such a quantity, in which case state dependent termination uni\ufb01es the episodic and discounted-continuing cases. (The undiscounted-continuing case still needs some special treatment.)\n\nThe generalization to variable bootstrapping is not a change in the problem, like discounting, but a change in the solution strategy. The generalization a\u21b5ects the \u03bbreturns for states and actions. The new state-based \u03bb-return can be written recursively as where now we have added the \u201cs\u201d to the superscript \u03bb to remind us that this is a return that bootstraps from state values, distinguishing it from returns that bootstrap from action values, which we present below with \u201ca\u201d in the superscript", "6a59747c-de42-4f85-8864-abdde5503ef4": "The focus so far in this book has been on unsupervised learning, including topics such as density estimation and data clustering. We turn now to a discussion of supervised learning, starting with regression. The goal of regression is to predict the value of one or more continuous target variables t given the value of a D-dimensional vector x of input variables. We have already encountered an example of a regression problem when we considered polynomial curve \ufb01tting in Chapter 1. The polynomial is a speci\ufb01c example of a broad class of functions called linear regression models, which share the property of being linear functions of the adjustable parameters, and which will form the focus of this chapter. The simplest form of linear regression models are also linear functions of the input variables. However, we can obtain a much more useful class of functions by taking linear combinations of a \ufb01xed set of nonlinear functions of the input variables, known as basis functions.\n\nSuch models are linear functions of the parameters, which gives them simple analytical properties, and yet can be nonlinear with respect to the input variables. Given a training data set comprising N observations {xn}, where n = 1,", "add04395-fe9c-4ea4-8006-10f27c2d7fd2": "We again have a sparse solution, and the only terms that have to be evaluated in the predictive model (7.64) are those that involve the support vectors.\n\nThe parameter b can be found by considering a data point for which 0 < an < C, which from (7.67) must have \u03ben = 0, and from (7.65) must therefore satisfy \u03f5 + yn \u2212 tn = 0. Using (7.1) and solving for b, we obtain where we have used (7.57). We can obtain an analogous result by considering a point for which 0 < \ufffdan < C. In practice, it is better to average over all such estimates of b. As with the classi\ufb01cation case, there is an alternative formulation of the SVM for regression in which the parameter governing complexity has a more intuitive interpretation . In particular, instead of \ufb01xing the width \u03f5 of the insensitive region, we \ufb01x instead a parameter \u03bd that bounds the fraction of points lying outside the tube. This involves maximizing It can be shown that there are at most \u03bdN data points falling outside the insensitive tube, while at least \u03bdN data points are support vectors and so lie either on the tube or outside it", "1a3ac030-9143-45cc-a3c0-1471e5c325a8": "Many researchers seemed to believe that they were studying reinforcement learning when they were actually studying supervised learning.\n\nFor example, arti\ufb01cial neural network pioneers such as Rosenblatt  and Widrow and Ho\u21b5  were clearly motivated by reinforcement learning\u2014they used the language of rewards and punishments\u2014but the systems they studied were supervised learning systems suitable for pattern recognition and perceptual learning. Even today, some researchers and textbooks minimize or blur the distinction between these types of learning. For example, some arti\ufb01cial neural network textbooks have used the term \u201ctrial-and-error\u201d to describe networks that learn from training examples. This is an understandable confusion because these networks use error information to update connection weights, but this misses the essential character of trial-and-error learning as selecting actions on the basis of evaluative feedback that does not rely on knowledge of what the correct action should be. Partly as a result of these confusions, research into genuine trial-and-error learning became rare in the 1960s and 1970s, although there were notable exceptions", "d274363c-b5ac-4d17-acdd-258dfa53c1be": "On the other hand, there are other discrepancies between the TD theory and experimental data that are not so easily accommodated by selecting parameter values and stimulus representations (we mention some of these discrepancies in the Bibliographical and Historical Remarks section at the end of this chapter), and more mismatches are likely to be discovered as neuroscientists conduct ever more re\ufb01ned experiments. But the reward prediction error hypothesis has been functioning very e\u21b5ectively as a catalyst for improving our understanding of how the brain\u2019s reward system works. Intricate experiments have been designed to validate or refute predictions derived from the hypothesis, and experimental results have, in turn, led to re\ufb01nement and elaboration of the TD error/dopamine hypothesis.\n\nA remarkable aspect of these developments is that the reinforcement learning algorithms and theory that connect so well with properties of the dopamine system were developed from a computational perspective in total absence of any knowledge about the relevant properties of dopamine neurons\u2014remember, TD learning and its connections to optimal control and dynamic programming were developed many years before any of the experiments were conducted that revealed the TD-like nature of dopamine neuron activity. This unplanned correspondence, despite not being perfect, suggests that the TD error/dopamine parallel captures something signi\ufb01cant about brain reward processes", "e982d8bf-906f-4e2e-be54-195b83abba18": "While various approaches have been proposed to tackle learning with limited labeled data \u2014 including unsupervised pre-training , multi-task learning , semi-supervised learning , and few-shot learning  \u2014 in this work, we focus on and compare different data augmentation methods and their application to supervised and semisupervised learning.\n\nIn this survey, we comprehensively review and perform experiments on recent data augmentation techniques developed for various NLP tasks. Our contributions are three-fold: (1) summarize and categorize recent methods in textual data augmentation; (2) compare different data augmentation methods through experiments with limited labeled data in supervised and semi-supervised settings on 11 NLP tasks, and (3) discuss current challenges and future directions of data augmentation, as well as learning with limited data in NLP more arXiv:2106.07499v1    14 Jun 2021 broadly. Our experimental results allow us to conclude that no single augmentation works best for every task, but (i) token-level augmentations work well for supervised learning, (ii) sentence-level augmentation usually works the best for semisupervised learning, and (iii) augmentation methods can sometimes hurt performance, even in the semi-supervised setting. Related Surveys. Recently, several surveys also explore the data augmentation techniques for NLP", "8cb2ba4e-0efb-4d71-bfac-eeab721965c1": "https://lilianweng.github.io/posts/2021-05-31-contrastive/  Contrastive Representation Learning | Lil'Log  Model STS12 STS13 STS14 STS15 STS16 STS-B_ SICK-R_ Avg. Unsupervised models GloVe embeddings (avg. )* 55.14 7066 59.73 68.25 63.66 58.02 53.76 61.32 BERT\u00bbase (first-last avg.) 39.70 59.38 49.67 66.03 66.19 53.87 62.06 56.70 BERT ase-flow 58.40 67.10 60.85 75.16 71.22 68.66 64.47 66.55 BERT) 2se-whitening 57.83 66.90 60.90 75.08 T1131 68.24 63.73 66.28 IS-BERTyase\u201d 56.77 69.24 61.21 75.23", "163debd7-ae2f-4aff-84e6-b042766db0e2": "2.39 (\u22c6 \u22c6) Starting from the results (2.141) and (2.142) for the posterior distribution of the mean of a Gaussian random variable, dissect out the contributions from the \ufb01rst N \u2212 1 data points and hence obtain expressions for the sequential update of \u00b5N and \u03c32 N. Now derive the same results starting from the posterior distribution p(\u00b5|x1, . .\n\n, xN\u22121) = N(\u00b5|\u00b5N\u22121, \u03c32 N\u22121) and multiplying by the likelihood function p(xN|\u00b5) = N(xN|\u00b5, \u03c32) and then completing the square and normalizing to obtain the posterior distribution after N observations. 2.40 (\u22c6 \u22c6) www Consider a D-dimensional Gaussian random variable x with distribution N(x|\u00b5, \u03a3) in which the covariance \u03a3 is known and for which we wish to infer the mean \u00b5 from a set of observations X = {x1, . , xN}. Given a prior distribution p(\u00b5) = N(\u00b5|\u00b50, \u03a30), \ufb01nd the corresponding posterior distribution p(\u00b5|X)", "bb4c26a9-4ce9-4afc-86cb-47d72e96a490": ":/)  Initialize replay memory D to capacity N Initialize action-value function Q with random weights 0 Initialize target action-value function Q with weights 0\u201d = 0 For episode = 1, M do Initialize sequence s, = {x, } and preprocessed sequence \u00a2, =\u00a2(s,) For t= 1,T do With probability \u00a2 select a random action a, otherwise select a; = argmax, O((s;),a; 0) Execute action a, in emulator and observe reward r, and image x, \u00bb", "d0f6789d-2a2e-49d8-b67d-a487e962050a": "As shown by Tipping and Bishop , probabilistic PCA becomes PCA as o \u2014 0.\n\nIn that case, the conditional expected value of h given x becomes an orthogonal projection of x \u2014 b onto the space spanned by the d columns of W, as in PCA. As o + 0, the density model defined by probabilistic PCA becomes very sharp around these d dimensions spanned by the columns of W. This can make the model assign very low likelihood to the data if the data do not actually cluster near a hyperplane. 13.2 Independent Component Analysis (ICA)  Independent component analysis (ICA) is among the oldest representation learning algorithms . It is an approach to modeling linear factors that seeks to separate an observed signal into many underlying signals that are scaled and added together to form the observed data. These signals are intended to be fully independent, rather than merely decorrelated from each other. ! https://www.deeplearningbook.org/contents/linear_factors.html    Many different specific methodologies are referred to as ICA. The variant that is most similar to the other generative models we have described here is a variant  that trains a fully parametric generative model", "f69a297c-47b7-4b76-b727-2e69a697a6ed": "While it is reasonable to omit batch normalization from the very first baseline, it should be introduced quickly if optimization appears to be problematic. Unless your training set contains tens of millions of examples or more, you should include some mild forms of regularization from the start. Early stopping should be used almost universally. Dropout is an excellent regularizer that is easy to implement and compatible with many models and training algorithms. Batch normalization also sometimes reduces generalization error and allows dropout to be omitted, because of the noise in the estimate of the statistics used to normalize each variable. 420  CHAPTER 11.\n\nPRACTICAL METHODOLOGY  If your task is similar to another task that has been studied extensively, you will probably do well by first copying the model and algorithm that is already known to perform best on the previously studied task. You may even want to copy a trained model from that task. For example, it is common to use the features from a convolutional network trained on ImageNet to solve other computer vision tasks . A common question is whether to begin by using unsupervised learning, de- scribed further in part III. This is somewhat domain specific", "eeb38f56-5bcd-4fc5-a6a5-35daadab189f": "All the involved paradigms of algorithms are perfectly encompassed in the single SE formulation, allowing users to simply tweak and evolve the relevant components for the interpolation. Thus far, we have discussed the standard equation as the uni\ufb01ed objective function. Learning the target model p\u03b8 amounts to optimizing the objective w.r.t the model parameters \u03b8. That is, the standardized objective presents an optimization problem, for which an optimization solver is applied to obtain the target model solution p\u03b8\u2217. This section is devoted to discussion of various optimization algorithms. For a simple objective such as that of the vanilla supervised MLE (Equation 2.1) with a tractable model, stochastic gradient descent can be used to optimize the model parameters \u03b8 straightforwardly. With a more complex model or a more complex objective like the general SE, more sophisticated optimization algorithms are needed, such as the teacher-student procedure exempli\ufb01ed in Equation 3.3", "86c2c7ea-c5f2-4a83-9349-e77a4a743839": "In batch form, TD(0) is faster than Monte Carlo methods because it computes the true certainty-equivalence estimate. This explains the advantage of TD(0) shown in the batch results on the random walk task (Figure 6.2). The relationship to the certaintyequivalence estimate may also explain in part the speed advantage of nonbatch TD(0) (e.g., Example 6.2, page 125, right graph).\n\nAlthough the nonbatch methods do not achieve either the certainty-equivalence or the minimum squared-error estimates, they can be understood as moving roughly in these directions. Nonbatch TD(0) may be faster than constant-\u21b5 MC because it is moving toward a better estimate, even though it is not getting all the way there. At the current time nothing more de\ufb01nite can be said about the relative e\ufb03ciency of online TD and Monte Carlo methods. Finally, it is worth noting that although the certainty-equivalence estimate is in some sense an optimal solution, it is almost never feasible to compute it directly", "3b6d53cb-68b3-4ab5-b294-9f3c47f0d825": "Then a simple weighting would result in the n-step return being zero, which could result in high variance when it was used as a target. Instead, in this more sophisticated approach, one uses an alternate, o\u21b5-policy de\ufb01nition of the n-step return ending at horizon h, as where again Gh:h .= Vh\u22121(Sh). In this approach, if \u21e2t is zero, then instead of the target being zero and causing the estimate to shrink, the target is the same as the estimate and causes no change. The importance sampling ratio being zero means we should ignore the sample, so leaving the estimate unchanged seems appropriate. The second, additional term in (7.13) is called a control variate (for obscure reasons). Notice that the control variate does not change the expected update; the importance sampling ratio has expected value one (Section 5.9) and is uncorrelated with the estimate, so the expected value of the control variate is zero.\n\nAlso note that the o\u21b5-policy de\ufb01nition (7.13) is a strict generalization of the earlier on-policy de\ufb01nition of the n-step return (7.1), as the two are identical in the on-policy case, in which \u21e2t is always 1", "136dcb89-020e-48b7-b760-490905c3d2e2": "The cross entropy and the Gini index are better measures than the misclassi\ufb01cation rate for growing the tree because they are more sensitive to the node probabilities. Also, unlike misclassi\ufb01cation rate, they are differentiable and Exercise 14.11 hence better suited to gradient based optimization methods. For subsequent pruning of the tree, the misclassi\ufb01cation rate is generally used. The human interpretability of a tree model such as CART is often seen as its major strength.\n\nHowever, in practice it is found that the particular tree structure that is learned is very sensitive to the details of the data set, so that a small change to the training data can result in a very different set of splits . There are other problems with tree-based methods of the kind considered in this section. One is that the splits are aligned with the axes of the feature space, which may be very suboptimal. For instance, to separate two classes whose optimal decision boundary runs at 45 degrees to the axes would need a large number of axis-parallel splits of the input space as compared to a single non-axis-aligned split", "47e4ce13-a972-4b35-b080-9c932ca35118": "IEEE Transactions on Systems, Man, and Cybernetics, 17(1):7\u201320. Werbos, P. J. Generalization of back propagation with applications to a recurrent gas Werbos, P. J. Neural networks for control and system identi\ufb01cation.\n\nIn Proceedings of the 28th Conference on Decision and Control, pp. 260\u2013265. IEEE Control Systems Society. modeling. In D. A. White and D. A. Sofge (Eds. ), Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches, pp. 493\u2013525. Van Nostrand Reinhold, New York. Werbos, P. J. The Roots of Backpropagation: From Ordered Derivatives to Neural Networks and Political Forecasting (Vol. 1). John Wiley and Sons. Wiering, M., Van Otterlo, M. Reinforcement Learning: State-of-the-Art", "026e2d28-c429-4070-9cb3-1a0026879556": "Another continuous error function that has sometimes been used to solve classi\ufb01cation problems is the squared error, which is again plotted in Figure 7.5. It has the property, however, of placing increasing emphasis on data points that are correctly classi\ufb01ed but that are a long way from the decision boundary on the correct side. Such points will be strongly weighted at the expense of misclassi\ufb01ed points, and so if the objective is to minimize the misclassi\ufb01cation rate, then a monotonically decreasing error function would be a better choice. The support vector machine is fundamentally a two-class classi\ufb01er. In practice, however, we often have to tackle problems involving K > 2 classes.\n\nVarious methods have therefore been proposed for combining multiple two-class SVMs in order to build a multiclass classi\ufb01er. One commonly used approach  is to construct K separate SVMs, in which the kth model yk(x) is trained using the data from class Ck as the positive examples and the data from the remaining K \u2212 1 classes as the negative examples. This is known as the one-versus-the-rest approach. However, in Figure 4.2 we saw that using the decisions of the individual classi\ufb01ers can lead to inconsistent results in which an input is assigned to multiple classes simultaneously", "ea83b3d5-dd6e-4fb0-8dc1-772b677831aa": "(Recall from Chapter 4 that optimal policies can be greedy with respect to many di\u21b5erent value functions, not just v\u21e4.) Checking for the emergence of an optimal policy before value iteration converges is not a part of the conventional DP algorithm and requires considerable additional computation.\n\nIn the racetrack example, by running many test episodes after each DP sweep, with actions selected greedily according to the result of that sweep, it was possible to estimate the earliest point in the DP computation at which the approximated optimal evaluation function was good enough so that the corresponding greedy policy was nearly optimal. For this racetrack, a close-to-optimal policy emerged after 15 sweeps of value iteration, or after 136,725 value-iteration updates. This is considerably less than the 252,784 updates DP needed to converge to v\u21e4, but still more than the 127,600 updates RTDP required. Although these simulations are certainly not de\ufb01nitive comparisons of the RTDP with conventional sweep-based value iteration, they illustrate some of advantages of on-policy trajectory sampling. Whereas conventional value iteration continued to update the value of all the states, RTDP strongly focused on subsets of the states that were relevant to the problem\u2019s objective", "d9a0c742-3840-4936-950b-7f416522d09b": "Adversarial models may also gain some statistical advantage from the generator network not being updated directly with data examples, but only with gradients \ufb02owing through the discriminator. This means that components of the input are not copied directly into the generator\u2019s parameters.\n\nAnother advantage of adversarial networks is that they can represent very sharp, even degenerate distributions, while methods based on Markov chains require that the distribution be somewhat blurry in order for the chains to be able to mix between modes. This framework admits many straightforward extensions: 1. A conditional generative model p(x | c) can be obtained by adding c as input to both G and D. 2. Learned approximate inference can be performed by training an auxiliary network to predict z given x. This is similar to the inference net trained by the wake-sleep algorithm  but with the advantage that the inference net may be trained for a \ufb01xed generator net after the generator net has \ufb01nished training. 3. One can approximately model all conditionals p(xS | x\u0338S) where S is a subset of the indices of x by training a family of conditional models that share parameters", "af788eab-d29e-4096-a358-6a9f59cc75f9": "https://www.deeplearningbook.org/contents/regularization.html    To gain some insight into the effect of the constraint, we can fix a\u201d and view the problem as just a function of 0:  0* = argmin \u00a3(0, a*) = argmin J(9; X, y) + a*O(8). (7.28) 6 0  This is exactly the same as the regularized training problem of minimizing J. We can thus think of a parameter norm penalty as imposing a constraint on the weights. If 0 is the L? norm, then the weights are constrained to lie in an L? ball. If Q is the L' norm, then the weights are constrained to lie in a region of limited Z' norm. Usually we do not know the size of the constraint region that we impose by using weight decay with coefficient a* because the value of a* does not directly tell us the value of k. In principle, one can solve for k, but the relationship between k and q* depends on the form of J", "5b2fe3f3-f91b-47b0-a1ef-b131d616241a": "Association for Computational Linguistics. Shiyue Zhang and Mohit Bansal. 2019.\n\nAddressing semantic drift in question generation for semisupervised question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2495\u20132509, Hong Kong, China. Association for Computational Linguistics. Wei Emma Zhang, Quan Z Sheng, Ahoud Alhazmi, and Chenliang Li. 2020b. Adversarial attacks on deep-learning models in natural language processing: A survey. ACM Transactions on Intelligent Systems and Technology (TIST), 11(3):1\u201341. Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015a. Character-level convolutional networks for text classi\ufb01cation. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS\u201915, page 649\u2013657, Cambridge, MA, USA. MIT Press. Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015b", "09b31e6d-fea9-4a22-8ab0-0554b4e698cf": "If the agent fails to learn, learns too slowly, or learns the wrong thing, then the designer tweaks the reward signal and tries again. To do this, the designer judges the agent\u2019s performance by criteria that he or she is attempting to translate into a reward signal so that the agent\u2019s goal matches his or her own. And if learning is too slow, the designer may try to design a non-sparse reward signal that e\u21b5ectively guides learning throughout the agent\u2019s interaction with its environment. It is tempting to address the sparse reward problem by rewarding the agent for achieving subgoals that the designer thinks are important way stations to the overall goal.\n\nBut augmenting the reward signal with well-intentioned supplemental rewards may lead the agent to behave very di\u21b5erently from what is intended; the agent may end up not achieving the overall goal at all. A better way to provide such guidance is to leave the reward signal alone and instead augment the value-function approximation with an initial guess of what it should ultimately be, or augment it with initial guesses as to what certain parts of it should be", "5ba078b9-ea91-4cfb-b92f-e8949cffaf38": "Rangel, Camerer, and Montague  review many of the outstanding issues involving habitual, goal-directed, and Pavlovian modes of control.\n\nComments on Terminology\u2014 The traditional meaning of reinforcement in psychology is the strengthening of a pattern of behavior (by increasing either its intensity or frequency) as a result of an animal receiving a stimulus (or experiencing the omission of a stimulus) in an appropriate temporal relationship with another stimulus or with a response. Reinforcement produces changes that remain in future behavior. Sometimes in psychology reinforcement refers to the process of producing lasting changes in behavior, whether the changes strengthen or weaken a behavior pattern . Letting reinforcement refer to weakening in addition to strengthening is at odds with the everyday meaning of reinforce, and its traditional use in psychology, but it is a useful extension that we have adopted here. In either case, a stimulus considered to be the cause of the behavioral change is called a reinforcer. Psychologists do not generally use the speci\ufb01c phrase reinforcement learning as we do. Animal learning pioneers probably regarded reinforcement and learning as being synonymous, so it would be redundant to use both words. Our use of the phrase follows its use in computational and engineering research, in\ufb02uenced mostly by Minsky", "e0261156-19e0-45af-b711-4d6aca3d9977": "We might be tempted be to build a K-class discriminant by combining a number of two-class discriminant functions. However, this leads to some serious dif\ufb01culties  as we now show.\n\nConsider the use of K\u22121 classi\ufb01ers each of which solves a two-class problem of separating points in a particular class Ck from points not in that class. This is known as a one-versus-the-rest classi\ufb01er. The left-hand example in Figure 4.2 shows an biguous regions, shown in green. On the left is an example involving the use of two discriminants designed to distinguish points in class Ck from points not in class Ck. On the right is an example involving three discriminant functions each of which is used to separate a pair of classes Ck and Cj. example involving three classes where this approach leads to regions of input space that are ambiguously classi\ufb01ed. An alternative is to introduce K(K \u2212 1)/2 binary discriminant functions, one for every possible pair of classes. This is known as a one-versus-one classi\ufb01er. Each point is then classi\ufb01ed according to a majority vote amongst the discriminant functions", "18d992d8-00ac-4def-b848-1bbe092949e3": "(3.27)  The Dirac delta function is defined such that it is zero valued everywhere except 0, yet integrates to 1. The Dirac delta function is not an ordinary function that associates each value x with a real-valued output; instead it is a different kind of  63  https://www.deeplearningbook.org/contents/prob.html    CHAPTER 3. PROBABILITY AND INFORMATION THEORY  mathematical object called a generalized function that is defined in terms of its properties when integrated. We can think of the Dirac delta function as being the limit point of a series of functions that put less and less density on all points other than zero. By defining p(x) to be 6 shifted by \u2014y we obtain an infinitely narrow and infinitely high peak of probability density where x = p.  A common use of the Dirac delta distribution is as a component of an empirical  distribution, 1< ; i(~) = \u2014 (a \u2014 a 3.28 Ale) = 7, le -20) (3.28) which puts probability mass 4 on each of the m points ao, al\u2122) forming  a given data set or collection of samples", "ef42ffb8-283f-4a21-811e-918647f1752b": "Multi-class support vector machines. In M. Verlysen (Ed. ), Proceedings ESANN\u201999, Brussels. D-Facto Publications. Whittaker, J. Graphical Models in Applied Multivariate Statistics. Wiley. Widrow, B. and M. E. Hoff . Adaptive switching circuits. In IRE WESCON Convention Record, Volume 4, pp. 96\u2013104. Reprinted in Anderson and Rosenfeld . Widrow, B. and M. A. Lehr . 30 years of adaptive neural networks: perceptron, madeline, and backpropagation. Proceedings of the IEEE 78(9), 1415\u20131442. Wiegerinck, W. and T. Heskes . Fractional belief propagation. In S. Becker, S. Thrun, and K. Obermayer (Eds", "b57d52de-2918-4374-b0b7-41665b795524": "Reinforcement learning algorithm for partially observable Markov decision problems.\n\nIn Advances in Neural Information Processing Systems 7 , pp. 345\u2013352. MIT Press, Cambridge, MA. Jacobs, R. A. Increased rates of convergence through learning rate adaptation. Neural Jaderberg, M., Mnih, V., Czarnecki, W. M., Schaul, T., Leibo, J. Z., Silver, D., Kavukcuoglu, K. Reinforcement learning with unsupervised auxiliary tasks. ArXiv:1611.05397. Jaeger, H. Observable operator models and conditioned continuation representations. ArJaeger, H. Discrete Time, Discrete Valued Observable Operator Models: A Tutorial. Jaeger, H. Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the \u2018echo state network\u2019 approach. German National Research Center for Information Technology, Technical Report GMD report 159, 2002", "40b9c950-787c-4452-8dcf-1b7818990fb6": "In order to use the data from b we must take into account the di\u21b5erence between the two policies, using their relative probability of taking the actions that were taken (see Section 5.5). In n-step methods, returns are constructed over n steps, so we are interested in the relative probability of just those n actions. For example, to make a simple o\u21b5-policy version of n-step TD, the update for time t (actually made at time t + n) can simply be weighted by \u21e2t:t+n\u22121: where \u21e2t:t+n\u22121, called the importance sampling ratio, is the relative probability under the two policies of taking the n actions from At to At+n\u22121 (cf. Eq.\n\n5.3): For example, if any one of the actions would never be taken by \u21e1 (i.e., \u21e1(Ak|Sk) = 0) then the n-step return should be given zero weight and be totally ignored. On the other hand, if by chance an action is taken that \u21e1 would take with much greater probability than b does, then this will increase the weight that would otherwise be given to the return", "12ca26f3-7696-4470-8921-bdfb1653c6d4": "It is plausible that f could be expanding in-between or far from the data manifolds (see, for example, what happens in the 1-D toy example of figure 14.7).\n\nWhen the  ala - - - - - - oe -  https://www.deeplearningbook.org/contents/autoencoders.html    s(n) penalty is applied to sigmoidal units, one easy way to shrink the Jacobian is to make the sigmoid units saturate to 0 or 1. This encourages the CAE to encode  input points with extreme values of the sigmoid, which may be interpreted as a binary code. It also ensures that the CAE will spread its code values throughout  most of the hypercube that its sigmoidal hidden units can span. We can think of the Jacobian matrix J at a point x as approximating the nonlinear encoder f(a) as being a linear operator. This allows us to use the word \u201ccontractive\u201d more formally. In the theory of linear operators, a linear operator is said to be contractive if the norm of Jz remains less than or equal to 1 for all unit-norm x", "6bcd01be-9bc4-48c3-8081-1521c7c360b2": "In principle, the SL or RL policy networks could have been used in the rollouts, but the forward propagation through these deep networks took too much time for either of them to be used in rollout simulations, a great many of which had to be carried out for each move decision during live play. For this reason, the rollout policy network was less complex than the other policy networks, and its input features could be computed more quickly than the features used for the policy networks. The rollout policy network allowed approximately 1,000 complete game simulations per second to be run on each of the processing threads that AlphaGo used. One may wonder why the SL policy was used instead of the better RL policy to select actions in the expansion phase of APV-MCTS. These policies took the same amount of time to compute because they used the same network architecture. The team actually found that AlphaGo played better against human opponents when APV-MCTS used as the SL policy instead of the RL policy. They conjectured that the reason for this was that the latter was tuned to respond to optimal moves rather than to the broader set of moves characteristic of human play.\n\nInterestingly, the situation was reversed for the value function used by APV-MCTS", "be116caa-e3e6-44aa-a579-430f8b9782fc": "Deep Q-learning  parameterizes the Q-function as Q\u03b8(x, a), and train the parameters by minimizing the following regression objective L(\u03b8) based on the Bellman temporal consistency: where \u00af\u03b8 is the parameters of the target Q-network, which is a slow copy of \u03b8 and considered as constant for gradient computation of \u03b8. Here \u03c0\u2032 is an behavior policy which can be an arbitrary distribution over text, such as the data distribution or replay buffer . This makes Q-learning an off-policy algorithm because of its ability to use samples coming from other policies. After learning Q\u03b8, one can induce a policy \u03c0 from it that takes arg maxa Q\u03b8(s, a) at each state s. Jaques et al.\n\ninstead sample tokens from the softmax function applied to Q\u03b8. However, the training can be unstable and inef\ufb01cient due to several challenges: (1) The bootstrapping nature of the above regression problem can make the training unstable. That is, the regression target rt + \u03b3 maxat+1 Q\u00af\u03b8(st+1, at+1) itself is derived from the Q-function to be learned", "10db9377-3aca-4830-814a-7d9498e5cdda": "Jiaao Chen, Zichao Yang, and Diyi Yang. 2020c. MixText: Linguistically-informed interpolation of hidden space for semi-supervised text classi\ufb01cation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2147\u2013 2157, Online. Association for Computational Linguistics. Luoxin Chen, Weitong Ruan, Xinyue Liu, and Jianhua Lu. 2020d. SeqVAT: Virtual adversarial training for semi-supervised sequence labeling. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8801\u20138811, Online. Association for Computational Linguistics. Mingda Chen, Qingming Tang, Sam Wiseman, and Kevin Gimpel. 2019. Controllable paraphrase generation with a syntactic exemplar. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5972\u20135984", "0e611dec-1b81-4714-901f-021b345ae15a": "(b) The same data set but with three additional outlying data points showing how the Gaussian (green curve) is strongly distorted by the outliers, whereas the t-distribution (red curve) is relatively unaffected. outliers is much less signi\ufb01cant for the t-distribution than for the Gaussian. Outliers can arise in practical applications either because the process that generates the data corresponds to a distribution having a heavy tail or simply through mislabelled data. Robustness is also an important property for regression problems. Unsurprisingly, the least squares approach to regression does not exhibit robustness, because it corresponds to maximum likelihood under a (conditional) Gaussian distribution", "9e0d104c-bdbb-4653-ac4d-529538d00520": "The scaling factor a adjusts the total magnitude of the simple cell\u2019s response, while 6, and By control how quickly its receptive field falls off. 363  https://www.deeplearningbook.org/contents/convnets.html    CHAPTER 9. CONVOLUTIONAL NETWORKS  The cosine factor cos( fx\u2019 +) controls how the simple cell responds to changing brightness along the x\u2019 axis. The parameter f controls the frequency of the cosine, and \u00a2 controls its phase offset.\n\nAltogether, this cartoon view of simple cells means that a simple cell responds to a specific spatial frequency of brightness in a specific direction at a specific location. Simple cells are most excited when the wave of brightness in the image has the same phase as the weights. This occurs when the image is bright where the weights are positive and dark where the weights are negative. Simple cells are most inhibited when the wave of brightness is fully out of phase with the weights\u2014when the image is dark where the weights are positive and bright where the weights are negative", "25ef8ea6-a384-49cb-8995-0f4021062586": "4.19 (\u22c6) www Write down expressions for the gradient of the log likelihood, as well as the corresponding Hessian matrix, for the probit regression model de\ufb01ned in Section 4.3.5.\n\nThese are the quantities that would be required to train such a model using IRLS. 4.20 (\u22c6 \u22c6) Show that the Hessian matrix for the multiclass logistic regression problem, de\ufb01ned by (4.110), is positive semide\ufb01nite. Note that the full Hessian matrix for this problem is of size MK \u00d7 MK, where M is the number of parameters and K is the number of classes. To prove the positive semide\ufb01nite property, consider the product uTHu where u is an arbitrary vector of length MK, and then apply Jensen\u2019s inequality. 4.23 (\u22c6 \u22c6) www In this exercise, we derive the BIC result (4.139) starting from the Laplace approximation to the model evidence given by (4.137). Show that if the prior over parameters is Gaussian of the form p(\u03b8) = N(\u03b8|m, V0), the log model evidence under the Laplace approximation takes the form where H is the matrix of second derivatives of the log likelihood ln p(D|\u03b8) evaluated at \u03b8MAP", "e993797f-aef7-4287-b333-86c740555cf1": "We are obligated then to say which states we care most about. We must specify a state distribution \u00b5(s) \u2265 0, P about the error in each state s. By the error in a state s we mean the square of the di\u21b5erence between the approximate value \u02c6v(s,w) and the true value v\u21e1(s). Weighting this over the state space by \u00b5, we obtain a natural objective function, the Mean Squared Value Error, denoted VE: The square root of this measure, the root VE, gives a rough measure of how much the approximate values di\u21b5er from the true values and is often used in plots. Often \u00b5(s) is chosen to be the fraction of time spent in s. Under on-policy training this is called the on-policy distribution; we focus entirely on this case in this chapter. In continuing tasks, the on-policy distribution is the stationary distribution under \u21e1.\n\nIn an episodic task, the on-policy distribution is a little di\u21b5erent in that it depends on how the initial states of episodes are chosen", "21071803-fb99-4bab-8ee5-e4ce386c8224": "Importance sam- pling may also be used to improve the estimate of the gradient of the cost function used to train model parameters with stochastic gradient descent, particularly for models, such as classifiers, in which most of the total value of the cost function comes from a small number of misclassified examples. Sampling more difficult examples more frequently can reduce the variance of the gradient in such cases . 17.3.\n\nMarkov Chain Monte Carlo Methods  In many cases, we wish to use a Monte Carlo technique but there is no tractable method for drawing exact samples from the distribution Pmodel(x) or from a good (low variance) importance sampling distribution q(x). In the context of deep learning, this most often happens when pyode|(X) is represented by an undirected model. In these cases, we introduce a mathematical tool called a Markov chain to approximately sample from pyodei (x). The family of algorithms that use Markov chains to perform Monte Carlo estimates is called Markov chain Monte Carlo methods (MCMC). Markov chain Monte Carlo methods for machine learning are described at greater length in Koller and Friedman", "bdba8548-e81e-4132-b11a-d361d6f75f8d": "SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  digital computer read from or write to a specific address.\n\nIt is difficult to optimize functions that produce exact integer addresses. To alleviate this problem, NTMs actually read to or write from many memory cells simultaneously. To read, they take a weighted average of many cells. To write, they modify multiple cells by different amounts. The coefficients for these operations are chosen to be focused on a small number of cells, for example, by producing them via a softmax function. Using these weights with nonzero derivatives enables the functions controlling access to the memory to be optimized using gradient descent. The gradient on these coefficients indicates whether each of them should  https://www.deeplearningbook.org/contents/rnn.html    be creased or decreased, but the gradient will typically be large only tor those memory addresses receiving a large coefficient. These memory cells are typically augmented to contain a vector, rather than the single scalar stored by an LSTM or GRU memory cell. There are two reasons  to increase the size of the memory cell. One reason is that we have increased the cost of accessing a memory cell", "ad18454e-14e5-47a7-a0f1-cda918894cea": "One di\u21b5erence is that the pseudocode uses the compact expression r ln \u21e1(At|St, \u2713t) for the fractional vector r\u21e1(At|St,\u2713t) these two expressions for the vector are equivalent follows from the identity r ln x = rx This vector has been given several names and notations in the literature; we will refer to it simply as the eligibility vector.\n\nNote that it is the only place that the policy REINFORCE: Monte-Carlo Policy-Gradient Control (episodic) for \u21e1\u21e4 Generate an episode S0, A0, R1, . , ST \u22121, AT \u22121, RT , following \u21e1(\u00b7|\u00b7, \u2713) Loop for each step of the episode t = 0, 1, . , T \u2212 1: The second di\u21b5erence between the pseudocode update and the REINFORCE update equation (13.8) is that the former includes a factor of \u03b3t. This is because, as mentioned earlier, in the text we are treating the non-discounted case (\u03b3 =1) while in the boxed algorithms we are giving the algorithms for the general discounted case", "1a2b6a46-60b4-4072-a8e0-43f57460a16e": "The remaining two plots in Figure 1.11 show the corresponding histogram estimates of p(X) and p(X|Y = 1). Let us now return to our example involving boxes of fruit. For the moment, we shall once again be explicit about distinguishing between the random variables and their instantiations. We have seen that the probabilities of selecting either the red or the blue boxes are given by respectively. Note that these satisfy p(B = r) + p(B = b) = 1. Now suppose that we pick a box at random, and it turns out to be the blue box. Then the probability of selecting an apple is just the fraction of apples in the blue box which is 3/4, and so p(F = a|B = b) = 3/4.\n\nIn fact, we can write out all four conditional probabilities for the type of fruit, given the selected box takes two possible values. The top left \ufb01gure shows a sample of 60 points drawn from a joint probability distribution over these variables", "df023886-2524-4d1d-9a50-b9df00e8030a": "Dopamine neurons receive information from many di\u21b5erent brain areas, so the input to the SNpc and VTA labeled \u2018Reward\u2019 in Figure 15.5b should be thought of as vector of reward-related information arriving to neurons in these nuclei along multiple input channels. What the theoretical scalar reward signal Rt might correspond to, then, is the net contribution of all reward-related information to dopamine neuron activity. It is the result of a pattern of activity across many neurons in di\u21b5erent areas of the brain. Although the actor\u2013critic neural implementation illustrated in Figure 15.5b may be correct on some counts, it clearly needs to be re\ufb01ned, extended, and modi\ufb01ed to qualify as a full-\ufb02edged model of the function of the phasic activity of dopamine neurons. The Historical and Bibliographic Remarks section at the end of this chapter cites publications that discuss in more detail both empirical support for this hypothesis and places where it falls short", "c3fbef67-fbf1-40ea-ae93-68e638b23a34": "Recall that these ratios multiply the step size in SGD methods, so high variance means taking steps that vary greatly in their sizes. This is problematic for SGD because of the occasional very large steps. They must not be so large as to take the parameter to a part of the space with a very di\u21b5erent gradient. SGD methods rely on averaging over multiple steps to get a good sense of the gradient, and if they make large moves from single samples they become unreliable. If the step-size parameter is set small enough to prevent this, then the expected step can end up being very small, resulting in very slow learning. The notions of momentum , of Polyak-Ruppert averaging , or further extensions of these ideas may signi\ufb01cantly help. Methods for adaptively setting separate step sizes for di\u21b5erent components of the parameter vector are also pertinent (e.g., Jacobs, 1988; Sutton, 1992b, c), as are the \u201cimportance weight aware\u201d updates of Karampatziakis and Langford .\n\nIn Chapter 5 we saw how weighted importance sampling is signi\ufb01cantly better behaved, with lower variance updates, than ordinary importance sampling", "f455fe10-fdd6-4d5f-b468-ed56e8db492a": "Exercise 12.3 Some insight into how TD(\u03bb) can closely approximate the o\u270fine \u03bb-return algorithm can be gained by seeing that the latter\u2019s error term (in brackets in (12.4)) can be written as the sum of TD errors (12.6) for a single \ufb01xed w. Show this, following the pattern of (6.6), and using the recursive relationship for the \u03bb-return you obtained in Exercise 12.1. \u21e4 Exercise 12.4 Use your result from the preceding exercise to show that, if the weight updates over an episode were computed on each step but not actually used to change the weights (w remained \ufb01xed), then the sum of TD(\u03bb)\u2019s weight updates would be the same as the sum of the o\u270fine \u03bb-return algorithm\u2019s updates.\n\n\u21e4 The o\u270fine \u03bb-return algorithm is an important ideal, but it is of limited utility because it uses the \u03bb-return (12.2), which is not known until the end of the episode. In the continuing case, the \u03bb-return is technically never known, as it depends on n-step returns for arbitrarily large n, and thus on rewards arbitrarily far in the future", "2bf0fa0c-4215-4daa-aeb1-e08f20b5243b": "We therefore wish to evaluate the predictive distribution p(t|x, x, t).\n\nHere we shall assume that the parameters \u03b1 and \u03b2 are \ufb01xed and known in advance (in later chapters we shall discuss how such parameters can be inferred from data in a Bayesian setting). A Bayesian treatment simply corresponds to a consistent application of the sum and product rules of probability, which allow the predictive distribution to be written in the form Here p(t|x, w) is given by (1.60), and we have omitted the dependence on \u03b1 and \u03b2 to simplify the notation. Here p(w|x, t) is the posterior distribution over parameters, and can be found by normalizing the right-hand side of (1.66). We shall see in Section 3.3 that, for problems such as the curve-\ufb01tting example, this posterior distribution is a Gaussian and can be evaluated analytically. Similarly, the integration in (1.68) can also be performed analytically with the result that the predictive distribution is given by a Gaussian of the form where the mean and variance are given by where I is the unit matrix, and we have de\ufb01ned the vector \u03c6(x) with elements \u03c6i(x) = xi for i = 0,", "f1cd7596-d97b-45e9-aa15-756700e26290": "Use these formulae to highlight the difference between Bayesian averaging of different models and the use of latent variables within a single model.\n\n14.2 (\u22c6) The expected sum-of-squares error EAV for a simple committee model can be de\ufb01ned by (14.10), and the expected error of the committee itself is given by (14.11). Assuming that the individual errors satisfy (14.12) and (14.13), derive the result (14.14). 14.3 (\u22c6) www By making use of Jensen\u2019s inequality (1.115), for the special case of the convex function f(x) = x2, show that the average expected sum-of-squares error EAV of the members of a simple committee model, given by (14.10), and the expected error ECOM of the committee itself, given by (14.11), satisfy 14.4 (\u22c6 \u22c6) By making use of Jensen\u2019s in equality (1.115), show that the result (14.54) derived in the previous exercise hods for any error function E(y), not just sum-ofsquares, provided it is a convex function of y", "4db4305a-8219-407f-9e2d-e799da18f400": "Another example is pixel-wise segmentation of images, where the computer program assigns every pixel in an image to a specific category. 99  CHAPTER 5. MACHINE LEARNING BASICS  For example, deep learning can be used to annotate the locations of roads in aerial photographs . The output form need not mirror the structure of the input as closely as in these annotation-style tasks. For example, in image captioning, the computer program observes an image and outputs a natural language sentence describing the image . These tasks are called structured output tasks because the program must output several values that are all tightly interrelated. For example, the words produced by an image captioning program must form a valid sentence. Anomaly detection: In this type of task, the computer program sifts through a set of events or objects and flags some of them as being unusual or atypical. An example of an anomaly detection task is credit card fraud detection. By modeling your purchasing habits, a credit card company can detect misuse of your cards", "11b4bbc9-d5cb-4f4a-bf04-30fb006a3d41": "An approach related to supervised pretraining extends the idea to the context of transfer learning: Yosinski et al.\n\npretrain a deep convolutional net with eight layers of weights on a set of tasks (a subset of the 1,000 ImageNet object categories) and then initialize a same-size network with the first & layers of the first net. All the layers of the second network (with the upper layers initialized randomly) are then jointly trained to perform a different set of tasks (another subset of the 1,000 ImageNet object categories), with fewer training examples than for the first set of tasks. Other approaches to transfer learning with neural networks are discussed in section 15.2. Another related line of work is the FitNets  approach. This approach begins by training a network that has low enough depth and great enough width (number of units per layer) to be easy to train. This network then becomes a teacher for a second network, designated the student. The student network is much deeper and thinner (eleven to nineteen layers) and would be difficult to train with SGD under normal circumstances", "ec95349e-f471-4cd0-b24c-8560d1c260e8": "(6.4) p  https://www.deeplearningbook.org/contents/mlp.html    J e=| % ], (6.5) 1 w=] 4 ]> (6.6)  and b= 0. We can now walk through how the model processes a batch of inputs. Let X be the design matrix containing all four points in the binary input space, with one  loa  X = | |. (6.7)  Lia]  The first step in the neural network is to multiply the input matrix by the first layer\u2019s weight matrix:  example per row:  0 0 1 1 xXW=|,, (6.8) 2 2 Next, we add the bias vector c, to obtain 0 -l 1 O 1 O (6.9) 2 1 171  CHAPTER 6.\n\nDEEP FEEDFORWARD NETWORKS  In this space, all the examples lie along a line with slope 1. As we move along this line, the output needs to begin at 0, then rise to 1, then drop back down to 0. A linear model cannot implement such a function", "cc81610f-c89c-4908-b4f9-909fc5d1e463": "military: the cavalry battalion a This essay discusses the potential for the development of a small infantry brigade as an infantry regiment. It is also a contribution to the larger cavalry corps as it would require a larger brigade for battle. For more information see the original article on this page. legal: In summary Currently: In 1966 the Act was amended into state of law through amendments.\\n\\n\\n Defent No.\n\n1 etc 695 [The character in question for judicial decision purposes; participation t concerned you; \"but not acceptance. \")\\n\\n Generally held: Just politics: In summary Senate candidates, senator (Republican); senator (Democrat); and opinion-former (2002-08). - 2012 Senate results are based on the federal Election Commission\u2019s October 2016 Current Opinion Polling Reports. Key \ufb01gures : Open Gallup poll Most Americans view the computers: In summary: 12-16 add-on chips", "8613a2e7-1241-4faf-a4be-d95b5699ed1b": "As an illustration we consider the case of a single input variable x in which f(x, t) is given by a zero-mean isotropic Gaussian over the variable z = (x, t) with variance \u03c32. The corresponding conditional distribution (6.48) is given by a Gaussian mixture, and is shown, together with the conditional mean, for the sinusoidal Exercise 6.18 An obvious extension of this model is to allow for more \ufb02exible forms of Gaussian components, for instance having different variance parameters for the input and target variables.\n\nMore generally, we could model the joint distribution p(t, x) using a Gaussian mixture model, trained using techniques discussed in Chapter 9 , and then \ufb01nd the corresponding conditional distribution p(t|x). In this latter case we no longer have a representation in terms of kernel functions evaluated at the training set data points. However, the number of components in the mixture model can be smaller than the number of training set points, resulting in a model that is faster to evaluate for test data points. We have thereby accepted an increased computational cost during the training phase in order to have a model that is faster at making predictions", "30271fed-2ff4-4d33-9103-2229c2e05b90": "The loss function for maximum  179  CHAPTER 6.\n\nDEEP FEEDFORWARD NETWORKS  likelihood learning of a Bernoulli parametrized by a sigmoid is  J(8) = \u2014log Ply | 2) (6.24) = \u2014 logo ((2y \u2014 1)z) (6.25) =\u00a2((1\u2014 2y)z). (6.26)  This derivation makes use of some properties from section 3.10. By rewriting the loss in terms of the softplus function, we can see that it saturates only when (1 \u2014 2y)z is very negative. Saturation thus occurs only when the model already has the right answer\u2014when y = 1 and z is very positive, or y = 0 and z is very negative. When z has the wrong sign, the argument to the softplus function, (1 \u20142y)z, may be simplified to |z|. As |z| becomes large while z has the wrong sign, the softplus function asymptotes toward simply returning its argument |z|. The derivative with respect to z asymptotes to sign(z), so, in the limit of extremely incorrect z, the softplus function does not shrink the gradient at all", "2fdd45f3-0b45-43f9-9e84-6c34cfb502fa": "In this fragment of the lattice, we see that the quantity \u03b2(zn1) is obtained by taking the components \u03b2(zn+1,k) of \u03b2(zn+1) at step n + 1 and summing them up with weights given by the products of A1k, corresponding to the values of p(zn+1|zn) and the corresponding values of the emission density p(xn|zn+1,k). Making use of the de\ufb01nition (13.35) for \u03b2(zn), we then obtain Note that in this case we have a backward message passing algorithm that evaluates \u03b2(zn) in terms of \u03b2(zn+1). At each step, we absorb the effect of observation xn+1 through the emission probability p(xn+1|zn+1), multiply by the transition matrix p(zn+1|zn), and then marginalize out zn+1. This is illustrated in Figure 13.13. Again we need a starting condition for the recursion, namely a value for \u03b2(zN)", "8172bd36-8e50-4a19-9605-6aa7ef6ed303": "Note that in a practical implementation, new variables \u03b7j de\ufb01ned by are introduced, and the minimization is performed with respect to the \u03b7j. This ensures that the parameters \u03c3j remain positive.\n\nIt also has the effect of discouraging pathological solutions in which one or more of the \u03c3j goes to zero, corresponding to a Gaussian component collapsing onto one of the weight parameter values. Such solutions are discussed in more detail in the context of Gaussian mixture models in Section 9.2.1. For the derivatives with respect to the mixing coef\ufb01cients \u03c0j, we need to take account of the constraints \ufffd which follow from the interpretation of the \u03c0j as prior probabilities. This can be done by expressing the mixing coef\ufb01cients in terms of a set of auxiliary variables {\u03b7j} using the softmax function given by in which the Cartesian coordinates (x1, x2) of the end effector are determined uniquely by the two joint angles \u03b81 and \u03b82 and the (\ufb01xed) lengths L1 and L2 of the arms. This is know as the forward kinematics of the arm", "652e7be0-445f-4b15-800c-ceea712a557c": "The objective is in the same form with the RL-as-inference formalism of policy optimization . Intuitively, the objective maximizes the expected reward under q, and enforces the model p\u03b8 to stay close to q, with a maximum entropy regularization over q. The problem is solved with an EM procedure that optimizes q and \u03b8 alternatingly: where Z is the normalization term. With the established framework, it is easy to show that the above optimization procedure reduces to maximum likelihood learning by taking \u03b1 \u2192 0, \u03b2 = 1, and the reward function: That is, a sample (x, y) receives a unit reward only when it matches a training example in the dataset, while the reward is negative in\ufb01nite in all other cases. To make the equivalence to maximum likelihood learning clearer, note that the above M-step now reduces to where the joint distribution p(x) exp{R\u03b4}/Z equals the empirical data distribution, which means the M-step is in fact maximizing the data log-likelihood of the model p\u03b8", "f50abbb5-d575-4c57-837c-976957a9abb8": "Afterstates are useful when we have knowledge of an initial part of the environment\u2019s dynamics but not necessarily of the full dynamics. For example, in games we typically know the immediate e\u21b5ects of our moves. We know for each possible chess move what the resulting position will be, but not how our opponent will reply. Afterstate value functions are a natural way to take advantage of this kind of knowledge and thereby produce a more e\ufb03cient learning method.\n\nThe reason it is more e\ufb03cient to design algorithms in terms of afterstates is apparent from the tic-tac-toe example. A conventional action-value function would map from positions and moves to an estimate of the value. But many position\u2013move pairs produce the same resulting position, as in the example below: In such cases the position\u2013move pairs are di\u21b5erent but produce the same \u201cafterposition,\u201d and thus must have the same value. A conventional action-value function would have to separately assess both pairs, whereas an afterstate value function would immediately assess both equally. Any learning about the position\u2013move pair on the left would immediately transfer to the pair on the right. Afterstates arise in many tasks, not just games", "bbdf89d2-b447-46a7-b3cd-5e573ff8881a": "(d \u00a9 v) + by\u2019)  Because P will be normalized, we can safely ignore multiplication by factors that are constant with respect to y:  Ponsemble(Y =yl|v) x on Il exp(W,|(d Ov)+ by) (7.64) de{0,1}\" 1 =exp|s, DY Wy(dov) +by (7.65) de{0,1}\" = exp (5 Wi .vt+ by) ; (7.66)  Substituting this back into equation 7.58, we obtain a softmax classifier with weights 3W. The weight scaling rule is also exact in other settings, including regression networks with conditionally normal outputs as well as deep networks that have hidden layers without nonlinearities. However, the weight scaling rule is only an approximation for deep models that have nonlinearities. Though the approximation has not been theoretically characterized, it often works well, empirically. Goodfellow et al.\n\nfound experimentally that the weight scaling approximation can work better (in terms of classification accuracy) than Monte Carlo approximations to the ensemble predictor", "7f6a840f-6ba6-462e-9c51-b4f370d1e269": ", M. We see that the variance, as well as the mean, of the predictive distribution in (1.69) is dependent on x. The \ufb01rst term in (1.71) represents the uncertainty in the predicted value of t due to the noise on the target variables and was expressed already in the maximum likelihood predictive distribution (1.64) through \u03b2\u22121 ML.\n\nHowever, the second term arises from the uncertainty in the parameters w and is a consequence of the Bayesian treatment. The predictive distribution for the synthetic sinusoidal regression problem is illustrated in Figure 1.17. In our example of polynomial curve \ufb01tting using least squares, we saw that there was an optimal order of polynomial that gave the best generalization. The order of the polynomial controls the number of free parameters in the model and thereby governs the model complexity. With regularized least squares, the regularization coef\ufb01cient \u03bb also controls the effective complexity of the model, whereas for more complex models, such as mixture distributions or neural networks there may be multiple parameters governing complexity. In a practical application, we need to determine the values of such parameters, and the principal objective in doing so is usually to achieve the best predictive performance on new data", "923ccd94-d975-4ce7-9c47-da92eecc9ba3": "As we will show later in Theorem 2, \u03b4 gives the same topology as the Jensen-Shannon divergence, pointing to the fact that the JS is a very strong distance, and is thus more propense to give a discontinuous loss function. Now, all dual spaces (such as Cb(X)\u2217 and thus Prob(X)) have a strong topology (induced by the norm), and a weak* topology. As the name suggests, the weak* topology is much weaker than the strong topology.\n\nIn the case of Prob(X), the strong topology is given by the total variation distance, and the weak* topology is given by the Wasserstein distance (among others) . Assumption 1. Let g : Z \u00d7Rd \u2192 X be locally Lipschitz between \ufb01nite dimensional vector spaces. We will denote g\u03b8(z) it\u2019s evaluation on coordinates (z, \u03b8). We say that g satis\ufb01es assumption 1 for a certain probability distribution p over Z if there are local Lipschitz constants L(\u03b8, z) such that Proof of Theorem 1", "21ab1919-e729-43e6-8227-d0227d283c81": "This stage is essentially drawing a sample from the RBM defined by the top two hidden layers. We can then use a single pass of ancestral sampling through the rest of the model to draw a sample from the visible units. Deep belief networks incur many of the problems associated with both directed models and undirected models. Inference in a deep belief network is intractable because of the explaining away effect within each directed layer and the interaction between the two hidden layers that have undirected connections. Evaluating or maximizing the standard evidence  Tannen Lae-- dd ne AL Ta Wet dd te 21a tenet nt abla Lann-enn thn net dn 1  https://www.deeplearningbook.org/contents/generative_models.html    1OUWEL VUULIU ULI LIC 1O8~ LBCLLOOU ib aldU IULLACLADVIE, VUCCaAUSE LUC CVIUCLICE 1OWEL bound takes the expectation of cliques whose size is equal to the network width", "ef02616d-430b-43ab-bd25-5bfaf500720f": "Nevertheless, it provides a good exercise in the use of variational methods and will also lay the foundation for variational treatment of Bayesian logistic regression in Section 10.6. Recall that the likelihood function for w, and the prior over w, are given by where \u03c6n = \u03c6(xn). We now introduce a prior distribution over \u03b1. From our discussion in Section 2.3.6, we know that the conjugate prior for the precision of a Gaussian is given by a gamma distribution, and so we choose This can be represented as a directed graphical model as shown in Figure 10.8. Our \ufb01rst goal is to \ufb01nd an approximation to the posterior distribution p(w, \u03b1|t). To do this, we employ the variational framework of Section 10.1, with a variational posterior distribution given by the factorized expression We can \ufb01nd re-estimation equations for the factors in this distribution by making use of the general result (10.9). Recall that for each factor, we take the log of the joint distribution over all variables and then average with respect to those variables not in that factor", "fc359be2-8a6a-4b87-859e-3b83a2515225": "This can make the border pixels somewhat underrepresented in the model.\n\nThis motivates the other extreme case, which MATLAB refers to as full convolution, in which enough zeros are added for every pixel to be visited k times in each direction, resulting in an output image of width m +k \u2014 1. In this case, the output pixels near the border are a function of fewer pixels than the output pixels near the center. This can make it difficult to learn a single kernel that performs well at all positions in the convolutional feature map. Usually the optimal amount of zero padding (in  344  CHAPTER 9. CONVOLUTIONAL NETWORKS  Ls PANSZAINS ASscoocl  SOO OOOOOOOOOOCOOOOONS\u00aee  w\u00e94, o0c00000006  ! \\  https://www.deeplearningbook.org/contents/convnets.html     PGOOOOOOOOCOOOOOOCSCOOO SS  Sscoccoo0 00s  Figure 9.13: The effect of zero padding on network size. Consider a convolutional network with a kernel of width six at every layer", "ed33c2a7-5398-4e15-b37a-7cff80c1c8ec": "\"C, Technlque< ho.-e lherefore bttn proposed for finding approximale pre-image< iB\"\"lr Nat.. 2(04). 12.4. Nonlinear Latent Variable Models In this chapter, we have focussed on the simplest class of models having continuous latent variables, namely those based on linear-Gaussian distributions. As well as having great practical importance, these models are relatively easy to analyse and to fit to data and can also be used as components in more complex models.\n\nHere we consider briefly some generalizations of this framework to models that are either nonlinear or non-Gaussian, or both. In fact, the issues of nonlinearity and non-Gaussianity are related because a general probability density can be obtained from a simple fixed reference density, such as a Gaussian, by making a nonlinear change of variables. This idea forms the basis of several practical latent variable models as we shall see shortly. We begin by considering models in which the observed variables are related linearly to the latent variables, but for which the latent distribution is non-Gaussian", "18b2a7bb-6add-4f31-bbb8-aa94e0708668": "These secondary hyperparameters are usually easier to choose, however, in the sense that acceptable performance may be achieved on a wide range of tasks using the same secondary hyperparameters for all tasks. 11.4.3. Grid Search  When there are three or fewer hyperparameters, the common practice is to perform grid search. For each hyperparameter, the user selects a small finite set of values to explore. The grid search algorithm then trains a model for every joint specification of hyperparameter values in the Cartesian product of the set of values for each individual hyperparameter.\n\nThe experiment that yields the best validation set error is then chosen as having found the best hyperparameters. See the left of figure 11.2 for an illustration of a grid of hyperparameter values. How should the lists of values to search over be chosen? In the case of numerical (ordered) hyperparameters, the smallest and largest element of each list is chosen  427  https://www.deeplearningbook.org/contents/guidelines.html    CHAPTER 11, PRACTICAL METHODOLOGY  Grid Random  Figure 11.2: Comparison of grid search and random search. For illustration purposes, we display two hyperparameters, but we are typically interested in having many more", "c33e3804-8272-491c-8f25-2b0382ba344a": "One way to resolve this problem is to make h a deep representation, encoding x into h in such a way that a Markov chain in the space of h can mix more easily. Many representation learning algorithms, such as autoencoders and RBMs, tend to yield a marginal distribution over h that is more uniform and more unimodal than the original data distribution over aw.\n\nIt can be argued that this arises from trying to minimize reconstruction error while using all the available representation space, because minimizing reconstruction error over  https://www.deeplearningbook.org/contents/monte_carlo.html    the training examples will be better achieved when different training examples are easily distinguishable from each other in 2-space, and thus well separated. Bengio et al. observed that deeper stacks of regularized autoencoders or RBMs yield marginal distributions in the top-level h-space that appeared more spread out  and more uniform, with less of a gap between the regions corresponding to different modes (categories, in the experiments). Training an RBM in that higher-level  601  CHAPTER 17. MONTE CARLO METHODS  space allowed Gibbs sampling to mix faster between modes", "05c45396-8456-410f-b1f4-f2ff7b16927a": "monkeys were internally keeping track of the timing of the reward. (Response timing is one area where the simplest version of TD learning needs to be modi\ufb01ed to account for some of the details of the timing of dopamine neuron responses. We consider this issue in the following section.) The observations from the studies described above led Schultz and his group to conclude that dopamine neurons respond to unpredicted rewards, to the earliest predictors of reward, and that dopamine neuron activity decreases below baseline if a reward, or a predictor of reward, does not occur at its expected time. Researchers familiar with reinforcement learning were quick to recognize that these results are strikingly similar to how the TD error behaves as the reinforcement signal in a TD algorithm. The next section explores this similarity by working through a speci\ufb01c example in detail. This section explains the correspondence between the TD error \u03b4 and the phasic responses of dopamine neurons observed in the experiments just described.\n\nWe examine how \u03b4 changes over the course of learning in a task something like the one described above where a monkey \ufb01rst sees an instruction cue and then a \ufb01xed time later has to respond correctly to a trigger cue in order to obtain reward", "793a9356-7054-464c-8a1d-5424595a1356": "If Al systems can gleana deeper, more nuanced understanding  of reality beyond what\u2019s specified in the  training data set, they\u2019ll be more useful and ultimately bring Al closer to human-level intelligence. As babies, we learn how the world works largely by observation.\n\nWe form generalized predictive models about objects in the world by learning concepts such as object permanence and gravity. Later in life, we observe the world, act onit, observe again, and build hypotheses to explain how our actions change our environment by trial and error. A working hypothesis is that generalized knowledge about the world, or common sense, forms the  https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/    Canadian leaders for their groundbreaking innovations and positive impact on the quality of life in the country. Artificial Intelligence  FACEBOOK  Faceboo Resear..  Facebook researchers will be participating   Self-supervised learning: The dark matter of intelligence   bulk of biological intelligence in both In several  humans and animals. This common activities  sense ability is taken for granted in at this  humans and animals, but has remained year's  an open challenge in Al research since virtual  its inception. In a way, common sense is ICLR 2020.  the dark matter of artificial intelligence", "044b563e-6d42-4d9d-bf70-65b9ea816b27": "Lswav (Zt, Zs) = (Zt, as) + (Zs; ae)  The swapped fit prediction depends on the cross entropy between the predicted code and a set of K trainable prototype vectors C = {e1, see ,cx}. The prototype vector matrix is shared across different batches and represents anchor clusters that each instance should be clustered to. Lz s=- (k) lo (k) where (k) _ exp(Z, cx/T) (2,4s) da BP; Pe eplas ex /7)  In a mini-batch containing B feature vectors Z =  \u20ac RE*B, We would like to maximize the similarity between the features and the prototypes:  maxTr(Q'C! 'Z) + cH(Q) Qca x 1 1 where Q = {QERE*? | Qlp= Kin QV = pies  where H is the entropy, H#(Q) = \u2014 vy Q;; log Q;;, controlling the smoothness of the code", "763436d8-6344-458f-b23e-fd20729064b7": "Then,  = S> d(s) S> (als; 0)Q.\n\n(s, a)  scS acA VI(8) = d7 as) $7 Va(als; 4)Qx(s,a) ses acA = Sod s) So x(als; 0) m(als;4) Q,(s, a) ses acA (als; 8) = So d(s) S> r(a\\s; 6) V In (als; 0)Q,. (s, a) scS acA  = E,,  https://lilianweng.github.io/posts/2018-02-19-rl-overview/  A (Long) Peek into Reinforcement Learning | Lil'Log   This result is named \u201cPolicy Gradient Theorem\" which lays the theoretical foundation for various policy gradient algorithms:  VI(0) =E,,  REINFORCE  REINFORCE, also known as Monte-Carlo policy gradient, relies on Q,(s, a), an estimated return by MC methods using episode samples, to update the policy parameter 0", "81607bcf-7d05-4ade-86f8-b778d8b45055": "When q is chosen to be a Gaussian distribution, with noise added to a predicted mean value, maximizing this entropy term encourages increasing the standard deviation  of this noise. More generally, this entropy term encourages the variational posterior to place high probability mass on many z values that could have generated x, rather than collapsing to a single point estimate of the most likely value. In equation 20.77, we recognize the first term as the reconstruction log-likelihood found in other autoencoders. The second term tries to make the approximate posterior distribution q(z | z) and the model prior Pmodei (Z) approach each other. Traditional approaches to variational inference and learning infer q via an opti- mization algorithm, typically iterated fixed-point equations (section 19.4). These approaches are slow and often require the ability to compute E,.glog pmodei(Z; \u00a3) in closed form.\n\nThe main idea behind the variational autoencoder is to train a parametric encoder (also sometimes called an inference network or recognition model) that produces the parameters of g. As long as z is a continuous variable, we  693  CHAPTER 20", "813d8844-4def-4248-8932-1f1337f6d27c": "DEEP GENERATIVE MODELS  Specifically, with binary mean units ho\u201d) and binary covariance units no), the mcRBM model is defined as the combination of two energy functions:  Enc (x, A, AO) = En(x, bh) + Ep(@, hb), (20.43)  where E,, is the standard Gaussian-Bernoulli RBM energy function,\u201d m 1 m m m Eq (@ hi) = Sale \u2014 Dra Wah\u201d \u2014 So vrr nee, (20.44) j j  and EF, is the cRBM energy function that models the conditional covariance information:  ~\\ 2 E\u00a2(a,h) = Soni? (er) = S209, (20.45) Jj J  The parameter r) corresponds to the covariance weight vector associated with no, and b(\u00a9) is a vector of covariance offsets", "661325c8-b9da-4972-ab43-a691aa77f082": "Although the discrete nature of textual data and its complex syntactic and semantic structures make \ufb01nding labelpreserving transformation more dif\ufb01cult, there nevertheless exists a wide range of methods for augmenting text data that in practice preserve labels. In the following subsections, we describe four broad classes of data augmentation methods: Token-level augmentations manipulate words and phrases in a sentence to generate augmented text while ideally retaining the semantic meaning and labels of the original text. Designed Replacement. Intuitively, the semantic meaning of a sentence remains unchanged if some of its tokens are replaced with other tokens that have the same meaning.\n\nA simple approach is to fetch synonyms as words for substitutions . The synonyms are discovered based on pre-de\ufb01ned dictionaries such as WordNet , or similarities in word embedding space . However, improvements from this technique are usually minimal  and in some cases, performance may even degrade . A major drawback stems from the lack of contextual information when fetching synonyms\u2014especially for words with multiple meanings and few synonyms. To resolve this, language models (LMs) have been used to replace the sampled words given their context", "3f95c83b-b8a0-4484-8bc5-69811bbf692d": "MACHINE LEARNING BASICS  In some cases, the cost function may be a function that we cannot actually evaluate, for computational reasons. In these cases, we can still approximately minimize it using iterative numerical optimization, as long as we have some way of approximating its gradients. Most machine learning algorithms make use of this recipe, though it may not be immediately obvious. If a machine learning algorithm seems especially unique or hand designed, it can usually be understood as using a special-case optimizer. Some models, such as decision trees and k-means, require special-case optimizers because heir cost functions have flat regions that make them inappropriate for minimization by gradient-based optimizers", "302d27d8-fbdb-403c-916c-c8536c07a9b0": "Semi-supervised sequence modeling with cross-view training. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1914\u2013 1925, Brussels, Belgium. Association for Computational Linguistics. Ryan Cotterell and Georg Heigold. 2017. Crosslingual character-level neural morphological tagging. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 748\u2013759, Copenhagen, Denmark. Association for Computational Linguistics. Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. 2019. Autoaugment: Learning augmentation strategies from data. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 113\u2013123.\n\nLi Deng, Jinyu Li, Jui-Ting Huang, Kaisheng Yao, Dong Yu, Frank Seide, Mike Seltzer, Geoff Zweig, Xiaodong He, Jason Williams, Yifan Gong, and Alex Acero. 2013. Recent advances in deep learning for speech research at microsoft", "297d7746-bfa9-4654-9c78-f1912610d00f": "Furthermore, as well as \ufb01nding the appropriate values for complexity parameters within a given model, we may wish to consider a range of different types of model in order to \ufb01nd the best one for our particular application. We have already seen that, in the maximum likelihood approach, the performance on the training set is not a good indicator of predictive performance on unseen data due to the problem of over-\ufb01tting.\n\nIf data is plentiful, then one approach is simply to use some of the available data to train a range of models, or a given model with a range of values for its complexity parameters, and then to compare them on independent data, sometimes called a validation set, and select the one having the best predictive performance. If the model design is iterated many times using a limited size data set, then some over-\ufb01tting to the validation data can occur and so it may be necessary to keep aside a third test set on which the performance of the selected model is \ufb01nally evaluated. In many applications, however, the supply of data for training and testing will be limited, and in order to build good models, we wish to use as much of the available data as possible for training. However, if the validation set is small, it will give a relatively noisy estimate of predictive performance", "6fcc0666-287c-48b2-834f-f0e1b502a00b": "The book is supported by a great deal of additional material, including lecture slides as well as the complete set of \ufb01gures used in the book, and the reader is encouraged to visit the book web site for the latest information: http://research.microsoft.com/\u223ccmbishop/PRML The exercises that appear at the end of every chapter form an important component of the book. Each exercise has been carefully chosen to reinforce concepts explained in the text or to develop and generalize them in signi\ufb01cant ways, and each is graded according to dif\ufb01culty ranging from (\u22c6), which denotes a simple exercise taking a few minutes to complete, through to (\u22c6 \u22c6 \u22c6), which denotes a signi\ufb01cantly more complex exercise.\n\nIt has been dif\ufb01cult to know to what extent these solutions should be made widely available. Those engaged in self study will \ufb01nd worked solutions very bene\ufb01cial, whereas many course tutors request that solutions be available only via the publisher so that the exercises may be used in class. In order to try to meet these con\ufb02icting requirements, those exercises that help amplify key points in the text, or that \ufb01ll in important details, have solutions that are available as a PDF \ufb01le from the book web site. Such exercises are denoted by www", "436cc709-940b-4bab-8575-9899eadd4530": "This is especially true for environments whose dynamics depend on the behavior of humans, such as in education, healthcare, transportation, and public policy, domains that can surely bene\ufb01t from improved decision making. However, it is for real-world embedded agents that warnings about potential dangers of arti\ufb01cial intelligence need to be heeded. Some of these warnings are particularly relevant to reinforcement learning. Because reinforcement learning is based on optimization, it inherits the pluses and minuses of all optimization methods. On the minus side is the problem of devising objective functions, or reward signals in the case of reinforcement learning, so that optimization produces the desired results while avoiding undesirable results. We said in Section 17.4 that reinforcement learning agents can discover unexpected ways to make their environments deliver reward, some of which might be undesirable, or even dangerous.\n\nWhen we specify what we want a system to learn only indirectly, as we do in designing a reinforcement learning system\u2019s reward signal, we will not know how closely the agent will ful\ufb01ll our desire until its learning is complete. This is hardly a new problem with reinforcement learning; recognition of it has a long history in both literature and engineering", "968b6b1d-3abc-4620-8782-c72f0fbb16c4": "We already know that the exact maximum likelihood solution for JL is given by the sample mean x defined by (12.1), and it is convenient to substitute for JL at this stage.\n\nMaking use of the expressions (12.31) and (12.32) for the latent and conditional distributions, respectively, and taking the expectation with respect to the posterior distribution over the latent variables, we obtain Note that this depends on the posterior distribution only through the sufficient statistics of the Gaussian. Thus in the E step, we use the old parameter values to evaluate which follow directly from the posterior distribution (12.42) together with the standard result lE = cov + JEJET. Here M is defined by (12.41). In the M step, we maximize with respect to Wand (J2, keeping the posterior statistics fixed. Maximization with respect to (T2 is straightforward. For the maximization with respect to W we make use of (C.24), and obtain the M-step equations The EM algorithm for probabilistic PCA proceeds by initializing the parameters and then alternately computing the sufficient statistics of the latent space posterior distribution using (12.54) and (12.55) in the E step and revising the parameter values using (12.56) and (12.57) in the M step", "bc69ec65-357c-4390-9ec6-42ed6b2339d3": "61  CHAPTER 3.\n\nPROBABILITY AND INFORMATION THEORY  https://www.deeplearningbook.org/contents/prob.html    0.40  0.35  0.30 Maximum at x = pw 7 0.25 Inflection points at & 0.20 t=pto  0.15 0.10 0.05 0.00  2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0  Figure 3.1: The normal distribution. The normal distributionN (x; j1, 07) exhibits a classic \u201cbell curve\u201d shape, with the x coordinate of its central peak given by pu, and the width of its peak controlled by o. In this example, we depict the standard normal distribution, with = 0 ando =1. First, many distributions we wish to model are truly close to being normal distributions. The central limit theorem shows that the sum of many indepen- dent random variables is approximately normally distributed. This means that in practice, many complicated systems can be modeled successfully as normally distributed noise, even if the system can be decomposed into parts with more structured behavior. Second, out of all possible probability distributions with the same variance, the normal distribution encodes the maximum amount of uncertainty over the real numbers", "e99c851f-5eb4-4c5a-b335-4bd57310be9f": "To set the stage for the full problem, we brie\ufb02y discuss the simplest way in which nonassociative tasks extend to the associative setting. As an example, suppose there are several di\u21b5erent k-armed bandit tasks, and that on each step you confront one of these chosen at random. Thus, the bandit task changes randomly from step to step. This would appear to you as a single, nonstationary k-armed bandit task whose true action values change randomly from step to step.\n\nYou could try using one of the methods described in this chapter that can handle nonstationarity, but unless the true action values change slowly, these methods will not work very well. Now suppose, however, that when a bandit task is selected for you, you are given some distinctive clue about its identity (but not its action values). Maybe you are facing an actual slot machine that changes the color of its display as it changes its action values. Now you can learn a policy associating each task, signaled by the color you see, with the best action to take when facing that task\u2014for instance, if red, select arm 1; if green, select arm 2", "eafca0ef-3ae8-4cc9-b029-766d744b52a5": "Points with an = C can lie inside the margin and can either be correctly classi\ufb01ed if \u03ben \u2a7d 1 or misclassi\ufb01ed if \u03ben > 1.\n\nTo determine the parameter b in (7.1), we note that those support vectors for which 0 < an < C have \u03ben = 0 so that tny(xn) = 1 and hence will satisfy Again, a numerically stable solution is obtained by averaging to give where M denotes the set of indices of data points having 0 < an < C. An alternative, equivalent formulation of the support vector machine, known as the \u03bd-SVM, has been proposed by Sch\u00a8olkopf et al. This involves maximizing This approach has the advantage that the parameter \u03bd, which replaces C, can be interpreted as both an upper bound on the fraction of margin errors (points for which \u03ben > 0 and hence which lie on the wrong side of the margin boundary and which may or may not be misclassi\ufb01ed) and a lower bound on the fraction of support vectors. An example of the \u03bd-SVM applied to a synthetic data set is shown in Figure 7.4", "5cea6bbe-a004-4287-870d-4ada14d1bf37": "The training criterion for the discriminator D, given any generator G, is to maximize the quantity V (G, D) a+b. The discriminator does not need to be de\ufb01ned outside of Supp(pdata) \u222a Supp(pg), concluding the proof. Note that the training objective for D can be interpreted as maximizing the log-likelihood for estimating the conditional probability P(Y = y|x), where Y indicates whether x comes from pdata (with y = 1) or from pg (with y = 0). The minimax game in Eq. 1 can now be reformulated as: C(G) = max D V (G, D) Theorem 1. The global minimum of the virtual training criterion C(G) is achieved if and only if pg = pdata. At that point, C(G) achieves the value \u2212 log 4", "9d4c0870-0177-4498-b248-52a8045c6928": "Optimizing models that make discrete decisions requires specialized optimization algorithms, described in section 20.9.1. So far, training these stochastic architectures that make discrete decisions remains harder than training deterministic algorithms that make soft decisions. Whether it is soft (allowing back-propagation) or stochastic and hard, the mechanism for choosing an address is in its form identical to the attention mechanism, which had been previously introduced in the context of machine translation  and is discussed in section 12.4.5.1. The idea of attention mechanisms for neural networks was introduced even earlier, in the context of handwriting generation , with an attention mechanism that was constrained to move only forward in time through the sequence. In the case of machine translation and memory networks, at each step, the focus of attention can move to a completely different place, compared to the previous step. Recurrent neural networks provide a way to extend deep learning to sequential data. They are the last major tool in our deep learning toolbox. Our discussion now moves to how to choose and use these tools and how to apply them to real-world tasks. 415  https://www.deeplearningbook.org/contents/rnn.html", "56ea3d04-19a8-4e78-b9e9-0acc997c009d": "In a latent variable model, we might want to extract features E describing the observed variables v. Sometimes we need to solve such problems in order to perform other tasks. We often train our models using the principle of maximum likelihood.\n\nBecause  C  log p(v) = hwvolhla) Nae mlh as) \u2014 law obh | a1 (18 Qa)  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    7 BVB Pee wR Uys (suey 580  CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  we often want to compute p(h | v) in order to implement a learning rule. All these are examples of inference problems in which we must predict the value of some variables given other variables, or predict the probability distribution over some variables given the value of other variables. Unfortunately, for most interesting deep models, these inference problems are intractable, even when we use a structured graphical model to simplify them", "a9e6f917-c02d-4c2f-a0b0-40efda1d9a79": "For the particular case of R = I, we see that the columns of W are the principal component eigenvectors scaled by the variance parameters Ai (J'2. The interpretation of these scaling factors is clear once we recognize that for a convolution of independent Gaussian distributions (in this case the latent space distribution and the noise model) the variances are additive. Thus the variance Ai in the direction of an eigenvector Ui is composed of the sum of a contribution Ai (J'2 from the projection of the unit-variance latent space distribution into data space through the corresponding column of W, plus an isotropic contribution of variance (J'2 which is added in all directions by the noise model.\n\nIt is worth taking a moment to study the form of the covariance matrix given by (12.36). Consider the variance of the predictive distribution along some direction specified by the unit vector v, where vTv = 1, which is given by vTCv. First suppose that v is orthogonal to the principal subspace, in other words it is given by some linear combination of the discarded eigenvectors. Then vTV = 0 and hence vTCv = (J'2", "0a7a8f99-9ef5-45a0-bdcf-11db2e298baa": "In particular, the result (8.63) shows that the local marginal at the node zn is given by the product of the incoming messages. Because we have conditioned on the variables X = {x1, . , xN}, we are computing the joint distribution Dividing both sides by p(X), we then obtain in agreement with (13.33). The result (13.43) can similarly be derived from (8.72). Exercise 13.11 There is an important issue that must be addressed before we can make use of the forward backward algorithm in practice.\n\nFrom the recursion relation (13.36), we note that at each step the new value \u03b1(zn) is obtained from the previous value \u03b1(zn\u22121) by multiplying by quantities p(zn|zn\u22121) and p(xn|zn). Because these probabilities are often signi\ufb01cantly less than unity, as we work our way forward along the chain, the values of \u03b1(zn) can go to zero exponentially quickly. For moderate lengths of chain (say 100 or so), the calculation of the \u03b1(zn) will soon exceed the dynamic range of the computer, even if double precision \ufb02oating point is used", "5e2c24ab-50b2-48d2-a0fe-bc19b3437378": "Overall, policy-gradient methods provide a signi\ufb01cantly di\u21b5erent set of strengths and weaknesses than action-value methods.\n\nToday they are less well understood in some respects, but a subject of excitement and ongoing research. Methods that we now see as related to policy gradients were actually some of the earliest to be studied in reinforcement learning  and in predecessor \ufb01elds . They were largely supplanted in the 1990s by the action-value methods that are the focus of the other chapters of this book. In recent years, however, attention has returned to actor\u2013critic methods and to policy-gradient methods in general. Among the further developments beyond what we cover here are natural-gradient methods , deterministic policy gradient methods , o\u21b5-policy policy-gradient methods , and entropy regularization . Major applications include acrobatic helicopter autopilots and AlphaGo (Section 16.6). Our presentation in this chapter is based primarily on that by Sutton, McAllester, Singh, and Mansour , who introduced the term \u201cpolicy gradient methods.\u201d A useful overview is provided by Bhatnagar et al", "aad787c6-a3b3-47fd-9b46-51823f54c81b": "Because \u03b3(zn) must also be Gaussian, we write it in the form To derive the required recursion, we start from the backward recursion (13.62) for \ufffd\u03b2(zn), which, for continuous latent variables, can be written in the form We now multiply both sides of (13.99) by \ufffd\u03b1(zn) and substitute for p(xn+1|zn+1) and p(zn+1|zn) using (13.75) and (13.76). Then we make use of (13.89), (13.90) and (13.91), together with (13.98), and after some manipulation we obtain Exercise 13.29 and we have made use of AVn = PnJT n", "01ee99ee-6942-45bb-a10a-215b32e2f01c": "(Left)Gabor functions with different values of the parameters that control the coordinate system: xo, yo, and +. Each Gabor function in this grid is assigned a value of xp and yo proportional to its position in its grid, and 7 is chosen so that each Gabor filter is sensitive to the direction radiating out from the center of the grid. For the other two plots, zo, yo, and 7 are fixed to zero. (Center)Gabor functions with different Gaussian scale parameters 8, and \u00a3,. Gabor functions are arranged in increasing width (decreasing 3,,) as we move left to right through the grid, and increasing height (decreasing 8,) as we move top to bottom. For the other two plots, the 6 values are fixed to 1.5 times the image width. (Right)Gabor functions with different sinusoid parameters f and \u00a2. As we move top to bottom, f increases, and as we move left to right, \u00a2 increases.\n\nFor the other two plots, \u00a2 is fixed to 0 and f is fixed to 5 times the image width", "51d43f60-200f-418d-ba6c-b777e3df15d7": "AlphaZero is a general reinforcement learning algorithm that improves over the world\u2019s hitherto best programs in the diverse games of Go, chess, and shogi. Personalizing web services such as the delivery of news articles or advertisements is one approach to increasing users\u2019 satisfaction with a website or to increase the yield of a marketing campaign.\n\nA policy can recommend content considered to be the best for each particular user based on a pro\ufb01le of that user\u2019s interests and preferences inferred from their history of online activity. This is a natural domain for machine learning, and in particular, for reinforcement learning. A reinforcement learning system can improve a recommendation policy by making adjustments in response to user feedback. One way to obtain user feedback is by means of website satisfaction surveys, but for acquiring feedback in real time it is common to monitor user clicks as indicators of interest in a link. A method long used in marketing called A/B testing is a simple type of reinforcement learning used to decide which of two versions, A or B, of a website users prefer. Because it is non-associative, like a two-armed bandit problem, this approach does not personalize content delivery", "4c311767-49e8-4939-bf01-41b6529f970b": "The generator nets used a mixture of recti\ufb01er linear activations  and sigmoid activations, while the discriminator net used maxout  activations. Dropout  was applied in training the discriminator net. While our theoretical framework permits the use of dropout and other noise at intermediate layers of the generator, we used noise as the input to only the bottommost layer of the generator network. We estimate probability of the test set data under pg by \ufb01tting a Gaussian Parzen window to the samples generated with G and reporting the log-likelihood under this distribution. The \u03c3 parameter of the Gaussians was obtained by cross validation on the validation set. This procedure was introduced in Breuleux et al. and used for various generative models for which the exact likelihood is not tractable . Results are reported in Table 1. This method of estimating the likelihood has somewhat high variance and does not perform well in high dimensional spaces but it is the best method available to our knowledge. Advances in generative models that can sample but not estimate likelihood directly motivate further research into how to evaluate such models", "204433d3-1aa0-49c6-b608-ff47686d17ba": "IEEE Transactions on Automatic Bertsekas, D. P. Distributed asynchronous computation of \ufb01xed points. Mathematical Bertsekas, D. P. Dynamic Programming: Deterministic and Stochastic Models. PrenticeBertsekas, D. P. Dynamic Programming and Optimal Control, Volume 1, third edition. Bertsekas, D. P. Dynamic Programming and Optimal Control, Volume 2: Approximate Dynamic Programming, fourth edition. Athena Scienti\ufb01c, Belmont, MA. Bertsekas, D. P. Rollout algorithms for discrete optimization: A survey. In Handbook of Combinatorial Optimization, pp. 2989\u20133013. Springer, New York. Bertsekas, D. P., Tsitsiklis, J. N., Wu, C. Rollout algorithms for combinatorial optimizaBertsekas, D. P., Yu, H", "850b7014-df67-447b-a64e-4676f58a6d83": "In either case, we say the model is used to simulate the environment and produce simulated experience. The word planning is used in several di\u21b5erent ways in di\u21b5erent \ufb01elds. We use the term to refer to any computational process that takes a model as input and produces or improves a policy for interacting with the modeled environment: In arti\ufb01cial intelligence, there are two distinct approaches to planning according to our de\ufb01nition.\n\nState-space planning, which includes the approach we take in this book, is viewed primarily as a search through the state space for an optimal policy or an optimal path to a goal. Actions cause transitions from state to state, and value functions are computed over states. In what we call plan-space planning, planning is instead a search through the space of plans. Operators transform one plan into another, and value functions, if any, are de\ufb01ned over the space of plans. Plan-space planning includes evolutionary methods and \u201cpartial-order planning,\u201d a common kind of planning in arti\ufb01cial intelligence in which the ordering of steps is not completely determined at all stages of planning. Plan-space methods are di\ufb03cult to apply e\ufb03ciently to the stochastic sequential decision problems that are the focus in reinforcement learning, and we do not consider them further", "9919813b-3680-4e7c-a9b6-ca9ac58428da": "For the univariate Gaussian, we have which, after some simple rearrangement, can be cast in the standard exponential family form (2.194) with Exercise 2.57 Let us now consider the problem of estimating the parameter vector \u03b7 in the general exponential family distribution (2.194) using the technique of maximum likelihood. Taking the gradient of both sides of (2.195) with respect to \u03b7, we have Rearranging, and making use again of (2.195) then gives where we have used (2.194). We therefore obtain the result Note that the covariance of u(x) can be expressed in terms of the second derivatives of g(\u03b7), and similarly for higher order moments. Thus, provided we can normalize a Exercise 2.58 distribution from the exponential family, we can always \ufb01nd its moments by simple differentiation. Now consider a set of independent identically distributed data denoted by X = {x1, . .\n\n, xn}, for which the likelihood function is given by Setting the gradient of ln p(X|\u03b7) with respect to \u03b7 to zero, we get the following condition to be satis\ufb01ed by the maximum likelihood estimator \u03b7ML which can in principle be solved to obtain \u03b7ML", "d021a8bb-9102-41bf-b46c-cd91e21c7087": "Later, is influenced by all the Markov chain steps that have  run so far. Our goal is for q) (x) to converge to p(x). Because we have reparametrized the problem in terms of a positive integer x, we can describe the probability distribution q using a vector v with  g(x = 1) = 4. (17.17)  Consider what happens when we update a single Markov chain\u2019s state x to a new state 2\u2019. The probability of a single state landing in state x\u2019 is given by  git ( =a T(a\u2019 | a). (17.18)  Using our integer parametrization, we can represent the effect of the transition operator T using a matrix A. We define A so that  Ay; =T(x' =i|x= J).\n\n(17.19)  Using this definition, we can now rewrite equation 17.18. Rather than writing it in terms of gq and T to understand how a single state is updated, we may now use v  593  CHAPTER 17", "6ad81439-42c0-44ee-8381-43239a384ddc": "Let P\u03b8 be the distribution of g\u03b8(Z) with Z a random variable with density p and g\u03b8 a function satisfying assumption 1. Then, there is a solution f : X \u2192 R to the problem Now comes the question of \ufb01nding the function f that solves the maximization problem in equation (2). To roughly approximate this, something that we can do is train a neural network parameterized with weights w lying in a compact space W and then backprop through Ez\u223cp(z), as we would do with a typical GAN.\n\nNote that the fact that W is compact implies that all the functions fw will be K-Lipschitz for some K that only depends on W and not the individual weights, therefore approximating (2) up to an irrelevant scaling factor and the capacity of the \u2018critic\u2019 fw. In order to have parameters w lie in a compact space, something simple we can do is clamp the weights to a \ufb01xed box (say W = l) after each gradient update. The Wasserstein Generative Adversarial Network (WGAN) procedure is described in Algorithm 1. Weight clipping is a clearly terrible way to enforce a Lipschitz constraint", "a5722758-16f6-4931-bb97-f6f4f8c64c27": "11, 12).\n\nRandom erasing is a Data Augmentation method that seeks to directly prevent overfit- ting by altering the input space. By removing certain input patches, the model is forced to find other descriptive characteristics. This augmentation method can also be stacked on top of other augmentation techniques such as horizontal flipping or color filters. Ran- dom erasing produced one of the highest accuracies on the CIFAR-10 dataset. DeVries and Taylor  conducted a similar study called Cutout Regularization. Like the random erasing study, they experimented with randomly masking regions of the image (Table 2). Mikolajcyzk and Grochowski  presented an interesting idea to combine random  erasing with GANs designed for image inpainting. Image inpainting describes the task of Shorten and Khoshgoftaar J Big Data  6:60   Image-aware Random Erasing  input image  Object-aware Random Erasin", "93e3be3f-a4db-4d65-a340-42bcf87371ef": "This readily falls into the dynamic SE setting described in Section 6, where now the experience function is f\u03c6(t) associated with learnable parameters \u03c6.\n\nFor example, in problem (1), f\u03c6(t) = \ufffd i \u03bbi \u03c6f i rule,\u03c6(t), where any learnable components in each knowledge constraint f i rule,\u03c6 (Section 4.2) and the weights \u03bbi \u03c6 constitute the \u03c6 to be learned . In problem (2), f\u03c6(t) is instantiated as fdata-w,\u03c6(t; D) (Equation 4.6) with learnable data weights w(t\u2217) \u2208 \u03c6, or as fdata-aug,\u03c6(t; D) (Equation 4.8) with the metric for augmentation at\u2217(t) \u2208 \u03c6 to be learned . In problem (3), we have discussed the training of f\u03c6(t) as the GAN discriminator in Section 6.1, but we want to improve the training stability . Thus, one approach for e\ufb03cient updates of the general experience function f\u03c6 would address all three problems together. To seek for solutions, we again take advantage of the uni\ufb01ed SE perspective that enables us to reuse existing successful techniques instead of having to invent new ones", "6376ee49-1106-4825-833c-0aba6c9aa0c1": "In order to solve this constrained optimization problem, we introduce Lagrange multipliers an \u2a7e 0, with one multiplier an for each of the constraints in (7.5), giving Appendix E where a = (a1, . , aN)T. Note the minus sign in front of the Lagrange multiplier term, because we are minimizing with respect to w and b, and maximizing with respect to a. Setting the derivatives of L(w, b, a) with respect to w and b equal to zero, we obtain the following two conditions Eliminating w and b from L(w, b, a) using these conditions then gives the dual representation of the maximum margin problem in which we maximize with respect to a subject to the constraints Here the kernel function is de\ufb01ned by k(x, x\u2032) = \u03c6(x)T\u03c6(x\u2032). Again, this takes the form of a quadratic programming problem in which we optimize a quadratic function of a subject to a set of inequality constraints. We shall discuss techniques for solving such quadratic programming problems in Section 7.1.1. The solution to a quadratic programming problem in M variables in general has computational complexity that is O(M 3)", "7af85381-b9c2-4fc4-a6e4-cb3ee4ac02ea": ", xN each of which has a uniform distribution over the interval  and then considering the distribution of the mean (x1 + \u00b7 \u00b7 \u00b7 + xN)/N. For large N, this distribution tends to a Gaussian, as illustrated in Figure 2.6. In practice, the convergence to a Gaussian as N increases can be very rapid. One consequence of this result is that the binomial distribution (2.9), which is a distribution over m de\ufb01ned by the sum of N observations of the random binary variable x, will tend to a Gaussian as N \u2192 \u221e (see Figure 2.1 for the case of N = 10). The Gaussian distribution has many important analytical properties, and we shall consider several of these in detail. As a result, this section will be rather more technically involved than some of the earlier sections, and will require familiarity with various matrix identities. However, we strongly encourage the reader to become proAppendix C \ufb01cient in manipulating Gaussian distributions using the techniques presented here as this will prove invaluable in understanding the more complex models presented in later chapters. We begin by considering the geometrical form of the Gaussian distribution", "f582ec8b-7f6e-40f8-a2fe-5503283555b7": "In practice, these methods usually converge much faster than their theoretical worst-case run times, particularly if they are started On problems with large state spaces, asynchronous DP methods are often preferred. To complete even one sweep of a synchronous method requires computation and memory for every state. For some problems, even this much memory and computation is impractical, yet the problem is still potentially solvable because relatively few states occur along optimal solution trajectories. Asynchronous methods and other variations of GPI can be applied in such cases and may \ufb01nd good or optimal policies much faster than synchronous methods can.\n\nIn this chapter we have become familiar with the basic ideas and algorithms of dynamic programming as they relate to solving \ufb01nite MDPs. Policy evaluation refers to the (typically) iterative computation of the value functions for a given policy. Policy improvement refers to the computation of an improved policy given the value function for that policy. Putting these two computations together, we obtain policy iteration and value iteration, the two most popular DP methods. Either of these can be used to reliably compute optimal policies and value functions for \ufb01nite MDPs given complete knowledge of the MDP", "eb29dbaf-2269-4d53-a004-6e333ba30d56": "We have seen that the effective kernel de\ufb01nes the weights by which the training set target values are combined in order to make a prediction at a new value of x, and it can be shown that these weights sum to one, in other words for all values of x.\n\nThis intuitively pleasing result can easily be proven informally Exercise 3.14 by noting that the summation is equivalent to considering the predictive mean \ufffdy(x) for a set of target data in which tn = 1 for all n. Provided the basis functions are linearly independent, that there are more data points than basis functions, and that one of the basis functions is constant (corresponding to the bias parameter), then it is clear that we can \ufb01t the training data exactly and hence that the predictive mean will be simply \ufffdy(x) = 1, from which we obtain (3.64). Note that the kernel function can be negative as well as positive, so although it satis\ufb01es a summation constraint, the corresponding predictions are not necessarily convex combinations of the training set target variables", "d9a7fea5-555f-4592-9d15-72a2f28649e4": "2018. Kevin Clark, Minh-Thang Luong, Christopher D Manning, and Quoc Le. 2018. Semi-supervised sequence modeling with cross-view training. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1914\u2013 1925. Ronan Collobert and Jason Weston. 2008. A uni\ufb01ed architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160\u2013167. ACM. Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data.\n\nIn Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 670\u2013680, Copenhagen, Denmark. Association for Computational Linguistics. Andrew M Dai and Quoc V Le. 2015. Semi-supervised sequence learning. In Advances in neural information processing systems, pages 3079\u20133087. J. Deng, W", "af384deb-a48b-457d-a497-5c72bdef72e8": "Thc simplest continuous latent variable model assumes Gaussian distributions for both thc latent and observed variables and makes use of a linear,Gaussian deSeCTion 8.1..J pendence of the observed variables on Ihe slate of the latent variables. This leads to a probabilislic fonnulation of the well-known technique of principal component analysis (PeA), as well as 10 a related model called factor analysis. Section 12.1 In this chapter w will begin wilh a slandard, nonprobabilistic treatment of PeA. and thcn we show how PeA arises naturally as the maximum likelihood solution 10 S'crio\" 12.2 a panlcula, fonn of linear-Gau\"ian latem \"ariable model", "b762fc18-41f9-4e85-9a7c-5ed445ecc896": "Pawlak, V., Kerr, J. N. D. .\n\nDopamine receptor activation is required for corticostriatal spike-timing-dependent plasticity. The Journal of Neuroscience, 28(10):2435\u20132446. thing: neuromodulation opens the STDP gate. Frontiers in Synaptic Neuroscience, 2:146. doi:10.3389/fnsyn.2010.00146. Pearce, J. M., Hall, G. A model for Pavlovian learning: Variation in the e\u21b5ectiveness of conditioning but not unconditioned stimuli. Psychological Review, 87(6):532\u2013552. Pearl, J. Causal diagrams for empirical research. Biometrika, 82(4):669-688. Pecevski, D., Maass, W., Legenstein, R. A. Theoretical analysis of learning with reward-modulated spike-timing-dependent plasticity. In Advances in Neural Information Processing Systems 20 , pp. 881\u2013888", "65faf282-80af-41b2-8e9c-c1a873ef98d2": "Notes: According to the Reptile paper, \u201cthe gradient of the squared euclidean distance between a point \u00a9 and a set S is the vector 2(0 - p), where p is the closest point in S to 0\". Technically the closest point in S is also a function of \u00a9, but I\u2019m not sure why the gradient does not need to worry  about the derivative of p. (Please feel free to leave me a comment or send me an email about this if you have ideas.)\n\nThus the update rule for one stochastic gradient step is:  https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Log  6 =0\u2014aVol ; dist(0, W*)?] = 0 \u2014 (0 \u2014 W2(6)) = (1\u2014 a) + aW (6)  The closest point on the optimal task manifold W; (8) cannot be computed exactly, but Reptile approximates it using SGD(L,,, 9, k)", "18a51604-3059-40a0-9a90-95cda64fe491": "Expectation learning in the brain using di\u21b5use ascending connections. In Society for Neuroscience Abstracts, 18:1210. Randl\u00f8v, J., Alstr\u00f8m, P. Learning to drive a bicycle using reinforcement learning and shaping. In Proceedings of the 15th International Conference on Machine Learning , pp. 463\u2013471. Rangel, A., Camerer, C., Montague, P. R. A framework for studying the neurobiology of value-based decision making. Nature Reviews Neuroscience, 9(7):545\u2013556. Rangel, A., Hare, T. Neural computations associated with goal-directed choice. Current Rao, R. P., Sejnowski, T. J. Spike-timing-dependent Hebbian plasticity as temporal di\u21b5erence learning. Neural Computation, 13(10):2221\u20132237. Ratcli\u21b5, R. Connectionist models of recognition memory: Constraints imposed by learning and forgetting functions", "bfd99acf-45c6-4001-98aa-dddd34504bcc": "Moreover, in the context of neural network training, we usually do not care about finding the exact minimum of a function, but seek only to reduce its value sufficiently to obtain good generalization error. Theoretical analysis of whether an optimization algorithm can accomplish this goal is extremely difficult. Developing more realistic bounds on the performance of optimization algorithms therefore remains an important goal for machine learning research. (omma) Ma. AlA27-241L-  https://www.deeplearningbook.org/contents/optimization.html    0.90 DadIC ALYULILLILLIS  We have previously introduced the gradient descent (section 4.3) algorithm that follows the gradient of an entire training set downhill. This may be accelerated considerably by using stochastic gradient descent to follow the gradient of randomly selected minibatches downhill, as discussed in section 5.9 and section 8.1.3. 8.3.1 Stochastic Gradient Descent  Stochastic gradient descent (SGD) and its variants are probably the most used optimization algorithms for machine learning in general and for deep learning in particular.\n\nAs discussed in section 8.1.3, it is possible to obtain an unbiased estimate of the gradient by taking the average gradient on a minibatch of m examples drawn i.i.d from the data-generating distribution", "6c3ec7d8-9f44-4c76-befb-ea45c0c8fba9": "It is also crucial that the minibatches be selected randomly. Computing an unbiased estimate of the expected gradient from a set of samples requires that those samples be independent.\n\nWe also wish for two subsequent gradient estimates to be independent from each other, so two subsequent minibatches of examples should also be independent from each other. Many datasets are most naturally arranged in a way where successive examples are highly correlated. For example, we might have a dataset of medical data with a long list of blood sample test results. This list might be arranged so that first we have five blood samples taken at different times from the first patient, then we have three blood samples taken from the second patient, then the blood samples from the third patient, and so on", "7d06ff7a-3faa-4943-a23f-4412fa50fa15": "We see that (2.17) has the same functional dependence on \u00b5 as the prior distribution, re\ufb02ecting the conjugacy properties of the prior with respect to the likelihood function.\n\nIndeed, it is simply another beta distribution, and its normalization coef\ufb01cient can therefore be obtained by comparison with (2.13) to give We see that the effect of observing a data set of m observations of x = 1 and l observations of x = 0 has been to increase the value of a by m, and the value of b by l, in going from the prior distribution to the posterior distribution. This allows us to provide a simple interpretation of the hyperparameters a and b in the prior as an effective number of observations of x = 1 and x = 0, respectively. Note that a and b need not be integers. Furthermore, the posterior distribution can act as the prior if we subsequently observe additional data. To see this, we can imagine taking observations one at a time and after each observation updating the current posterior with parameters a = 2, b = 2, and the likelihood function, given by (2.9) with N = m = 1, corresponds to a single observation of x = 1, so that the posterior is given by a beta distribution with parameters a = 3, b = 2", "3d090b7b-8303-4ef0-b805-8c9f686b3d4b": "We call eligibility traces like these of the critic unit\u2019s synapses noncontingent eligibility traces because they only depend on presynaptic activity and are not contingent in any way on postsynaptic activity. The non-contingent eligibility traces of the critic unit\u2019s synapses mean that the critic unit\u2019s learning rule is essentially the TD model of classical conditioning described in Section 14.2. With the de\ufb01nition we have given above of the critic unit and its learning rule, the critic in Figure 15.5a is the same as the critic in the ANN actor\u2013critic of Barto et al. Clearly, a critic like this consisting of just one linear neuron-like unit is the simplest starting point; this critic unit is a proxy for a more complicated neural network able to learn value functions of greater complexity.\n\nThe actor in Figure 15.5a is a one-layer network of k neuron-like actor units, each receiving at time t the same feature vector, x(St), that the critic unit receives. Each actor unit j, j = 1,", "edfb4796-edce-4127-8021-c231e41f5fcc": "Here \u0393(x) is the gamma function de\ufb01ned by (1.141) while Plots of the Dirichlet distribution over the simplex, for various settings of the parameters \u03b1k, are shown in Figure 2.5.\n\nMultiplying the prior (2.38) by the likelihood function (2.34), we obtain the posterior distribution for the parameters {\u00b5k} in the form We see that the posterior distribution again takes the form of a Dirichlet distribution, con\ufb01rming that the Dirichlet is indeed a conjugate prior for the multinomial. This allows us to determine the normalization coef\ufb01cient by comparison with (2.38) so that where we have denoted m = (m1, . , mK)T. As for the case of the binomial distribution with its beta prior, we can interpret the parameters \u03b1k of the Dirichlet prior as an effective number of observations of xk = 1. Note that two-state quantities can either be represented as binary variables and in Belgium, and the name Lejeune Dirichlet comes from \u2018le jeune de Richelet\u2019 (the young person from Richelet). Dirichlet\u2019s \ufb01rst paper, which was published in 1825, brought him instant fame", "34f89a01-2fb5-4756-846c-01ddbdc3732a": "With the right policy you can usually do much better than you could in the absence of any information distinguishing one bandit task from another. This is an example of an associative search task, so called because it involves both trial-and-error learning to search for the best actions, and association of these actions with the situations in which they are best. Associative search tasks are often now called contextual bandits in the literature. Associative search tasks are intermediate between the k-armed bandit problem and the full reinforcement learning problem. They are like the full reinforcement learning problem in that they involve learning a policy, but like our version of the k-armed bandit problem in that each action a\u21b5ects only the immediate reward.\n\nIf actions are allowed to a\u21b5ect the next situation as well as the reward, then we have the full reinforcement learning problem. We present this problem in the next chapter and consider its rami\ufb01cations throughout the rest of the book. Exercise 2.10 Suppose you face a 2-armed bandit task whose true action values change randomly from time step to time step", "f7c94cb0-3b98-4503-8547-fd74cc7b93e6": "(Right) In general, moralization may add many edges to the graph, thus losing many implied independences. For example, this sparse coding graph requires adding moralizing edges between every pair of hidden units, thus introducing a quadratic number of new direct dependences. 575  CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  OHO) OO) CeO 55 SS ss  Figure 16.12: Converting an undirected model to a directed model. (Left) This undirected model cannot be converted to a directed model because it has a loop of length four with no chords. Specifically, the undirected model encodes two different independences that  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    no directed model can capture Simultaneously: alc | {b,d} and bld | {a,c}.\n\n(Center) To convert the undirected model to a directed model, we must toimgalate the grap  by ensuring that all loops of greater than length three have a chord", "8426884f-9f39-4410-9e59-af020880e132": "Our approach draws inspiration from the recent work  that shows equivalence between the data in supervised learning and the reward function in reinforcement learning.\n\nWe thus adapt an off-the-shelf reward learning algorithm  to the supervised setting for automated data manipulation. The marriage of the two paradigms results in a simple yet general algorithm, where various manipulation schemes are reduced to different parameterization of the 33rd Conference on Neural Information Processing Systems , Vancouver, Canada. data reward. Free parameters of manipulation are learned jointly with the target model through ef\ufb01cient gradient descent on validation examples. We demonstrate instantiations of the approach for automatically \ufb01ne-tuning an augmentation network and learning data weights, respectively. We conduct extensive experiments on text and image classi\ufb01cation in challenging situations of very limited data and imbalanced labels. Both augmentation and weighting by our approach signi\ufb01cantly improve over strong base models, even though the models are initialized with large-scale pretrained networks such as BERT  for text and ResNet  for images. Our approach, besides its generality, also outperforms a variety of dedicated rule- and learning-based methods for either augmentation or weighting, respectively", "652b4b9a-7bc9-41a9-bdd0-f805878f68cf": "Space is thus subdivided into nonoverlapping regions, with a one-to-one  https://www.deeplearningbook.org/contents/ml.html    correspondence between leat nodes and input regions. Hach leat node usually maps every point in its input region to the same output. Decision trees are usua  trained with specialized algorithms that are beyond the scope of this book. The learning algorithm can be considered nonparametric if it is allowed to learn a tree  of arbitrary size, though decision trees are usually regularized with size constraints that turn them into parametric models in practice. Decision trees as they are typically used, with axis-aligned splits and constant outputs within each node, struggle to solve some problems that are easy even for logistic regression. For example, if we have a two-class problem, and the positive class occurs wherever a > a1, the decision boundary is not axis aligned.\n\nThe decision tree will thus need to approximate the decision boundary with many nodes, implementing a step function that constantly walks back and forth across the true decision function with axis-aligned steps. As we have seen, nearest neighbor predictors and decision trees have many limitations", "63f7f689-019a-403e-ad20-6b92a75454fd": "As described in section 6.2.2.4, a general strategy for designing the output units and the loss function of a feedforward network is to define an output distribution p(y | a) and minimize the negative log-likelihood \u2014 log p(y | x). In that setting, y is a vector of targets, such as class labels. In an autoencoder, x is now the target as well as the input. Yet we can still apply the same machinery as before. Given a hidden code h, we may think of the decoder as providing a conditional distribution pgecoder(@ | h). We may then train the autoencoder by minimizing \u2014 log pdecoder(#@ | h). The exact form of this loss function will change depending on the form of pgecoder- AS With traditional feedforward networks, we usually use linear output units to parametrize the mean of a Gaussian distribution if a is real valued.\n\nIn that case, the negative log-likelihood yields a mean squared error criterion", "821ca787-b8ad-4a8a-b4a6-64d076b92730": "Iyyer et al. , Xie et al. Artetxe et al. , Lample et al. Xie et al. , Wei and Zou  Jia and Liang  , Andreas  Nye et al. , Feng et al. Furrer et al. , Guo et al. Yu et al. , Xie et al. Chen et al. , He et al. Chen et al. , Cai et al. Anaby-Tavor et al. , Kumar et al. Zhang and Bansal , Yang et al. Miyato et al. , Ebrahimi et al. Ebrahimi et al. , Cheng et al. , Chen et al", "2d38b21b-de65-415f-acec-329f0f8f795b": "11.2 The earliest w-to-2w example was given by Tsitsiklis and Van Roy , who also introduced the speci\ufb01c counterexample in the box on page 263. Baird\u2019s counterexample is due to Baird , though the version we present here is slightly modi\ufb01ed. Averaging methods for function approximation were developed by Gordon . Other examples of instability with o\u21b5-policy DP methods and more complex methods of function approximation are given by Boyan and Moore . Bradtke  gives an example in which Q-learning using linear function approximation in a linear quadratic regulation problem converges to a destabilizing policy. including the dynamic programming operator. Diagrams like Figure 11.3 were introduced by Lagoudakis and Parr . What we have called the Bellman operator, and denoted B\u21e1, is more commonly denoted T \u21e1 and called a \u201cdynamic programming operator,\u201d while a generalized form, denoted T (\u03bb), is called the \u201cTD(\u03bb) operator\u201d", "2e41a155-6276-407e-9bce-eb9673a2027b": "The EM algorithm breaks down the potentially dif\ufb01cult problem of maximizing the likelihood function into two stages, the E step and the M step, each of which will often prove simpler to implement. Nevertheless, for complex models it may be the case that either the E step or the M step, or indeed both, remain intractable. This leads to two possible extensions of the EM algorithm, as follows. The generalized EM, or GEM, algorithm addresses the problem of an intractable M step. Instead of aiming to maximize L(q, \u03b8) with respect to \u03b8, it seeks instead to change the parameters in such a way as to increase its value. Again, because L(q, \u03b8) is a lower bound on the log likelihood function, each complete EM cycle of the GEM algorithm is guaranteed to increase the value of the log likelihood (unless the parameters already correspond to a local maximum). One way to exploit the GEM approach would be to use one of the nonlinear optimization strategies, such as the conjugate gradients algorithm, during the M step", "ce76afcb-2233-4ac6-a147-349c903d915d": "An alternative way to reduce the number of independent parameters in a model is by sharing parameters (also known as tying of parameters).\n\nFor instance, in the chain example of Figure 8.10, we can arrange that all of the conditional distributions p(xi|xi\u22121), for i = 2, . , M, are governed by the same set of K(K\u22121) parameters. Together with the K\u22121 parameters governing the distribution of x1, this gives a total of K2 \u2212 1 parameters that must be speci\ufb01ed in order to de\ufb01ne the joint distribution. We can turn a graph over discrete variables into a Bayesian model by introducing Dirichlet priors for the parameters. From a graphical point of view, each node then acquires an additional parent representing the Dirichlet distribution over the parameters associated with the corresponding discrete node. This is illustrated for the chain model in Figure 8.11. The corresponding model in which we tie the parameters governing the conditional distributions p(xi|xi\u22121), for i = 2, . , M, is shown in Figure 8.12", "ec2e7ba6-4a95-408c-820b-755236639379": "This is because the partition function appears in both the numerator and the denominator of the ratio and cancels out:  4D  ZP(x) \u2014 p(x) Py) spy) Bly) (8.17)  The pseudolikelihood is based on the observation that conditional probabilities take this ratio-based form and thus can be computed without knowledge of the partition function.\n\nSuppose that we partition x into a, b and c, where a contains the variables we want to find the conditional distribution over, b contains the variables we want to condition on, and c contains the variables that are not part of our query:  p(ab)_\u2014\u2014p(ab) \u2014 _ (ab) p(b) Vac pla, b, c) Vac pla, b, c) :  This quantity requires marginalizing out a, which can be a very efficient operation provided that a andc do not contain many variables. In the extreme case, a can be a single variable and c can be empty, making this operation require only as many evaluations of p as there are values of a single random variable", "da1a7709-7106-4555-bd9a-8398b66f7afd": "Some approaches to back-propagation take a computational graph and a set of numerical values for the inputs to the graph, then return a set of numerical values describing the gradient at those input values. We call this approach symbol-to- number differentiation.\n\nThis is the approach used by libraries such as Torch  and Caffe . Another approach is to take a computational graph and add additional nodes  209  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  https://www.deeplearningbook.org/contents/mlp.html    \u00a9  Figure 6.10: An example of the symbol-to-symbol approach to computing derivatives. In this approach, the back-propagation algorithm does not need to ever access any actual specific numeric values. Instead, it adds nodes to a computational graph describing how to compute these derivatives. A generic graph evaluation engine can later compute the derivatives for any specific numeric values. (Left) In this example, we begin with a graph representing z = f(f(f(w))). (Right) We run the back-propagation algorithm, instructing it to construct the graph for the expression corresponding to #", "c1226b04-28d2-4e60-9607-814028720632": "12.5 True online TD(\u03bb) is primarily due to Harm van Seijen  though some of its key ideas were discovered independently by Hado van Hasselt (personal communication). The name \u201cdutch traces\u201d is in recognition of the contributions of both scientists. introduced by van Seijen and Sutton . The algorithm on page 307 was adapted from van Seijen et al. The Mountain Car results were made for this text, except for Figure 12.11 which is adapted from van Seijen and Sutton . Variable \u03bb was introduced in the \ufb01rst edition of this text. The roots of variable \u03b3 are in the work on options  and its precursors , becoming explicit in the GQ(\u03bb) paper , which also introduced some of these recursive forms for the \u03bb-returns. A di\u21b5erent notion of variable \u03bb has been developed by Yu . 12.9 O\u21b5-policy eligibility traces were introduced by Precup et al.\n\n, then further developed by Bertsekas and Yu , Maei , Yu , and by Sutton, Mahmood, Precup, and van Hasselt", "8af392b6-7155-4915-a756-22fedf75a7a0": "13.21 (\u22c6 \u22c6) Use the results (2.115) and (2.116), together with the matrix identities (C.5) and (C.7), to derive the results (13.89), (13.90), and (13.91), where the Kalman gain matrix Kn is de\ufb01ned by (13.92).\n\n13.23 (\u22c6 \u22c6) Using (13.93), together with the de\ufb01nitions (13.76) and (13.77) and the result (2.116), derive (13.94), (13.95) and (13.97). Show that this extension can be re-case in the framework discussed in this chapter by de\ufb01ning a state vector z with an additional component \ufb01xed at unity, and then augmenting the matrices A and C using extra columns corresponding to the parameters a and c. 13.25 (\u22c6 \u22c6) In this exercise, we show that when the Kalman \ufb01lter equations are applied to independent observations, they reduce to the results given in Section 2.3 for the maximum likelihood solution for a single Gaussian distribution. Consider the problem of \ufb01nding the mean \u00b5 of a single Gaussian random variable x, in which we are given a set of independent observations {x1, . , xN}", "27de51e2-bfbf-4371-af94-dc8e2d4a3400": "In this analogy, the kernel function is the potential of a point charge, which falls o\u21b5 as the reciprocal of the distance from the charge.\n\nConnell and Utgo\u21b5  applied an actor\u2013critic method to the pole-balancing task in which the critic approximated the value function using kernel regression with an inverse-distance weighting. Predating widespread interest in kernel regression in machine learning, these authors did not use the term kernel, but referred to \u201cShepard\u2019s method\u201d . Other kernelbased approaches to reinforcement learning include those of Ormoneit and Sen , Dietterich and Wang , Xu, Xie, Hu, and Lu , Taylor and Parr , Barreto, Precup, and Pineau , and Bhat, Farias, and Moallemi . The earliest example we know of in which function approximation methods were used for learning value functions was Samuel\u2019s checkers player . Samuel followed Shannon\u2019s  suggestion that a value function did not have to be exact to be a useful guide to selecting moves in a game and that it might be approximated by a linear combination of features", "571c097f-9076-467f-8873-f5f603f4fd95": "The number of parameters can then be much smaller than in a completely general model (for example it may grow linearly with M), although this is achieved at the expense of a restricted family of conditional distributions. Suppose we wish to build a model for sequences that is not limited by the Markov assumption to any order and yet that can be speci\ufb01ed using a limited number of free parameters. We can achieve this by introducing additional latent variables to permit a rich class of models to be constructed out of simple components, as we did with mixture distributions in Chapter 9 and with continuous latent variable models in Chapter 12.\n\nFor each observation xn, we introduce a corresponding latent variable zn (which may be of different type or dimensionality to the observed variable). We now assume that it is the latent variables that form a Markov chain, giving rise to the graphical structure known as a state space model, which is shown in Figure 13.5. It satis\ufb01es the key conditional independence property that zn\u22121 and zn+1 are independent given zn, so that The joint distribution for this model is given by p(x1, . , xN, z1,", "a9ca7f99-2775-49c1-abe5-e3899fe20a15": "The initial conditions for these recursion equations are obtained from Because p(z1) is given by (13.77), and p(x1|z1) is given by (13.76), we can again make use of (2.115) to calculate c1 and (2.116) to calculate \u00b51 and V1 giving Similarly, the likelihood function for the linear dynamical system is given by (13.63) in which the factors cn are found using the Kalman \ufb01ltering equations. We can interpret the steps involved in going from the posterior marginal over zn\u22121 to the posterior marginal over zn as follows. In (13.89), we can view the quantity A\u00b5n\u22121 as the prediction of the mean over zn obtained by simply taking the mean over zn\u22121 and projecting it forward one step using the transition probability matrix A. This predicted mean would give a predicted observation for xn given by CAzn\u22121 obtained by applying the emission probability matrix C to the predicted hidden state mean.\n\nWe can view the update equation (13.89) for the mean of the hidden variable distribution as taking the predicted mean A\u00b5n\u22121 and then adding a correction that is proportional to the error xn \u2212 CAzn\u22121 between the predicted observation and the actual observation", "d1eb5550-6bba-49be-9c3d-af93b0f75490": "Similarly, binary x values correspond to a Bernoulli distribution whose parameters are given by a sigmoid output unit, discrete x values correspond to a softmax distribution, and so on. Typically, the  506  CHAPTER 14. AUTOENCODERS  Pencoder(h | &) Paecoder (@ | h)  Figure 14.2: The structure of a stochastic autoencoder, in which both the encoder and the decoder are not simple functions but instead involve some noise injection, meaning that their output can be seen as sampled from a distribution, pencoder(h | x) for the encoder and Pdecoder  (a | h) for the decoder. https://www.deeplearningbook.org/contents/autoencoders.html    output variables are treated as being conditionally independent given Ff so that this probability distribution is inexpensive to evaluate, but some techniques, such as mixture density outputs, allow tractable modeling of outputs with correlations", "25481a08-7807-4ff9-8031-7efab75532ea": "You can infer that she is probably not also sick.\n\n(d) The explaining away effect happens even if any descendant ofs is observed! For example, suppose that c is a variable representing whether you have received a report from your colleague. If you notice that you have not received the report, this increases your estimate of the probability that she is not at work today, which in turn makes it more likely that she is either sick or on vacation. The only way to block a path through a V-structure is to observe none of the descendants of the shared child. 571  CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  O \u00a9  Figure 16.9: From this graph, we can read out several d-separation properties", "875a3777-ffc2-40c1-bfa5-2a2efbae8c35": "However, our preliminary experiments suggest that, as a downside, this makes \ufb01netuning an MLE-trained model with SQL objectives more challenging. Future work to scale the proposed methods to tasks such as machine translation and language modeling, and with signi\ufb01cantly larger and (MLE-)pretrained models would be exciting. This work develops a new RL formulation for text generation. While we demonstrate the framework in four applications, it could be adapted to other (emerging) applications. One major component in these applications is the design of the reward function, which in\ufb02uences the behavior of the trained agent. While we believe the MaxEnt RL framework is more robust against reward misspeci\ufb01cation , the potential failures of sub-optimal reward functions are widely known and discussed.7 To this end, deploying this model to the wild requires careful and extensive examination, using tools such as Ribeiro et al. Further, we highlight the application for (blackbox) adversarial attacks in the paper, with the intention of using adversarial attacks to understand the model\u2019s inner workings.\n\nThat being said, this could potentially be misused to conduct malicious attacks against systems. Hence, users of this framework might want to conduct adversarial attacks against their own models to avoid being attacked by other people with bad intentions", "43314b45-5d52-422f-b445-626eb62711a4": "These models naturally learn high-capacity, overcomplete encodings of the input and do not require regularization for these encodings to be useful. Their encodings are naturally useful because the models were trained to approximately maximize the probability of the training data rather than to copy the input to the output.\n\n14.2.1 Sparse Autoencoders  A sparse autoencoder is simply an autoencoder whose training criterion involves a sparsity penalty Q(h) on the code layer h, in addition to the reconstruction error:  L(x, 9(f(#))) + Q(h), (14.2)  where g(h) is the decoder output, and typically we have h = f(a), the encoder output. Sparse autoencoders are typically used to learn features for another task, such as classification. An autoencoder that has been regularized to be sparse must respond to unique statistical features of the dataset it has been trained on, rather than simply acting as an identity function. In this way, training to perform the copying task with a sparsity penalty can yield a model that has learned useful features as a byproduct", "0c5faa4e-8dfa-4a52-80cd-08b4b87d8c69": ", wL are constructed using (11.23). Finally, a second set of L samples is drawn from the discrete distribution (z(1), . , z(L)) with probabilities given by the weights (w1, . , wL). The resulting L samples are only approximately distributed according to p(z), but the distribution becomes correct in the limit L \u2192 \u221e. To see this, consider the univariate case, and note that the cumulative distribution of the resampled values is given by where I(.)\n\nis the indicator function (which equals 1 if its argument is true and 0 otherwise). Taking the limit L \u2192 \u221e, and assuming suitable regularity of the distributions, we can replace the sums by integrals weighted according to the original sampling distribution q(z) which is the cumulative distribution function of p(z). Again, we see that the normalization of p(z) is not required. For a \ufb01nite value of L, and a given initial sample set, the resampled values will only approximately be drawn from the desired distribution", "112e262f-5174-44ba-b023-627e2dec3599": "To best exploit this extra data, one can perform extra training after the initial training with early stopping has completed. In the second, extra raining step, all the training data is included. There are two basic strategies one can use for this second training procedure. One strategy (algorithm 7.2) is to initialize the model again and retrain on all he data. In this second training pass, we train for the same number of steps as che early stopping procedure determined was optimal in the first pass. There are some subtleties associated with this procedure. For example, there is not a good way of knowing whether to retrain for the same number of parameter updates or he same number of passes through the dataset. On the second round of training, each pass through the dataset will require more parameter updates because the raining set is bigger. Another strategy for using all the data is to keep the parameters obtained from  che first round of training and then continue training, but now using all the data. At this stage, we now no longer have a guide for when to stop in terms of a number of steps.\n\nInstead, we can monitor the average loss function on the validation set and continue training until it falls below the value of the training set objective at which the early stopping procedure halted", "5eb302bf-8ef8-484f-a1dd-9ab1166304b5": "Here a suf\ufb01x i or j denotes the index of a component, whereas (x)i denotes x raised to the power of i. 1.3 (\u22c6 \u22c6) Suppose that we have three coloured boxes r (red), b (blue), and g (green). Box r contains 3 apples, 4 oranges, and 3 limes, box b contains 1 apple, 1 orange, and 0 limes, and box g contains 3 apples, 3 oranges, and 4 limes", "a270bd06-4329-4233-bd5d-2c7933f49ccc": "Theory of Neural-Analog Reinforcement Systems and Its Application to the Brain-Model Problem. Ph.D. thesis, Princeton University. Minsky, M. L. Steps toward arti\ufb01cial intelligence. Proceedings of the Institute of Radio Engineers, 49:8\u201330. Reprinted in E. A. Feigenbaum and J. Feldman (Eds. ), Computers and Thought, pp. 406\u2013450. McGraw-Hill, New York, 1963. Minsky, M. L. Computation: Finite and In\ufb01nite Machines. Prentice-Hall, Englewood Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., Riedmiller, M. Playing atari with deep reinforcement learning. ArXiv:1312.5602. Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A", "0bc2bed5-20a4-4b66-b215-1f3173e17c10": "(8.27)  307  https://www.deeplearningbook.org/contents/optimization.html    Thus for a locally quadratic function (with positive definite H), by rescaling the gradient by H~!, Newton\u2019s method jumps directly to the minimum.\n\nIf the objective function is convex but not quadratic (there are higher-order terms), this update can be iterated, yielding the training algorithm associated with Newton\u2019s method, given in algorithm 8.8. For surfaces that are not quadratic, as long as the Hessian remains positive definite, Newton\u2019s method can be applied iteratively. This implies a two-step iterative procedure. First, update or compute the inverse Hessian (i.e., by updat- ing the quadratic approximation). Second, update the parameters according to equation 8.27. In section 8.2.3, we discuss how Newton\u2019s method is appropriate only when the Hessian is positive definite. In deep learning, the surface of the objective function is typically nonconvex, with many features, such as saddle points, that are problematic for Newton\u2019s method", "509e193b-f186-467e-aea9-d82bd1718e49": "Classification  t Pseudo-labels  ~~, Clustering  Convnet ot\u2019 me  https://lilianweng.github.io/posts/2021-05-31-contrastive/  Contrastive Representation Learning | Lil'Log   In each iteration, DeepCluster clusters data points using the prior representation and then produces the new cluster assignments as the classification targets for the new representation. However this iterative process is prone to trivial solutions. While avoiding the use of negative pairs, it requires a costly clustering phase and specific precautions to avoid collapsing to trivial solutions. SwAV  SwAV  is an online contrastive learning algorithm. It computes a code from an augmented version of the image and tries to predict this code using another augmented version of the same image. Contrastive instance learning Swapping Assignments between Views (Ours)  Given features of images with two different augmentations, z; and z,, SwAV computes corresponding codes q; and q, and the loss quantifies the fit by swapping two codes using L(. )\n\nto measure the fit between a feature and a code", "af0bbc28-fb63-420a-a1ed-a45dbc41f1bb": "The TD error at this stage of learning is analogous to a dopamine neuron responding to an unpredicted reward (e.g., a drop of apple juice) at the start of training. Throughout this \ufb01rst trial and all successive trials, TD(0) updates occur at each state transition as described in Chapter 6.\n\nThis successively increases the values of the reward-predicting states, with the increases spreading backwards from the rewarding state, until the values converge to the correct return predictions. In this case (because we are assuming no discounting) the correct predictions are equal to R? for all the reward-predicting states. This can be seen in Figure 15.4 as the graph of V labeled \u2018learning complete\u2019 where the values of all the states from the earliest to the latest reward-predicting states all equal R?. The values of the states preceding the earliest reward-predicting state remain low (which Figure 15.4 shows as zero) because they are not reliable predictors of reward. When learning is complete, that is, when V attains its correct values, the TD errors associated with transitions from any reward-predicting state are zero because the predictions are now accurate", "a207860f-b50e-4a9a-aa6e-73ee51b38e08": "For nonparametric models, more data yield better generalization until the best possible error is achieved. Any fixed parametric model with less than optimal capacity will asymptote to an error value that exceeds the Bayes error. https://www.deeplearningbook.org/contents/ml.html    See figure 5.4 for an illustration.\n\nNote that it is possible for the model to have optimal capacity and yet still have a large gap between training and generalization errors. In this situation, we may be able to reduce this gap by gathering more training examples. 5.2.1 The No Free Lunch Theorem  Learning theory claims that a machine learning algorithm can generalize well from a finite training set of examples. This seems to contradict some basic principles of logic. Inductive reasoning, or inferring general rules from a limited set of examples, is not logically valid. To logically infer a rule describing every member of a set, one must have information about every member of that set. In part, machine learning avoids this problem by offering only probabilistic rules, rather than the entirely certain rules used in purely logical reasoning. Machine  learning promises to find rules that are probably correct about most members of the set they concern", "36bdac67-6d7b-480e-912c-190436225b55": "The value of \u03b7 needs to be chosen with care to ensure that the algorithm converges . In Section 1.1, we introduced the idea of adding a regularization term to an error function in order to control over-\ufb01tting, so that the total error function to be minimized takes the form ED(w) + \u03bbEW (w) (3.24) where \u03bb is the regularization coef\ufb01cient that controls the relative importance of the data-dependent error ED(w) and the regularization term EW (w).\n\nOne of the simplest forms of regularizer is given by the sum-of-squares of the weight vector elements EW (w) = 1 If we also consider the sum-of-squares error function given by This particular choice of regularizer is known in the machine learning literature as weight decay because in sequential learning algorithms, it encourages weight values to decay towards zero, unless supported by the data. In statistics, it provides an example of a parameter shrinkage method because it shrinks parameter values towards zero. It has the advantage that the error function remains a quadratic function of w, and so its exact minimizer can be found in closed form", "473b817f-0503-4d84-b69b-661756212df9": "FllIu.e 12.21 ?lot ot trle oillkYw <:lata Wllisualiz.ed using PeA on the left and GTM on Itle ngr,t FOf tile GTM model. each <lata poinIls plollfld at tile mean ot its posM'k>< dislribution in ..tent s;>ace, Tile \"\",\"ineanty mlhe GTM 1TlOd8I._lha sepamlion betwoon the groups of data points to be .....n \"\"\". ckl.arfy, bilistic foundation also makes it very straightforward to define generalizations of GTM  such as a Bayesian treatment, dealing with missing valSection 6.4 ues, a principled extension to discrete variables, the use of Gaussian processes to define the manifold, or a hierarchical GTM model", "cf5cb558-2ecb-4cf8-adea-9fd9b7713b55": "Curriculum-based strategies are more effective for teaching humans than strategies based on uniform sampling of examples and can also increase the effectiveness of other teaching strategies . Another important contribution to research on curriculum learning arose in the context of training recurrent neural networks to capture long-term dependencies: Zaremba and Sutskever  found that much better results were obtained with a stochastic curriculum, in which a random mix of easy and difficult examples is always presented to the learner, but where the average proportion of the more difficult examples (here, those with longer-term dependencies) is gradually increased. With a deterministic curriculum, no improvement over the baseline (ordinary training from the full training set) was observed. We have now described the basic family of neural network models and how to regularize and optimize them. In the chapters ahead, we turn to specializations of he neural network family that allow neural networks to scale to very large sizes and process input data that has special structure.\n\nThe optimization methods discussed in this chapter are often directly applicable to these specialized architectures with ittle or no modification. https://www.deeplearningbook.org/contents/optimization.html    325  https://www.deeplearningbook.org/contents/optimization.html", "95f3e4ae-a320-4d97-b248-188f68819c04": "These artifacts can be avoided if the tilings are o\u21b5set asymmetrically, as shown in the lower half of the \ufb01gure. These lower generalization patterns are better because they are all well centered on the trained state with no obvious asymmetries. Tilings in all cases are o\u21b5set from each other by a fraction of a tile width in each dimension. If w denotes the tile width and n the number of tilings, then w n on a side, all states activate the same tiles, have the same feature representation, and the same approximated value. If a state is moved by w in any cartesian direction, the feature representation changes by one component/tile. Uniformly o\u21b5set tilings are o\u21b5set from each other by exactly this unit distance", "eb3712d6-4e5c-4556-88e9-13d854367949": "In so doing, we incur some level of loss that we denote by Lkj, which we can view as the k, j element of a loss matrix. For instance, in our cancer example, we might have a loss matrix of the form shown in Figure 1.25. This particular loss matrix says that there is no loss incurred if the correct decision is made, there is a loss of 1 if a healthy patient is diagnosed as having cancer, whereas there is a loss of 1000 if a patient having cancer is diagnosed as healthy. The optimal solution is the one which minimizes the loss function. However, the loss function depends on the true class, which is unknown. For a given input vector x, our uncertainty in the true class is expressed through the joint probability distribution p(x, Ck) and so we seek instead to minimize the average loss, where the average is computed with respect to this distribution, which is given by Each x can be assigned independently to one of the decision regions Rj", "63203fa4-abca-4e32-9970-737c88542f62": "shearing, rotation, invert, etc.) for image classification as an RL problem and looks for the combination that leads to the highest accuracy on the evaluation set. RandAugment : RandAugment greatly reduces the search space of AutoAugment by controlling the magnitudes of different transformation operations with a single magnitude parameter. PBA : PBA combined PBT  with AutoAugment, using the evolutionary algorithm to train a population of children models in parallel to evolve the best augmentation strategies.\n\nUDA : Among a set of possible augmentation strategies, UDA selects those to minimize the KL divergence between the predicted distribution over an unlabelled example and its unlabelled augmented version. Liflog  Image Mixture  Image mixture methods can construct new training examples from existing data points. Mixup : It runs global-level mixture by creating a weighted pixel-wise combination of two existing images I, and Ig: Inixup <\u2014 2; + (1 \u2014 a)Iy anda \u20ac 0, 1]. Cutmix : Cutmix does region-level mixture by generating a new example by combining a local region of one image with the rest of the other image", "3d26d85b-8428-4afc-8f85-de4671099bc2": "18.5 Denoising Score Matching  Tew nae AA nnn en nee eet be en eatin Anne ent ahinn Lee Gait 2 Atsbnth--4in-  https://www.deeplearningbook.org/contents/partition.html   dil SULLIC Cases We illay WISIL LO LCYULALIZE BSCULE IUALCLIUILY , vy dit LLLLY, a@ UISLLIVDULIVILL  psmoothed (a) = [restate | y)dy (18.27)  rather than the true pqata: The distribution q(a | y) is a corruption process, usually one that forms a by adding a small amount of noise to y. 617  CHAPTER 18. CONFRONTING THE PARTITION FUNCTION  Denoising score matching is especially useful because in practice, we usually do not have access to the true pdata but rather only an empirical distribution defined by samples from it", "c363d41b-51ef-4f8b-9661-2d3549273ac4": "have seen this repeatedly, such as in Chapters 7 (Figure 7.2) and 9 (Figure 9.2), where some degree of bootstrapping performed much better than Monte Carlo methods on the random-walk prediction task, and in Chapter 10 where the same was seen on the Mountain-Car control task (Figure 10.4). Many other problems show much faster learning with bootstrapping (e.g., see Figure 12.14). Bootstrapping often results in faster learning because it allows learning to take advantage of the state property, the ability to recognize a state upon returning to it. On the other hand, bootstrapping can impair learning on problems where the state representation is poor and causes poor generalization . A poor state representation can also result in bias; this is the reason for the poorer bound on the asymptotic approximation quality of bootstrapping methods (Equation 9.14).\n\nOn balance, the ability to bootstrap has to be considered extremely valuable. One may sometimes choose not to use it by selecting long n-step updates (or a large bootstrapping parameter, \u03bb \u21e1 1; see Chapter 12) but often bootstrapping greatly increases e\ufb03ciency. It is an ability that we would very much like to keep in our toolkit", "52338354-3ad1-41f9-97f9-336acd961e2d": "Suppose we have a distributionp(x) and wish to approximate it with another distribution g(x). We have the choice of minimizing either Dxi(p||q) or Di (q||p). We illustrate the effect of this choice using a mixture of two Gaussians for p, and a single Gaussian for g. The choice of which direction of the KL divergence to use is problem dependent. Some applications require an approximation that usually places high probability anywhere that the true distribution places high probability, while other applications require an approximation that rarely places high probability anywhere that the true distribution places low probability. The choice of the direction of the KL divergence reflects which of these considerations takes priority for each application.\n\n(Left)The effect of minimizing Dxx(p||q). In this case, we select aq that has high probability where p has high probability. When p has multiple modes, g chooses to blur the modes together, in order to put high probability mass on all of them. (Right)The effect of minimizing Dx1(q||p)", "d686ad98-34e2-4b0e-9455-17b40c9b594e": "To estimate the value of zN, we take only the most recent few measurements, say xN\u2212L, . , xN and just average these. If z is changing slowly, and the random noise level in the sensor is high, it would make sense to choose a relatively long window of observations to average. Conversely, if the signal is changing quickly, and the noise levels are small, we might be better just to use xN directly as our estimate of zN. Perhaps we could do even better if we take a weighted average, in which more recent measurements make a greater contribution than less recent ones. Although this sort of intuitive argument seems plausible, it does not tell us how to form a weighted average, and any sort of hand-crafted weighing is hardly likely to be optimal. Fortunately, we can address problems such as this much more systematically by de\ufb01ning a probabilistic model that captures the time evolution and measurement processes and then applying the inference and learning methods developed in earlier chapters", "0a5d8c35-1a61-4a19-8cdc-af75f29bafb2": "Even though we may not have labeled examples translating word A in anguage X to word B in language Y , we can generalize and guess a translation for word A because we have learned a distributed representation for words in language X and a distributed representation for words in language Y, then created a link possibly two-way) relating the two spaces, via training examples consisting of matched pairs of sentences in both languages. This transfer will be most successful if all three ingredients (the two representations and the relations between them) are learned jointly. https://www.deeplearningbook.org/contents/representation.html    Zero-shot learning is a particular form of transfer learning. The same principle  537  CHAPTER 15. REPRESENTATION LEARNING  ax&\u2014space  y\u2014space  \u2014 \u2014 = (x,y) pairs in the training set \u2014 f,: encoder function for \u00ab ooc> fy: encoder function for y ------ \u00bb Relationship between embedded points within one of the domains  <\u2014\u2014 = Maps between representation spaces  Figure 15.3: Transfer learning between two domains x and y enables zero-shot learning", "fd0fc04a-c23c-4e9b-a5c2-e523fd3d56b0": "The integral over w can now be evaluated simply by appealing to the standard result for the normalization coef\ufb01cient of a multivariate Gaussian, giving Exercise 3.19 Using (3.78) we can then write the log of the marginal likelihood in the form which is the required expression for the evidence function. Returning to the polynomial regression problem, we can plot the model evidence against the order of the polynomial, as shown in Figure 3.14. Here we have assumed a prior of the form (1.65) with the parameter \u03b1 \ufb01xed at \u03b1 = 5 \u00d7 10\u22123. The form of this plot is very instructive. Referring back to Figure 1.4, we see that the M = 0 polynomial has very poor \ufb01t to the data and consequently gives a relatively low value for the evidence. Going to the M = 1 polynomial greatly improves the data \ufb01t, and hence the evidence is signi\ufb01cantly higher. However, in going to M = 2, the data \ufb01t is improved only very marginally, due to the fact that the underlying sinusoidal function from which the data is generated is an odd function and so has no even terms in a polynomial expansion.\n\nIndeed, Figure 1.5 shows that the residual data error is reduced only slightly in going from M = 1 to M = 2", "8f7eda3d-6166-4aac-a1bd-82ab5685943e": "Feedforward networks are a conceptual stepping stone on the path to recurrent networks, which power many natural language applications. Feedforward neural networks are called networks because they are typically represented by composing together many different functions.\n\nThe model is asso- ciated with a directed acyclic graph describing how the functions are composed together. For example, we might have three functions f@), f@, and f@) connected in a chain, to form f(a) = f(f@(f(#))). These chain structures are the most commonly used structures of neural networks. In this case, fH is called  (2)  ro a ed . 1 L\u00a3 . Woot Be a 1 rat  https://www.deeplearningbook.org/contents/mlp.html    the urs tayer ot the network, J 18 called the 5evulu tayer, and so on. Lhe 164  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  overall length of the chain gives the depth of the model. The name \u201cdeep learning\u201d arose from this terminology", "17929747-0194-497a-8ef5-c35157fe2502": "For example, can be real valued. Srivastava e\u00a2 al. showed that multiplying the weights by pe~ N(1,J) can outperform dropout based on binary masks. Because E = 1, the standard network automatically implements approximate inference in the ensemble, without needing any weight scaling.\n\nSo far we have described dropout purely as a means of performing efficient, approximate bagging. Another view of dropout goes further than this. Dropout trains not just a bagged ensemble of models, but an ensemble of models that share hidden units. This means each hidden unit must be able to perform well regardless of which other hidden units are in the model. Hidden units must be prepared to be swapped and interchanged between models. Hinton ef al. were inspired by an idea from biology: sexual reproduction, which involves swapping genes between two different organisms, creates evolutionary pressure for genes to become not just good but readily swapped between different organisms. Such genes and such features are robust to changes in their environment because they are not able to incorrectly adapt to unusual features of any one organism or model", "1bf4ebda-cc0b-44fa-a508-e77f73a2571a": "The data may lie on many disconnected manifolds, but the class remains constant within each one of these. This assumption motivates a variety of learning algorithms, including tangent propagation, double backprop, the manifold tangent classifier and adversarial training. e Temporal and spatial coherence: Slow feature analysis and related algorithms make the assumption that the most important explanatory factors change slowly over time, or at least that it is easier to predict the true underlying explanatory factors than to predict raw observations such as pixel values. https://www.deeplearningbook.org/contents/representation.html    See section 13.3 for further description of this approach.\n\ne Sparsity: Most features should presumably not be relevant to describing most inputs\u2014there is no need to use a feature that detects elephant trunks when  representing an image of a cat. It is therefore reasonable to impose a prior that any feature that can be interpreted as \u201cpresent\u201d or \u201cabsent\u201d should be absent most of the time. e Simplicity of factor dependencies: In good high-level representations, the factors are related to each other through simple dependencies", "bcea1915-203c-47ef-b315-b5e1f022ef46": "First we write the tree-backup n-step return (7.16) in terms of the horizon h = t + n and then in terms of the expected approximate value \u00afV (7.8): = Rt+1 + \u03b3 \u00afVh\u22121(St+1) \u2212 \u03b3\u21e1(At+1|St+1)Qh\u22121(St+1, At+1) + \u03b3\u21e1(At+1|St+1)Gt+1:h after which it is exactly like the n-step return for Sarsa with control variates (7.14) except with the action probability \u21e1(At+1|St+1) substituted for the importance-sampling ratio \u21e2t+1. For Q(\u03c3), we slide linearly between these two cases: GT \u22121:T .= RT if h = T. Then we use the general (o\u21b5-policy) update for n-step Sarsa (7.11)", "637a3b1b-de7d-4acd-a558-380c6d846ff7": "Policies derived by reinforcement learning in simulated environments can advise human decision makers in such areas as education, healthcare, transportation, energy, and public-sector resource allocation. Particularly relevant is the key feature of reinforcement learning that it takes long-term consequences of decisions into account. This is very clear in games like backgammon and Go, where some of the most impressive results of reinforcement learning have been demonstrated, but it is also a property of many high-stakes decisions that a\u21b5ect our lives and our planet. Reinforcement learning follows related methods for advising human decision making that have been developed in the past by decision analysts in many disciplines.\n\nWith advanced function approximation methods and massive computational power, reinforcement learning methods have the potential to overcome some of the di\ufb03culties of scaling up traditional decision-support methods to larger and more complex problems. The rapid pace of advances in arti\ufb01cial intelligence has led to warnings that arti\ufb01cial intelligence poses serious threats to our societies, even to humanity itself. The renowned scientist and arti\ufb01cial intelligence pioneer Herbert Simon anticipated the warnings we are hearing today in a presentation at the Earthware Symposium at CMU in 2000", "1e6f8881-5939-4976-a393-8efcb75e00a3": ", which is hat the transition operator (defined by the stochastic mapping going from one state of the chain to the next) should satisfy a property called detailed balance, which specifies that a Markov chain at equilibrium will remain in equilibrium whether the transition operator is run in forward or reverse. An experiment in clamping half of the pixels (the right part of the image) and running the Markov chain on the other half is shown in figure 20.12. 20.11.38 Walk-Back Training Procedure  The walk-back training procedure was proposed by Bengio et al. as a way to accelerate the convergence of generative training of denoising autoencoders.\n\nInstead of performing a one-step encode-decode reconstruction, this procedure consists of alternative multiple stochastic encode-decode steps (as in the generative Markov chain), initialized at a training example (just as with the contrastive divergence algorithm, described in section 18.2), and penalizing the last probabilistic reconstructions (or all the reconstructions along the way). Training with & steps is equivalent (in the sense of achieving the same stationary distribution) as training with one step but practically has the advantage that spurious modes further from the data can be removed more efficiently. 709  CHAPTER 20", "dc88bef8-879d-4dd4-a0c5-c79ce8a7aeb8": "The question-answering example in Figure 1 will serve as a running example for this section. A distinctive feature of BERT is its uni\ufb01ed architecture across different tasks. There is minimal difference between the pre-trained architecture and the \ufb01nal downstream architecture. Model Architecture BERT\u2019s model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation described in Vaswani et al.\n\nand released in the tensor2tensor library.1 Because the use of Transformers has become common and our implementation is almost identical to the original, we will omit an exhaustive background description of the model architecture and refer readers to Vaswani et al. as well as excellent guides such as \u201cThe Annotated Transformer.\u201d2 In this work, we denote the number of layers (i.e., Transformer blocks) as L, the hidden size as H, and the number of self-attention heads as A.3 We primarily report results on two model sizes: BERTBASE (L=12, H=768, A=12, Total Parameters=110M) and BERTLARGE (L=24, H=1024, A=16, Total Parameters=340M)", "0743b513-d22f-4c0a-8ef2-e4dd119e2073": "Here we discuss an approximation in which we set the hyperparameters to speci\ufb01c values determined by maximizing the marginal likelihood function obtained by \ufb01rst integrating over the parameters w. This framework is known in the statistics literature as empirical Bayes , or type 2 maximum likelihood , or generalized maximum likelihood , and in the machine learning literature is also called the evidence approximation . If we introduce hyperpriors over \u03b1 and \u03b2, the predictive distribution is obtained by marginalizing over w, \u03b1 and \u03b2 so that where p(t|w, \u03b2) is given by (3.8) and p(w|t, \u03b1, \u03b2) is given by (3.49) with mN and SN de\ufb01ned by (3.53) and (3.54) respectively.\n\nHere we have omitted the dependence on the input variable x to keep the notation uncluttered", "d886e572-68d4-4c84-8b86-8fb04b1244e4": "Like supervised learning, contrastive learning bene\ufb01ts from deeper and wider networks. We combine these \ufb01ndings to achieve a new state-of-the-art in self-supervised and semi-supervised learning on ImageNet ILSVRC-2012 . Under the linear evaluation protocol, SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art . When \ufb01ne-tuned with only 1% of the ImageNet labels, SimCLR achieves 85.8% top-5 accuracy, a relative improvement of 10% .\n\nWhen \ufb01ne-tuned on other natural image classi\ufb01cation datasets, SimCLR performs on par with or better than a strong supervised baseline  on 10 out of 12 datasets. 2.1. The Contrastive Learning Framework Inspired by recent contrastive learning algorithms (see Section 7 for an overview), SimCLR learns representations by maximizing agreement between differently augmented views of the same data example via a contrastive loss in the latent space. As illustrated in Figure 2, this framework comprises the following four major components", "706b4d83-4926-4802-aa0b-e2ed348436bd": "One reason we have treated all states encountered equally is that then we are updating according to the on-policy distribution, for which stronger theoretical results are available for semi-gradient methods. Recall that the on-policy distribution was de\ufb01ned as the distribution of states encountered in an MDP while following the target policy. Now we will generalize this concept signi\ufb01cantly. Rather than having one on-policy distribution for the MDP, we will have many. All of them will have in common that they are a distribution of states encountered in trajectories while following the target policy, but they will vary in how the trajectories are, in a sense, initiated. We now introduce some new concepts.\n\nFirst we introduce a non-negative scalar measure, a random variable It called interest, indicating the degree to which we are interested in accurately valuing the state (or state\u2013action pair) at time t. If we don\u2019t care at all about the state, then the interest should be zero; if we fully care, it might be one, though it is formally allowed to take any non-negative value. The interest can be set in any causal way; for example, it may depend on the trajectory up to time t or the learned parameters at time t", "987dcca5-ce3d-4857-9563-d999e225b8f7": "Take \u00b5 = Pr \u2212 P\u03b8, which is a signed measure, and (P, Q) its Hahn decomposition. Then, we can de\ufb01ne f \u2217 := 1Q \u2212 1P . By construction, then Furthermore, if f is bounded between -1 and 1, we get Since \u03b4 is positive, we can conclude Ex\u223cPr \u2212 Ex\u223cP\u03b8 \u2265 \u2212\u03b4(Pr, P\u03b8).", "d3ee0843-08eb-4ed8-b77e-04d77764235d": "The better performance of networks trained with weights initialized this way could be due to many factors, but one idea is that this method places the network in a region of weight space from which a gradient-based algorithm can make good progress. Batch normalization  is another technique that makes it easier to train deep ANNs.\n\nIt has long been known that ANN learning is easier if the network input is normalized, for example, by adjusting each input variable to have zero mean and unit variance. Batch normalization for training deep ANNs normalizes the output of deep layers before they feed into the following layer. Io\u21b5e and Szegedy  used statistics from subsets, or \u201cmini-batches,\u201d of training examples to normalize these between-layer signals to improve the learning rate of deep ANNs. Another technique useful for training deep ANNs is deep residual learning (He, Zhang, Ren, and Sun, 2016). Sometimes it is easier to learn how a function di\u21b5ers from the identity function than to learn the function itself. Then adding this di\u21b5erence, or residual function, to the input produces the desired function. In deep ANNs, a block of layers can be made to learn a residual function simply by adding shortcut, or skip, connections around the block", "ff95c7a0-f634-4fad-8a08-891abd54afc3": "For some models, the addition of noise with infinitesimal variance at the input of the model is equivalent to imposing a penalty on the norm of the weights (Bishop, 1995a,b). In the general case, it is important to remember that noise injection can be much more powerful than simply shrinking the parameters, especially when the noise is added to the hidden units. Noise applied to the hidden units is such an important topic that it merits its own separate discussion; the dropout algorithm described in section 7.12 is the main development of that approach. Another way that noise has been used in the service of regularizing models is by adding it to the weights.\n\nThis technique has been used primarily in the context of recurrent neural networks . This can be interpreted as a stochastic implementation of Bayesian inference over the weights. The Bayesian treatment of learning would consider the model weights to be uncertain and representable via a probability distribution that reflects this uncertainty. Adding noise to the weights is a practical, stochastic way to reflect this uncertainty. Noise applied to the weights can also be interpreted as equivalent (under some assumptions) to a more traditional form of regularization, encouraging stability of the function to be learned", "51b6d495-a691-4a78-aaa8-afa2a649853b": "The DP target is an estimate not because of the expected values, which are assumed to be completely provided by a model of the environment, but because v\u21e1(St+1) is not known and the current estimate, V (St+1), is used instead. The TD target is an estimate for both reasons: it samples the expected values in (6.4) and it uses the current estimate V instead of the true v\u21e1. Thus, TD methods combine the sampling of Monte Carlo with the bootstrapping of DP. As we shall see, with care and imagination this can take us a long way toward obtaining the advantages of both Monte Carlo and DP methods. Shown to the right is the backup diagram for tabular TD(0).\n\nThe value estimate for the state node at the top of the backup diagram is updated on the basis of the one sample transition from it to the immediately following state. We refer to TD and Monte Carlo updates as sample updates because they involve looking ahead to a sample successor state (or state\u2013action pair), using the value of the successor and the reward along the way to compute a backed-up value, and then updating the value of the original state (or state\u2013 action pair) accordingly", "3271d86a-f89a-4e2d-bb96-d937fc56ccde": "DEEP GENERATIVE MODELS  https://www.deeplearningbook.org/contents/generative_models.html    PURY =1\\v,r) =0 (vw? + win) ; (20.27) and  PK =1| bh) =0 (nOTw?) (20.28)  The bipartite structure makes Gibbs sampling in a deep Boltzmann machine efficient. The naive approach to Gibbs sampling is to update only one variable at a time. RBMs allow all the visible units to be updated in one block and all the hidden units to be updated in a second block. One might naively assume that a DBM with / layers requires | + 1 updates, with each iteration updating a block consisting of one layer of units. Instead, it is possible to update all the units in only two iterations. Gibbs sampling can be divided into two blocks of updates, one including all even layers (including the visible layer) and the other including all odd layers. Because of the bipartite DBM connection pattern, given the even layers, the distribution over the odd layers is factorial and thus can be sampled simultaneously and independently as a block", "b5834aab-ad85-48c5-9e37-b95537495daa": "B., Hassabis, D., Spiers, H. J. Hippocampal place cells construct reward related sequences through unexplored space. Elife, 4:e06063. Oh, J., Guo, X., Lee, H., Lewis, R. L., Singh, S. Action-conditional video prediction using deep networks in Atari games. In Advances in Neural Information Processing Systems 28 , pp. 2845\u20132853. Curran Associates, Inc. Olds, J., Milner, P. Positive reinforcement produced by electrical stimulation of the septal O\u2019Reilly, R. C., Frank, M. J. Making working memory work: A computational model of learning in the prefrontal cortex and basal ganglia. Neural Computation, 18(2):283\u2013328. learned value Pavlovian learning algorithm. Behavioral Neuroscience, 121(1):31\u201349. Omohundro, S. M. E\ufb03cient algorithms with neural network behavior", "1161c9c4-0fe9-40ea-9741-89086b5d571f": "Self-supervised learning enables Al systems to learn from orders of magnitude more data, which is important to recognize and understand patterns of more subtle, less common representations of the world. Self- supervised learning has long had great success in advancing the field of natural language processing (NLP), including the Collobert-Weston 2008 model, Word2Vec, GIoVE, fastText, and, more recently, BERT, ROBERTa, XLM-R, and others. Systems pretrained this way  yield considerably higher performance than when solely trained in a supervised manner.\n\nOur latest research project SEER leverages SwAV and other methods to pretrain a large network ona billion random unlabeled images, yielding top accuracy ona diverse set of vision tasks. This progress demonstrates that self-supervised learning can excel at CV  tasks in complex, real-world settings as  https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/  Self-supervised learning: The dark matter of intelligence   ell. Today, we\u2019re sharing details on why self-supervised learning may be helpful in unlocking the dark matter of intelligence \u2014 and the next frontier of Al", "8a8e921a-e8c5-435b-abcb-260ee5ff2009": "The momentum algorithm accumulates an exponentially decaying moving average of past gradients and continues to move  292  CHAPTER 8.\n\nOPTIMIZATION FOR TRAINING DEEP MODELS  20  10  https://www.deeplearningbook.org/contents/optimization.html  Sw \u201410  \u201430 \u201420 -10 0 10.20  Figure 8.5: Momentum aims primarily to solve two problems: poor conditioning of the Hessian matrix and variance in the stochastic gradient. Here, we illustrate how momentum overcomes the first of these two problems. The contour lines depict a quadratic loss function with a poorly conditioned Hessian matrix. The red path cutting across the contours indicates the path followed by the momentum learning rule as it minimizes this function. At each step along the way, we draw an arrow indicating the step that gradient descent would take at that point. We can see that a poorly conditioned quadratic objective looks like a long, narrow valley or canyon with steep sides. Momentum correctly traverses the canyon lengthwise, while gradient steps waste time moving back and forth across the narrow axis of the canyon. Compare also figure 4.6, which shows the behavior of gradient descent without momentum. in their direction", "5c58025b-11cc-4813-b309-2152250af5a6": "In follow-on work, Tesauro and Galperin  explored trajectory sampling methods as an alternative to full-width search, which reduced the error rate of live play by large numerical factors (4x\u20136x) while keeping the think time reasonable at \u21e05\u201310 seconds per move. During the 1990s, Tesauro was able to play his programs in a signi\ufb01cant number of games against world-class human players. A summary of the results is given in Table 16.1. Based on these results and analyses by backgammon grandmasters , TD-Gammon 3.0 appeared to play at close to, or possibly better than, the playing strength of the best human players in the world.\n\nTesauro reported in a subsequent article  the results of an extensive rollout analysis of the move decisions and doubling decisions of TD-Gammon relative to top human players. The conclusion was that TD-Gammon 3.1 had a \u201clopsided advantage\u201d in piece-movement decisions, and a \u201cslight edge\u201d in doubling decisions, over top humans. TD-Gammon had a signi\ufb01cant impact on the way the best human players play the game", "60a4cc0d-55de-4738-8857-a98c34bddc02": "Much as a Turing machine\u2019s memory needs only to be able to store 0 or 1 states, we can build a universal function approximator from rectified linear functions,  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  affine transformation from an input vector to an output scalar. Now, we describe an affine transformation from a vector x to a vector h, so an entire vector of bias parameters is needed. The activation function g is typically chosen to be a function that is applied element-wise, with h; = ga! W. ; +c;). In modern neural networks, the default recommendation is to use the rectified linear unit, or ReLU , defined by the activation function g(z) = max{0, z}, depicted in figure 6.3. We can now specify our complete network as f(a;W,c,w,b) = w! max{0, Wlat c} +b. (6.3)  We can then specify a solution to the XOR problem. Let  w= it", "9954a6ac-8ed6-4fa2-80c6-224527edc7f6": "The dictionary is structured as a large FIFO queue of encoded representations of data samples. Given a query sample x,, we get a query representation through an encoder q = fa(Xq)- A list of key representations {k,, ky,.. } in the dictionary are encoded by a momentum encoder  k; = fh (x*). Let's assume among them there is a single positive key k* in the dictionary that matches q. In the paper, they create k* using a noise copy of x, with different augmentation. Then the InfoNCE contrastive loss with temperature 7 is used over one positive and NV \u2014 1 negative samples:  ex kt /r Catoce = \u2014log p(q-k\"/r)  i=1 \u20acxP(q- k;/7)  Compared to the memory bank, a queue-based dictionary in MoCo enables us to reuse representations of immediately preceding mini-batches of data", "565bea6c-03d9-469b-beec-045228224c17": "If we de\ufb01ne a joint distribution over observed and latent variables, the corresponding distribution of the observed variables alone is obtained by marginalization.\n\nThis allows relatively complex marginal distributions over observed variables to be expressed in terms of more tractable joint distributions over the expanded space of observed and latent variables. The introduction of latent variables thereby allows complicated distributions to be formed from simpler components. In this chapter, we shall see that mixture distributions, such as the Gaussian mixture discussed in Section 2.3.9, can be interpreted in terms of discrete latent variables. Continuous latent variables will form the subject of Chapter 12. As well as providing a framework for building more complex probability distributions, mixture models can also be used to cluster data. We therefore begin our discussion of mixture distributions by considering the problem of \ufb01nding clusters in a set of data points, which we approach \ufb01rst using a nonprobabilistic technique called the K-means algorithm . Then we introduce the latent variable Section 9.1 view of mixture distributions in which the discrete latent variables can be interpreted as de\ufb01ning assignments of data points to speci\ufb01c components of the mixture. A genSection 9.2 eral technique for \ufb01nding maximum likelihood estimators in latent variable models is the expectation-maximization (EM) algorithm", "a22b9001-54ee-46c5-9218-0e90f3f922fa": "The joint distribution Section 8.4.4 We seek an approximation q(x) that has the same factorization, so that Note that normalization constants have been omitted, and these can be re-instated at the end by local normalization, as is generally done in belief propagation.\n\nNow suppose we restrict attention to approximations in which the factors themselves factorize with respect to the individual variables so that which corresponds to the factor graph shown on the right in Figure 10.18. Because the individual factors are factorized, the overall distribution q(x) is itself fully factorized. Now we apply the EP algorithm using the fully factorized approximation. Suppose that we have initialized all of the factors and that we choose to re\ufb01ne factor the right is the corresponding factorized approximation. \ufffdfb(x2, x3) = \ufffdfb2(x2)\ufffdfb3(x3)", "b45170d1-8eb3-4f71-ac04-66a189b3d100": "For continuous-valued 2, the denoising criterion with Gaussian corruption and reconstruction distribution yields an estimator of the score that is applicable to general encoder and decoder parametrizations . This means a generic encoder-decoder architecture may be made to estimate the score  509  CHAPTER 14. AUTOENCODERS  https://www.deeplearningbook.org/contents/autoencoders.html    Figure 14.4: A denoising autoencoder is trained to map a corrupted data point & back to  the original data point x. We illustrate training examples x as red crosses lying near a  low-dimensional manifold, illustrated with the bold black line. We illustrate the corruption process C (& | x) with a gray circle of equiprobable corruptions. A gray arrow demonstrates how one training example is transformed into one sample from this corruption process. When the denoising autoencoder is trained to minimize the average of squared errors llg( f(\u20ac)) \u2014 #|P, the reconstruction g( f(%)) estimates Ex, x. panta(x)C (Xx) X |X]", "4afab07c-2021-4965-b257-3199424184bf": "5.26 (\u22c6 \u22c6) Consider a multilayer perceptron with arbitrary feed-forward topology, which is to be trained by minimizing the tangent propagation error function (5.127) in which the regularizing function is given by (5.128).\n\nShow that the regularization term \u2126 can be written as a sum over patterns of terms of the form where G is a differential operator de\ufb01ned by By acting on the forward propagation equations with the operator G, show that \u2126n can be evaluated by forward propagation using the following equations: where we have de\ufb01ned the new variables Now show that the derivatives of \u2126n with respect to a weight wrs in the network can be written in the form Write down the backpropagation equations for \u03b4kr, and hence derive a set of backpropagation equations for the evaluation of the \u03c6kr. 5.27 (\u22c6 \u22c6) www Consider the framework for training with transformed data in the special case in which the transformation consists simply of the addition of random noise x \u2192 x + \u03be where \u03be has a Gaussian distribution with zero mean and unit covariance. By following an argument analogous to that of Section 5.5.5, show that the resulting regularizer reduces to the Tikhonov form (5.135)", "fdf80700-c3bb-4b78-94f9-ff57be40af04": "This is not a team problem as de\ufb01ned here. A second requirement for collective learning in a team problem is that there has to be variability in the actions of the team members in order for the team to explore the space of collective actions. The simplest way for a team of reinforcement learning agents to do this is for each member to independently explore its own action space through persistent variability in its output. This will cause the team as a whole to vary its collective actions. For example, a team of the actor units described in Section 15.8 explores the space of collective actions because the output of each unit, being a Bernoulli-logistic unit, probabilistically depends on the weighted sum of its input vector\u2019s components. The weighted sum biases \ufb01ring probability up or down, but there is always variability.\n\nBecause each unit uses a REINFORCE policy gradient algorithm (Chapter 13), each unit adjusts its weights with the goal of maximizing the average reward rate it experiences while stochastically exploring its own action space", "3dd26edc-02e2-44f7-b0cc-8ff873f6fbfc": "We shall make use of the multivariate Gaussian distribution brie\ufb02y in this chapter, although its properties will be studied in detail in Section 2.3. Now suppose that we have a data set of observations x = (x1, . , xN)T, representing N observations of the scalar variable x. Note that we are using the typeface x to distinguish this from a single observation of the vector-valued variable (x1, . , xD)T, which we denote by x. We shall suppose that the observations are drawn independently from a Gaussian distribution whose mean \u00b5 and variance \u03c32 are unknown, and we would like to determine these parameters from the data set. Data points that are drawn independently from the same distribution are said to be independent and identically distributed, which is often abbreviated to i.i.d", "5c2927b6-4d22-4e1c-a3dd-2ecc07724d6c": "By learning a model Pmodel(#) and a representation Pyoge(h | 2), a generative model can provide answers to many inference problems about the relationships between input variables in x and can offer many different ways of representing x by taking expectations of h at different layers of the hierarchy.\n\nGenerative models hold the promise to provide AI systems with a framework for all the many different intuitive concepts they need to understand, giving them the ability to reason about these concepts in the face of uncertainty. We hope that our readers will find new ways to make these approaches more powerful and continue the journey to understanding the principles that underlie learning and intelligence. https://www.deeplearningbook.org/contents/generative_models.html    716  https://www.deeplearningbook.org/contents/generative_models.html", "29da0138-ec32-45f7-9fa9-44fa7c333ea3": "The dependencies on (r, w) were estimated from Watson\u2019s actual accuracies over many thousands of historical categories.\n\nWith the previously learned value function \u02c6v and in-category DD con\ufb01dence pDD, Watson computed \u02c6q(s, bet) for each legal round-dollar bet as follows: where SW is Watson\u2019s current score, and \u02c6v gives the estimated value for the game state after Watson\u2019s response to the DD clue, which is either correct or incorrect. Computing an action value this way corresponds to the insight from Exercise 3.19 that an action value is the expected next state value given the action (except that here it is the expected next afterstate value because the full next state of the entire game depends on the next square selection). Tesauro et al. found that selecting bets by maximizing action values incurred \u201ca frightening amount of risk,\u201d meaning that if Watson\u2019s response to the clue happened to be wrong, the loss could be disastrous for its chances of winning. To decrease the downside risk of a wrong answer, Tesauro et al. adjusted (16.2) by subtracting a small fraction of the standard deviation over Watson\u2019s correct/incorrect afterstate evaluations", "d393b14a-4dd3-4c13-83d1-5a126504f16d": "3.8 (\u22c6 \u22c6) www Consider the linear basis function model in Section 3.1, and suppose that we have already observed N data points, so that the posterior distribution over w is given by (3.49). This posterior can be regarded as the prior for the next observation. By considering an additional data point (xN+1, tN+1), and by completing the square in the exponential, show that the resulting posterior distribution is again given by (3.49) but with SN replaced by SN+1 and mN replaced by mN+1. 3.9 (\u22c6 \u22c6) Repeat the previous exercise but instead of completing the square by hand, make use of the general result for linear-Gaussian models given by (2.116).\n\n3.10 (\u22c6 \u22c6) www By making use of the result (2.115) to evaluate the integral in (3.57), verify that the predictive distribution for the Bayesian linear regression model is given by (3.58) in which the input-dependent variance is given by (3.59). 3.11 (\u22c6 \u22c6) We have seen that, as the size of a data set increases, the uncertainty associated with the posterior distribution over model parameters decreases", "5b7d6ab9-4aec-473c-8057-13b864e7937e": "We therefore consider conditional distributions of the target variable of the form p(t|\u03b7, s) = 1 Using the same line of argument as led to the derivation of the result (2.226), we see that the conditional mean of t, which we denote by y, is given by Thus y and \u03b7 must related, and we denote this relation through \u03b7 = \u03c8(y). Following Nelder and Wedderburn , we de\ufb01ne a generalized linear model to be one for which y is a nonlinear function of a linear combination of the input (or feature) variables so that y = f(wT\u03c6) (4.120) where f(\u00b7) is known as the activation function in the machine learning literature, and f \u22121(\u00b7) is known as the link function in statistics.\n\nNow consider the log likelihood function for this model, which, as a function of \u03b7, is given by where we are assuming that all observations share a common scale parameter (which corresponds to the noise variance for a Gaussian distribution for instance) and so s is independent of n", "83dfb307-0964-416f-86fc-10ec9e5c54f4": "While the phrase \u201cmultitask learning\u201d typically refers to supervised learning tasks, the more general notion of transfer learning is applicable to unsupervised learning and reinforcement learning as well. In all these cases, the objective is to take advantage of data from the first setting to extract information that may be useful when learning or even when directly making predictions in the second setting. The core idea of representation learning is that the same representation may be useful in both settings. Using the same representation in both settings allows the representation to benefit from the training data that is available for both tasks.\n\nAs mentioned before, unsupervised deep learning for transfer learning has found success in some machine learning competitions . In the first of these competitions, the experimental setup is the following. Each participant is first given a dataset from the first setting (from distribution P,), illustrating examples of some set of categories. The participants must use this to learn a good feature space (mapping the raw input to some representation), such that when we apply this learned transformation to inputs from the transfer setting (distribution P,), a linear classifier can be trained and generalize well from few labeled examples", "e8b13b4a-7ded-4c5a-b40e-42a417e84d90": "We describe several variants on the convolution function that are widely used in practice for neural networks. We also show how convolution may be applied to many kinds of data, with different numbers of dimensions. We chen discuss means of making convolution more efficient.\n\nConvolutional networks stand out as an example of neuroscientific principles influencing deep learning. We discuss these neuroscientific principles, then conclude with comments about he role convolutional networks have played in the history of deep learning. One opic this chapter does not address is how to choose the architecture of your convolutional network. The goal of this chapter is to describe the kinds of tools hat convolutional networks provide, while chapter 11 describes general guidelines  https://www.deeplearningbook.org/contents/convnets.html    326  CHAPTER 9. CONVOLUTIONAL NETWORKS  for choosing which tools to use in which circumstances. Research into convolutional network architectures proceeds so rapidly that a new best architecture for a given benchmark is announced every few weeks to months, rendering it impractical to describe in print the best architecture. Nonetheless, the best architectures have consistently been composed of the building blocks described here", "200d3a54-0f82-4f5c-a68b-f7e6c7add060": "The natural algorithm is then while the values of all other states remain unchanged: Qt+n(s, a) = Qt+n\u22121(s, a), for all s, a such that s 6= St or a 6= At. This is the algorithm we call n-step Sarsa. Pseudocode is shown in the box on the next page, and an example of why it can speed up learning compared to one-step methods is given in Figure 7.4. Initialize Q(s, a) arbitrarily, for all s 2 S, a 2 A Initialize \u21e1 to be \"-greedy with respect to Q, or to a \ufb01xed given policy Algorithm parameters: step size \u21b5 2 (0, 1], small \" > 0, a positive integer n All store and access operations (for St, At, and Rt) can take their index mod n + 1 Exercise 7.4 Prove that the n-step return of Sarsa (7.4) can be written exactly in terms of a novel TD error, as What about Expected Sarsa? The backup diagram for the n-step version of Expected Sarsa is shown on the far right in Figure 7.3", "890cf01e-ded7-494f-a1e3-3b216369f070": "The robot\u2019s control system has components for interpreting sensory information, for navigating, and for controlling the arm and gripper. High-level decisions about how to search for cans are made by a reinforcement learning agent based on the current charge level of the battery. To make a simple example, we assume that only two charge levels can be distinguished, comprising a small state set S = {high, low}. In each state, the agent can decide whether to (1) actively search for a can for a certain period of time, (2) remain stationary and wait for someone to bring it a can, or (3) head back to its home base to recharge its battery. When the energy level is high, recharging would always be foolish, so we do not include it in the action set for this state. The action sets are then A(high) = {search, wait} and A(low) = {search, wait, recharge}. The rewards are zero most of the time, but become positive when the robot secures an empty can, or large and negative if the battery runs all the way down.\n\nThe best way to \ufb01nd cans is to actively search for them, but this runs down the robot\u2019s battery, whereas waiting does not", "0a866308-b407-406e-864c-d762268779b1": "Although the bias-variance decomposition may provide some interesting insights into the model complexity issue from a frequentist perspective, it is of limited practical value, because the bias-variance decomposition is based on averages with respect to ensembles of data sets, whereas in practice we have only the single observed data set. If we had a large number of independent training sets of a given size, we would be better off combining them into a single large training set, which of course would reduce the level of over-\ufb01tting for a given model complexity. Given these limitations, we turn in the next section to a Bayesian treatment of linear basis function models, which not only provides powerful insights into the issues of over-\ufb01tting but which also leads to practical techniques for addressing the question model complexity. In our discussion of maximum likelihood for setting the parameters of a linear regression model, we have seen that the effective model complexity, governed by the number of basis functions, needs to be controlled according to the size of the data set.\n\nAdding a regularization term to the log likelihood function means the effective model complexity can then be controlled by the value of the regularization coef\ufb01cient, although the choice of the number and form of the basis functions is of course still important in determining the overall behaviour of the model", "591025fd-12a2-43e4-b7da-eae1276d51a3": "The dashed line indicates the value of the  .\u00a2 at VW ad 1 ay rose at 1 1  https://www.deeplearningbook.org/contents/numerical.html    COsl LULICLIOLL WE WOULU EXPECL VASCU OLL LLLE ZrAaACIELLL LIOLMAaLLON aALOWEe ad WE LllaKke a rradient step downhill. With negative curvature, the cost function actually decreases aster than the gradient predicts. With no curvature, the gradient predicts the decrease correctly.\n\nWith positive curvature, the function decreases more slowly than expected and  eventually begins to increase, so steps that are too large can actually increase the function inadvertently. figure 4.4 to see how different forms of curvature affect the relationship between the value of the cost function predicted by the gradient and the true value. When our function has multiple input dimensions, there are many second derivatives. These derivatives can be collected together into a matrix called the Hessian matrix. The Hessian matrix H(f)(a) is defined such that  e2 A(f)(x);; =~ f (2)", "64472bc3-ccdc-4319-b570-adf2424875dd": "If \u03b3 = 0.9, then we can consider that with probability 0.1 the process terminates on every time step and then immediately restarts in the state that is transitioned to.\n\nA discounted problem is one that is continually terminating and restarting with probability 1 \u2212 \u03b3 on every step. This way of thinking about discounting is an example of a more general notion of pseudo termination\u2014termination that does not a\u21b5ect the sequence of state transitions, but does a\u21b5ect the learning process and the quantities being learned. This kind of pseudo termination is important to o\u21b5-policy learning because the restarting is optional\u2014remember we can start any way we want to\u2014and the termination relieves the need to keep including encountered states within the on-policy distribution. That is, if we don\u2019t consider the new states as restarts, then discounting quickly gives us a limited on-policy distribution. The one-step Emphatic-TD algorithm for learning episodic state values is de\ufb01ned by: with It, the interest, being arbitrary and Mt, the emphasis, being initialized to Mt\u22121 = 0", "d8fe9e12-13e2-4dcb-ae61-780616e8f90a": "Trained in neurophysiology and long interested in machine intelligence, Harry was a senior scientist a\ufb03liated with the Avionics Directorate of the Air Force O\ufb03ce of Scienti\ufb01c Research (AFOSR) at Wright-Patterson Air Force Base, Ohio. He was dissatis\ufb01ed with the great importance attributed to equilibrium-seeking processes, including homeostasis and error-correcting pattern classi\ufb01cation methods, in explaining natural intelligence and in providing a basis for machine intelligence.\n\nHe noted that systems that try to maximize something (whatever that might be) are qualitatively di\u21b5erent from equilibriumseeking systems, and he argued that maximizing systems hold the key to understanding important aspects of natural intelligence and for building arti\ufb01cial intelligences. Harry was instrumental in obtaining funding from AFOSR for a project to assess the scienti\ufb01c merit of these and related ideas. This project was conducted in the late 1970s at the University of Massachusetts Amherst (UMass Amherst), initially under the direction of Michael Arbib, William Kilmer, and Nico Spinelli, professors in the Department of Computer and Information Science at UMass Amherst, and founding members of the Cybernetics Center for Systems Neuroscience at the University, a farsighted group focusing on the intersection of neuroscience and arti\ufb01cial intelligence. Barto, a recent Ph.D", "ea2155b5-18f8-4f66-bab3-317cb87ead83": "Our view of reinforcement learning has been signi\ufb01cantly enriched by discussions with Paul Cohen, Paul Utgo\u21b5, Martha Steenstrup, Gerry Tesauro, Mike Jordan, Leslie Kaelbling, Andrew Moore, Chris Atkeson, Tom Mitchell, Nils Nilsson, Stuart Russell, Tom Dietterich, Tom Dean, and Bob Narendra. We thank Michael Littman, Gerry Tesauro, Bob Crites, Satinder Singh, and Wei Zhang for providing speci\ufb01cs of Sections 4.7, 15.1, 15.4, 15.4, and 15.6 respectively.\n\nWe thank the Air Force O\ufb03ce of Scienti\ufb01c Research, the National Science Foundation, and GTE Laboratories for their long and farsighted support. We also wish to thank the many people who have read drafts of this book and provided valuable comments, including Tom Kalt, John Tsitsiklis, Pawel Cichosz, Olle G\u00a8allmo, Chuck Anderson, Stuart Russell, Ben Van Roy, Paul Steenstrup, Paul Cohen, Sridhar Mahadevan, Jette Randlov, Brian Sheppard, Thomas O\u2019Connell, Richard Coggins, Cristina Versino, John H", "85758af1-c920-46aa-96db-5f767a404bc2": "It can also be useful to make versions of convolution or locally connected layers in which the connectivity is further restricted, for example to constrain each output channel j to be a function of only a subset of the input channels /. A common way to do this is to make the first m output channels connect to only the first n input channels, the second m output channels connect to only the second n input channels, and so on. See figure 9.15 for an example. Modeling interactions between few channels allows the network to have fewer parameters, reducing memory consumption, increasing statistical efficiency, and reducing the amount of computation needed to perform forward and back-propagation. It accomplishes these goals without reducing the number of hidden units. Tiled convolution  offers a com- promise between a convolutional layer and a locally connected layer. Rather than learning a separate set of weights at every spatial location, we learn a set of kernels that we rotate through as we move through space", "02388012-28e6-4f59-a541-e488ac0d5a7a": "Let \u21b5n(a) denote the step-size parameter used to process the reward received after the nth selection of action a. As we have noted, the choice \u21b5n(a) = 1 which is guaranteed to converge to the true action values by the law of large numbers. But of course convergence is not guaranteed for all choices of the sequence {\u21b5n(a)}. A well-known result in stochastic approximation theory gives us the conditions required to assure convergence with probability 1: The \ufb01rst condition is required to guarantee that the steps are large enough to eventually overcome any initial conditions or random \ufb02uctuations. The second condition guarantees that eventually the steps become small enough to assure convergence. Note that both convergence conditions are met for the sample-average case, \u21b5n(a) = 1 but not for the case of constant step-size parameter, \u21b5n(a) = \u21b5. In the latter case, the second condition is not met, indicating that the estimates never completely converge but continue to vary in response to the most recently received rewards", "263cac85-5cd7-4d7e-a1bc-aa5d1cd348e9": "We usually give matrices uppercase variable names with bold typeface, such as A. If a real-valued matrix A has a height of m and a width of n, then we say that A \u20ac R\u201d*\u201d. We usually identify the elements of a matrix using its name in italic but not bold font, and the indices are listed with separating commas. For example, A;, is the upper left entry of A and A,,,, is the bottom right entry. We can identify  https://www.deeplearningbook.org/contents/linear_algebra.html  all the numbers with vertical coordinate ? by writing a \u201c:\u201d for the horigontal cro i wit  or t coordinate. For example, 4%: denotes the horizontal cross section of vertical coordinate i. This is known as the +th row of A. Likewise, A. ; is  30  CHAPTER 2. LINEAR ALGEBRA  the i-th column of A. When we need to explicitly identify the elements of a matrix, we write them as an array enclosed in square brackets:  Ai At y y", "564ce5f7-5725-4160-8b28-abf59f9a936c": "Injecting noise in the input to a neural network  can also be seen as a form of data augmentation. For many classification and even some regression tasks, the task should still be possible to solve even if small random noise is added to the input. Neural networks prove not to be very robust to noise, however . One way to improve the robustness of neural networks is simply to train them with random noise applied to their inputs.\n\nInput noise injection is part of some unsupervised learning algorithms, such as the denoising autoencoder . Noise injection also works when the noise is applied to the hidden units, which can be seen as doing dataset augmentation at multiple levels of abstraction. Poole et al. recently showed that this approach can be highly effective provided that the magnitude of the noise is carefully tuned. Dropout, a powerful regularization strategy that will be described in section 7.12, can be seen as a process of constructing new inputs by multiplying by noise. When comparing machine learning benchmark results, taking the effect of dataset augmentation into account is important. Often, hand-designed dataset augmentation schemes can dramatically reduce the generalization error of a ma- chine learning technique", "f18612d1-1ca7-4c22-9b38-cdb7a7d7ba0e": "This means we can obtain a model with better validation set error (and thus, hopefully better test set error) by returning to the parameter setting at the point in time with the lowest validation set error.\n\nEvery time the error on the validation set improves, we store a copy of the model parameters. When the training algorithm terminates, we return these parameters, rather than the latest parameters. The algorithm terminates when no parameters have improved over the best recorded validation error for some pre-specified number of iterations. This procedure is specified more formally in algorithm 7.1. This strategy is known as early stopping. It is probably the most commonly used form of regularization in deep learning. Its popularity is due to both its effectiveness and its simplicity. One way to think of early stopping is as a very efficient hyperparameter selection algorithm. In this view, the number of training steps is just another hyperparameter. We can see in figure 7.3 that this hyperparameter has a U-shaped validation set performance curve. Most hyperparameters that control model capacity have such a U-shaped validation set performance curve, as illustrated in figure 5.3", "5d2f6f21-c8dd-4479-9f3b-2eaa9dbd8641": "If we consider the limit N \u226b M in which the number of data points is large in relation to the number of parameters, then from (3.87) all of the parameters will be well determined by the data because \u03a6T\u03a6 involves an implicit sum over data points, and so the eigenvalues \u03bbi increase with the size of the data set. In this case, \u03b3 = M, and the re-estimation equations for \u03b1 and \u03b2 become where EW and ED are de\ufb01ned by (3.25) and (3.26), respectively. These results can be used as an easy-to-compute approximation to the full evidence re-estimation synthetic data set. It is the intersection of these two curves that de\ufb01nes the optimum value for \u03b1 given by the evidence procedure.\n\nThe right plot shows the corresponding graph of log evidence ln p(t|\u03b1, \u03b2) versus ln \u03b1 (red curve) showing that the peak coincides with the crossing point of the curves in the left plot. Also shown is the test set error (blue curve) showing that the evidence maximum occurs close to the point of best generalization. formulae, because they do not require evaluation of the eigenvalue spectrum of the Hessian. 3.6", "ca2ff36a-fc77-4549-ad92-c4ce21e78d7c": "See algorithm 1 for a basic approach to compute the stochastic gradients. A connection with auto-encoders becomes clear when looking at the objective function given at eq. (7). The \ufb01rst term is (the KL divergence of the approximate posterior from the prior) acts as a regularizer, while the second term is a an expected negative reconstruction error. The function g\u03c6(.) is chosen such that it maps a datapoint x(i) and a random noise vector \u03f5(l) to a sample from the approximate posterior for that datapoint: z(i,l) = g\u03c6(\u03f5(l), x(i)) where z(i,l) \u223c q\u03c6(z|x(i)). Subsequently, the sample z(i,l) is then input to function log p\u03b8(x(i)|z(i,l)), which equals the probability density (or mass) of datapoint x(i) under the generative model, given z(i,l)", "488fb87b-1f91-47b7-b388-5f588d4e41d9": "We can use this approach to de\ufb01ne a kernel function measuring the similarity of two sequences X and X\u2032 by extending the mixture representation (6.29) to give so that both observed sequences are generated by the same hidden sequence Z. This model can easily be extended to allow sequences of differing length to be compared. An alternative technique for using generative models to de\ufb01ne kernel functions is known as the Fisher kernel . Consider a parametric generative model p(x|\u03b8) where \u03b8 denotes the vector of parameters. The goal is to \ufb01nd a kernel that measures the similarity of two input vectors x and x\u2032 induced by the generative model. Jaakkola and Haussler  consider the gradient with respect to \u03b8, which de\ufb01nes a vector in a \u2018feature\u2019 space having the same dimensionality as \u03b8. In particular, they consider the Fisher score from which the Fisher kernel is de\ufb01ned by Here F is the Fisher information matrix, given by where the expectation is with respect to x under the distribution p(x|\u03b8).\n\nThis can be motivated from the perspective of information geometry , which considers the differential geometry of the space of model parameters", "c9ddc06a-2809-4e61-b6c5-d0bd9caa3071": "As a speci\ufb01c example, we consider once again the sequential estimation of the mean of a Gaussian distribution, in which case the parameter \u03b8(N) is the estimate \u00b5(N) ML of the mean of the Gaussian, and the random variable z is given by Thus the distribution of z is Gaussian with mean \u00b5 \u2212 \u00b5ML, as illustrated in Figure 2.11.\n\nSubstituting (2.136) into (2.135), we obtain the univariate form of (2.126), provided we choose the coef\ufb01cients aN to have the form aN = \u03c32/N. Note that although we have focussed on the case of a single variable, the same technique, together with the same restrictions (2.130)\u2013(2.132) on the coef\ufb01cients aN, apply equally to the multivariate case . The maximum likelihood framework gave point estimates for the parameters \u00b5 and \u03a3. Now we develop a Bayesian treatment by introducing prior distributions over these parameters. Let us begin with a simple example in which we consider a single Gaussian random variable x", "0e2c8f47-df15-48a1-b367-09395c987383": "Of these, the most prominent is ill-conditioning of the Hessian matrix H. This is a very general problem in most numerical optimization, convex or otherwise, and is described in more detail in section 4.3.1. The ill-conditioning problem is generally believed to be present in neural network training problems. Ill-conditioning can manifest by causing SGD to get \u201cstuck\u201d in the sense that even very small steps increase the cost function.\n\nRecall from equation 4.9 that a second-order Taylor series expansion of the cost function predicts that a gradient descent step of \u2014eg will add  5\u00a29' Ha \u2014eg'g (8.10)  to the cost. Ill-conditioning of the gradient becomes a problem when 4\u00a2\u00b0g 'H. g exceeds eg'g. To determine whether ill-conditioning is detrimental to a neural  279  CHAPTER 8", "da6dafb7-7f9d-4a83-bf11-a5e57f3cb62b": "Other features of computational reinforcement learning, such as eligibility traces and the ability of teams of reinforcement learning agents to learn to act collectively under the in\ufb02uence of a globally-broadcast reinforcement signal, may also turn out to parallel experimental data as neuroscientists continue to unravel the neural basis of reward-based animal learning and behavior.\n\nThe number of publications treating parallels between the neuroscience of learning and decision making and the approach to reinforcement learning presented in this book is enormous. We can cite only a small selection. Niv , Dayan and Niv , Gimcher , Ludvig, Bellemare, and Pearson , and Shah  are good places to start. Together with economics, evolutionary biology, and mathematical psychology, reinforcement learning theory is helping to formulate quantitative models of the neural mechanisms of choice in humans and non-human primates. With its focus on learning, this chapter only lightly touches upon the neuroscience of decision making. Glimcher  introduced the \ufb01eld of \u201cneuroeconomics,\u201d in which reinforcement learning contributes to the study of the neural basis of decision making from an economics perspective. See also Glimcher and Fehr . The text on computational and mathematical modeling in neuroscience by Dayan and Abbott  includes reinforcement learning\u2019s role in these approaches", "0c55d043-67fb-4b55-a107-dc86d01615cf": "For example, if it were black\u2019s move, he could use the dice roll of 2 to move a piece from the 24 point to the 22 point, \u201chitting\u201d the white piece there.\n\nPieces that have been hit are placed on the \u201cbar\u201d in the middle of the board (where we already see one previously hit black piece), from whence they reenter the race from the start. However, if there are two pieces on a point, then the opponent cannot move to that point; the pieces are protected from being hit. Thus, white cannot use his 5\u20132 dice roll to move either of his pieces on the 1 point, because their possible resulting points are occupied by groups of black pieces. Forming contiguous blocks of occupied points to block the opponent is one of the elementary strategies of the game. Backgammon involves several further complications, but the above description gives the basic idea. With 30 pieces and 24 possible locations (26, counting the bar and o\u21b5-the-board) it should be clear that the number of possible backgammon positions is enormous, far more than the number of memory elements one could have in any physically realizable computer. The number of moves possible from each position is also large", "e23a3e21-6ebc-4ecd-86c9-fdeb09b5b32b": "For this reason we rule them out for use on large problems where function approximation is most needed. To get some intuitive feel for how to set the step-size parameter manually, it is best to go back momentarily to the tabular case. There we can understand that a step size of \u21b5 = 1 will result in a complete elimination of the sample error after one target (see (2.4) with a step size of one). As discussed on page 201, we usually want to learn slower than this. In the tabular case, a step size of \u21b5 = 1 10 would take about 10 experiences to approach the mean of its targets, with the most recent targets having the greatest e\u21b5ect, after about \u2327 experiences with the state. With general function approximation there is not such a clear notion of number of experiences with a state, as each state may be similar to and dissimilar from all the others to various degrees. However, there is a similar rule that gives similar behavior in the case of linear function approximation", "f30c0b7d-688c-42ae-b1b8-62b082355aa4": "The distributions introduced in this chapter will also serve another important purpose, namely to provide us with the opportunity to discuss some key statistical concepts, such as Bayesian inference, in the context of simple models before we encounter them in more complex situations in later chapters. One role for the distributions discussed in this chapter is to model the probability distribution p(x) of a random variable x, given a \ufb01nite set x1, . , xN of observations. This problem is known as density estimation. For the purposes of this chapter, we shall assume that the data points are independent and identically distributed.\n\nIt should be emphasized that the problem of density estimation is fundamentally ill-posed, because there are in\ufb01nitely many probability distributions that could have given rise to the observed \ufb01nite data set. Indeed, any distribution p(x) that is nonzero at each of the data points x1, . , xN is a potential candidate. The issue of choosing an appropriate distribution relates to the problem of model selection that has already been encountered in the context of polynomial curve \ufb01tting in Chapter 1 and that is a central issue in pattern recognition", "0eb7f4dc-6708-408d-8d7f-02261e960b78": "We would like to further explore the combinatorics of GAN samples with other augmentation techniques such as applying a range of style transfers to GAN-generated samples. Super-resolution networks through the use of SRCNNs, Super-Resolution Convolu- tional Neural Networks, and SRGANs are also very interesting areas for future work in Data Augmentation. We want to explore the performance differences across archi- tectures with upsampled images such as expanding CIFAR-10 images from 32 x 32 to 64 x 64 to 128 x 128 and so on. One of the primary difficulties with GAN samples is try- ing to achieve high-resolution outputs. Therefore, it will be interesting to see how we  can use super-resolution networks to achieve high-resolution such as DCGAN samples Shorten and Khoshgoftaar J Big Data  6:60   inputted into an SRCNN or SRGAN. The result of this strategy will be compared with the performance of the Progressively Growing GAN architecture. Test-time augmentation has the potential to make a massive difference in Computer Vision performance and has not been heavily explored", "6c6d5ad8-95d4-49ba-b76c-396549622292": "(Heuristic search methods such as A\u21e4 are almost always based on the episodic case.) The methods of dynamic programming can be related even more closely to the Bellman optimality equation. Many reinforcement learning methods can be clearly understood as approximately solving the Bellman optimality equation, using actual experienced transitions in place of knowledge of the expected transitions. We consider a variety of such methods in the following chapters. Exercise 3.20 Draw or describe the optimal state-value function for the golf example. \u21e4 Exercise 3.24 Figure 3.5 gives the optimal value of the best state of the gridworld as 24.4, to one decimal place. Use your knowledge of the optimal policy and (3.8) to express this value symbolically, and then to compute it to three decimal places. \u21e4 We have de\ufb01ned optimal value functions and optimal policies. Clearly, an agent that learns an optimal policy has done very well, but in practice this rarely happens.\n\nFor the kinds of tasks in which we are interested, optimal policies can be generated only with extreme computational cost. A well-de\ufb01ned notion of optimality organizes the approach to learning we describe in this book and provides a way to understand the theoretical properties of various learning algorithms, but it is an ideal that agents can only approximate to varying degrees", "b2b7d9b2-0a80-42cb-b514-c3f350a19f9b": "Thus, when a node receives a message on one of its links, this creates pending messages on all of its other links. Only pending messages need to be transmitted because other messages would simply duplicate the previous message on the same link. For graphs that have a tree structure, any schedule that sends only pending messages will eventually terminate once a message has passed in each direction across every link. At this point, there are no pending messages, and the product of the received Exercise 8.29 messages at every variable give the exact marginal. In graphs having loops, however, the algorithm may never terminate because there might always be pending messages, although in practice it is generally found to converge within a reasonable time for most applications. Once the algorithm has converged, or once it has been stopped if convergence is not observed, the (approximate) local marginals can be computed using the product of the most recently received incoming messages to each variable node or factor node on every link. In some applications, the loopy belief propagation algorithm can give poor results, whereas in other applications it has proven to be very effective.\n\nIn particular, state-of-the-art algorithms for decoding certain kinds of error-correcting codes are equivalent to loopy belief propagation", "360c272d-adb0-4907-9c1f-ea597e5635f5": "It is also useful to learn how the environment generates rewards.\n\nIn this case, examples are of the form S\u2013R or SA\u2013R, where R is a reward signal associated with S or the SA pair. These are all forms of supervised learning by which an agent can acquire cognitive-like maps whether or not it receives any non-zero reward signals while exploring its environment. The distinction between model-free and model-based reinforcement learning algorithms corresponds to the distinction psychologists make between habitual and goal-directed control of learned behavioral patterns. Habits are behavior patterns triggered by appropriate stimuli and then performed more-or-less automatically. Goal-directed behavior, according to how psychologists use the phrase, is purposeful in the sense that it is controlled by knowledge of the value of goals and the relationship between actions and their consequences. Habits are sometimes said to be controlled by antecedent stimuli, whereas goal-directed behavior is said to be controlled by its consequences . Goal-directed control has the advantage that it can rapidly change an animal\u2019s behavior when the environment changes its way of reacting to the animal\u2019s actions. While habitual behavior responds quickly to input from an accustomed environment, it is unable to quickly adjust to changes in the environment", "baf4ebac-8032-409f-bce7-b7fd7cdb490c": "Among the variants, one can choose to use the cell state s  LSTM networks have been shown to learn long-term dependencies more easily than the simple recurrent architectures, first on artificial datasets designed for testing the ability to learn long-term dependencies , then on challenging sequence processing tasks where state-of-the-art performance was obtained . Variants and alternatives to the LSTM that have been studied and used are discussed next.\n\n10.10.2 Other Gated RNNs  Which pieces of the LSTM architecture are actually necessary? What other successful architectures could be designed that allow the network to dynamically control the time scale and forgetting behavior of different units? Some answers to these questions are given with the recent work on gated RNNs, whose units are also known as gated recurrent units, or GRUs . The main difference with the LSTM is that a single gating unit simultaneously controls the forgetting factor and the decision to update the state unit. The update equations are the following:  HO uM Lalo Taga age PAOD J J  (10.45) where u stands for \u201cupdate\u201d gate and r for deesot\u201d gate", "a95ed249-4388-4ae7-8ea7-412652dc827d": "Chapter 15  Representation Learning  In this chapter, we first discuss what it means to learn representations and how the notion of representation can be useful to design deep architectures. We explore how learning algorithms share statistical strength across different tasks, including using information from unsupervised tasks to perform supervised tasks. Shared representations are useful to handle multiple modalities or domains, or to transfer learned knowledge to tasks for which few or no examples are given but a task representation exists. Finally, we step back and argue about the reasons for the success of representation learning, starting with the theoretical advantages of distributed representations  and deep representations, ending with the more general idea of underlying assumptions about the data-generating process, in particular about underlying causes of the observed data. Many information processing tasks can be very easy or very difficult depending on how the information is represented. This is a general principle applicable to daily life, to computer science in general, and to machine learning. For example, it is straightforward for a person to divide 210 by 6 using long division. The task becomes considerably less straightforward if it is instead posed using the Roman numeral representation of the numbers.\n\nMost modern people asked to divide CCX by VI would begin by converting the numbers to the Arabic numeral representation, permitting long division procedures that make use of the place value system", "30762230-7111-49ea-9be0-e675ab9ac73f": "Let  s=) ola) f(x) = Elf] (17.1)  s= [ v(x) f(e)ae = BF) (17.2)  be the sum or integral to estimate, rewritten as an expectation, with the constraint that p is a probability distribution (for the sum) or a probability density (for the integral) over random variable x. We can approximate s by drawing n samples #@),..., a from p and then forming the empirical average  bn = LY? f(a. (17.3) i=1  This approximation is justified by a few different properties.\n\nThe first trivial observation is that the estimator \u00a7 is unbiased, since  n n  BlJ=>  EY@O== sas (17.4) 3 3  https://www.deeplearningbook.org/contents/monte_carlo.html    But in addition, the law of large numbers states that if the samples \u00a3\u2018\u2019\u2019 are ii.d., then the average converges almost surely to the expected value:  lim &, =, (17.5)  n->oCo  588  CHAPTER 17", "c6da1c9e-bae2-4606-8fd8-024bd079d4f8": "Section 10 reviews related work.\n\nSection 11 concludes the article with discussion of future directions\u2014in particular, we discuss the broader aspects of ML not covered in the present work (e.g., more advanced learning such as continual learning in complex evolving environments, theoretical analysis of learnability, generalization and complexity, and automated algorithm composition) and how their uni\ufb01ed characterization based on or inspired by the current framework could potentially lead toward a full \u2018Standard Model\u2019 of ML and a turnkey approach to panoramic learning with all types of experience. 2", "6db7a6ac-741c-4b6b-b22b-5e72899dffe3": "Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv. https://arxiv.org/abs/1805.00909 Li, L., Littman, M. L., Walsh, T. J., & Strehl, A. L. Knows what it knows: A framework for self-aware learning. Machine Learning, 3(82), 399\u2013443. Li, X. L., & Liang, P. Pre\ufb01x-tuning: Optimizing continuous prompts for generation. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 4582\u20134597. Liu, G., Feng, Z., Gao, Y., Yang, Z., Liang, X., Bao, J., He, X., Cui, S., Li, Z., & Hu, Z. Composable text controls in latent space with ODEs.\n\narXiv", "2ce36cc6-ce16-4ea8-84dd-b34ced10c270": "Here a 'Gaussian' kernel of the form is applied to a synthetic data set.\n\nThe lines correspond to contours along which the projection onto the corresponding principal component, defined by One obvioo' dls.aJmota~eof I:emel ! 'CA Is thaf if invoh'es finding lhe elgen\"e<tors of the N x N malri>: K raW. Ihan lhe D x D malri, S of cor,..emionallinear ! 'CA. and!iO In prac1lce for large data \"'1' appro,lmation< are often uS(:d Finally. \"\"e OOIe that i\" <tandard linear I'CA, we often retain some redoce<l num\u00b7 ber L < Dof eigenvectors and then appro,lmale 0 data vttl<:>r Xn b}' its projection I\" kernell'CA", "75cae5df-3ee1-4ffb-a26d-c334f8344cd1": "We pay the computational cost of producing a coefficient for many cells, but we expect these coefficients to cluster around a small number of cells. By reading a vector value, rather than a scalar value, we can offset some of this cost.\n\nAnother reason to use vector valued memory cells is that they allow for content-based addressing, where the weight used to read to or write from a cell is a function of that cell. Vector valued cells allow us to retrieve a complete vector valued memory if we are able to produce a pattern that matches some but not all its elements. This is analogous to how people can recall the lyrics of a song based on a few words. We can think of a content-based read instruction as saying, \u201cRetrieve the lyrics of the song that has the chorus \u2018We all live in a yellow submarine.\u2019\u201d Content-based addressing is more useful when we make the objects to be retrieved large\u2014if every letter of the song was stored in a separate memory cell, we would not be able to find them this way. By comparison, location-based addressing is not allowed to refer to the content of the memory", "5f3fee32-ddbd-4af6-bac0-e0fe1483cf1a": "0F(@) _ I(0+eu) - (6) 00, \u20ac  Or analytically,  J (8) = Ex slr] = J) dyo(8) >) (als; 6) R(s, a)  seS acA  Actually we have nice theoretical support for (replacing d(. ) with d,(. )):  T(0) = S~dz,(s) S> m(als;0)Q,(s,a) x S$ d(s) S> r(a\\s; 0)Q,(s, a)  seS acA seS acA  Check Sec 13.1 in Sutton & Barto  for why this is the case", "f53411bb-1a22-4066-b69a-932ea017c0dd": "S OOP PPL HH JN 2V VBA QBQBOSAAPPPPKLHLH ds ~ 2 SVQ QaKsDAPNPNNHKHDE SSM QQNAD ND WWNNNHO -\u2014~wwwwmmama mow www www OO cK KK ee ennnwMVVWWDIDoOOoO cK KK eee OOnwuwNuvoo cK KK eee OOwmnwwwwwuDgod eK KKK EO OOSWWWWWOODOODO HK KK KAHN HSHSWSWWHWWHaBOODO HFK KK KRHHHHHHHHHHQABOOO HFK KAHHHHOHHHYHHHQRWDOCOO S=FFBHAHAHAHH HHHHHHRQAWOO =\u201c=F=FAHHH HHHHHHHHHAWOHO S~SANHHHHEHVHSSSEEEOOOHO SS AASSSSLPRSVNVSVSELEOGH  https://www.deeplearningbook.org/contents/generative_models.html   eee  Figure 20.6: Examples of 2-D coordinate systems for high-dimensional manifolds, learned by a variational autoencoder ", "93781d6e-16c5-4b72-b664-ec87ae1deb2a": "Then, considering all limits as n \u2192 \u221e, \u2022 \u03b4(Pn, P) \u2192 0 with \u03b4 the total variation distance. \u2022 JS(Pn, P) \u2192 0 with JS the Jensen-Shannon divergence. 3. KL(Pn\u2225P) \u2192 0 or KL(P\u2225Pn) \u2192 0 imply the statements in (1). 4. The statements in (1) imply the statements in (2). This highlights the fact that the KL, JS, and TV distances are not sensible cost functions when learning distributions supported by low dimensional manifolds. However the EM distance is sensible in that setup. This obviously leads us to the next section where we introduce a practical approximation of optimizing the EM distance.\n\nAgain, Theorem 2 points to the fact that W(Pr, P\u03b8) might have nicer properties when optimized than JS(Pr, P\u03b8). However, the in\ufb01mum in (1) is highly intractable", "eb908359-c28c-439b-b790-802d3318a8f5": "In: Conference on Innovative Data Systems Research  47. Rekatsinas, T., Chu, X., Ilyas, I.F., R\u00e9, C.: HoloClean: Holistic data repairs with probabilistic inference. PVLDB 10(11), 1190\u20131201  48.\n\nRekatsinas,T.,Joglekar,M.,Garcia-Molina,H.,Parameswaran,A., R\u00e9, C.: SLiMFast: Guaranteed results for data fusion and source reliability. In: ACM SIGMOD International Conference on Management of Data (SIGMOD)  49. Riedel, S., Yao, L., McCallum, A.: Modeling relations and their mentions without labeled text. In: European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD)  50. Roth, B., Klakow, D.: Combining generative and discriminative model scores for distant supervision. In: Conference on Empirical Methods on Natural Language Processing (EMNLP)  51", "a47be785-9bcd-41a3-b3c2-e4b62c171617": "Each of these ideas taken from DP is extended to the Monte Carlo case in which only sample experience is available. We begin by considering Monte Carlo methods for learning the state-value function for a given policy. Recall that the value of a state is the expected return\u2014expected cumulative future discounted reward\u2014starting from that state. An obvious way to estimate it from experience, then, is simply to average the returns observed after visits to that state. As more returns are observed, the average should converge to the expected value. This idea underlies all Monte Carlo methods. In particular, suppose we wish to estimate v\u21e1(s), the value of a state s under policy \u21e1, given a set of episodes obtained by following \u21e1 and passing through s. Each occurrence of state s in an episode is called a visit to s. Of course, s may be visited multiple times in the same episode; let us call the \ufb01rst time it is visited in an episode the \ufb01rst visit to s. The \ufb01rst-visit MC method estimates v\u21e1(s) as the average of the returns following \ufb01rst visits to s, whereas the every-visit MC method averages the returns following all visits to s", "0c28dc6f-9c71-41ad-a021-293e5116e028": "(The exceptions are cases of direct electric coupling between neurons, but these will not concern us here.) Neurotransmitter molecules released from the presynaptic side of the synapse di\u21b5use across the synaptic cleft, the very small space between the presynaptic ending and the postsynaptic neuron, and then bind to receptors on the surface of the postsynaptic neuron to excite or inhibit its spike-generating activity, or to modulate its behavior in other ways. A particular neurotransmitter may bind to several di\u21b5erent types of receptors, with each producing a di\u21b5erent e\u21b5ect on the postsynaptic neuron.\n\nFor example, there are at least \ufb01ve di\u21b5erent receptor types by which the neurotransmitter dopamine can a\u21b5ect a postsynaptic neuron. Many di\u21b5erent chemicals have been identi\ufb01ed as neurotransmitters in animal nervous systems", "f5bb080e-e805-42c4-b42e-dbb6fbabf753": "The dot over the equals sign in the equation reminds us that it is a de\ufb01nition (in this case of the function p) rather than a fact that follows from previous de\ufb01nitions. The dynamics function p : S \u21e5 R \u21e5 S \u21e5 A ! is an ordinary deterministic function of four arguments. The \u2018|\u2019 in the middle of it comes from the notation for conditional probability, 1We use the terms agent, environment, and action instead of the engineers\u2019 terms controller, controlled system (or plant), and control signal because they are meaningful to a wider audience. 2We restrict attention to discrete time to keep things as simple as possible, even though many of the ideas can be extended to the continuous-time case . 3To simplify notation, we sometimes assume the special case in which the action set is the same in all 4We use Rt+1 instead of Rt to denote the reward due to At because it emphasizes that the next reward and next state, Rt+1 and St+1, are jointly determined", "41a58e54-66d8-44b7-b162-7c83dca8c8f2": "(s! s'cS V,(s) = max (R(s,a) +7) / PyyV-(s')) ac s'eS Q,(s,a) = R(s, a) +7 >) PS max Qs (s',a\u2019) s'cS  Unsurprisingly they look very similar to Bellman expectation equations. If we have complete information of the environment, this turns into a planning problem, solvable by DP. Unfortunately, in most scenarios, we do not know P\u201c,, or R(s, a), so we cannot solve MDPs by directly applying Bellmen equations, but it lays the theoretical foundation for many RL algorithms. Common Approaches  Now it is the time to go through the major approaches and classic algorithms for solving RL problems. In future posts, | plan to dive into each approach further.\n\nDynamic Programming  When the model is fully known, following Bellman equations, we can use Dynamic Programming (DP) to iteratively evaluate value functions and improve policy", "49ce61df-a82d-4f9e-98a2-d761ff580b48": "GQ(\u03bb) is the Gradient-TD algorithm for action values with eligibility traces. Its goal is to learn a parameter wt such that \u02c6q(s, a, wt) .= w> data. If the target policy is \"-greedy, or otherwise biased toward the greedy policy for \u02c6q, then GQ(\u03bb) can be used as a control algorithm. Its update is where \u00afxt is the average feature vector for St under the target policy, t is the expectation form of the TD error, which can be written zt is de\ufb01ned in the usual way for action values (12.29), and the rest is as in GTD(\u03bb), including the update for vt (12.30). HTD(\u03bb) is a hybrid state-value algorithm combining aspects of GTD(\u03bb) and TD(\u03bb). Its most appealing feature is that it is a strict generalization of TD(\u03bb) to o\u21b5-policy learning, meaning that if the behavior policy happens to be the same as the target policy, then HTD(\u03bb) becomes the same as TD(\u03bb), which is not true for GTD(\u03bb)", "7560d479-2460-4be2-98c9-3d9e638dda43": "The panoramic learning we discussed here makes use of broader forms of experience not necessarily amenable to be converted into supervised labels, such as reward, discriminator-like models, and many structured constraints. The experience function f(y) o\ufb00ers such \ufb02exibility for expressing all those experiences. We have presented a standardized machine learning formalism, materialized as the standard equation of the objective function, that formulates a vast algorithmic space governed by a few components regarding the experience, model \ufb01tness measured with divergence, and uncertainty. The formalism gives a holistic view of the diverse landscape of learning paradigms, allows a mechanical way of designing ML approaches to new problems, and provides a vehicle toward panoramic learning that integrates all available experience in building an AI agent. The work shapes a range of exciting open questions and opportunities for future study. We discuss a few of these directions below. Continual learning in complex dynamic environments", "9490114d-a403-4ebf-9dae-f773f77d207c": "https://www.deeplearningbook.org/contents/rnn.html    10.11.1 Clipping Gradients  As discussed in section 8.2.4, strongly nonlinear functions, such as those computed by a recurrent net over many time steps, tend to have derivatives that can be either very large or very small in magnitude. This is illustrated in figure 8.3 and in figure 10.17, in which we see that the objective function (as a function of the parameters) has a \u201clandscape\u201d in which one finds \u201ccliffs\u201d: wide and rather flat regions separated by tiny regions where the objective function changes quickly, forming a kind of cliff.\n\nThe difficulty that arises is that when the parameter gradient is very large, a gradient descent parameter update could throw the parameters very far into a region where the objective function is larger, undoing much of the work that had been done to reach the current solution. The gradient tells us the direction that corresponds to the steepest descent within an infinitesimal region surrounding the current parameters. Outside this infinitesimal region, the cost function may  409  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  begin to curve back upward", "9aca9b05-556e-4bcb-9f67-678ab9932714": "Contrastive divergence is slow for deep Boltzmann machines because they do not allow efficient sampling of the hidden units given the visible units\u2014instead, contrastive divergence would require burning in a Markov chain every time a new negative phase sample is needed. The nonvariational version of the stochastic maximum likelihood algorithm is discussed in section 18.2. Variational stochastic maximum likelihood as applied to the DBM is given in algorithm 20.1. Recall that we describe a simplified variant of the DBM that lacks bias parameters; including them is trivial. 20.4.4 Layer-Wise Pretraining  Unfortunately, training a DBM using stochastic maximum likelihood (as described above) from a random initialization usually results in failure. In some cases, the model fails to learn to represent the distribution adequately.\n\nIn other cases, the  https://www.deeplearningbook.org/contents/generative_models.html    DBM may represent the distribution well, but with no higher likelihood than could be obtained with just an RBM. A DBM with very small weights in all but the first layer represents approximately the same distribution as an RBM. Various techniques that permit joint training have been developed and are described in section 20.4.5", "e7ccd8e8-d49a-412b-bad3-08ddeb388c40": "We often refer to the error rate as the expected 0-1 loss. The 0-1 loss on a particular example is 0 if it is correctly classified and 1 if it is not. For tasks such as density estimation, it does not make sense to measure accuracy, error rate, or any other kind of 0-1 loss. Instead, we must use a different performance metric that gives the model a continuous-valued score for each example. The most common approach is to report the average log-probability the model assigns to some examples. Usually we are interested in how well the machine learning algorithm performs on data that it has not seen before, since this determines how well it will work when deployed in the real world. We therefore evaluate these performance measures using a test set of data that is separate from the data used for training the machine learning system. The choice of performance measure may seem straightforward and objective, but it is often difficult to choose a performance measure that corresponds well to the desired behavior of the system", "258b27dc-f16e-42ee-8c83-9880a4cce44c": "APPLICATIONS  equation 12.8, the gradient can be written as follows:  Olog P(y| C) _ Alog softmax,(a)  12.1 00 00 (12.18) 0 ew =\u2014 \u2014\u2014 12.14 30 log yew ( ) a ai, 6 = ap (tu \u2014 log > e%) (12.15) Ody . 0a; = = 12.1 wot Pu = 105 (12.16)  where a is the vector of presoftmax activations (or scores), with one element per word. The first term is the positive phase term, pushing a, up, while the second term is the negative phase term, pushing a; down for all 7, with weight P(i| C). Since the negative phase term is an expectation, we can estimate it with a Monte Carlo sample. However, that would require sampling from the model itself. Sampling from the model requires computing P(i | C) for all i in the vocabulary, which is precisely what we are trying to avoid.\n\nInstead of sampling from the model, we can sample from another distribution, called the proposal distribution (denoted q), and use appropriate weights to correct for the bias introduced by sampling from the wrong distribution", "d78d7e97-7d0f-41f9-a739-6ed33dc0cbdc": "The tree-backup algorithm is due to Precup, Sutton, and Singh , but the presentation of it here is new. 7.6 The Q(\u03c3) algorithm is new to this text, but closely related algorithms have been explored further by De Asis, Hernandez-Garcia, Holland, and Sutton . In this chapter we develop a uni\ufb01ed view of reinforcement learning methods that require a model of the environment, such as dynamic programming and heuristic search, and methods that can be used without a model, such as Monte Carlo and temporal-di\u21b5erence methods. These are respectively called model-based and model-free reinforcement learning methods. Model-based methods rely on planning as their primary component, while model-free methods primarily rely on learning. Although there are real di\u21b5erences between these two kinds of methods, there are also great similarities. In particular, the heart of both kinds of methods is the computation of value functions.\n\nMoreover, all the methods are based on looking ahead to future events, computing a backed-up value, and then using it as an update target for an approximate value function", "0326fd68-ead4-4ab9-b3f0-54b614586b20": "The variational framework for Gaussian process classi\ufb01cation can also be extended to multiclass (K > 2) problems by using a Gaussian approximation to the softmax function . A second approach uses expectation propagation (Opper and Winther, 2000b; Section 10.7 Minka, 2001b; Seeger, 2003). Because the true posterior distribution is unimodal, as we shall see shortly, the expectation propagation approach can give good results. The third approach to Gaussian process classi\ufb01cation is based on the Laplace approximation, which we now consider in detail", "986f81d7-aaa4-4340-8b2f-475544916cc9": "Now consider the derivative of the term involving ln |A| in (3.86) with respect to \u03b1.\n\nWe have Thus the stationary points of (3.86) with respect to \u03b1 satisfy Multiplying through by 2\u03b1 and rearranging, we obtain Since there are M terms in the sum over i, the quantity \u03b3 can be written The interpretation of the quantity \u03b3 will be discussed shortly. From (3.90) we see that the value of \u03b1 that maximizes the marginal likelihood satis\ufb01es Exercise 3.20 Note that this is an implicit solution for \u03b1 not only because \u03b3 depends on \u03b1, but also because the mode mN of the posterior distribution itself depends on the choice of \u03b1. We therefore adopt an iterative procedure in which we make an initial choice for \u03b1 and use this to \ufb01nd mN, which is given by (3.53), and also to evaluate \u03b3, which is given by (3.91). These values are then used to re-estimate \u03b1 using (3.92), and the process repeated until convergence. Note that because the matrix \u03a6T\u03a6 is \ufb01xed, we can compute its eigenvalues once at the start and then simply multiply these by \u03b2 to obtain the \u03bbi", "11db2562-1070-4dbc-a6cb-d07e98ee4fe1": "Specifically, every real symmetric matrix can be decomposed into an expression using only real-valued eigenvectors and eigenvalues:  A=QAQ|, (2.41)  where Q is an orthogonal matrix composed of eigenvectors of A, and A is a diagonal matrix. The eigenvalue A; ; is associated with the eigenvector in column i of Q, denoted as Q,;. Because Q is an orthogonal matrix, we can think of A as scaling space by \\; in direction v. See figure 2.3 for an example. While any real symmetric matrix A is guaranteed to have an eigendecomposi- tion, the eigendecomposition may not be unique. If any two or more eigenvectors share the same eigenvalue, then any set of orthogonal vectors lying in their span are also eigenvectors with that eigenvalue, and we could equivalently choose a Q using those eigenvectors instead. By convention, we usually sort the entries of A in descending order. Under this convention, the eigendecomposition is unique only if all the eigenvalues are unique", "747749a8-af02-4dd3-96e0-61a25c22c824": "The final debiased contrastive loss looks like:  exp( f(x)\" f(x\") ]  cN\u2122M =E No yg ga  using  importance sampling where both the partition functions Z,, Z5 can be estimated empirically.\n\nhttps://lilianweng.github.io/posts/2021-05-31-contrastive/  Contrastive Representation Learning | Lil'Log   u~gslexP(f(x) ' f(u))] = Eucal \u00bb exPlf(%)\u201d fu) = wl, P((8+ 1) f(x)\" f(u))]  vagy exP(F(X) ' f(\u00a5))] = Ev.pe  = volgen ((8 + DAG)\" F(W))] P Z;  1] # pos : exp of inner products for positive examples 2) # neg : exp of inner products for negative examples 3) 4 N : number of negative examples  Je ot : temperature scaling  s|# tau_plus: class probability  6|# beta : concentration parameter  8] #Original objective 9| standard_loss = -log(pos.sum() / (pos.sum() + neg.sum()))  11] #Debiased objective 122]}Neg = max((-N*tau_plus\u00abpos + neg).sum() / (1-tau_plus), e\u00ab\u00ab*(-1/t)) 13} debiased_loss = -log(pos.sum() / (pos.sum() + Neg))  1s} #Hard sampling objective (Ours)  16] reweight = (beta*neg) / neg.mean() 17|Neg = max((-N*tau_plus*pos + reweight*neg).sum() / (1-tau_plus), e**(-1/t)) hard_loss = -log( pos.sum() / (pos.sum() + Neg))  Vision: Image Embedding  Image Augmentations  Most approaches for contrastive representation learning in the vision domain rely on creating a noise version of a sample by applying a sequence of data augmentation techniques", "552c6cd3-94bc-4c4c-b5f3-88d71cf46dcd": "Earlier in this book we presented Monte Carlo and temporal-di\u21b5erence methods as distinct alternatives, then showed how they can be uni\ufb01ed by n-step methods. Our goal in this chapter is a similar integration of model-based and model-free methods. Having established these as distinct in earlier chapters, we now explore the extent to which they can be intermixed. By a model of the environment we mean anything that an agent can use to predict how the environment will respond to its actions. Given a state and an action, a model produces a prediction of the resultant next state and next reward. If the model is stochastic, then there are several possible next states and next rewards, each with some probability of occurring. Some models produce a description of all possibilities and their probabilities; these we call distribution models. Other models produce just one of the possibilities, sampled according to the probabilities; these we call sample models. For example, consider modeling the sum of a dozen dice", "a25a497a-c4f3-4ed8-a87e-cef3fe0f4a82": "Suppose that we have a design matrix of m example inputs that we will not use for training, only for evaluating how well the model performs. We also have a vector of regression targets providing the correct value of y for each of these examples. Because this dataset will only be used for evaluation, we call it the test set. We refer to the design matrix of inputs as X (test) and the vector of regression targets as y (est)  One way of measuring the performance of the model is to compute the mean squared error of the model on the test set. If gs\") gives the predictions of the model on the test set, then the mean squared error is given by  1 ~ eS\u2019 ies MSEtest = \u2014 So(gl) = yO)? (5.4)  Intuitively, one can see that this error measure decreases to 0 when g{tes*) = y (test),  We can also see that 1 Z eS\u2019 es MSEtest = |\" %) \u2014 yltes (5, (5.5)  so the error increases whenever the Euclidean distance between the predictions and the targets increases", "cb3584cd-0f92-4eb4-a403-bb85dd965cd9": "In addition, on some problems the policy is just simpler to represent parametrically than the value function; these problems are more suited to parameterized policy methods. Parameterized policy methods also have an important theoretical advantage over action-value methods in the form of the policy gradient theorem, which gives an exact formula for how performance is a\u21b5ected by the policy parameter that does not involve derivatives of the state distribution. This theorem provides a theoretical foundation for all policy gradient methods. The REINFORCE method follows directly from the policy gradient theorem. Adding a state-value function as a baseline reduces REINFORCE\u2019s variance without introducing bias. Using the state-value function for bootstrapping introduces bias but is often desirable for the same reason that bootstrapping TD methods are often superior to Monte Carlo methods (substantially reduced variance). The state-value function assigns credit to\u2014critizes\u2014the policy\u2019s action selections, and accordingly the former is termed the critic and the latter the actor, and these overall methods are termed actor\u2013critic methods", "928879e9-6e26-4e07-a48a-5ea2a9783ceb": "Input: two men on bicycles competing in a race . Generated: two men are riding bikes . Input: families waiting in line at an amusement park for their turn to ride . Generated: families at a amusement park . Input: man in a black suit , white shirt and black bowtie playing an instrument with the rest of his symphony surrounding him . Generated: a man is playing music . Input: a white dog with long hair jumps to catch a red and green toy . Generated: a dog is jumping Input: a man in a black shirt is playing golf outside . Generated: a man is playing golf Input: a man wearing sunglasses is sitting on the steps outside , reading a magazine . Generated: a man is sitting outside . Input: a young child is jumping into the arms of a woman wearing a black swimming suit while in a pool . Generated: a child is jumping into a pool . Input: a carefully balanced male stands on one foot near a clean ocean beach area . Generated: a man is on the beach", "81f9b886-aa2d-4120-969c-6368f0f0e9b6": "One approach to accelerating mixing during learning relies not on changing the Monte Carlo sampling technology but rather on changing the parametrization of the model and the cost function. Fast PCD, or FPCD  involves replacing the parameters @ of a traditional model with an expression  0 \u2014 o(slow) 4 @(fast) | (18.16)  There are now twice as many parameters as before, and they are added together element-wise to provide the parameters used by the original model definition. The ast copy of the parameters is trained with a much larger learning rate, allowing it to adapt rapidly in response to the negative phase of learning and push the Markov chain to new territory. This forces the Markov chain to mix rapidly, though his effect occurs only during learning while the fast weights are free to change. Typically one also applies significant weight decay to the fast weights, encouraging hem to converge to small values, after only transiently taking on large values long enough to encourage the Markov chain to change modes", "a4851d7e-0dc1-48fd-a4f8-8321626dd511": "There are N ways to choose the \ufb01rst object, (N \u2212 1) ways to choose the second object, and so on, leading to a total of N! ways to allocate all N objects to the bins, where N! (pronounced \u2018factorial N\u2019) denotes the product N \u00d7(N \u22121)\u00d7\u00b7 \u00b7 \u00b7\u00d72\u00d71. However, we don\u2019t wish to distinguish between rearrangements of objects within each bin. In the ith bin there are ni! ways of reordering the objects, and so the total number of ways of allocating the N objects to the bins is given by which is called the multiplicity. The entropy is then de\ufb01ned as the logarithm of the multiplicity scaled by an appropriate constant We now consider the limit N \u2192 \u221e, in which the fractions ni/N are held \ufb01xed, and apply Stirling\u2019s approximation i ni = N. Here pi = limN\u2192\u221e(ni/N) is the probability of an object being assigned to the ith bin. In physics terminology, the speci\ufb01c arrangements of objects in the bins is called a microstate, and the overall distribution of occupation numbers, expressed through the ratios ni/N, is called a macrostate", "e061ec82-e96a-4d2f-a378-2dd8f3712500": "If we set the derivative of (9.55) with respect to \u00b5k equal to zero and rearrange the terms, we obtain Exercise 9.15 We see that this sets the mean of component k equal to a weighted mean of the data, with weighting coef\ufb01cients given by the responsibilities that component k takes for data points.\n\nFor the maximization with respect to \u03c0k, we need to introduce a Lagrange multiplier to enforce the constraint \ufffd which represents the intuitively reasonable result that the mixing coef\ufb01cient for component k is given by the effective fraction of points in the data set explained by that component. Note that in contrast to the mixture of Gaussians, there are no singularities in which the likelihood function goes to in\ufb01nity. This can be seen by noting that the likelihood function is bounded above because 0 \u2a7d p(xn|\u00b5k) \u2a7d 1. There exist Exercise 9.17 singularities at which the likelihood function goes to zero, but these will not be found by EM provided it is not initialized to a pathological starting point, because the EM algorithm always increases the value of the likelihood function, until a local maximum is found. We illustrate the Bernoulli mixture model in Figure 9.10 by Section 9.4 using it to model handwritten digits", "b959b835-7a1c-464e-9430-7dbe14904f1d": "We do not ordinarily think of rollout algorithms as learning algorithms because they do not maintain long-term memories of values or policies. However, these algorithms take advantage of some of the features of reinforcement learning that we have emphasized in this book. As instances of Monte Carlo control, they estimate action values by averaging the returns of a collection of sample trajectories, in this case trajectories of simulated interactions with a sample model of the environment.\n\nIn this way they are like reinforcement learning algorithms in avoiding the exhaustive sweeps of dynamic programming by trajectory sampling, and in avoiding the need for distribution models by relying on sample, instead of expected, updates. Finally, rollout algorithms take advantage of the policy improvement property by acting greedily with respect to the estimated action values. Monte Carlo Tree Search (MCTS) is a recent and strikingly successful example of decisiontime planning. At its base, MCTS is a rollout algorithm as described above, but enhanced by the addition of a means for accumulating value estimates obtained from the Monte Carlo simulations in order to successively direct simulations toward more highly-rewarding trajectories", "4f58f9ae-95ac-4581-a321-374c0aebe20f": "Now the later estimate is not one step later, but n steps later. Methods in which the temporal di\u21b5erence extends over n steps are called n-step TD methods. The TD methods introduced in the previous chapter all used one-step updates, which is why we called them one-step TD methods. More formally, consider the update of the estimated value of state St as a result of the state\u2013reward sequence, St, Rt+1, St+1, Rt+2, . , RT , ST (omitting the actions). We know that in Monte Carlo updates the estimate of v\u21e1(St) is updated in the direction of the where T is the last time step of the episode. Let us call this quantity the target of the update", "5ad9167d-51cb-4c03-9d49-b92b55b2f0a9": "There has been insu\ufb03cient experience with o\u21b5-policy Monte Carlo methods to assess how serious this problem is. If it is serious, the most important way to address it is probably by incorporating temporaldi\u21b5erence learning, the algorithmic idea developed in the next chapter. Alternatively, if \u03b3 is less than 1, then the idea developed in the next section may also help signi\ufb01cantly. Exercise 5.11 In the boxed algorithm for o\u21b5-policy MC control, you may have been expecting the W update to have involved the importance-sampling ratio \u21e1(At|St) like those shown in Figure 5.5. You want to go as fast as possible, but not so fast as to run o\u21b5 the track. In our simpli\ufb01ed racetrack, the car is at one of a discrete set of grid positions, the cells in the diagram.\n\nThe velocity is also discrete, a number of grid cells moved horizontally and vertically per time step. The actions are increments to the velocity components. Each may be changed by +1, \u22121, or 0 in each step, for a total of nine (3 \u21e5 3) actions", "81ac88bd-4dbc-4d8d-b934-18210cffae61": "Extensions to networks of temporal relationships were developed by Tanner  and then extended to options . The theory of reinforcement learning with a non-Markov state representation was developed explicitly by Singh, Jaakkola, and Jordan . Early reinforcement learning approaches to partial observability were developed by Chrisman , McCallum , Parr and Russell , Littman, Cassandra, and Kaelbling , and by Lin and Mitchell . Skinner\u2019s shaping should not be confused with the \u201cpotential-based shaping\u201d technique introduced by Ng, Harada, and Russell . Their technique has been shown by Wiewiora  to be equivalent to the simpler idea of providing an initial approximation to the value function, as in (17.10). 17.5 We recommend the book by Goodfellow, Bengio, and Courville  for discussion of today\u2019s deep learning techniques.\n\nThe problem of catastrophic interference in ANNs was developed by McCloskey and Cohen , Ratcli\u21b5 , and French . The idea of a replay bu\u21b5er was introduced by Lin  and used prominently in deep learning in the Atari game playing system", "1d5dbcd3-8f55-463b-b99a-076657738145": "Speci\ufb01cally, the parameter of the function approximator has three components, one giving the value of state B, one giving the value of state C, and one giving the value of both states A1 and A2. Other than the selection of the initial state, the system is deterministic. If it starts in A1, then it transitions to B with a reward of 0 and then on to termination with a reward of 1. If it starts in A2, then it transitions to C, and then to termination, with both rewards zero. To a learning algorithm, seeing only the features, the system looks identical to the A-split example. The system seems to always start in A, followed by either B or C with equal probability, and then terminating with a 1 or a 0 depending deterministically on the previous state. As in the A-split example, the true values of B and C are 1 and 0, and the best shared value of A1 and A2 is 1 Because this problem appears externally identical to the A-split example, we transitions are deterministic, so the non-naive residual-gradient algorithm will also converge to these values (it is the same algorithm in this case)", "c72e22af-649d-40d2-a25e-13017a0c6145": "With the KKT approach, we introduce a new function called the generalized Lagrangian or generalized Lagrange function. To define the Lagrangian, we first need to describe S in terms of equations and inequalities. We want a description of S in terms of m functions g\u00ae and n functions h) so that S = {a | Vi, g(x) =0 and Vj,h\u00ae (a) < 0}. The equations involving g are called the equality constraints, and the inequalities involving h\u00ae@) are called inequality constraints. We introduce new variables A; and a; for each constraint, these are called the KKT multipliers. The generalized Lagrangian is then defined as  L(a, X, a) )+ 2d a) + So ajha). (4.14)  We can now solve a constrained minimization problem using unconstrained optimization of the generalized Lagrangian", "478faa10-b265-419f-a30d-f9c14d41b390": "The true values of all the states, A through E, are 1 6, 2 The left graph above shows the values learned after various numbers of episodes on a single run of TD(0).\n\nThe estimates after 100 episodes are about as close as they ever come to the true values\u2014with a constant step-size parameter (\u21b5 = 0.1 in this example), the values \ufb02uctuate inde\ufb01nitely in response to the outcomes of the most recent episodes. The right graph shows learning curves for the two methods for various values of \u21b5. The performance measure shown is the root mean-squared (RMS) error between the value function learned and the true value function, averaged over the \ufb01ve states, then averaged over 100 runs. In all cases the approximate value function was initialized to the intermediate value V (s) = 0.5, for all s. The TD method was consistently better than the MC method on this task. Exercise 6.3 From the results shown in the left graph of the random walk example it appears that the \ufb01rst episode results in a change in only V (A)", "0cd94b82-d353-43f4-9781-1df249ef6b56": "This can be understood quantitatively through an estimate of the variance of our estimate Z:  ~ fo Ks (x(h)  \\? Var (4:) = 4 \u00bb (Be a . (18.46)  This quantity is largest when there is significant deviation in the values of the Bix) Po(x(\u00ae))\u00b0  We now turn to two related strategies developed to cope with the challeng-  importance weights =  ing task of estimating partition functions for complex distributions over high- dimensional spaces: annealed importance sampling and bridge sampling. Both start with the simple importance sampling strategy introduced above, and both attempt to overcome the problem of the proposal po being too far from p; by introducing intermediate distributions that attempt to bridge the gap between pp and p", "3439cd44-74b8-47e4-89da-d41a9c398e50": "Figure 14.9 illustrates how a manifold can be tiled by  516  CHAPTER 14.\n\nAUTOENCODERS  https://www.deeplearningbook.org/contents/autoencoders.html    Figure 14.9: If the tangent planes (see figure 14.6) at each location are known, then they can be tiled to form a global coordinate system or a density function. Each local patch can be thought of as a local Euclidean coordinate system or as a locally flat Gaussian, or \u201cpancake,\u201d with a very small variance in the directions orthogonal to the pancake and a very large variance in the directions defining the coordinate system on the pancake. A mixture of these Gaussians provides an estimated density function, as in the manifold Parzen window algorithm  or in its nonlocal neural-net-based variant . CHAPTER 14. AUTOENCODERS  a large number of locally linear Gaussian-like patches (or \u201cpancakes,\u201d because the Gaussians are flat in the tangent directions)", "3e86044b-f1e7-4e61-9c0d-2924a4d7bc4c": "Thus a data point that is on the decision boundary y(xn) = 0 will have \u03ben = 1, and points with \u03ben > 1 will be misclassi\ufb01ed.\n\nThe exact classi\ufb01cation constraints (7.5) are then replaced with in which the slack variables are constrained to satisfy \u03ben \u2a7e 0. Data points for which \u03ben = 0 are correctly classi\ufb01ed and are either on the margin or on the correct side of the margin. Points for which 0 < \u03ben \u2a7d 1 lie inside the margin, but on the correct side of the decision boundary, and those data points for which \u03ben > 1 lie on the wrong side of the decision boundary and are misclassi\ufb01ed, as illustrated in Figure 7.3. This is sometimes described as relaxing the hard margin constraint to give a soft margin and allows some of the training set data points to be misclassi\ufb01ed. Note that while slack variables allow for overlapping class distributions, this framework is still sensitive to outliers because the penalty for misclassi\ufb01cation increases linearly with \u03be. Our goal is now to maximize the margin while softly penalizing points that lie on the wrong side of the margin boundary", "95817e94-fd39-46d2-aee9-bad01476b4ae": "Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention \ufb02ow for machine comprehension. In ICLR. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631\u20131642. Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL \u201910, pages 384\u2013394. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need", "9e29bf20-0d31-43b7-92da-52683bf05332": "As with GANs, it is possible to train a generator net using MMD even if that generator net assigns zero probability to the training points. 20.10.6 Convolutional Generative Networks  When generating images, it is often useful to use a generator network that includes a convolutional structure (see, for example, Goodfellow et al. or Dosovitskiy et al. ). To do so, we use the \u201ctranspose\u201d of the convolution operator, described in section 9.5. This approach often yields more realistic images and does so using fewer parameters than using fully connected layers without parameter sharing.\n\nConvolutional networks for recognition tasks have information flow from the image to some summarization layer at the top of the network, often a class label. As this image flows upward through the network, information is discarded as the representation of the image becomes more invariant to nuisance transformations. In a generator network, the opposite is true. Rich details must be added as the representation of the image to be generated propagates through the network, culminating in the final representation of the image, which is of course the image itself, in all its detailed glory, with object positions and poses and textures and lighting", "75eb924d-fe44-42f7-b634-00520a931b9c": "Previously, the size of the step was simply the norm of the gradient multiplied by the learning rate.\n\nNow, the size of the step depends on how large and how aligned a sequence of gradients are. The step size is largest when many successive gradients point in exactly the same direction. If the momentum algorithm always observes gradient g, then it will accelerate in the direction of \u2014g, until reaching a terminal velocity where the size of each step is  call. l-a  (8.17)  It is thus helpful to think of the momentum hyperparameter in terms of a For example, a = 0.9 corresponds to multiplying the maximum speed by 10 relative to the gradient descent algorithm. Common values of a used in practice include 0.5, 0.9, and 0.99. Like the learning rate, a may also be adapted over time. Typically it begins with a small value and is later raised. Adapting a over time is less important than shrinking \u00a2\u20ac over time. We can view the momentum algorithm as simulating a particle subject to continuous-time Newtonian dynamics. The physical analogy can help build intuition for how the momentum and gradient descent algorithms behave", "54e92eb3-a9c9-42ec-943b-63f193ea8c6f": "Just as with the mcRBM, the mPoT model energy function specifies a mul- tivariate Gaussian, with a conditional distribution over x that has nondiagonal covariance. Learning in the mPoT model\u2014again, like the mcRBM\u2014is com- plicated by the inability to sample from the nondiagonal Gaussian conditional PmPor(& | hi, \u00a9), so Ranzato et al. also advocate direct sampling of p(x) via Hamiltonian (hybrid) Monte Carlo. Spike and Slab Restricted Boltzmann Machines Spike and slab restricted Boltzmann machines  or ssRBMs provide another means of modeling the covariance structure of real-valued data. Compared to mcRBMs, ssRBMs have the advantage of requiring neither matrix inversion nor Hamiltonian Monte Carlo methods.\n\nLike the mcRBM and the mPoT model, the ssRBM\u2019s binary hidden units encode the conditional covariance across pixels through the use of auxiliary real-valued variables", "b02ee85f-eded-453c-a4e2-f59cdb008451": "It is instructive to note the exact representation Tesauro chose. There were a total of 198 input units to the network. For each point on the backgammon board, four units indicated the number of white pieces on the point. If there were no white pieces, then all four units took on the value zero. If there was one piece, then the \ufb01rst unit took on the value 1. This encoded the elementary concept of a \u201cblot,\u201d i.e., a piece that can be hit by the opponent. If there were two or more pieces, then the second unit was set to 1. This encoded the basic concept of a \u201cmade point\u201d on which the opponent cannot land. If there were exactly three pieces on the point, then the third unit was set to 1. This encoded the basic concept of a \u201csingle spare,\u201d i.e., an extra piece in addition to the two pieces that made the point", "4f7bc60e-8f7e-43f8-994b-16518da042e6": "W., M. I. Jordan, M. J. Beal, and D. M. Blei . Hierarchical Dirichlet processes. Journal of the Americal Statistical Association. to appear. Tesauro, G. TD-Gammon, a self-teaching backgammon program, achieves master-level play. Neural Computation 6(2), 215\u2013219. Thiesson, B., D. M. Chickering, D. Heckerman, and C. Meek . ARMA time-series modelling with graphical models. In M. Chickering and J. Halpern (Eds. ), Proceedings of the Twentieth Conference on Uncertainty in Arti\ufb01cial Intelligence, Banff, Canada, pp. 552\u2013560. AUAI Press. Tibshirani, R. Regression shrinkage and selection via the lasso", "83124d2d-7ec0-4c2a-a890-97159901374b": "Computationally intensive nonlinear optimization techniques must be used, and there is the risk of finding a suboptimal local minimum of the error function. Also, the dimensionality of the subspace must be specified before training the network. As we have already noted, many natural sources of data correspond to lowdimensional, possibly noisy, nonlinear manifolds embedded within the higher dimensional observed data space. Capturing this property explicitly can lead to improved density modelling compared with more general methods. Here we consider briefly a range of techniques that attempt to do this. One way to model the nonlinear structure is through a combination of linear models, so that we make a piece-wise linear approximation to the manifold.\n\nThis can be obtained, for instance, by using a clustering technique such as K -means based on Euclidean distance to partition the data set into local groups with standard PCA applied to each group. A better approach is to use the reconstruction error for cluster assignment  as then a common cost function is being optimized in each stage. However, these approaches still suffer from limitations due to the absence of an overall density model", "02142b1e-9815-4d1a-b0df-b70ef53b0f61": "Given a unit of computational e\u21b5ort, is it better devoted to a few expected updates or to b times as many sample updates? shows the estimation error as a function of computation time for expected and sample updates for a variety of branching factors, b. The case considered is that in which all b successor states are equally likely and in which the error in the initial estimate is 1.\n\nThe values at the next states are assumed correct, so the expected update reduces the error to zero upon its completion. In this case, sample updates reduce the error (assuming sample averages, i.e., \u21b5 = 1/t). The key observation is that for moderately large b the error falls dramatically with a tiny fraction of b updates. For these cases, many state\u2013action pairs could have their values improved dramatically, to within a few percent of the e\u21b5ect of an expected update, in the same time that a single state\u2013action pair could undergo an expected update. The advantage of sample updates shown in Figure 8.7 is probably an underestimate of the real e\u21b5ect. In a real problem, the values of the successor states would be estimates that are themselves updated", "539bf100-c036-4463-9e3a-24fa0867f722": "An algorithm for \ufb01nding best matches in logarithmic expected time. ACM Transactions on Mathematical Software, 3(3):209\u2013226. Friston, K. J., Tononi, G., Reeke, G. N., Sporns, O., Edelman, G. M. Value-dependent selection in the brain: Simulation in a synthetic neural model. Neuroscience, 59(2):229\u2013243. Galanter, E., Gerstenhaber, M. On thought: The extrinsic theory. Psychological Review, Gallistel, C. R. .\n\nDeconstructing the law of e\u21b5ect. Games and Economic Behavior, Gardner, M. Mathematical games. Scienti\ufb01c American, 228(1):108\u2013115. Geist, M., Scherrer, B. O\u21b5-policy learning with eligibility traces: A survey. Journal of Gelly, S., Silver, D. Combining online and o\u270fine knowledge in UCT", "d74f64fa-870f-43a3-bf12-0810c7f72c84": "Alopex: A stochastic method for determining visual receptive Hassabis, D., Maguire, E. A. Deconstructing episodic memory with construction. Trends Hauskrecht, M., Meuleau, N., Kaelbling, L. P., Dean, T., Boutilier, C. Hierarchical solution of Markov decision processes using macro-actions. In Proceedings of the Fourteenth Conference on Uncertainty in Arti\ufb01cial Intelligence, pp. 220\u2013229. Morgan Kaufmann. Hawkins, R. D., Kandel, E. R. Is there a cell-biological alphabet for simple forms of Haykin, S. .\n\nNeural networks: A Comprehensive Foundation, Macmillan, New York. He, K., Huertas, M., Hong, S. Z., Tie, X., Hell, J. W., Shouval, H., Kirkwood, A. Distinct eligibility traces for LTP and LTD in cortical synapses", "f9519ed2-95ef-43c4-b5ff-73f798982513": "By a similar argument to that used above, it is easy to see that the marginal associated with a factor is given by the Exercise 8.21 product of messages arriving at the factor node and the local factor at that node in complete analogy with the marginals at the variable nodes.\n\nIf the factors are parameterized functions and we wish to learn the values of the parameters using the EM algorithm, then these marginals are precisely the quantities we will need to calculate in the E step, as we shall see in detail when we discuss the hidden Markov model in Chapter 13. The message sent by a variable node to a factor node, as we have seen, is simply the product of the incoming messages on other links. We can if we wish view the sum-product algorithm in a slightly different form by eliminating messages from variable nodes to factor nodes and simply considering messages that are sent out by factor nodes. This is most easily seen by considering the example in Figure 8.50. So far, we have rather neglected the issue of normalization. If the factor graph was derived from a directed graph, then the joint distribution is already correctly normalized, and so the marginals obtained by the sum-product algorithm will similarly be normalized correctly", "0c094258-9297-46d3-99af-e5cced5acf79": "Is o\u21b5-policy learning possible without importance sampling? Q-learning and Expected Sarsa from Chapter 6 do this for the one-step case, but is there a corresponding multi-step algorithm? In this section we present just such an n-step method, called the tree-backup algorithm. The idea of the algorithm is suggested by the 3-step tree-backup backup diagram shown to the right. Down the central spine and labeled in the diagram are three sample states and rewards, and two sample actions. These are the random variables representing the events occurring after the initial state\u2013action pair St, At. Hanging o\u21b5 to the sides of each state are the actions that were not selected. (For the last state, all the actions are considered to have not (yet) been selected.) Because we have no sample data for the unselected actions, we bootstrap and use the estimates of their values in forming the target for the update. This slightly extends the idea of a backup diagram.\n\nSo far we have always updated the estimated value of the node at the top of the diagram toward a target combining the rewards along the way (appropriately discounted) and the estimated values of the nodes at the bottom", "5c64828e-60d4-48d9-9013-eff35efc3027": "Exercise 4.15 The Newton-Raphson update formula for the logistic regression model then becomes where z is an N-dimensional vector with elements We see that the update formula (4.99) takes the form of a set of normal equations for a weighted least-squares problem.\n\nBecause the weighing matrix R is not constant but depends on the parameter vector w, we must apply the normal equations iteratively, each time using the new weight vector w to compute a revised weighing matrix R. For this reason, the algorithm is known as iterative reweighted least squares, or IRLS . As in the weighted least-squares problem, the elements of the diagonal weighting matrix R can be interpreted as variances because the mean and variance of t in the logistic regression model are given by where we have used the property t2 = t for t \u2208 {0, 1}. In fact, we can interpret IRLS as the solution to a linearized problem in the space of the variable a = wT\u03c6", "54c7c2d9-ac94-4faf-a615-e6defa80cf80": "In many cases, we may derive some general machine learning algorithm in terms of arbitrary matrices but obtain a less expensive (and less descriptive) algorithm by restricting some matrices to be diagonal. Not all diagonal matrices need be square. It is possible to construct a rectangular diagonal matrix. Nonsquare diagonal matrices do not have inverses, but we can still multiply by them cheaply. For a nonsquare diagonal matrix D, the product Dz will involve scaling each element of x and either concatenating some zeros to the result, if D is taller than it is wide, or discarding some of the last elements of the vector, if D is wider than it is tall. A symmetric matrix is any matrix that is equal to its own transpose: A=Al.\n\n(2.35) Symmetric matrices often arise when the entries are generated by some function of two arguments that does not depend on the order of the arguments. For example, if A is a matrix of distance measurements, with A; giving the distance from point i to point j, then Aj; = Aj; because distance functions are symmetric. A unit vector is a vector with unit norm:  ||z||2 = 1", "131ffd89-9e64-43da-b5d1-c1e4a97d40da": "Graphical models can describe the probabilistic relationships between neighboring pixels. Alternatively, the convolutional network can be trained to  out ved ne reas inte ood  https://www.deeplearningbook.org/contents/convnets.html    maximize an approximation or the grapnical model training opjective . 353  CHAPTER 9. CONVOLUTIONAL NETWORKS  9.7 Data Types  The data used with a convolutional network usually consist of several channels, each channel being the observation of a different quantity at some point in space or time. See table 9.1 for examples of data types with different dimensionalities and number of channels. For an example of convolutional networks applied to video, see Chen ef al. So far we have discussed only the case where every example in the train and test data has the same spatial dimensions.\n\nOne advantage to convolutional networks is that they can also process inputs with varying spatial extents. These kinds of input simply cannot be represented by traditional, matrix multiplication-based neural networks. This provides a compelling reason to use convolutional networks even when computational cost and overfitting are not significant issues", "6ed7592b-4f27-43c9-9539-0506bd1c1f17": "In: AAAI Conference on Arti\ufb01cial Intelligence (AAAI)  56.\n\nSun, C., Shrivastava, A., Singh, S., Gupta, A.: Revisiting unreasonable effectiveness of data in deep learning era  arXiv preprint arXiv:1707.02968 57. Takamatsu, S., Sato, I., Nakagawa, H.: Reducing wrong labels in distant supervision for relation extraction. In: Meeting of the Association for Computational Linguistics (ACL)  58. Varma, P., He, B., Bajaj, P., Khandwala, N., Banerjee, I., Rubin, D., R\u00e9, C.: Inferring generative model structure with static analysis. In: Proceedings of NIPS  59. Varma, P., R\u00e9, C.: Snuba: Automating weak supervision to label training data. In: Proceedings of VLDB  60", "ee9095f7-8fef-4326-9ec0-91c002428c86": "Solutions for the remaining exercises are available to course tutors by contacting the publisher (contact details are given on the book web site). Readers are strongly encouraged to work through the exercises unaided, and to turn to the solutions only as required. Although this book focuses on concepts and principles, in a taught course the students should ideally have the opportunity to experiment with some of the key algorithms using appropriate data sets. A companion volume  will deal with practical aspects of pattern recognition and machine learning, and will be accompanied by Matlab software implementing most of the algorithms discussed in this book. First of all I would like to express my sincere thanks to Markus Svens\u00b4en who has provided immense help with preparation of \ufb01gures and with the typesetting of the book in LATEX. His assistance has been invaluable.\n\nI am very grateful to Microsoft Research for providing a highly stimulating research environment and for giving me the freedom to write this book (the views and opinions expressed in this book, however, are my own and are therefore not necessarily the same as those of Microsoft or its af\ufb01liates)", "c91447b4-46ea-4412-9d66-49ee88513b35": "For example, if two labeling functions express similar heuristics, we can include this dependency in the model and avoid a \u201cdouble counting\u201d problem. We observe that such pairwise correlations are the most common, so we focus on them in this paper (though handling higher order dependencies is straightforward).\n\nWe use our structure learning method for generative models  to select a set C of labeling function pairs ( j, k) to model as correlated (see Sect. 3.2). Now we can construct the full generative model as a factor graph. We \ufb01rst apply all the labeling functions to the unlabeled data points, resulting in a label matrix \ufffd, where \ufffdi, j = \u03bb j(xi). We then encode the generative model pw(\ufffd, Y) using three factor types, representing the labeling propensity, accuracy, and pairwise correlations of labeling functions: For a given data point xi, we de\ufb01ne the concatenated vector of these factors for all the labeling functions j = 1, ..., n and potential correlations C as \u03c6i(\ufffd, Y), and the corresponding vector of parameters w \u2208 R2n+|C|", "57291768-ef50-4242-a2ff-84a24531eeca": "This is more of an art than a science, and most guidance on this subject should be regarded with some skepticism. When using the linear schedule, the parameters to choose are \u00ab, \u20ac-, and 7. Usually 7 may be set to the number of iterations required to make a few hundred passes through the training set. Usually \u20ac, should be set to roughly 1 percent the value of \u20ac9. The main question is how to set eo. If it is too large, the learning curve will show violent oscillations, with the cost function often increasing significantly. Gentle oscillations are fine, especially if training with a stochastic cost function, such as the cost function arising from the use of dropout. If the learning rate is too low, learning proceeds slowly, and if the initial learning rate is too low, learning may become stuck with a high cost value. Typically, the optimal initial learning rate, in terms of total training time and the  291  CHAPTER 8.\n\nOPTIMIZATION FOR TRAINING DEEP MODELS  final cost value, is higher than the learning rate that yields the best performance after the first 100 iterations or so", "71c96665-dc42-4423-a6c4-2fb8da6f12c8": "We subsample a small training set on each task by randomly picking 40 instances for each class. We further create small validation sets, i.e., 2 instances per class for SST-5, and 5 instances per class for IMDB and TREC, respectively. The reason we use slightly more validation examples on IMDB and TREC is that the model can easily achieve 100% validation accuracy if the validation sets are too small. Thus, the SST-5 task has 210 labeled examples in total, while IMDB has 90 labels and TREC has 270.\n\nSuch extremely small datasets pose signi\ufb01cant challenges for learning deep neural networks. Since the manipulation parameters are trained using the small validation sets, to avoid possible over\ufb01tting we restrict the training to small number (e.g., 5 or 10) of epochs. For image classi\ufb01cation, we similarly create a small subset of the CIFAR10 data, which includes 40 instances per class for training, and 2 instances per class for validation. Results Table 1 shows the manipulation results on text classi\ufb01cation. For data augmentation, our approach signi\ufb01cantly improves over the base model on all the three datasets", "50561d5c-2cd1-4e6e-b4a9-1b4dd72df9c4": "If we choose an initial point from the distribution (11.63) and then update it using L leapfrog interactions, the probability of the transition going from R to R\u2032 is given by where the factor of 1/2 arises from the probability of choosing to integrate with a positive step size rather than a negative one. Similarly, the probability of starting in momentum variable ri. Because the change to one variable is a function only of the other, any region in phase space will be sheared without change of volume. region R\u2032 and integrating backwards in time to end up in region R is given by It is easily seen that the two probabilities (11.68) and (11.69) are equal, and hence detailed balance holds.\n\nNote that this proof ignores any overlap between the regions Exercise 11.17 R and R\u2032 but is easily generalized to allow for such overlap. It is not dif\ufb01cult to construct examples for which the leapfrog algorithm returns to its starting position after a \ufb01nite number of iterations. In such cases, the random replacement of the momentum values before each leapfrog integration will not be suf\ufb01cient to ensure ergodicity because the position variables will never be updated. Such phenomena are easily avoided by choosing the magnitude of the step size at random from some small interval, before each leapfrog integration", "6cec44f4-9f77-4e08-a509-62dc061003b4": "For example, replacing \u03c6i in Eq. (11) with log \u03c6i in effect changes the softmax normalization in Eq. (12) to linear normalization, which is used in . We empirically validate the proposed data manipulation approach through extensive experiments on learning augmentation and weighting. We study both text and image classi\ufb01cation, in two dif\ufb01cult settings of low data regime and imbalanced labels1. Base Models. We choose strong pretrained networks as our base models for both text and image classi\ufb01cation. Speci\ufb01cally, on text data, we use the BERT (base, uncased) model ; while on image data, we use ResNet-34  pretrained on ImageNet. We show that, even with the largescale pretraining, data manipulation can still be very helpful to boost the model performance on downstream tasks. Since our approach uses validation sets for manipulation parameter learning, for a fair comparison with the base model, we train the base model in two ways. The \ufb01rst is to train the model on the training sets as usual and select the best step using the validation sets; the second is to train on the merged training and validation sets for a \ufb01xed number of steps.\n\nThe step number is set to the average number of steps selected in the \ufb01rst method", "935491fa-1b80-4b88-b82e-2498b757bc28": "Traditional noise injection techniques that add unstructured noise at the input are not able to randomly erase the information about a nose from an image of a face unless the magnitude of the noise is so great that nearly all the information in  the image is removed. Destroying extracted features rather than original values allows the destruction process to make use of all the knowledge about the input distribution that the model has acquired so far. Another important aspect of dropout is that the noise is multiplicative. If the  264  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  noise were additive with fixed scale, then a rectified linear hidden unit h; with added noise \u20ac could simply learn to have h; become very large in order to make the added noise \u20ac insignificant by comparison. Multiplicative noise does not allow such a pathological solution to the noise robustness problem.\n\nAnother deep learning algorithm, batch normalization, reparametrizes the model in a way that introduces both additive and multiplicative noise on the hidden units at training time. The primary purpose of batch normalization is to improve optimization, but the noise can have a regularizing effect, and sometimes makes dropout unnecessary. Batch normalization is described further in section 8.7.1", "390a05b9-f192-43c3-abca-e246aa9d8a9d": "In S. A. Barnett and A. McLaren (Eds. ), Science Survey, Michie, D. Experiments on the mechanisation of game learning. 1. characterization of the model and its parameters. The Computer Journal, 6(3):232\u2013263. Michie, D. On Machine Intelligence. Edinburgh University Press, Edinburgh. Michie, D., Chambers, R. A. BOXES, An experiment in adaptive control. In E. Dale and D. Michie (Eds. ), Machine Intelligence 2, pp. 137\u2013152. Oliver and Boyd, Edinburgh. Miller, R. Meaning and Purpose in the Intact Brain: A Philosophical, Psychological, and Biological Account of Conscious Process. Clarendon Press, Oxford. Miller, W. T., An, E., Glanz, F., Carter, M. The design of CMAC neural networks for Miller, W", "d7185c70-f71d-4963-980a-25e46912ddbd": "An image\u2019s pixel values in each RGB color channel is aggregated to form a color histogram. This his- togram can be manipulated to apply filters that change the color space characteristics of an image. There is a lot of freedom for creativity with color space augmentations. Altering the color distribution of images can be a great solution to lighting challenges faced by testing data (Figs. 3, 4). Image datasets can be simplified in representation by converting the RGB matri-  ces into a single grayscale image. This results in smaller images, height x width x 1, Shorten and Khoshgoftaar J Big Data   6:60  \u2018A  ee |  Original photo  RGB all changed  Red color casting  ee |  Vignette  Green color casting  ye |  More vignette  Blue casting + vignette  Fig. 4 Examples of color augmentations tested by Wu et al. resulting in faster computation. However, this has been shown to reduce per- formance accuracy. Chatifled et al. found a ~ 3% classification accuracy drop between grayscale and RGB images with their experiments on ImageNet  and the PASCAL  VOC dataset", "39beb921-1855-4766-9571-b2304f9323b9": "Greedy layer-wise unsupervised pretraining relies on a single-layer represen- tation learning algorithm such as an RBM, a single-layer autoencoder, a sparse coding model, or another model that learns latent representations. Each layer is pretrained using unsupervised learning, taking the output of the previous layer and producing as output a new representation of the data, whose distribution (or its relation to other variables, such as categories to predict) is hopefully simpler. See algorithm 15.1 for a formal description. Greedy layer-wise training procedures based on unsupervised criteria have long been used to sidestep the difficulty of jointly training the layers of a deep neural net for a supervised task", "d8593338-a59e-4d7e-b09d-71b0de7d5323": "First of all, the data vectors {xn} typically lie close to a nonlinear manifold whose intrinsic dimensionality is smaller than that of the input space as a result of strong correlations between the input variables. We will see an example of this when we consider images of handwritten digits in Chapter 12. If we are using localized basis functions, we can arrange that they are scattered in input space only in regions containing data. This approach is used in radial basis function networks and also in support vector and relevance vector machines. Neural network models, which use adaptive basis functions having sigmoidal nonlinearities, can adapt the parameters so that the regions of input space over which the basis functions vary corresponds to the data manifold. The second property is that target variables may have signi\ufb01cant dependence on only a small number of possible directions within the data manifold. Neural networks can exploit this property by choosing the directions in input space to which the basis functions respond.\n\nHence show that a general linear combination of logistic sigmoid functions of the form is equivalent to a linear combination of \u2018tanh\u2019 functions of the form and \ufb01nd expressions to relate the new parameters {u1, . , uM} to the original parameters {w1,", "caac61a4-1b05-4bbd-b050-0934a902e086": "Because the charge decreases over time, each DRAM cell needs to be recharged\u2014refreshed\u2014every few milliseconds to prevent memory content from being lost.\n\nThis need to refresh the cells is why DRAM is called \u201cdynamic.\u201d Each cell array has a row bu\u21b5er that holds a row of bits that can be transferred into or out of one of the array\u2019s rows. An activate command \u201copens a row,\u201d which means moving the contents of the row whose address is indicated by the command into the row bu\u21b5er. With a row open, the controller can issue read and write commands to the cell array. Each read command transfers a word (a short sequence of consecutive bits) in the row bu\u21b5er to the external data bus, and each write command transfers a word in the external data bus to the row bu\u21b5er. Before a di\u21b5erent row can be opened, a precharge command must be issued which transfers the (possibly updated) data in the row bu\u21b5er back into the addressed row of the cell array. After this, another activate command can open a new row to be accessed", "07092149-ad54-4fdd-881b-9e878401ec67": "The corrupted input is fed to a large neural network that is trained to reproduce the original text y. An uncorrupted text will be reconstructed as itself (low reconstruction error), while a corrupted text will be reconstructed as an uncorrupted version of itself (large reconstruction error). If one interprets the reconstruction error as an energy, it will have the desired property: low energy for \u201cclean\u201d text and higher energy for \u201ccorrupted\u201d text. The general technique of training a model to restore a corrupted version of an input is called denoising auto-  https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/  Self-supervised learning: The dark matter of intelligence   encoder.\n\nWhile early forms of this idea go back to the 1980s, it was  revived in 2008 by Pascal Vincent and colleagues at the University of Montr\u00e9al, introduced in the context of NLP by Collobert and Weston, and popularized by the BERT paper from our friends at Google", "cfbefb8b-0473-46a1-bf53-42657cc167e6": "In the case of early stopping, we are controlling the effective capacity of the model by determining how many steps it can take to fit the training set. Most hyperparameters must be  243  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  chosen using an expensive guess and check process, where we set a hyperparameter at the start of training, then run training for several steps to see its effect.\n\nThe \u201ctraining time\u201d hyperparameter is unique in that by definition, a single run of training tries out many values of the hyperparameter. The only significant cost to choosing this hyperparameter automatically via early stopping is running the validation set evaluation periodically during training. Ideally, this is done in parallel to the training process on a separate machine, separate CPU, or separate GPU from the main training process. If such resources are not available, then the cost of these periodic evaluations may be reduced by using a validation set that is  Algorithm 7.1 The early stopping meta-algorithm for determining the best amount of time to train", "492e1c5b-f662-4f85-96bd-0562de751acd": "Houk, J. L. Davis, and D. G. Beiser (Eds.\n\n), Models of Information Processing in the Basal Ganglia, pp. 215\u2013232. MIT Press, Cambridge, MA. Barto, A. G. Reinforcement learning. In M. A. Arbib (Ed. ), Handbook of Brain Theory and Neural Networks, pp. 804\u2013809. MIT Press, Cambridge, MA. Barto, A. G. Adaptive real-time dynamic programming. In C. Sammut and G. I Webb (Eds. ), Encyclopedia of Machine Learning, pp. 19\u201322. Springer Science and Business Media. Barto, A. G. Intrinsic motivation and reinforcement learning. In G. Baldassarre and M. Mirolli (Eds", "4c8038ef-8fcb-49de-9a37-99da9fec73a9": "The way we obtained the manipulation algorithm represents a general means of innovating problem solutions based on unifying formalisms of different learning paradigms.\n\nSpeci\ufb01cally, a unifying formalism not only offers new understandings of the seemingly distinct paradigms, but also allows us to systematically apply solutions to problems in one paradigm to similar problems in another. Previous work along this line has made fruitful results in other domains. For example, an extended formulation of  that connects RL and posterior regularization (PR)  has enabled to similarly export a reward learning algorithm to the context of PR for learning structured knowledge . By establishing a uniform abstration of GANs  and VAEs , Hu et al. exchange techniques between the two families and get improved generative modeling. Other work in the similar spirit includes . By extrapolating algorithms between paradigms, one can go beyond crafting new algorithms from scratch as in most existing studies, which often requires deep expertise and yields unique solutions in a dedicated context. Instead, innovation becomes easier by importing rich ideas from other paradigms, and is repeatable as a new algorithm can be methodically extrapolated to multiple different contexts. A. Abdolmaleki, J. T", "fb866f56-e88a-4a08-ad82-5288bbe2000f": "The score of a candidate span from position i to position j is de\ufb01ned as S\u00b7Ti + E\u00b7Tj, and the maximum scoring span where j \u2265 i is used as a prediction. The training objective is the sum of the log-likelihoods of the correct start and end positions. We \ufb01ne-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32. Table 2 shows top leaderboard entries as well as results from top published systems . The top results from the SQuAD leaderboard do not have up-to-date public system descriptions available,11 and are allowed to use any public data when training their systems. We therefore use modest data augmentation in our system by \ufb01rst \ufb01ne-tuning on TriviaQA  befor \ufb01ne-tuning on SQuAD. Our best performing system outperforms the top leaderboard system by +1.5 F1 in ensembling and +1.3 F1 as a single system. In fact, our single BERT model outperforms the top ensemble system in terms of F1 score. Without TriviaQA \ufb01ne11QANet is described in Yu et al", "6a2fd546-7601-46c4-a495-7047da1ae899": "In the alternative approach, f(w) is recomputed each time it is needed. When the memory required to store the value of these expressions is low, the back-propagation approach of equation 6.51 is clearly preferable because of its reduced runtime. However, equation 6.52 is also a valid implementation of the chain rule and is useful when memory is limited. 205  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  That algorithm specifies the forward propagation computation, which we could put in a graph G. To perform back-propagation, we can construct a computational graph that depends on G and adds to it an extra set of nodes. These form a subgraph B with one node per node of G. Computation in B proceeds in exactly the reverse of the order of computation in G, and each node of B computes the  dul\u2122  derivative KO} associated with the forward graph node u. This is done using  the chain rule with respect to scalar output ul;  du - du\u2122 Au  OuG) \u2014 du Aud i:je\u20acPa(u)  (6.53)  as specified by algorithm 6.2", "1f3bad3f-c957-47e9-955a-6ab4ffdab4d5": "e Shared factors across tasks: When we have many tasks corresponding to different y; variables sharing the same input x, or when each task is associated with a subset or a function f@ (x) of a global input x, the assumption is that each y; is associated with a different subset from a common pool of  553  CHAPTER 15. REPRESENTATION LEARNING  relevant factors h. Because these subsets overlap, learning all the P(y; | x) via a shared intermediate representation P(h | x) allows sharing of statistical strength between the tasks.\n\nManifolds: Probability mass concentrates, and the regions in which it con- centrates are locally connected and occupy a tiny volume. In the continuous case, these regions can be approximated by low-dimensional manifolds with a much smaller dimensionality than the original space where the data live. Many machine learning algorithms behave sensibly only on this manifold . Some machine learning algorithms, especially autoencoders, attempt to explicitly learn the structure of the manifold. Natural clustering: Many machine learning algorithms assume that each connected manifold in the input space may be assigned to a single class", "a2a9a082-1a6d-409b-aad1-d25c3a6e43e4": "Equation 3.41 provides extra justification for the name \u201csoftplus.\u201d The softplus function is intended as a smoothed version of the positive part function, \u00ab> = 0. The positive part function is the counterpart of the negative part  max at  https://www.deeplearningbook.org/contents/prob.html    function, \u00ab~ = max{0,\u2014zx}. To obtain a smooth function that is analogous to the negative part, one can use 6(\u20142). Just as 2 can be recovered from its positive part and its negative part via the identity x\" \u2014x~ = 2, it is also possible to recover x using the same relationship between \u00a2(x) and \u00a2(\u20142), as shown in equation 3.41. 3.11 Bayes\u2019 Rule  We often find ourselves in a situation where we know P(y | x) and need to know  P(x | y)", "409e55e1-c492-4f9a-94e8-40b1b117fe41": "In this respect, Rt is like primary reward for an animal if we think of the problem the animal faces as the problem of obtaining as much primary reward as possible over its lifetime (and thereby, through the prospective \u201cwisdom\u201d of evolution, improve its chances of solving its real problem, which is to pass its genes on to future generations). However, as we suggest in Chapter 15, it is unlikely that there is a single \u201cmaster\u201d reward signal like Rt in an animal\u2019s brain. Not all reinforcers are rewards or penalties. Sometimes reinforcement is not the result of an animal receiving a stimulus that evaluates its behavior by labeling the behavior good or bad. A behavior pattern can be reinforced by a stimulus that arrives to an animal no matter how the animal behaved.\n\nAs described in Section 14.1, whether the delivery of reinforcer depends, or does not depend, on preceding behavior is the de\ufb01ning di\u21b5erence between instrumental, or operant, conditioning experiments and classical, or Pavlovian, conditioning experiments. Reinforcement is at work in both types of experiments, but only in the former is it feedback that evaluates past behavior", "e68d13a2-5b07-4088-a798-3817db79eef9": "Exercise 2.40 We have already seen how the maximum likelihood expression for the mean of a Gaussian can be re-cast as a sequential update formula in which the mean after Section 2.3.5 observing N data points was expressed in terms of the mean after observing N \u2212 1 data points together with the contribution from data point xN. In fact, the Bayesian paradigm leads very naturally to a sequential view of the inference problem. To see this in the context of the inference of the mean of a Gaussian, we write the posterior distribution with the contribution from the \ufb01nal data point xN separated out so that The term in square brackets is (up to a normalization coef\ufb01cient) just the posterior distribution after observing N \u2212 1 data points.\n\nWe see that this can be viewed as a prior distribution, which is combined using Bayes\u2019 theorem with the likelihood function associated with data point xN to arrive at the posterior distribution after observing N data points. This sequential view of Bayesian inference is very general and applies to any problem in which the observed data are assumed to be independent and identically distributed. So far, we have assumed that the variance of the Gaussian distribution over the data is known and our goal is to infer the mean", "83df33b9-a04d-429d-bbc4-5ec1d1d7d8eb": "He conjectured that these traces are implemented by molecular mechanisms local to each synapse and therefore di\u21b5erent from the electrical activity of both the pre- and the postsynaptic neurons. In the Bibliographical and Historical Remarks section of this chapter we bring attention to some similar proposals made by others. Klopf speci\ufb01cally conjectured that synaptic e\ufb03cacies change in the following way. When a neuron \ufb01res an action potential, all of its synapses that were active in contributing to that action potential become eligible to undergo changes in their e\ufb03cacies. If the action potential is followed within an appropriate time period by an increase of reward, the e\ufb03cacies of all the eligible synapses increase. Symmetrically, if the action potential is followed within an appropriate time period by an increase of punishment, the e\ufb03cacies of eligible synapses decrease", "2a1b1ff3-7da7-482b-9a9c-ac61ed2494be": "Today, we know that the machine learning models described in part II work very well when trained with gradient descent. The optimization algorithm may not be guaranteed to arrive at even a local minimum in a reasonable amount of time, but it often finds a very low value of the cost function quickly enough to be useful. Stochastic gradient descent has many important uses outside the context of deep learning. It is the main way to train large linear models on very large datasets. For a fixed model size, the cost per SGD update does not depend on the training set size m. In practice, we often use a larger model as the training set size increases, but we are not forced to do so. The number of updates required to reach convergence usually increases with training set size. However, as m approaches infinity, the model will eventually converge to its best possible test error before SGD has sampled every example in the training set. Increasing m further will not extend the amount of training time needed to reach the model\u2019s best possible test error.\n\nFrom this point of view, one can argue that the asymptotic cost of training a model with SGD is O(1) as a function of m", "9a94a1fb-1a19-49d1-a9c2-b97349e838e0": "Following the discussions in Section 1.2.5 and 3.1, we assume that t has a Gaussian distribution with an xdependent mean, which is given by the output of the neural network, so that where \u03b2 is the precision (inverse variance) of the Gaussian noise. Of course this is a somewhat restrictive assumption, and in Section 5.6 we shall see how to extend this approach to allow for more general conditional distributions.\n\nFor the conditional distribution given by (5.12), it is suf\ufb01cient to take the output unit activation function to be the identity, because such a network can approximate any continuous function from x to y. Given a data set of N independent, identically distributed observations X = {x1, . , xN}, along with corresponding target values t = {t1, . , tN}, we can construct the corresponding likelihood function Taking the negative logarithm, we obtain the error function which can be used to learn the parameters w and \u03b2. In Section 5.7, we shall discuss the Bayesian treatment of neural networks, while here we consider a maximum likelihood approach", "0442dbd4-c2e2-4dfd-ade7-215574433d87": "d. Data-ef\ufb01cient image recognition with contrastive predictive coding. arXiv preprint arXiv:1905.09272, 2019. Hinton, G. E., Osindero, S., and Teh, Y.-W. A fast learning algorithm for deep belief nets. Neural computation, 18(7):1527\u2013 1554, 2006. Hjelm, R. D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Bachman, P., Trischler, A., and Bengio, Y. Learning deep representations by mutual information estimation and maximization. arXiv preprint arXiv:1808.06670, 2018. Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. Ji, X., Henriques, J. F., and Vedaldi, A", "54033b8f-d86a-4442-97ca-b60daa598b7f": "The estimate b is usually obtained by adding extra outputs to the neural network and training the new outputs to estimate Exy lJ ) Dlogptw)\u201d ] and E,vy) [sgn for each element of w. These extra  outputs can be trained with the mean squared error objective, using respectively  Jy) 2g w)\u201d and tee )? as targets when y is sampled from p(y), for a given w. The estimate b may then be recovered by substituting these estimates into equation 20.68. Mnih and Gregor  preferred to use a single shared output (across all elements 7 of w) trained with the target J(y), using as baseline b(w) ~ Envy) [J(y)I-  Variance reduction methods have been introduced in the reinforcement learning context , generalizing previous work on the case of binary reward by Dayan . See Bengio et al. , Mnih and Gregor , Ba et al. , Mnih et al. , or Xu et al", "ccf3780d-cd0b-47d1-ba64-42bb9e108164": "When viewing an object, information flows from the retina, through the LGN, to V1, then onward to V2, then V4, then IT. This happens within the first 100ms of glimpsing an object. If a person is allowed to continue looking at the object for more time, then information will begin to flow backward as the brain uses top-down feedback to update the activations in the lower level brain areas. If we interrupt the person\u2019s gaze, however, and observe only the firing rates that result from the first 100ms of mostly feedforward activation, then IT proves to be similar to a convolutional network. Convolutional networks can predict IT firing rates and perform similarly to (time-limited) humans on object recognition tasks . That being said, there are many differences between convolutional networks and the mammalian vision system.\n\nSome of these differences are well known to computational neuroscientists but outside the scope of this book. Some of these differences are not yet known, because many basic questions about how the mammalian vision system works remain unanswered. As a brief list:  e The human eye is mostly very low resolution, except for a tiny patch called the fovea", "b7a1050d-f4be-43ba-aef6-868a8092ab78": "From now on, we shall switch to the use of natural logarithms in de\ufb01ning entropy, as this will provide a more convenient link with ideas elsewhere in this book. In this case, the entropy is measured in units of \u2018nats\u2019 instead of bits, which differ simply by a factor of ln 2. We have introduced the concept of entropy in terms of the average amount of information needed to specify the state of a random variable. In fact, the concept of entropy has much earlier origins in physics where it was introduced in the context of equilibrium thermodynamics and later given a deeper interpretation as a measure of disorder through developments in statistical mechanics.\n\nWe can understand this alternative view of entropy by considering a set of N identical objects that are to be divided amongst a set of bins, such that there are ni objects in the ith bin. Consider the number of different ways of allocating the objects to the bins", "4e2fbdf6-0d7c-4561-9850-4f407a1e1397": "A positive \u03b4 means that the action was \u2018good\u2019 because it led to a state with a better-than-expected value; a negative \u03b4 means that the action was \u2018bad\u2019 because it led to a state with a worse-than-expected value. Based on these critiques, the actor continually updates its policy. Two distinctive features of actor\u2013critic algorithms are responsible for thinking that the brain might implement an algorithm like this. First, the two components of an actor\u2013critic algorithm\u2014the actor and the critic\u2014suggest that two parts of the striatum\u2014the dorsal and ventral subdivisions (Section 15.4), both critical for reward-based learning\u2014may function respectively something like an actor and a critic.\n\nA second property of actor\u2013 critic algorithms that suggests a brain implementation is that the TD error has the dual role of being the reinforcement signal for both the actor and the critic, though it has a di\u21b5erent in\ufb02uence on learning in each of these components", "d74b8277-df97-4ca7-af92-bf9560dabd2f": "Data for this problem is generated by sampling a variable x uniformly over the interval (0, 1), to give a set of values {xn}, and the corresponding target values tn are obtained by computing the function xn + 0.3 sin(2\u03c0xn) and then adding uniform noise over the interval (\u22120.1, 0.1). The inverse problem is then obtained by keeping the same data points but exchanging the roles of x and t. Figure 5.19 shows the data sets for the forward and inverse problems, along with the results of \ufb01tting two-layer neural networks having 6 hidden units and a single linear output unit by minimizing a sumof-squares error function. Least squares corresponds to maximum likelihood under a Gaussian assumption. We see that this leads to a very poor model for the highly non-Gaussian inverse problem. We therefore seek a general framework for modelling conditional probability distributions. This can be achieved by using a mixture model for p(t|x) in which both the mixing coef\ufb01cients as well as the component densities are \ufb02exible functions of the input vector x, giving rise to the mixture density network", "9bcb1e92-cd8f-4c57-84e0-5e85ac858add": "While intellectually appealing, this model is challenging to make work in practice, and usually does not perform as well as a classifier as traditional convolutional networks trained with supervised learning.\n\nMany convolutional models work equally well with inputs of many different  \u2019The publication describes the model as a \u201cdeep belief network,\u201d but because it can be described as a purely undirected model with tractable layer-wise mean field fixed-point updates, it best fits the definition of a deep Boltzmann machine. 680  CHAPTER 20. DEEP GENERATIVE MODELS  spatial sizes. For Boltzmann machines, it is difficult to change the input size for various reasons. The partition function changes as the size of the input changes. Moreover, many convolutional networks achieve size invariance by scaling up the size of their pooling regions proportional to the size of the input, but scaling Boltzmann machine pooling regions is awkward. Traditional convolutional neural networks can use a fixed number of pooling units and dynamically increase the size of their pooling regions to obtain a fixed-size representation of a variably sized input. For Boltzmann machines, large pooling regions become too expensive for the naive approach", "ecf7d35e-2150-4b36-a7bf-5bd9ffb3da2a": "In practice, we have to \ufb01nd the joint angles that will give rise to a desired end effector position and, as shown in the right \ufb01gure, this inverse kinematics has two solutions corresponding to \u2018elbow up\u2019 and \u2018elbow down\u2019. We see that \u03c0j is therefore driven towards the average posterior probability for component j. The goal of supervised learning is to model a conditional distribution p(t|x), which for many simple regression problems is chosen to be Gaussian. However, practical machine learning problems can often have signi\ufb01cantly non-Gaussian distributions.\n\nThese can arise, for example, with inverse problems in which the distribution can be multimodal, in which case the Gaussian assumption can lead to very poor predictions. As a simple example of an inverse problem, consider the kinematics of a robot arm, as illustrated in Figure 5.18. The forward problem involves \ufb01nding the end efExercise 5.33 fector position given the joint angles and has a unique solution. However, in practice we wish to move the end effector of the robot to a speci\ufb01c position, and to do this we must set appropriate joint angles", "99f37c57-b3d2-487f-93bf-1a0d018d5582": "Because of this, the \ufb01rst episode was exactly the same (about 1700 steps) for all values of n, and its data are not shown in the \ufb01gure. After the \ufb01rst episode, performance improved for all values of n, but much more rapidly for larger values. Recall that the n = 0 agent is a nonplanning agent, using only direct reinforcement learning (one-step tabular Q-learning). This was by far the slowest agent on this problem, despite the fact that the parameter values (\u21b5 and \") were optimized for it. The nonplanning agent took about 25 episodes to reach (\"-)optimal performance, whereas the n = 5 agent took about \ufb01ve episodes, and the n = 50 agent took only three episodes. the nonplanning agent.\n\nShown are the policies found by the n = 0 and n = 50 agents halfway through the second episode. Without planning (n = 0), each episode adds only one additional step to the policy, and so only one step (the last) has been learned so far. With planning, again only one step is learned during the \ufb01rst episode, but here during the second episode an extensive policy has been developed that by the end of the episode will reach almost back to the start state", "9c7de3e9-3dc4-42ae-8ab4-241016f20286": "Practical Methods of Optimization (Second ed.). Wiley. Forsyth, D. A. and J. Ponce . Computer Vision: A Modern Approach. Prentice Hall. Freund, Y. and R. E. Schapire . Experiments with a new boosting algorithm. In L. Saitta (Ed. ), Thirteenth International Conference on Machine Learning, pp. 148\u2013156. Morgan Kaufmann. Frey, B. J. and D. J. C. MacKay . A revolution: Belief propagation in graphs with cycles.\n\nIn M. I. Jordan, M. J. Kearns, and S. A. Solla (Eds. ), Advances in Neural Information Processing Systems, Volume 10. MIT Press. Friedman, J. H", "f48218cc-57d0-4db9-a660-621f29ce9654": "Informally, this follows from the fact that in a space of dimension D, the additional computational cost of evaluating a gradient compared with evaluating the function itself will typically be a \ufb01xed factor independent of D, whereas the D-dimensional gradient vector conveys D pieces of information compared with the one piece of information given by the function itself. As we discussed in the previous section, for a nonzero step size \u03f5, the discretization of the leapfrog algorithm will introduce errors into the integration of the Hamiltonian dynamical equations. Hybrid Monte Carlo  combines Hamiltonian dynamics with the Metropolis algorithm and thereby removes any bias associated with the discretization. Speci\ufb01cally, the algorithm uses a Markov chain consisting of alternate stochastic updates of the momentum variable r and Hamiltonian dynamical updates using the leapfrog algorithm.\n\nAfter each application of the leapfrog algorithm, the resulting candidate state is accepted or rejected according to the Metropolis criterion based on the value of the Hamiltonian H", "70bdcdfb-a108-4e90-9e4a-7710654246ec": "Finally, we use (10.208) to evaluate the approximation to the model evidence, given by Examples factor approximations for the clutter problem with a one-dimensional parameter space \u03b8 are shown in Figure 10.16. Note that the factor approximations can have in\ufb01nite or even negative values for the \u2018variance\u2019 parameter vn. This simply corresponds to approximations that curve upwards instead of downwards and are not necessarily problematic provided the overall approximate posterior q(\u03b8) has positive variance. Figure 10.17 compares the performance of EP with variational Bayes (mean \ufb01eld theory) and the Laplace approximation on the clutter problem. So far in our general discussion of EP, we have allowed the factors fi(\u03b8) in the distribution p(\u03b8) to be functions of all of the components of \u03b8, and similarly for the approximating factors \ufffdf(\u03b8) in the approximating distribution q(\u03b8). We now consider situations in which the factors depend only on subsets of the variables.\n\nSuch restrictions can be conveniently expressed using the framework of probabilistic graphical models, as discussed in Chapter 8. Here we use a factor graph representation because this encompasses both directed and undirected graphs. the clutter problem", "b4677848-6889-48df-af22-d07eed114755": "Given a set of samples {x;})_,, le  corresponding covariance matrix:  X; and \u00a5 be the transformed samples and  1x 1x w= 2 x; & NW Do Ht) | (x; \u2014 #) & =(xi-pW S=W'SW=TI thusd  (wo')'w  If we get SVD decomposition of \u00a7 = UAUT, we willhave W~-! = /AU'T and W = UV A-!. https://lilianweng.github.io/posts/2021-05-31-contrastive/     Contrastive Representation Learning | Lil'Log  Note that within SVD, U is an or\u2019 diagonal matrix with all positive  A dimensionality reduction strat  Whitening -k.  thogonal matrix with column vectors as eigenvectors and A is a elements as sorted eigenvalues", "e0334e97-3adc-4bb2-b12c-7dd7e1ed8518": "(13.23)  i=d+1  Hence, if the covariance has rank d, the eigenvalues A441 to Ap are 0 and recon- struction error is 0. 496  https://www.deeplearningbook.org/contents/linear_factors.html    CHAPTER 13, LINEAR FACTOR MODELS. Figure 13.3: Flat Gaussian capturing probability concentration near a low-dimensional manifold. The figure shows the upper half of the \u201cpancake\u201d above the \u201cmanifold plane,\u201d which goes through its middle. The variance in the direction orthogonal to the manifold is very small (arrow pointing out of plane) and can be considered \u201cnoise,\u201d while the other variances are large (arrows in the plane) and correspond to \u201csignal\u201d and to a coordinate  system for the reduced-dimension data. 497    https://www.deeplearningbook.org/contents/linear_factors.html   CHAPTER 13. LINEAR FACTOR MODELS  Furthermore, one can also show that the above solution can be obtained by maximizing the variances of the elements of h, under orthogonal W, instead of minimizing reconstruction error", "8afcf7cf-3dfa-4a91-b2ef-eea898db876b": "An Empirical Survey of Data Augmentation for Limited Data Learning in NLP NLP has achieved great progress in the past decade through the use of neural models and large labeled datasets. The dependence on abundant data prevents NLP models from being applied to low-resource settings or novel tasks where signi\ufb01cant time, money, or expertise is required to label massive amounts of textual data. Recently, data augmentation methods have been explored as a means of improving data ef\ufb01ciency in NLP. To date, there has been no systematic empirical overview of data augmentation for NLP in the limited labeled data setting, making it dif\ufb01cult to understand which methods work in which settings. In this paper, we provide an empirical survey of recent progress on data augmentation for NLP in the limited labeled data setting, summarizing the landscape of methods (including token-level augmentations, sentencelevel augmentations, adversarial augmentations and hidden-space augmentations) and carrying out experiments on 11 datasets covering topics/news classi\ufb01cation, inference tasks, paraphrasing tasks, and single-sentence tasks. Based on the results, we draw several conclusions to help practitioners choose appropriate augmentations in different settings and discuss the current challenges and future directions for limited data learning in NLP", "614877ad-b372-4e4b-a865-aa430b296f24": "It achieved a reward-per-step of only about 1, compared with the best possible of about 1.55 on this testbed. The greedy method performed signi\ufb01cantly worse in the long run because it often got stuck performing suboptimal actions. The lower graph shows that the greedy method found the optimal action in only approximately one-third of the tasks. In the other two-thirds, its initial samples of the optimal action were disappointing, and it never returned to it. The \"-greedy methods eventually performed better because they continued to explore and to improve their chances of recognizing the optimal action. The \" = 0.1 method explored more, and usually found the optimal action earlier, but it never selected that action more than 91% of the time.\n\nThe \" = 0.01 method improved more slowly, but eventually would perform better than the \" = 0.1 method on both performance measures shown in the \ufb01gure. It is also possible to reduce \" over time to try to get the best of both high and low values. The advantage of \"-greedy over greedy methods depends on the task. For example, suppose the reward variance had been larger, say 10 instead of 1", "2fcb83ec-9226-4070-9151-a4462de8cd8c": "One limitation of decision trees is that the division of input space is based on hard splits in which only one model is responsible for making predictions for any given value of the input variables. The decision process can be softened by moving to a probabilistic framework for combining models, as discussed in Section 14.5.\n\nFor example, if we have a set of K models for a conditional distribution p(t|x, k) where x is the input variable, t is the target variable, and k = 1, . , K indexes the model, then we can form a probabilistic mixture of the form in which \u03c0k(x) = p(k|x) represent the input-dependent mixing coef\ufb01cients. Such models can be viewed as mixture distributions in which the component densities, as well as the mixing coef\ufb01cients, are conditioned on the input variables and are known as mixtures of experts. They are closely related to the mixture density network model discussed in Section 5.6. It is important to distinguish between model combination methods and Bayesian model averaging, as the two are often confused. To understand the difference, consider the example of density estimation using a mixture of Gaussians in which several Section 9.2 Gaussian components are combined probabilistically", "abf6b191-272d-43bf-8ba9-3da8081f9a43": "Ratio matching consists of minimizing the average over  https://www.deeplearningbook.org/contents/partition.html    CHAPTER 18. CONFRONTING THE PARTITION FUNCTION  examples of the following objective function:  2 / \u201c 1 LM), @) > 1 Pmodel(\u00aei9) _\u2014 J\u201d (18.26) j=1 Pmodel (f(#,3);4)  where f(a, 7) returns x with the bit at position j flipped. Ratio matching avoids he partition function using the same trick as the pseudolikelihood estimator: in a ratio of two probabilities, the partition function cancels out. Marlin et al. found that ratio matching outperforms SML, pseudolikelihood and GSM in terms of the ability of models trained with ratio matching to denoise test set images. Like the pseudolikelihood estimator, ratio matching requires n evaluations of p per data point, making its computational cost per update roughly n times higher chan that of SML. As with the pseudolikelihood estimator, ratio matching can be thought of as pushing down on all fantasy states that have only one variable different from a raining example", "d300b90f-cee5-4d99-9d66-be2c658492fb": "Purely linear models, like logistic regression, are not able to resist adversarial examples because they are forced to be linear. Neural networks are able to represent functions that can range  https://www.deeplearningbook.org/contents/regularization.html    rom nearly linear to nearly locally constant and thus have the flexibility to capture inear trends in the training data while still learning to resist local perturbation. Adversarial examples also provide a means of accomplishing semi-supervised learning. At a point x that is not associated with a label in the dataset, the model itself assigns some label gy. The model\u2019s label Y may not be the true label, but if the model is high quality, then y has a high probability of providing the rue label. We can seek an adversarial example a that causes the classifier to output a label y/ with y\u2019 4 g. Adversarial examples generated using not the true  label but a label provided by a trained model are called virtual adversarial examples", "90ed6020-c3c1-42ad-b075-63e79aa53e7d": "MACHINE LEARNING BASICS  5.10 Building a Machine Learning Algorithm  Nearly all deep learning algorithms can be described as particular instances of a fairly simple recipe: combine a specification of a dataset, a cost function, an optimization procedure and a model.\n\nFor example, the linear regression algorithm combines a dataset consisting of X and y, the cost function  J(w, b) = \u2014Exy~Baata log Pmodel (Y | x), (5.100)  the model specification pode (y |e) = NV (y; zlwt b, 1), and, in most cases, the optimization algorithm defined by solving for where the gradient of the cost is zero using the normal equations. By realizing that we can replace any of these components mostly independently from the others, we can obtain a wide range of algorithms. The cost function typically includes at least one term that causes the learning process to perform statistical estimation. The most common cost function is the negative log-likelihood, so that minimizing the cost function causes maximum ikelihood estimation. The cost function may also include additional terms, such as regularization erms", "9033c5ff-c462-43e7-a9f4-2f5622911deb": "User embeddings and item embeddings can then be conveniently visualized when they are first reduced to a low dimension (two or three), or they can be used to compare users or items against each other, just like word embeddings. One way to obtain these embeddings is by performing a singular value decomposition of the matrix R of actual targets (such as ratings).\n\nThis corresponds to factorizing R = UDV\u2019 (or a normalized variant) into the product of two factors, the lower rank matrices A = UD and B = V\u2019. One problem with the SVD is that it treats the missing entries in an arbitrary way, as if they corresponded to a target value of 0. Instead we would like to avoid  474  https://www.deeplearningbook.org/contents/applications.html    CHAPTER +42\u2014APPHEICAFIONS  paying any cost for the predictions made on missing entries. Fortunately, the sum of squared errors on the observed ratings can also be easily minimized by gradient- based optimization", "0fee7664-5882-4cf2-bc8c-0e2dcc74c6fb": "A feature map is a pattern of activity over an array of units, where each unit performs the same operation on data in its receptive \ufb01eld, which is the part of the data it \u201csees\u201d from the preceding layer (or from the external input in the case of the \ufb01rst convolutional layer). The units of a feature map are identical to one another except that their receptive \ufb01elds, which are all the same size and shape, are shifted to di\u21b5erent locations on the arrays of incoming data. Units in the same feature map share the same weights. This means that a feature map detects the same feature no matter where it is located in the input array.\n\nIn the network in Figure 9.15, for example, the \ufb01rst convolutional layer produces 6 feature maps, each consisting of 28 \u21e5 28 units. Each unit in each feature map has a 5 \u21e5 5 receptive \ufb01eld, and these receptive \ufb01elds overlap (in this case by four columns and four rows). Consequently, each of the 6 feature maps is speci\ufb01ed by just 25 adjustable weights. The subsampling layers of a deep convolutional network reduce the spatial resolution of the feature maps", "39863e79-80bf-4a80-8378-c1859066b4eb": "Consider the regression setting, where we wish to train  https://www.deeplearningbook.org/contents/regularization.html    a function 9 (2) that maps a set of features \u00a9 to a scalar using the least-squares cost function between the model predictions j(a) and the true values y:  J =Ep(xy)  . This form of regularization encourages the parameters to go to regions of parameter space where small perturbations of the weights have a relatively small influence on the output. In other words, it pushes the model into regions where the model is relatively insensitive to small variations in the weights, finding points that are not merely minima, but minima surrounded by flat regions .\n\nIn the simplified case of linear regression (where, for instance, \u00a7(x) = wa +), this regularization term collapses  into NEp(z) , which is not a function of parameters and therefore does not  contribute to the gradient of Jw with respect to the model parameters. 7.5.1 Injecting Noise at the Output Targets  Most datasets have some number of mistakes in the y labels. It can be harmful to maximize log p(y | x) when y is a mistake", "dc76bf0a-08e3-41e1-b4b2-434b742b060d": "For better or worse, the second edition is about twice as large as the \ufb01rst. This book is designed to be used as the primary text for a one- or two-semester course on reinforcement learning.\n\nFor a one-semester course, the \ufb01rst ten chapters should be covered in order and form a good core, to which can be added material from the other chapters, from other books such as Bertsekas and Tsitsiklis , Wiering and van Otterlo , and Szepesv\u00b4ari , or from the literature, according to taste. Depending of the students\u2019 background, some additional material on online supervised learning may be helpful. The ideas of options and option models are a natural addition . A two-semester course can cover all the chapters as well as supplementary material. The book can also be used as part of broader courses on machine learning, arti\ufb01cial intelligence, or neural networks. In this case, it may be desirable to cover only a subset of the material. We recommend covering Chapter 1 for a brief overview, Chapter 2 through Section 2.4, Chapter 3, and then selecting sections from the remaining chapters according to time and interests", "ff5273b6-ac12-48a0-b110-b7888edfbe4a": "Hiett, Andreas Badelt, Jay Ponte, Joe Beck, Justus Piater, Martha Steenstrup, Satinder Singh, Tommi Jaakkola, Dimitri Bertsekas, Torbj\u00a8orn Ekman, Christina Bj\u00a8orkman, Jakob Carlstr\u00a8om, and Olle Palmgren. Finally, we thank Gwyn Mitchell for helping in many ways, and Harry Stanton and Bob Prior for being our champions at MIT Press. Capital letters are used for random variables, whereas lower case letters are used for the values of random variables and for scalar functions. Quantities that are required to be real-valued vectors are written in bold and in lower case (even if random variables). Matrices are bold capitals. The idea that we learn by interacting with our environment is probably the \ufb01rst to occur to us when we think about the nature of learning", "e8ec6c79-a288-42ac-b368-264cda2f0fc2": "Although the experimental rats did not appear to learn much during the \ufb01rst, unrewarded, stage, as soon as they discovered the food that was introduced in the second stage, they rapidly caught up with the rats in the control group. It was concluded that \u201cduring the non-reward period, the rats  were developing a latent learning of the maze which they were able to utilize as soon as reward was introduced\u201d . Latent learning is most closely associated with the psychologist Edward Tolman, who interpreted this result, and others like it, as showing that animals could learn a \u201ccognitive map of the environment\u201d in the absence of rewards or penalties, and that they could use the map later when they were motivated to reach a goal . A cognitive map could also allow a rat to plan a route to the goal that was di\u21b5erent from the route the rat had used in its initial exploration. Explanations of results like these led to the enduring controversy lying at the heart of the behaviorist/cognitive dichotomy in psychology.\n\nIn modern terms, cognitive maps are not restricted to models of spatial layouts but are more generally environment models, or models of an animal\u2019s \u201ctask space\u201d", "424e688c-a469-466c-80d0-ae8a76e4a329": "The approach we explore, called reinforcement learning, is much more focused on goal-directed learning from interaction than are other approaches to machine learning. Reinforcement learning is learning what to do\u2014how to map situations to actions\u2014so as to maximize a numerical reward signal.\n\nThe learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them. In the most interesting and challenging cases, actions may a\u21b5ect not only the immediate 1The relationships to psychology and neuroscience are summarized in Chapters 14 and 15. reward but also the next situation and, through that, all subsequent rewards. These two characteristics\u2014trial-and-error search and delayed reward\u2014are the two most important distinguishing features of reinforcement learning. Reinforcement learning, like many topics whose names end with \u201cing,\u201d such as machine learning and mountaineering, is simultaneously a problem, a class of solution methods that work well on the problem, and the \ufb01eld that studies this problem and its solution methods. It is convenient to use a single name for all three things, but at the same time essential to keep the three conceptually separate. In particular, the distinction between problems and solution methods is very important in reinforcement learning; failing to make this distinction is the source of many confusions", "350dc4c7-7177-4468-a23d-71128b78fb5f": "These approximate values are close to the global minimum of the VE (9.1). Some of the details of the approximate values are best appreciated by reference to the state distribution \u00b5 for this task, shown in the lower portion of the \ufb01gure with a right-side scale. State 500, in the center, is the \ufb01rst state of every episode, but is rarely visited again. On average, about 1.37% of the time steps are spent in the start state. The states reachable in one step from the start state are the second most visited, with about 0.17% of the time steps being spent in each of them. From there \u00b5 falls o\u21b5 almost linearly, reaching about 0.0147% at the extreme states 1 and 1000.\n\nThe most visible e\u21b5ect of the distribution is on the leftmost groups, whose values are clearly shifted higher than the unweighted average of the true values of states within the group, and on the rightmost groups, whose values are clearly shifted lower. This is due to the states in these areas having the greatest asymmetry in their weightings by \u00b5. For example, in the leftmost group, state 100 is weighted more than 3 times more strongly than state 1", "2caa4ead-b9be-4f9e-ac83-e4803b36c479": "However, we then have to compensate for the effects of our modi\ufb01cations to the training data. Suppose we have used such a modi\ufb01ed data set and found models for the posterior probabilities. From Bayes\u2019 theorem (1.82), we see that the posterior probabilities are proportional to the prior probabilities, which we can interpret as the fractions of points in each class. We can therefore simply take the posterior probabilities obtained from our arti\ufb01cially balanced data set and \ufb01rst divide by the class fractions in that data set and then multiply by the class fractions in the population to which we wish to apply the model.\n\nFinally, we need to normalize to ensure that the new posterior probabilities sum to one. Note that this procedure cannot be applied if we have learned a discriminant function directly instead of determining posterior probabilities. Combining models. For complex applications, we may wish to break the problem into a number of smaller subproblems each of which can be tackled by a separate module. For example, in our hypothetical medical diagnosis problem, we may have information available from, say, blood tests as well as X-ray images", "8b5ee54c-7f4c-461c-a16f-02cf3fea0bda": "Curran Associates, Inc., 2016. Alfred M\u00a8uller. Integral probability metrics and their generating classes of functions. Advances in Applied Probability, 29(2):429\u2013443, 1997. Radford M. Neal. Annealed importance sampling. Statistics and Computing, 11(2):125\u2013139, April 2001. Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. pages 271\u2013279, 2016. Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. CoRR, abs/1511.06434, 2015. Aaditya Ramdas, Sashank J. Reddi, Barnabas Poczos, Aarti Singh, and Larry Wasserman.\n\nOn the high-dimensional power of linear-time kernel two-sample testing under mean-di\ufb00erence alternatives. Corr, abs/1411.6314, 2014", "6ccc0029-cf1b-44bd-9456-6efe5af11223": "It is unclear how to generalize this to the setting of energy-based models. We could introduce a binary pooling unit p over n binary detector units d and enforce p = max; d; by setting the energy function to be co whenever that constraint is violated. This does not scale well though, as it requires evaluating 2\u201d different energy configurations to compute the normalization constant. For a small 3 x 3 pooling region this requires 29 = 512 energy function evaluations per pooling unit! Lee et al. developed a solution to this problem called probabilistic max pooling (not to be confused with \u201cstochastic pooling,\u201d which is a technique for implicitly constructing ensembles of convolutional feedforward networks). The strategy behind probabilistic max pooling is to constrain the detector units so at most one may be active at a time. This means there are only n+ 1 total states (one state for each of the n detector units being on, and an additional state corresponding to all the detector units being off). The pooling unit is on if and only if one of the detector units is on", "26e55edd-3ae4-4797-b8fd-acfb21cc5d46": "\u21e4 2.1 Bandit problems have been studied in statistics, engineering, and psychology. In statistics, bandit problems fall under the heading \u201csequential design of experiments,\u201d introduced by Thompson  and Robbins , and studied by Bellman . Berry and Fristedt  provide an extensive treatment of bandit problems from the perspective of statistics.\n\nNarendra and Thathachar  treat bandit problems from the engineering perspective, providing a good discussion of the various theoretical traditions that have focused on them. In psychology, bandit problems have played roles in statistical learning theory . The term greedy is often used in the heuristic search literature . The con\ufb02ict between exploration and exploitation is known in control engineering as the con\ufb02ict between identi\ufb01cation (or estimation) and control . Feldbaum  called it the dual control problem, referring to the need to solve the two problems of identi\ufb01cation and control simultaneously when trying to control a system under uncertainty. In discussing aspects of genetic algorithms, Holland  emphasized the importance of this con\ufb02ict, referring to it as the con\ufb02ict between the need to exploit and the need for new information. 2.2 Action-value methods for our k-armed bandit problem were \ufb01rst proposed by Thathachar and Sastry . These are often called estimator algorithms in the learning automata literature", "e8d5581b-f8cf-444c-bf8d-33e2c2ad842d": "With these conventions, the three \ufb01rst sequences described in the previous paragraph can be given explicitly: The online \u03bb-return algorithm is fully online, determining a new weight vector wt at each step t during an episode, using only information available at time t. Its main drawback is that it is computationally complex, passing over the portion of the episode experienced so far on every step. Note that it is strictly more complex than the o\u270fine \u03bb-return algorithm, which passes through all the steps at the time of termination but does not make any updates during the episode.\n\nIn return, the online algorithm can be expected to perform better than the o\u270fine one, not only during the episode when it makes an update while the o\u270fine algorithm makes none, but also at the end of the episode because updates. This e\u21b5ect can be seen if one looks carefully at Figure 12.8, which compares the two algorithms on the 19-state random walk task. The online \u03bb-return algorithm just presented is currently the best performing temporaldi\u21b5erence algorithm. It is an ideal which online TD(\u03bb) only approximates. As presented, however, the online \u03bb-return algorithm is very complex", "5fb26e74-5596-4a95-a202-bf21e2b4b184": "Mean field inference is also intractable because the variational lower bound involves taking expectations of cliques that encompass entire layers. This problem has remained difficult enough to restrict the popularity of directed discrete networks. https://www.deeplearningbook.org/contents/generative_models.html    One approach tor pertorming interence in a sigmoid beliet network is to con- struct a different lower bound that is specialized for sigmoid belief networks . This approach has only been applied to very small networks. Another approach is to use learned inference mechanisms as described in section 19.5. The Helmholtz machine  is a sigmoid belief network combined with an inference network that predicts the parame- ters of the mean field distribution over the hidden units. Modern approaches  to sigmoid belief networks still use this inference network approach.\n\nThese techniques remain difficult because of the discrete nature of the latent variables. One cannot simply back-propagate through the output of the inference network, but instead must use the relatively unreliable machinery for back-propagating through discrete sampling processes, as described in section 20.9.1", "85698c9a-1ad0-4abe-a5a9-fcf84ade1119": "With the dynamic experience, a learning procedure, using the special case of SE in Equation 3.2, can be written as: Here the SE governs the optimization of the target model p\u03b8 given the updated experience at each \u03c4. We discuss in more detail the SE with dynamic experience (Section 6.1) and other dynamic components (Section 6.2), which further recovers several well-known algorithms in existing learning paradigms. 6.1. Dynamic Experience. The experience can change over time due to di\ufb00erent reasons. For example, the experience can involve optimization, often together with the target model, resulting in a bi-level optimization scheme (Section 6.1.1); or alternatively, the experience may just come from the environment sequentially with an unknown dynamic (Section 6.1.2). 6.1.1. Variational Experience With Optimization", "48d1ddf1-5639-4795-95de-6ffd9cd71cc0": "Finally, show that the binomial distribution is normalized, so that which can be done by \ufb01rst pulling out a factor (1 \u2212 \u00b5)N out of the summation and then making use of the binomial theorem. 2.4 (\u22c6 \u22c6) Show that the mean of the binomial distribution is given by (2.11). To do this, differentiate both sides of the normalization condition (2.264) with respect to \u00b5 and then rearrange to obtain an expression for the mean of n. Similarly, by differentiating (2.264) twice with respect to \u00b5 and making use of the result (2.11) for the mean of the binomial distribution prove the result (2.12) for the variance of the binomial. From the de\ufb01nition (1.141) of the gamma function, we have Use this expression to prove (2.265) as follows", "5e87870f-31ca-4807-a860-91902235d899": "This provides a geometric argument to explain the generalization power of distributed representation: with O(nd) parameters (for n linear threshold features in R4, we can distinctly represent O(n*) regions in input space.\n\nIf instead we made no assumption at all about the data, and used a representation with one unique symbol for each region, and separate parameters for each symbol to recognize its  Potentially, we may want to learn a function whose behavior is distinct in exponentially many regions: in a d-dimensional space with at least 2 different values to distinguish per dimension, we might want f to differ in 27 different regions, requiring O(2%) training examples. 548  https://www.deeplearningbook.org/contents/representation.html    CHAPTER 15. REPRESENTATION LEARNING  corresponding portion of R\u00a2, then specifying O(n*) regions would require O(n\u00ae) examples. More generally, the argument in favor of the distributed representation could be extended to the case where instead of using linear threshold units we use nonlinear, possibly continuous, feature extractors for each of the attributes in the distributed representation", "51afa798-110c-44f4-891d-07a74dcfb49b": "The second theorem says that any symmetric real matrix S is positive de\ufb01nite if all of its diagonal entries are positive and greater than the sum of the absolute values of the corresponding o\u21b5-diagonal entries (Varga 1962, p. 23). For our key matrix, D(I \u2212 \u03b3P), the diagonal entries are positive and the o\u21b5-diagonal entries are negative, so all we have to show is that each row sum plus the corresponding column sum is positive. The row sums are all positive because P is a stochastic matrix and \u03b3 < 1. Thus it only remains to show that the column sums are nonnegative.\n\nNote that the row vector of the column sums of any matrix M can be written as 1>M, where 1 is the column vector with all components equal to 1. Let \u00b5 denote the |S|-vector of the \u00b5(s), where \u00b5 = P>\u00b5 by virtue of \u00b5 being the stationary distribution. The column sums of our key matrix, then, are: all components of which are positive. Thus, the key matrix and its A matrix are positive de\ufb01nite, and on-policy TD(0) is stable", "6e4605e6-6f5d-42f0-aeb9-034e4acb5a1b": "CONVOLUTIONAL NETWORKS  reduction in the input size can also result in improved statistical efficiency and reduced memory requirements for storing the parameters. For many tasks, pooling is essential for handling inputs of varying size. For example, if we want to classify images of variable size, the input to the classification layer must have a fixed size. This is usually accomplished by varying the size of an offset between pooling regions so that the classification layer always receives the same number of summary statistics regardless of the input size. For example, the final pooling layer of the network may be defined to output four sets of summary statistics, one for each quadrant of an image, regardless of the image size. Some theoretical work gives guidance as to which kinds of pooling one should use in various situations .\n\nIt is also possible to dynamically pool features together, for example, by running a clustering algorithm on the locations of interesting features . This approach yields a different set of pooling regions for each image. Another approach is to learn a single pooling structure that is then applied to all images . Pooling can complicate some kinds of neural network architectures that use top-down information, such as Boltzmann machines and autoencoders", "22bc72c3-a2b3-4683-98d6-fc11fc9b15f5": "), Models of Information Processing in the Basal Ganglia, pp. 187\u2013214. MIT Press, Cambridge, MA. Widrow, B., Gupta, N. K., Maitra, S. Punish/reward: Learning with a critic in adaptive threshold systems. IEEE Transactions on Systems, Man, and Cybernetics, 3(5):455\u2013465. Widrow, B., Ho\u21b5, M. E. .\n\nAdaptive switching circuits. In 1960 WESCON Convention Record Part IV, pp. 96\u2013104. Institute of Radio Engineers, New York. Reprinted in J. A. Anderson and E. Rosenfeld, Neurocomputing: Foundations of Research, pp. 126\u2013134. MIT Press, Cambridge, MA, 1988. Widrow, B., Stearns, S. D. Adaptive Signal Processing. Prentice-Hall, Englewood Cli\u21b5s, Wiener, N", "cebe76b0-3008-45f4-8e79-b7b343788128": "However, we are doing so with the opposite di- rection of the KL divergence than we are used to using for fitting an approximation.\n\nWhen we use maximum likelihood learning to fit a model to data, we minimize Dx (Paata||Pmodel)- As illustrated in figure 3.6, this means that maximum likelihood encourages the model to have high probability everywhere that the data has high probability, while our optimization-based inference procedure encourages q to have low probability everywhere the true posterior has low probability. Both directions of the KL divergence can have desirable and undesirable properties. The choice of which to use depends on which properties are the highest priority for each applica- tion. In the inference optimization problem, we choose to use Dxr(q( ) (  v)) for computational reasons. Specifically, computing Der (q( daly \\|p) ADF  vIn hla  https://www.deeplearningbook.org/contents/inference.html    volves evaluating expectations with respect to 7, so by designin g'to be simple, we can simplify the required expectations. The opposite direction of the KL divergence would require computing expectations with respect to the true posterior", "c2d0ea88-d67c-4ca7-8931-64e027e3ecd3": "In the context of deep learning, it is rare to compute a single second derivative of a scalar function. Instead, we are usually interested in properties of the Hessian matrix. If we have a function f : RR\u201d \u2014 R, then the Hessian matrix is of size n x n. In typical deep learning applications, n will be the number of parameters in the model, which could easily number in the billions. The entire Hessian matrix is thus infeasible to even represent. Instead of explicitly computing the Hessian, the typical deep learning approach is to use Krylov methods. Krylov methods are a set of iterative techniques for performing various operations, such as approximately inverting a matrix or finding approximations to its eigenvectors or eigenvalues, without using any operation other than matrix-vector products. To use Krylov methods on the Hessian, we only need to be able to com- pute the product between the Hessian matrix H and an arbitrary vector v", "bffff813-28d1-4290-9b1c-27539075e188": "Also show that in the M step, the initial probability and transition probability parameters are re-estimated using modi\ufb01ed forms of (13.18 ) and (13.19) given by where, for notational convenience, we have assumed that the sequences are of the same length (the generalization to sequences of different lengths is straightforward).\n\nSimilarly, show that the M-step equation for re-estimation of the means of Gaussian emission models is given by Note that the M-step equations for other emission model parameters and distributions take an analogous form. 13.13 (\u22c6 \u22c6) www Use the de\ufb01nition (8.64) of the messages passed from a factor node to a variable node in a factor graph, together with the expression (13.6) for the joint distribution in a hidden Markov model, to show that the de\ufb01nition (13.50) of the alpha message is the same as the de\ufb01nition (13.34). 13.14 (\u22c6 \u22c6) Use the de\ufb01nition (8.67) of the messages passed from a factor node to a variable node in a factor graph, together with the expression (13.6) for the joint distribution in a hidden Markov model, to show that the de\ufb01nition (13.52) of the beta message is the same as the de\ufb01nition (13.35)", "02cec2a8-ca65-4ca5-9e9c-84d38350fceb": "We can actually interpret the singular value decomposition of A in terms of the eigendecomposition of functions of A. The left-singular vectors of A are the eigenvectors of AA\u2019. The right-singular vectors of A are the eigenvectors of A! A. The nonzero singular values of A are the square roots of the eigenvalues of A! A. The same is true for AA\u2019.\n\nPerhaps the most useful feature of the SVD is that we can use it to partially generalize matrix inversion to nonsquare matrices, as we will see in the next section. 2.9 'The Moore-Penrose Pseudoinverse  Matrix inversion is not defined for matrices that are not square. Suppose we want to make a left-inverse B of a matrix A so that we can solve a linear equation  https://www.deeplearningbook.org/contents/linear_algebra.html    Ar=y (2.44) by left-multiplying each side to obtain x= By. (2.45) 43  CHAPTER 2", "66cc38b5-9aa9-46b5-b2d2-4009d26532ae": "Deep learning is a crucial component of modern speech recognition systems used at major companies, including Microsoft, IBM and Google . e Machine translation: In a machine translation task, the input already consists of a sequence of symbols in some language, and the computer program must convert this into a sequence of symbols in another language. This is commonly applied to natural languages, such as translating from English to French. Deep learning has recently begun to have an important impact on this kind of task . https://www.deeplearningbook.org/contents/ml.html    e Structured output: Structured output tasks involve any task where the output is a vector (or other data structure containing multiple values) with important relationships between the different elements. This is a broad  category and subsumes the transcription and translation tasks described above, as well as many other tasks.\n\nOne example is parsing\u2014mapping a natural language sentence into a tree that describes its grammatical structure by tagging nodes of the trees as being verbs, nouns, adverbs, and so on. See Collobert  for an example of deep learning applied to a parsing task", "64244b4a-4415-4a44-b155-991ba8cad037": "They derive specific values of the scaling factor for different types of nonlinear activation functions. This initialization scheme is also motivated by a model of a deep network as a sequence of matrix multiplies without nonlinearities. 299  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  Under such a model, this initialization scheme guarantees that the total number of training iterations required to reach convergence is independent of depth.\n\nIncreasing the scaling factor g pushes the network toward the regime where activations increase in norm as they propagate forward through the network and gradients increase in norm as they propagate backward. Sussillo  showed that setting the gain factor correctly is sufficient to train networks as deep as 1,000 layers, without needing to use orthogonal initializations. A key insight of this approach is that in feedforward networks, activations and gradients can grow or shrink on each step of forward or back-propagation, following a random walk behavior. This is because feedforward networks use a different weight matrix at each layer", "603cf3d0-45df-4aec-936f-c7406d5ee735": "Many regularization approaches are based on limiting the capacity of models, such as neural networks, linear regression, or logistic regression, by adding a pa- rameter norm penalty 2(@) to the objective function J. We denote the regularized objective function by J:  J(0;X,y) = J(O;X,y) + a6), (7.1)  where a \u20ac [0, 00) is a hyperparameter that weights the relative contribution of the norm penalty term, Q, relative to the standard objective function J. Setting a to 0 results in no regularization. Larger values of a correspond to more regularization.\n\nWhen our training algorithm minimizes the regularized objective function J it will decrease both the original objective J on the training data and some measure of the size of the parameters @ (or some subset of the parameters). Different choices for the parameter norm ( can result in different solutions being preferred. In this section, we discuss the effects of the various norms when used as penalties on the model parameters", "ce7562d9-8acf-4418-bb2c-d040140c6c32": "used DQN to show how a reinforcement learning agent can achieve a high level of performance on any of a collection of di\u21b5erent problems without having to use di\u21b5erent problem-speci\ufb01c feature sets. To demonstrate this, they let DQN learn to play 49 di\u21b5erent Atari 2600 video games by interacting with a game emulator.\n\nDQN learned a di\u21b5erent policy for each of the 49 games (because the weights of its ANN were reset to random values before learning on each game), but it used the same raw input, network architecture, and parameter values (e.g., step size, discount rate, exploration parameters, and many more speci\ufb01c to the implementation) for all the games. DQN achieved levels of play at or beyond human level on a large fraction of these games. Although the games were alike in being played by watching streams of video images, they varied widely in other respects. Their actions had di\u21b5erent e\u21b5ects, they had di\u21b5erent state-transition dynamics, and they needed di\u21b5erent policies for learning high scores. The deep convolutional ANN learned to transform the raw input common to all the games into features specialized for representing the action values required for playing at the high level DQN achieved for most of the games", "c52fb7da-f03a-4d7a-a801-ff16165cc08c": "Another elegant feature ofthe EM approach is that we can take the limit a 2 ----t 0, corresponding to standard PCA, and still obtain a valid EM-like algorithm . From (12.55), we see that the only quantity we need to compute in the Estep is JE. Furthermore, the M step is simplifie~ because M = WTW. To emphasize the simplicity of the algorithm, let us define X to be a matrix of size N x D whose nth row is given by the vector X n - x and similarly define 0 to be a matrix of size D x M whose nth row is given by the vector JE. The Estep (12.54) of the EM algorithm for PCA then becomes Again these can be implemented in an on-line form. These equations have a simple interpretation as follows. From our earlier discussion, we see that the E step involves an orthogonal projection of the data points onto the current estimate for the principal subspace. Correspondingly, the M step represents a re-estimation of the principal Fig\".", "ebf83eff-611c-482f-a6ef-b19986b27f94": "Barto, Sutton, and Watkins  argued that ANNs can play signi\ufb01cant roles for approximating functions required for solving sequential decision problems. Williams  related REINFORCE learning rules (Section 13.3) to the error backpropagation method for training multi-layer ANNs. Tesauro\u2019s TD-Gammon (Tesauro 1992, 1994; Section 16.1) in\ufb02uentially demonstrated the learning abilities of TD(\u03bb) algorithm with function approximation by multi-layer ANNs in learning to play backgammon.\n\nThe AlphaGo, AlphaGo Zero, and AlphaZero programs of Silver et al. (2016, 2017a, b; Section 16.6) used reinforcement learning with deep convolutional ANNs in achieving impressive results with the game of Go. Schmidhuber  reviews applications of ANNs in reinforcement learning, including applications of recurrent ANNs. 9.8 LSTD is due to Bradtke and Barto , and was further developed by Boyan , Nedi\u00b4c and Bertsekas , and Yu . The incremental update of the inverse matrix has been known at least since 1949", "c6784215-6b28-4d63-8599-041844da15cb": "How can we estimate the performance gradient with respect to the policy parameter when the gradient depends on the unknown e\u21b5ect of policy changes on the state distribution?\n\nFortunately, there is an excellent theoretical answer to this challenge in the form of the policy gradient theorem, which provides an analytic expression for the gradient of With just elementary calculus and re-arranging of terms, we can prove the policy gradient theorem from \ufb01rst principles. To keep the notation simple, we leave it implicit in all cases that \u21e1 is a function of \u2713, and all gradients are also implicitly with respect to \u2713. First note that the gradient of the state-value function can be written in terms of the action-value function as after repeated unrolling, where Pr(s ! x, k, \u21e1) is the probability of transitioning from state s to state x in k steps under policy \u21e1. It is then immediate that performance with respect to the policy parameter (which is what we need to approximate for gradient ascent (13.1)) that does not involve the derivative of the state distribution", "e6de87a2-0b00-43c4-8f6a-d1aafda8a92c": "The idea of implementing trial-and-error learning in a computer appeared among the earliest thoughts about the possibility of arti\ufb01cial intelligence. In a 1948 report, Alan Turing described a design for a \u201cpleasure-pain system\u201d that worked along the lines of the Law of E\u21b5ect: When a con\ufb01guration is reached for which the action is undetermined, a random choice for the missing data is made and the appropriate entry is made in the description, tentatively, and is applied. When a pain stimulus occurs all tentative entries are cancelled, and when a pleasure stimulus occurs they are all made permanent. Many ingenious electro-mechanical machines were constructed that demonstrated trialand-error learning. The earliest may have been a machine built by Thomas Ross  that was able to \ufb01nd its way through a simple maze and remember the path through the settings of switches. In 1951 W. Grey Walter built a version of his \u201cmechanical tortoise\u201d  capable of a simple form of learning. In 1952 Claude Shannon demonstrated a maze-running mouse named Theseus that used trial and error to \ufb01nd its way through a maze, with the maze itself remembering the successful directions via magnets and relays under its \ufb02oor . J.\n\nA", "de425c4d-1d6c-4fea-bd46-adf0c931abf4": "Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M., et al. : TensorFlow: A system for large-scale machine learning. In: USENIX Symposium on Operating Systems Design and Implementation (OSDI)  2. Agrawala, A.K. : Learning with a probabilistic teacher. IEEE Trans. Infom. Theory 16, 373\u2013379  3. Alfonseca, E., Filippova, K., Delort, J.-Y., Garrido, G.: Pattern learning for relation extraction with a hierarchical topic model", "ce4c7e70-e911-4e1d-9ab5-52a96810eafe": "This feedback loop means that it is not straightforward to evaluate the learner\u2019s performance using a fixed set of test set input values. The policy itself determines which inputs will be seen. Dudik e\u00a2 al. present techniques for evaluating contextual bandits. 12.5.2 Knowledge Representation, Reasoning and Question Answering  https://www.deeplearningbook.org/contents/applications.html    Deep learning approaches have been very successful in language modeling, machine translation and natural language processing because of the use of embeddings for symbols (Rumelhart e\u00a2 al., SGA) and words .\n\nThese embeddings represent semantic knowledge about individual words  and concepts. A research frontier is to develop embeddings for phrases and for  ATT  CHAPTER 12. APPLICATIONS  relations between words and facts. Search engines already use machine learning for this purpose, but much more remains to be done to improve these more advanced representations. 12.5.2.1 Knowledge, Relations and Question Answering  One interesting research direction is determining how distributed representations can be trained to capture the relations between two entities. These relations allow us to formalize facts about objects and how objects interact with each other. In mathematics, a binary relation is a set of ordered pairs of objects", "e9be91c9-17ff-4979-9aab-4eab43f11e78": "Note that the use of convolution for processing variably sized inputs makes sense only for inputs that have variable size because they contain varying amounts of observation of the same kind of thing\u2014different lengths of recordings over time, different widths of observations over space, and so forth. Convolution does not make sense if the input has variable size because it can optionally include different kinds of observations. For example, if we are processing college applications, and our features consist of both grades and standardized test scores, but not every applicant took the standardized test, then it does not make sense to convolve the same weights over features corresponding to the grades as well as the features  354  CHAPTER 9. CONVOLUTIONAL NETWORKS  Single channel  Multichannel  1-D | Audio waveform: The axis we | Skeleton animation data: Anima- convolve over corresponds to | tions of 3-D computer-rendered time. We discretize time and | characters are generated by alter- measure the amplitude of the | ing the pose of a \u201cskeleton\u201d over waveform once per time step. time", "d62be48c-b065-40bc-9e3e-0a12ee67710d": "What is an \"independent\" political party in the United States, the U.S. political party, and the United computers: This essay discusses how you can build a new browser to view and share your favorite web sites.\\n\\n\\n A browser that is open source can also be built from a web browser, which can be a browser that does not allow browser extensions (e.g. Firefox, Chrome, Opera space: This essay discusses how you can build a life with a healthy diet and how you can use it when you\u2019re ready to move forward.\n\nIt\u2019s a very simple approach to building a life with a healthy diet and what it means to be healthy and healthy for the religion: This essay discusses how you can build a new game without having to play the original game, and how you can make a new title that is completely different to the original. It has been around since 2007, when the \ufb01rst game, The Elder Scrolls IV: Oblivion, was released in the PlayStation science: This essay discusses how we can build on previous research \ufb01ndings about the role of obesity in human metabolism and how we can improve our health.\\n\\n\\n\\n In this essay, we explore why eating a whole whole diet does not help prevent obesity (1)", "478b580a-8514-45c7-8280-9c722ceffb34": "In this model  g=w' a+b, (5.13)  so the mapping from parameters to predictions is still a linear function but the mapping from features to predictions is now an affine function. This extension to  5 Linear regression example 0.55 Optimization of w \u00a9 0) 2 0.50 1 2 0.45 & 0.40 > 0 = a 0.35 -1 = 0.30 -2 0.25 ~3 0.20 \u20141.0 -05 00 05 1.0 0.5 1.0 1.5 xy WI  Figure 5.1: A linear regression problem, with a training set consisting of ten data points, each containing one feature. Because there is only one feature, the weight vector w contains only a single parameter to learn, w1.\n\n(Left)Observe that linear regression learns to set wy such that the line y = w;x comes as close as possible to passing through all the training points. (Right)The plotted point indicates the value of w1 found by the normal equations, which we can see minimizes the mean squared error on the training set. 107  https://www.deeplearningbook.org/contents/ml.html    CHAPTER 5", "0a754b83-fffd-4d70-9699-d1d3f774da44": "These concepts are illustrated by the undirected graph over four variables shown in Figure 8.29. This graph has \ufb01ve cliques of two nodes given by {x1, x2}, {x2, x3}, {x3, x4}, {x4, x2}, and {x1, x3}, as well as two maximal cliques given by {x1, x2, x3} and {x2, x3, x4}. The set {x1, x2, x3, x4} is not a clique because of the missing link from x1 to x4. We can therefore de\ufb01ne the factors in the decomposition of the joint distribution to be functions of the variables in the cliques. In fact, we can consider functions of the maximal cliques, without loss of generality, because other cliques must be subsets of maximal cliques. Thus, if {x1, x2, x3} is a maximal clique and we de\ufb01ne an arbitrary function over this clique, then including another factor de\ufb01ned over a subset of these variables would be redundant", "bdd4b5f1-af7c-4243-99c0-5d681cc1c127": "We therefore consider the general problem of minimizing KL(p\u2225q) when Section 10.7 q(Z) is a factorized approximation of the form (10.5). The KL divergence can then be written in the form where the constant term is simply the entropy of p(Z) and so does not depend on q(Z). We can now optimize with respect to each of the factors qj(Zj), which is easily done using a Lagrange multiplier to give Exercise 10.3 In this case, we \ufb01nd that the optimal solution for qj(Zj) is just given by the corresponding marginal distribution of p(Z). Note that this is a closed-form solution and so does not require iteration. To apply this result to the illustrative example of a Gaussian distribution p(z) over a vector z we can use (2.98), which gives the result shown in Figure 10.2(b).\n\nWe see that once again the mean of the approximation is correct, but that it places signi\ufb01cant probability mass in regions of variable space that have very low probability", "fefc8e61-c5fe-4dc3-aae0-d3ed77bb5607": "Using the standard result E = aN/bN for the mean of a gamma distribution, together with Appendix B Then, using (10.26) and (10.27), we obtain the \ufb01rst and second order moments of We can now substitute these moments into (10.31) and then solve for E to give Exercise 10.9 We recognize the right-hand side as the familiar unbiased estimator for the variance of a univariate Gaussian distribution, and so we see that the use of a Bayesian approach has avoided the bias of the maximum likelihood solution. Section 1.2.4 As well as performing inference over the hidden variables Z, we may also wish to compare a set of candidate models, labelled by the index m, and having prior probabilities p(m). Our goal is then to approximate the posterior probabilities p(m|X), where X is the observed data", "7eb74b8b-4b08-47a7-8428-18ffda549da2": "Recall that we made an analogous interpretation for the Dirichlet prior. These distributions Section 2.2 are examples of the exponential family, and we shall see that the interpretation of a conjugate prior in terms of effective \ufb01ctitious data points is a general one for the exponential family of distributions. Instead of working with the precision, we can consider the variance itself.\n\nThe conjugate prior in this case is called the inverse gamma distribution, although we shall not discuss this further because we will \ufb01nd it more convenient to work with the precision. Now suppose that both the mean and the precision are unknown. To \ufb01nd a conjugate prior, we consider the dependence of the likelihood function on \u00b5 and \u03bb We now wish to identify a prior distribution p(\u00b5, \u03bb) that has the same functional dependence on \u00b5 and \u03bb as the likelihood function and that should therefore take the form where c, d, and \u03b2 are constants. Since we can always write p(\u00b5, \u03bb) = p(\u00b5|\u03bb)p(\u03bb), we can \ufb01nd p(\u00b5|\u03bb) and p(\u03bb) by inspection", "e55e6066-3e17-4095-9b2f-b12a28a81b78": "If we take the derivative with respect to the parameter vector w of the contribution to the error function from a data point n, this takes the form of the \u2018error\u2019 yn \u2212 tn times the feature vector \u03c6n, where yn = wT\u03c6n. Similarly, for the combination of the logistic sigmoid activation function and the cross-entropy error function (4.90), and for the softmax activation function with the multiclass cross-entropy error function (4.108), we again obtain this same simple form.\n\nWe now show that this is a general result of assuming a conditional distribution for the target variable from the exponential family, along with a corresponding choice for the activation function known as the canonical link function. We again make use of the restricted form (4.84) of exponential family distributions. Note that here we are applying the assumption of exponential family distribution to the target variable t, in contrast to Section 4.2.4 where we applied it to the input vector x", "b865e654-4fb8-4e9e-b70a-ad9d5721906f": "Then the environment responds with a transition to the next state\u2019s node via one of the arrows leaving action node (s, a). Each arrow corresponds to a triple (s, s0, a), where s0 is the next state, and we label the arrow with the transition probability, p(s0|s, a), and the expected reward for that transition, r(s, a, s0). Note that the transition probabilities labeling the arrows leaving an action node always sum to 1. In reinforcement learning, the purpose or goal of the agent is formalized in terms of a special signal, called the reward, passing from the environment to the agent.\n\nAt each time step, the reward is a simple number, Rt 2 R. Informally, the agent\u2019s goal is to maximize the total amount of reward it receives. This means maximizing not immediate reward, but cumulative reward in the long run. We can clearly state this informal idea as the reward hypothesis: That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward)", "8ec050fd-444d-4a10-939e-4faaa750cae2": "The regularizer pulls w; close to zero. In the second dimension, the objective function is very sensitive to movements away from w*. The corresponding eigenvalue is large, indicating high curvature. As a result, weight decay affects the position ofw2 relatively little. 229  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  Only directions along which the parameters contribute significantly to reducing the objective function are preserved relatively intact.\n\nIn directions that do not contribute to reducing the objective function, a small eigenvalue of the Hessian tells us that movement in this direction will not significantly increase the gradient. Components of the weight vector corresponding to such unimportant directions are decayed away through the use of the regularization throughout training. So far we have discussed weight decay in terms of its effect on the optimization of an abstract, general quadratic cost function. How do these effects relate to machine learning in particular? We can find out by studying linear regression, a model for which the true cost function is quadratic and therefore amenable to the same kind of analysis we have used so far", "b2f9aed7-a8fc-4eba-85b7-f6f60533f52b": "Recall that linear semi-gradient TD methods are e\ufb03cient and stable when trained under the on-policy distribution, and that we showed in Section 9.4 that this has to do with the positive de\ufb01niteness of the matrix A (9.11)4 and the match between the on-policy state distribution \u00b5\u21e1 and the state-transition probabilities p(s|s, a) under the target policy. In o\u21b5-policy learning, we reweight the state transitions using importance sampling so that they become appropriate for learning about the target policy, but the state distribution is still that of the behavior policy. There is a mismatch. A natural idea is to somehow reweight the states, emphasizing some and de-emphasizing others, so as to return the distribution of updates to the on-policy distribution. There would then be a match, and stability and convergence would follow from existing results.\n\nThis is the idea of 4In the o\u21b5-policy case, the matrix A is generally de\ufb01ned as Es\u21e0b Emphatic-TD methods, \ufb01rst introduced for on-policy training in Section 9.11", "afb07fde-10a1-41b7-aa09-f180ad062085": "showed that if the autoencoder p(x | X) forms a consistent estimator of the corresponding true conditional distribution, then the stationary distribution of the above Markov chain forms a consistent estimator (albeit an implicit one) of the data-generating distribution of x.\n\n20.11.2 Clamping and Conditional Sampling  https://www.deeplearningbook.org/contents/generative_models.html    Similarly to Boltzmann machines, denoising autoencoders and their generalizations such as GSNs, described below) can be used to sample from a conditional distri- bution p(xf| Xo), simply by clamping the observed units x, and only resampling he free units xf given x, and the sampled latent variables (if any). For example, MP-DBMs can be interpreted as a form of denoising autoencoder and are able o sample missing inputs. GSNs later generalized some of the ideas present in MP-DBMs to perform the same operation . Alain et al. identified a missing condition from Proposition 1 of Bengio et al", "67f1d90e-a6c1-4743-bd2a-3919d1539a81": "We know from the form of the normalized Gaussian given by (2.43), that this coef\ufb01cient is independent of the mean and depends only on the determinant of the covariance matrix. Thus, by completing the square with respect to xb, we can integrate out xb and the only term remaining from the contributions on the left-hand side of (2.84) that depends on xa is the last term on the right-hand side of (2.84) in which m is given by (2.85). Combining this term with the remaining terms from where \u2018const\u2019 denotes quantities independent of xa.\n\nAgain, by comparison with (2.71), we see that the covariance of the marginal distribution of p(xa) is given by Thus we obtain the intuitively satisfying result that the marginal distribution p(xa) has mean and covariance given by We see that for a marginal distribution, the mean and covariance are most simply expressed in terms of the partitioned covariance matrix, in contrast to the conditional distribution for which the partitioned precision matrix gives rise to simpler expressions. Our results for the marginal and conditional distributions of a partitioned Gaussian are summarized below", "46461e62-477f-4ee6-b78d-aa1ef19c45cb": "In this work, we present a simple framework and its instantiation for contrastive visual representation learning. We carefully study its components, and show the effects of different design choices. By combining our \ufb01ndings, we improve considerably over previous methods for selfsupervised, semi-supervised, and transfer learning. Our approach differs from standard supervised learning on ImageNet only in the choice of data augmentation, the use of a nonlinear head at the end of the network, and the loss function. The strength of this simple framework suggests that, despite a recent surge in interest, self-supervised learning remains undervalued.\n\nWe would like to thank Xiaohua Zhai, Rafael M\u00fcller and Yani Ioannou for their feedback on the draft. We are also grateful for general support from Google Research teams in Toronto and elsewhere. Asano, Y. M., Rupprecht, C., and Vedaldi, A. A critical analysis of self-supervision, or what we can learn from a single image. arXiv preprint arXiv:1904.13132, 2019. Bachman, P., Hjelm, R. D., and Buchwalter, W", "b0ad87f7-72cd-462d-8127-0008f04af50a": "view representation projection prediction  Given an image x, the BYOL loss is constructed as follows:  Create two augmented views: v = t(x); v\u2019 = t'(x) with augmentations sampled tr T,tvT'  \u00abThen they are encoded into representations, yg = fa(v),y\u2019 = fe(v\u2019); Then they are projected into latent variables, zg = go(yo), 2\u2019 = ge(y\u2019); The online network outputs a prediction g9(z9);  Both qg(z9) and z\u2019 are L2-normalized, giving us Ge(z) = go(zo)/|go(zo)| and 2! = 2'/|z'|; BYOL 6  >The loss is MSE between L2-normalized prediction gg(z) and z\u2019;  can be generated by switching v\u2019 and v; that is, feeding v\u2019 to  The other symmetric loss LRrYoe  online network and v to target network. The final loss is Lpryoe + LRrYoe and only parameters 0 are optimized. Unlike most popular contrastive learning based approaches, BYOL does not use negative pairs", "63e125d1-c105-471c-8a60-8548d6c9c13f": "AutoAugment on discrete space    Model AutoAugment ARS-Aug Wide-ResNet-28-10 268 2.33 Shake-Shake (26 2 x 32 days) 247 2.14 Shake-Shake (26 2 x 96 days) 1.99 1.68 Shake-Shake (26 2 x 112 days) 1.89 1.59 AmoebaNet-B (6,128) 1.75 1.49 PyramidNet + ShakeDrop 1.48 1.26  trained with Reinforcement Learning augmentation search performs much better. On the CIFAR-10 dataset this results in 50.99% versus 70.06% accuracy when the models are evaluated on augmented test data. A disadvantage to meta-learning is that it is a relatively new concept and has not been heavily tested. Additionally, meta-learning schemes can be difficult and time-consuming to implement. Practitioners of meta-learning will have to solve problems primarily with  vanishing gradients , amongst others, to train these networks.\n\nShorten and Khoshgoftaar J Big Data  6:60   Comparing Augmentations  As shown throughout \u201cDesign considerations for image Data Augmentation\u201d section, possibilities for Data Augmentation", "084c1ae8-978a-41fd-b99c-fcb4f3e27114": "He spoke of the eternal con\ufb02ict between the promise and perils of any new knowledge, reminding us of the Greek myths of Prometheus, the hero of modern science, who stole \ufb01re from the gods for the bene\ufb01t of mankind, and Pandora, whose box could be opened by a small and innocent action to release untold perils on the world. While accepting that this con\ufb02ict is inevitable, Simon urged us to recognize that as designers of our future and not mere spectators, the decisions we make can tilt the scale in Prometheus\u2019 favor. This is certainly true for reinforcement learning, which can bene\ufb01t society but can also produce undesirable outcomes if it is carelessly deployed. Thus, the safety of arti\ufb01cial intelligence applications involving reinforcement learning is a topic that deserves careful attention.\n\nA reinforcement learning agent can learn by interacting with either the real world or with a simulation of some piece of the real world, or by a mixture of these two sources of experience. Simulators provide safe environments in which an agent can explore and learn without risking real damage to itself or to its environment. In most current applications, policies are learned from simulated experience instead of direct interaction with the real world", "389d1208-8021-48cd-a07d-2003ba50e4b7": "We shall also use \u03b3(znk) to denote the conditional probability of znk = 1, with a similar use of notation for \u03be(zn\u22121,j, znk) and for other probabilistic variables introduced later. Because the expectation of a binary random variable is just the probability that it takes the value 1, we have If we substitute the joint distribution p(X, Z|\u03b8) given by (13.10) into (13.12), and make use of the de\ufb01nitions of \u03b3 and \u03be , we obtain The goal of the E step will be to evaluate the quantities \u03b3(zn) and \u03be(zn\u22121, zn) ef\ufb01ciently, and we shall discuss this in detail shortly. In the M step, we maximize Q(\u03b8, \u03b8old) with respect to the parameters \u03b8 = {\u03c0, A, \u03c6} in which we treat \u03b3(zn) and \u03be(zn\u22121, zn) as constant.\n\nMaximization with respect to \u03c0 and A is easily achieved using appropriate Lagrange multipliers with the results Exercise 13.5 The EM algorithm must be initialized by choosing starting values for \u03c0 and A, which should of course respect the summation constraints associated with their probabilistic interpretation", "4a562317-8ad2-4ece-bb21-57e198f5db89": "We begin by considering the functional form of the network model, including the speci\ufb01c parameterization of the basis functions, and we then discuss the problem of determining the network parameters within a maximum likelihood framework, which involves the solution of a nonlinear optimization problem. This requires the evaluation of derivatives of the log likelihood function with respect to the network parameters, and we shall see how these can be obtained ef\ufb01ciently using the technique of error backpropagation. We shall also show how the backpropagation framework can be extended to allow other derivatives to be evaluated, such as the Jacobian and Hessian matrices. Next we discuss various approaches to regularization of neural network training and the relationships between them. We also consider some extensions to the neural network model, and in particular we describe a general framework for modelling conditional probability distributions known as mixture density networks. Finally, we discuss the use of Bayesian treatments of neural networks. Additional background on neural network models can be found in Bishop .\n\nThe linear models for regression and classi\ufb01cation discussed in Chapters 3 and 4, respectively, are based on linear combinations of \ufb01xed nonlinear basis functions \u03c6j(x) and take the form where f(\u00b7) is a nonlinear activation function in the case of classi\ufb01cation and is the identity in the case of regression", "190a12e8-7a78-443e-982e-32d624ca31f0": "A much more \ufb02exible model is obtained by using a multilevel gating function to give the hierarchical mixture of experts, or HME model . To understand the structure of this model, imagine a mixture distribution in which each component in the mixture is itself a mixture distribution. For simple unconditional mixtures, this hierarchical mixture is trivially equivalent to a single \ufb02at mixture distribution. However, when the mixing Exercise 14.17 coef\ufb01cients are input dependent, this hierarchical model becomes nontrivial. The HME model can also be viewed as a probabilistic version of decision trees discussed in Section 14.4 and can again be trained ef\ufb01ciently by maximum likelihood using an EM algorithm with IRLS in the M step. A Bayesian treatment of the HME has been Section 4.3.3 given by Bishop and Svens\u00b4en  based on variational inference. We shall not discuss the HME in detail here.\n\nHowever, it is worth pointing out the close connection with the mixture density network discussed in Section 5.6. The principal advantage of the mixtures of experts model is that it can be optimized by EM in which the M step for each mixture component and gating model involves a convex optimization (although the overall optimization is nonconvex)", "8c44f626-673f-4647-b8b7-742f59a15d12": "This suggests that it is part of a system that uses an environment model for planning. Reinforcement learning theory is also in\ufb02uencing thinking about neural processes underlying drug abuse. A model of some features of drug addiction is based on the reward prediction error hypothesis.\n\nIt proposes that an addicting stimulant, such as cocaine, destabilizes TD learning to produce unbounded growth in the values of actions associated with drug intake. This is far from a complete model of addiction, but it illustrates how a computational perspective suggests theories that can be tested with further research. The new \ufb01eld of computational psychiatry similarly focuses on the use of computational models, some derived from reinforcement learning, to better understand mental disorders. This chapter only touched the surface of how the neuroscience of reinforcement learning and the development of reinforcement learning in computer science and engineering have in\ufb02uenced one another. Most features of reinforcement learning algorithms owe their design to purely computational considerations, but some have been in\ufb02uenced by hypotheses about neural learning mechanisms. Remarkably, as experimental data has accumulated about the brain\u2019s reward processes, many of the purely computationallymotivated features of reinforcement learning algorithms are turning out to be consistent with neuroscience data", "4606f79c-b935-4517-8b95-2fdd74547841": "GTM: a principled alternative to the Self-Organizing Map. In M. C. Mozer, M. I. Jordan, and T. Petche (Eds. ), Advances in Neural Information Processing Systems, Volume 9, pp. 354\u2013360. MIT Press. Bishop, C. M., M. Svens\u00b4en, and C. K. I. Williams . Magni\ufb01cation factors for the GTM algorithm. In Proceedings IEE Fifth International Conference on Arti\ufb01cial Neural Networks, Cambridge, U.K., pp. 64\u201369. Institute of Electrical Engineers. Bishop, C. M., M. Svens\u00b4en, and C. K. I. Williams . GTM: the Generative Topographic Mapping. Neural Computation 10(1), 215\u2013234. Bishop, C. M. and M. E. Tipping", "38722701-52ee-4c5d-b55e-da5be5c9c527": "Any choice of sampling distribution gq is valid (in the sense of yielding the correct expected value), and q* is the optimal one (in the sense of yielding minimum variance). Sampling from q* is usually infeasible, but other choices of q can be feasible while still reducing the variance somewhat. 590  CHAPTER 17.\n\nMONTE CARLO METHODS  Another approach is to use biased importance sampling, which has the advantage of not requiring normalized p or q. In the case of discrete variables, the biased importance sampling estimator is given by  n p(w) ; Vie ee) f(a! )) \u00a7BIS = (17.14)  (17.15)  rn pla i et Fat) f(a = ne ; (17.16) =1 g(a)  where and g are the unnormalized forms of p and q, and the \u00ab are the samples from g. This estimator is biased because E 4 s, except asymptotically when nm \u2014 oo and the denominator of equation 17.14 converges to 1", "172f3720-8f45-4748-82f7-add696bc8cc5": "This probabilistic reformulation bring~ many ad\\'imlag~s, su~h as tl>l: use I)f EM for parameter eslimalion, rrinciple<J c~tensioos 10 Oli~turc, of PeA model\" and Ba)'~sian formulat;ons that allow tbe number of rrincipal com[>Oncnts to be detennined aUlOmatically from !be data. Finally'. \"c diSl;us< briefly \"\"'eral gencrali,ation, ofthe latent Yariable concept that g<l be~ood tbe linear-Gaussian assumption including non\u00b7Gau\"i\"n I.tcnt yariabies.....hich lea'\" to tbe fr.me....ork of indrl\"'mJ.m compon.nl anal,-.\n\n;., as ....ell a, S'di\"\" 12.4 models ha\"ing a nonlinear rclationship bet....een latent and oose\",e<J ,'lUiable,", "ffed57e7-34cc-4303-b19b-d77ee09223ab": "We can think of the lower layers in the hierarchy depicted in figure 10.13a as playing a role in transforming the raw input into a representation that is more appropriate, at the higher levels of the hidden state. Pascanu et al. go a step further and propose to have a separate MLP (possibly deep) for each of the three blocks enumerated above, as illustrated in figure 10.13b. Considerations of representational capacity suggest allocating enough capacity in each of these three steps, but doing so by adding depth may hurt learning by making optimization difficult. In general, it is easier to optimize shallower architectures, and adding the extra depth of figure 10.13b makes the shortest path from a variable in time step t to a variable in time step t+ 1 become longer. For example, if an MLP with a single hidden layer is used for the state-to-state transition, we have doubled the length of the shortest path between variables in any two different time steps, compared with the ordinary RNN of figure 10.3. However, as argued by Pascanu ef al", "9937ff28-38ef-4855-b090-e5dfd1e6c1ac": "They found that learning was not successful with this reward signal for episodes of realistic duration and that eligibility traces did not help. By experimenting with various reward signals, they found that learning was best with a reward signal that at each time step linearly combined the vertical wind velocity and vertical wind acceleration observed on the previous time step. Learning was by one-step Sarsa, with actions selected according to a soft-max distribution based on normalized action values.\n\nSpeci\ufb01cally, the action probabilities were computed according to (13.2) with action preferences: where \u2713 is a parameter vector with one component for each action and aggregated group of states, and \u02c6q(s, a, \u2713) merely returned the component corresponding to s, a in the usual way for state aggregation methods. The above equation forms the action preferences by normalizing the approximate action values to the interval  then dividing by \u2327, a positive \u201ctemperature parameter.\u201d3 As \u2327 increases, the probability of selecting an action becomes less dependent on its preference; as \u2327 decreases toward zero, the probability of selecting the most highly-preferred action approaches one, making the policy approach the greedy policy. The temperature parameter \u2327 was initialized to 2.0 and incrementally decreased to 0.2 during learning", "b525260a-9cd9-4799-a18a-3c1d773e349f": "Because the output log-likelihood can be computed efficiently (as low as log |V| rather than |V|), its gradients may also be computed efficiently. This includes not only the gradient with respect to the output parameters but also the gradients with respect to the hidden layer activations. It is possible but usually not practical to optimize the tree structure to minimize the expected number of computations. Tools from information theory specify how to choose the optimal binary code given the relative frequencies of the words. To do so, we could structure the tree so that the number of bits associated with a word is approximately equal to the logarithm of the frequency of that word.\n\nIn practice, however, the computational savings are typically not worth the effort because the computation of the output probabilities is only one part of the total computation in the neural language model. For example, suppose there are | fully connected hidden layers of width y,. Let np be the weighted average of the number  462  https://www.deeplearningbook.org/contents/applications.html    CHAPTER 12", "df72bc58-9ca3-47d7-ab74-a1e9362782c0": "5.23 (\u22c6 \u22c6) Extend the results of Section 5.4.5 for the exact Hessian of a two-layer network to include skip-layer connections that go directly from inputs to outputs. 5.24 (\u22c6) Verify that the network function de\ufb01ned by (5.113) and (5.114) is invariant under the transformation (5.115) applied to the inputs, provided the weights and biases are simultaneously transformed using (5.116) and (5.117). Similarly, show that the network outputs can be transformed according (5.118) by applying the transformation (5.119) and (5.120) to the second-layer weights and biases. where w\u22c6 represents the minimum, and the Hessian matrix H is positive de\ufb01nite and constant.\n\nSuppose the initial weight vector w(0) is chosen to be at the origin and is updated using simple gradient descent where \u03c4 denotes the step number, and \u03c1 is the learning rate (which is assumed to be small). Show that, after \u03c4 steps, the components of the weight vector parallel to the eigenvectors of H can be written Show that as \u03c4 \u2192 \u221e, this gives w(\u03c4) \u2192 w\u22c6 as expected, provided |1 \u2212 \u03c1\u03b7j| < 1", "8049bc32-6cc4-4161-9008-0866f2fe42b8": "In PSRs and related approaches, the semantics of the agent state is instead grounded in predictions about future observations and actions, which are readily observable.\n\nIn PSRs, a Markov state is de\ufb01ned as a d-vector of the probabilities of d specially chosen \u201ccore\u201d tests as de\ufb01ned above (17.6). The vector is then updated by a state-update function u that is analogous to Bayes rule, but with a semantics grounded in observable data, which arguably makes it easier to learn. This approach has been extended in many ways, including end-tests, compositional tests, powerful \u201cspectral\u201d methods, and closed-loop and temporally abstract tests learned by TD methods. Some of the best theoretical developments are for systems known as Observable Operator Models (OOMs) and Sequential Systems . The fourth and \ufb01nal step in our brief outline of how to handle partial observability in reinforcement learning is to re-introduce approximation. As discussed in the introduction to Part II, to approach arti\ufb01cial intelligence ambitiously one must embrace approximation. This is just as true for states as it is for value functions. We must accept and work with an approximate notion of state", "9ff795ee-dc39-4a9c-b994-085a4dbff632": "For instance in speech recognition, we might wish to \ufb01nd the most probable phoneme sequence for a given series of acoustic observations. Because the graph for the hidden Markov model is a directed tree, this problem can be solved exactly using the max-sum algorithm. We recall from our discussion in Section 8.4.5 that the problem of \ufb01nding the most probable sequence of latent states is not the same as that of \ufb01nding the set of states that are individually the most probable. The latter problem can be solved by \ufb01rst running the forward-backward (sum-product) algorithm to \ufb01nd the latent variable marginals \u03b3(zn) and then maximizing each of these individually . However, the set of such states will not, in general, correspond to the most probable sequence of states.\n\nIn fact, this set of states might even represent a sequence having zero probability, if it so happens that two successive states, which in isolation are individually the most probable, are such that the transition matrix element connecting them is zero. In practice, we are usually interested in \ufb01nding the most probable sequence of states, and this can be solved ef\ufb01ciently using the max-sum algorithm, which in the context of hidden Markov models is known as the Viterbi algorithm", "6c42e194-28c0-467e-be19-bb6a38750748": "Where do these intermediate distributions come from? Just as the original proposal distribution pp is a design choice, so is the sequence of distributions Pm +++Pyn\u20141- That is, it can be specifically constructed to suit the problem domain.\n\nOne general purpose and popular choice for the intermediate distributions is to use the weighted geometric average of the target distribution p; and the starting proposal distribution (for which the partition function is known) po:  Pn, XPi'py (18.50)  In order to sample from these intermediate distributions, we define a series of Markov chain transition functions T,, ,(a\u2019 | ) that define the conditional probability distribution of transitioning to x\u2019 given we are currently at 2. The transition operator T;,,(x\u2019 | #) is defined to leave p,, (a) invariant:  pry) = | my, (a!) | 1) da (18.51)  These transitions may be constructed as any Markov chain Monte Carlo method (e.g., Metropolis-Hastings, Gibbs), including methods involving multiple passes through all the random variables or other kinds of iterations. 624  CHAPTER 18", "b4c3676f-480b-4b12-a9fd-a1413aa2d4de": "7 . ' . \u2018 0.75 . \u2018 f a Oo \" ? es 3070 \u00a9 a a . 2 . iss a 065 \u00a9 a \u2018 7 : rs 0.60 . No augmentation sa * Aug. (train) 0.55 * \u00a9 Aug. (train + test) 0.50 125 500 1000 1500 2000 125 500 1000 1500 2000 125 500 1000 1500 2000 Inception-v4 ResNet-152 DenseNet-161 Dataset Length Fig. 32 Impact of test-time data augmentation for skin lesion classification   Perez et al. present a study on the effectiveness of test-time augmentation with many augmentation techniques. These augmentations tested include color augmen- tation, rotation, shearing, scaling, flipping, random cropping, random erasing, elastic, mixing, and combinations between the techniques. Table 9 shows the higher perfor- mance achieved when augmenting test images as well as training images", "fe6c677c-01e7-47a1-a465-aaa33a1f0778": "studied the effect of pretraining on machine learning models for chemical activity prediction and found that, on average, pretraining was  https://www.deeplearningbook.org/contents/representation.html       slightly harmful, but for many tasks was significantly helpful. Because unsupervised pretraining is sometimes helpful but often harmful, it is important to understand when and why it works in order to determine whether it is applicable to a particular  ask. At the outset, it is important to clarify that most of this discussion is restricted o greedy unsupervised pretraining in particular. There are other, completely different paradigms for performing semi-supervised learning with neural networks, such as virtual adversarial training, described in section 7.13. It is also possible to  rain an autoencoder or generative model at the same time as the supervised model. Examples of this single-stage approach include the discriminative RBM  and the ladder network , in which the total objective is an explicit sum of the two terms (one using the labels, and one only using the input). Unsupervised pretraining combines two different ideas", "da5e9f7c-4cf1-49c8-a46c-cc186edd4603": "We are given a joint distribution over observed data D and stochastic variables \u03b8 in the form of a product of factors We also wish to approximate the model evidence p(D). 1. Initialize all of the approximating factors \ufffdfi(\u03b8). 2. Initialize the posterior approximation by setting (b) Remove \ufffdfj(\u03b8) from the posterior by division (c) Evaluate the new posterior by setting the suf\ufb01cient statistics (moments) of qnew(\u03b8) equal to those of q\\j(\u03b8)fj(\u03b8), including evaluation of the normalization constant 4. Evaluate the approximation to the model evidence A special case of EP, known as assumed density \ufb01ltering (ADF) or moment matching , is obtained by initializing all of the approximating factors except the \ufb01rst to unity and then making one pass through the factors updating each of them once. Assumed density \ufb01ltering can be appropriate for on-line learning in which data points are arriving in a sequence and we need to learn from each data point and then discard it before considering the next point", "83a9b8ff-cdac-451d-82ca-826987c5053c": "It is also perhaps surprising that we are able to achieve such signi\ufb01cant improvements on top of models which are already quite large relative to the existing literature. For example, the largest Transformer explored in Vaswani et al. is (L=6, H=1024, A=16) with 100M parameters for the encoder, and the largest Transformer we have found in the literature is (L=64, H=512, A=2) with 235M parameters . By contrast, BERTBASE contains 110M parameters and BERTLARGE contains 340M parameters.\n\nIt has long been known that increasing the model size will lead to continual improvements on large-scale tasks such as machine translation and language modeling, which is demonstrated by the LM perplexity of held-out training data shown in Table 6. However, we believe that this is the \ufb01rst work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been suf\ufb01ciently pre-trained. Peters et al. presented mixed results on the downstream task impact of increasing the pre-trained bi-LM size from two to four layers and Melamud et al. mentioned in passing that increasing hidden dimension size from 200 to 600 helped, but increasing further to 1,000 did not bring further improvements", "be5cfe66-cdb9-437a-b19e-e9e793e8725c": "This is because for a transition from a reward-predicting state to another reward-predicting state, we have \u03b4t\u22121 = Rt + Vt \u2212 Vt\u22121 = 0 + R? \u2212 R? = 0, and for the transition from the latest reward-predicting state to the rewarding state, we have \u03b4t\u22121 = Rt +Vt \u2212Vt\u22121 = R? +0\u2212R? = 0. On the other hand, the TD error on a transition from any state to the earliest reward-predicting state is positive because of the mismatch between this state\u2019s low value and the larger value of the following reward-predicting state.\n\nIndeed, if the value of a state preceding the earliest reward-predicting state were zero, then after the transition to the earliest reward-predicting state, we would have that \u03b4t\u22121 = Rt + Vt \u2212 Vt\u22121 = 0 + R? \u2212 0 = R?. The \u2018learning complete\u2019 graph of \u03b4 in Figure 15.4 shows this positive value at the earliest reward-predicting state, and zeros everywhere else. The positive TD error upon transitioning to the earliest reward-predicting state is analogous to the persistence of dopamine responses to the earliest stimuli predicting reward", "984bdfef-fc6a-468e-8268-5e7867ea3312": "The number of points in column i, corresponding to X = xi, is denoted by ci, and the number of points in row j, corresponding to Y = yj, is denoted by rj. and the probability of selecting the blue box is 6/10. We write these probabilities as p(B = r) = 4/10 and p(B = b) = 6/10. Note that, by de\ufb01nition, probabilities must lie in the interval . Also, if the events are mutually exclusive and if they include all possible outcomes (for instance, in this example the box must be either red or blue), then we see that the probabilities for those events must sum to one.\n\nWe can now ask questions such as: \u201cwhat is the overall probability that the selection procedure will pick an apple?\u201d, or \u201cgiven that we have chosen an orange, what is the probability that the box we chose was the blue one?\u201d. We can answer questions such as these, and indeed much more complex questions associated with problems in pattern recognition, once we have equipped ourselves with the two elementary rules of probability, known as the sum rule and the product rule. Having obtained these rules, we shall then return to our boxes of fruit example", "d37a728c-f154-477d-9427-a79e136a2a00": "We need to keep track of up to 10 10 = 100 regions, and we need at least that gnany examples to cover all those regions. (Right)With three dimensions, this grows to 10 = 1,000 regions and at least that many examples. For d dimensions and v values to be distinguished along  each axis, we seem to need O( v4) regions and examples. This is an instance of the curse of dimensionality. Figure graciously provided by Nicolas Chapados. One challenge posed by the curse of dimensionality is a statistical challenge. As illustrated in figure 5.9, a statistical challenge arises because the number of possible configurations of x is much larger than the number of training examples.\n\nTo understand the issue, let us consider that the input space is organized into a grid, as in the figure. We can describe low-dimensional space with a small number of grid cells that are mostly occupied by the data. When generalizing to a new data point, we can usually tell what to do simply by inspecting the training examples that lie in the same cell as the new input", "bd023ab6-fb92-4449-b480-5885eedb400b": "left-hand plot shows the original distribution (yellow) along with the Laplace (red) and variational (green) approximations, and the right-hand plot shows the negative logarithms of the corresponding curves. However, we shall suppose the model is such that working with the true posterior distribution is intractable. We therefore consider instead a restricted family of distributions q(Z) and then seek the member of this family for which the KL divergence is minimized. Our goal is to restrict the family suf\ufb01ciently that they comprise only tractable distributions, while at the same time allowing the family to be suf\ufb01ciently rich and \ufb02exible that it can provide a good approximation to the true posterior distribution.\n\nIt is important to emphasize that the restriction is imposed purely to achieve tractability, and that subject to this requirement we should use as rich a family of approximating distributions as possible. In particular, there is no \u2018over-\ufb01tting\u2019 associated with highly \ufb02exible distributions. Using more \ufb02exible approximations simply allows us to approach the true posterior distribution more closely. One way to restrict the family of approximating distributions is to use a parametric distribution q(Z|\u03c9) governed by a set of parameters \u03c9", "a639aa45-c2a6-451e-a607-1d90fbdfc25a": "The equation S = k ln W is carved on Boltzmann\u2019s tombstone.\n\nThe constrained maximization can be performed using Lagrange multipliers so that Appendix E we maximize the following functional with respect to p(x) Using the calculus of variations, we set the derivative of this functional to zero giving Appendix D The Lagrange multipliers can be found by back substitution of this result into the three constraint equations, leading \ufb01nally to the result Exercise 1.34 and so the distribution that maximizes the differential entropy is the Gaussian. Note that we did not constrain the distribution to be nonnegative when we maximized the entropy. However, because the resulting distribution is indeed nonnegative, we see with hindsight that such a constraint is not necessary. If we evaluate the differential entropy of the Gaussian, we obtain Exercise 1.35 Thus we see again that the entropy increases as the distribution becomes broader, i.e., as \u03c32 increases. This result also shows that the differential entropy, unlike the discrete entropy, can be negative, because H(x) < 0 in (1.110) for \u03c32 < 1/(2\u03c0e)", "e848ffd2-4119-4d5b-bd4e-1935d504fb72": "The timing and shape of this CR is critical to its adaptive signi\ufb01cance\u2014covering the eye too early reduces vision (even though the nictitating membrane is translucent), while covering it too late is of little protective value. Capturing CR features like these is challenging for models of classical conditioning.\n\nThe TD model does not include as part of its de\ufb01nition any mechanism for translating the time course of the US prediction, \u02c6v(St,wt), into a pro\ufb01le that can be compared with the properties of an animal\u2019s CR. The simplest choice is to let the time course of a simulated CR equal the time course of the US prediction. In this case, features of simulated CRs and how they change over trials depend only on the stimulus representation chosen and the values of the model\u2019s parameters \u21b5, \u03b3, and \u03bb. occurred 25 time steps after the onset of the CS, and \u21b5 = .05, \u03bb = .95 and \u03b3 = .97. With the CSC representation (Figure 14.4 left), the curve of the US prediction formed by the TD model increases exponentially throughout the interval between the CS and the US until it reaches a maximum exactly when the US occurs (at time step 25)", "5471d01a-0886-40cc-b03d-dfae529cd77f": "Early stopping is also useful because it reduces the computational cost of the training procedure. Besides the obvious reduction in cost due to limiting the number of training iterations, it also has the benefit of providing regularization without requiring the addition of penalty terms to the cost function or the computation of the gradients of such additional terms. How early stopping acts as a regularizer: So far we have stated that early stopping is a regularization strategy, but we have supported this claim only by showing learning curves where the validation set error has a U-shaped curve. What  246  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  https://www.deeplearningbook.org/contents/regularization.html    wy  Figure 7.4: An illustration of the effect of early stopping. (Left)The solid contour lines indicate the contours of the negative log-likelihood. The dashed line indicates the trajectory taken by SGD beginning from the origin. Rather than stopping at the point w* that minimizes the cost, early stopping results in the trajectory stopping at an earlier point w. (Right)An illustration of the effect of L? regularization for comparison", "b008bd59-9c02-474f-b9a6-e6fc0380e455": "Different Computer Vision tasks require certain con- straints on the test-time augmentations that can be used. For example, image recogni- tion can easily aggregate predictions across warped images.\n\nHowever, it is difficult to aggregate predictions on geometrically transformed images in object detection and  semantic segmentation. Curriculum learning  Aside from the study of Data Augmentation, many researchers have been interested in trying to find a strategy for selecting training data that beats random selection. In the context of Data Augmentation, research has been published investigating the rela-  tionship between original and augmented data across training epochs. Some research   Shorten and Khoshgoftaar J Big Data  6:60   Table 10 Comparison of resolution across three very popular open-source image datasets  Dataset Resolution MNIST handwritten digits 28x 28x1 CIFAR-10/100 32x 32x 3 ImageNet 256 x 256 x 3  suggests that it is best to initially train with the original data only and then finish training with the original and augmented data, although there is no clear consensus. In the SamplePairing  study, one epoch on ImageNet and 100 epochs on other datasets are completed without SamplePairing before mixed image data is added to the training", "e79189c5-ab88-4705-95ac-d941014b2b7a": "All of these distributions are members of the exponential family and are widely used as building blocks for more sophisticated probabilistic models.\n\nThis is the distribution for a single binary variable x \u2208 {0, 1} representing, for example, the result of \ufb02ipping a coin. It is governed by a single continuous parameter \u00b5 \u2208  that represents the probability of x = 1. The Bernoulli is a special case of the binomial distribution for the case of a single observation. Its conjugate prior for \u00b5 is the beta distribution. This is a distribution over a continuous variable \u00b5 \u2208 , which is often used to represent the probability for some binary event. It is governed by two parameters a and b that are constrained by a > 0 and b > 0 to ensure that the distribution can be normalized. The beta is the conjugate prior for the Bernoulli distribution, for which a and b can be interpreted as the effective prior number of observations of x = 1 and x = 0, respectively. Its density is \ufb01nite if a \u2a7e 1 and b \u2a7e 1, otherwise there is a singularity at \u00b5 = 0 and/or \u00b5 = 1", "b4db1c0d-93ba-407c-92e9-87491999ae19": "Shorten and Khoshgoftaar J Big Data  6:60  55. 56. 93. 94,  95. Olaf R, Philipp F, Thomas B. U-Net: convolutional networks for biomedical image segmentation.\n\nIn: MICCAI. Springer; 2015, p. 234-41. Hessam B, Maxwell H, Mohammad R, Ali F. Label refinery: improving imagenet classification through label pro- gression. arXiv preprint. 2018. Francisco JM-B, Fiammetta S, Jose MJ, Daniel U, Leonardo F. Forward noise adjustment scheme for data augmen- tation. arXiv preprints. 2018. Dua D, Karra TE. UCI machine learning repository . Irvine, CA: University of California, School of Information and Computer Science; 2017. Ken C, Karen S, Andrea V, Andrew Z. Return of the devil in the details: delving deep into convolutional nets. In: Proceedings of BMVC", "023cb4be-5d80-4885-9f6b-efeb923bec02": "The phasic responses of these neurons had shifted from the reward itself to stimuli predicting the availability of the reward. In a followup study, Romo and Schultz found that most of the dopamine neurons whose activity they monitored did not respond to the sight and sound of the bin opening outside the context of the behavioral task. These observations suggested that the dopamine neurons were responding neither to the initiation of a movement nor to the sensory properties of the stimuli, but were rather signaling an expectation of reward.\n\nSchultz\u2019s group conducted many additional studies involving both SNpc and VTA dopamine neurons. A particular series of experiments was in\ufb02uential in suggesting that the phasic responses of dopamine neurons correspond to TD errors and not to simpler errors like those in the Rescorla\u2013Wagner model (14.3). In the \ufb01rst of these experiments , monkeys were trained to depress a lever after a light was illuminated as a \u2018trigger cue\u2019 to obtain a drop of apple juice. As Romo and Schultz had observed earlier, many dopamine neurons initially responded to the reward\u2014the drop of juice (Figure 15.2, top panel)", "b005e038-a267-43ed-a439-a41c2eada598": "Many regularization strategies have the advantage of allowing the user to control the strength of the regularization by adjusting the value of a single hyperparameter. Unsupervised pretraining does not offer a clear way to adjust the strength of the regularization arising from the unsupervised stage. Instead, there are very  531  CHAPTER 15. REPRESENTATION LEARNING  vv\u00a5y With pretraining  e@@ Without pretraining  https://www.deeplearningbook.org/contents/representation.html    \u2014500 \u20141000 Oe  500 \u20144000 \u20143000 \u20142000 \u20141000 0 1000 2000 3000 4000  Figure 15.1: Visualization via nonlinear projection of the learning trajectories of different neural networks in function space (not parameter space, to avoid the issue of many-to-one mappings from parameter vectors to functions), with different random initializations and with or without unsupervised pretraining. Each point corresponds to a different neural network at a particular time during its training process. This figure is adapted with permission from Erhan ei al. A coordinate in function space is an infinite- dimensional vector associating every input x with an output y", "45242168-786e-4f8b-954d-1bb082c775cf": "This means that an autoencoder with a single hidden layer is able to represent the identity function along the domain of the data arbitrarily well. However, the mapping from input to code is shallow. This means that we are not able to enforce arbitrary constraints, such as that the code should be sparse. A deep autoencoder, with at least one additional hidden layer inside the encoder itself, can approximate any mapping from input to code arbitrarily well, given enough hidden units. Depth can exponentially reduce the computational cost of representing some functions. Depth can also exponentially decrease the amount of training data needed to learn some functions. See section 6.4.1 for a review of the advantages of depth in feedforward networks", "ed041fc5-64ff-4ccb-b34a-71d4a8442468": "Despite these complications, aligning the classical/instrumental distinction with the prediction/control distinction is a convenient \ufb01rst approximation in connecting reinforcement learning to animal learning. In psychology, the term reinforcement is used to describe learning in both classical and instrumental conditioning. Originally referring only to the strengthening of a pattern of behavior, it is frequently also used for the weakening of a pattern of behavior. A stimulus considered to be the cause of the change in behavior is called a reinforcer, whether or not it is contingent on the animal\u2019s previous behavior.\n\nAt the end of this chapter we discuss this terminology in more detail and how it relates to terminology used in machine learning. While studying the activity of the digestive system, the celebrated Russian physiologist Ivan Pavlov found that an animal\u2019s innate responses to certain triggering stimuli can come to be triggered by other stimuli that are quite unrelated to the inborn triggers. His experimental subjects were dogs that had undergone minor surgery to allow the intensity of their salivary re\ufb02ex to be accurately measured. In one case he describes, the dog did not salivate under most circumstances, but about 5 seconds after being presented with food it produced about six drops of saliva over the next several seconds", "5ed39faf-e0c3-49ba-950e-e40548ca0a4f": "Satisfying this constraint requires some careful design effort. Suppose we were : a ta re ee hy onts  https://www.deeplearningbook.org/contents/mlp.html   LO USE a WNCar ULL ALG LLIPeSlold 1b8 Value LO ODLALLL aA VALIG PrOvDaDIILy: . T P(y=1| #) = max {0,min 1, ha oyy. (6.18)  This would indeed define a valid conditional distribution, but we would not be able to train it very effectively with gradient descent. Any time that w!h +b strayed  178  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  outside the unit interval, the gradient of the output of the model with respect to its parameters would be 0. A gradient of 0 is typically problematic because the learning algorithm no longer has a guide for how to improve the corresponding parameters. Instead, it is better to use a different approach that ensures there is always a strong gradient whenever the model has the wrong answer", "34a8f61f-af41-4b15-a2c4-094a87a45d9c": "softmax(x); (4.1)  There is still one small problem. Underflow in the numerator can still cause he expression as a whole to evaluate to zero. This means that if we implement og softmax(ax) by first running the softmax subroutine then passing the result to he log function, we could erroneously obtain \u2014oo. Instead, we must implement a separate function that calculates log softmax in a numerically stable way. The og softmax function can be stabilized using the same trick as we used to stabilize che softmax function. For the most part, we do not explicitly detail all the numerical considerations involved in implementing the various algorithms described in this book. Developers  https://www.deeplearningbook.org/contents/numerical.html    ot low-level libraries should keep numerical issues in mind when implementing deep learning algorithms.\n\nMost readers of this book can simply rely on low- level libraries that provide stable implementations. In some cases, it is possible to implement a new algorithm and have the new implementation automatically  79  CHAPTER 4. NUMERICAL COMPUTATION  stabilized", "1d1bcbd8-d9ad-42b5-91df-cd8e598da1ba": "(7.56) y  We can index into the family of submodels by element-wise multiplication of the input with a binary vector d:  P(y =y | v;d) = softmax (wa Ov)+ b) .\n\n(7.57) y  The ensemble predictor is defined by renormalizing the geometric mean over all ensemble members\u2019 predictions:  Pensemble(Y =Yy | v)  Pensemble(Y =Y | v) = ~ ; (7.58) Ly Pensemble(\u00a5 = y! | v) where PensemblelY =Y | v) = 9nr Il Ply =y | Vv; d)", "5926d2ba-179d-4891-9ba3-ce45dc23965d": "This has become a more pressing issue in recent years as deep learning has gained in popularity in industrial products, and as the great impact of faster hardware was demonstrated with GPUs. Another factor that motivates current research on specialized hardware for deep networks is that the rate of progress of a single CPU or GPU core has slowed down, and most recent improvements in  446  CHAPTER 12. APPLICATIONS  computing speed have come from parallelization across cores (either in CPUs or GPUs).\n\nThis is very different from the situation of the 1990s (the previous neural network era), when the hardware implementations of neural networks (which might take two years from inception to availability of a chip) could not keep up with the rapid progress and low prices of general-purpose CPUs. Building specialized hardware is thus a way to push the envelope further, at a time when new hardware designs are being developed for low-power devices such as phones, aiming for  https://www.deeplearningbook.org/contents/applications.html    general-public applications ot deep learning  suggests that between 8 and 16 bits of precision can suffice for using or training deep neural networks with back-propagation", "d7c7a079-b494-4e3f-875b-e2160f415867": "We develop a new RL formulation for text generation based on soft Q-learning and path consistency learning. We conduct experiments on learning with noisy and negative data, black box adversarial attack, prompting a pretrained language model for controllable generation, and standard supervised tasks. This formulation opens up new opportunities to integrate more advances made in the fertile RL literature to improve text generation problems. A well-documented limitation of RL methods is the importance of the reward function. The proposed methods are no different in this aspect. This is especially relevant as our reward function could involve a learned model itself, which we proactively leveraged in Sec. 4.2. We refer interested readers to Deng et al. for more algorithmic considerations.\n\nWe also noticed that adapting the pretraining-\ufb01netuning paradigm to the proposed methods requires careful designs. A hypothesis points to the discrepancy between MLE objectives (commonly used in pretraining context) and SQL objectives. As discussed in Sec. 3.1, the SQL formulation re-interprets the \u201clogit\u201d as the Q-value, for many good reasons", "2d8ffef9-9f90-4e01-9ac3-630e7ee31520": "Minimization with respect to h is fast because f(a) provides a good initial value of h, and the cost function constrains h to remain near f(a) anyway. Simple gradient descent can obtain reasonable values of h in as few as ten steps. The training procedure used by PSD is different from first training a sparse coding model and then training f(a) to predict the values of the sparse coding features. The PSD training procedure regularizes the decoder to use parameters for which f(a) can infer good code values. Predictive sparse coding is an example of learned approximate inference. In section 19.5, this topic is developed further. The tools presented in chapter 19 make it clear that PSD can be interpreted as training a directed sparse coding probabilistic model by maximizing a lower bound on the log-likelihood of the model. 521  CHAPTER 14. AUTOENCODERS  In practical applications of PSD, the iterative optimization is used only during training.\n\nThe parametric encoder f is used to compute the learned features when  https://www.deeplearningbook.org/contents/autoencoders.html    the model is deployed", "fee86717-22b0-4203-98ea-c93bbf3373fa": "The relationship between forward mode and backward mode is analogous to the relationship between left-multiplying versus right-multiplying a sequence of matrices, such as  ABCD, (6.58)  where the matrices can be thought of as Jacobian. For example, if D is a column vector while A has many rows, the graph will have a single output and many inputs, and starting the multiplications from the end and going backward requires only matrix-vector products. This order corresponds to the backward mode. Instead, starting to multiply from the left would involve a series of matrix-matrix products, which makes the whole computation much more expensive. If A has fewer rows than D has columns, however, it is cheaper to run the multiplications left-to-right, corresponding to the forward mode.\n\nIn many communities outside machine learning, it is more common to implement differentiation software that acts directly on traditional programming language code, such as Python or C code, and automatically generates programs that differentiate functions written in these languages. In the deep learning community, computational graphs are usually represented by explicit data structures created  https://www.deeplearningbook.org/contents/mlp.html    by specialized libraries", "7552f325-889b-4307-a2c7-11fc2d89d859": "Exercise 12.12 Show in detail the steps outlined above for deriving (12.29) from (12.27).\n\nStart with the update (12.15), substitute G\u03bba At \u03bb = 1, these algorithms become closely related to corresponding Monte Carlo algorithms. One might expect that an exact equivalence would hold for episodic problems and o\u270fine updating, but in fact the relationship is subtler and slightly weaker than that. Under these most favorable conditions still there is not an episode by episode equivalence of updates, only of their expectations. This should not be surprising as these methods make irrevocable updates as a trajectory unfolds, whereas true Monte Carlo methods would make no update for a trajectory if any action within it has zero probability under the target policy. In particular, all of these methods, even at \u03bb = 1, still bootstrap in the sense that their targets depend on the current value estimates\u2014it\u2019s just that the dependence cancels out in expected value. Whether this is a good or bad property in practice is another question. Recently, methods have been proposed that do achieve an exact equivalence", "1f4d39ca-ee04-4124-b9f4-84fc4c7fd3b5": "Here b is shaded to indicate that it is observed. Because observingb blocks the only path from a toc, we say that a and c are separated from each other given b. The observation of b also blocks one path between a and d, but there is a second, active path between them. Therefore, a and d are not separated given b.  path exists. In directed nets, determining whether a path is active is somewhat more complicated. See figure 16.8 for a guide to identifying active paths in a directed model. See figure 16.9 for an example of reading some properties from a graph. It is important to remember that separation and d-separation tell us only about those conditional independences that are implied by the graph. There is no requirement that the graph imply all independences that are present. In particular, it is always legitimate to use the complete graph (the graph with all possible edges) to represent any distribution. In fact, some distributions contain independences that are not possible to represent with existing graphical notation", "6e5c155e-cde0-4254-b33b-428627b0812b": "How can we perform ef\ufb01cient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially ef\ufb01cient by \ufb01tting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are re\ufb02ected in experimental results. How can we perform ef\ufb01cient approximate inference and learning with directed probabilistic models whose continuous latent variables and/or parameters have intractable posterior distributions? The variational Bayesian (VB) approach involves the optimization of an approximation to the intractable posterior. Unfortunately, the common mean-\ufb01eld approach requires analytical solutions of expectations w.r.t", "a8a63ef2-71e8-415a-a67f-d1e577a7a785": "it is envisaged that in a particular sensory situation neurone B, by chance, \ufb01res a \u2018meaningful burst\u2019 of activity, which is then translated into motor acts, which then change the situation.\n\nIt must be supposed that the meaningful burst has an in\ufb02uence, at the neuronal level, on all of its own synapses which are active at the time ... thereby making a preliminary selection of the synapses to be strengthened, though not yet actually strengthening them. ...The strengthening signal ... makes the \ufb01nal selection ... and accomplishes the de\ufb01nitive change in the appropriate synapses. (Miller, 1981, p. 81) Miller\u2019s hypothesis also included a critic-like mechanism, which he called a \u201csensory analyzer unit,\u201d that worked according to classical conditioning principles to provide reinforcement signals to neurons so that they would learn to move from lower- to higher-valued states, thus anticipating the use of the TD error as a reinforcement signal in the actor\u2013critic architecture", "752e8d9b-8f24-4cc9-b8b1-e3806ad0ab8a": "(xi,yi)ESc  V2  Vi  (a) Few-shot (b) Zero-shot  The distribution over classes for a given test input x is a softmax over the inverse of distances between the test data embedding and prototype vectors. exp(\u2014d,,(fo(x), V-))  Ply = c|x) =~ softmax(\u2014d,,(fo(x), v.)) \u2014 > ; cexp(\u2014d (fo(x), Vv '))  where d, can be any distance function as long as is differentiable. In the paper, they used the squared euclidean distance. The loss function is the negative log-likelihood: \u00a3(0) = \u2014 log Po(y = c|x).\n\nModel-Based  Model-based meta-learning models make no assumption on the form of P9(y|x). Rather it  https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Log   depends on a model designed specifically for fast learning \u2014 a model that updates its parameters rapidly with a few training steps", "cf97a6f6-09d2-413a-b65d-14cb9346faa4": "If we use q(x) to construct a coding scheme for the purpose of transmitting values of x to a receiver, then the average additional amount of information (in nats) required to specify the value of x (assuming we choose an ef\ufb01cient coding scheme) as a result of using q(x) instead of the true distribution p(x) is given by This is known as the relative entropy or Kullback-Leibler divergence, or KL divergence , between the distributions p(x) and q(x). Note that it is not a symmetrical quantity, that is to say KL(p\u2225q) \u0338\u2261 KL(q\u2225p). We now show that the Kullback-Leibler divergence satis\ufb01es KL(p\u2225q) \u2a7e 0 with equality if, and only if, p(x) = q(x). To do this we \ufb01rst introduce the concept of convex functions.\n\nA function f(x) is said to be convex if it has the property that every chord lies on or above the function, as shown in Figure 1.31", "d2f5770d-de45-48fb-88dd-0eac0803dcd9": "The latter may be valid states, but to be able to accurately value them is a di\u21b5erent skill from evaluating positions in real games. We will also see in Part II that the on-policy distribution has signi\ufb01cant advantages when function approximation is used. Whether or not function approximation is used, one might expect on-policy focusing to signi\ufb01cantly improve the speed of planning. Focusing on the on-policy distribution could be bene\ufb01cial because it causes vast, uninteresting parts of the space to be ignored, or it could be detrimental because it causes the same old parts of the space to be updated over and over. We conducted a small experiment to assess the e\u21b5ect empirically. To isolate the e\u21b5ect of the update distribution, we used entirely one-step expected tabular updates, as de\ufb01ned by (8.1).\n\nIn the uniform case, we cycled through all state\u2013action pairs, updating each in place, and in the on-policy case we simulated episodes, all starting in the same state, updating each state\u2013action pair that occurred under the current \"-greedy policy (\"=0.1). The tasks were undiscounted episodic tasks, generated randomly as follows", "cc50cc9d-dac2-480a-9009-63017c0ba870": "It is useful to distinguish between stationary and nonstationary sequential distributions. In the stationary case, the data evolves in time, but the distribution from which it is generated remains the same. For the more complex nonstationary situation, the generative distribution itself is evolving with time. Here we shall focus on the stationary case.\n\nFor many applications, such as \ufb01nancial forecasting, we wish to be able to predict the next value in a time series given observations of the previous values. Intuitively, we expect that recent observations are likely to be more informative than more historical observations in predicting future values. The example in Figure 13.1 shows that successive observations of the speech spectrum are indeed highly correlated. Furthermore, it would be impractical to consider a general dependence of future observations on all previous observations because the complexity of such a model would grow without limit as the number of observations increases. This leads us to consider Markov models in which we assume that future predictions are independent of all but the most recent observations. Although such models are tractable, they are also severely limited. We can obtain a more general framework, while still retaining tractability, by the introduction of latent variables, leading to state space models", "25d6e509-c9cf-4172-97da-1ac1e7ebd0cd": "who used GANs to make their simulated data as realistic as possible (Fig. 27). Using simulated data to build Computer Vision models has been heavily investigated. One example of this is from Richter et al. They use computer graphics from mod- ern open-world games such as Grand Theft Auto to produce semantic segmentation datasets.\n\nThe authors highlight anecdotes of the manual annotation costs required to build these pixel-level datasets. They mention the CamVid dataset  requires 60 min per image to manually annotate, and the Cityscapes dataset  requires 90 min per image. This high labor and time cost motivates the use and development of synthetic datasets. Neural Style Transfer is a very interesting strategy to improve the generaliza- tion ability of simulated datasets. A disadvantage of Neural Style Transfer Data Augmentation is the effort required to select styles to transfer images into. If the style set is too small, further biases could be introduced into the dataset. Trying to replicate the experiments of Tobin et al. will require a massive amount of additional memory and compute to transform and store 79,433 new images from each image", "b7b1b415-c5a9-48b2-84b8-35a5c98c1fd4": "Chapter 11  Practical Methodology  Successfully applying deep learning techniques requires more than just a good knowledge of what algorithms exist and the principles that explain how they work. A good machine learning practitioner also needs to know how to choose an algorithm for a particular application and how to monitor and respond to feedback obtained from experiments in order to improve a machine learning system. During day-to-day development of machine learning systems, practitioners need to decide whether to gather more data, increase or decrease model capacity, add or remove regularizing features, improve the optimization of a model, improve approximate inference in a model, or debug the software implementation of the model. All these operations are at the very least time consuming to try out, so it is important to be able to determine the right course of action rather than blindly guessing. Most of this book is about different machine learning models, training algo- rithms, and objective functions. This may give the impression that the most important ingredient to being a machine learning expert is knowing a wide variety of machine learning techniques and being good at different kinds of math. In prac- tice, one can usually do much better with a correct application of a commonplace algorithm than by sloppily applying an obscure algorithm", "7f0a93b3-9b6e-4880-86d6-4267b0adaa28": "An advantage of Fourier basis features in this regard is that it is easy to select features by setting the ci vectors to account for suspected interactions among the state variables and by limiting the values in the cj vectors so that the approximation can \ufb01lter out high frequency components considered to be noise.\n\nOn the other hand, because Fourier features are non-zero over the entire state space (with the few zeros excepted), they represent global properties of states, which can make it di\ufb03cult to \ufb01nd good ways to represent local properties. 2There are families of polynomials more complicated than those we have discussed, for example, di\u21b5erent families of orthogonal polynomials, and these might work better, but at present there is little experience with them in reinforcement learning. proximation, consider the e\u21b5ect of the size and density of the circles. Corresponding to each circle is a single weight (a component of w) that is a\u21b5ected by learning. If we train at one state, a point in the space, then the weights of all circles intersecting that state will be a\u21b5ected", "17112285-51d4-496d-b0f8-89bad861eb0c": "Policies learned in strong turbulence were more conservative in that they preferred small bank angles, whereas in weak turbulence, the best action was to turn as much as possible by banking sharply. Systematic study of the bank angles preferred by the policies learned under the di\u21b5erent conditions led the authors to suggest that by detecting when vertical wind acceleration crosses a certain threshold the controller can adjust its policy to cope with di\u21b5erent turbulence regimes. Reddy et al. also conducted experiments to investigate the e\u21b5ect of the discount-rate parameter \u03b3 on the performance of the learned policies. They found that the altitude gained in an episode increased as \u03b3 increased, reaching a maximum for \u03b3 = .99, suggesting that e\u21b5ective thermal soaring requires taking into account long-term e\u21b5ects of control decisions. This computational study of thermal soaring illustrates how reinforcement learning access to di\u21b5erent sets of environmental cues and control actions contributes to both the engineering objective of designing autonomous gliders and the scienti\ufb01c objective of improving understanding of the soaring skills of birds", "82f89bb0-341f-4a53-8df8-25478f520211": "Ratner, A., Bach, S.H., Ehrenberg, H.R., Fries, J.A., Wu, S., R\u00e9, C.: Snorkel: Rapid training data creation with weak supervision  CoRR, arXiv:1711.10160 43. Ratner, A., De Sa, C., Wu, S., Selsam, D., R\u00e9, C.: Data programming: Creating large training sets, quickly. In: Neural Information Processing Systems (NIPS)  44. Ratner, A., Hancock, B., Dunnmon, J., Goldman, R., R\u00e9, C.: Snorkel metal: Weak supervision for multi-task learning. In: Proceedings of the Second Workshop on Data Management for End-To-End Machine Learning, page 3. ACM  45. Ratner, A., Hancock, B., Dunnmon, J., Sala, F., Pandey, S., R\u00e9, C.: Training complex models with multi-task weak supervision. AAAI  46. Ratner, A., Hancock, B., R\u00e9, C.: The role of massively multi-task and weak supervision in software 2.0", "7e3f9e26-9c42-4de5-b567-edfd9864b3d9": "While data augmentation has been widely used in both supervised and unsupervised representation learning (Krizhevsky et al., 3Although max performance is not reached in 100 epochs, reasonable results are achieved, allowing fair and ef\ufb01cient ablations.\n\nA Simple Framework for Contrastive Learning of Visual Representations 2012; H\u00e9naff et al., 2019; Bachman et al., 2019), it has not been considered as a systematic way to de\ufb01ne the contrastive prediction task. Many existing approaches de\ufb01ne contrastive prediction tasks by changing the architecture. For example, Hjelm et al. ; Bachman et al. achieve global-to-local view prediction via constraining the receptive \ufb01eld in the network architecture, whereas Oord et al. ; H\u00e9naff et al. achieve neighboring view prediction via a \ufb01xed image splitting procedure and a context aggregation network. We show that this complexity can be avoided by performing simple random cropping (with resizing) of target images, which creates a family of predictive tasks subsuming the above mentioned two, as shown in Figure 3", "e5619336-cbbd-483e-bb86-970d4ffbc45a": "Journal of the Royal Statistical Society, B 58, 267\u2013288. Tikhonov, A. N. and V. Y. Arsenin . Solutions of Ill-Posed Problems. V. H. Winston. Tino, P. and I. T. Nabney .\n\nHierarchical GTM: constructing localized non-linear projection manifolds in a principled way. IEEE Transactions on Pattern Analysis and Machine Intelligence 24(5), 639\u2013656. Tino, P., I. T. Nabney, and Y. Sun . Using directional curvatures to visualize folding patterns of the GTM projection manifolds. In Tipping, M. E. Probabilistic visualisation of high-dimensional binary data. In M. S. Kearns, S. A. Solla, and D. A. Cohn (Eds", "b9b1f97f-12eb-4314-af90-a0b4f11b4586": "We can make use of the evidence framework, discussed in Section 3.5, together with the Gaussian approximation to the posterior obtained using the Laplace approximation, to obtain a practical procedure for choosing the values of such hyperparameters. The marginal likelihood, or evidence, for the hyperparameters is obtained by integrating over the network weights This is easily evaluated by making use of the Laplace approximation result (4.135). Exercise 5.39 where W is the total number of parameters in w, and the regularized error function is de\ufb01ned by We see that this takes the same form as the corresponding result (3.86) for the linear regression model. In the evidence framework, we make point estimates for \u03b1 and \u03b2 by maximizing ln p(D|\u03b1, \u03b2). Consider \ufb01rst the maximization with respect to \u03b1, which can be done by analogy with the linear regression case discussed in Section 3.5.2.\n\nWe \ufb01rst de\ufb01ne the eigenvalue equation \u03b2Hui = \u03bbiui (5.177) where H is the Hessian matrix comprising the second derivatives of the sum-ofsquares error function, evaluated at w = wMAP", "80ae028a-fe24-4cff-b227-60b658f5ba0b": "This is applied to image-to-image translation. Neural Style Transfer , dis- cussed further in the section below, learns a single image to single image translation. However, CycleGAN learns to translate from a domain of images to another domain, such as horses to zebras. This is implemented via forward and backward consistency loss functions.\n\nA generator takes in images of horses and learns to map them to zebras such that the discriminator cannot tell if they were originally a part of the zebra set or not, as discussed above. After this, the generated zebras from horse images are passed through a network which translates them back into horses. A second discriminator determines if this re-translated image belongs to the horse set or not. Both of these discriminator losses are aggregated to form the cycle-consistency loss. The use of CycleGANs was tested by Zhu et al. in the task of Emotion Clas- sification. Using the emotion recognition dataset, FER2013 , Facial Expression Recognition Database, they build a CNN classifier to recognize 7 different emotions: angry, disgust, fear, happy, sad, surprise, and neutral", "e02059c1-0618-44e8-bbd7-20d2f77fabea": "If x and y are compatible, the energy is a small number; if they are incompatible, the energy is a larger number. Training an EBM consists of two parts: (1) showing it examples of x and y that are compatible and training it to produce a low energy, and (2) finding a way to ensure that for a particular x, the y values that are incompatible with x produce a higher energy than the y values that are compatible with x. Part one is simple, but part two is where the difficulty lies. https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/  Self-supervised learning: The dark matter of intelligence   For image recognition, our model takes two images, x and y, as inputs. If x and y are slightly distorted versions of the same image, the model is trained to produce a low energy on its output", "e2a61d84-5fdd-4324-a05b-d0be4dd6313f": "In this chapter we discuss this hypothesis, the neuroscience \ufb01ndings that led to it, and why it is a signi\ufb01cant contribution to understanding brain reward systems. We also discuss parallels between reinforcement learning and neuroscience that are less striking than this dopamine/TD-error parallel but that provide useful conceptual tools for thinking about reward-based learning in animals. Other elements of reinforcement learning have the potential to impact the study of nervous systems, but their connections to neuroscience are still relatively undeveloped. We discuss several of these evolving connections that we think will grow in importance over time. As we outlined in the history section of this book\u2019s introductory chapter (Section 1.7), objective of this chapter is to acquaint readers with ideas about brain function that have contributed to our approach to reinforcement learning. Some elements of reinforcement learning are easier to understand when seen in light of theories of brain function.\n\nThis is particularly true for the idea of the eligibility trace, one of the basic mechanisms of reinforcement learning, that originated as a conjectured property of synapses, the structures by which nerve cells\u2014neurons\u2014communicate with one another. In this chapter we do not delve very deeply into the enormous complexity of the neural systems underlying reward-based learning in animals: this chapter is too short, and we are not neuroscientists", "e8074267-566e-4648-9b38-65b517b8acd1": "The origin of the problem is illustrated in Figure 1.21, which shows that, if we divide a region of a space into regular cells, then the number of such cells grows exponentially with the dimensionality of the space. The problem with an exponentially large number of cells is that we would need an exponentially large quantity of training data in order to ensure that the cells are not empty. Clearly, we have no hope of applying such a technique in a space of more than a few variables, and so we need to \ufb01nd a more sophisticated approach. We can gain further insight into the problems of high-dimensional spaces by returning to the example of polynomial curve \ufb01tting and considering how we would Section 1.1 extend this approach to deal with input spaces having several variables.\n\nIf we have D input variables, then a general polynomial with coef\ufb01cients up to order 3 would take the form As D increases, so the number of independent coef\ufb01cients (not all of the coef\ufb01cients are independent due to interchange symmetries amongst the x variables) grows proportionally to D3. In practice, to capture complex dependencies in the data, we may need to use a higher-order polynomial", "f1fe9b2f-e854-4790-b9d9-7a7d00cb8759": "Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NIPS, pages 2672\u20132680, 2014. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778, 2016. G. E. Hinton, P. Dayan, B. J. Frey, and R. M. Neal. The\" wake-sleep\" algorithm for unsupervised neural networks. Science, 268:1158, 1995. Z. Hu, Z. Yang, X. Liang, R. Salakhutdinov, and E. P. Xing. Toward controlled generation of text. In ICML, 2017. Z. Hu, Z", "e2b1bc76-b14b-44ea-ab33-fae0ebd2324f": "The Journal of Neuroscience, 25(26):6235\u20136242. Park, J., Kim, J., Kang, D. An RLS-based natural actor\u2013critic algorithm for locomotion of a two-linked robot arm. Computational Intelligence and Security:65\u201372. Parks, P. C., Militzer, J. Improved allocation of weights for associative memory storage Parr, R. Hierarchical Control and Learning for Markov Decision Processes. Ph.D. thesis, Parr, R., Li, L., Taylor, G., Painter-Wake\ufb01eld, C., Littman, M. L. An analysis of linear models, linear value-function approximation, and feature selection for reinforcement learning. In Proceedings of the 25th international conference on Machine learning, pp. 752\u2013759). Parr, R., Russell, S. Approximating optimal policies for partially observable stochastic Pavlov, P. I. Conditioned Re\ufb02exes. Oxford University Press, London", "60c7d44b-5751-438e-bcc1-8581f9d57e52": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models , BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be \ufb01netuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeci\ufb01c architecture modi\ufb01cations. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)", "3ecbbd50-470a-4dca-8d8b-b30046f1f269": "This can be verified directly by noting that the marginal density (12.35), and hence the likelihood function, is unchanged if we make the transformation W -) WR where R is an orthogonal matrix satisfying RRT = I, because the matrix C given by (12.36) is itself invariant. Extending the model to allow more general Gaussian latent distributions does not change this conclusion because, as we have seen, such a model is equivalent to the zero-mean isotropic Gaussian latent variable model. Another way to see why a Gaussian latent variable distribution in a linear model is insufficient to find independent components is to note that the principal components represent a rotation of the coordinate system in data space such as to diagonalize the covariance matrix, so that the data distribution in the new coordinates is then uncorrelated. Although zero correlation is a necessary condition for independence it is not, however, sufficient. In practice, a common choice for the latent-variable distribution is given by which has heavy tails compared to a Gaussian, reflecting the observation that many real-world distributions also exhibit this property.\n\nThe original ICA model  was based on the optimization of an objective function defined by information maximization", "120dc020-1c11-4e11-90cd-74ab9d2632d5": "The left-hand plot shows the error in the predicted posterior mean versus the number of \ufb02oating point operations, and the right-hand plot shows the corresponding results for the model evidence. We shall focus on the case in which the approximating distribution is fully factorized, and we shall show that in this case expectation propagation reduces to loopy belief propagation . To start with, we show this in the context of a simple example, and then we shall explore the general case. First of all, recall from (10.17) that if we minimize the Kullback-Leibler divergence KL(p\u2225q) with respect to a factorized distribution q, then the optimal solution for each factor is simply the corresponding marginal of p. Now consider the factor graph shown on the left in Figure 10.18, which was introduced earlier in the context of the sum-product algorithm", "d5407398-afa5-4f91-bcf0-673fc02f7168": "It corresponds to a two-class model in which the input vector x is \ufb01rst transformed using a \ufb01xed nonlinear transformation to give a feature vector \u03c6(x), and this is then used to construct a generalized linear model of the form where the nonlinear activation function f(\u00b7) is given by a step function of the form The vector \u03c6(x) will typically include a bias component \u03c60(x) = 1. In earlier discussions of two-class classi\ufb01cation problems, we have focussed on a target coding scheme in which t \u2208 {0, 1}, which is appropriate in the context of probabilistic models. For the perceptron, however, it is more convenient to use target values t = +1 for class C1 and t = \u22121 for class C2, which matches the choice of activation function.\n\nThe algorithm used to determine the parameters w of the perceptron can most easily be motivated by error function minimization. A natural choice of error function would be the total number of misclassi\ufb01ed patterns. However, this does not lead to a simple learning algorithm because the error is a piecewise constant function of w, with discontinuities wherever a change in w causes the decision boundary to move across one of the data points", "f05a2ed0-e7cb-4b62-9813-ce6b4803e9d6": "For example, in this book we have emphasized the deep similarities between dynamic programming and temporal-di\u21b5erence methods, even though one was designed for planning and the other for model-free learning. Dyna-Q includes all of the processes shown in the diagram above\u2014planning, acting, model-learning, and direct RL\u2014all occurring continually. The planning method is the random-sample one-step tabular Q-planning method on page 161. The direct RL method is one-step tabular Q-learning. The model-learning method is also table-based and assumes the environment is deterministic. After each transition St, At ! Rt+1, St+1, the model records in its table entry for St, At the prediction that Rt+1, St+1 will deterministically follow. Thus, if the model is queried with a state\u2013action pair that has been experienced before, it simply returns the last-observed next state and next reward as its prediction.\n\nDuring planning, the Q-planning algorithm randomly samples only from state\u2013action pairs that have previously been experienced (in Step 1), so the model is never queried with a pair about which it has no information", "6c7912be-d2be-48ab-9273-532b5055b371": "The coef\ufb01cient of this correction is given by the Kalman gain matrix. Thus we can view the Kalman \ufb01lter as a process of making successive predictions and then correcting these predictions in the light of the new observations. This is illustrated graphically in Figure 13.21. certainty in the state variable due to diffusion is compensated by the arrival of new data. In the left-hand plot, the blue curve shows the distribution p(zn\u22121|x1, . , xn\u22121), which incorporates all the data up to step n \u2212 1. The diffusion arising from the nonzero variance of the transition probability p(zn|zn\u22121) gives the distribution p(zn|x1, . , xn\u22121), shown in red in the centre plot. Note that this is broader and shifted relative to the blue curve (which is shown dashed in the centre plot for comparison). The next data observation xn contributes through the emission density p(xn|zn), which is shown as a function of zn in green on the right-hand plot. Note that this is not a density with respect to zn and so is not normalized to one", "cbc5e388-cc14-4cb7-b1f1-55c8d9473224": "In this example we empirically compare the prediction abilities of TD(0) and constant-\u21b5 MC when applied to the following Markov reward process: A Markov reward process, or MRP, is a Markov decision process without actions. We will often use MRPs when focusing on the prediction problem, in which there is no need to distinguish the dynamics due to the environment from those due to the agent. In this MRP, all episodes start in the center state, C, then proceed either left or right by one state on each step, with equal probability. Episodes terminate either on the extreme left or the extreme right. When an episode terminates on the right, a reward of +1 occurs; all other rewards are zero. For example, a typical episode might consist of the following state-and-reward sequence: C, 0, B, 0, C, 0, D, 0, E, 1. Because this task is undiscounted, the true value of each state is the probability of terminating on the right if starting from that state. Thus, the true value of the center state is v\u21e1(C) = 0.5", "a11e2e33-d450-4a15-a8e7-fc76ad7f7628": "The performance of a particular learning algorithm is then assessed by taking the average over this ensemble of data sets. Consider the integrand of the \ufb01rst term in (3.37), which for a particular data set D takes the form {y(x; D) \u2212 h(x)}2. (3.38) Because this quantity will be dependent on the particular data set D, we take its average over the ensemble of data sets. If we add and subtract the quantity ED inside the braces, and then expand, we obtain {y(x; D) \u2212 ED + ED \u2212 h(x)}2 We now take the expectation of this expression with respect to D and note that the \ufb01nal term will vanish, giving We see that the expected squared difference between y(x; D) and the regression function h(x) can be expressed as the sum of two terms. The \ufb01rst term, called the squared bias, represents the extent to which the average prediction over all data sets differs from the desired regression function. The second term, called the variance, measures the extent to which the solutions for individual data sets vary around their average, and hence this measures the extent to which the function y(x; D) is sensitive to the particular choice of data set", "9b0249e9-9177-49d3-8259-da3c44c0af21": "Because a skilled player can play so as never to lose, let us assume that we are playing against an imperfect player, one whose play is sometimes incorrect and allows us to win. For the moment, in fact, let us consider draws and losses to be equally bad for us. How might we construct a player that will \ufb01nd the imperfections in its opponent\u2019s play and learn to maximize its chances of winning? Although this is a simple problem, it cannot readily be solved in a satisfactory way through classical techniques. For example, the classical \u201cminimax\u201d solution from game theory is not correct here because it assumes a particular way of playing by the opponent.\n\nFor example, a minimax player would never reach a game state from which it could lose, even if in fact it always won from that state because of incorrect play by the opponent. Classical optimization methods for sequential decision problems, such as dynamic programming, can compute an optimal solution for any opponent, but require as input a complete speci\ufb01cation of that opponent, including the probabilities with which the opponent makes each move in each board state. Let us assume that this information is not available a priori for this problem, as it is not for the vast majority of problems of practical interest", "aa406ef4-f735-49ec-9f0d-1cc65fea36ba": "Two images are taken from the reference and target class and used to generate new data in the target class  Using CycleGANs to translate images from the other 7 classes into the minority classes was very effective in improving the performance of the CNN model on emo- tion recognition. Employing these techniquess, accuracy improved 5\u201410%. To further understand the effectiveness of adding GAN-generated instances, a t-SNE visualiza- tion is used. t-SNE  is a visualization technique that learns to map between high- dimensional vectors into a low-dimensional space to facilitate the visualization of decision boundaries (Fig. 21). Another interesting GAN architecture for use in Data Augmentation is Conditional GANSs . Conditional GANs add a conditional vector to both the generator and the discriminator in order to alleviate problems with mode collapse. In addition to inputting a random vector z to the generator, Conditional GANs also input a y vector which could be something like a one-hot encoded class label, e.g. This class label targets a  specific class for the generator and the discriminator (Fig. 22)", "9576daf4-ec52-4f2b-92fe-cc878ed4bd15": "More generally, it would also apply to any model de\ufb01ned by a directed probabilistic graph in which each factor is a conditional distribution corresponding to one of the nodes, or an undirected graph in which each factor is a clique potential. We are interested in evaluating the posterior distribution p(\u03b8|D) for the purpose of making predictions, as well as the model evidence p(D) for the purpose of model comparison. From (10.188) the posterior is given by Here we are considering continuous variables, but the following discussion applies equally to discrete variables with integrals replaced by summations. We shall suppose that the marginalization over \u03b8, along with the marginalizations with respect to the posterior distribution required to make predictions, are intractable so that some form of approximation is required.\n\nExpectation propagation is based on an approximation to the posterior distribution which is also given by a product of factors in which each factor \ufffdfi(\u03b8) in the approximation corresponds to one of the factors fi(\u03b8) in the true posterior (10.189), and the factor 1/Z is the normalizing constant needed to ensure that the left-hand side of (10.191) integrates to unity", "d26e2535-d6b8-41b2-bec6-c191356abfb8": "To begin with the simplest possible baseline, che first implementation of the output layer of the model consisted of n different softmax units to predict a sequence of n characters. These softmax units were rained exactly the same as if the task were classification, with each softmax unit rained independently. Our recommended methodology is to iteratively refine the baseline and test whether each change makes an improvement. The first change to the Street View  ranscription system was motivated by a theoretical understanding of the coverage metric and the structure of the data.\n\nSpecifically, the network refused to classify  tom 1 ay Voovstes eu 1 1 malas lo m\\ 4 4 6  https://www.deeplearningbook.org/contents/guidelines.html    all INpul & wieuever tue propavulty OL Lue OULpUL Sequeuce PLY | &) \\ \u00a9 Lor some threshold t. Initially, the definition of p(y ) was ad-hoc, based on simply multiplying all the softmax outputs together. This motivated the development of a specialized output layer and cost function that actually computed a principled  og-likelihood", "02ab5060-a70f-489f-b80e-bb4076f45f74": "Suppose that the current value of the parameter vector is \u03b8old. In the E step, the lower bound L(q, \u03b8old) is maximized with respect to q(Z) while holding \u03b8old \ufb01xed. The solution to this maximization problem is easily seen by noting that the value of ln p(X|\u03b8old) does not depend on q(Z) and so the largest value of L(q, \u03b8old) will occur when the Kullback-Leibler divergence vanishes, in other words when q(Z) is equal to the posterior distribution p(Z|X, \u03b8old). In this case, the lower bound will equal the log likelihood, as illustrated in Figure 9.12. In the subsequent M step, the distribution q(Z) is held \ufb01xed and the lower bound L(q, \u03b8) is maximized with respect to \u03b8 to give some new value \u03b8new.\n\nThis will cause the lower bound L to increase (unless it is already at a maximum), which will necessarily cause the corresponding log likelihood function to increase", "fec13890-d4d3-48b3-af90-2cddc0508bc2": "DEEP GENERATIVE MODELS  can then back-propagate through samples of z drawn from q(z | \u00a9) = q(z; f(a; 0)) to obtain a gradient with respect to 6. Learning then consists solely of maximizing \u00a3 with respect to the parameters of the encoder and decoder. All the expectations in \u00a3 may be approximated by Monte Carlo sampling. The variational autoencoder approach is elegant, theoretically pleasing, and simple to implement. It also obtains excellent results and is among the state-of-the- art approaches to generative modeling. Its main drawback is that samples from variational autoencoders trained on images tend to be somewhat blurry. The causes of this phenomenon are not yet known. One possibility is that the blurriness is an intrinsic effect of maximum likelihood, which minimizes Dxr(paatal|Pmodel)- AS illustrated in figure 3.6, this means that the model will assign high probability to points that occur in the training set but may also assign high probability to other points", "d397a042-2e12-4b52-9e30-1b0223c183fd": "nan nae tn dha 2 AAAI fee tne 2 Laren ttn Ate nn-e ANG  https://www.deeplearningbook.org/contents/inference.html    AIUIIUs Le LeaSUl 1S Liat LU SPCCLUIC LULICLIOL ACIUCVES MINiital CULLUOPYyY. AS functions place more probability density on the two points x = wt+oand x =p-o, and place less probability density on all other values of Z, they lose entropy while maintaining the desired variance.\n\nHowever, any function placing exactly zero mass  on all but two points does not integrate to one and is not a valid probability distribution. Thus there is no single minimal entropy probability distribution function, much as there is no single minimal positive real number. Instead, we can say that there is a sequence of probability distributions converging toward putting mass only on these two points. This degenerate scenario may be described as a mixture of Dirac distributions. Because Dirac distributions are not described by a single probability distribution function, no Dirac or mixture of Dirac distribution corresponds to a single specific point in function space", "7aa953d5-9206-4a73-b47d-c132c7e1cc0d": "@ = by 2.12 Ay .x = be 2.13 2.14 Amt = bm 2.15 or even more explicitly as Aj i2,+Ajo%9+---+ Ai ntn = by 2.16 33  CHAPTER 2. LINEAR ALGEBRA Ajit} + Ag 2r9 free Aantn = bo (2.17) (2.18) Amv t Am 2@2 forse AmnIn = bm. (2.19)  Matrix-vector product notation provides a more compact representation for equations of this form. 2.3 Identity and Inverse Matrices  Linear algebra offers a powerful tool called matrix inversion that enables us to analytically solve equation 2.11 for many values of A. To describe matrix inversion, we first need to define the concept of an identity matrix. An identity matrix is a matrix that does not change any vector when we multiply that vector by that matrix. We denote the identity matrix that preserves n-dimensional vectors as I,,. Formally, I, \u20ac R\"*\u201d\", and  Vee R\u201d\" I,x =a", "dfa32f21-006e-4aa2-acf1-f52a8117f416": "Completing the square in the usual way, we obtain Similarly, the optimal solution for the factor q(\u03b1) is obtained from ln q(\u03b1) = Ew  + ln p(\u03b1) + const.\n\nSubstituting for ln p(w|\u03b1) using (10.165), and for ln p(\u03b1) using (10.166), we obtain We recognize this as the log of a gamma distribution, and so we obtain We also need to optimize the variational parameters \u03ben, and this is also done by maximizing the lower bound \ufffdL(q, \u03be). Omitting terms that are independent of \u03be, and integrating over \u03b1, we have Note that this has precisely the same form as (10.159), and so we can again appeal to our earlier result (10.163), which can be obtained by direct optimization of the marginal likelihood function, leading to re-estimation equations of the form We have obtained re-estimation equations for the three quantities q(w), q(\u03b1), and \u03be, and so after making suitable initializations, we can cycle through these quantities, updating each in turn", "ce752b77-331d-4b18-b58f-9ebb52d5ad57": "One technique that is often used to control the over-\ufb01tting phenomenon in such cases is that of regularization, which involves adding a penalty term to the error function (1.2) in order to discourage the coef\ufb01cients from reaching large values. The simplest such penalty term takes the form of a sum of squares of all of the coef\ufb01cients, leading to a modi\ufb01ed error function of the form M, and the coef\ufb01cient \u03bb governs the relative importance of the regularization term compared with the sum-of-squares error term.\n\nNote that often the coef\ufb01cient w0 is omitted from the regularizer because its inclusion causes the results to depend on the choice of origin for the target variable , or it may be included but with its own regularization coef\ufb01cient (we shall discuss this topic in more detail in Section 5.5.1). Again, the error function in (1.4) can be minimized exactly in closed form. Techniques such as this are known Exercise 1.2 in the statistics literature as shrinkage methods because they reduce the value of the coef\ufb01cients. The particular case of a quadratic regularizer is called ridge regression . In the context of neural networks, this approach is known as weight decay. same data set as before but now using the regularized error function given by (1.4)", "55f84fe0-df5f-4eb8-ab29-a5364213c0ef": "Maximizing this evidence function with respect to \u03b1 again leads to the re-estimation equation given by (5.178). The use of the evidence procedure to determine \u03b1 is illustrated in Figure 5.22 for the synthetic two-dimensional data discussed in Appendix A. Finally, we need the predictive distribution, which is de\ufb01ned by (5.168). Again, this integration is intractable due to the nonlinearity of the network function. The simplest approximation is to assume that the posterior distribution is very narrow and hence make the approximation We can improve on this, however, by taking account of the variance of the posterior distribution. In this case, a linear approximation for the network outputs, as was used in the case of regression, would be inappropriate due to the logistic sigmoid outputunit activation function that constrains the output to lie in the range (0, 1). Instead, we make a linear approximation for the output unit activation in the form where aMAP(x) = a(x, wMAP), and the vector b \u2261 \u2207a(x, wMAP) can be found by backpropagation", "529d4735-2cac-4837-9b05-a2e329cc02df": "After 1000 iterations we are still far from an optimal solution, as we can see from the VE, which remains almost 2.\n\nThe system is actually converging to an optimal solution, but progress is extremely slow because the PBE is already so close to zero. GTD2 and TDC both involve two learning processes, a primary one for w and a secondary one for v. The logic of the primary learning process relies on the secondary learning process having \ufb01nished, at least approximately, whereas the secondary learning process proceeds without being in\ufb02uenced by the \ufb01rst. We call this sort of asymmetrical dependence a cascade. In cascades we often assume that the secondary learning process is proceeding faster and thus is always at its asymptotic value, ready and accurate to assist the primary learning process. The convergence proofs for these methods often make this assumption explicitly. These are called two-time-scale proofs. The fast time scale is that of the secondary learning process, and the slower time scale is that of the primary learning process", "dcd5e3ee-ac8d-4683-8d51-410f400b5a6d": "12.23 (*) III!I Draw a directed probabilistic graphical model representing a discrete mixture of probabilistic PCA models in which each PCA model has its own values of W, JL, and 0-2\u2022 Now draw a modified graph in which these parameter values are shared between the components of the mixture. 12.24 (***) We saw in Section 2.3.7 that Student's t-distribution can be viewed as an infinite mixture of Gaussians in which we marginalize with respect to a continuous latent variable. By exploiting this representation, formulate an EM algorithm for maximizing the log likelihood function for a multivariate Student's t-distribution given an observed set of data points, and derive the forms of the E and M step equations. 12.25 (**)III!I Consider a linear-Gaussian latent-variable model having a latent space distribution p(z) = N(xIO, I) and a conditional distribution for the observed variable p(xlz) = N(xlWz + IL, <p) where <P is an arbitrary symmetric, positivedefinite noise covariance matrix", "093a9ddc-8694-48b6-9a74-892877cf3514": "Shorten and Khoshgoftaar J Big Data  https://doi.org/10.1186/s40537-019-0197-0   6:60  \u00a9 Journal of Big Data  SURVEY PAPER Open Access  A survey on  , \u00ae Image Data Augmentation pa  for Deep Learning  Connor Shorten*\u00ae and Taghi M. Khoshgoftaar  *Correspondence: cshorten2015@fau.edu Department of Computer and Electrical Engineering and Computer Science, Florida Atlantic University, Boca Raton, USA  g) Springer Open  Abstract  Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfor- tunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them", "d3bcdfa3-b4fe-4875-ba70-6165d19dffbd": "In the supervised and semisupervised setting, hidden level augmentations work well, with cutoff performing the best. We analyze several data augmentation methods and check whether the label is preserved for these and if this affects its performance. We look at 25 examples for the best performing data augmentation method and the worst performing data augmentation method for 20 News Group and RTE. For 20 News Group, Random Deletion was the best performing, and Language Model was the worst performing. In both cases, there were no examples where the label \ufb02ipped, which makes sense since the input is usually several paragraphs with multiple references to the topic. Several examples are shown in Appendix. For RTE, Language Model was the worst performing and Cutoff was the best performing augmentation. Language Model \ufb02ipped 24% of the labels with 4% uncertain, while Cutoff \ufb02ipped 4% of the labels with 12% uncertain. We show several examples of when the label \ufb02ipped for RTE in the Table 6.", "73c298c9-7f39-4208-9902-58f2c32ae162": "Suppose \u03bb is 3 and 4 for rental requests at the \ufb01rst and second locations and 3 and 2 for returns. To simplify the problem slightly, we assume that there can be no more than 20 cars at each location (any additional cars are returned to the nationwide company, and thus disappear from the problem) and a maximum of \ufb01ve cars can be moved from one location to the other in one night. We take the discount rate to be \u03b3 = 0.9 and formulate this as a continuing \ufb01nite MDP, where the time steps are days, the state is the number of cars at each location at the end of the day, and the actions are the net numbers of cars moved between the two locations overnight. Figure 4.2 shows the sequence of policies found by policy iteration starting from the policy that never moves any cars. Policy iteration often converges in surprisingly few iterations, as the example of Jack\u2019s car rental illustrates, and as is also illustrated by the example in Figure 4.1.\n\nThe bottomleft diagram of Figure 4.1 shows the value function for the equiprobable random policy, and the bottom-right diagram shows a greedy policy for this value function. The policy improvement theorem assures us that these policies are better than the original random policy", "2541654e-16b9-4360-8558-01e7dbe16ac9": "They are different from both things, and that is all we know. These issues illustrate some of the reasons that we may prefer a distributed representation to a one-hot representation. A distributed representation could have two attributes for each vehicle\u2014one representing its color and one representing  https://www.deeplearningbook.org/contents/ml.html    whether it is a car or a truck. It is still not entirely clear what the optimal distributed representation is (how can the learning algorithm know whether the two attributes we are interested in are color and car-versus-truck rather than  manufacturer and age? ), but having many attributes reduces the burden on the algorithm to guess which single attribute we care about, and gives us the ability to measure similarity between objects in a fine-grained way by comparing many  148  CHAPTER 5. MACHINE LEARNING BASICS  attributes instead of just testing whether one attribute matches. 5.9 Stochastic Gradient Descent  Nearly all of deep learning is powered by one very important algorithm: stochastic gradient descent (SGD). Stochastic gradient descent is an extension of the gradient descent algorithm introduced in section 4.3", "c4e4a599-aec4-4d2d-babe-b009f97ecf35": "In Section 4.1.7, we described the perceptron algorithm that is guaranteed to \ufb01nd a solution in a \ufb01nite number of steps.\n\nThe solution that it \ufb01nds, however, will be dependent on the (arbitrary) initial values chosen for w and b as well as on the order in which the data points are presented. If there are multiple solutions all of which classify the training data set exactly, then we should try to \ufb01nd the one that will give the smallest generalization error. The support vector machine approaches this problem through the concept of the margin, which is de\ufb01ned to be the smallest distance between the decision boundary and any of the samples, as illustrated in Figure 7.1. In support vector machines the decision boundary is chosen to be the one for which the margin is maximized. The maximum margin solution can be motivated using computational learning theory, also known as statistical learning theory. HowSection 7.1.5 ever, a simple insight into the origins of maximum margin has been given by Tong and Koller  who consider a framework for classi\ufb01cation based on a hybrid of generative and discriminative approaches. They \ufb01rst model the distribution over input vectors x for each class using a Parzen density estimator with Gaussian kernels of the data points, as shown on the left \ufb01gure", "b9175301-1ecb-411d-a11a-fb03097e9719": "GANs applied to the MNIST data are able to produce convincing results.\n\nHowever, MNIST images are far less challenging than other image datasets due to low intra-class vari- ance and resolution, to name a couple differences of many. This is in heavy contrast with other datasets studied in most academic Computer Vision papers such as Ima- geNet or CIFAR-10. For immediate reference, an ImageNet image is of resolution 256 x 256 x 3, totaling 196,608 pixels, a 250x increase in pixel count compared with MNIST. Many research papers have been published that modify the GAN framework through different network architectures, loss functions, evolutionary methods, and many more. This research has significantly improved the quality of samples created by GANs. There have been many new architectures proposed for expanding on the con- cept of GANs and producing higher resolution output images, many of which are out of the scope of this paper. Amongst these new architectures, DCGANs, Progressively Growing GANs, CycleGANs, and Conditional GANs seem to have the most applica- tion potential in Data Augmentation", "9db0c784-c531-41c4-94b3-34f756319cf7": "We should not overlook the most obvious way in which heuristic search focuses updates: on the current state. Much of the e\u21b5ectiveness of heuristic search is due to its search tree being tightly focused on the states and actions that might immediately follow the current state. You may spend more of your life playing chess than checkers, but when you play checkers, it pays to think about checkers and about your particular checkers position, your likely next moves, and successor positions.\n\nNo matter how you select actions, it is these states and actions that are of highest priority for updates and where you most urgently want your approximate value function to be accurate. Not only should your computation be preferentially devoted to imminent events, but so should your limited memory resources. In chess, for example, there are far too many possible positions to store distinct value estimates for each of them, but chess programs based on heuristic search can easily store distinct estimates for the millions of positions they encounter 2There are interesting exceptions to this . looking ahead from a single position. This great focusing of memory and computational resources on the current decision is presumably the reason why heuristic search can be so e\u21b5ective", "2f16bc49-33cf-4bf2-bc9d-f6643882d531": "Instead, some models are most easily described using a directed graph, or most easily described using an undirected graph. 570  CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  OOO O-O-O P ODO  (c) (d)  Figure 16.8: All the kinds of active paths of length two that can exist between random variables a and b. (a) Any path with arrows proceeding directly froma to b or vice versa. This kind of path becomes blocked if s is observed. We have already seen this kind of path in the relay race example. (b) Variables a and b are connected by a common causes.\n\nFor example, suppose s is a variable indicating whether or not there is a hurricane, anda and b measure the wind speed at two different nearby weather monitoring outposts. If we observe very high winds at station a, we might expect to also see high winds at b. This kind of path can be blocked by observing s. If we already know there is a hurricane, we expect to see high winds at b, regardless of what is observed at a", "e42d310f-00bd-4963-8163-c095f7c63874": "It is therefore necessary to introduce some form of approximation.\n\nLater in the book we shall consider a range of techniques based on analytical approximations Chapter 10 Here we introduce a simple, but widely used, framework called the Laplace approximation, that aims to \ufb01nd a Gaussian approximation to a probability density de\ufb01ned over a set of continuous variables. Consider \ufb01rst the case of a single continuous variable z, and suppose the distribution p(z) is de\ufb01ned by where Z = \ufffd f(z) dz is the normalization coef\ufb01cient. We shall suppose that the value of Z is unknown. In the Laplace method the goal is to \ufb01nd a Gaussian approximation q(z) which is centred on a mode of the distribution p(z). The \ufb01rst step is to \ufb01nd a mode of p(z), in other words a point z0 such that p\u2032(z0) = 0, or equivalently A Gaussian distribution has the property that its logarithm is a quadratic function of the variables", "daa798ad-ef50-4bcf-bcb2-b915c077f394": "Aviral Kumar, Justin Fu, George Tucker, and Sergey Levine. 2019. Stabilizing off-policy q-learning via bootstrapping error reduction. In NeurIPS. Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-ef\ufb01cient prompt tuning. arXiv preprint arXiv:2104.08691. Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, and Jianfeng Gao. 2016. Deep reinforcement learning for dialogue generation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1192\u2013 1202. Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. 2020. CommonGen: A constrained text generation challenge for generative commonsense reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1823\u20131840, Online", "2faf349f-5db7-42a9-80d7-aba664666d13": "So far we have given a rather heuristic motivation for the de\ufb01nition of information (1.92) and the corresponding entropy (1.93). We now show that these de\ufb01nitions indeed possess useful properties. Consider a random variable x having 8 possible states, each of which is equally likely. In order to communicate the value of x to a receiver, we would need to transmit a message of length 3 bits. Notice that the entropy of this variable is given by Now consider an example  of a variable having 8 possible states {a, b, c, d, e, f, g, h} for which the respective probabilities are given by ( 1 64). The entropy in this case is given by We see that the nonuniform distribution has a smaller entropy than the uniform one, and we shall gain some insight into this shortly when we discuss the interpretation of entropy in terms of disorder. For the moment, let us consider how we would transmit the identity of the variable\u2019s state to a receiver. We could do this, as before, using a 3-bit number", "13d77923-8a56-4176-9e0e-66598fc4e2ec": "In Figure 2.25, we apply the model (2.250) to the data smoothing parameter and that if it is set too small (top panel), the result is a very noisy density model, whereas if it is set too large (bottom panel), then the bimodal nature of the underlying distribution from which the data is generated (shown by the green curve) is washed out. The best density model is obtained for some intermediate value of h (middle panel). set used earlier to demonstrate the histogram technique. We see that, as expected, the parameter h plays the role of a smoothing parameter, and there is a trade-off between sensitivity to noise at small h and over-smoothing at large h. Again, the optimization of h is a problem in model complexity, analogous to the choice of bin width in histogram density estimation, or the degree of the polynomial used in curve \ufb01tting. We can choose any other kernel function k(u) in (2.249) subject to the condiwhich ensure that the resulting probability distribution is nonnegative everywhere and integrates to one.\n\nThe class of density model given by (2.249) is called a kernel density estimator, or Parzen estimator", "7d82345e-6f0b-43f2-962b-9b2044473ff0": "C. Hardie, K. J. Barnard, and E. E. Armstrong. Joint MAP registration and high-resolution image estimation using a sequence of undersampled images. IEEE Transactions on Image Processing, 6(12):1621\u20131633, December 1997. Geo\ufb00rey McLachlan and Thriyambakam Krishnan. The EM Algorithm and Extensions. John Wiley & Sons, New York, 1996. Geo\ufb00rey McLachlan and David Peel. Finite Mixture Models. John Wiley & Sons, New York, 2000. Yair Weiss. Bayesian motion estimation and segmentation. PhD thesis, Massachusetts Institute of Technology, May 1998.", "a30cc8c7-98ed-455b-b01f-817896475105": "From the de\ufb01nition of ev\u21e4 we know that it is the unique solution to However, this equation is the same as the previous one, except for the substitution of v\u21e1 for ev\u21e4. Because ev\u21e4 is the unique solution, it must be that v\u21e1 = ev\u21e4. In essence, we have shown in the last few pages that policy iteration works for \"-soft policies. Using the natural notion of greedy policy for \"-soft policies, one is assured of improvement on every step, except when the best policy has been found among the \"-soft policies. This analysis is independent of how the action-value functions are determined at each stage, but it does assume that they are computed exactly. This brings us to roughly the same point as in the previous section.\n\nNow we only achieve the best policy among the \"-soft policies, but on the other hand, we have eliminated the assumption of exploring starts. All learning control methods face a dilemma: They seek to learn action values conditional on subsequent optimal behavior, but they need to behave non-optimally in order to explore all actions (to \ufb01nd the optimal actions)", "26e8ddcc-0182-4cc8-971e-4fa7dd0f9c22": "Consider the illustration of a convex function f(x) shown in the left-hand plot in Figure 10.11. In this example, the function \u03bbx is a lower bound on f(x) but it is not the best lower bound that can be achieved by a linear function having slope \u03bb, because the tightest bound is given by the tangent line. Let us write the equation of the tangent line, having slope \u03bb as \u03bbx \u2212 g(\u03bb) where the (negative) intercept g(\u03bb) clearly depends on the slope \u03bb of the tangent. To determine the intercept, we note that the line must be moved vertically by an amount equal to the smallest vertical distance between the line and the function, as shown in Figure 10.11. Thus Now, instead of \ufb01xing \u03bb and varying x, we can consider a particular x and then adjust \u03bb until the tangent plane is tangent at that particular x. Because the y value of the tangent line at a particular x is maximized when that value coincides with its contact point, we have f(x) = max \u03bb {\u03bbx \u2212 g(\u03bb)}", "a7ec12bf-d02f-434f-bb11-4eed64deb124": "If one is interested in only the value at one point, or any \ufb01xed small set of points, then this Monte Carlo method can be far more e\ufb03cient than the iterative method based on local consistency. If a model is not available, then it is particularly useful to estimate action values (the values of state\u2013action pairs) rather than state values. With a model, state values alone are su\ufb03cient to determine a policy; one simply looks ahead one step and chooses whichever action leads to the best combination of reward and next state, as we did in the chapter on DP. Without a model, however, state values alone are not su\ufb03cient. One must explicitly estimate the value of each action in order for the values to be useful in suggesting a policy. Thus, one of our primary goals for Monte Carlo methods is to estimate q\u21e4. To achieve this, we \ufb01rst consider the policy evaluation problem for action values. The policy evaluation problem for action values is to estimate q\u21e1(s, a), the expected return when starting in state s, taking action a, and thereafter following policy \u21e1", "ea18efc3-619f-42a8-b236-fd9b58d4aa7b": "The Atari 2600 is a home video game console that was sold in various versions by Atari Inc. from 1977 to 1992. It introduced or popularized many arcade video games that are now considered classics, such as Pong, Breakout, Space Invaders, and Asteroids. Although much simpler than modern video games, Atari 2600 games are still entertaining and challenging for human players, and they have been attractive as testbeds for developing and evaluating reinforcement learning methods .\n\nBellemare, Naddaf, Veness, and Bowling  developed the publicly available Arcade Learning Environment (ALE) to encourage and simplify using Atari 2600 games to study learning and planning algorithms. These previous studies and the availability of ALE made the Atari 2600 game collection a good choice for Mnih et al.\u2019s demonstration, which was also in\ufb02uenced by the impressive human-level performance that TD-Gammon was able to achieve in backgammon. DQN is similar to TD-Gammon in using a multi-layer ANN as the function approximation method for a semi-gradient form of a TD algorithm, with the gradients computed by the backpropagation algorithm", "d33a29e6-56e5-482c-9d10-143970fd63bc": "For example, paraphrasing works well for general text classi\ufb01cation tasks, but may fail for some subtle scenarios like classifying bias because paraphrasing might change the label in this setting. Automatically learning data augmentation strategies or searching for an optimal augmentation policy for given datasets/tasks/models could enhance the generalizability of data augmentation techniques . We would like to thank the members of Georgia Tech SALT and UNC-NLP groups for their feedback. This work is supported by grants from Amazon and Salesforce, ONR Grant N00014-181-2871, DARPA YFA17-D17AP00022.\n\nAlberto Abad, Peter Bell, Andrea Carmantini, and Steve Renais. 2020. Cross lingual transfer learning for zero-resource domain adaptation. ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Ateret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich, Amir Kantor, George Kour, Segev Shlomov, Naama Tepper, and Naama Zwerdling. 2020", "68596345-774d-40fa-b86f-566d1a39f39f": "On the other hand, the TD error (when combined with eligibility traces) tells the critic the direction and magnitude in which to change the parameters of the value function in order to improve its predictive accuracy. The critic works to reduce \u03b4\u2019s magnitude to be as close to zero as possible using a learning rule like the TD model of classical conditioning (Section 14.2). The di\u21b5erence between the critic and actor learning rules is relatively simple, but this di\u21b5erence has a profound e\u21b5ect on learning and is essential to how the actor\u2013critic algorithm works.\n\nThe di\u21b5erence lies solely in the eligibility traces each type of learning rule uses. More than one set of learning rules can be used in actor\u2013critic neural networks like those in Figure 15.5b but, to be speci\ufb01c, here we focus on the actor\u2013critic algorithm for continuing problems with eligibility traces presented in Section 13.6", "53f3977d-7c37-4bad-a30d-7519a1519aef": "The use of a distributed representation combined with a linear classifier thus expresses a prior belief that the classes to be recognized are linearly separable as a function of the underlying causal factors captured by h. We will typically want to learn categories such as the set of all images of all green objects or the set of all images of cars, but not categories that require nonlinear XOR logic. For example, we typically do not want to partition the data into the set of all red cars and green trucks as one class and the set of all green cars and red trucks as another class. The ideas discussed so far have been abstract, but they may be experimentally validated. Zhou et al.\n\nfound that hidden units in a deep convolutional network trained on the ImageNet and Places benchmark datasets learn features that are often interpretable, corresponding to a label that humans would naturally assign. In practice it is certainly not always the case that hidden units learn something that has a simple linguistic name, but it is interesting to see this emerge near the top levels of the best computer vision deep networks. What such features have in  common is that one could imagine learning about each of them without having to see all the configurations of all the others. Radford et al", "86810733-ae01-4fb8-800e-5d0f76ae57ce": "Zhu, J., Chen, N., & Xing, E. P. Bayesian inference with posterior regularization and applications to in\ufb01nite latent svms. The Journal of Machine Learning Research, 15(1), 1799\u20131847. Zhu, J., & Xing, E. P. Maximum entropy discrimination markov networks. Journal of Machine Learning Research, 10(11). Zhu, Y., Gao, T., Fan, L., Huang, S., Edmonds, M., Liu, H., Gao, F., Zhang, C., Qi, S., Wu, Y. N., et al. .\n\nDark, beyond deep: A paradigm shift to cognitive AI with humanlike common sense. Engineering, 6(3), 310\u2013345. Ziebart, B. D., Maas, A. L., Bagnell, J. A., & Dey, A. K. Maximum entropy inverse reinforcement learning", "a7f942cb-7503-4abd-8966-87c1402d720d": "We use a learning rate of 2e\u22125, batch size of 16, ratio of unlabeled to labeled data of 3, and dropout ratio of 0.1 for different augmentation methods. News/Topic Classi\ufb01cation Tasks The results are shown in Table 4. We observe that overall, in both the supervised settings and semi-supervised setting, all the methods perofrmly similarly, with 2 points of each other. This indicates that data augmentation methods work well with limited labeled data, and with more labeled data, its effectiveness is removed. Inference Tasks As shown in Table 5, we observe that most augmentation methods hurt the performance in both the supervised and semisupervised setting, with a greater drop in performance in the semi-supervised setting. Similarity and Paraphrase Tasks Similar to inference tasks, we observe in Table 5 that most augmentation methods hurt the performance in both the supervised and semi-supervised setting, with a greater drop in performance in the semisupervised setting. Single Sentence Tasks Unlike inference tasks and paraphrase tasks, augmentations methods help performance, as seen in Table 5, except for CoLA.\n\nWe hypothesize the reason is because most augmentatiom methods seek to preserves meaning, not grammatical correctness, which is what CoLA measures", "cb90f542-deab-48b9-bcf9-9bb5526bf7a9": "The \ufb01rst step embodies a \u2018teacher\u2019s update\u2019 where the teacher q ingests experience f and builds on current states of the student p\u03b8(n); the second step is reminiscent of a \u2018student\u2019s update\u2019 where the student p\u03b8 updates its states by maximizing its alignment (here measured by CE) with the teacher. Besides, the auxiliary q is an easy-to-manipulate intermediate form in the training that permits rich approximate inference tools for tractable optimization.\n\nWe have the \ufb02exibility of choosing its surrogate functions, ranging from the principled variational approximations for the target distribution in a properly relaxed space (e.g., mean \ufb01elds) where gaps and bounds can be characterized, to the arbitrary neural network-based \u2018inference networks\u2019 that are highly expressive and easy to compute. As can be easily shown (e.g., see Section 4.1.3), popular training heuristics, such as EM, variational EM, wake-sleep, forward and backward propagation, and so on, are all direct instantiations or variants of the above teacher-student mechanism with di\ufb00erent choices of the form of q", "4134dae3-ac6e-4977-ae63-c3b1efa78948": "Q-learning (o\u21b5-policy TD control) for estimating \u21e1 \u21e1 \u21e1\u21e4 Algorithm parameters: step size \u21b5 2 (0, 1], small \" > 0 Initialize Q(s, a), for all s 2 S+, a 2 A(s), arbitrarily except that Q(terminal, \u00b7) = 0 What is the backup diagram for Q-learning? The rule (6.8) updates a state\u2013action pair, so the top node, the root of the update, must be a small, \ufb01lled action node. The update is also from action nodes, maximizing over all those actions possible in the next state.\n\nThus the bottom nodes of the backup diagram should be all these action nodes. Finally, remember that we indicate taking the maximum of these \u201cnext action\u201d nodes with an arc across them (Figure 3.4-right). Can you guess now what the diagram is? If so, please do make a guess before turning to the answer in Figure 6.4 on page 134. highlighting the di\u21b5erence between on-policy (Sarsa) and o\u21b5-policy (Q-learning) methods", "b0831043-c722-44d7-bd8d-aa196b840758": "In Association for Computational Linguistics Findings. Daniel Furrer, Marc van Zee, Nathan Scales, and Nathanael Sch\u00a8arli. 2020. Compositional generalization in semantic parsing: Pre-training vs. specialized architectures. Fei Gao, Jinhua Zhu, Lijun Wu, Yingce Xia, Tao Qin, Xueqi Cheng, Wengang Zhou, and Tie-Yan Liu. 2019. Soft contextual data augmentation for neural machine translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5539\u20135544. Siddhant Garg and Goutham Ramakrishnan. 2020. BAE: BERT-based adversarial examples for text classi\ufb01cation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6174\u20136181, Online. Association for Computational Linguistics. Dan Garrette and Jason Baldridge. 2013", "760eb06f-ca0b-4f93-9a29-0bf58144b07e": "Generating sentences from a continuous space. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, pages 10\u201321, Berlin, Germany. Association for Computational Linguistics. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel HerbertVoss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.\n\nLanguage models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc", "8647917c-689d-41e6-a51b-4505fb4cf9ff": "van Seijen, H., Whiteson, S., van Hasselt, H., Wiering, M. Exploiting best-match Varga, R. S. .\n\nMatrix Iterative Analysis. Englewood Cli\u21b5s, NJ: Prentice-Hall. Vasilaki, E., Fr\u00b4emaux, N., Urbanczik, R., Senn, W., Gerstner, W. Spike-based reinforcement learning in continuous state and action space: when policy gradient methods fail. PLoS Computational Biology, 5(12). Viswanathan, R., Narendra, K. S. Games of stochastic automata. IEEE Transactions Wagner, A. R. Evolution of an elemental theory of Pavlovian conditioning. Learning & Walter, W. G. An imitation of life. Scienti\ufb01c American, 182(5):42\u201345. Walter, W. G. A machine that learns. Scienti\ufb01c American, 185(2):60\u201363", "bec1a1b5-4ab8-4f52-a3bc-d37eab546d19": "We can continue to add more powers of x as additional features, for example, to obtain a polynomial of degree 9:  Ga=b+  uyat. (5.17) i=l  Machine learning algorithms will eendzally perform best when their capacity is appropriate for the true complexity of the task they need to perform and the amount of training data they are provided with. Models with insufficient capacity  https://www.deeplearningbook.org/contents/ml.html    are unable to solve complex tasks. Models with high capacit can solve complex tasks, but when their capacity is higher than needed to solve the present task, they may overfit. Figure 5.2 shows this principle in action. We compare a linear, quadratic and degree-9 predictor attempting to fit a problem where the true underlying  110  CHAPTER 5. MACHINE LEARNING BASICS  Underfitting Appropriate capacity Overfitting  X% v9 T%  Figure 5.2: We fit three models to this example training set. The training data was generated synthetically, by randomly sampling x values and choosing y deterministically by evaluating a quadratic function", "5a11cd99-79a5-47f9-822f-e41fb3b045c4": "Value Estimation  The key idea in TD learning is to update the value function V (.S;) towards an estimated return Ris + YV (S11) (known as \u201cTD target\"). To what extent we want to update the value function is controlled by the learning rate hyperparameter a:  V(S;) \u2014 (1\u2014a)V(S;) +aG;  V(St) \u2014 V(S:) + a(G: \u2014 V(S;)) V(St) \u2014 V(St) + a(Rea + WV (Sts1) \u2014 VOS%))  Similarly, for action-value estimation: Q(S1, Ar) \u2014 Q(S;, At) + (Rez + YQ(St41, Atzi) \u2014 Q(St, At))  Next, let's dig into the fun part on how to learn optimal policy in TD learning (aka \u201cTD control\u201d).\n\nBe prepared, you are gonna see many famous names of classic algorithms in this section. SARSA: On-Policy TD control  \u201cSARSA' refers to the procedure of updaing Q-value by following a sequence of ++) S4, At, Rizr, Sti1, Atyi,-.-", "be8fd581-fae8-41a4-8695-59d9cfc03135": ", mK, conditioned on the parameters \u00b5 and on the total number N of observations. From (2.29) this takes the form Note that the variables mk are subject to the constraint We now introduce a family of prior distributions for the parameters {\u00b5k} of the multinomial distribution (2.34). By inspection of the form of the multinomial distribution, we see that the conjugate prior is given by k \u00b5k = 1. Here \u03b11, . , \u03b1K are the parameters of the distribution, and \u03b1 denotes (\u03b11, . , \u03b1K)T. Note that, because of the summation constraint, the distribution over the space of the {\u00b5k} is con\ufb01ned to a simplex of dimensionality K \u2212 1, as illustrated for K = 3 in Figure 2.4. The normalized form for this distribution is by Exercise 2.9 which is called the Dirichlet distribution", "63b4f0d8-0443-4523-a7f1-15b8da42ba9e": "Forester (Ed. ), Cybernetics. Transactions of the Eighth Conference, pp. 173\u2013180. Josiah Macy Jr. Foundation. Shannon, C. E. \u201cTheseus\u201d maze-solving mouse. http://cyberneticzoo.com/mazesolvers/1952-theseus-maze-solving-mouse--claude-shannon-american/. Shelton, C. R. Importance Sampling for Reinforcement Learning with Multiple Objectives. Ph.D. thesis, Massachusetts Institute of Technology, Cambridge MA. Shepard, D. A two-dimensional interpolation function for irregularly-spaced data. In Proceedings of the 23rd ACM National Conference, pp. 517\u2013524. ACM, New York. Sherman, J., Morrison, W. J", "8f096430-e7d2-45ef-8439-f9862e27a646": "Regularization of an estimator works by trading increased bias for reduced variance. An effective regularizer is one that makes a profitable rade, reducing variance significantly while not overly increasing the bias.\n\nWhen we discussed generalization and overfitting in chapter 5, we focused on three situations, where the model family being trained either (1) excluded the true data-generating process\u2014corresponding to underfitting and inducing bias, or (2) matched the true data-generating process, or (3) included the generating process but also many other possible generating processes\u2014the overfitting regime where variance rather han bias dominates the estimation error. The goal of regularization is to take a model from the third regime into the second regime. In practice, an overly complex model family does not necessarily include the arget function or the true data-generating process, or even a close approximation  of either. We almost never have access to the true data-generating process so we can never know for sure if the model family being estimated includes the  generating process or not. Most applications of deep learning algorithms, however, are to domains where the true data-generating process is almost certainly outside the model family", "e8be68f3-b324-4888-bf0b-636ec45acc1f": "Indeed, experience with engineering applications has highlighted the importance of building into reinforcement learning systems knowledge that is analogous to what evolution provides to animals. Ludvig, Bellemare, and Pearson  and Shah  review reinforcement learning in the contexts of psychology and neuroscience.\n\nThese publications are useful companions to this chapter and the following chapter on reinforcement learning and neuroscience. 14.1 Dayan, Niv, Seymour, and Daw  focused on interactions between classical and instrumental conditioning, particularly situations where classicallyconditioned and instrumental responses are in con\ufb02ict. They proposed a Qlearning framework for modeling aspects of this interaction. Modayil and Sutton  used a mobile robot to demonstrate the e\u21b5ectiveness of a control method combining a \ufb01xed response with online prediction learning. Calling this Pavlovian control, they emphasized that it di\u21b5ers from the usual control methods of reinforcement learning, being based on predictively executing \ufb01xed responses and not on reward maximization. The electro-mechanical machine of Ross  and especially the learning version of Walter\u2019s turtle  were very early illustrations of Pavlovian control. 14.2.1 Kamin  \ufb01rst reported blocking, now commonly known as Kamin blocking, summary of the blocking phenomenon, the research it stimulated, and its lasting in\ufb02uence on animal learning theory", "b496774f-ab8e-417e-9b5c-0d98d65b458d": "This is much less efficient, in the sense of how much infor- mation can be gleaned early in an experiment, than manual search by a human practitioner, since one can usually tell early on if some set of hyperparameters is completely pathological. Swersky et al.\n\nhave introduced an early version of an algorithm that maintains a set of multiple experiments. At various time points, the hyperparameter optimization algorithm can choose to begin a new experiment, to \u201cfreeze\u201d a running experiment that is not promising, or to \u201cthaw\u201d and resume an experiment that was earlier frozen but now appears promising given more information. 11.5 Debugging Strategies  When a machine learning system performs poorly, it is usually difficult to tell whether the poor performance is intrinsic to the algorithm itself or whether there is a bug in the implementation of the algorithm. Machine learning systems are difficult to debug for various reasons. In most cases, we do not know a priori what the intended behavior of the algorithm is. In fact, the entire point of using machine learning is that it will discover useful behavior that we were not able to specify ourselves", "40fa55d5-de7d-4b80-a19a-62844289b0c4": "CONVOLUTIONAL NETWORKS  Generally, we do not use only a linear operation to transform from the inputs to the outputs in a convolutional layer. We generally also add some bias term to each output before applying the nonlinearity. This raises the question of how to share parameters among the biases. For locally connected layers, it is natural to give each unit its own bias, and for tiled convolution, it is natural to share the biases with the same tiling pattern as the kernels. For convolutional layers, it is typical to have one bias per channel of the output and share it across all locations within each convolution map. If the input is of known, fixed size, however, it is also possible to learn a separate bias at each location of the output map.\n\nSeparating the biases may slightly reduce the statistical efficiency of the model, but it allows the model to correct for differences in the image statistics at different locations. For example, when using implicit zero padding, detector units at the edge of the image receive less total input and may need larger biases", "b9385d4a-7a81-4251-b98f-24698dfffa48": "DEEP FEEDFORWARD NETWORKS  It would perhaps be better to call the softmax function \u201csoftargmax,\u201d but the current name is an entrenched convention. 6.2.2.4 Other Output Types  The linear, sigmoid, and softmax output units described above are the most common. Neural networks can generalize to almost any kind of output layer that we wish. The principle of maximum likelihood provides a guide for how to design a good cost function for nearly any kind of output layer. In general, if we define a conditional distribution p(y | x; @), the principle of maxinsenelle ieee dhipkses the agurallagtsyark as egpasscutivess function f(x; 4).\n\nhttps://www.deeplearningbook.org/contents/mlp.html    = W provides the parameters for a distribution over \u00a5. Our loss function can then be interpreted as \u2014 log p(y; w(a)). He By ee of this function are not direct predictions of the value y", "1e260b9f-c0d2-4b6c-8dc4-55b18fec4df0": "generate adversarial exam- ples to improve performance on the MNIST classification task.\n\nUsing a technique for generating adversarial examples known as the \u201cfast gradient sign method\u2019, a maxout network  misclassified 89.4% of adversarial examples with an average confidence of 97.6%. This test is done on the MNIST dataset. With adversarial training, the error rate of adversarial examples fell from 89.4% to 17.9% (Fig. 15). Li et al. experiment with a novel adversarial training approach and compare the  performance on original testing data and adversarial examples", "7ea4ab92-562e-4efb-abdc-fcc80daa6184": "Using (10.37) and (10.38) we can \ufb01rst perform the summation over \ufffdz to give Because the remaining integrations are intractable, we approximate the predictive density by replacing the true posterior distribution p(\u03c0, \u00b5, \u039b|X) with its variational approximation q(\u03c0)q(\u00b5, \u039b) to give where we have made use of the factorization (10.55) and in each term we have implicitly integrated out all variables {\u00b5j, \u039bj} for j \u0338= k The remaining integrations can now be evaluated analytically giving a mixture of Student\u2019s t-distributions Exercise 10.19 in which the kth component has mean mk, and the precision is given by in which \u03bdk is given by (10.63). When the size N of the data set is large the predictive distribution (10.81) reduces to a mixture of Gaussians. Exercise 10.20 We have seen that the variational lower bound can be used to determine a posterior distribution over the number K of components in the mixture model", "587de0b0-d488-4d95-9732-1b221fbdca9a": "Next, prove the results (10.220)\u2013 (10.222) by using (10.207) and completing the square in the exponential. Finally, use (10.208) to derive the result (10.223). For most probabilistic models of practical interest, exact inference is intractable, and so we have to resort to some form of approximation.\n\nIn Chapter 10, we discussed inference algorithms based on deterministic approximations, which include methods such as variational Bayes and expectation propagation. Here we consider approximate inference methods based on numerical sampling, also known as Monte Carlo techniques. Although for some applications the posterior distribution over unobserved variables will be of direct interest in itself, for most situations the posterior distribution is required primarily for the purpose of evaluating expectations, for example in order to make predictions. The fundamental problem that we therefore wish to address in this chapter involves \ufb01nding the expectation of some function f(z) with respect to a probability distribution p(z). Here, the components of z might comprise discrete or continuous variables or some combination of the two. Thus in the case of continuous variables, we wish to evaluate the expectation where the integral is replaced by summation in the case of discrete variables", "2acb1e4b-2106-4fff-a038-f28bcf9db69a": "The activations of the hidden units in the \ufb01rst hidden layer while the activations of the output units are given by Suppose we perform a linear transformation of the input data of the form Then we can arrange for the mapping performed by the network to be unchanged by making a corresponding linear transformation of the weights and biases from the inputs to the units in the hidden layer of the form Exercise 5.24 Similarly, a linear transformation of the output variables of the network of the form can be achieved by making a transformation of the second-layer weights and biases using If we train one network using the original data and one network using data for which the input and/or target variables are transformed by one of the above linear transformations, then consistency requires that we should obtain equivalent networks that differ only by the linear transformation of the weights as given.\n\nAny regularizer should be consistent with this property, otherwise it arbitrarily favours one solution over another, equivalent one. Clearly, simple weight decay (5.112), that treats all weights and biases on an equal footing, does not satisfy this property. We therefore look for a regularizer which is invariant under the linear transformations (5.116), (5.117), (5.119) and (5.120)", "84274483-b077-4308-933f-4f519b3edf36": "First, the optimization algorithm used for training may not be able to find the value of the parameters that corresponds o the desired function. Second, the training algorithm might choose the wrong function as a result of overfitting. Recall from section 5.2.1 that the no free lunch heorem shows that there is no universally superior machine learning algorithm.\n\nFeedforward networks provide a universal system for representing functions in the sense that, given a function, there exists a feedforward network that approximates  he function. There is no universal procedure for examining a training set of specific examples and choosing a function that will generalize to points not in the  https://www.deeplearningbook.org/contents/mlp.html    training set. According to the universal approximation theorem, there exists a network large enough to achieve any degree of accuracy we desire, but the theorem does not  say how large this network will be. Barron  provides some bounds on the size of a single-layer network needed to approximate a broad class of functions. Unfortunately, in the worst case, an exponential number of hidden units (possibly with one hidden unit corresponding to each input configuration that needs to be distinguished) may be required", "937fe110-d861-4ec0-9dbf-a0d69f027d5a": "Some methods may in fact diverge, with their VE approaching in\ufb01nity in the limit. In the last two sections we outlined a framework for combining a wide range of reinforcement learning methods for value prediction with a wide range of function approximation methods, using the updates of the former to generate training examples for the latter. We also described a VE performance measure which these methods may aspire to minimize. The range of possible function approximation methods is far too large to cover all, and anyway too little is known about most of them to make a reliable evaluation or recommendation. Of necessity, we consider only a few possibilities.\n\nIn the rest of this chapter we focus on function approximation methods based on gradient principles, and on linear gradient-descent methods in particular. We focus on these methods in part because we consider them to be particularly promising and because they reveal key theoretical issues, but also because they are simple and our space is limited. We now develop in detail one class of learning methods for function approximation in value prediction, those based on stochastic gradient descent (SGD). SGD methods are among the most widely used of all function approximation methods and are particularly well suited to online reinforcement learning", "93970af5-aa44-4f07-8bd0-0a89ea7bb4d7": "In the tree-backup update, the target includes all these things plus the estimated values of the dangling action nodes hanging o\u21b5 the sides, at all levels. This is why it is called a treebackup update; it is an update from the entire tree of estimated action values. More precisely, the update is from the estimated action values of the leaf nodes of the tree. The action nodes in the interior, corresponding to the actual actions taken, do not participate. Each leaf node contributes to the target with a weight proportional to its probability of occurring under the target policy \u21e1. Thus each \ufb01rst-level action a contributes with a weight of \u21e1(a|St+1), except that the action actually taken, At+1, does not contribute at all. Its probability, \u21e1(At+1|St+1), is used to weight all the second-level action values. Thus, each non-selected second-level action a0 contributes with weight \u21e1(At+1|St+1)\u21e1(a0|St+2)", "b87141c7-8fc6-433d-8e28-c922bdcf06f6": "Finally, there are undirected links connecting each factor node to all of the variables nodes on which that factor depends. Consider, for example, a distribution that is expressed in terms of the factorization This can be expressed by the factor graph shown in Figure 8.40. Note that there are two factors fa(x1, x2) and fb(x1, x2) that are de\ufb01ned over the same set of variables. In an undirected graph, the product of two such factors would simply be lumped together into the same clique potential. Similarly, fc(x2, x3) and fd(x3) could be combined into a single potential over x2 and x3. The factor graph, however, keeps such factors explicit and so is able to convey more detailed information about the underlying factorization. f(x1, x2, x3) = \u03c8(x1, x2, x3) representing the same distribution as the undirected graph. (c) A different factor graph representing the same distribution, whose factors satisfy fa(x1, x2, x3)fb(x1, x2) = \u03c8(x1, x2, x3)", "b461845d-8444-4c37-90a7-2022ec02eb6a": "Association for Computational Linguistics.\n\nPeter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-based n-gram models of natural language. Computational linguistics, 18(4):467\u2013479. Daniel Cer, Mona Diab, Eneko Agirre, Inigo LopezGazpio, and Lucia Specia. 2017. Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation , pages 1\u201314, Vancouver, Canada. Association for Computational Linguistics. Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. 2013. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint arXiv:1312.3005. Z. Chen, H. Zhang, X. Zhang, and L. Zhao", "9d6e96c8-df10-4f5b-95d9-3c6bf8caa4fc": "Generalization in reinforcement learning: Safely approximating the value function. In Advances in Neural Information Processing Systems 7 , pp. 369\u2013376. MIT Press, Cambridge, MA. Bradtke, S. J. Reinforcement learning applied to linear quadratic regulation.\n\nIn Advances in Neural Information Processing Systems 5 , pp. 295\u2013302. Morgan Kaufmann. Bradtke, S. J. Incremental Dynamic Programming for On-Line Adaptive Optimal Control. Bradtke, S. J., Barto, A. G. Linear least\u2013squares algorithms for temporal di\u21b5erence Bradtke, S. J., Ydstie, B. E., Barto, A. G. Adaptive linear quadratic control using policy iteration. In Proceedings of the American Control Conference, pp. 3475\u20133479. American Automatic Control Council, Evanston, IL. Brafman, R", "5858ff76-0f9b-4329-b074-36e66a81a3d9": "MIT Press. Williams, O., A. Blake, and R. Cipolla . Sparse Bayesian learning for ef\ufb01cient visual tracking. IEEE Transactions on Pattern Analysis and Machine Intelligence 27(8), 1292\u20131304. Williams, P. M. Using neural networks to model conditional multivariate densities. Neural Computation 8(4), 843\u2013854. Winn, J. and C. M. Bishop . Variational message passing. Journal of Machine Learning Research 6, 661\u2013694. Page numbers in bold indicate the primary source of information for the corresponding topic.", "8a61b289-88f9-42c7-9131-506f246eb32c": "Samples taken from GANs can be augmented with traditional augmentations such as lighting filters, or even used in neural network augmentation strategies such as Smart Augmentation or Neural Augmentation to cre- ate even more samples. These samples can be fed into further GANs and dramatically increase the size of the original dataset. The extensibility of the GAN framework is amongst many reasons they are so interesting to Deep Learning researchers. Test-time augmentation is analogous to ensemble learning in the data space. Instead of aggregating the predictions of different learning algorithms, we aggregate predictions across augmented images.\n\nWe can even extend the solution algorithm to parameterize prediction weights from different augmentations. This seems like a good solution for sys-  tems concerned with achieving very high performance scores, more so than prediction Shorten and Khoshgoftaar J Big Data  6:60   speed. Determining the effectiveness of test-time augmentation by primarily exploring test-time geometric transformations and Neural Style Transfer, is an area of future work. An interesting question for practical Data Augmentation is how to determine post- augmented dataset size. There is no consensus as to which ratio of original to final dataset size will result in the best performing model", "be0299c5-8cf3-4fb2-be2c-7e152eeecff8": "Example 6.4 illustrates a general di\u21b5erence between the estimates found by batch TD(0) and batch Monte Carlo methods. Batch Monte Carlo methods always \ufb01nd the estimates that minimize mean-squared error on the training set, whereas batch TD(0) always \ufb01nds the estimates that would be exactly correct for the maximum-likelihood model of the Markov process.\n\nIn general, the maximum-likelihood estimate of a parameter is the parameter value whose probability of generating the data is greatest. In this case, the maximum-likelihood estimate is the model of the Markov process formed in the obvious way from the observed episodes: the estimated transition probability from i to j is the fraction of observed transitions from i that went to j, and the associated expected reward is the average of the rewards observed on those transitions. Given this model, we can compute the estimate of the value function that would be exactly correct if the model were exactly correct. This is called the certainty-equivalence estimate because it is equivalent to assuming that the estimate of the underlying process was known with certainty rather than being approximated. In general, batch TD(0) converges to the certainty-equivalence estimate. This helps explain why TD methods converge more quickly than Monte Carlo methods", "9c5a3271-94df-4a7f-9624-4f0f6e090222": "Formally, we consider a Markov decision process (MDP) as illustrated in Figure 3, where t = (x, y) is the state-action pair. For example, in playing a video game, the state x is the game screen by the environment (the game engine) and y can be any game actions.\n\nAt time t, the A base concept that plays a central role in characterizing the learning in this setting is the action value function, also known as the Q function, which is the expected discounted future reward of taking action y in state x and continuing with the policy p\u03b8: where the expectation is taken by following the state dynamics induced by the policy (thus the dependence of Q\u03b8 on policy parameters \u03b8). We next discuss how Q\u03b8(x, y) can be used to specify the experience function in di\ufb00erent ways, which in turn derives various known algorithms in reinforcement learning (RL) . Note that here we are primarily interested in learning the conditional model (policy) p\u03b8(y|x). Yet we can still de\ufb01ne the joint distribution as p\u03b8(x, y) = p\u03b8(y|x)p0(x). 4.3.1. Expected future reward", "a12d771f-7f7a-4d77-b326-d1db9c1897f2": "The predictive distribution over t, given a new input x, is easily evaluated for this model using the Gaussian variational posterior for the parameters where we have evaluated the integral by making use of the result (2.115) for the linear-Gaussian model. Here the input-dependent variance is given by Note that this takes the same form as the result (3.59) obtained with \ufb01xed \u03b1 except that now the expected value E appears in the de\ufb01nition of SN. Another quantity of importance is the lower bound L de\ufb01ned by Evaluation of the various terms is straightforward, making use of results obtained in Exercise 10.27 10.4. Exponential Family Distributions In Chapter 2, we discussed the important role played by the exponential family of distributions and their conjugate priors.\n\nFor many of the models discussed in this book, the complete-data likelihood is drawn from the exponential family. However, in general this will not be the case for the marginal likelihood function for the observed data. For example, in a mixture of Gaussians, the joint distribution of observations xn and corresponding hidden variables zn is a member of the exponential family, whereas the marginal distribution of xn is a mixture of Gaussians and hence is not. Up to now we have grouped the variables in the model into observed variables and hidden variables", "2a7ce84e-af3d-48c2-ba06-13ca1db569ec": "359  CHAPTER 9. CONVOLUTIONAL NETWORKS  These grandmother cells have been shown to actually exist in the human brain, in a region called the medial temporal lobe . Researchers tested whether individual neurons would respond to photos of famous individuals. They found what has come to be called the \u201cHalle Berry neuron,\u201d an individual neuron that is activated by the concept of Halle Berry.\n\nThis neuron fires when a person sees a photo of Halle Berry, a drawing of Halle Berry, or even text containing the words \u201cHalle Berry.\u201d Of course, this has nothing to do with Halle Berry herself; other neurons responded to the presence of Bill Clinton, Jennifer Aniston, and so  https://www.deeplearningbook.org/contents/convnets.html    forth. These medial temporal lobe neurons are somewhat more general than modern convolutional networks, which would not automatically generalize to identifying  a person or object when reading its name. The closest analog to a convolutional network\u2019s last layer of features is a brain area called the inferotemporal cortex (IT)", "791df5dd-31e7-4a69-923d-04d69655a1aa": "In the episodic case, the extension is straightforward, but in the continuing case we have to take a few steps backward and re-examine how we have used discounting to de\ufb01ne an optimal policy.\n\nSurprisingly, once we have genuine function approximation we have to give up discounting and switch to a new \u201caverage-reward\u201d formulation of the control problem, with new \u201cdi\u21b5erential\u201d value functions. Starting \ufb01rst in the episodic case, we extend the function approximation ideas presented in the last chapter from state values to action values. Then we extend them to control following the general pattern of on-policy GPI, using \"-greedy for action selection. We show results for n-step linear Sarsa on the Mountain Car problem. Then we turn to the continuing case and repeat the development of these ideas for the average-reward case with di\u21b5erential values. The extension of the semi-gradient prediction methods of Chapter 9 to action values is straightforward. In this case it is the approximate action-value function, \u02c6q \u21e1 q\u21e1, that is represented as a parameterized functional form with weight vector w", "e51f81db-b4ac-4247-9185-f199a8131c8d": "Today, that does not appear to be the case.\n\nThe problem remains an active area of research, but experts now suspect that, for sufficiently large neural networks, most local minima have a low cost function value, and that it is not important to find a true global minimum rather than to find a point in parameter space that has low but not minimal cost . Many practitioners attribute nearly all difficulty with neural network optimiza- ion to local minima. We encourage practitioners to carefully test for specific problems. A test that can rule out local minima as the problem is plotting the norm of the gradient over time. If the norm of the gradient does not shrink to insignificant size, the problem is neither local minima nor any other kind of critical point. In high-dimensional spaces, positively establishing that local minima are  he problem can be very difficult. Many structures other than local minima also have small gradients. 8.2.3 Plateaus, Saddle Points and Other Flat Regions  For many high-dimensional nonconvex functions, local minima (and maxima) are in fact rare compared to another kind of point with zero gradient: a saddle point. Some points around a saddle point have greater cost than the saddle point, while others have a lower cost", "98491fe2-a784-488a-92af-64789ab66ecd": "a minimum of the training error has been reached then represents a way of limiting the effective network complexity. In the case of a quadratic error function, we can verify this insight, and show that early stopping should exhibit similar behaviour to regularization using a simple weight-decay term. This can be understood from Figure 5.13, in which the axes in weight space have been rotated to be parallel to the eigenvectors of the Hessian matrix.\n\nIf, in the absence of weight decay, the weight vector starts at the origin and proceeds during training along a path that follows the local negative gradient vector, then the weight vector will move initially parallel to the w2 axis through a point corresponding roughly to \ufffdw and then move towards the minimum of the error function wML. This follows from the shape of the error surface and the widely differing eigenvalues of the Hessian. Stopping at a point near \ufffdw is therefore similar to weight decay", "b2e59cae-b820-41c5-b9ef-22f8c8fd7c71": "Generative models used in this setting have been based on conditional VAE , GAN  or pre-trained language models like GPT-2 . Overall, these conditional generation methods can create novel and diverse data that might be unseen in the original dataset, but require signi\ufb01cant training effort. Adversarial methods create augmented examples by adding adversarial perturbations to the original data, which dramatically in\ufb02uences the model\u2019s predictions and con\ufb01dence without changing human judgements.\n\nThese adversarial examples  could be leveraged in adversarial training  to increase neural models\u2019 robustness, and can also be utilized as data augmentation to increase the models\u2019 generalization ability .1 White-Box methods rely on model architecture and parameters being accessible and create adversarial examples directly using a model\u2019s gradients. Unlike image pixel values that are continuous, textual tokens are discrete and cannot be directly modi\ufb01ed based on gradients. To this end, adversarial perturbations are added directly to token embeddings or sentence hidden representations  which creates \u201cvirtual adversarial examples\u201d. Other approaches vectorize modi\ufb01cation operations as the difference of one-hot vectors (Ebrahimi et al., 2018b,a), or \ufb01nd real word neighbors in a model\u2019s hidden representations via its gradients", "1a66f19b-6ac6-4363-ad6a-d44edcb11215": "DEEP FEEDFORWARD NETWORKS  \u00a9 ia a  vuracy (percent)  \u00b0 = >  https://www.deeplearningbook.org/contents/mlp.html    ure  93.5 93.0  92.5  92.0 3 4 5 6 7 8 9 10 11  Test acc  Figure 6.6: Effect of depth. Empirical results showing that deeper networks generalize better when used to transcribe multidigit numbers from photographs of addresses. Data from Goodfellow et al. The test set accuracy consistently increases with increasing depth. See figure 6.7 for a control experiment demonstrating that other increases to the model size do not yield the same effect. Many neural network architectures have been developed for specific tasks.\n\nSpecialized architectures for computer vision called convolutional networks are described in chapter 9. Feedforward networks may also be generalized to the recurrent neural networks for sequence processing, described in chapter 10, which have their own architectural considerations. In general, the layers need not be connected in a chain, even though this is the most common practice. Many architectures build a main chain but then add extra architectural features to it, such as skip connections going from layer i to layer 4+ 2 or higher", "4bf1bb7a-6f52-4be3-8772-df0e5d8a0957": "Theoretical Neuroscience: Computational and Mathematical Modeling of Neural Systems. MIT Press, Cambridge, MA. Dayan, P., Niv, Y. Reinforcement learning: the good, the bad and the ugly. Current Dayan, P., Niv, Y., Seymour, B., Daw, N. D. .\n\nThe misbehavior of value and the discipline De Asis, K., Hernandez-Garcia, J. F., Holland, G. Z., Sutton, R. S. Multi-step Reinforcement Learning: A Unifying Algorithm. ArXiv:1703.01327. de Farias, D. P. The Linear Programming Approach to Approximate Dynamic Programming: Theory and Application. Stanford University PhD thesis. de Farias, D. P., Van Roy, B. The linear programming approach to approximate dynamic In Proceedings of the Fourteenth International Joint Conference on Arti\ufb01cial Intelligence (IJCAI-95), pp. 1121\u20131127", "49c1a744-b53a-414d-a3fc-48e59a2f5f65": "CONVOLUTIONAL NETWORKS  Output of softmax: 1,000 class probabilities  Output of matrix multiply: 1,000 units  Output of reshape to vector: 16,384 units  Output of poolng with stride 4: 16x16x64  Output o convolution + ReLU: 64x64x64  Output of poolng with stride 4: 64x64x64  Output o: convolution + ReLU: 256x256x64  Output of softmax: 1,000 class probabilities  Output of matrix multiply: 1,000 units  Output of reshape to vector: 576 units  Output of pooling to 3x3 grid: 3x3x64  Output 0: convolution + ReLU: 64x64x64  Output of poolng with stride 4: 64x64x64  Output 0: convolution + ReLU: 256x256x64  Output of softmax: 1,000 class probabilities  Output of average pooling: 1x1x1,000  Output 0! convolution: 16x16x1,000  Output of pooling with stride 4: 16x16x64  Output o: convolution + ReLU: 64x64x64  Output of pooling with stride 4: 64x64x64  Output 0: convolution + ReLU: 256x256x64  256x256x\u00ab Figure 9.11: Examples of architectures for classification with convolutional networks", "e3404bad-52d1-46ac-9f3e-f2b96bb92b89": "SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  wise clipping, the direction of the update is not aligned with the true gradient or the minibatch gradient, but it is still a descent direction. It has also been proposed  to clip the back-propagated gradient (with respect to hidden units), but no comparison has been published between these variants; we conjecture that all these methods behave similarly. 10.11.2 Regularizing to Encourage Information Flow  Gradient clipping helps to deal with exploding gradients, but it does not help with vanishing gradients. To address vanishing gradients and better capture long-term dependencies, we discussed the idea of creating paths in the computational graph of the unfolded recurrent architecture along which the product of gradients associated with arcs is near 1", "1d82320b-f6d3-42d9-8ce9-19d95271f3a2": "(10.130) We see that the functions f(x) and g(\u03bb) play a dual role, and are related through (10.129) and (10.130). Let us apply these duality relations to our simple example f(x) = exp(\u2212x).\n\nFrom (10.129) we see that the maximizing value of x is given by \u03be = \u2212 ln(\u2212\u03bb), and back-substituting we obtain the conjugate function g(\u03bb) in the form as obtained previously. The function \u03bb\u03be \u2212g(\u03bb) is shown, for \u03be = 1 in the right-hand plot in Figure 10.10. As a check, we can substitute (10.131) into (10.130), which gives the maximizing value of \u03bb = \u2212 exp(\u2212x), and back-substituting then recovers the original function f(x) = exp(\u2212x). For concave functions, we can follow a similar argument to obtain upper bounds, in which max\u2019 is replaced with \u2018min\u2019, so that If the function of interest is not convex (or concave), then we cannot directly apply the method above to obtain a bound", "58733b49-3b6e-4bc7-8c22-6d9570b1e541": "Semi-gradient TD methods are not true gradient methods. In such bootstrapping methods (including DP), the weight vector appears in the update target, yet this is not taken into account in computing the gradient\u2014thus they are semi-gradient methods. As such, they cannot rely on classical SGD results. Nevertheless, good results can be obtained for semi-gradient methods in the special case of linear function approximation, in which the value estimates are sums of features times corresponding weights. The linear case is the most well understood theoretically and works well in practice when provided with appropriate features. Choosing the features is one of the most important ways of adding prior domain knowledge to reinforcement learning systems.\n\nThey can be chosen as polynomials, but this case generalizes poorly in the online learning setting typically considered in reinforcement learning. Better is to choose features according the Fourier basis, or according to some form of coarse coding with sparse overlapping receptive \ufb01elds. Tile coding is a form of coarse coding that is particularly computationally e\ufb03cient and \ufb02exible. Radial basis functions are useful for one- or two-dimensional tasks in which a smoothly varying response is important", "b14fa393-a655-4fba-ae96-62e93098b253": "For instance, one common criticism of the Bayesian approach is that the prior distribution is often selected on the basis of mathematical convenience rather than as a re\ufb02ection of any prior beliefs. Even the subjective nature of the conclusions through their dependence on the choice of prior is seen by some as a source of dif\ufb01culty.\n\nReducing the dependence on the prior is one motivation for so-called noninformative priors. Section 2.4.3 However, these lead to dif\ufb01culties when comparing different models, and indeed Bayesian methods based on poor choices of prior can give poor results with high con\ufb01dence. Frequentist evaluation methods offer some protection from such problems, and techniques such as cross-validation remain useful in areas such as model Section 1.3 comparison. This book places a strong emphasis on the Bayesian viewpoint, re\ufb02ecting the huge growth in the practical importance of Bayesian methods in the past few years, while also discussing useful frequentist concepts as required. Although the Bayesian framework has its origins in the 18th century, the practical application of Bayesian methods was for a long time severely limited by the dif\ufb01culties in carrying through the full Bayesian procedure, particularly the need to marginalize (sum or integrate) over the whole of parameter space, which, as we shall see, is required in order to make predictions or to compare different models", "ab90d733-48c5-4582-942a-535a8a1d9e9e": "Future work  Future work in Data Augmentation will be focused on many different areas such as establishing a taxonomy of augmentation techniques, improving the quality of GAN samples, learning new ways to combine meta-learning and Data Augmentation, discov- ering relationships between Data Augmentation and classifier architecture, and extend- ing these principles to other data types. We are interested in seeing how the time-series component in video data impacts the use of static image augmentation techniques. Data Augmentation is not limited to the image domain and can be useful for text, bioinfor- matics, tabular records, and many more. Our future work intends to explore performance benchmarks across geometric and color space augmentations across several datasets from different image recognition tasks. These datasets will be constrained in size to test the effectiveness with respect to limited data problems. Zhang et al. test their novel GAN augmentation technique on the SVHN dataset across 50, 80, 100, 200, and 500 training instances.\n\nSimilar to this work, we will look to further establish benchmarks for different levels of limited data. Improving the quality of GAN samples and testing their effectiveness on a wide range of datasets is another very important area for future work", "af5863a6-58b9-478d-870e-d8ce91e724cf": "from torchvision import transforms def get_color_distortion(s=1.0): # s is the strength of color distortion.\n\ncolor_jitter = transforms.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.2*s) rnd_color_jitter = transforms.RandomApply(, p=0.8) rnd_gray = transforms.RandomGrayscale(p=0.2) color_distort = transforms.Compose() 12Our code and results are based on Tensor\ufb02ow, the Pytorch code here is a reference. A Simple Framework for Contrastive Learning of Visual Representations Gaussian blur This augmentation is in our default policy. We \ufb01nd it helpful, as it improves our ResNet-50 trained for 100 epochs from 63.2% to 64.5%. We blur the image 50% of the time using a Gaussian kernel. We randomly sample \u03c3 \u2208 , and the kernel size is set to be 10% of the image height/width. In both Figure 9 and Figure B.1, we use a linear scaling of learning rate similar to  when training with different batch sizes", "67c08829-82e1-4a58-bd56-3691c497ca85": "We therefore need to \ufb01nd a measure of performance which depends only on the training data and which does not suffer from bias due to over-\ufb01tting. Historically various \u2018information criteria\u2019 have been proposed that attempt to correct for the bias of maximum likelihood by the addition of a penalty term to compensate for the over-\ufb01tting of more complex models. For example, the Akaike information criterion, or AIC , chooses the model for which the quantity is largest. Here p(D|wML) is the best-\ufb01t log likelihood, and M is the number of adjustable parameters in the model.\n\nA variant of this quantity, called the Bayesian information criterion, or BIC, will be discussed in Section 4.4.1. Such criteria do not take account of the uncertainty in the model parameters, however, and in practice they tend to favour overly simple models. We therefore turn in Section 3.4 to a fully Bayesian approach where we shall see how complexity penalties arise in a natural and principled way. In the polynomial curve \ufb01tting example we had just one input variable x. For practical applications of pattern recognition, however, we will have to deal with spaces of high dimensionality comprising many input variables", "e9ac1855-c5d5-4f2d-9fef-60b306669a53": "If we used localized basis functions such as Gaussians, then in regions away from the basis function centres, the contribution from the second term in the predictive variance (3.59) will go to zero, leaving only the noise contribution \u03b2\u22121.\n\nThus, the model becomes very con\ufb01dent in its predictions when extrapolating outside the region occupied by the basis functions, which is generally an undesirable behaviour. This problem can be avoided by adopting an alternative Bayesian approach to regression known as a Gaussian process. Section 6.4 Note that, if both w and \u03b2 are treated as unknown, then we can introduce a conjugate prior distribution p(w, \u03b2) that, from the discussion in Section 2.3.6, will be given by a Gaussian-gamma distribution . In this case, the Exercise 3.12 The posterior mean solution (3.53) for the linear basis function model has an interesting interpretation that will set the stage for kernel methods, including Gaussian processes. If we substitute (3.53) into the expression (3.3), we see that the predictive Chapter 6 where SN is de\ufb01ned by (3.51). Thus the mean of the predictive distribution at a point x is given by a linear combination of the training set target variables tn, so that we can write is known as the smoother matrix or the equivalent kernel", "e3c49685-b058-4d5f-a2a8-e7d076c30dbf": "It will be inter- esting to see if better results can be achieved by erasing different shaped patches such as circles rather than 1 x m rectangles. An extension of this will be to parameterize the geometries of random erased patches and learn an optimal erasing configuration. A disadvantage to random erasing is that it will not always be a label-preserving trans- formation.\n\nIn handwritten digit recognition, if the top part of an \u20188\u2019 is randomly cropped out, it is not any different from a \u20186: In many fine-grained tasks such as the Stanford Cars dataset , randomly erasing sections of the image (logo, etc.) may make the car brand unrecognizable. Therefore, some manual intervention may be necessary depending on the dataset and task. Anote on combining augmentations  Of the augmentations discussed, geometric transformations, color space transforma- tions, kernel filters, mixing images, and random erasing, nearly all of these transforma- tions come with an associated distortion magnitude parameter as well. This parameter encodes the distortional difference between a 45\u00b0 rotation and a 30\u00b0 rotation", "057197da-ca25-467e-a0cd-e7c55005fc6e": "Chapter 6 is the most important for the subject and for the rest of the book. A course focusing on machine learning or neural networks should cover Chapters 9 and 10, and a course focusing on arti\ufb01cial intelligence or planning should cover Chapter 8. Throughout the book, sections and chapters that are more di\ufb03cult and not essential to the rest of the book are marked with a \u21e4. These can be omitted on \ufb01rst reading without creating problems later on.\n\nSome exercises are also marked with a \u21e4 to indicate that they are more advanced and not essential to understanding the basic material of the chapter. Most chapters end with a section entitled \u201cBibliographical and Historical Remarks,\u201d wherein we credit the sources of the ideas presented in that chapter, provide pointers to further reading and ongoing research, and describe relevant historical background. Despite our attempts to make these sections authoritative and complete, we have undoubtedly left out some important prior work. For that we again apologize, and we welcome corrections and extensions for incorporation into the electronic version of the book. Like the \ufb01rst edition, this edition of the book is dedicated to the memory of A. Harry Klopf. It was Harry who introduced us to each other, and it was his ideas about the brain and arti\ufb01cial intelligence that launched our long excursion into reinforcement learning", "7fca4f1a-0f45-4f8f-ae9f-e308b0be0ca5": "However, assembling enormous datasets can be a very daunting task due to the manual effort of collecting and labeling data. Limited datasets is an especially prevalent challenge in medical image analysis. Given big data, deep convolutional net- works have been shown to be very powerful for medical image analysis tasks such as skin lesion classification as demonstrated by Esteva et al. This has inspired the use of CNNs on medical image analysis tasks  such as liver lesion classification, brain scan analysis, continued research in skin lesion classification, and more. Many of the images studied are derived from computerized tomography (CT) and magnetic resonance imag- ing (MRI) scans, both of which are expensive and labor-intensive to collect. It is espe-  cially difficult to build big medical image datasets due to the rarity of diseases, patient Shorten and Khoshgoftaar J Big Data  6:60   privacy, the requirement of medical experts for labeling, and the expense and manual effort needed to conduct medical imaging processes. These obstacles have led to many studies on image Data Augmentation, especially GAN-based oversampling, from the application perspective of medical image classification.\n\nMany studies on the effectiveness of Data Augmentation utilize popular academic image datasets to benchmark results", "cc6ee4d8-76c7-41f6-b04e-06967b3586ae": "STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  16.2.4 Energy-Based Models  Many interesting theoretical results about undirected models depend on the as- sumption that Vx, p(x) > 0. A convenient way to enforce this condition is to use an energy-based model (EBM) where  P(x) = exp(\u2014E(x)), (16.7)  and E(x) is known as the energy function.\n\nBecause exp(z) is positive for all z, this guarantees that no energy function will result in a probability of zero for any state x. Being completely free to choose the energy function makes learning simpler. If we learned the clique potentials directly, we would need to use constrained optimization to arbitrarily impose some specific minimal probability value. By learning the energy function, we can use unconstrained optimization.\u201d The probabilities in an energy-based model can approach arbitrarily close to zero but never reach it. Any distribution of the form given by equation 16.7 is an example of a Boltz- mann distribution. For this reason, many energy-based models are called Boltzmann machines", "7214f0a3-8c33-439b-96ad-1e712995c180": "DEEP FEEDFORWARD NETWORKS  Figure 6.11: The computational graph used to compute the cost to train our example of a single-layer MLP using the cross-entropy loss and weight decay. explore two different branches. On the shorter branch, it adds H'G to the gradient on we), using the back-propagation rule for the second argument to the matrix multiplication operation. The other branch corresponds to the longer chain descending further along the network. First, the back-propagation algorithm computes VyJ = GW)' using the back-propagation rule for the first argument to the matrix multiplication operation. Next, the relu operation uses its back- propagation rule to zero out components of the gradient corresponding to entries of U\u00ae that are less than 0. Let the result be called G\u2019. The last step of the back-propagation algorithm is to use the back-propagation rule for the second argument of the matmul operation to add XG\u2019 to the gradient on W", "5be4c71b-1a91-4cb5-9114-9573a0d90db7": "This type of function is sometimes also called a \u2018squashing function\u2019 because it maps the whole real axis into a \ufb01nite interval. The logistic sigmoid has been encountered already in earlier chapters and plays an important role in many classi\ufb01cation algorithms. It satis\ufb01es the following symmetry property as is easily veri\ufb01ed. The inverse of the logistic sigmoid is given by and is known as the logit function.\n\nIt represents the log of the ratio of probabilities ln  for the two classes, also known as the log odds. Note that in (4.57) we have simply rewritten the posterior probabilities in an equivalent form, and so the appearance of the logistic sigmoid may seem rather vacuous. However, it will have signi\ufb01cance provided a(x) takes a simple functional form. We shall shortly consider situations in which a(x) is a linear function of x, in which case the posterior probability is governed by a generalized linear model. For the case of K > 2 classes, we have which is known as the normalized exponential and can be regarded as a multiclass generalization of the logistic sigmoid", "b8e73661-4632-4dd2-a152-fd23df795a74": "(Right)This directed model has interactions between latent variables when the visible variables are observed, because every two latent variables are coparents. Some probabilistic models are able to provide tractable inference over the latent variables despite having one of the graph structures depicted above. This is possible if the conditional probability distributions are chosen to introduce additional independences beyond those described by the graph.\n\nFor example, probabilistic PCA has the graph structure shown in the right yet still has simple inference because of special properties of the specific conditional distributions it uses (linear-Gaussian conditionals with mutually orthogonal basis vectors). https://www.deeplearningbook.org/contents/inference.html    630  CHAPTER 19. APPROXIMATE INFERENCE  19.1 Inference as Optimization  Many approaches to confronting the problem of difficult inference make use of the observation that exact inference can be described as an optimization problem. Approximate inference algorithms may then be derived by approximating the underlying optimization problem. To construct the optimization problem, assume we have a probabilistic model consisting of observed variables v and latent variables h", "135175af-dcd6-4da9-9094-42632d1fc90a": ".\n\nAnother nonlinear extension of ICA is the approach of nonlinear independent components estimation, or NICE , which stacks a series of invertible transformations (encoder stages) with the property that the determinant of the Jacobian of each transformation can be computed efficiently. This makes it possible to compute the likelihood exactly, and like ICA, NICE attempts to transform the data into a space where it has a factorized marginal distribution, but it is more likely to succeed thanks to the nonlinear encoder. Because the encoder is associated with a decoder that is its perfect inverse, generating samples from the model is straightforward (by first sampling from p(h) and then applying the decoder). Another generalization of ICA is to learn groups of features, with statistical  ire  https://www.deeplearningbook.org/contents/linear_factors.html    dependence allowed within a group but discouraged between groups . When the TOUpS of related units are chosen to be nonoverlapping, this is called independent subspace analysis. It is also possible to assign spatial coordinates to each hidden unit and form overlapping groups of spatially neighboring units. This encourages nearby units to learn similar features", "849a8af0-e4b1-45a1-9f1a-179ee4847474": "The connectivity is the same as for the original neural auto-regressive network of Bengio and Bengio , but NADE introduces an additional parameter sharing scheme, as illustrated in figure 20.10.\n\nThe parameters of the hidden units of different groups j are shared. The weights Wei from the i-th input 2; to the k-th element of the j-th group  hi he hg  https://www.deeplearningbook.org/contents/generative_models.html    aa 'Wio L7 \u201cWw  CeOMO  Figure 20.10: An illustration of the neural autoregressive density estimator (NADE). The hidden units are organized in groupsh\u201d) so that only the inputs x1,...,2; participate in computing h and predicting P(x; | xj-1,...,\u00ab1), for j > i. NADE is \u2018differentiated from earlier neural auto-regressive networks by the use of a particular weight sharing pattern: Ww; \u2018,, = Wx, is shared (indicated in the figure by the use of the same line pattern for every instance of a replicated weight) for all the weights going out froma; to the k-th unit of any group j >i", "1d21c617-5e42-4f86-925b-3b8755a6d7f5": "Schraudolph, N. N. Local gain adaptation in stochastic gradient descent. In Proceedings of the International Conference on Arti\ufb01cial Neural Networks, pp. 569\u2013574. IEEE, London. Schraudolph, N. N. Fast curvature matrix-vector products for second-order gradient Schraudolph, N. N., Yu, J., Aberdeen, D. Fast online policy gradient learning with SMD Schulman, J., Chen, X., Abbeel, P. Equivalence between policy gradients and soft Schultz, D. G., Melsa, J. L. State Functions and Linear Control Systems. McGraw-Hill, Schultz, W. Predictive reward signal of dopamine neurons. Journal of Neurophysiology, Schultz, W., Apicella, P., Ljungberg, T. .\n\nResponses of monkey dopamine neurons to reward and conditioned stimuli during successive steps of learning a delayed response task. The Journal of Neuroscience, 13(3):900\u2013913", "0af8302f-a752-4bca-9fb6-49c2465fd33f": "If we take the particular case of a two-dimensional input space x = (x1, x2) we can expand out the terms and thereby identify the corresponding nonlinear feature mapping We see that the feature mapping takes the form \u03c6(x) = (x2 therefore comprises all possible second order terms, with a speci\ufb01c weighting between them. More generally, however, we need a simple way to test whether a function constitutes a valid kernel without having to construct the function \u03c6(x) explicitly. A necessary and suf\ufb01cient condition for a function k(x, x\u2032) to be a valid kernel  is that the Gram matrix K, whose elements are given by k(xn, xm), should be positive semide\ufb01nite for all possible choices of the set {xn}. Note that a positive semide\ufb01nite matrix is not the same thing as a matrix whose elements are nonnegative. Appendix C One powerful technique for constructing new kernels is to build them out of simpler kernels as building blocks", "049009de-37f2-4b48-8cdb-652831205ccf": "In the case of bagging, each model is trained to convergence on its respective training set.\n\nIn the case of dropout, typically most models are not explicitly trained at all\u2014usually, the model is large enough that it would be infeasible to sample all possible subnetworks within the lifetime of the universe. Instead, a tiny fraction of the possible subnetworks are each trained for a single step, and the parameter sharing causes the remaining subnetworks to arrive at good settings of the parameters. These are the only differences. Beyond these, dropout follows the bagging algorithm. For example, the training set encountered by each subnetwork is indeed a subset of the original training set sampled with replacement. To make a prediction, a bagged ensemble must accumulate votes from all its members. We refer to this process as inference in this context. So far, our description of bagging and dropout has not required that the model be explicitly probabilistic. Now, we assume that the model\u2019s role is to output a probability distribution. In the case of bagging, each model 7 produces a probability distribution  257  CHAPTER 7", "cb3d0397-5b3d-457c-800f-2325592bb040": "Do the same for the posterior distribution of the means q\u22c6(\u00b5k|\u039bk).\n\nNext consider the posterior distribution q\u22c6(\u03c0) for the mixing coef\ufb01cients and show that this too becomes sharply peaked around the maximum likelihood solution. Similarly, show that the responsibilities become equal to the corresponding maximum likelihood values for large N, by making use of the following asymptotic result for the digamma function for large x Finally, by making use of (10.80), show that for large N, the predictive distribution becomes a mixture of Gaussians. 10.21 (\u22c6) Show that the number of equivalent parameter settings due to interchange symmetries in a mixture model with K components is K!. 10.22 (\u22c6 \u22c6) We have seen that each mode of the posterior distribution in a Gaussian mixture model is a member of a family of K! equivalent modes. Suppose that the result of running the variational inference algorithm is an approximate posterior distribution q that is localized in the neighbourhood of one of the modes. We can then approximate the full posterior distribution as a mixture of K! such q distributions, once centred on each mode and having equal mixing coef\ufb01cients", "75f078a8-bfaa-409d-973c-be0dc3bbd4b5": "The classifier is also often able to ignore many parts of the input. For example, when recognizing an object in a photo, it is usually possible to ignore the background of the photo. It is possible to ask probabilistic models to do many other tasks. These tasks are often more expensive than classification. Some of them require producing multiple output values.\n\nMost require a complete understanding of the entire structure of  file:///Users/lichenghu/Desktop/DSC-291%20copy/structed_models.html    the input, with no option to ignore sections of it. These tasks include the following: ' A natural image is an image that might be captured by a camera in a reasonably ordinary environment, as opposed to a synthetically rendered image, a screenshot of a web page, etc. 556  CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  e Density estimation: Given an input x, the machine learning system returns an estimate of the true density p(x) under the data-generating distribution. This requires only a single output, but it also requires a complete understanding of the entire input", "3b726367-c7c3-4ae7-ba69-977b2e7f0218": "Google\u2019s multilingual neural machine translation system: Enabling zero-shot translation. Transactions of the Association for Computational Linguistics, 5:339\u2013351. Sosuke Kobayashi. 2018. Contextual augmentation: Data augmentation by words with paradigmatic relations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 452\u2013457, New Orleans, Louisiana. Association for Computational Linguistics. Oleksandr Kolomiyets, Steven Bethard, and MarieFrancine Moens. 2011. Model-portability experiments for textual temporal analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 271\u2013276, Portland, Oregon, USA. Association for Computational Linguistics.\n\nAlex Krizhevsky, I. Sutskever, and G. Hinton. 2012. Imagenet classi\ufb01cation with deep convolutional neural networks. Advances in neural information processing systems, 25(2)", "b21ae364-4eee-497d-b854-b67a53e06125": "We denote the action selected on time step t as At, and the corresponding reward as Rt. The value then of an arbitrary action a, denoted q\u21e4(a), is the expected reward given that a is selected: If you knew the value of each action, then it would be trivial to solve the k-armed bandit problem: you would always select the action with highest value. We assume that you do not know the action values with certainty, although you may have estimates. We denote the estimated value of action a at time step t as Qt(a).\n\nWe would like Qt(a) to be close to q\u21e4(a). If you maintain estimates of the action values, then at any time step there is at least one action whose estimated value is greatest. We call these the greedy actions. When you select one of these actions, we say that you are exploiting your current knowledge of the values of the actions. If instead you select one of the nongreedy actions, then we say you are exploring, because this enables you to improve your estimate of the nongreedy action\u2019s value", "da4cd253-f58e-4db4-b896-1708d1fb74b0": "Finally, if there were more than three pieces, the fourth unit was set to a value proportionate to the number of additional pieces beyond three.\n\nLetting n denote the total number of pieces on the point, if n > 3, then the fourth unit took on the value (n\u22123)/2. This encoded a linear representation of \u201cmultiple spares\u201d at the given point. With four units for white and four for black at each of the 24 points, that made a total of 192 units. Two additional units encoded the number of white and black pieces on the bar (each took the value n/2, where n is the number of pieces on the bar), and two more encoded the number of black and white pieces already successfully removed from the board (these took the value n/15, where n is the number of pieces already borne o\u21b5). Finally, two units indicated in a binary fashion whether it was white\u2019s or black\u2019s turn to move. The general logic behind these choices should be clear. Basically, Tesauro tried to represent the position in a straightforward way, while keeping the number of units relatively small", "b06eb2ca-dc5e-484d-86e2-02d8e40ebee7": "A background in physics is not required as this section is self-contained and the key results are all derived from \ufb01rst principles. The dynamical approach to stochastic sampling has its origins in algorithms for simulating the behaviour of physical systems evolving under Hamiltonian dynamics. In a Markov chain Monte Carlo simulation, the goal is to sample from a given probability distribution p(z). The framework of Hamiltonian dynamics is exploited by casting the probabilistic simulation in the form of a Hamiltonian system. In order to remain in keeping with the literature in this area, we make use of the relevant dynamical systems terminology where appropriate, which will be de\ufb01ned as we go along. The dynamics that we consider corresponds to the evolution of the state variable z = {zi} under continuous time, which we denote by \u03c4.\n\nClassical dynamics is described by Newton\u2019s second law of motion in which the acceleration of an object is proportional to the applied force, corresponding to a second-order differential equation over time", "b7b9bd55-32dc-4632-8fe4-4c42339067db": "In the mcRBM or mPoT models, an overcomplete representation would mean that to capture variation in a particular direction in the observation space would require removing potentially all constraints with positive projection in that direction. This would suggest that these models are less well  suited to the overcomplete setting. The primary disadvantage of the spike and slab restricted Boltzmann machine is that some settings of the parameters can correspond to a covariance matrix that is not positive definite. Such a covariance matrix places more unnormalized probability on values that are farther from the mean, causing the integral over all possible outcomes to diverge. Generally this issue can be avoided with simple heuristic tricks. There is not yet any theoretically satisfying solution.\n\nUsing constrained optimization to explicitly avoid the regions where the probability is undefined is difficult to do without being overly conservative and also preventing the model from accessing high-performing regions of parameter space. Qualitatively, convolutional variants of the ssRBM produce excellent samples of natural images. Some examples are shown in figure 16.1. The ssRBM allows for several extensions", "eda344fb-5d93-415c-a50c-863c7e1553c1": "This force pushes the particle downhill along the cost function surface. The gradient descent algorithm would simply take a single step based on each gradient, but the Newtonian scenario used by the momentum algorithm instead uses this force to alter the velocity of the particle. We can think of the particle as being like a hockey puck sliding down an icy surface. Whenever it descends a steep part of the surface, it gathers speed and continues sliding in that direction  until it begins to go uphill again. One other force is necessary.\n\nIf the only force is the gradient of the cost function, then the particle might never come to rest. Imagine a hockey puck sliding down one side of a valley and straight up the other side, oscillating back and forth forever, assuming the ice is perfectly frictionless. To resolve this problem, we add one other force, proportional to \u2014v(t). In physics terminology, this force corresponds to viscous drag, as if the particle must push through a resistant medium such as syrup. This causes the particle to gradually lose energy over time and eventually converge to a local minimum", "f4fcb684-947f-4895-ab1c-b957f45368ff": "(3) Snorkel uses these labels to train a discriminative classi\ufb01cation model, such as a deep neural network ended; they can leverage information from multiple contexts simultaneously, such as combining information from a document\u2019s title, named entities in the text, and knowledge bases. This heterogeneity was cumbersome enough to completely block users of early versions of Snorkel. To address this challenge, we built an interface layer around the abstract concept of a labeling function (LF). We developed a \ufb02exible language for expressing weak supervision strategies and supporting data structures. We observed accelerated user productivity with these tools, which we validated in a user study where SMEs build models 2.8\u00d7 faster and increase predictive performance an average 45.5% versus seven hours of hand labeling. Trade-offs in Modeling of Sources Snorkel learns the accuracies of weak supervision sources without access to ground truth using a generative model . Furthermore, it also learns correlations and other statistical dependencies among sources, correcting for dependencies in labeling functions that skew the estimated accuracies .\n\nThis paradigm gives rise to previously unexplored trade-off spaces between predictive performance and speed", "e32fadbf-feba-46db-b5a1-f9e40a818dd9": "Here we shall focus on a widely used model known as a linear dynamical system.\n\nAs we have seen, the HMM corresponds to the state space model shown in Figure 13.5 in which the latent variables are discrete but with arbitrary emission probability distributions. This graph of course describes a much broader class of probability distributions, all of which factorize according to (13.6). We now consider extensions to other distributions for the latent variables. In particular, we consider continuous latent variables in which the summations of the sum-product algorithm become integrals. The general form of the inference algorithms will, however, be the same as for the hidden Markov model. It is interesting to note that, historically, hidden Markov models and linear dynamical systems were developed independently. Once they are both expressed as graphical models, however, the deep relationship between them immediately becomes apparent. One key requirement is that we retain an ef\ufb01cient algorithm for inference which is linear in the length of the chain. This requires that, for instance, when we take a quantity \ufffd\u03b1(zn\u22121), representing the posterior probability of zn given observations x1,", "de0d799a-91f2-4aa2-9483-4cb7ba8d1011": "Other issues like the introduction of the OS6 operating system space: comet asteroid spaceship In summary, this asteroid asteroid ship is about to make its maiden \ufb02ight to explore deep-space.\\n\\n \"The asteroid craft was a very early attempt at the discovery of a new home for the world\u2019s \ufb01rst black hole,\" NASA said in a news release. \"Today we religion: faith faith faith salvation In summary, Christian beliefs are not a new way to use the time spent thinking about God\u2019s world as a source for faith. Faith is an effort to think of the world without fear that it might become a dangerous place for the human family.\n\nBecause it represents the very essence that science: climate research chemistry In summary of the study, this review aims to determine how in a single study where the same number of data was analysed, a new methodology is needed to better understand who produced a different graph than the one suggested. The paper will be published in issue #5, Issue #18. military: the cavalry battalion a In summary, the army are a unit of the same type and in all, so there is no need to declare one", "1eb1af5d-493e-4bbc-b699-daf1405dfac4": "The main drawback to this approach is that we will only be able to train the inference network on values of v that have high probability under the model. Early in learning, the model distribution will not resemble the data distribution, so the inference network will not have an opportunity to learn on samples that resemble data. In section 18.2 we saw that one possible explanation for the role of dream sleep in human beings and animals is that dreams could provide the negative phase samples that Monte Carlo training algorithms use to approximate the negative gradient of the log partition function of undirected models. Another possible explanation for biological dreaming is that it is providing samples from p(h, v) which can be used to train an inference network to predict h given v. In some senses, this explanation is more satisfying than the partition function explanation. Monte Carlo algorithms generally do not perform well if they are run using only the positive phase of the gradient for several steps then with only the negative phase of the gradient for several steps. Human beings and animals are usually awake for several consecutive hours then asleep for several consecutive hours.\n\nIt is not readily apparent how this schedule could support Monte Carlo training of an undirected model", "44876aef-e1ec-4781-b0cf-69766b56d824": "There are many possible choices of encoders and decoders, depending on the type of data and model. In our example we used relatively simple neural networks, namely multi-layered perceptrons (MLPs). For the encoder we used a MLP with Gaussian output, while for the decoder we used MLPs with either Gaussian or Bernoulli outputs, depending on the type of data. In this case let p\u03b8(x|z) be a multivariate Bernoulli whose probabilities are computed from z with a fully-connected neural network with a single hidden layer: where f\u03c3(.) is the elementwise sigmoid activation function, and where \u03b8 = {W1, W2, b1, b2} are the weights and biases of the MLP. In this case let encoder or decoder be a multivariate Gaussian with a diagonal covariance structure: where {W3, W4, W5, b3, b4, b5} are the weights and biases of the MLP and part of \u03b8 when used as decoder", "e1ba6a3f-06b8-4db5-9ff8-eee9df8946eb": "Note that these recursions require that the forward pass be completed \ufb01rst so that the quantities \u00b5n and Vn will be available for the backward pass.\n\nFor the EM algorithm, we also require the pairwise posterior marginals, which can be obtained from (13.65) in the form \u03be(zn\u22121, zn) = (cn)\u22121 \ufffd\u03b1(zn\u22121)p(xn|zn)p(zn|z\u22121)\ufffd\u03b2(zn) Substituting for \ufffd\u03b1(zn) using (13.84) and rearranging, we see that \u03be(zn\u22121, zn) is a Gaussian with mean given with components \u03b3(zn\u22121) and \u03b3(zn), and a covariance between zn and zn\u22121 given by Exercise 13.31 So far, we have considered the inference problem for linear dynamical systems, assuming that the model parameters \u03b8 = {A, \u0393, C, \u03a3, \u00b50, V0} are known. Next, we consider the determination of these parameters using maximum likelihood . Because the model has latent variables, this can be addressed using the EM algorithm, which was discussed in general terms in Chapter 9", "1804cbb2-9638-48e0-a009-6f3f52e709db": "For a = b = 1, it reduces to a uniform distribution. The beta distribution is a special case of the K-state Dirichlet distribution for K = 2. The binomial distribution gives the probability of observing m occurrences of x = 1 in a set of N samples from a Bernoulli distribution, where the probability of observing x = 1 is \u00b5 \u2208 . denotes the number of ways of choosing m objects out of a total of N identical objects. Here m!, pronounced \u2018factorial m\u2019, denotes the product m \u00d7 (m \u2212 1) \u00d7 . , \u00d72 \u00d7 1.\n\nThe particular case of the binomial distribution for N = 1 is known as the Bernoulli distribution, and for large N the binomial distribution is approximately Gaussian. The conjugate prior for \u00b5 is the beta distribution. The Dirichlet is a multivariate distribution over K random variables 0 \u2a7d \u00b5k \u2a7d 1, where k = 1, . ., K, subject to the constraints Denoting \u00b5 = (\u00b51, . , \u00b5K)T and \u03b1 = (\u03b11,", "64672158-2e35-4ff6-8dec-12e7311cb925": "Another strategy is to place a penalty on the activations of the units in a neural network,  encouraging their activations to be sparse. This indirectly imposes a complicated penalty on the model parameters. We have already discussed (in section 7.1.2) how L! penalization induces a sparse parametrization\u2014meaning that many of the parameters become zero  (or close to zero). Representational sparsity, on the other hand, describes a  representation where many of the elements of the representation are zero (or close to zero)", "ccaad645-a382-4a72-ad97-e63bcb442052": "Because a goal of their study was to try to determine what minimal set of sensory cues are necessary for e\u21b5ective soaring, both to shed light on the cues birds might use for soaring and to minimize the sensing complexity required for automated glider soaring, the authors tried various sets of signals as input to the reinforcement learning agent.\n\nThey started by using state aggregation (Section 9.3) of a four-dimensional state space with dimensions giving local vertical wind speed, local vertical wind acceleration, torque depending on the di\u21b5erence between the vertical wind velocities at the left and right wing tips, and the local temperature. Each dimension was discretized into three bins: positive high, negative high, and small. Results, described below, showed that only two of these dimensions were critical for e\u21b5ective soaring behavior. The overall objective of thermal soaring is to gain as much altitude as possible from each rising column of air. Reddy et al. tried a straightforward reward signal that rewarded the agent at the end of each episode based on the altitude gained over the episode, a large negative reward signal if the glider touched the ground, and zero otherwise", "d39113ca-82a1-4845-9ac4-21babfd2713a": "The methods also di\u21b5er in several ways with respect to their e\ufb03ciency and speed of convergence. The remaining two chapters describe how these three classes of methods can be combined to obtain the best features of each of them.\n\nIn one chapter we describe how the strengths of Monte Carlo methods can be combined with the strengths of temporaldi\u21b5erence methods via multi-step bootstrapping methods. In the \ufb01nal chapter of this part of the book we show how temporal-di\u21b5erence learning methods can be combined with model learning and planning methods (such as dynamic programming) for a complete and uni\ufb01ed solution to the tabular reinforcement learning problem. The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions. This is what creates the need for active exploration, for an explicit search for good behavior. Purely evaluative feedback indicates how good the action taken was, but not whether it was the best or the worst action possible. Purely instructive feedback, on the other hand, indicates the correct action to take, independently of the action actually taken. This kind of feedback is the basis of supervised learning, which includes large parts of pattern classi\ufb01cation, arti\ufb01cial neural networks, and system identi\ufb01cation", "1773e277-0492-430f-a821-c77fbbbb1b56": "The techniques required to obtain good performance on GPU are very different from those used on CPU. For example, good CPU-based code is usually designed to read information from the cache as much as possible. On GPU, most writable memory locations are not cached, so it can actually be faster to compute the same value twice, rather than compute it once and read it back from memory. GPU code is also inherently multithreaded and the different threads must be coordinated with each other carefully. For example, memory operations are faster if they can be coalesced. Coalesced reads or writes occur when several threads can each read or write a value that they need simultaneously, as part of a single memory transaction. Different models of GPUs are able to coalesce different kinds of read patterns and different kinds of write patterns. Typically, memory operations are easier to coalesce if among n threads, thread i accesses byte i+ 7 of memory, and 7 is a multiple of some power of 2. The exact specifications differ between models of GPU.\n\nAnother common consideration for GPUs is making sure that each thread in a group executes the same instruction simultaneously. This means that branching can be difficult on GPU", "9a908ec4-f876-4595-8471-aaa1221026d4": "To evaluate a policy an evolutionary method holds the policy \ufb01xed and plays many games against the opponent, or simulates many games using a model of the opponent. The frequency of wins gives an unbiased estimate of the probability of winning with that policy, and can be used to direct the next policy selection. But each policy change is made only after many games, and only the \ufb01nal outcome of each game is used: what happens during the games is ignored.\n\nFor example, if the player wins, then all of its behavior in the game is given credit, independently of how speci\ufb01c moves might have been critical to the win. Credit is even given to moves that never occurred! Value function methods, in contrast, allow individual states to be evaluated. In the end, evolutionary and value function methods both search the space of policies, but learning a value function takes advantage of information available during the course of play. This simple example illustrates some of the key features of reinforcement learning methods. First, there is the emphasis on learning while interacting with an environment, in this case with an opponent player. Second, there is a clear goal, and correct behavior requires planning or foresight that takes into account delayed e\u21b5ects of one\u2019s choices", "0ca8624f-87bd-490b-9e59-01cfc0d35e42": "The state with all units off is assigned energy zero.\n\nWe can think of this as describing a model with a single variable that has n+ 1 states, or equivalently as a model that has n + 1 variables that assigns energy co to all but n+ 1 joint assignments of variables. While efficient, probabilistic max pooling does force the detector units to be mutually exclusive, which may be a useful regularizing constraint in some contexts or a harmful limit on model capacity in other contexts. It also does not support  1 + \" : vay 1 : 1\" : u : 1  https://www.deeplearningbook.org/contents/generative_models.html    to obtain th pOOlllg TexIOls,. UVELla J PLL POOLLLY, TERIOUs are Usually requirea to obtain the best performance from feedforward convolutional networks, so this constraint probably greatly reduces the performance of convolutional Boltzmann machines. Lee et al. demonstrated that probabilistic max pooling could be used to build convolutional deep Boltzmann machines. * This model is able to perform operations such as filling in missing portions of its input", "33fdadc0-0b4c-45fc-bcad-e8e5e00b5f0e": "It may simply be that convolutional networks were more computationally efficient than fully connected networks, so it was easier to run multiple experiments with them and tune their implementation and hyperparameters. Larger networks also seem  o be easier to train. With modern hardware, large fully connected networks appear to perform reasonably on many tasks, even when using datasets that were available and activation functions that were popular during the times when fully connected networks were believed not to work well.\n\nIt may be that the primary barriers to the success of neural networks were psychological (practitioners did not expect neural networks to work, so they did not make a serious effort to use neural networks). Whatever the case, it is fortunate that convolutional networks performed well decades ago. In many ways, they carried the torch for the rest of deep learning and paved the way to the acceptance of neural networks in general. Convolutional networks provide a way to specialize neural networks to work with data that has a clear grid-structured topology and to scale such models to very large size. This approach has been the most successful on a two-dimensional image topology. To process one-dimensional sequential data, we turn next to another powerful specialization of the neural networks framework: recurrent neural networks", "892813e8-31d5-4fe8-b490-8e0257f7e535": "Models that use probabilistic max pooling may still accept variably sized input images as long as  https://www.deeplearningbook.org/contents/generative_models.html    the output of the model is a feature map that can scale in size proportional to the input image. Pixels at the boundary of the image also pose some difficulty, which is exacer- bated by the fact that connections in a Boltzmann machine are symmetric. If we do not implicitly zero pad the input, there will be fewer hidden units than visible units, and the visible units at the boundary of the image will not be modeled well because they lie in the receptive field of fewer hidden units. However, if we do implicitly zero pad the input, then the hidden units at the boundary will be driven by fewer input pixels and may fail to activate when needed.\n\n20.7 Boltzmann Machines for Structured or Sequential Outputs  In the structured output scenario, we wish to train a model that can map from some input a to some output y, and the different entries of y are related to each other and must obey some constraints. For example, in the speech synthesis task,  681  CHAPTER 20", "0447c10a-fa4f-4fde-8b93-ada79326003a": "If we follow the evolution of the Hamiltonian equations for a \ufb01nite time, then the volume of this region will remain unchanged as will the value of H in this region, and hence the probability density, which is a function only of H, will also be unchanged. Although H is invariant, the values of z and r will vary, and so by integrating the Hamiltonian dynamics over a \ufb01nite time duration it becomes possible to make large changes to z in a systematic way that avoids random walk behaviour. Evolution under the Hamiltonian dynamics will not, however, sample ergodically from p(z, r) because the value of H is constant.\n\nIn order to arrive at an ergodic sampling scheme, we can introduce additional moves in phase space that change the value of H while also leaving the distribution p(z, r) invariant. The simplest way to achieve this is to replace the value of r with one drawn from its distribution conditioned on z. This can be regarded as a Gibbs sampling step, and hence from Section 11.3 we see that this also leaves the desired distribution invariant", "98935a67-e0f4-4bd3-93a8-f49f8a3a6c96": "Equivalently, the  383  CHAPTER 10.\n\nSEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  assumption is that the conditional probability distribution over the variables at time t+ 1 given the variables at time tis stationary , meaning that the relationship between the previous time step and the next time step does not depend on t. In principle, it would be possible to use \u00a2 as an extra input at each time step and let the learner discover any time-dependence while sharing as much as it can between different time steps. This would already be much better than using a different conditional probability distribution for each t, but the network would then have to extrapolate when faced with new values of t.  To complete our view of an RNN as a graphical model, we must describe how to draw samples from the model. The main operation that we need to perform is simply to sample from the conditional distribution at each time step. However, there is one additional complication. The RNN must have some mechanism for determining the length of the sequence. This can be achieved in various ways. When the output is a symbol taken from a vocabulary, one can add a special symbol corresponding to the end of a sequence", "41d908cc-e661-4c48-b968-ac6022aaac2b": "MACHINE LEARNING BASICS  https://www.deeplearningbook.org/contents/ml.html    = Vellx ee yoo (5.8) => Vw (xeerw - ye) (xOw - ye) =0 (5.9)  => Vw (wT x(a) Tx (train gy _ 2w X (train) T ay(train) + yin) Ty(eain) ) =0 (5.10) = 2X (train)T y (train), _ 2.X (train) Ty (train) =0 (5.11) =swe= (x(n Xx(rain)) -1 X (train) T ay(train) (5.12)  The system of equations whose solution is given by equation 5.12 is known as the normal equations. Evaluating equation 5.12 constitutes a simple learning algorithm. For an example of the linear regression learning algorithm in action, see figure 5.1. It is worth noting that the term linear regression is often used to refer to a slightly more sophisticated model with one additional parameter\u2014an intercept term b", "387dacac-d097-42b9-b327-c7167857a602": "(5.70) Expressed as a Gaussian conditional distribution on y \u2018\u2122), we have ply (train) | X (train) w) _ N (yt), X (ain)ay, I) (5.71)  ox exp (su _ X (train) w)! (y (train) _ xp) ) , (5.72)  where we follow the standard MSE formulation in assuming that the Gaussian variance on y is one. In what follows, to reduce the notational burden, we refer to (X (rain) y (train) as simply (X, y). To determine the posterior distribution over the model parameter vector w, we first need to specify a prior distribution.\n\nThe prior should reflect our naive belief about the value of these parameters. While it is sometimes difficult or unnatural to express our prior beliefs in terms of the parameters of the model, in practice we typically assume a fairly broad distribution, expressing a high degree of uncertainty about @", "5034e489-f5da-4091-860c-c21b63574b85": "In forming a GVF, for example, what should the cumulant, the policy, and the termination function be? The current state of the art is to select these manually, but far greater power and generality would come from making these task choices automatically, particularly when they derive from what the agent has previously constructed as a result of representation learning or experience with previous subproblems.\n\nIf GVF design is automated, then the design choices themselves will have to be explicitly represented. Rather than the task choices being in the mind of the designer and built into the code, they will have to be in the machine itself in such a way that they can be set and changed, monitored, \ufb01ltered, and searched among automatically. Tasks could then be built hierarchically upon others much like features are in an ANN. The tasks are the questions, and the contents of the ANN are the answers to those questions. We expect there will need to be a full hierarchy of questions to match the hierarchy of answers provided by modern deep learning methods. The \ufb01fth issue that we would like to highlight for future research is that of the interaction between behavior and learning via some computational analog of curiosity", "bed37ec7-ef79-415c-8dfb-e1b6be38a36a": "Thus if we take the exponential of both sides and normalize, we have In practice, we shall \ufb01nd it more convenient to work with the form (10.9) and then reinstate the normalization constant (where required) by inspection. This will become clear from subsequent examples. The set of equations given by (10.9) for j = 1, . , M represent a set of consistency conditions for the maximum of the lower bound subject to the factorization constraint. However, they do not represent an explicit solution because the expression on the right-hand side of (10.9) for the optimum q\u22c6 j (Zj) depends on expectations computed with respect to the other factors qi(Zi) for i \u0338= j.\n\nWe will therefore seek a consistent solution by \ufb01rst initializing all of the factors qi(Zi) appropriately and then cycling through the factors and replacing each in turn with a revised estimate given by the right-hand side of (10.9) evaluated using the current estimates for all of the other factors. Convergence is guaranteed because bound is convex with respect to each of the factors qi(Zi) . Our approach to variational inference is based on a factorized approximation to the true posterior distribution", "99b54f5c-56cf-429f-9055-0ad44976c955": "Exercises 8.1 (\u22c6) www By marginalizing out the variables in order, show that the representation (8.5) for the joint distribution of a directed graph is correctly normalized, provided each of the conditional distributions is normalized.\n\n8.2 (\u22c6) www Show that the property of there being no directed cycles in a directed graph follows from the statement that there exists an ordered numbering of the nodes such that for each node there are no links going to a lower-numbered node. 8.3 (\u22c6 \u22c6) Consider three binary variables a, b, c \u2208 {0, 1} having the joint distribution given in Table 8.2. Show by direct evaluation that this distribution has the property that a and b are marginally dependent, so that p(a, b) \u0338= p(a)p(b), but that they become independent when conditioned on c, so that p(a, b|c) = p(a|c)p(b|c) for both c = 0 and c = 1. 8.4 (\u22c6 \u22c6) Evaluate the distributions p(a), p(b|c), and p(c|a) corresponding to the joint distribution given in Table 8.2", "50dfe7b4-0191-492d-8710-5944fbe6a56f": "In this case, it is just as easy for you to cause your roommate to get sick as it is for your roommate to make you sick, so there is not a clean unidirectional narrative on which to base the model. This motivates using an undirected model. As with directed models, if two nodes in an undirected model are connected by an edge, then the random variables corresponding to those nodes interact with each other directly. Unlike directed models, the edge in an undirected model has no arrow and is not associated with a conditional probability distribution.\n\nWe denote the random variable representing your health as h,, the random variable representing your roommate\u2019s health as h,, and the random variable representing your colleague\u2019s health as h,. See figure 16.3 for a drawing of the graph representing this scenario. Formally, an undirected graphical model is a structured probabilistic model  563  CHAPTER 16. STRUCTURED PROBABILISTIC MODELS FOR DEEP LEARNING  OOO  Figure 16.3: An undirected graph representing how your roommate\u2019s healthh,, your  health h,, and your work colleague\u2019s health h affect each other", "4395da93-c986-419d-9450-f79711202cd1": "We introduced an algorithm that we deemed WGAN, an alternative to traditional GAN training. In this new model, we showed that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we showed that the corresponding optimization problem is sound, and provided extensive theoretical work highlighting the deep connections to other distances between distributions. We would like to thank Mohamed Ishmael Belghazi, Emily Denton, Ian Goodfellow, Ishaan Gulrajani, Alex Lamb, David Lopez-Paz, Eric Martin, Maxime Oquab, Aditya Ramesh, Ronan Riochet, Uri Shalit, Pablo Sprechmann, Arthur Szlam, Ruohan Wang, for helpful comments and advice. Martin Arjovsky and L\u00b4eon Bottou. Towards principled methods for training generative adversarial networks. In International Conference on Learning Representations, 2017. Under review. Gintare Karolina Dziugaite, Daniel M. Roy, and Zoubin Ghahramani", "7050aa3a-e762-4e42-b461-f338f44d3c7c": "This simpli\ufb01es simulations as well as theory, but we regard these simple eligibility traces as a placeholders for traces closer to Klopf\u2019s original conception, which would have computational advantages in complex reinforcement learning systems by re\ufb01ning the credit-assignment process. Klopf\u2019s hedonistic neuron hypothesis is not as implausible as it may at \ufb01rst appear. A well-studied example of a single cell that seeks some stimuli and avoids others is the bacterium Escherichia coli.\n\nThe movement of this single-cell organism is in\ufb02uenced by chemical stimuli in its environment, behavior known as chemotaxis. It swims in its liquid environment by rotating hairlike structures called \ufb02agella attached to its surface. (Yes, it rotates them!) Molecules in the bacterium\u2019s environment bind to receptors on its surface. Binding events modulate the frequency with which the bacterium reverses \ufb02agellar rotation. Each reversal causes the bacterium to tumble in place and then head o\u21b5 in a random new direction", "45aabc77-eb9e-42ad-a0ff-64e2089ee88f": "In particular, points that are misclassi\ufb01ed by one of the base classi\ufb01ers are given greater weight when used to train the next classi\ufb01er in the sequence. Once all the classi\ufb01ers have been trained, their predictions are then combined through a weighted majority voting scheme, as illustrated schematically in Figure 14.1.\n\nConsider a two-class classi\ufb01cation problem, in which the training data comprises input vectors x1, . , xN along with corresponding binary target variables t1, . , tN where tn \u2208 {\u22121, 1}. Each data point is given an associated weighting parameter wn, which is initially set 1/N for all data points. We shall suppose that we have a procedure available for training a base classi\ufb01er using weighted data to give a function y(x) \u2208 {\u22121, 1}. At each stage of the algorithm, AdaBoost trains a new classi\ufb01er using a data set in which the weighting coef\ufb01cients are adjusted according to the performance of the previously trained classi\ufb01er so as to give greater weight to the misclassi\ufb01ed data points. Finally, when the desired number of base classi\ufb01ers have been trained, they are combined to form a committee using coef\ufb01cients that give different weight to different base classi\ufb01ers", "81ab0a17-8487-43f7-97bf-3cdde90d43d5": "Because the table-based model has an astronomical number of parameters, it will require an astronomically large training set to fit accurately.\n\nAny such model will overfit the training set very badly unless additional assumptions are made linking the different entries in the table (as in back-off or smoothed n-gram models; section 12.4.1). e Runtime\u2014the cost of inference: Suppose we want to perform an inference task where we use our model of the joint distribution P(x) to compute some other distribution, such as the marginal distribution P(x1) or the conditional distribution P(x2 | x1). Computing these distributions will require summing across the entire table, so the runtime of these operations is as high as the intractable memory cost of storing the model. Runtime\u2014the cost of sampling: Likewise, suppose we want to draw a sample from the model. The naive way to do this is to sample some value u ~ U(0, 1), then iterate through the table, adding up the probability values until they exceed u and return the outcome corresponding to that position in the table. This requires reading through the whole table in the worst case, so it has the same exponential cost as the other operations", "66014063-43d7-4a25-ab9b-d72f3a5af9dc": "We therefore consider a Taylor expansion of ln f(z) centred on the mode z0 so that Note that the \ufb01rst-order term in the Taylor expansion does not appear since z0 is a local maximum of the distribution. Taking the exponential we obtain We can then obtain a normalized distribution q(z) by making use of the standard result for the normalization of a Gaussian, so that The Laplace approximation is illustrated in Figure 4.14. Note that the Gaussian approximation will only be well de\ufb01ned if its precision A > 0, in other words the stationary point z0 must be a local maximum, so that the second derivative of f(z) at the point z0 is negative.\n\nwhere \u03c3(z) is the logistic sigmoid function de\ufb01ned by \u03c3(z) = (1 + e\u2212z)\u22121. The left plot shows the normalized distribution p(z) in yellow, together with the Laplace approximation centred on the mode z0 of p(z) in red. The right plot shows the negative logarithms of the corresponding curves", "6b7cdaf3-334f-4d0e-b32c-f2b6fce7e2ab": "The final layer of a feedforward network is called the output layer. During neural network training, we drive f(x) to match f*(z). The training data provides us with noisy, approximate examples of f*(a) evaluated at different training points. Each example a is accompanied by a label y ~ f*(ax). The training examples specify directly what the output layer must do at each point x; it must produce a value that is close to y. The behavior of the other layers is not directly specified by the training data. The learning algorithm must decide how to use those layers to produce the desired output, but the training data do not say what each individual layer should do. Instead, the learning algorithm must decide how to use these layers to best implement an approximation of f*.\n\nBecause the training data does not show the desired output for each of these layers, they are called hidden layers. Finally, these networks are called neural because they are loosely inspired by neuroscience. Each hidden layer of the network is typically vector valued. The dimensionality of these hidden layers determines the width of the model", "877d85b0-9179-4027-829d-a1cf49f7d994": "REINFORCE with Baseline (episodic), for estimating \u21e1\u2713 \u21e1 \u21e1\u21e4 Input: a di\u21b5erentiable policy parameterization \u21e1(a|s, \u2713) Input: a di\u21b5erentiable state-value function parameterization \u02c6v(s,w) Algorithm parameters: step sizes \u21b5\u2713 > 0, \u21b5w > 0 Initialize policy parameter \u2713 2 Rd0 and state-value weights w 2 Rd (e.g., to 0) Generate an episode S0, A0, R1, . , ST \u22121, AT \u22121, RT , following \u21e1(\u00b7|\u00b7, \u2713) Loop for each step of the episode t = 0, 1, . , T \u2212 1: This algorithm has two step sizes, denoted \u21b5\u2713 and \u21b5w (where \u21b5\u2713 is the \u21b5 in (13.11)).\n\nChoosing the step size for values (here \u21b5w) is relatively easy; in the linear case we have rules of thumb for setting it, such as \u21b5w = 0.1/E much less clear how to set the step size for the policy parameters, \u21b5\u2713, whose best value depends on the range of variation of the rewards and on the policy parameterization", "579e51ed-ef22-471b-88bb-3b92e6b6c518": "Experiments conducted in the late 1980s and 1990s in the laboratory of neuroscientist Wolfram Schultz showed that dopamine neurons respond to rewarding events with substantial bursts of activity, called phasic responses, only if the animal does not expect those events, suggesting that dopamine neurons are signaling reward prediction errors instead of reward itself.\n\nFurther, these experiments showed that as an animal learns to predict a rewarding event on the basis of preceding sensory cues, the phasic activity of dopamine neurons shifts to earlier predictive cues while decreasing to later predictive cues. This parallels the backing-up e\u21b5ect of the TD error as a reinforcement learning agent learns to predict reward. Other experimental results \ufb01rmly establish that the phasic activity of dopamine neurons is a reinforcement signal for learning that reaches multiple areas of the brain by means of profusely branching axons of dopamine producing neurons. These results are consistent with the distinction we make between a reward signal, Rt, and a reinforcement signal, which is the TD error \u03b4t in most of the algorithms we present. Phasic responses of dopamine neurons are reinforcement signals, not reward signals", "7555b003-33cf-4901-b57c-45d7ca3001a2": "This is a useful property near a local minimum, but it can be a harmful property near a saddle point.\n\nAs discussed in section 8.2.3, Newton\u2019s method is only appropriate when the nearby critical point is a minimum (all the eigenvalues of the Hessian are positive), whereas gradient descent is not attracted to saddle points unless the gradient points toward them. Optimization algorithms that use only the gradient, such as gradient descent, are called first-order optimization algorithms. Optimization algorithms that also use the Hessian matrix, such as Newton\u2019s method, are called second-order optimization algorithms . The optimization algorithms employed in most contexts in this book are applicable to a wide variety of functions but come with almost no guarantees. Deep learning algorithms tend to lack guarantees because the family of functions used in deep learning is quite complicated. In many other fields, the dominant approach to optimization is to design optimization algorithms for a limited family of functions. In the context of deep learning, we sometimes gain some guarantees by restrict- ing ourselves to functions that are either Lipschitz continuous or have Lipschitz continuous derivatives", "6180a799-3ade-43e3-94e3-e7dd401fdc60": "We can further increase the capability of such models by allowing the mixing coef\ufb01cients themselves to be functions of the input variable, so that This is known as a mixture of experts model  in which the mixing coef\ufb01cients \u03c0k(x) are known as gating functions and the individual component densities pk(t|x) are called experts. The notion behind the terminology is that different components can model the distribution in different regions of input space (they are \u2018experts\u2019 at making predictions in their own regions), and the gating functions determine which components are dominant in which region.\n\nThe gating functions \u03c0k(x) must satisfy the usual constraints for mixing coef\ufb01cients, namely 0 \u2a7d \u03c0k(x) \u2a7d 1 and \ufffd k \u03c0k(x) = 1. They can therefore be represented, for example, by linear softmax models of the form (4.104) and (4.105). If the experts are also linear (regression or classi\ufb01cation) models, then the whole model can be \ufb01tted ef\ufb01ciently using the EM algorithm, with iterative reweighted least squares being employed in the M step . Such a model still has signi\ufb01cant limitations due to the use of linear models for the gating and expert functions", "0e84086f-8303-41b6-ad53-207a49bd65fe": "URs are often preparatory in some way, like the salivation of Pavlov\u2019s dog, or protective in some way, like an eye blink in response to something irritating to the eye, or freezing in response to seeing a predator. Experiencing the CS-US predictive relationship over a series of trials causes the animal to learn that the CS predicts the US so that the animal can respond to the CS with a CR that prepares the animal for, or protects it from, the predicted US. Some CRs are similar to the UR but begin earlier and di\u21b5er in ways that increase their e\u21b5ectiveness. In one intensively studied type of experiment, for example, a tone CS reliably predicts a pu\u21b5 of air (the US) to a rabbit\u2019s eye, triggering a UR consisting of the closure of a protective inner eyelid called the nictitating membrane. After one or more trials, the tone comes to trigger a CR consisting of membrane closure that begins before the air pu\u21b5 and eventually becomes timed so that peak closure occurs just when the air pu\u21b5 is likely to occur", "1fc596be-7f4f-4b50-a0d6-6c231e495aec": ") as a function and 6, as a value point.\n\nThe Taylor expansion of g  the value point 0 is:  g{ = VoL (61)  = VoL (0) + V3  = g) \u2014 gH g\u00ae = gf) \u2014 gH g\u00ae  Plug in the expanded form of g  9FOMAML = gs  JMAML = gt \u2014 7 = Io \u2014 ,(1 = gi  The Reptile gradient beco  f'(a)  9   9JFOMAML >= g 9JMAML = gl _ aH} gi JReptile \u2014 g + gi!) (1) 1  at anumber a is:  using Taylor expansion. Recall that Taylor expansion of a function  uO  9 (x \u2014a)?+---  rT (a \u2014 a)  a  0  ( at  1 L (5)(8; \u2014 9) + 9 VL (89)(O1 \u2014 09)? +. _ @ o3 n(t)g y(A(0))2 3 Vol Go)(95')\u00b0 +++  + O(a?) ( ; because 6 \u201409=\u2014ag\\  (1)  1", "f8ee8abd-2d09-458d-8f64-4f2da52cde8d": "Inclusion of this new data point leads to a revised distribution p(zn|x1, . , xn) for the state density shown in blue.\n\nWe see that observation of the data has shifted and narrowed the distribution compared to p(zn|x1, . , xn\u22121) (which is shown in dashed in the right-hand plot for comparison). If we consider a situation in which the measurement noise is small compared to the rate at which the latent variable is evolving, then we \ufb01nd that the posterior distribution for zn depends only on the current measurement xn, in accordance with Exercise 13.27 the intuition from our simple example at the start of the section. Similarly, if the latent variable is evolving slowly relative to the observation noise level, we \ufb01nd that the posterior mean for zn is obtained by averaging all of the measurements obtained up to that time. Exercise 13.28 One of the most important applications of the Kalman \ufb01lter is to tracking, and this is illustrated using a simple example of an object moving in two dimensions in Figure 13.22. So far, we have solved the inference problem of \ufb01nding the posterior marginal for a node zn given observations from x1 up to xn", "96ea456a-1e97-4180-95c8-91a0b908a348": "An eigenvector of a square matrix A is a nonzero vector v such that multi- plication by A alters only the scale of v:  Av =v. (2.39)  The scalar \\ is known as the eigenvalue corresponding to this eigenvector. (One can also find a left eigenvector such that v' A = dAv!, but we are usually concerned with right eigenvectors.) If v is an eigenvector of A, then so is any rescaled vector sv for s \u20ac R,s \u00a30. Moreover, sv still has the same eigenvalue.\n\nFor this reason, we usually look only for unit eigenvectors. Suppose that a matrix A has n linearly independent eigenvectors fo, weeg vl} with corresponding eigenvalues {\\1,...,\\n}", "b2394cec-139d-479c-b1f5-22ee380bc207": "nmurally form two groups comprising a set of ,mall ,'alues separated by a ,ign;flcant gap from a \",t of relativel)\" large ,'alues, indicating a natural cholcc f<>r AI, In practice. such a gap i, oflen ''''' seen Flgu.. 12.12 Synt\"'elic <lata illustrating too EM algorithm !of PCA defined by (12.58) and . (8) A data set X with the data points shown in 1JI'e\u00abl, t\"ll\"tM' W'i1!1l!>e t'IM pMdpal \"\"\"\"\"\"\",IS (shown as eigenveclor1 scaled by It>e squafll 'OOIS 04 the eigeJ'l\\lllluel)", "03894c46-768e-4c08-bfe1-de70e48e36f8": "PRACTICAL METHODOLOGY  network designed to detect a disease outputs \u00a7 = P(y =1| x), estimating the probability that a person whose medical results are described by features a has the disease. We choose to report a detection whenever this score exceeds some threshold. By varying the threshold, we can trade precision for recall. In many cases, we wish to summarize the performance of the classifier with a single number rather than a curve. To do so, we can convert precision p and recall r into an F-score given by 2pr a7  (11.1)  Another option is to report the total area lying beneath the PR curve. In some applications, it is possible for the machine learning system to refuse to make a decision. This is useful when the machine learning algorithm can estimate how confident it should be about a decision, especially if a wrong decision can be harmful and if a human operator is able to occasionally take over. The Street View transcription system provides an example of this situation. The task is to ranscribe the address number from a photograph to associate the location where che photo was taken with the correct address in a map", "3a32394a-dbfb-459e-87ec-972e611a98de": "Motivated by the weak relationship between dopamine neuron activity and stimulus-triggered eye and body movements, Romo and Schultz  and Schultz and Romo  took the \ufb01rst steps toward the reward prediction error hypothesis by recording the activity of dopamine neurons and muscle activity while monkeys moved their arms. They trained two monkeys to reach from a resting hand position into a bin containing a bit of apple, a piece of cookie, or a raisin, when the monkey saw and heard the bin\u2019s door open. The monkey could then grab and bring the food to its mouth. After a monkey became good at this, it was trained on two additional tasks. The purpose of the \ufb01rst task was to see what dopamine neurons do when movements are self-initiated. The bin was left open but covered from above so that the monkey could not see inside but could reach in from below", "8712678c-0160-44b4-b488-474f483dac2b": "MACHINE LEARNING BASICS  might be greater than zero, if two identical inputs are associated with different outputs) on any regression dataset.\n\nFinally, we can also create a nonparametric learning algorithm by wrapping a parametric learning algorithm inside another algorithm that increases the number of parameters as needed. For example, we could imagine an outer loop of learning that changes the degree of the polynomial learned by linear regression on top of a polynomial expansion of the input. The ideal model is an oracle that simply knows the true probability distribution that generates the data. Even such a model will still incur some error on many problems, because there may still be some noise in the distribution. In the case of supervised learning, the mapping from gx to y may be inherently stochastic, or y may be a deterministic function that involves other variables besides those included in aw. The error incurred by an oracle making predictions from the true distribution p(a, y) is called the Bayes error. Training and generalization error vary as the size of the training set varies. Expected generalization error can never increase as the number of training examples increases", "8d04c7df-2ca1-4789-8493-68550debbeb0": "Given a greedy strategy for growing the tree, there remains the issue of when to stop adding nodes. A simple approach would be to stop when the reduction in residual error falls below some threshold. However, it is found empirically that often none of the available splits produces a signi\ufb01cant reduction in error, and yet after several more splits a substantial error reduction is found. For this reason, it is common practice to grow a large tree, using a stopping criterion based on the number of data points associated with the leaf nodes, and then prune back the resulting tree. The pruning is based on a criterion that balances residual error against a measure of model complexity. If we denote the starting tree for pruning by T0, then we de\ufb01ne T \u2282 T0 to be a subtree of T0 if it can be obtained by pruning nodes from T0 (in other words, by collapsing internal nodes by combining the corresponding regions). Suppose the leaf nodes are indexed by \u03c4 = 1,", "8262d816-d9c8-41a7-9a2d-ea0e62ca4783": "This is achieved by including nonzero Gaussian means by the addition of Gaussian RBM-like hidden units.\n\nLike the mcRBM, the PoT conditional distribution over the observation is a multivariate Gaussian (with nondiagonal covariance) distribution; however, unlike the mcRBM, the complementary conditional distribution over the hidden variables is given by conditionally independent Gamma distributions. The Gamma distribution G(k, #) is a probability distribution over positive real numbers, with mean k6. It is not necessary to have a more detailed understanding of the Gamma distribution to understand the basic ideas underlying the mPoT model. The mPoT energy function is  Empor(@, h\u2019\u2122, RO) (20.48) _ m (c) 1 j)T 2 ; (c) = E,,(x,h'\u2122) +> (H (145 z) ) +a =a) town ). J (20.49)  where 9) is the covariance weight vector associated with unit no, and E,,(x, ni) is as defined in equation 20.44", "77b61e52-f3fa-462f-b629-2ca15ab12ca3": "We can try to approximate this distribution using some parametric distribution q(x|\u03b8), governed by a set of adjustable parameters \u03b8, for example a multivariate Gaussian.\n\nOne way to determine \u03b8 is to minimize the Kullback-Leibler divergence between p(x) and q(x|\u03b8) with respect to \u03b8. We cannot do this directly because we don\u2019t know p(x). Suppose, however, that we have observed a \ufb01nite set of training points xn, for n = 1, . , N, drawn from p(x). Then the expectation with respect to p(x) can be approximated by a \ufb01nite sum over these points, using (1.35), so that The second term on the right-hand side of (1.119) is independent of \u03b8, and the \ufb01rst term is the negative log likelihood function for \u03b8 under the distribution q(x|\u03b8) evaluated using the training set. Thus we see that minimizing this Kullback-Leibler divergence is equivalent to maximizing the likelihood function. Now consider the joint distribution between two sets of variables x and y given by p(x, y)", "a83ed730-39b2-41c4-bd81-592f8000f018": "We can then de\ufb01ne the distance between value functions using the norm \u00b5. For any value function v, the operation of \ufb01nding its closest value function in the subspace of representable value functions is a projection operation. We de\ufb01ne a The subspace of all value functions representable as  Unlike the original Bellman equation, for most function approximators (e the projected Bellman equation can be solved exactly. If it can\u2019t be solved minimize the mean-squared projected Bellman error: The minimum is achieved at the projection \ufb01xpoint, at which . Baird  extended it to TDL based on stochastic gradient d Engel, Mannor, and Meir  extended it to least squares (O(n2)) methods known as Gaussian Process TDL. In the literature, BE minimization is often referred to as Bellman residual minimization.\n\nThe third goal for approximation is to approximately solve the projected Bellman equation: Unlike the original Bellman equation, for most function approximators (e.g., linear ones) the projected Bellman equation can be solved exactly. The original TDL methods  converge to this solution, as does least-squares TDL", "e0ce4177-c3f8-4d3e-ad53-181d2de47df4": "All the four operations in EDA help improve the classification accuracy, but get to optimal at different a's.\n\nhttps://lilianweng.github.io/posts/2021-05-31-contrastive/  Contrastive Representation Learning | Lil'Log   SST-2 (N=7,447) CR (N=4,082) SUBJ (N=9,000)  Accuracy Accuracy  0.4 0.4 0.4 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 (a) Percent of Dataset (%) (b) Percent of Dataset (%) () Percent of Dataset (%) TREC (N=5,452) PC (N=39,418) All Datasets p> : | Beeeess gl g z 0.8 ey 0.8 8 0.8 : : 4 3 0.6 # 3 0.6 & 0.6 <7 <<\u201d s o 0.48 0.4 \u2014\u2014 Zz 04 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 (@) Percent of Dataset (%) () Percent of Dataset (%) re) Percent of Dataset (%)  In Contextual Augmentation , new substitutes for word w; at position 7 can be smoothly sampled from a given probability distribution, p(.| S \\ {w;}), which is predicted by a bidirectional LM like BERT", "a84c3f85-de80-4bc7-a176-fbbfa7f463e2": "The learner must predict the clean example a from its corrupted version @, or more generally predict the conditional probability distribution p(a | #). e Density estimation or probability mass function estimation: In the density estimation problem, the machine learning algorithm is asked to learn a function Pmode : R\u201d + R, where pode) (2) can be interpreted as a probability density function (if x is continuous) or a probability mass function (if x is discrete) on the space that the examples were drawn from. To do such a task well (we will specify exactly what that means when we discuss performance measures P), the algorithm needs to learn the structure of the data it has seen. It must know where examples cluster tightly and where they are unlikely to occur. Most of the tasks described above require the learning algorithm to at least implicitly capture the structure of the probability distribution. Density estimation enables us to explicitly capture that distribution.\n\nIn principle, we can then perform computations on that distribution to solve the other tasks as well. For example, if we have performed density estimation to obtain  https://www.deeplearningbook.org/contents/ml.html    a probability distribution p(\u00ae), we can use that distribution to solve the missing value imputation task", "4c3d7e50-e967-4944-bdde-cb63c1188957": "These two factors make it possible to learn a single model f that operates on all time steps and all sequence lengths, rather than needing to learn a separate model g\u00ae for all possible time steps. Learning a single shared model allows generalization to sequence lengths that did not appear in the training set, and enables the model to be estimated with far fewer training examples than would be required without parameter sharing. 371  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  Both the recurrent graph and the unrolled graph have their uses. The recurrent graph is succinct. The unfolded graph provides an explicit description of which computations to perform.\n\nThe unfolded graph also helps illustrate the idea of information flow forward in time (computing outputs and losses) and backward in time (computing gradients) by explicitly showing the path along which this information flows. 10.2. Recurrent Neural Networks  Armed with the graph-unrolling and parameter-sharing ideas of section 10.1, we can design a wide variety of recurrent neural networks", "4716faa4-711a-4a59-80af-91c4b6024d4b": "In the continuous version of the derivation, we use Leibniz\u2019s rule for differentiation under the integral sign to obtain the identity  Ve [00x = | Vorkxax.\n\n(18.14)  This identity is applicable only under certain regularity conditions on p and Vep(x). In measure theoretic terms, the conditions are: (1) The unnormalized distribution p must be a Lebesgue-integrable function of a for every value of 8. (2) The gradient Vo p(x) must exist for all @ and almost all g. (3) There must exist an integrable function R(a) that bounds V p(x) in the sense that max; \\s@- P(x) | < R(x) for all 6 and almost all x. Fortunately, most machine learning models of interest have  these properties. This identity Vo log Z= Ex p(x) Vo log p(x) (18. 15)  is the basis for a variety of Monte Carlo methods for approximately maximizing che likelihood of models with intractable partition functions. The Monte Carlo approach to learning undirected models provides an intuitive framework in which we can think of both the positive phase and the negative phase", "6300dea8-c6a7-43ae-9646-684b8e64b99f": "After replacing dot products with kernel evaluations, we can make predictions using the function  f(a) =b+ > ajk(w, x). (5.83)  This function is nonlinear with respect to x, but the relationship between (a) and f(a) is linear. Also, the relationship between @ and f(z) is linear. The kernel-based function is exactly equivalent to preprocessing the data by applying \u00a2(a) to all inputs, then learning a linear model in the new transformed space. The kernel trick is powerful for two reasons. First, it enables us to learn models that are nonlinear as a function of x using convex optimization techniques that are guaranteed to converge efficiently. This is possible because we consider @ fixed and optimize only a, that is, the optimization algorithm can view the decision function as being linear in a different space. Second, the kernel function k often  139  CHAPTER 5.\n\nMACHINE LEARNING BASICS  admits an implementation that is significantly more computationally efficient than naively constructing two \u00a2(a) vectors and explicitly taking their dot product", "e31ebb38-9d69-45cd-8e50-805432885e53": "We take an MNIST image with 784 pixels and transform it by translating it vertically.\n\nThe amount of vertical translation defines a coordinate along a 1-D manifold that traces out a curved path through image space. This plot shows a few points along this manifold. For visualization, we have projected the manifold into  https://www.deeplearningbook.org/contents/autoencoders.html    Z-D space using PUA. An \u201d-dimensional manifold has an 7-dimensional tangent plane at every point. This tangent plane touches the manifold exactly at that point and is oriented parallel to the surface at that point. It defines the space of directions in which it is possible to move while remaining on the manifold. This 1-D manifold has a single  tangent line. We indicate an example tangent line at one point, with an image showing how this tangent direction appears in image space. Gray pixels indicate pixels that do not change as we move along the tangent line, white pixels indicate pixels that brighten, and black pixels indicate pixels that darken. 514  CHAPTER 14", "3b640d00-da09-42f9-85d1-96159dea1643": "This follows from the fact that the composition of successive linear transformations is itself a linear transformation.\n\nHowever, if the number of hidden units is smaller than either the number of input or output units, then the transformations that the network can generate are not the most general possible linear transformations from inputs to outputs because information is lost in the dimensionality reduction at the hidden units. In Section 12.4.2, we show that networks of linear units give rise to principal component analysis. In general, however, there is little interest in multilayer networks of linear units. The network architecture shown in Figure 5.1 is the most commonly used one in practice. However, it is easily generalized, for instance by considering additional layers of processing each consisting of a weighted linear combination of the form (5.4) followed by an element-wise transformation using a nonlinear activation function. Note that there is some confusion in the literature regarding the terminology for counting the number of layers in such networks. Thus the network in Figure 5.1 may be described as a 3-layer network (which counts the number of layers of units, and treats the inputs as units) or sometimes as a single-hidden-layer network (which counts the number of layers of hidden units)", "80403ab1-2af7-4777-8b16-f87bfa49be79": "This allows us to use the inequality (10.152) and place a lower bound on L(q), which will therefore also be a lower bound on the log marginal likelihood With this factorization we can appeal to the general result (10.9) to \ufb01nd expressions for the optimal factors. Consider \ufb01rst the distribution q(w). Discarding terms that are independent of w, we have We now substitute for ln h(w, \u03be) using (10.153), and for ln p(w|\u03b1) using (10.165), giving We see that this is a quadratic function of w and so the solution for q(w) will be Gaussian", "4ac64b92-160f-4f1c-acc2-ab31f6bbac10": "However, it also presents us with some unique opportunities for achieving useful approximations. For example, in approximating optimal behavior, there may be many states that the agent faces with such a low probability that selecting suboptimal actions for them has little impact on the amount of reward the agent receives. Tesauro\u2019s backgammon player, for example, plays with exceptional skill even though it might make very bad decisions on board con\ufb01gurations that never occur in games against experts. In fact, it is possible that TD-Gammon makes bad decisions for a large fraction of the game\u2019s state set. The online nature of reinforcement learning makes it possible to approximate optimal policies in ways that put more e\u21b5ort into learning to make good decisions for frequently encountered states, at the expense of less e\u21b5ort for infrequently encountered states. This is one key property that distinguishes reinforcement learning from other approaches to approximately solving MDPs. Let us summarize the elements of the reinforcement learning problem that we have presented in this chapter. Reinforcement learning is about learning from interaction how to behave in order to achieve a goal.\n\nThe reinforcement learning agent and its environment interact over a sequence of discrete time steps", "db9a8df7-0010-4487-90fb-fd6f6484ed3d": "Therefore, when k=1:  gromamt = Vo,\u00a3(0,) = g? guamt = VoL (01) -(Z\u2014 aV QL 60) = gi!) \u2014 Hy 9 The Reptile gradient is defined as:  reptile = (99 \u2014 42)/a = gh) + gi  Up to now we have:  GT Rertile FoMAML  https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Lo:  Next let's try further expand g  f(x) that is differentiable  f(a) +  We can consider Vol (", "645789f5-15a1-4485-9ca1-2865485911b9": "Similar to the MLE framework for supervised learning, unsupervised learning via MLE can also be reformulated as a constraint optimization problem with entropy maximization. Consider learning a multivariate model with latent variables, where each data instance is partitioned into observed variables x \u2208 X and latent variables y \u2208 Y. For example, in the problem of image clustering, x \u2208 Rd is the observed image of d pixels and y \u2208 {1, . , K} is the unobserved cluster indicator (where K is the number of clusters).\n\nThe goal is to learn a model p\u03b8(x, y) that captures the joint distribution of x and y. Since y is unobserved, we minimize the Direct optimization of the marginal log-likelihood is typically intractable due to the summation over y. Earlier work thus developed di\ufb00erent solvers with varying levels of approximations. It can be shown that the intractable negative log-likelihood above can be upper bounded by a more tractable term known as the variational free energy", "6282bdfc-457b-49d0-b531-8398dac67745": "The lower bound L(q) then becomes a function of \u03c9, and we can exploit standard nonlinear optimization techniques to determine the optimal values for the parameters. An example of this approach, in which the variational distribution is a Gaussian and we have optimized with respect to its mean and variance, is shown in Figure 10.1. Here we consider an alternative way in which to restrict the family of distributions q(Z). Suppose we partition the elements of Z into disjoint groups that we denote by Zi where i = 1, . , M. We then assume that the q distribution factorizes with respect to these groups, so that It should be emphasized that we are making no further assumptions about the distribution. In particular, we place no restriction on the functional forms of the individual factors qi(Zi). This factorized form of variational inference corresponds to an approximation framework developed in physics called mean \ufb01eld theory .\n\nAmongst all distributions q(Z) having the form (10.5), we now seek that distribution for which the lower bound L(q) is largest", "e7fbad97-1744-42e8-ac8a-9e50362cb3ce": "Very few changes in the ideas already presented in this chapter are required in order to extend eligibility-traces to action-value methods.\n\nTo learn approximate action values, \u02c6q(s, a, w), rather than approximate state values, \u02c6v(s,w), we need to use the action-value form of the n-step return, from Chapter 10: with Gt:t+n .= Gt if t + n \u2265 T. Using this, we can form the action-value form of the truncated \u03bb-return, which is otherwise identical to the state-value form (12.9). The action-value form of the o\u270fine \u03bb-return algorithm (12.4) simply uses \u02c6q rather than \u02c6v: t:1. The compound backup diagram for this forward view is shown in The temporal-di\u21b5erence method for action values, known as Sarsa(\u03bb), approximates this forward view. It has the same update rule as given earlier for TD(\u03bb): except, naturally, using the action-value form of the TD error: Complete pseudocode for Sarsa(\u03bb) with linear function approximation, binary features, and either accumulating or replacing traces is given in the box on the next page", "6bcaaad7-e377-4253-a1ee-1bfee3a6d279": "Formally, the softmax function is given by  _exp(%i) (6.29)  cows  softmax(z); =  https://www.deeplearningbook.org/contents/mlp.html    S25 eXP( 27)  As with the logistic sigmoid, the use of the \u00a9xp function works well when training the softmax to output a target value y using maximum log-likelihood. In this case, we wish to maximize log P(y = 1; z) = logsoftmax(z), Defining the softmax in terms of exp is natural because the log in the log-likelihood can undo the exp of the softmax:  log softmax(z); = 2; \u2014 log )> exp(z;).\n\n(6.30) j  The first term of equation 6.30 shows that the input z; always has a direct contribution to the cost function. Because this term cannot saturate, we know that learning can proceed, even if the contribution of z; to the second term of equation 6.30 becomes very small. When maximizing the log-likelihood, the first term encourages z; to be pushed up, while the second term encourages all of z to be pushed down", "3fb6e2f0-108d-40e3-9695-fff53a66576e": "}4,  exp(v'fi/r) \u2014_exp(v\"f,/7)  P(ilv) = ~ Ve exp(vjfi/t) tp DL exp(vj.fi/7)  Because there is only one instance per class, the training is unstable and fluctuates a lot. To improve the training smoothness, they introduced an extra term for positive samples in the loss function based on the proximal optimization method.\n\nThe final NCE loss objective looks like:  https://lilianweng.github.io/posts/2021-05-31-contrastive/  Contrastive Representation Learning | Lil'Log   Linstance = \u2014Ep,  \u2014 MEp,  Pl h(i, v) = Plilv) rp) where the noise distribution is uniform P, = 1/N  where {yD} are embeddings stored in the memory bank from the previous iteration. The difference between iterations jv? \u2014 vi) [2 will gradually vanish as the learned embedding converges. MoCo & MoCo-V2  Momentum Contrast  provides a framework of unsupervised learning visual representation as a dynamic dictionary look-up", "836ea7f9-f883-4606-8d52-da1fd69f5102": "If we assume that the posterior distribution is sharply peaked, as will occur for sufficiently large data sets, then the re-estimation equations obtained by maximizing the marginal likelihood with respect to ai take the simple form which follows from (3.98), noting that the dimensionality of Wi is D. These reestimations are interleaved with the EM algorithm updates for determining Wand a 2 \u2022 The E-step equations are again given by (12.54) and (12.55). Similarly, the Mstep equation for a 2 is again given by (12.57). The only change is to the M-step equation for W, which is modified to give where A = diag(ai)' The value of I-\" is given by the sample mean, as before", "1cd06e7b-bd98-42c2-ac27-fb5eee09afc8": "This encourages the agent to keep testing all accessible state transitions and even to \ufb01nd long sequences of actions in order to carry out such tests.1 Of course all this testing has its cost, but in many cases, as in the shortcut maze, this kind of computational curiosity is well worth the extra exploration. Exercise 8.4 (programming) The exploration bonus described above actually changes the estimated values of states and actions. Is this necessary? Suppose the bonus \uf8ffp\u2327 was used not in updates, but solely in action selection.\n\nThat is, suppose the action selected was always that for which Q(St, a) + \uf8ff Exercise 8.5 How might the tabular Dyna-Q algorithm shown on page 164 be modi\ufb01ed to handle stochastic environments? How might this modi\ufb01cation perform poorly on changing environments such as considered in this section? How could the algorithm be modi\ufb01ed to handle stochastic environments and changing environments? \u21e4 In the Dyna agents presented in the preceding sections, simulated transitions are started in state\u2013action pairs selected uniformly at random from all previously experienced pairs. But a uniform selection is usually not the best; planning can be much more e\ufb03cient if simulated transitions and updates are focused on particular state\u2013action pairs. For example, consider 1The Dyna-Q+ agent was changed in two other ways as well", "3ec457f0-429f-4a97-b34d-5ea6a169c5d6": "Require: Initial parameter 0  kol  while stopping criterion not met do Sample a minibatch of m examples from the training set fa, nr 7 (my with corresponding targets y @, Compute gradient estimate: g < 1Ve Fi L(f (a; 6), y) Apply update: 6 <+ @\u2014 \u00ab4g kok+1  end while >  https://www.deeplearningbook.org/contents/optimization.html    descent, so batch gradient descent can_use a fixed learning rate.\n\nA sufficient condition to guarantee convergence of SGD is that  S- \u20ac, = 00, and (8.12) k=l SoG < x. (8.13) k=l  In practice, it is common to decay the learning rate linearly until iteration 7: \u20ack = (1 _ ajeg+ Qe, (8.14)  with a = x. After iteration 7, it is common to leave \u20ac constant. The learning rate may be chosen by trial and error, but it is usually best to choose it by monitoring learning curves that plot the objective function as a function of time", "c39a8e00-9164-4447-8b7d-7e7b6014025e": "; Set s\u00a24.1 =51,41,%+41 and preprocess $,,; =$(S++1) Store transition (\u00a2,,1.7150;41) in D Sample random minibatch of transitions (,a)s7505.\n\n:) from D  sety, = rj if episode terminates at step j +1 Y= n+ maxy O(4),.0'30-) otherwise Perform a gradient descent step on (\u00bb -@Q (65.45 0) ) ; with respect to the network parameters 0 Every C steps reset OQ=Q End For End For  https://lilianweng.github.io/posts/2018-02-19-rl-overview/  A (Long) Peek into Reinforcement Learning | Lil'Log   There are many extensions of DQN to improve the original design, such as DQN with dueling architecture  which estimates state-value function V(s) and advantage function A(s, a) with shared network parameters. Combining TD and MC Learning  In the previous section on value estimation in TD learning, we only trace one step further down the action chain when calculating the TD target. One can easily extend it to take multiple steps to estimate the return", "0b09f184-cfac-4160-85d6-634970f01784": "Rather than combine all of this heterogeneous information into one huge input space, it may be more effective to build one system to interpret the Xray images and a different one to interpret the blood data. As long as each of the two models gives posterior probabilities for the classes, we can combine the outputs systematically using the rules of probability. One simple way to do this is to assume that, for each class separately, the distributions of inputs for the X-ray images, denoted by xI, and the blood data, denoted by xB, are This is an example of conditional independence property, because the indepenSection 8.2 dence holds when the distribution is conditioned on the class Ck.\n\nThe posterior probability, given both the X-ray and blood data, is then given by Thus we need the class prior probabilities p(Ck), which we can easily estimate from the fractions of data points in each class, and then we need to normalize the resulting posterior probabilities so they sum to one. The particular conditional independence assumption (1.84) is an example of the naive Bayes model. Section 8.2.2 Note that the joint marginal distribution p(xI, xB) will typically not factorize under this model", "7acf5d79-6c60-4976-b423-8f6ad52093a4": "To see, this consider an extreme example in which we take a data set and double its size by duplicating every data point. Note that this simply multiplies the error function by a factor of 2 and so is equivalent to using the original error function. Batch methods will require double the computational effort to evaluate the batch error function gradient, whereas online methods will be unaffected.\n\nAnother property of on-line gradient descent is the possibility of escaping from local minima, since a stationary point with respect to the error function for the whole data set will generally not be a stationary point for each data point individually. Nonlinear optimization algorithms, and their practical application to neural network training, are discussed in detail in Bishop and Nabney . Our goal in this section is to \ufb01nd an ef\ufb01cient technique for evaluating the gradient of an error function E(w) for a feed-forward neural network. We shall see that this can be achieved using a local message passing scheme in which information is sent alternately forwards and backwards through the network and is known as error backpropagation, or sometimes simply as backprop. It should be noted that the term backpropagation is used in the neural computing literature to mean a variety of different things", "d6e480c0-8edc-4096-be91-65b2d8595dab": "Case study: Learning with imperfect experience.\n\nTo illustrate, let us consider a set of concrete problems concerning with distinct types of experience, which were often studied by researchers in di\ufb00erent areas: (1) The \ufb01rst problem is to integrate structured knowledge constraints in model training, where some components of the constraints, as well as the constraint weights, cannot be speci\ufb01ed a priori and are to be induced automatically; (2) The second problem concerns supervised learning, where one has access to only a small set of data instances with imbalanced labels, and we want to automate the data manipulation (e.g., augmentation and reweighting) to maximize the training performance; (3) The last problem is to stabilize the notoriously di\ufb03cult training of generative adversarial networks (GANs) for a wide range of image and text generation tasks. The three problems, though seemingly unrelated at \ufb01rst sight, can all be reduced to the same underlying problem in the uni\ufb01ed SE view, namely, learning with imperfect experience f (e.g., underspeci\ufb01ed knowledge constraints, small imbalanced data, and unstable discriminator). We want to automatically adapt/improve the imperfect experience in order to better supervise the target model training", "d21ac959-788a-4eb1-bb07-47eb74c599b3": "For example, we know that f(# \u2014 \u20ac signe (x))) is less than fay for small enough \u20ac. We can thus reduce f(x) by moving # in small steps with the opposite sign of the derivative. This technique is called gradient descent . See figure 4.1 for an example of this technique. When f\u2019(x) = 0, the derivative provides no information about which direction to move. Points where f'(2) = 0 are known as critical points, or stationary points. A local minimum is a point where f(x) is lower than at all neighboring points, so it is no longer possible to decrease f(x) by making infinitesimal steps. A local maximum is a point where f(x) is higher than at all neighboring points,  81  CHAPTER 4. NUMERICAL COMPUTATION  Minimum Maximum Saddle point  Figure 4.2: Types of critical points. Examples of the three types of critical points in one dimension", "076d08a3-7609-458d-b30f-138467eb775e": "To model this we can use a linear dynamical system governed by (13.75) and (13.76), with latent variables {z1, . , zN} in which C becomes the identity matrix and where the transition probability A = 0 because the observations are independent. Let the parameters m0 and V0 of the initial state be denoted by \u00b50 and \u03c32 0, respectively, and suppose that \u03a3 becomes \u03c32. Write down the corresponding Kalman \ufb01lter equations starting from the general results (13.89) and (13.90), together with (13.94) and (13.95). Show that these are equivalent to the results (2.141) and (2.142) obtained directly by considering independent data.\n\n13.26 (\u22c6 \u22c6 \u22c6) Consider a special case of the linear dynamical system of Section 13.3 that is equivalent to probabilistic PCA, so that the transition matrix A = 0, the covariance \u0393 = I, and the noise covariance \u03a3 = \u03c32I. By making use of the matrix inversion identity (C.7) show that, if the emission density matrix C is denoted W, then the posterior distribution over the hidden states de\ufb01ned by (13.89) and (13.90) reduces to the result (12.42) for probabilistic PCA", "84fe7710-a642-4253-9f47-10ea3c4cd26b": "distribution (5.16) for a multioutput neural network is equivalent to minimizing the sum-of-squares error function (5.11). where y(x, w) is the output of a neural network with input vector x and weight vector w, and \u03a3 is the covariance of the assumed Gaussian noise on the targets. Given a set of independent observations of x and t, write down the error function that must be minimized in order to \ufb01nd the maximum likelihood solution for w, if we assume that \u03a3 is \ufb01xed and known. Now assume that \u03a3 is also to be determined from the data, and write down an expression for the maximum likelihood solution for \u03a3. Note that the optimizations of w and \u03a3 are now coupled, in contrast to the case of independent target variables discussed in Section 5.2.\n\n5.4 (\u22c6 \u22c6) Consider a binary classi\ufb01cation problem in which the target values are t \u2208 {0, 1}, with a network output y(x, w) that represents p(t = 1|x), and suppose that there is a probability \u03f5 that the class label on a training data point has been incorrectly set", "da251e0f-a917-4ba3-87bc-26f3969b72d0": "We shall see an example of this shortly. Here, however, we note that the problem is suf\ufb01ciently simple that a closed form solution can be found. In particular, because E = m1 and E = m2, we see that the two equations are satis\ufb01ed if we take E = \u00b51 and E = \u00b52, and it is easily shown that this is the only solution provided the distribution is nonsingular. This result is illustrated in Figure 10.2(a).\n\nWe see that the Exercise 10.2 mean is correctly captured but that the variance of q(z) is controlled by the direction of smallest variance of p(z), and that the variance along the orthogonal direction is signi\ufb01cantly under-estimated. It is a general result that a factorized variational approximation tends to give approximations to the posterior distribution that are too compact. By way of comparison, suppose instead that we had been minimizing the reverse Kullback-Leibler divergence KL(p\u2225q). As we shall see, this form of KL divergence is used in an alternative approximate inference framework called expectation propagation", "64886139-4105-46c1-b29c-53cf6f924c71": "Let's denote the learner model as My parameterized by 0, the meta-learner as Ro with parameters O, and the loss function L.  Why LSTM? The meta-learner is modeled as a LSTM, because:  There is similarity between the gradient-based update in backpropagation and the cell-state update in LSTM. Knowing a history of gradients benefits the gradient update; think about how momentum works. The update for the learner's parameters at time step t with a learning rate a; is: = O14 _ arVo, Lt  It has the same form as the cell state update in LSTM, if we set forget gate f; = 1, input gate it = My, Cell state c, = 6, and new cell state & = \u2014Vo,_,L::  c= frOgyt+tuO& = O44 _ arVo, Lt  While fixing f; = 1 and 7; = a; might not be the optimal, both of them can be learnable and  https://lilianweng.github.io/posts/2018-11-30-meta-learning/  Meta-Learning: Learning to Learn Fast | Lil'Log   adaptable to different datasets", "b95fcad2-e049-4c24-8f26-8dcb727b69e3": "Although a model is required, the model need only generate sample transitions, not the complete probability distributions of all possible transitions that is required for dynamic programming (DP). In surprisingly many cases it is easy to generate experience sampled according to the desired probability distributions, but infeasible to obtain the distributions in explicit form. Monte Carlo methods are ways of solving the reinforcement learning problem based on averaging sample returns. To ensure that well-de\ufb01ned returns are available, here we de\ufb01ne Monte Carlo methods only for episodic tasks. That is, we assume experience is divided into episodes, and that all episodes eventually terminate no matter what actions are selected. Only on the completion of an episode are value estimates and policies changed.\n\nMonte Carlo methods can thus be incremental in an episode-by-episode sense, but not in a step-by-step (online) sense. The term \u201cMonte Carlo\u201d is often used more broadly for any estimation method whose operation involves a signi\ufb01cant random component. Here we use it speci\ufb01cally for methods based on averaging complete returns (as opposed to methods that learn from partial returns, considered in the next chapter)", "b0e3e4f6-ebaa-4d12-95a9-5ccb6e0b72d0": "In principle, updates can be distributed any way one likes (to assure convergence, all states or state\u2013action pairs must be visited in the limit an in\ufb01nite number of times; although an exception to this is discussed in Section 8.7 below), but in practice exhaustive sweeps are often used. The second approach is to sample from the state or state\u2013action space according to some distribution. One could sample uniformly, as in the Dyna-Q agent, but this would su\u21b5er from some of the same problems as exhaustive sweeps. More appealing is to distribute updates according to the on-policy distribution, that is, according to the distribution observed when following the current policy.\n\nOne advantage of this distribution is that it is easily generated; one simply interacts with the model, following the current policy. In an episodic task, one starts in a start state (or according to the starting-state distribution) and simulates until the terminal state. In a continuing task, one starts anywhere and just keeps simulating. In either case, sample state transitions and rewards are given by the model, and sample actions are given by the current policy. In other words, one simulates explicit individual trajectories and performs updates at the state or state\u2013action pairs encountered along the way", "2d685e1c-1dee-413f-a1c3-d7c1b9f252ab": "14.9 (\u22c6) www Show that the sequential minimization of the sum-of-squares error function for an additive model of the form (14.21) in the style of boosting simply involves \ufb01tting each new base classi\ufb01er to the residual errors tn\u2212fm\u22121(xn) from the previous model. 14.10 (\u22c6) Verify that if we minimize the sum-of-squares error between a set of training values {tn} and a single predictive value t, then the optimal solution for t is given by the mean of the {tn}. 14.11 (\u22c6 \u22c6) Consider a data set comprising 400 data points from class C1 and 400 data points from class C2. Suppose that a tree model A splits these into (300, 100) at the \ufb01rst leaf node and (100, 300) at the second leaf node, where (n, m) denotes that n points are assigned to C1 and m points are assigned to C2. Similarly, suppose that a second tree model B splits them into (200, 400) and (200, 0)", "48446fe6-64bf-493f-a48e-22b8cf61ddb3": "Regularized autoencoders defy such an interpretation because the regularizer depends on the data and is therefore by definition not a prior in the formal sense of the word. We can still think of these regularization ferms as implicitly expressing a preference over functions. Rather than thinking of the sparsity penalty as a regularizer for the copying ask, we can think of the entire sparse autoencoder framework as approximating  502  CHAPTER 14. AUTOENCODERS  maximum likelihood training of a generative model that has latent variables. Suppose we have a model with visible variables x and latent variables h, with an explicit joint distribution Dmodel(@, A.) = Pmodel(h)Pmodel (e | h).\n\nWe refer to Pmodel(h) as the model\u2019s prior distribution over the latent variables, representing the model\u2019s beliefs prior to seeing xz. This is different from the way we have previously used the word \u201cprior,\u201d to refer to the distribution p(@) encoding our beliefs about the model\u2019s parameters before we have seen the training data", "3214e675-f799-440f-b57f-40ff053fb48f": "For example, we may assume that y = f(a) +, where e stands for the part of y that is not predictable from z. In function estimation, we are interested in approximating f with a model or  estimate f . Function estimation is really just the same as estimating a parameter 0; the function estimator f is simply a point estimator in function space. The linear regression example (discussed in section 5.1.4) and the polynomial regression example (discussed in section 5.2) both illustrate scenarios that may be interpreted as either estimating a parameter w or estimating a function f mapping from x to y. We now review the most commonly studied properties of point estimators and discuss what they tell us about these estimators.\n\n5.4.2 Bias The bias of an estimator is defined as bias( Am) = E(A@mn) \u2014 9, (5.20)  where the expectation is over the data (seen as samples from a random variable) and @ is the true underlying value of 8 used to define the data-generating distri- bution. An estimator @m is said to be unbiased if bias(@;,) = 0, which implies that E (Om) = 6", "0ff34a2b-eed1-4937-be65-0e6ecccae744": "This is probably due to their great simplicity: they can be applied online, with a minimal amount of computation, to experience generated from interaction with an environment; they can be expressed nearly completely by single equations that can be implemented with small computer programs. In the next few chapters we extend these algorithms, making them slightly more complicated and signi\ufb01cantly more powerful. All the new algorithms will retain the essence of those introduced here: they will be able to process experience online, with relatively little computation, and they will be driven by TD errors. The special cases of TD methods introduced in the present chapter should rightly be called one-step, tabular, model-free TD methods. In the next two chapters we extend them to n-step forms (a link to Monte Carlo methods) and forms that include a model of the environment (a link to planning and dynamic programming). Then, in the second part of the book we extend them to various forms of function approximation rather than tables (a link to deep learning and arti\ufb01cial neural networks).\n\nFinally, in this chapter we have discussed TD methods entirely within the context of reinforcement learning problems, but TD methods are actually more general than this. They are general methods for learning to make long-term predictions about dynamical systems", "f564a1f9-3a01-4bb5-8600-d55c6c698006": "Contemporary Animal Learning Theory. Cambridge University Press. Dickinson, A. Actions and habits: the development of behavioral autonomy. Phil. Trans.\n\nDickinson, A., Balleine, B. W. The role of learning in motivation. In C. R. Gallistel (Ed. ), Stevens\u2019 Handbook of Experimental Psychology, volume 3, pp. 497\u2013533. Wiley, NY. Dietterich, T. G., Buchanan, B. G. The role of the critic in learning systems. In O. G. Selfridge, E. L. Rissland, and M. A. Arbib (Eds. ), Adaptive Control of Ill-De\ufb01ned Systems, pp. 127\u2013147. Plenum Press, NY. Dietterich, T. G., Flann, N. S", "cd465751-4bbf-4c83-8104-66800df47cd2": "In Advances in Neural Information Processing Systems, pages 6000\u20136010. Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. 2008. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pages 1096\u20131103. ACM. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018a. Glue: A multi-task benchmark and analysis platform for natural language understanding.\n\nIn Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353\u2013355. Wei Wang, Ming Yan, and Chen Wu. 2018b. Multigranularity hierarchical attention fusion networks for reading comprehension and question answering. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics. Alex Warstadt, Amanpreet Singh, and Samuel R Bowman", "5c0ba23b-9659-4326-a03e-176e747de387": "CONDENSATION \u2013 conditional density propagation for visual tracking. International Journal of Computer Vision 29(1), 5\u201318. Ito, Y. Representation of functions by superpositions of a step or sigmoid function and their applications to neural network theory. Neural Networks 4(3), 385\u2013394. Jaakkola, T. and M. I. Jordan . Bayesian parameter estimation via variational methods. Statistics and Computing 10, 25\u201337. Jaakkola, T. S. Tutorial on variational approximation methods. In M. Opper and D. Saad (Eds. ), Advances in Mean Field Methods, pp. 129\u2013159. MIT Press. Jaakkola, T. S. and D. Haussler . Exploiting generative models in discriminative classi\ufb01ers. In M. S. Kearns, S. A. Solla, and D", "683f18f2-03bd-4efe-9e9e-3da4d246fb02": "The total number of independent parameters {wij} and {vi} in the covariance matrix is therefore D(D + 1)/2 corresponding to a general symmetric covariance matrix. Section 2.3 Graphs having some intermediate level of complexity correspond to joint Gaussian distributions with partially constrained covariance matrices. Consider for example the graph shown in Figure 8.14, which has a link missing between variables x1 and x3. Using the recursion relations (8.15) and (8.16), we see that the mean and covariance of the joint distribution are given by Exercise 8.7 We can readily extend the linear-Gaussian graphical model to the case in which the nodes of the graph represent multivariate Gaussian variables. In this case, we can write the conditional distribution for node i in the form where now Wij is a matrix (which is nonsquare if xi and xj have different dimensionalities)", "9cc1bdac-96ef-459a-bc6f-826321854da2": "de Freitas, A. Doucet, and M. I. Jordan . An introduction to MCMC for machine learning. Machine Learning 50, 5\u201343. Attias, H. Independent factor analysis. Neural Computation 11(4), 803\u2013851. Attias, H. Inferring parameters and structure of latent variable models by variational Bayes. In K. B. Laskey and H. Prade (Eds. ), Bishop, C. M. A fast procedure for retraining the multilayer perceptron. International Journal of Neural Systems 2(3), 229\u2013236. Bishop, C. M. .\n\nExact calculation of the Hessian matrix for the multilayer perceptron. Neural Computation 4(4), 494\u2013501. Bishop, C. M. Novelty detection and neural network validation. IEE Proceedings: Vision, Image and Signal Processing 141(4), 217\u2013222", "6b134912-d268-45d0-a034-4cec16bed1c7": "when downsampling images to different resolutions.\n\nThe researchers found that com- posing an ensemble of models trained with high and low-resolution images performed better than any one model individually. This ensemble prediction is found by averag- ing the softmax predictions. The models trained on 256 x 256 images and 512 x 512 images achieve 7.96% and 7.42% top-5 error rates, respectively. When aggregated they achieved a lower top-5 error rate of 6.97%. Therefore, different downsampled images can be viewed as another Data Augmentation scheme (Fig. 33). With the advance of Super-Resolution Convolutional Neural Networks presented by Chong et al. or SRGANs, Super-Resolution Generative Adversarial Networks, presented by Ledig et al. , it is interesting to consider if upsampling images to an even higher resolution would result in better models. Quality upsampling on CIFAR-10 images from even 32 x 32 x 3 to 64 x 64 x 3 could lead to better and more robust image classifiers. Resolution is also a very important topic with GANs", "50ec0fdb-9bf2-4fe4-811b-5a4f435a10ab": "Other sim- ple image manipulations such as color augmentations, mixing images, kernel filters, and random erasing can also be extended to oversample data in the same manner as geo- metric augmentations. This can be useful for ease of implementation and quick exper- imentation with different class ratios. One problem of oversampling with basic image transformations is that it could cause overfitting on the minority class which is being oversampled. The biases present in the minority class are more prevalent post-sampling with these techniques. Oversampling methods based on Deep Learning such as adversarial training, Neu- ral Style Transfer, GANs, and meta-learning schemes can also be used as a more intel- ligent oversampling strategy. Neural Style Transfer is an interesting way to create new images. These new images can be created either through extrapolating style with a for-  eign style or by interpolating styles amongst instances within the dataset", "d938bc88-29a0-4811-9a1a-cf3c43e8d217": "If we have multiple target variables, and we assume that they are independent conditional on x and w with shared noise precision \u03b2, then the conditional distribution of the target values is given by Following the same argument as for a single target variable, we see that the maximum likelihood weights are determined by minimizing the sum-of-squares error function (5.11). The noise precision is then given by Exercise 5.2 where K is the number of target variables. The assumption of independence can be dropped at the expense of a slightly more complex optimization problem. Exercise 5.3 Recall from Section 4.3.6 that there is a natural pairing of the error function (given by the negative log likelihood) and the output unit activation function. In the regression case, we can view the network as having an output activation function that is the identity, so that yk = ak. The corresponding sum-of-squares error function has the property \u2202E \u2202ak = yk \u2212 tk (5.18) which we shall make use of when discussing error backpropagation in Section 5.3", "4d476b7f-604e-4335-b0c0-f07ffbc5c400": "Similarly, a nonzero value for \ufffdan implies \u03f5 +\ufffd\u03ben \u2212 yn + tn = 0, and such points must lie either on or below the lower boundary of the \u03f5-tube. Furthermore, the two constraints \u03f5+\u03ben +yn \u2212tn = 0 and \u03f5+\ufffd\u03ben \u2212yn +tn = 0 are incompatible, as is easily seen by adding them together and noting that \u03ben and \ufffd\u03ben are nonnegative while \u03f5 is strictly positive, and so for every data point xn, either an or \ufffdan (or both) must be zero. The support vectors are those data points that contribute to predictions given by (7.64), in other words those for which either an \u0338= 0 or \ufffdan \u0338= 0. These are points that lie on the boundary of the \u03f5-tube or outside the tube. All points within the tube have an = \ufffdan = 0", "a6616fe6-583c-420c-93a5-7e84a3064898": "If we use a learning rate of \u00a2, then the new point a will be given by 2 \u2014 eg. Substituting this into our approximation, we obtain  f(e@\u00ae \u2014 eg) & f(x) - eg'g+ 5\u00a29 Ho. (4.9)  There are three terms here: the original value of the function, the expected improvement due to the slope of the function, and the correction we must apply to account for the curvature of the function. When this last term is too large, the gradient descent step can actually move uphill. When g'H g is zero or negative, the Taylor series approximation predicts that increasing \u00a2 forever will decrease f forever. In practice, the Taylor series is unlikely to remain accurate for large e\u20ac, so one must resort to more heuristic choices of \u20ac in this case.\n\nWhen g' Hg is positive, solving for the optimal step size that decreases the Taylor series approximation of  the function the most yields \u00ab__9'9 (4.10) e=", "5c6caec3-25ab-4c19-a3be-3ec625705735": "Christensen and Korf  experimented with regression methods for modifying coe\ufb03cients of linear value function approximations in the game of chess. Chapman and Kaelbling  and Tan  adapted decision-tree methods for learning value functions. Explanation-based learning methods have also been adapted for learning value functions, yielding compact representations . In this chapter we return to the control problem, now with parametric approximation of the action-value function \u02c6q(s, a, w) \u21e1 q\u21e4(s, a), where w 2 Rd is a \ufb01nite-dimensional weight vector. We continue to restrict attention to the on-policy case, leaving o\u21b5-policy methods to Chapter 11. The present chapter features the semi-gradient Sarsa algorithm, the natural extension of semi-gradient TD(0) (last chapter) to action values and to on-policy control", "fb1d2aca-a571-4992-9e94-1d6da790a6d0": "Learning a part-of-speech tagger from two hours of annotation. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 138\u2013147, Atlanta, Georgia. Association for Computational Linguistics. Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011.\n\nDomain adaptation for large-scale sentiment classi\ufb01cation: A deep learning approach. In International Conference of Machine Learning. Karan Goel, Nazneen Rajani, Jesse Vig, Samson Tan, Jason Wu, Stephan Zheng, Caiming Xiong, Mohit Bansal, and Christopher R\u00b4e. 2021. Robustness gym: Unifying the nlp evaluation landscape. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Z. Ghahramani, M. Welling, C", "43bd21b4-965f-4b85-9a05-c35c81a387e7": "They are interesting in the context of regularization, however, because one can reduce the error rate on the original i.i.d. est set via adversarial training\u2014training on adversarially perturbed examples from the training set . Goodfellow et al. showed that one of the primary causes of these adversarial examples is excessive linearity. Neural networks are built out of primarily linear building blocks. In some experiments the overall function they implement proves to be highly linear as a result. These linear functions are easy o optimize. Unfortunately, the value of a linear function can change very rapidly if it has numerous inputs. If we change each input by e, then a linear function with weights w can change by as much as \u00a2||w||1, which can be a very large amount if w is highdimensional.\n\nAdversarial training discourages this highly sensitive locally linear behavior by encouraging the network to be locally constant in the neighborhood of the training data. This can be seen as a way of explicitly introducing a local constancy prior into supervised neural nets. Adversarial training helps to illustrate the power of using a large function family in combination with aggressive regularization", "81a48e99-f0e2-4790-a465-a516310f84a5": "Show that the predictive variance is given by (7.91).\n\n7.15 (\u22c6 \u22c6) www Using the results (7.94) and (7.95), show that the marginal likelihood (7.85) can be written in the form (7.96), where \u03bb(\u03b1n) is de\ufb01ned by (7.97) and the sparsity and quality factors are de\ufb01ned by (7.98) and (7.99), respectively. 7.16 (\u22c6) By taking the second derivative of the log marginal likelihood (7.97) for the regression RVM with respect to the hyperparameter \u03b1i, show that the stationary point given by (7.101) is a maximum of the marginal likelihood. 7.19 (\u22c6 \u22c6) Verify that maximization of the approximate log marginal likelihood function (7.114) for the classi\ufb01cation relevance vector machine leads to the result (7.116) for re-estimation of the hyperparameters. Probabilities play a central role in modern pattern recognition. We have seen in Chapter 1 that probability theory can be expressed in terms of two simple equations corresponding to the sum rule and the product rule. All of the probabilistic inference and learning manipulations discussed in this book, no matter how complex, amount to repeated application of these two equations", "36f12e29-0206-4db4-8356-80ba91cad703": "Would this have any e\u21b5ect, or would it leave the task unchanged as in the continuing task above? Why or why not? Give an example. \u21e4 task, we count a penalty (negative reward) of \u22121 for each stroke until we hit the ball into the hole. The state is the location of the ball. The value of a state is the negative of the number of strokes to the hole from that location. Our actions are how we aim and swing at the ball, of course, and which club we select. Let us take the former as given and consider just the choice of club, which we assume is either a putter or a driver.\n\nThe upper part of Figure 3.3 shows a possible state-value function, vputt(s), for the policy that Exercise 3.18 The value of a state depends on the values of the actions possible in that state and on how likely each action is to be taken under the current policy. We can think of this in terms of a small backup diagram rooted at the state and considering each possible action: Give the equation corresponding to this intuition and diagram for the value at the root node, v\u21e1(s), in terms of the value at the expected leaf node, q\u21e1(s, a), given St = s", "12efb870-69b6-4ea1-bb48-50318caaea2a": "These methods require an additional vector of \u201cprovisional weights\u201d that keep track of updates which have been made but may need to be retracted (or emphasized) depending on the actions taken later. The state and state\u2013action versions of these methods are called PTD(\u03bb) and PQ(\u03bb) respectively, where the \u2018P\u2019 stands for Provisional. The practical consequences of all these new o\u21b5-policy methods have not yet been established.\n\nUndoubtedly, issues of high variance will arise as they do in all o\u21b5-policy methods using importance sampling (Section 11.9). If \u03bb < 1, then all these o\u21b5-policy algorithms involve bootstrapping and the deadly triad applies (Section 11.3), meaning that they can be guaranteed stable only for the tabular case, for state aggregation, and for other limited forms of function approximation. For linear and more-general forms of function approximation the parameter vector may diverge to in\ufb01nity as in the examples in Chapter 11. As we discussed there, the challenge of o\u21b5-policy learning has two parts", "ca998af9-e801-4220-8b3e-ac9ccea8860b": "https://lilianweng.github.io/posts/2021-05-31-contrastive/     Contrastive Representation Learning | Lil'Log   Memory Bank  Computing embeddings for a large number of negative samples in every batch is extremely expensive. One common approach is to store the representation in memory to trade off data staleness for cheaper compute. Instance Discrimination with Memoy Bank  Instance contrastive learning  pushes the class-wise supervision to the extreme by considering each instance as a distinct class of its own. It implies that the number of \u201cclasses\u201d  will be the same as the number of samples in the training dataset.\n\nHence, it is unfeasible to train a softmax layer with these many heads, but instead it can be approximated by NCE. CNN backbone ja 1th image lowdim \u2014 L2norm Ya", "5dc7c7e5-8bda-40f1-bf4d-2afd0d366515": "If we now multiply both sides by \u03c0k and sum over k making use of the constraint (9.9), we \ufb01nd \u03bb = \u2212N. Using this to eliminate \u03bb and rearranging we obtain so that the mixing coef\ufb01cient for the kth component is given by the average responsibility which that component takes for explaining the data points. It is worth emphasizing that the results (9.17), (9.19), and (9.22) do not constitute a closed-form solution for the parameters of the mixture model because the responsibilities \u03b3(znk) depend on those parameters in a complex way through (9.13). However, these results do suggest a simple iterative scheme for \ufb01nding a solution to the maximum likelihood problem, which as we shall see turns out to be an instance of the EM algorithm for the particular case of the Gaussian mixture model. We \ufb01rst choose some initial values for the means, covariances, and mixing coef\ufb01cients. Then we alternate between the following two updates that we shall call the E step algorithm in Figure 9.1. See the text for details. and the M step, for reasons that will become apparent shortly", "fdd21fd0-0cbd-4e32-a04d-7e3b7249ac89": "Graphs of the training and test set RMS errors are shown, for various values of M, in Figure 1.5. The test set error is a measure of how well we are doing in predicting the values of t for new data observations of x. We note from Figure 1.5 that small values of M give relatively large values of the test set error, and this can be attributed to the fact that the corresponding polynomials are rather in\ufb02exible and are incapable of capturing the oscillations in the function sin(2\u03c0x). Values of M in the range 3 \u2a7d M \u2a7d 8 give small values for the test set error, and these also give reasonable representations of the generating function sin(2\u03c0x), as can be seen, for the case of M = 3, from Figure 1.4. For M = 9, the training set error goes to zero, as we might expect because this polynomial contains 10 degrees of freedom corresponding to the 10 coef\ufb01cients w0, . , w9, and so can be tuned exactly to the 10 data points in the training set. However, the test set error has become very large and, as we saw in Figure 1.4, the corresponding function y(x, w\u22c6) exhibits wild oscillations", "4ba10f2f-e8df-43ea-8c92-41093cfd2f7c": "Fundamentally, the choice of \u03f5 determines the complexity of the generative model.12 We study the trade-off between predictive performance and computational cost that this induces. We \ufb01nd that generally there is an \u201celbow point\u201d beyond which the number of correlations selected\u2014and thus the computational cost\u2014explodes, and that this point is a safe trade-off point between predictive performance and computation time.\n\nPredictive Performance At one extreme, a very large value of \u03f5 will not include any correlations in the generative model, making it identical to the independent model. As \u03f5 is decreased, correlations will be added. At \ufb01rst, when \u03f5 is still high, only the strongest correlations will be included. As these correlations are added, we observe that the generative model\u2019s predictive performance tends to improve. Figure 9, left, shows the result of varying \u03f5 in a simulation where more than half the labeling functions are correlated. After adding a few key dependencies, the generative model resolves the discrepancies among the labeling functions. Figure 9, middle, shows the effect of varying \u03f5 for the CDR task. Predictive performance improves as \u03f5 decreases until the model over\ufb01ts", "88f47fe2-f80a-437e-b1f6-77926b8dce67": "For some algorithms, the reward signal alone is the critical multiplier in the parameterreward signal.\n\nBut for most of the algorithms we discuss in this book, reinforcement signals include terms in addition to the reward signal, an example being a TD error \u03b4t = Rt+1 + \u03b3V (St+1) \u2212 V (St), which is the reinforcement signal for TD state-value learning (and analogous TD errors for action-value learning). In this reinforcement signal, Rt+1 is the primary reinforcement contribution, and the temporal di\u21b5erence in predicted values, \u03b3V (St+1) \u2212 V (St) (or an analogous temporal di\u21b5erence for action values), is the conditioned reinforcement contribution. Thus, whenever \u03b3V (St+1) \u2212 V (St) = 0, \u03b4t signals \u2018pure\u2019 primary reinforcement; and whenever Rt+1 = 0, it signals \u2018pure\u2019 conditioned reinforcement, but it often signals a mixture of these. Note as we mentioned in Section 6.1, this \u03b4t is not available until time t + 1. We therefore think of \u03b4t as the reinforcement signal at time t + 1, which is \ufb01tting because it reinforces predictions and/or actions made earlier at step t", "bd88b130-36d9-4289-948e-229c23403d62": "In this section, we present Boltzmann machines that define a probability density over real-valued data.\n\n20.5.1 Gaussian-Bernoulli RBMs  Restricted Boltzmann machines may be developed for many exponential family conditional distributions . Of these, the most common is the RBM with binary hidden units and real-valued visible units, with the conditional distribution over the visible units being a Gaussian distribution whose mean is a function of the hidden units. There are many ways of parametrizing Gaussian-Bernoulli RBMs. One choice is whether to use a covariance matrix or a precision matrix for the Gaussian distribution. Here we present the precision formulation. The modification to obtain the covariance formulation is straightforward. We wish to have the conditional distribution  p(v | h) = N(v;Wh, 37). (20.38)  We can find the terms we need to add to the energy function by expanding the  673  CHAPTER 20", "c7987d1a-106d-448a-9aaf-1509a109f524": "This is because they do not update their value estimates on the basis of the value estimates of successor states. In other words, it is because they do not bootstrap. In designing Monte Carlo control methods we have followed the overall schema of generalized policy iteration (GPI) introduced in Chapter 4. GPI involves interacting processes of policy evaluation and policy improvement. Monte Carlo methods provide an alternative policy evaluation process. Rather than use a model to compute the value of each state, they simply average many returns that start in the state. Because a state\u2019s value is the expected return, this average can become a good approximation to the value. In control methods we are particularly interested in approximating action-value functions, because these can be used to improve the policy without requiring a model of the environment\u2019s transition dynamics. Monte Carlo methods intermix policy evaluation and policy improvement steps on an episode-by-episode basis, and can be incrementally implemented on an episode-by-episode basis", "193aa5cb-9943-4cc3-8a00-10a19c95b790": "Pattern Classi\ufb01cation (Second ed.). Wiley. Durbin, R., S. Eddy, A. Krogh, and G. Mitchison . Biological Sequence Analysis. Cambridge University Press. Dybowski, R. and S. Roberts .\n\nAn anthology of probabilistic models for medical informatics. In D. Husmeier, R. Dybowski, and S. Roberts (Eds. ), Probabilistic Modeling in Bioinformatics and Medical Informatics, pp. 297\u2013349. Springer. Efron, B. Bootstrap methods: another look at the jackknife. Annals of Statistics 7, 1\u201326. Elkan, C. Using the triangle inequality to accelerate k-means. In Proceedings of the Twelfth International Conference on Machine Learning, pp. 147\u2013153. AAAI. Ephraim, Y., D", "c2f3a38c-6234-4c90-930c-79c73d08a468": "Ambiguity arises because it is not clear if each clique actually has a corresponding factor whose scope encompasses the entire clique\u2014for example, a clique containing three nodes may correspond to a factor over all three nodes, or may correspond to three factors that each contain only a pair of the nodes.\n\nFactor graphs resolve this ambiguity by explicitly representing the scope of each \u00a2 function. Specifically, a factor graph is a graphical representation of an undirected model that consists of a bipartite undirected graph. Some of the nodes are drawn as circles. These nodes correspond to random variables, as in a standard undirected model. The rest of the nodes are drawn as squares. These nodes correspond to the factors @ of the unnormalized probability distribution. Variables and factors may be connected with undirected edges. A variable and a factor are connected  576  CHAPTER 16", "21003a1d-3683-4cbd-8d96-25dd188c86d1": "It has a great merit that there is no computation involved in the \u2018training\u2019 phase because this simply requires storage of the training set. However, this is also one of its great weaknesses because the computational cost of evaluating the density grows linearly with the size of the data set. One of the dif\ufb01culties with the kernel approach to density estimation is that the parameter h governing the kernel width is \ufb01xed for all kernels. In regions of high data density, a large value of h may lead to over-smoothing and a washing out of structure that might otherwise be extracted from the data. However, reducing h may lead to noisy estimates elsewhere in data space where the density is smaller. Thus the optimal choice for h may be dependent on location within the data space. This issue is addressed by nearest-neighbour methods for density estimation. We therefore return to our general result (2.246) for local density estimation, and instead of \ufb01xing V and determining the value of K from the data, we consider a \ufb01xed value of K and use the data to \ufb01nd an appropriate value for V", "e98baacc-c77d-4e2f-b27d-59a9bc3d5de4": "REPRESENTATION LEARNING  he hg  h=[1,1,0)\" h=(1,0,1)\"  https://www.deeplearningbook.org/contents/representation.html    hatiiit  h= \" h= (0,1, 1)\" h= (0,0, 1)\"  Figure 15.7: Illustration of how a learning algorithm based on a distributed representation breaks up the input space into regions. In this example, there are three binary features hi, hg, and h3. Each feature is defined by thresholding the output of a learned linear transformation. Each feature divides R? into two half-planes. Let hi be the set of input points for which h; = 1, and h; be the set of input points for which h; = 0. In this illustration, each line represents the decision boundary for oneh;, with the corresponding arrow pointing to the h7 side of the boundary.\n\nThe representation as a whole takes on a unique value at each possible intersection of these half-planes. For example, the representation value ' corresponds to the region hf N hx NAF", "3f353707-3b54-497d-be0b-8b5280286577": "2.52 (\u22c6 \u22c6) For large m, the von Mises distribution (2.179) becomes sharply peaked around the mode \u03b80.\n\nBy de\ufb01ning \u03be = m1/2(\u03b8 \u2212 \u03b80) and making the Taylor expansion of the cosine function given by show that as m \u2192 \u221e, the von Mises distribution tends to a Gaussian. 2.54 (\u22c6) By computing \ufb01rst and second derivatives of the von Mises distribution (2.179), and using I0(m) > 0 for m > 0, show that the maximum of the distribution occurs when \u03b8 = \u03b80 and that the minimum occurs when \u03b8 = \u03b80 + \u03c0 (mod 2\u03c0). 2.55 (\u22c6) By making use of the result (2.168), together with (2.184) and the trigonometric identity (2.178), show that the maximum likelihood solution mML for the concentration of the von Mises distribution satis\ufb01es A(mML) = r where r is the radius of the mean of the observations viewed as unit vectors in the two-dimensional Euclidean plane, as illustrated in Figure 2.17", "ba2a7ddc-0037-41bd-86c4-3eebda3c1a72": "For example, the simple reinforcement learning player would learn to set up multi-move traps for a shortsighted opponent. It is a striking feature of the reinforcement learning solution that it can achieve the e\u21b5ects of planning and lookahead without using a model of the opponent and without conducting an explicit search over possible sequences of future states and actions. While this example illustrates some of the key features of reinforcement learning, it is so simple that it might give the impression that reinforcement learning is more limited than it really is.\n\nAlthough tic-tac-toe is a two-person game, reinforcement learning also applies in the case in which there is no external adversary, that is, in the case of a \u201cgame against nature.\u201d Reinforcement learning also is not restricted to problems in which behavior breaks down into separate episodes, like the separate games of tic-tac-toe, with reward only at the end of each episode. It is just as applicable when behavior continues inde\ufb01nitely and when rewards of various magnitudes can be received at any time. Reinforcement learning is also applicable to problems that do not even break down into discrete time steps like the plays of tic-tac-toe", "107df2f3-3cf3-4f8f-8e49-1ea652c30d2a": "Learning a deep convolutional network for image super-resolution. In: ECCV. Berlin: Springer; 2014. , p. 184-99.\n\nChristian L, Lucas T, Ferenc H, Jose C, Andrew C, Alejandro A, Andrew A, Alykhan T, Johannes T, Zehan W, Wenzhe S. Photo-realistic single image super-resolution using a generative adversarial network. arXiv preprint. 2016. Han Z, Tao X, Hongsheng L, Shaoting Z, Xiaogang W, Xiaolei H, Dimitris M. StackGAN: text to photo-realistic image synthesis with stacked generative adversarial networks. In: ICCV, 2017.  . Trishul CG, Yutaka S, Johnson A, Karthik K. Project adam: building an efficient and scalable deep learning training  system. In: Proceedings of OSDI. 2014. P", "030f4a10-7cf1-44cf-8e68-0e16b5535128": "Self-improving reactive agents based on reinforcement learning, planning and Lin, L.-J., Mitchell, T. Reinforcement learning with hidden states. In Proceedings of the Second International Conference on Simulation of Adaptive Behavior: From Animals to Animats, pp. 271\u2013280. MIT Press, Cambridge, MA. observable environments: Scaling up.\n\nIn Proceedings of the 12th International Conference on Machine Learning , pp. 362\u2013370. Morgan Kaufmann. Littman, M. L., Dean, T. L., Kaelbling, L. P. On the complexity of solving Markov decision problems. In Proceedings of the Eleventh Annual Conference on Uncertainty in Arti\ufb01cial Intelligence, pp. 394\u2013402. Littman, M. L., Sutton, R. S., Singh . Predictive representations of state. In Advances Liu, J. S. Monte Carlo Strategies in Scienti\ufb01c Computing. Springer-Verlag, Berlin. Ljung, L", "b7564d1f-5f67-44fd-a3c9-5414d3a6d504": "For example, the target in the update given by (16.3) is \u03b3 maxa \u02c6q(St+1, a, wt). Its dependence on wt complicates the process compared to the simpler supervised-learning situation in which the targets do not depend on the parameters being updated. As discussed in Chapter 11 this can lead to oscillations and/or divergence. To address this problem Mnih et al. used a technique that brought Q-learning closer to the simpler supervised-learning case while still allowing it to bootstrap.\n\nWhenever a certain number, C, of updates had been done to the weights w of the action-value network, they inserted the network\u2019s current weights into another network and held these duplicate weights \ufb01xed for the next C updates of w. The outputs of this duplicate network over the next C updates of w were used as the Q-learning targets. Letting \u02dcq denote the output of this duplicate network, then instead of (16.3) the update rule was: A \ufb01nal modi\ufb01cation of standard Q-learning was also found to improve stability", "fdbaa567-5f2a-4f18-acc3-315639e229df": "When your goal is to build the best possible real-world product or service, you can typically collect more data but must determine the value of reducing error further and weigh  https://www.deeplearningbook.org/contents/guidelines.html    this against the cost_of collecting more data. Data collection can require time, money, or human suffering (for example, if your data collection process involves performing invasive medical tests). When your goal is to answer a scientific question about which algorithm performs better on a fixed benchmark, the benchmark  417  CHAPTER 11.\n\nPRACTICAL METHODOLOGY  specification usually determines the training set, and you are not allowed to collect more data. How can one determine a reasonable level of performance to expect? Typically, in the academic setting, we have some estimate of the error rate that is attainable based on previously published benchmark results. In the real-word setting, we have some idea of the error rate that is necessary for an application to be safe, cost-effective, or appealing to consumers. Once you have determined your realistic desired error rate, your design decisions will be guided by reaching this error rate. Another important consideration besides the target value of the performance metric is the choice of which metric to use", "cd8ce9f2-e45b-4cdf-82f8-332c2d53a427": "For any node that may be reached by going backward from z through two or more paths, we simply sum the gradients arriving from different paths at that node. More formally, each node in the graph G corresponds to a variable. To achieve maximum generality, we describe this variable as being a tensor V. Tensors in general can have any number of dimensions. They subsume scalars, vectors, and matrices. We assume that each variable V is associated with the following subroutines:  get_operation(V): This returns the operation that computes V, repre- sented by the edges coming into V in the computational graph. For example, there may be a Python or C++ class representing the matrix multiplication operation, and the get_operation function. Suppose we have a variable that is created by matrix multiplication, C = AB. Then get_operation(V) returns a pointer to an instance of the corresponding C++ class. e get consumers(V,G): This returns the list of variables that are children of V in the computational graph G", "e1a53b65-ef97-430f-81bd-eb8e24dfa957": "The value of a state is estimated as its group\u2019s component, and when the state is updated, that component alone is updated. State aggregation is a special case of SGD (9.7) in which the gradient, r\u02c6v(St,wt), is 1 for St\u2019s group\u2019s component and 0 for the other components. Example 9.1: State Aggregation on the 1000-state Random Walk Consider a 1000-state version of the random walk task (Examples 6.2 and 7.1 on pages 125 and 144). The states are numbered from 1 to 1000, left to right, and all episodes begin near the center, in state 500. State transitions are from the current state to one of the 100 neighboring states to its left, or to one of the 100 neighboring states to its right, all with equal probability. Of course, if the current state is near an edge, then there may be fewer than 100 neighbors on that side of it", "d14eb29f-f7b3-484b-b92f-f8d4a1d5339f": "Using a combination of bridge sampling, short-chain AIS and parallel tempering, Desjardins et al. devised a scheme to track the partition function of an RBM throughout the training process. The strategy is based on the maintenance of independent estimates of the partition functions of the RBM at every temperature operating in the parallel tempering scheme. The authors combined bridge sampling estimates of the ratios of partition functions of neighboring chains (i.e., from parallel tempering) with AIS estimates across time to come up with a low variance estimate of the partition functions at every iteration of learning. The tools described in this chapter provide many different ways of overcoming the problem of intractable partition functions, but there can be several other difficulties involved in training and using generative models. Foremost among these is the problem of intractable inference, which we confront next. https://www.deeplearningbook.org/contents/partition.html    628  https://www.deeplearningbook.org/contents/partition.html", "ebb62313-2817-40e8-a3a2-233ad379b5ce": "For graphical demonstrations of sparse connectivity, see figure 9.2 and figure 9.3.\n\nIn a deep convolutional network, units in the deeper layers may indirectly interact with a larger portion of the input, as shown in figure 9.4. This allows the network to efficiently describe complicated interactions between many variables by  constructing such interactions from simple building blocks that each describe only sparse interactions. Parameter sharing refers to using the same parameter for more than one function in a model. In a traditional neural net, each element of the weight matrix  RRR  https://www.deeplearningbook.org/contents/convnets.html    Figure 9.2: Sparse connectivity, viewed from below. We highlight one input unit,73, and highlight the output units in s that are affected by this unit. (Top)Whens is formed by convolution with a kernel of width 3, only three outputs are affected bya. (Bottom)When  s is formed by matrix multiplication, connectivity is no longer sparse, so all the outputs are affected by 23. CHAPTER 9", "17e94393-a894-4bf2-846f-b883f93fda92": "We can de\ufb01ne the total within-class variance for the whole data set to be simply s2 2.\n\nThe Fisher criterion is de\ufb01ned to be the ratio of the between-class variance to the within-class variance and is given by We can make the dependence on w explicit by using (4.20), (4.23), and (4.24) to rewrite the Fisher criterion in the form Exercise 4.5 where SB is the between-class covariance matrix and is given by and SW is the total within-class covariance matrix, given by Differentiating (4.26) with respect to w, we \ufb01nd that J(w) is maximized when Note that if the within-class covariance is isotropic, so that SW is proportional to the unit matrix, we \ufb01nd that w is proportional to the difference of the class means, as discussed above. The result (4.30) is known as Fisher\u2019s linear discriminant, although strictly it is not a discriminant but rather a speci\ufb01c choice of direction for projection of the data down to one dimension. However, the projected data can subsequently be used to construct a discriminant, by choosing a threshold y0 so that we classify a new point as belonging to C1 if y(x) \u2a7e y0 and classify it as belonging to C2 otherwise", "3274c033-fb49-4716-b2b2-fbdae257f752": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics. Jeremy Howard and Sebastian Ruder. 2018. Universal language model \ufb01ne-tuning for text classi\ufb01cation. In ACL. Association for Computational Linguistics. Yacine Jernite, Samuel R. Bowman, and David Sontag. 2017.\n\nDiscourse-based objectives for fast unsupervised sentence representation learning. CoRR, abs/1705.00557. Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In ACL. Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors. In Advances in neural information processing systems, pages 3294\u20133302", "953cdb36-79de-4d60-ac13-5fc8dd9e17a5": "Traditionally, these distributions are more commonly expressed in an equivExercise 13.24 alent form in terms of noisy linear equations given by where the noise terms have the distributions The parameters of the model, denoted by \u03b8 = {A, \u0393, C, \u03a3, \u00b50, V0}, can be determined using maximum likelihood through the EM algorithm. In the E step, we need to solve the inference problem of determining the local posterior marginals for the latent variables, which can be solved ef\ufb01ciently using the sum-product algorithm, as we discuss in the next section. We now turn to the problem of \ufb01nding the marginal distributions for the latent variables conditional on the observation sequence. For given parameter settings, we also wish to make predictions of the next latent state zn and of the next observation xn conditioned on the observed data x1, . , xn\u22121 for use in real-time applications. These inference problems can be solved ef\ufb01ciently using the sum-product algorithm, which in the context of the linear dynamical system gives rise to the Kalman \ufb01lter and Kalman smoother equations", "870d5de3-7a1e-45e7-857d-ae4573c0d6c9": "The description of the symbol-to-symbol based approach subsumes the symbol-  210  CHAPTER 6. DEEP FEEDFORWARD NETWORKS  to-number approach. The symbol-to-number approach can be understood as performing exactly the same computations as are done in the graph built by the symbol-to-symbol approach. The key difference is that the symbol-to-number approach does not expose the graph. AREA Oanaral Racl_Dranaeatian  https://www.deeplearningbook.org/contents/mlp.html    Vive at en a Up Ug The back-propagation algorithm is very simple. To compute the gradient of some scalar 2 with respect to one of its ancestors \u00a9 in the graph, we begin by observing that the gradient with respect to z is given by de = 1.\n\nWe can then compute the gradient with respect to each parent of z in the graph by multiplying the current gradient by the Jacobian of the operation that produced z. We continue multiplying by Jacobians, traveling backward through the graph in this way until we reach a", "496bacc7-29f6-435f-b88e-873e5c51d21d": "But in many cases of interest, and certainly in the lives of all natural intelligences, the sensory input gives only partial information about the state of the world. Some objects may be occluded by others, or behind the agent, or miles away. In these cases, potentially important aspects of the environment\u2019s state are not directly observable, and it is a strong, unrealistic, and limiting assumption to assume that the learned value function is implemented as a table over the environment\u2019s state space. The framework of parametric function approximation that we developed in Part II is far less restrictive and, arguably, no limitation at all. In Part II we retained the assumption that the learned value functions (and policies) are functions of the environment\u2019s state, but allowed these functions to be arbitrarily restricted by the parameterization. It is somewhat surprising and not widely recognized that function approximation includes important aspects of partial observability. For example, if there is a state variable that is not observable, then the parameterization can be chosen such that the approximate value does not depend on that state variable.\n\nThe e\u21b5ect is just as if the state variable were not observable", "23f69753-36e8-4ec0-b133-bcaab908c183": "Gri\ufb03th, A. K. A comparison and evaluation of three machine learning procedures as applied to the game of checkers. Arti\ufb01cial Intelligence, 5(2):137\u2013148. Grondman, I., Busoniu, L., Lopes, G. A., Babuska, R. A survey of actor\u2013critic reinforcement learning: Standard and natural policy gradients. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 42(6):1291\u20131307. Grossberg, S. A neural model of attention, reinforcement, and discrimination learning. discrimination during associative learning. Neural Networks, 2(2):79\u2013102. Gullapalli, V. .\n\nA stochastic reinforcement algorithm for learning real-valued functions. Gullapalli, V., Barto, A. G. Shaping as a method for accelerating reinforcement learning. Hackman, L. Faster Gradient-TD Algorithms. M.Sc", "04a879b3-9f9d-44a3-ac32-00235af838be": "The final, k-th state\u2019s probability is given by 1 \u2014 1! p. Note that we must constrain 17 1. Multinoulli distributions are often used to refer to distributions over categories of objects, so we do not usually assume that state  https://www.deeplearningbook.org/contents/prob.html    1 has numerical value 1, and so on. For this reason, we do not usually need to compute the expectation or variance of multinoulli-distributed random variables. The Bernoulli and multinoulli distributions are sufficient to describe any distri- bution over their domain. They are able to describe any distribution over their domain not so much because they are particularly powerful but rather because their domain is simple; they model discrete variables for which it is feasible to enumerate all the states.\n\nWhen dealing with continuous variables, there are uncountably many states, so any distribution described by a small number of parameters must impose strict limits on the distribution. 3.9.3. Gaussian Distribution  The most commonly used distribution over real numbers is the normal distribu- tion, also known as the Gaussian distribution:  N (a; p,07) = (pee ( - (a 1)", "8575a426-6ad0-4f9c-8921-9441b14aa324": "Plot (a) shows the data points in green, together with the initial con\ufb01guration of the mixture model in which the one standard-deviation contours for the two Gaussian components are shown as blue and red circles.\n\nPlot (b) shows the result of the initial E step, in which each data point is depicted using a proportion of blue ink equal to the posterior probability of having been generated from the blue component, and a corresponding proportion of red ink given by the posterior probability of having been generated by the red component. Thus, points that have a signi\ufb01cant probability for belonging to either cluster appear purple. The situation after the \ufb01rst M step is shown in plot (c), in which the mean of the blue Gaussian has moved to the mean of the data set, weighted by the probabilities of each data point belonging to the blue cluster, in other words it has moved to the centre of mass of the blue ink. Similarly, the covariance of the blue Gaussian is set equal to the covariance of the blue ink. Analogous results hold for the red component. Plots (d), (e), and (f) show the results after 2, 5, and 20 complete cycles of EM, respectively. In plot (f) the algorithm is close to convergence", "de78ad15-e931-46dc-a5a5-04b93d9cbae4": ", Rk = rk, \u03c6(Sk) = \u03c6k}. The dist then is a complete characterization of a source of data trajectories. To know P everything about the statistics of the data, but it is still less than knowing th particular, the VE and BE objectives are readily computed from the MDP as d Section 3, but these cannot be determined from P alone.\n\nThe problem can be seen in very simple, POMDP-like examples, in which the data produced by two di\u21b5erent MDPs is identical in every respect, yet the BE i In such a case the BE is literally not a function of the data, and thus there is estimate it from data. One of the simplest examples is the pair of MDPs shown These MDPs have only one action (or, equivalently, no actions), so they are in e\u21b5 chains. Where two edges leave a state, both possibilities are assumed to occur probability. The numbers on the edges indicate the reward emitted if that edge is The MDP on the left has two states that are represented distinctly; each has weight so that they can take on any value", "5f5f2814-d0b7-4b1c-8d72-19a4b8c7bf0c": "MoreSonpally, we ae interested in computing  bias(62) =E[62 (5.37)  m m  J-o  https://www.deeplearningbook.org/contents/ml.html    We begin by evaluating the term a:  E =E E 3 (x @) in) (5.38) i=1 \u2014\u2014 13? (5.39)  Returning to equation 5.37, we conclude that the bias of 6 is \u2014o?/m. Therefore, the sample variance is a biased estimator. 124  CHAPTER 5. MACHINE LEARNING BASICS  The unbiased sample variance estimator  ra ea By (0)  (5.40)  provides an alternative approach. As the name suggests this estimator is unbiased. That is, we find that E = o? :  - 1 Siw .\\2 - Ela] =E Pe (o\" - jim) | (5.41) = Ele (5.42) ~_m_ (mals 5 - a ( = ) (5.43) =o? (5.44)  We have two estimators: one is biased, and the other is not", "123dc41d-67cc-478a-91bf-788d83af8e2d": "5.1\u20132 Singh and Sutton  distinguished between every-visit and \ufb01rst-visit MC methods and proved results relating these methods to reinforcement learning algorithms. The blackjack example is based on an example used by Widrow, Gupta, and Maitra . The soap bubble example is a classical Dirichlet problem whose Monte Carlo solution was \ufb01rst proposed by Kakutani . Barto and Du\u21b5  discussed policy evaluation in the context of classical Monte Carlo algorithms for solving systems of linear equations.\n\nThey used the analysis of Curtiss  to point out the computational advantages of Monte Carlo policy evaluation for large problems. 5.3\u20134 Monte Carlo ES was introduced in the 1998 edition of this book. That may have been the \ufb01rst explicit connection between Monte Carlo estimation and control methods based on policy iteration. An early use of Monte Carlo methods to estimate action values in a reinforcement learning context was by Michie and Chambers . In pole balancing (page 56), they used averages of episode durations to assess the worth (expected balancing \u201clife\u201d) of each possible action in each state, and then used these assessments to control action selections. Their method is similar in spirit to Monte Carlo ES with every-visit MC estimates", "94b23259-dfc2-4430-88be-bed855e3dbd0": "The strength or e\u21b5ectiveness by which the neurotransmitter released at a synapse in\ufb02uences the postsynaptic neuron is the synapse\u2019s e\ufb03cacy.\n\nOne way a nervous system can change through experience is through changes in synaptic e\ufb03cacies as a result of combinations of the activities of the presynaptic and postsynaptic neurons, and sometimes by the presence of a neuromodulator, which is a neurotransmitter having e\u21b5ects other than, or in addition to, direct fast excitation or inhibition. Brains contain several di\u21b5erent neuromodulation systems consisting of clusters of neurons with widely branching axonal arbors, with each system using a di\u21b5erent neurotransmitter. Neuromodulation can alter the function of neural circuits, mediate motivation, arousal, attention, memory, mood, emotion, sleep, and body temperature. Important here is that a neuromodulatory system can distribute something like a scalar signal, such as a reinforcement signal, to alter the operation of synapses in widely distributed sites critical for learning. The ability of synaptic e\ufb03cacies to change is called synaptic plasticity", "261001cc-3106-4c09-a8e4-70a3482b716d": "We can also use this synthetic data set to illustrate the \u2018responsibilities\u2019 by evaluating, for every data point, the posterior probability for each component in the mixture distribution from which this data set was generated.\n\nIn particular, we can represent the value of the responsibilities \u03b3(znk) associated with data point xn by plotting the corresponding point using proportions of red, blue, and green ink given by \u03b3(znk) for k = 1, 2, 3, respectively, as shown in Figure 9.5(c). So, for instance, a data point for which \u03b3(zn1) = 1 will be coloured red, whereas one for which \u03b3(zn2) = \u03b3(zn3) = 0.5 will be coloured with equal proportions of blue and green ink and so will appear cyan. This should be compared with Figure 9.5(a) in which the data points were labelled using the true identity of the component from which they were generated. Suppose we have a data set of observations {x1, . , xN}, and we wish to model this data using a mixture of Gaussians", "3e5d1f55-5b07-4521-8f16-6774c9a436e1": "APPLICATIONS  (0,0,0) \u2014 (0,0,1) (0,1,0) \u2014 (0,1,1) (1,0,0) \u2014 (1,0,1) (1,1,0) (1,1,1)  Figure 12.4: Illustration of a simple hierarchy of word categories, with 8 words up, ..,w7 organized into a three-level hierarchy. The leaves of the tree represent actual specific words. Internal nodes represent groups of words. Any node can be indexed by the sequence of binary decisions (0 = left, 1 = right) to reach the node from the root. Super-class (0) contains the classes (0,0) and (0,1), which respectively contain the sets of words {up, wi } and {tw2, ws}, and similarly super-class (1) contains the classes (1, 0) and (1,1), which respectively contain the words (w,ws) and (we,w7)", "ba04d474-f7a7-47f9-af4b-08016848dad8": "In this case, there are no parameters wij and so there are just D parameters bi and D parameters vi. From the recursion relations (8.15) and (8.16), we see that the mean of p(x) is given by (b1, . , bD)T and the covariance matrix is diagonal of the form diag(v1, . , vD). The joint distribution has a total of 2D parameters and represents a set of D independent univariate Gaussian distributions. Now consider a fully connected graph in which each node has all lower numbered nodes as parents.\n\nThe matrix wij then has i \u2212 1 entries on the ith row and hence is a lower triangular matrix (with no entries on the leading diagonal). Then the total number of parameters wij is obtained by taking the number D2 of elements in a D \u00d7D matrix, subtracting D to account for the absence of elements on the leading diagonal, and then dividing by 2 because the matrix has elements only below the diagonal, giving a total of D(D\u22121)/2", "1ed82d2b-e418-41d3-ab75-4d391ba68e50": "If the tree is condensed, so that any clique that is a subset of another clique is absorbed into the larger clique, this gives a junction tree. As a consequence of the triangulation step, the resulting tree satis\ufb01es the running intersection property, which means that if a variable is contained in two cliques, then it must also be contained in every clique on the path that connects them.\n\nThis ensures that inference about variables will be consistent across the graph. Finally, a two-stage message passing algorithm, essentially equivalent to the sum-product algorithm, can now be applied to this junction tree in order to \ufb01nd marginals and conditionals. Although the junction tree algorithm sounds complicated, at its heart is the simple idea that we have used already of exploiting the factorization properties of the distribution to allow sums and products to be interchanged so that partial summations can be performed, thereby avoiding having to work directly with the joint distribution. The role of the junction tree is to provide a precise and ef\ufb01cient way to organize these computations", "ef44a100-5fac-40a6-97ee-9af2f287fc9a": "For additional general coverage of reinforcement learning, we refer the reader to the books by Szepesv\u00b4ari , Bertsekas and Tsitsiklis , Kaelbling , and Sugiyama, Hachiya, and Morimura . Books that take a control or operations research perspective include those of Si, Barto, Powell, and Wunsch , Powell , Lewis and Liu , and Bertsekas . Cao\u2019s  review places reinforcement learning in the context of other approaches to learning and optimization of stochastic dynamic systems. Three special issues of the journal Machine Learning focus on reinforcement learning: Sutton , Kaelbling , and Singh . Useful surveys are provided by Barto ; Kaelbling, Littman, and Moore ; and Keerthi and Ravindran . The volume edited by Weiring and van Otterlo  provides an excellent overview of recent developments. In this part of the book we describe almost all the core ideas of reinforcement learning algorithms in their simplest forms: that in which the state and action spaces are small enough for the approximate value functions to be represented as arrays, or tables", "fde40e3e-f1fa-4d0b-9416-b783628fb18a": "For example, if we want to match all the moments of the form ax;, then we need to minimize the difference between a number of values that is quadratic in the dimension of 2.\n\nMoreover, even matching all the first and second moments would only be sufficient to fit a multivariate Gaussian distribution, which captures only linear relationships between values. Our ambitions for neural networks are to capture complex nonlinear relationships, which would require far more moments. GANs avoid this problem of exhaustively enumerating all moments by using a dynamically updated discriminator which automatically focuses its attention on whichever statistic the generator network is matching the least effectively. Instead, generative moment matching networks can be trained by minimizing a cost function called maximum mean discrepancy, or MMD . This cost function measures the error in the first moments in an infinite-dimensional space, using an implicit mapping to feature space defined by a kernel function to make computations on infinite-dimensional vectors tractable. The MMD cost is zero if and only if the two distributions being compared are equal. Visually, the samples from generative moment matching networks are somewhat disappointing. Fortunately, they can be improved by combining the generator network with an autoencoder", "b125d64f-4c55-4c8d-91b3-af8f71064699": "In other words, they are trained to make sure we do not wrongly reject an input when the object is present. The final classifier is trained to have high precision. At test time, we run inference by running the classifiers in a sequence, abandoning any example as soon as any one element in  https://www.deeplearningbook.org/contents/applications.html    the cascade rejects It. Uverall, this allows us to verity the presence ot objects with high confidence, using a high capacity model, but does not force us to pay the cost of full inference for every example. There are two different ways that the cascade can achieve high capacity.\n\nOne way is to make the later members of the cascade  individually have high capacity. In this case, the system as a whole obviously has high capacity, because some of its individual members do. It is also possible to make a cascade in which every individual model has low capacity but the system as a whole has high capacity as a result of the combination of many small models. Viola and Jones  used a cascade of boosted decision trees to implement a fast and robust face detector suitable for use in handheld digital cameras", "1f858372-730a-4673-adb5-1e32b9f73805": "Narendra and Wheeler  studied a Monte Carlo method for ergodic \ufb01nite Markov chains that used the return accumulated between successive visits to the same state as a reward for adjusting a learning automaton\u2019s action probabilities. 5.5 E\ufb03cient o\u21b5-policy learning has become recognized as an important challenge that arises in several \ufb01elds. For example, it is closely related to the idea of \u201cinterventions\u201d and \u201ccounterfactuals\u201d in probabilistic graphical (Bayesian) models . O\u21b5-policy methods using importance sampling have a long history and yet still are not well understood. Weighted importance sampling, which is also sometimes called normalized importance sampling , is discussed by Rubinstein , Hesterberg , Shelton , and Liu  among others.\n\nThe target policy in o\u21b5-policy learning is sometimes referred to in the literature as the \u201cestimation\u201d policy, as it was in the \ufb01rst edition of this book. 5.9 Per-decision importance sampling was introduced by Precup, Sutton, and Singh . They also combined o\u21b5-policy learning with temporal-di\u21b5erence learning, eligibility traces, and approximation methods, introducing subtle issues that we consider in later chapters", "50760670-5416-41c7-872c-8bec245aac9f": "Thus a completely disconnected graph (no links) will be a trivial D map for any distribution. Alternatively, we can consider a speci\ufb01c distribution and ask which graphs have the appropriate conditional independence properties. If every conditional independence statement implied by a graph is satis\ufb01ed by a speci\ufb01c distribution, then the graph is said to be an I map (for \u2018independence map\u2019) of that distribution. Clearly a fully connected graph will be a trivial I map for any distribution. If it is the case that every conditional independence property of the distribution is re\ufb02ected in the graph, and vice versa, then the graph is said to be a perfect map for that distribution. A perfect map is therefore both an I map and a D map.\n\nConsider the set of distributions such that for each distribution there exists a directed graph that is a perfect map. This set is distinct from the set of distributions such that for each distribution there exists an undirected graph that is a perfect map. In addition there are distributions for which neither directed nor undirected graphs offer a perfect map. This is illustrated as a Venn diagram in Figure 8.34", "0856782d-4697-4e08-adb6-4c9ca8903a0a": "6.10 (\u22c6) Show that an excellent choice of kernel for learning a function f(x) is given by k(x, x\u2032) = f(x)f(x\u2032) by showing that a linear learning machine based on this kernel will always \ufb01nd a solution proportional to f(x). 6.11 (\u22c6) By making use of the expansion (6.25), and then expanding the middle factor as a power series, show that the Gaussian kernel (6.23) can be expressed as the inner product of an in\ufb01nite-dimensional feature vector. 6.12 (\u22c6 \u22c6) www Consider the space of all possible subsets A of a given \ufb01xed set D. Show that the kernel function (6.27) corresponds to an inner product in a feature space of dimensionality 2|D| de\ufb01ned by the mapping \u03c6(A) where A is a subset of D and the element \u03c6U(A), indexed by the subset U, is given by Here U \u2286 A denotes that U is either a subset of A or is equal to A", "da9886dd-0355-4766-965c-52ad49a81ad3": "Assuming a prior distribution \u03c0(\u03b8) over the parameters, and considering a probabilistic model that de\ufb01nes a conditional distribution p(x|\u03b8), the inference is based on the Bayes\u2019 theorem: Interestingly, the early work by Zellner  showed the relations between Bayesian inference and maximum entropy, by reformulating the statistical inference problem from the perspective of information processing, and rediscovering the Bayes\u2019 theorem as the optimal information processing rule. More speci\ufb01cally, statistical inference can be seen as a procedure of information processing, where the system receives input information in the form of prior knowledge and data, and emits output information in the form of parameter estimates and others. An e\ufb03cient inference procedure should generate an output distribution such that the system retains all input information and not inject any extraneous information. The learning objective is thus to minimize the di\ufb00erence between the input and output information w.r.t. the output distribution: where the \ufb01rst two terms measure the output information in the output distribution q(\u03b8) and marginal p(D), and the third term measures the input information in the prior \u03c0(\u03b8) and data likelihood p(x\u2217|\u03b8)", "a4767116-4abf-4c68-bfbc-2c007bf03a21": "We\u2019re also highlighting what we believe are some of the most promising new directions of energy-based models for prediction in the presence of uncertainty, joint embedding methods and latent-variable architectures for self-supervised learning and reasoning in Al systems. Self-supervised learning is predictive learning  Self-supervised learning obtains supervisory signals from the data itself, often leveraging the underlying structure in the data. The general technique of self-supervised learning is to predict any unobserved or hidden part (or property) of the input from any observed or unhidden part of the input. For example, as is common in NLP, we can hide part of a sentence and predict the hidden words from the remaining  https://ai.meta.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/  Self-supervised learning: The dark matter of intelligence   words.\n\nWe can also predict past or future frames in a video (hidden data) from current ones (observed data). Since self-supervised learning uses the structure of the data itself, it can make use of a variety of supervisory signals across co-occurring modalities (e.g., video and audio) and across large data sets \u2014 all without relying on labels", "c7042a73-bfbe-4f94-9072-727977bc6663": "We define the output to be  jg=w'a, (5.3)  where w \u20ac R\u201d is a vector of parameters. Parameters are values that control the behavior of the system. In this case, w; is the coefficient that we multiply by feature x; before summing up the contributions from all the features. We can think of w as a set of weights that determine how  105  CHAPTER 5. MACHINE LEARNING BASICS  each feature affects the prediction. If a feature x; receives a positive weight w;, then increasing the value of that feature increases the value of our prediction 4%. If a feature receives a negative weight, then increasing the value of that feature decreases the value of our prediction. If a feature\u2019s weight is large in magnitude,  https://www.deeplearningbook.org/contents/ml.html    then it has a large effect on the prediction. If a feature\u2019s weight is zero, it has no effect on the prediction. We thus have a definition of our task T: to predict y from a by outputting  y= w 'a.\n\nNext we need a definition of our performance measure, P", "337dac65-e5a9-45f7-b16d-5028620a8971": "Modelling conditional probability distributions for periodic variables. Neural Computation 8(5), 1123\u20131133. Bishop, C. M. and I. T. Nabney . Pattern Recognition and Machine Learning: A Matlab Companion. Springer. In preparation. Bishop, C. M., D. Spiegelhalter, and J. Winn . VIBES: A variational inference engine for Bayesian networks. In S. Becker, S. Thrun, and K. Obermeyer (Eds. ), Advances in Neural Bishop, C. M. and M. Svens\u00b4en . Bayesian hierarchical mixtures of experts. In U. Kjaerulff and C. Meek (Eds. ), Proceedings Nineteenth Conference on Uncertainty in Arti\ufb01cial Intelligence, pp. 57\u201364. Morgan Kaufmann.\n\nBishop, C. M., M", "e484c831-75ae-40ce-953d-3392a4ed5bdc": "On the face of an extremely steep cliff structure, the gradient update step can move the parameters extremely far, usually jumping off  the cliff structure altogether. The cliff can be dangerous whether we approach it from above or from below,  285  CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  but fortunately its most serious consequences can be avoided using the gradient clipping heuristic described in section 10.11.1. The basic idea is to recall that the gradient specifies not the optimal step size, but only the optimal direction within an infinitesimal region. When the traditional gradient descent algorithm proposes making a very large step, the gradient clipping heuristic intervenes to reduce the step size, making it less likely to go outside the region where the gradient indicates the direction of approximately steepest descent. Cliff structures are most common in the cost functions for recurrent neural networks, because such models involve a multiplication of many factors, with one factor for each time step.\n\nLong temporal sequences thus incur an extreme amount of multiplication. 8.2.5 Long-Term Dependencies  Another difficulty that neural network optimization algorithms must overcome arises when the computational graph becomes extremely deep. Feedforward networks with many layers have such deep computational graphs", "4fb48231-fcb4-477b-ad94-f0851857f599": "then is a complete characterization of a source of data trajectories. T everything about the statistics of the data, but it is still less than k particular, the VE and BE objectives are readily computed from the Section 3, but these cannot be determined from P alone.\n\nThe problem can be seen in very simple, POMDP-like examples, in data produced by two di\u21b5erent MDPs is identical in every respect, ye In such a case the BE is literally not a function of the data, and thu estimate it from data. One of the simplest examples is the pair of MD These MDPs have only one action (or, equivalently, no actions), so they chains. Where two edges leave a state, both possibilities are assumed probability. The numbers on the edges indicate the reward emitted if t The MDP on the left has two states that are represented distinctly; weight so that they can take on any value. The MDP on the right of which, B and B\u2032, are represented identically and must be given th value. We can imagine that the value of state A is given by the \ufb01rst the value of B and B\u2032 is given by the second. Notice that the observ for the two MDPs", "7683217e-8403-4660-a5a2-7be1beefd193": "But if the system is not ready for one of these commands, or executing a command would result in resources being underutilized (e.g., due to timing constraints arising from servicing that one command), it makes sense to begin servicing a newer request before \ufb01nishing the older one. Policies can gain e\ufb03ciency by reordering requests, for example, by giving priority to read requests over write requests, or by giving priority to read/write commands to already open rows. The policy called First-Ready, First-Come-First-Serve (FR-FCFS), gives priority to column commands (read and write) over row commands (activate and precharge), and in case of a tie gives priority to the oldest command. FR-FCFS was shown to outperform other scheduling policies in terms of average memory-access latency under conditions commonly encountered .\n\nThey modeled the DRAM access process as an MDP whose states are the contents of the transaction queue and whose actions are commands to the DRAM system: precharge, activate, read, write, and NoOp. The reward signal is 1 whenever the action is read or write, and otherwise it is 0", "1dfa3555-5ca7-47ad-b618-02e715a0befd": "However, if we wish to use the SVM as a module in a larger probabilistic system, then probabilistic predictions of the class label t for new inputs x are required. To address this issue, Platt  has proposed \ufb01tting a logistic sigmoid to the outputs of a previously trained support vector machine. Speci\ufb01cally, the required conditional probability is assumed to be of the form where y(x) is de\ufb01ned by (7.1). Values for the parameters A and B are found by minimizing the cross-entropy error function de\ufb01ned by a training set consisting of pairs of values y(xn) and tn. The data used to \ufb01t the sigmoid needs to be independent of that used to train the original SVM in order to avoid severe over-\ufb01tting. This twostage approach is equivalent to assuming that the output y(x) of the support vector machine represents the log-odds of x belonging to class t = 1.\n\nBecause the SVM training procedure is not speci\ufb01cally intended to encourage this, the SVM can give a poor approximation to the posterior probabilities", "310a5aab-7947-480e-bdda-bbc2caf296ad": "The precise form of the AdaBoost algorithm is given below. 1. Initialize the data weighting coef\ufb01cients {wn} by setting w(1) (a) Fit a classi\ufb01er ym(x) to the training data by minimizing the weighted where I(ym(xn) \u0338= tn) is the indicator function and equals 1 when ym(xn) \u0338= tn and 0 otherwise. (c) Update the data weighting coef\ufb01cients 3. Make predictions using the \ufb01nal model, which is given by We see that the \ufb01rst base classi\ufb01er y1(x) is trained using weighting coef\ufb01cients w(1) n that are all equal, which therefore corresponds to the usual procedure for training a single classi\ufb01er.\n\nFrom (14.18), we see that in subsequent iterations the weighting coef\ufb01cients w(m) n are increased for data points that are misclassi\ufb01ed and decreased for data points that are correctly classi\ufb01ed. Successive classi\ufb01ers are therefore forced to place greater emphasis on points that have been misclassi\ufb01ed by previous classi\ufb01ers, and data points that continue to be misclassi\ufb01ed by successive classi\ufb01ers receive ever greater weight", "774c7050-d08b-4b9f-b8a9-e330b277d7ce": "We can do this using the directional derivative:  min ul Val (x) (4.3) u,ulu=1 = min |lulla||Ves(e)]he e084 (4.4) uu! u=1  where @ is the angle between u and the gradient. Substituting in ||e||2 = 1 and ignoring factors that do not depend on w, this simplifies to ming cos 6. This is minimized when w points in the opposite direction as the gradient. In other words, the gradient points directly uphill, and the negative gradient points directly downhill. We can decrease f by moving in the direction of the negative gradient. This is known as the method of steepest descent, or gradient descent. Steepest descent proposes a new point a =2-\u20acVaf (x) (4.5)  83  CHAPTER 4. NUMERICAL COMPUTATION  where \u00a2\u20ac is the learning rate, a positive scalar determining the size of the step. We can choose \u20ac in several different ways. A popular approach is to set \u20ac to a small constant", "23d67cc2-9d1b-4854-863a-c45f6fbb5911": "Ordinary importance sampling produces unbiased estimates, but has larger, possibly in\ufb01nite, variance, whereas weighted importance sampling always has \ufb01nite variance and is preferred in practice.\n\nDespite their conceptual simplicity, o\u21b5-policy Monte Carlo methods for both prediction and control remain unsettled and are a subject of ongoing research. The Monte Carlo methods treated in this chapter di\u21b5er from the DP methods treated in the previous chapter in two major ways. First, they operate on sample experience, and thus can be used for direct learning without a model. Second, they do not bootstrap. That is, they do not update their value estimates on the basis of other value estimates. These two di\u21b5erences are not tightly linked, and can be separated. In the next chapter we consider methods that learn from experience, like Monte Carlo methods, but also bootstrap, like DP methods. The term \u201cMonte Carlo\u201d dates from the 1940s, when physicists at Los Alamos devised games of chance that they could study to help understand complex physical phenomena relating to the atom bomb. Coverage of Monte Carlo methods in this sense can be found in several textbooks", "ae720c27-b013-4348-96fc-97480742dedb": "Reinforcement in the context of game theory is a much di\u21b5erent subject than reinforcement learning used in programs to play tic-tac-toe, checkers, and other recreational games. See, for example, Szita  for an overview of this aspect of reinforcement learning and games. John Holland  outlined a general theory of adaptive systems based on selectional form, as in evolutionary methods and the k-armed bandit. In 1976 and more fully in 1986, he introduced classi\ufb01er systems, true reinforcement learning systems including association and value functions. A key component of Holland\u2019s classi\ufb01er systems was the \u201cbucket-brigade algorithm\u201d for credit assignment, which is closely related to the temporal di\u21b5erence algorithm used in our tic-tac-toe example and discussed in Chapter 6.\n\nAnother key component was a genetic algorithm, an evolutionary method whose role was to evolve useful representations. Classi\ufb01er systems have been extensively developed by many researchers to form a major branch of reinforcement learning research , but genetic algorithms\u2014which we do not consider to be reinforcement learning systems by themselves\u2014have received much more attention, as have other approaches to evolutionary computation . The individual most responsible for reviving the trial-and-error thread to reinforcement learning within arti\ufb01cial intelligence was Harry Klopf", "45f3e1f4-b02a-4d2c-8eee-d285149c9af8": "After several repetitions of presenting another stimulus, one not related to food, in this case the sound of a metronome, shortly before the introduction of food, the dog salivated in response to the sound of the metronome in the same way it did to the food. \u201cThe activity of the salivary gland has thus been called into play by impulses of sound\u2014a stimulus quite alien to food\u201d (Pavlov, 1927, p. 22).\n\nSummarizing the signi\ufb01cance of this \ufb01nding, Pavlov wrote: It is pretty evident that under natural conditions the normal animal must respond not only to stimuli which themselves bring immediate bene\ufb01t or harm, but also to other physical or chemical agencies\u2014waves of sound, light, and the like\u2014which in themselves only signal the approach of these stimuli; though it is not the sight and sound of the beast of prey which is in itself harmful to the smaller animal, but its teeth and claws. (Pavlov, 1927, p. 14) Connecting new stimuli to innate re\ufb02exes in this way is now called classical, or Pavlovian, conditioning", "4b98e133-41e2-4d9f-a064-b094481f6dfe": "Similarly, the target for an arbitrary n-step update is the n-step return: for all n, t such that n \u2265 1 and 0 \uf8ff t < T \u2212 n. All n-step returns can be considered approximations to the full return, truncated after n steps and then corrected for the remaining missing terms by Vt+n\u22121(St+n). If t + n \u2265 T (if the n-step return extends to or beyond termination), then all the missing terms are taken as zero, and the n-step return de\ufb01ned to be equal to the ordinary full return (Gt:t+n .= Gt if t + n \u2265 T). Note that n-step returns for n > 1 involve future rewards and states that are not available at the time of transition from t to t + 1. No real algorithm can use the n-step return until after it has seen Rt+n and computed Vt+n\u22121. The \ufb01rst time these are available is t + n", "3f29ffdd-a71b-47c4-ad09-1acfbcd00acc": "For any given path, the corresponding probability is given by the product of the elements of the transition matrix Ajk, corresponding to the probabilities p(zn+1|zn) for each segment of the path, along with the emission densities p(xn|k) associated with each node on the path. If we eliminate \u00b5zn\u2192fn+1(zn) between these two equations, and make use of (13.46), we obtain a recursion for the f \u2192 z messages of the form where we have introduced the notation \u03c9(zn) \u2261 \u00b5fn\u2192zn(zn). From (8.95) and (8.96), these messages are initialized using where we have used (13.45). Note that to keep the notation uncluttered, we omit the dependence on the model parameters \u03b8 that are held \ufb01xed when \ufb01nding the most probable sequence. The Viterbi algorithm can also be derived directly from the de\ufb01nition (13.6) of the joint distribution by taking the logarithm and then exchanging maximizations and summations", "e234d02b-2014-4205-90ef-50b1a93e1940": "We use the hyper-parameters speci\ufb01ed in Algorithm 1 for all of our experiments.\n\nBecause the WGAN algorithm attempts to train the critic f (lines 2\u20138 in Algorithm 1) relatively well before each generator update (line 10 in Algorithm 1), the loss function at this point is an estimate of the EM distance, up to constant factors related to the way we constrain the Lipschitz constant of f. Our \ufb01rst experiment illustrates how this estimate correlates well with the quality of the generated samples. Besides the convolutional DCGAN architecture, we also ran experiments where we replace the generator or both the generator and the critic by 4-layer ReLU-MLP with 512 hidden units. Figure 3 plots the evolution of the WGAN estimate (3) of the EM distance during WGAN training for all three architectures. The plots clearly show that these curves correlate well with the visual quality of the generated samples. To our knowledge, this is the \ufb01rst time in GAN literature that such a property is shown, where the loss of the GAN shows properties of convergence. This property is extremely useful when doing research in adversarial networks as one does not need to stare at the generated samples to \ufb01gure out failure modes and to gain information on which models are doing better over others", "2327d0da-9f71-407d-8792-b52106168102": "Then make use of (1.135), together with (1.136), to show that, if the result holds at order M \u2212 1, then it will also hold at order M 1.16 (\u22c6 \u22c6 \u22c6) In Exercise 1.15, we proved the result (1.135) for the number of independent parameters in the M th order term of a D-dimensional polynomial. We now \ufb01nd an expression for the total number N(D, M) of independent parameters in all of the terms up to and including the M6th order. First show that N(D, M) satis\ufb01es where n(D, m) is the number of independent parameters in the term of order m. Now make use of the result (1.137), together with proof by induction, to show that This can be done by \ufb01rst proving that the result holds for M = 0 and arbitrary D \u2a7e 1, then assuming that it holds at order M, and hence showing that it holds at order M + 1.\n\nFinally, make use of Stirling\u2019s approximation in the form for large n to show that, for D \u226b M, the quantity N(D, M) grows like DM, and for M \u226b D it grows like M D", "c820acba-add2-4083-9829-583d4584c7ee": "Zaidan, O.F., Eisner, J.: Modeling annotators: A generative approach to learning from annotator rationales.\n\nIn: Conference on Empirical Methods in Natural Language Processing (EMNLP)  65. Zhang, C., R\u00e9, C., Cafarella, M., De Sa, C., Ratner, A., Shin, J., Wang, F., Wu, S.: DeepDive: Declarative knowledge base construction. Commun. ACM 60(5), 93\u2013102  66. Zhang, Y., Chen, X., Zhou, D., Jordan, M.I. : Spectral methods meet EM: a provably optimal algorithm for crowdsourcing. J. Mach. Learn. Res. 17, 1\u201344  67. Zhao, B., Rubinstein, B.I., Gemmell, J., Han, J.: A Bayesian approach to discovering truth from con\ufb02icting sources for data integration. PVLDB 5(6), 550\u2013561  Publisher\u2019s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional af\ufb01liations.", "5388c9be-566b-4b36-84cb-aeb61a148e5f": "\u2022 By the Kantorovich-Rubinstein duality , we know that W(Pr, P\u03b8) = dF(Pr, P\u03b8) when F is the set of 1-Lipschitz functions. Furthermore, if F is the set of KLipschitz functions, we get K \u00b7 W(Pr, P\u03b8) = dF(Pr, P\u03b8). \u2022 When F is the set of all measurable functions bounded between -1 and 1 (or all continuous functions between -1 and 1), we retrieve dF(Pr, P\u03b8) = \u03b4(Pr, P\u03b8) the total variation distance .\n\nThis already tells us that going from 1-Lipschitz to 1-Bounded functions drastically changes the topology of the space, and the regularity of dF(Pr, P\u03b8) as a loss function (as by Theorems 1 and 2). \u2022 Energy-based GANs (EBGANs)  can be thought of as the generative approach to the total variation distance. This connection is stated and proven in depth in Appendix D", "1fa7ce66-f86f-4dce-998f-a46e286f2fe1": "Again it is easy to verify that the joint distribution over all variables is Gaussian.\n\nNote that we have already encountered a speci\ufb01c example of the linear-Gaussian relationship when we saw that the conjugate prior for the mean \u00b5 of a Gaussian Section 2.3.6 variable x is itself a Gaussian distribution over \u00b5. The joint distribution over x and \u00b5 is therefore Gaussian. This corresponds to a simple two-node graph in which the node representing \u00b5 is the parent of the node representing x. The mean of the distribution over \u00b5 is a parameter controlling a prior, and so it can be viewed as a hyperparameter. Because the value of this hyperparameter may itself be unknown, we can again treat it from a Bayesian perspective by introducing a prior over the hyperparameter, sometimes called a hyperprior, which is again given by a Gaussian distribution. This type of construction can be extended in principle to any level and is an illustration of a hierarchical Bayesian model, of which we shall encounter further examples in later chapters", "3febe61e-198c-46f6-a551-e67c6774e0b1": "It would seem more reasonable to choose the complexity of the model according to the complexity of the problem being solved. We shall see that the least squares approach to \ufb01nding the model parameters represents a speci\ufb01c case of maximum likelihood (discussed in Section 1.2.5), and that the over-\ufb01tting problem can be understood as a general property of maximum likelihood. By adopting a Bayesian approach, the Section 3.4 over-\ufb01tting problem can be avoided.\n\nWe shall see that there is no dif\ufb01culty from a Bayesian perspective in employing models for which the number of parameters greatly exceeds the number of data points. Indeed, in a Bayesian model the effective number of parameters adapts automatically to the size of the data set. For the moment, however, it is instructive to continue with the current approach and to consider how in practice we can apply it to data sets of limited size where we function (1.4) for two values of the regularization parameter \u03bb corresponding to ln \u03bb = \u221218 and ln \u03bb = 0. The case of no regularizer, i.e., \u03bb = 0, corresponding to ln \u03bb = \u2212\u221e, is shown at the bottom right of Figure 1.4. may wish to use relatively complex and \ufb02exible models", "20c0263f-48a9-442b-8d42-4b6c23c99a1e": "Now consider the case of binary classi\ufb01cation in which we have a single target variable t such that t = 1 denotes class C1 and t = 0 denotes class C2.\n\nFollowing the discussion of canonical link functions in Section 4.3.6, we consider a network having a single output whose activation function is a logistic sigmoid so that 0 \u2a7d y(x, w) \u2a7d 1. We can interpret y(x, w) as the conditional probability p(C1|x), with p(C2|x) given by 1 \u2212 y(x, w). The conditional distribution of targets given inputs is then a Bernoulli distribution of the form If we consider a training set of independent observations, then the error function, which is given by the negative log likelihood, is then a cross-entropy error function of the form where yn denotes y(xn, w). Note that there is no analogue of the noise precision \u03b2 because the target values are assumed to be correctly labelled. However, the model is easily extended to allow for labelling errors. Simard et al", "49ed6b04-c85d-4471-af16-a6699709a1f3": "2.42 (\u22c6 \u22c6) Evaluate the mean, variance, and mode of the gamma distribution (2.146). is a generalization of the univariate Gaussian distribution. Show that this distribution is normalized so that \ufffd \u221e and that it reduces to the Gaussian when q = 2. Consider a regression model in which the target variable is given by t = y(x, w) + \u03f5 and \u03f5 is a random noise variable drawn from the distribution (2.293). Show that the log likelihood function over w and \u03c32, for an observed data set of input vectors X = {x1, . , xN} and corresponding target variables t = (t1, . , tN)T, is given by where \u2018const\u2019 denotes terms independent of both w and \u03c32. Note that, as a function of w, this is the Lq error function considered in Section 1.5.5", "9bd00b72-ee93-4ac6-8d40-6791c9edd7a4": "Then we start a set of messages propagating inwards from the leaves of the tree towards the root, with each node sending its message towards the root once it has received all incoming messages from its other neighbours. The \ufb01nal maximization is performed over the product of all messages arriving at the root node, and gives the maximum value for p(x). This could be called the max-product algorithm and is identical to the sum-product algorithm except that summations are replaced by maximizations. Note that at this stage, messages have been sent from leaves to the root, but not in the other direction.\n\nIn practice, products of many small probabilities can lead to numerical under\ufb02ow problems, and so it is convenient to work with the logarithm of the joint distribution. The logarithm is a monotonic function, so that if a > b then ln a > ln b, and hence the max operator and the logarithm function can be interchanged, so that The distributive property is preserved because Thus taking the logarithm simply has the effect of replacing the products in the max-product algorithm with sums, and so we obtain the max-sum algorithm", "c3929ade-f732-4f9c-84d3-a0a25cb5461d": "Consequently parameter nodes never themselves have parents and so all paths through these nodes will always be tail-to-tail and hence blocked. Consequently they play no role in d-separation. Another example of conditional independence and d-separation is provided by the concept of i.i.d. (independent identically distributed) data introduced in Section 1.2.4. Consider the problem of \ufb01nding the posterior distribution for the mean of a univariate Gaussian distribution. This can be represented by the directed graph Section 2.3 shown in Figure 8.23 in which the joint distribution is de\ufb01ned by a prior p(\u00b5) together with a set of conditional distributions p(xn|\u00b5) for n = 1, . , N. In practice, we observe D = {x1, . , xN} and our goal is to infer \u00b5. Suppose, for a moment, that we condition on \u00b5 and consider the joint distribution of the observations", "d97c0c3f-ac61-405e-9cad-2f9a9b655a4b": "We can easily achieve 99.9999 percent accuracy on the detection task, by simply hard coding the classifier to always report that the disease is absent. Clearly, accuracy is a poor way to characterize the performance of such a system. One way to solve this problem is to instead measure precision and recall. Precision is the fraction of detections reported by the model that were correct, while recall is the fraction of true events  https://www.deeplearningbook.org/contents/guidelines.html    that were detected. A detector that says no one has the disease would achieve perfect precision, but zero recall. A detector that says everyone has the disease would achieve perfect recall, but precision equal to the percentage of people who  have the disease (0.0001 percent in our example of a disease that only one people in a million have). When using precision and recall, it is common to plot a PR curve, with precision on the y-axis and recall on the z-axis.\n\nThe classifier generates a score that is higher if the event to be detected occurred. For example, a feedforward  418  CHAPTER 11", "538a3c02-d667-474e-bdd3-f811d0b1eaf3": "Note that the error function in this case is quadratic and hence the Newton-Raphson formula gives the exact solution in one step. Now let us apply the Newton-Raphson update to the cross-entropy error function (4.90) for the logistic regression model. From (4.91) we see that the gradient and Hessian of this error function are given by We see that the Hessian is no longer constant but depends on w through the weighting matrix R, corresponding to the fact that the error function is no longer quadratic. Using the property 0 < yn < 1, which follows from the form of the logistic sigmoid function, we see that uTHu > 0 for an arbitrary vector u, and so the Hessian matrix H is positive de\ufb01nite. It follows that the error function is a concave function of w and hence has a unique minimum", "397bf348-b941-4f4c-9e9f-b322239f484b": "Curran Associates, Inc. Peng, J. E\ufb03cient Dynamic Programming-Based Learning for Control. Ph.D. thesis, Peng, J. E\ufb03cient memory-based dynamic programming. In Proceedings of the 12th International Conference on Machine Learning , pp. 438\u2013446. Peng, J., Williams, R. J. E\ufb03cient learning and planning within the Dyna framework. 11th International Conference on Machine Learning , pp. 226\u2013232. Morgan Kaufmann, San Francisco. Perkins, T. J., Pendrith, M. D. On the existence of \ufb01xed points for Q-learning and Sarsa in partially observable domains. In Proceedings of the 19th International Conference on Machine Learning , pp. 490\u2013497. Perkins, T. J., Precup, D. A convergent form of approximate policy iteration. In Advances in Neural Information Processing Systems 15 , pp.\n\n1627\u20131634. MIT Press, Cambridge, MA", "5b47412a-80a5-49c8-b29d-b7a1a8db7e2a": "As a concrete example, we de\ufb01ne f as a binary classi\ufb01er f\u03c6 with sigmoid activation and parameters \u03c6, where the value f\u03c6(t) measures the log probability of the sample t being a real instance (as opposed to a model generation). Thus the higher f\u03c6(t) value, the higher quality of sample t. The parameters \u03c6 of the experience function need to be learned. We can do so by augmenting the standard equation (Equation 3.2) with added optimization of \u03c6 in various ways. The following equation gives one of the approaches: where, besides the optimization of q and \u03b8, we additionally maximize over \u03c6 with the extra term Epd  to form the classi\ufb01cation problem max\u03c6 \u2212Eq  + Epd .\n\nFurther assuming a particular con\ufb01guration of the tradeo\ufb00 hyperparameters \u03b1 = 0 and \u03b2 = 1, the resulting objective turns out to relate closely to generative adversarial learning. In particular, with proofs adapted from Farnia and Tse , Equation 6.4 recovers the vanilla GAN algorithm when D is the Jensen-Shannon divergence and assuming the space of f\u03c6, denoted as F, is convex", "01949e5b-84a0-4207-bf79-979ba55a2ba6": "10.25 (\u22c6 \u22c6) The variational treatment of the Bayesian mixture of Gaussians, discussed in Section 10.2, made use of a factorized approximation (10.5) to the posterior distribution. As we saw in Figure 10.2, the factorized assumption causes the variance of the posterior distribution to be under-estimated for certain directions in parameter space. Discuss qualitatively the effect this will have on the variational approximation to the model evidence, and how this effect will vary with the number of components in the mixture. Hence explain whether the variational Gaussian mixture will tend to under-estimate or over-estimate the optimal number of components.\n\n10.26 (\u22c6 \u22c6 \u22c6) Extend the variational treatment of Bayesian linear regression to include a gamma hyperprior Gam(\u03b2|c0, d0) over \u03b2 and solve variationally, by assuming a factorized variational distribution of the form q(w)q(\u03b1)q(\u03b2). Derive the variational update equations for the three factors in the variational distribution and also obtain an expression for the lower bound and for the predictive distribution", "853eebd4-2390-4a96-a50c-f618cfd1cfc1": "R., Sutton, R. S. O\u21b5-policy learning based on weighted importance sampling with linear computational complexity. In Proceedings of the 31st Conference on Uncertainty in Arti\ufb01cial Intelligence , pp. 552\u2013561. AUAI Press Corvallis, Oregon. Mahmood, A. R., Sutton, R. S., Degris, T., Pilarski, P. M. Tuning-free step-size adaptation. In 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Proceedings, pp. 2121\u20132124. IEEE. Mahmood, A. R., Yu, H, Sutton, R. S. Multi-step o\u21b5-policy learning without importance Mahmood, A. R., van Hasselt, H., Sutton, R. S. Weighted importance sampling for o\u21b5-policy learning with linear function approximation", "a3a2967f-2637-4190-8e37-726dfc8a36dc": "Note that these diagonal elements lie in the range (0, 1/4), and hence WN is a positive de\ufb01nite matrix.\n\nBecause CN (and hence its inverse) is positive de\ufb01nite by construction, and because the sum of two positive de\ufb01nite matrices is also positive de\ufb01nite, we see Exercise 6.24 that the Hessian matrix A = \u2212\u2207\u2207\u03a8(aN) is positive de\ufb01nite and so the posterior distribution p(aN|tN) is log convex and therefore has a single mode that is the global maximum. The posterior distribution is not Gaussian, however, because the Hessian is a function of aN. Using the Newton-Raphson formula (4.92), the iterative update equation for aN is given by Exercise 6.25 These equations are iterated until they converge to the mode which we denote by a\u22c6 N. At the mode, the gradient \u2207\u03a8(aN) will vanish, and hence a\u22c6 N will satisfy where the elements of WN are evaluated using a\u22c6 N. This de\ufb01nes our Gaussian approximation to the posterior distribution p(aN|tN) given by We can now combine this with (6.78) and hence evaluate the integral (6.77)", "3a1da66f-2bd7-4b66-aec0-7ddcbae9f874": "I., Tennenholtz, M. R-max \u2013 a general polynomial time algorithm for near-optimal reinforcement learning. Journal of Machine Learning Research, 3:213\u2013231. Breiman, L. Random forests. Machine Learning, 45(1):5\u201332. Breiter, H. C., Aharon, I., Kahneman, D., Dale, A., Shizgal, P. Functional imaging Bridle, J. S. Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimates of parameters. In Advances in Neural Information Processing Systems 2 , pp. 211\u2013217. Morgan Kaufmann, San Mateo, CA. Broomhead, D. S., Lowe, D. Multivariable functional interpolation and adaptive networks. Bromberg-Martin, E. S., Matsumoto, M., Hong, S., Hikosaka, O", "ea50bfb7-07dd-4b84-a121-8c4e45c65e4a": "Many of the arguments presented here are summarized from or inspired by Pearl . Nearly all activities require some ability to reason in the presence of uncertainty.\n\nIn fact, beyond mathematical statements that are true by definition, it is difficult to think of any proposition that is absolutely true or any event that is absolutely guaranteed to occur. There are three possible sources of uncertainty:  1. Inherent stochasticity in the system being modeled. For example, most interpretations of quantum mechanics describe the dynamics of subatomic particles as being probabilistic. We can also create theoretical scenarios that we postulate to have random dynamics, such as a hypothetical card game where we assume that the cards are truly shuffled into a random order. 2. Incomplete observability. Even deterministic systems can appear stochastic when we cannot observe all the variables that drive the behavior of the system. For example, in the Monty Hall problem, a game show contestant is asked to choose between three doors and wins a prize held behind the chosen door. Two doors lead to a goat while a third leads to a car. The outcome given the contestant\u2019s choice is deterministic, but from the contestant\u2019s point of view, the outcome is uncertain", "8c5009f5-c023-4219-b5b8-9eab73f5e32c": "14.7 Contractive Autoencoders  The contractive autoencoder (Rifai et al., 2011a,b) introduces an explicit regularizer on the code h = f(a), encouraging the derivatives of f to be as small as possible: at(a) | Ox  Q(h) =r | (14.18)  F The penalty Q(h) is the squared Frobenius norm (sum of squared elements) of the Jacobian matrix of partial derivatives associated with the encoder function. There is a connection between the denoising autoencoder and the contractive autoencoder: Alain and Bengio  showed that in the limit of small Gaussian input noise, the denoising reconstruction error is equivalent to a contractive penalty on the reconstruction function that maps x tor = g(f(x)).\n\nIn other words, denoising autoencoders make the reconstruction function resist small but finite-sized perturbations of the input, while contractive autoencoders make the feature extraction function resist infinitesimal perturbations of the input. When using the Jacobian-based contractive penalty to pretrain features f(a) for use with a classifier, the best classification accuracy usually results from applying the  518  CHAPTER 14", "ca6f8076-2a3b-4018-a0b3-25a7c679e562": "In addition, we have also found an expression for the bias value w0 given by (4.34). This tells us that a new vector x should be classi\ufb01ed as belonging to class C1 if y(x) = wT(x\u2212m) > 0 and class C2 otherwise. We now consider the generalization of the Fisher discriminant to K > 2 classes, and we shall assume that the dimensionality D of the input space is greater than the number K of classes. Next, we introduce D\u2032 > 1 linear \u2018features\u2019 yk = wT k x, where k = 1, . , D\u2032. These feature values can conveniently be grouped together to form a vector y. Similarly, the weight vectors {wk} can be considered to be the columns of a matrix W, so that y = WTx.\n\n(4.39) Note that again we are not including any bias parameters in the de\ufb01nition of y. The generalization of the within-class covariance matrix to the case of K classes follows from (4.28) to give and Nk is the number of patterns in class Ck", "168d13dc-e76d-4626-81d3-73857dd0e491": "Exercise 10.21 In the context of maximum likelihood, this redundancy is irrelevant because the parameter optimization algorithm (for example EM) will, depending on the initialization of the parameters, \ufb01nd one speci\ufb01c solution, and the other equivalent solutions play no role. In a Bayesian setting, however, we marginalize over all possible parameter values. We have seen in Figure 10.2 that if the true posterior distribution is multimodal, variational inference based on the minimization of KL(q\u2225p) will tend to approximate the distribution in the neighbourhood of one of the modes and ignore the others.\n\nAgain, because equivalent modes have equivalent predictive densities, this is of no concern provided we are considering a model having a speci\ufb01c number K of components. If, however, we wish to compare different values of K, then we need to take account of this multimodality. A simple approximate solution is to add a term ln K! onto the lower bound when used for model comparison and averaging. Exercise 10.22 tor, versus the number K of components for the Old Faithful data set", "2afd8562-4964-4271-b54f-3d4f0fdf4721": "We use  these properties liberally throughout the following derivations without highlighting exactly where we use each one. In the binary sparse coding model, the input v \u20ac R\u201d is generated from the model by adding Gaussian noise to the sum of m different components, which can each be present or absent. Each component is switched on or off by the corresponding hidden unit in h \u20ac {0,1}\u2122:  p(hi = 1) = o(bi), (19.19)  p(v | h) =N(v;Wh, 8\"), (19.20) where 6 is a learnable set of biases, W is a learnable weight matrix, and Bisa learnable, diagonal precision matrix. Training this model with maximum likelihood requires taking the derivative  with respect to the parameters", "3b2a57c6-8323-4a57-98e1-bdd4fcba9bdc": "If each element is in R, and the vector has n elements, then the vector lies in the set formed by taking the Cartesian product of R n times, denoted as R\u201d. When we need to explicitly identify the elements of a vector, we write them as a column enclosed in square brackets:  e=-| |. (2.1)  Ls |  We can think of vectors as identifying points in space, with each element giving the coordinate along a different axis. Sometimes we need to index a set of elements of a vector. In this case, we define a set containing the indices and write the set as a subscript. For example, to access 21, 3 and x6, we define the set S = {1,3,6} and write ag. We use the \u2014 sign to index the complement of a set.\n\nFor example x_, is the vector containing all elements of a except for x1, and w_g is the vector containing all elements of x except for 71, x3 and 26.  e Matrices: A matrix is a 2-D array of numbers, so each element is identified by two indices instead of just one", "1a35b381-c742-4239-9c31-8eb1c34e35f7": "As discussed in the earlier sections, the common learning and inference approaches for probabilistic graphic models, such as the (variational) EM algorithm, are special instances of SE and its teacher-student mechanism. The SE framework o\ufb00ers a generalized formulation for learning graphical and composite models.\n\nThe preceding sections have presented a standardized formalism of machine learning, on the basis of the standard equation of objective function, that provides a succinct, structured formulation of a broad design space of learning algorithms, and subsumes a wide range of known algorithms in a uni\ufb01ed manner. The simplicity, modularity, and generality of the framework is particularly appealing not only from the theoretical perspective but also because it o\ufb00ers guiding principles for mechanical design of algorithmic approaches to challenging problems, in the presence of diverse experience, and hence serves as an important step toward the goal of panoramic learning. In this section, we discuss the use of the standard equation to drive systematic design of new learning methods, which in turn yield various algorithmic approaches to problems in di\ufb00erent application domains. 9.1. Combining Rich Experience. As one of the original motivations for the standardization, the framework allows us to combine together all di\ufb00erent experience to learn models of interest", "c1cdd013-bab6-438f-8847-5e1a5fa6a05e": "The lack of generalization error in sparse coding\u2019s optimization-based encoding process may result in better generalization when sparse coding is used as a feature extractor for a classifier than when a parametric function is used to predict the code.\n\nCoates and Ng  demonstrated that sparse coding features generalize better for object recognition tasks than the features of a related model based on a parametric encoder, the linear-sigmoid autoencoder. Inspired by their work, Goodfellow e# al. showed that a variant of sparse coding generalizes better than other feature extractors in the regime where extremely few labels are available (twenty or fewer labels per class). The primary disadvantage of the nonparametric encoder is that it requires greater time to compute h given x because the nonparametric approach requires running an iterative algorithm. The parametric autoencoder approach, developed in chapter 14, uses only a fixed number of layers, often only one", "1d874bac-14f8-4add-a62b-e31fd20c58d7": "Given a horizon, say of 1000 steps, one can consider all possible actions, all possible resulting rewards, all possible next actions, all next rewards, and so on for all 1000 steps.\n\nGiven the assumptions, the rewards and probabilities of each possible chain of events can be determined, and one need only pick the best. But the tree of possibilities grows extremely rapidly; even if there were only two actions and two rewards, the tree would have 22000 leaves. It is generally not feasible to perform this immense computation exactly, but perhaps it could be approximated e\ufb03ciently. This approach would e\u21b5ectively turn the bandit problem into an instance of the full reinforcement learning problem. In the end, we may be able to use approximate reinforcement learning methods such as those presented in Part II of this book to approach this optimal solution. But that is a topic for research and beyond the scope of this introductory book. Exercise 2.11 (programming) Make a \ufb01gure analogous to Figure 2.6 for the nonstationary case outlined in Exercise 2.5. Include the constant-step-size \"-greedy algorithm with \u21b5=0.1. Use runs of 200,000 steps and, as a performance measure for each algorithm and parameter setting, use the average reward over the last 100,000 steps", "341dc58d-2576-4509-8832-7bf1429a9d09": "Our analysis is readily extended to more general Gaussian priors, for instance if we wish to associate a different hyperparameter with different subsets of the parameters wj. As usual, we consider a conjugate hyperprior over \u03b1 given by a gamma distribution p(\u03b1) = Gam(\u03b1|a0, b0) (10.166) governed by the constants a0 and b0.\n\nThe marginal likelihood for this model now takes the form where the joint distribution is given by We are now faced with an analytically intractable integration over w and \u03b1, which we shall tackle by using both the local and global variational approaches in the same model To begin with, we introduce a variational distribution q(w, \u03b1), and then apply the decomposition (10.2), which in this instance takes the form where the lower bound L(q) and the Kullback-Leibler divergence KL(q\u2225p) are de\ufb01ned by At this point, the lower bound L(q) is still intractable due to the form of the likelihood factor p(t|w). We therefore apply the local variational bound to each of the logistic sigmoid factors as before", "c1b1b4c3-1689-4136-bdcc-73632c4da698": "OPTIMIZATION FOR TRAINING DEEP MODELS  we alan d feted. AL eena lee 2 Ate el Land 21a ntti Lan ne nnn  https://www.deeplearningbook.org/contents/optimization.html    PELLOLINCU Lallly LOVUDSLIY, WO SIUYIC DESL ALBULILUI Lad CLllelL yeu. Currently, the most popular oppmization algorithms actively in use include SGD, SGD with momentum, RMSProp, RMSProp with momentum, AdaDelta, and Adam. The choice of which algorithm to use, at this point, seems to depend  largely on the user\u2019s familiarity with the algorithm (for ease of hyperparameter tuning). 8.6 Approximate Second-Order Methods  In this section we discuss the application of second-order methods to training deep networks. See LeCun e? al. for an earlier treatment of this subject.\n\nFor simplicity of exposition, the only objective function we examine is the empirical risk:  So (f(a: 8),y)", "3994d684-aade-47d5-bf52-4700547ac2d6": "We can simplify the situation slightly by rearranging equation 18.38 into a form in  621  CHAPTER 18. CONFRONTING THE PARTITION FUNCTION  which we need to know only the ratio of the two model\u2019s partition functions:  . x(4) DL lospalx: bad log pa (x; Oz) = \u00bb (1 eur a ) mlog 78 (18.39) We can thus determine whether M 4 is a better model than Mg without knowing the partition function of either model but only their ratio. As we will see shortly, we can estimate this ratio using importance sampling, provided that the two models are similar. If, however, we wanted to compute the actual probability of the test data under either M4 or Mg, we would need to compute the actual value of the partition functions. That said, if we knew the ratio of two partition functions, r = OnE and we knew the actual value of just one of the two, say Z (0,4), we could compute the value of the other:  Z 98) 7164)", "8cd62041-81cf-4ce9-8be8-8187ce5c76c6": "If we wanted to constrain 0(@) to be less than some constant k, we could construct a generalized Lagrange function  \u00a3(8,0; X,y) = J(6;X,y) + a((8) ki). (7.26) The solution to the constrained problem is given by  6* = argmin max L(0,a). (7.27) () a,a>0  As described in section 4.4, solving this problem requires modifying both 0 and a. Section 4.5 provides a worked example of linear regression with an L? constraint.\n\nMany different procedures are possible\u2014some may use gradient descent,  233  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  while others may use analytical solutions for where the gradient is zero\u2014but in all procedures a must increase whenever (6) > k and decrease whenever 2. (@) < k. All positive a encourage 2(@) to shrink. The optimal value a* will encourage 2(6) to shrink, but not so strongly to make Q(@) become less than k", "80f7f081-82cd-48f6-80c6-2b3d5f52fb76": "With the SE, designing an approach to a problem thus boils down to choosing and formulating what experience to use depending on the problem structure and available resources, without worrying too much about how to use the experience. This provides a potentially new modularized and repeatable way of producing ML approaches to di\ufb00erent problems, as compared to the previous practice of designing bespoke algorithm for each individual problem.\n\nSections 4 and 6 have discussed possible formulations of the diverse types of experience as an experience function to be plugged into Equation 9.1. It is still an open question how even more types of experience, such as massive knowledge graphs , can e\ufb03ciently be formulated as an experience function that assesses the \u2018goodness\u2019 of an input t. On the other hand, the discussion in the next subsection o\ufb00ers new opportunities that relieve users from having to manually specify every detail of the experience function. Instead, users only need to specify parts of the experience function of which they are certain, and leave the remaining parts plus the weights \u03bbi (Equation 9.1) to be automatically learned together with the target model. Case study: Text attribute transfer. As a case study of learning from rich experience, consider the problem of text attribute transfer where we want to rewrite a given piece of text to possess a desired attribute", "618187ba-6f0c-49b4-8b48-8bf62e0f2ed8": "Underflow occurs when numbers near zero are rounded to zero.\n\nMany functions behave qualitatively differently when their argument is zero rather than a small positive number. For example, we usually want to avoid division by zero (some  https://www.deeplearningbook.org/contents/numerical.html    78  CHAPTER 4. NUMERICAL COMPUTATION  software environments will raise exceptions when this occurs, others will return a result with a placeholder not-a-number value) or taking the logarithm of zero (this is usually treated as \u2014oo, which then becomes not-a-number if it is used for many further arithmetic operations). Another highly damaging form of numerical error is overflow. Overflow occurs when numbers with large magnitude are approximated as oo or \u2014oo. Further arithmetic will usually change these infinite values into not-a-number values. One example of a function that must be stabilized against underflow and overflow is the softmax function. The softmax function is often used to predict the probabilities associated with a multinoulli distribution", "412ee4fe-aa62-4e43-9835-2cbb9b79d3cb": "Thus, we perform signi\ufb01cance testing for a difference in accuracy on VOC 2007 rather than a difference in mAP. A caveat of this procedure is that it does not consider run-to-run variability when training the models, only variability arising from using a \ufb01nite sample of images for evaluation. The ResNet-50 (4\u00d7) results shown in Table 8 of the text show no clear advantage to the supervised or self-supervised models. With the narrower ResNet-50 architecture, however, supervised learning maintains a clear advantage over self-supervised learning.\n\nThe supervised ResNet-50 model outperforms the self-supervised model on all datasets with linear evaluation, and most (10 of 12) datasets with \ufb01ne-tuning. The weaker performance of the ResNet model compared to the ResNet (4\u00d7) A Simple Framework for Contrastive Learning of Visual Representations Food CIFAR10 CIFAR100 Birdsnap SUN397 Cars Aircraft VOC2007 DTD Pets Caltech-101 Flowers model may relate to the accuracy gap between the supervised and self-supervised models on ImageNet", "297fa5d2-5496-48ed-b279-cafd7f0014e4": "The DAE training procedure is illustrated in figure 14.3. We introduce a corruption process C(x | x), which represents a conditional distribution over  507  CHAPTER 14. AUTOENCODERS  https://www.deeplearningbook.org/contents/autoencoders.html    LU)  Figure 14.3: The computational graph of the cost function for a denoising autoencoder, which is trained to reconstruct the clean data point a from its corrupted version z. This is accomplished by minimizing the loss L = \u2014 log Pgecoder(#@ | h = f(#)), where & is a corrupted version of the data example x, obtained through a given corruption process C(# | x). Typically the distribution paecoder is a factorial distribution whose mean parameters are emitted by a feedforward network g.  corrupted samples x, given a data sample x. The autoencoder then learns a reconstruction distribution Pyeconstruct(x | x) estimated from training pairs (a, @) as follows:  1. Sample a training example x from the training data.\n\n2", "68aed7e8-0fd9-4d99-8c0e-a75019ca1eb8": "Appleton-Century-Crofts, Kingma, D., Ba, J. Adam: A method for stochastic optimization. ArXiv:1412.6980. Klopf, A. H. Brain function and adaptive systems\u2014A heterostatic theory. Technical Report AFCRL-72-0164, Air Force Cambridge Research Laboratories, Bedford, MA. A summary appears in Proceedings of the International Conference on Systems, Man, and Cybernetics . IEEE Systems, Man, and Cybernetics Society, Dallas, TX. Klopf, A. H. A comparison of natural and arti\ufb01cial intelligence. SIGART Newsletter, Klopf, A. H. The Hedonistic Neuron: A Theory of Memory, Learning, and Intelligence. Klopf, A. H. A neuronal model of classical conditioning. Psychobiology, 16(2):85\u2013125. Klyubin, A. S., Polani, D., Nehaniv, C. L", "291f9321-7d36-4d84-84d7-83680c506568": "Quantities of the form R{zj}, R{aj} and R{yk} are to be regarded as new variables whose values are found using the above equations. Because we are considering a sum-of-squares error function, we have the following standard backpropagation expressions: Again, we act on these equations with the R{\u00b7} operator to obtain a set of backpropagation equations in the form Finally, we have the usual equations for the \ufb01rst derivatives of the error and acting on these with the R{\u00b7} operator, we obtain expressions for the elements of the vector vTH The implementation of this algorithm involves the introduction of additional variables R{aj}, R{zj} and R{\u03b4j} for the hidden units and R{\u03b4k} and R{yk} for the output units. For each input pattern, the values of these quantities can be found using the above results, and the elements of vTH are then given by (5.110) and (5.111)", "61a9f91f-3f40-4327-9e56-e1e13c1e8943": "Indeed, the evidence is not de\ufb01ned if the prior is improper, as can be seen by noting that an improper prior has an arbitrary scaling factor (in other words, the normalization coef\ufb01cient is not de\ufb01ned because the distribution cannot be normalized). If we consider a proper prior and then take a suitable limit in order to obtain an improper prior (for example, a Gaussian prior in which we take the limit of in\ufb01nite variance) then the evidence will go to zero, as can be seen from (3.70) and Figure 3.12.\n\nIt may, however, be possible to consider the evidence ratio between two models \ufb01rst and then take a limit to obtain a meaningful answer. In a practical application, therefore, it will be wise to keep aside an independent test set of data on which to evaluate the overall performance of the \ufb01nal system. In a fully Bayesian treatment of the linear basis function model, we would introduce prior distributions over the hyperparameters \u03b1 and \u03b2 and make predictions by marginalizing with respect to these hyperparameters as well as with respect to the parameters w. However, although we can integrate analytically over either w or over the hyperparameters, the complete marginalization over all of these variables is analytically intractable", "0c831d25-706e-46e7-894e-93329ea1b73a": "Continuity means that when a sequence of parameters \u03b8t converges to \u03b8, the distributions P\u03b8t also converge to P\u03b8. However, it is essential to remember that the notion of the convergence of the distributions P\u03b8t depends on the way we compute the distance between distributions. The weaker this distance, the easier it is to de\ufb01ne a continuous mapping from \u03b8-space to P\u03b8-space, since it\u2019s easier for the distributions to converge. The main reason we care about the mapping \u03b8 \ufffd\u2192 P\u03b8 to be continuous is as follows. If \u03c1 is our notion of distance between two distributions, we would like to have a loss function \u03b8 \ufffd\u2192 \u03c1(P\u03b8, Pr) that is continuous, and this is equivalent to having the mapping \u03b8 \ufffd\u2192 P\u03b8 be continuous when using the distance between distributions \u03c1. 1More exactly, the topology induced by \u03c1 is weaker than that induced by \u03c1\u2032 when the set of convergent sequences under \u03c1 is a superset of that under \u03c1\u2032", "e79bb77c-66bb-4b3c-bb35-0f3765e386d4": "DSC 291 Machine Learning with Few Labels   Logistics  Grading Participation Homework Assignments \u00b0 Collaboration Policy o Late Policy \u00a9 Regrade Policy Paper Presentation Course Project  Grading  The class requirements include participation, a paper presentation, two homework assignments, and a course project. The grading breakdown is as follows:  e Homework assignments (30%) e Paper presentation (20%)  e Course project (46%) Participation (4%)  Participation  We appreciate everyone being actively involved in the class! There are several ways of earning participation credit, which will be capped at 4%:  e Piazza participation: The top ~10 contributors to Piazza will get 3.5%; others will get credit  http://zhiting.ucsd.edu/teaching/dsc291winter2023/logistics.html  DSC 291 Machine Learning with Few Labels   in proportion to the participation of the ~10th person. (To prevent abuse of the system, not all contributions are counted and instructors hold the right to determine to count contributions as positive or negative.) e Completing mid-quarter evaluation: Around the middle of the quarter, we will send out a survey to help us understand how the course is going, and how we can improve. Completing it is worth 1%", "91109a06-b21a-46ee-a370-e385090f207f": "From the product rule of probability we have p(x, Ck) = p(Ck|x)p(x). Because the factor p(x) is common to both terms, we can restate this result as saying that the minimum probability of making a mistake is obtained if each value of x is assigned to the class for which the posterior probability p(Ck|x) is largest. This result is illustrated for two classes, and a single input variable x, in Figure 1.24.\n\nFor the more general case of K classes, it is slightly easier to maximize the probability of being correct, which is given by which is maximized when the regions Rk are chosen such that each x is assigned to the class for which p(x, Ck) is largest. Again, using the product rule p(x, Ck) = p(Ck|x)p(x), and noting that the factor of p(x) is common to all terms, we see that each x should be assigned to the class having the largest posterior probability p(Ck|x). For many applications, our objective will be more complex than simply minimizing the number of misclassi\ufb01cations. Let us consider again the medical diagnosis problem", "0244e792-5b05-426e-b060-a801005c68a0": "Also, \ufb01netuning large pre-trained models on speci\ufb01c tasks might attenuate improvements due to preexisting generalization abilities of the model . Compositional Augmentation. To increase the compositional generalization abilities of models, recent efforts have also focused on compositional augmentations  where different fragments from different sentences are re-combined to create augmented examples.\n\nCompared to random swapping, compositional augmentation often requires more carefully-designed rules such as lexical overlap , neural-symbolic stack machines , and neural program synthesis . With the potential to greatly improve the generalization abilities to out-of-distribution data, compositional augmentation has been utilized in sequence labeling , semantic parsing , language modeling , and text generation . Instead of modifying tokens, sentence-level augmentation modi\ufb01es the entire sentence at once. Paraphrasing. Paraphrasing has been widely adopted as a data augmentation technique in various NLP tasks , as it generally provides more diverse augmented text with different word choices and sentence structures while preserving the meaning of the original text. The most popular is round-trip translation , a pipeline which \ufb01rst translates sentences into certain intermediate languages and then translates them back to generate paraphrases", "af01b9e4-0e51-4889-8566-ae3eaded8320": "Further science: the chemical microscope is In summary, the most sophisticated of these experiments is a technique that gives no obvious, no apparent way of revealing that the material was obtained. In this study, we examine how the compounds in the samples in question make up the composition of the chemical and its properties. The chemical composition military: arms defense battalion battalion cavalry In summary: 6th Panzer Field Division, Second Division.\\n\\n The main task of the battalion in the main counterinsurgency campaign was to counter the enemy in any counter-incursion.\n\nThe main objective of this campaign is to eliminate enemy groups and the remnants of legal: legal space religion and space This essay discusses the idea of space and time as a space, in both theoretical and conceptual terms, as not an individual time period or anything else. The emphasis is on time itself, rather than having a \ufb01xed central space. Space was the object of the \ufb01rst chapter, and politics: the primary referendum is This essay discusses the nature of the EU referendum", "b32015db-7252-465e-a07d-585ed5b0b409": "Henrion, L. N. Kanal, and J. F. Lemmer (Eds. ), Uncertainty in Arti\ufb01cial Intelligence, Volume 5, pp. 208\u2013219. Elsevier. Hassibi, B. and D. G. Stork . Second order derivatives for network pruning: optimal brain surgeon. In S. J. Hanson, J. D. Cowan, and C. L. Giles (Eds. ), Advances in Neural Information Processing Systems, Volume 5, pp. 164\u2013171. Morgan Kaufmann. Hastie, T. and W. Stuetzle . Principal curves. Journal of the American Statistical Association 84(106), 502\u2013516. Hastie, T., R. Tibshirani, and J. Friedman . The Elements of Statistical Learning. Springer", "fc4917d1-1753-4a51-9a7d-0c2a5735bfe9": "A recent model of V1 involves multiple quadratic filters for each neuron . Indeed our cartoon picture of \u201csimple cells\u201d and \u201ccomplex cells\u201d might create a nonexistent  distinction; simple cells and complex cells might both be the same kind of cell but with their \u201cparameters\u201d enabling a continuum of behaviors ranging from what we call \u201csimple\u201d to what we call \u201ccomplex.\u201d  It is also worth mentioning that neuroscience has told us relatively little about how to train convolutional networks.\n\nModel structures with parameter sharing across multiple spatial locations date back to early connectionist models of vision , but these models did not use the modern back-propagation algorithm and gradient descent. For example, the neocognitron  incorporated most of the model architecture design elements of the modern convolutional network but relied on a layer-wise unsupervised clustering algorithm. Lang and Hinton  introduced the use of back-propagation to train time-delay neural networks (TDNNs). To use contemporary terminology, TDNNs are one-dimensional convolutional networks applied to time series. Back- propagation applied to these models was not inspired by any neuroscientific observa-  361  CHAPTER 9", "f732766d-605c-46df-9e81-7b62f7f36fc7": "The particle \ufb01ltering, or sequential Monte Carlo, approach has appeared in the literature under various names including the bootstrap \ufb01lter , survival of the \ufb01ttest , and the condensation algorithm .\n\nExercises 13.1 (\u22c6) www Use the technique of d-separation, discussed in Section 8.2, to verify that the Markov model shown in Figure 13.3 having N nodes in total satis\ufb01es the conditional independence properties (13.3) for n = 2, . , N. Similarly, show that a model described by the graph in Figure 13.4 in which there are N nodes in total L samples is then drawn from this distribution and the new weights w(l) satis\ufb01es the conditional independence properties 13.2 (\u22c6 \u22c6) Consider the joint probability distribution (13.2) corresponding to the directed graph of Figure 13.3. Using the sum and product rules of probability, verify that this joint distribution satis\ufb01es the conditional independence property (13.3) for n = 2, . , N. Similarly, show that the second-order Markov model described by the joint distribution (13.4) satis\ufb01es the conditional independence property 13.3 (\u22c6) By using d-separation, show that the distribution p(x1,", "8ae37f89-c97d-4724-9179-fc9f315876ae": "In the \ufb01gure, E represents the input embedding, Ti represents the contextual representation of token i,  is the special symbol for classi\ufb01cation output, and  is the special symbol to separate non-consecutive token sequences. MNLI Multi-Genre Natural Language Inference is a large-scale, crowdsourced entailment classi\ufb01cation task . Given a pair of sentences, the goal is to predict whether the second sentence is an entailment, contradiction, or neutral with respect to the \ufb01rst one. QQP Quora Question Pairs is a binary classi\ufb01cation task where the goal is to determine if two questions asked on Quora are semantically equivalent .\n\nQNLI Question Natural Language Inference is a version of the Stanford Question Answering Dataset  which has been converted to a binary classi\ufb01cation task . The positive examples are (question, sentence) pairs which do contain the correct answer, and the negative examples are (question, sentence) from the same paragraph which do not contain the answer. SST-2 The Stanford Sentiment Treebank is a binary single-sentence classi\ufb01cation task consisting of sentences extracted from movie reviews with human annotations of their sentiment", "b3ad6583-6c47-4038-9122-ebc3a1026b36": "The brute force solution is for an unsupervised learner to learn a representation that captures all the reasonably salient generative factors h; and disentangles them from each other, thus making it easy to predict y from h, regardless of which h; is associated with y.\n\nIn practice, the brute force solution is not feasible because it is not possible to capture all or most of the factors of variation that influence an observation. For example, in a visual scene, should the representation always encode all the smallest objects in the background? It is a well-documented psychological phenomenon  ahat Livan Late nn C22 bn 2 aenannte-n Aha mnn fe LL ate apc ALA A  https://www.deeplearningbook.org/contents/representation.html    LUAL LUIlall VELUIYS lall LU PELCCIVE CHAMPS I LULL CLIVILUOLINECIL LUat ale LLUL immediately relevant to the task they are performing\u2014see, for example Simons and Levin . An important research frontier in semi-supervised learning is determining what to encode in each situation", "30697e70-e8b4-409a-beeb-cfc3b0d7f93d": "DEEP FEEDFORWARD NETWORKS  these low-rank relationships are often sufficient.\n\nLinear hidden units thus offer an effective way of reducing the number of parameters in a network. Softmax units are another kind of unit that is usually used as an output (as described in section 6.2.2.3) but may sometimes be used as a hidden unit. Softmax units naturally represent a probability distribution over a discrete variable with k possible values, so they may be used as a kind of switch. These kinds of hidden units are usually only used in more advanced architectures that explicitly learn to manipulate memory, as described in section 10.12. A few other reasonably common hidden unit types include  e Radial basis function (RBF), unit: h; = exp (\u20142 4 3||W. 5 \u2014 a|P). This function becomes more active as # approaches a temp ate W. ;. Because it saturates to 0 for most x, it can be difficult to optimize. Softplus: g(a) = \u00a2(a) = log(1+e*). This is a smooth version of the rectifier, introduced by Dugas et al", "0f401d33-8954-4521-bf24-44ae2ceb963a": "Exercise 14.6 From (14.22) we see that, having found \u03b1m and ym(x), the weights on the data points are updated using Because the term exp(\u2212\u03b1m/2) is independent of n, we see that it weights all data points by the same factor and so can be discarded. Thus we obtain (14.18). Finally, once all the base classi\ufb01ers are trained, new data points are classi\ufb01ed by evaluating the sign of the combined function de\ufb01ned according to (14.21). Because the factor of 1/2 does not affect the sign it can be omitted, giving (14.19). The exponential error function that is minimized by the AdaBoost algorithm differs from those considered in previous chapters. To gain some insight into the nature of the exponential error function, we \ufb01rst consider the expected error given by which is half the log-odds.\n\nThus the AdaBoost algorithm is seeking the best approximation to the log odds ratio, within the space of functions represented by the linear combination of base classi\ufb01ers, subject to the constrained minimization resulting from the sequential optimization strategy. This result motivates the use of the sign function in (14.19) to arrive at the \ufb01nal classi\ufb01cation decision", "edfc7da1-5ed0-4f88-939f-e3a7a5ad70e7": "CONVOLUTIONAL NETWORKS  is useful for writing proofs, it is not usually an important property of a neural network implementation. Instead, many neural network libraries implement a related function called the cross-correlation, which is the same as convolution but without flipping the kernel:  S(i,j) = (K * D(i = DDIM + mg + n)K (msn) (9.6)  Many machine learning libraries implement cross-correlation but call it convolution.\n\nIn this text we follow this convention of calling both operations convolution and specify whether we mean to flip the kernel or not in contexts where kernel flipping is relevant. In the context of machine learning, the learning algorithm will learn the appropriate values of the kernel in the appropriate place, so an algorithm based on convolution with kernel flipping will learn a kernel that is flipped relative to the kernel learned by an algorithm without the flipping. It is also rare for convolution to be used alone in machine learning; instead convolution is used simultaneously with other functions, and the combination of these functions does not commute regardless of whether the convolution operation flips its kernel or not. See figure 9.1 for an example of convolution (without kernel flipping) applied to a 2-D tensor", "f6184e1b-70f3-44d4-b803-6f6009636c93": "Let us look more closely at the gradient of log Z:  Vo log Z (18.5) VoZ  \u2014 le\" 8.6 Z ( )  Vo dix P(X) Z ( )  dx VoP(x) = =. 8.8 Z ( ) For models that guarantee p(x) > 0 for all x, we can substitute exp (log p(x))  for p(x):  V log p  x esp ( 0g P(x) (18.9) los p  > a log p(x) (18.10)  exp (log p&)) V  https://www.deeplearningbook.org/contents/partition.html    = Expoev Hes pes (18.11)  = > p(x)Volog p(x) (18.12)  CHAPTER 18. CONFRONTING THE PARTITION FUNCTION  = Ex p(x) VO log p(x). (18.13)  This derivation made use of summation over discrete x, but a similar result applies using integration over continuous x", "99fc16dc-1e0c-44ff-8252-55ed551c9432": "A benefit of learning the underlying causal factors, as pointed out by Sch\u00e9lkopf et al. , is that if the true generative process has x as an effect and y as a cause, then modeling p(x | y) is robust to changes in p(y).\n\nIf the cause-effect relationship were reversed, this would not be true, since by Bayes\u2019 rule, p(x | y) would be sensitive to changes in p(y). Very often, when we consider changes in distribution due to different domains, temporal nonstationarity, or changes in the nature of the task, the causal mechanisms remain invariant (\u201cthe laws of the universe are constant\u201d), while the marginal distribution over the underlying causes can change. Hence, better generalization and robustness to all kinds of changes can be expected via learning a generative model that attempts to recover the causal  543  CHAPTER 15. REPRESENTATION LEARNING  factors h and p(x | h). 15.4 Distributed Representation  Distributed representations of concepts\u2014representations composed of many ele- ments that can be set separately from each other\u2014are one of the most important tools for representation learning", "17cebac0-b3d6-40e1-b49f-bb2d1fb9be48": "Task #2: Next Sentence Prediction (NSP) Many important downstream tasks such as Question Answering (QA) and Natural Language Inference (NLI) are based on understanding the relationship between two sentences, which is not directly captured by language modeling. In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Speci\ufb01cally, when choosing the sentences A and B for each pretraining example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext).\n\nAs we show in Figure 1, C is used for next sentence prediction (NSP).5 Despite its simplicity, we demonstrate in Section 5.1 that pre-training towards this task is very bene\ufb01cial to both QA and NLI. 6 5The \ufb01nal model achieves 97%-98% accuracy on NSP. 6The vector C is not a meaningful sentence representation without \ufb01ne-tuning, since it was trained with NSP. The NSP task is closely related to representationlearning objectives used in Jernite et al. and Logeswaran and Lee", "a8d4e592-5ed1-46ea-b04c-4856d1f11cf7": "However, if we started from an undirected graph, then in general there will be an unknown normalization coef\ufb01cient 1/Z. As with the simple chain example of Figure 8.38, this is easily handled by working with an unnormalized version \ufffdp(x) of the joint distribution, where p(x) = \ufffdp(x)/Z. We \ufb01rst run the sum-product algorithm to \ufb01nd the corresponding unnormalized marginals \ufffdp(xi).\n\nThe coef\ufb01cient 1/Z is then easily obtained by normalizing any one of these marginals, and this is computationally ef\ufb01cient because the normalization is done over a single variable rather than over the entire set of variables as would be required to normalize \ufffdp(x) directly. At this point, it may be helpful to consider a simple example to illustrate the operation of the sum-product algorithm. Figure 8.51 shows a simple 4-node factor graph whose unnormalized joint distribution is given by In order to apply the sum-product algorithm to this graph, let us designate node x3 as the root, in which case there are two leaf nodes x1 and x4. Starting with the leaf nodes, we then have the following sequence of six messages The direction of \ufb02ow of these messages is illustrated in Figure 8.52", "bd40823f-0280-499b-ab51-8f59907b5feb": "For example, in a large collection of cardiac MRI videos from the UK Biobank, creating segmentations of the aorta 20 See the image tutorial at http://snorkel.stanford.edu/. Fig. 15 In a traditional programming stack, progressively higher-level languages and abstractions provide increasingly simple and declarative interfaces.\n\nSimilarly, we envision a code-as-supervision stack built on topofthebasicunitoflabelingfunctions,allowinguserstolabeltraining data in increasingly higher-level ways. Figure from  enabled a cardiologist to de\ufb01ne labeling functions for identifying rare aortic valve malformations . An even higher-level interface is natural language. The Babble Labble project  accepts natural language explanations of data points and then uses semantic parsers to parse these explanations into labeling functions. In this way, users without programming knowledge have the capability to write labeling functions just by explaining reasons why data points have speci\ufb01c labels. Another related approach is to use program synthesis techniques, combined with a small set of labeled data points, to automatically generate labeling functions", "7ab5a189-81fd-4116-b557-5f1ee4f04e92": "In addition, when xt+1 reached the left bound, \u02d9xt+1 was reset to zero. When it reached the right bound, the goal was reached and the episode was terminated. Each episode started from a random position xt 2 [\u22120.6, \u22120.4) and zero velocity. To convert the two continuous state variables to binary features, we used grid-tilings as in Figure 9.9.\n\nWe used 8 tilings, with each tile covering 1/8th of the bounded distance in each dimension, and asymmetrical o\u21b5sets as described in Section 9.5.4.1 The feature vectors x(s, a) created by tile coding were then combined linearly with the parameter vector to approximate the action-value function: form of function approximation.2 Shown is the negative of the value function (the costto-go function) learned on a single run. The initial action values were all zero, which was optimistic (all true values are negative in this task), causing extensive exploration to occur even though the exploration parameter, \", was 0. This can be seen in the middle-top panel of the \ufb01gure, labeled \u201cStep 428\u201d", "caa45738-9336-45d9-8b4d-3f3eb72498e6": "ha,-e a probabilislic formulalion of PeA, il s\u00abms natural 10 s\u00abk u Buye,ian approach 10 model seleclion. To do thi,. ,,\"'e nee<! 10 marginalize 001 the model paramele\" /'. \\V. und ,,' wilh \"\"peel to appropriate prior distribution'. This Can be done by u,ing a ,-ariation.l framework to .pproxim'le the allulylic.lly intractable murginaliUOi;oo, (Bi,hop. 1mb). 1lIc marginal likelihood v.lues. given by ttle ,'ari.,ionallower bour.d, cun lhen be c<>mpun:d for a r.nge of different '\"Tue' \"f;\\I ar.d Itie '\"Iue giving Iht largest marginal likelihood \",Iecloo_ l1ere we consider", "4e58a60f-5f61-4564-8009-784f102e0a8a": "The solution is to set both to zero.\n\nNewton\u2019s method can solve the problem in a single step because it is a positive definite quadratic problem. For small a, however, coordinate descent will make very slow progress because the first term does not allow a single variable to be changed to a value that differs  significantly from the current value of the other variable. 8.7.3 Polyak Averaging  Polyak averaging  consists of averaging several points in the trajectory through parameter space visited by an optimization algorithm. If t iterations of gradient descent visit points 0 ,...,@, then the output of the Polyak averaging algorithm is a) = 4 > a. On some problem classes, such as gradient descent applied to convex problems, this approach has strong convergence guarantees. When applied to neural networks, its justification is more heuristic, but it performs well in practice. The basic idea is that the optimization algorithm may leap back and forth across a valley several times without ever visiting a point near the bottom of the valley. The average of all the locations on either side should be close to the bottom of the valley though. In nonconvex problems, the path taken by the optimization trajectory can be very complicated and visit many different regions", "db1db3c0-224b-429e-8bbd-89715b3a95aa": "Around this time, Holland  incorporated temporal-di\u21b5erence ideas explicitly into his classi\ufb01er systems in the form of his bucket-brigade algorithm. A key step was taken by Sutton  by separating temporal-di\u21b5erence learning from control, treating it as a general prediction method. That paper also introduced the TD(\u03bb) algorithm and proved some of its convergence properties. As we were \ufb01nalizing our work on the actor\u2013critic architecture in 1981, we discovered a paper by Ian Witten  which appears to be the earliest publication of a temporal-di\u21b5erence learning rule. He proposed the method that we now call tabular TD(0) for use as part of an adaptive controller for solving MDPs. This work was \ufb01rst submitted for journal publication in 1974 and also appeared in Witten\u2019s 1976 PhD dissertation. Witten\u2019s work was a descendant of Andreae\u2019s early experiments with STeLLA and other trial-and-error learning systems.\n\nThus, Witten\u2019s 1977 paper spanned both major threads of reinforcement learning research\u2014trial-and-error learning and optimal control\u2014while making a distinct early contribution to temporal-di\u21b5erence learning", "91205039-eb3f-4392-b9ac-1c65df993b17": "As we mentioned above, this is actually desirable in a nonstationary environment, and problems that are e\u21b5ectively nonstationary are the most common in reinforcement learning.\n\nIn addition, sequences of step-size parameters that meet the conditions (2.7) often converge very slowly or need considerable tuning in order to obtain a satisfactory convergence rate. Although sequences of step-size parameters that meet these convergence conditions are often used in theoretical work, they are seldom used in applications and empirical research. Exercise 2.4 If the step-size parameters, \u21b5n, are not constant, then the estimate Qn is a weighted average of previously received rewards with a weighting di\u21b5erent from that given by (2.6). What is the weighting on each prior reward for the general case, analogous to (2.6), in terms of the sequence of step-size parameters? \u21e4 di\ufb03culties that sample-average methods have for nonstationary problems. Use a modi\ufb01ed version of the 10-armed testbed in which all the q\u21e4(a) start out equal and then take independent random walks (say by adding a normally distributed increment with mean zero and standard deviation 0.01 to all the q\u21e4(a) on each step)", "859ba622-411d-4274-a558-c24a9409e408": "Colorful image colorization. In European conference on computer vision, pp. 649\u2013666. Springer, 2016. Zhuang, C., Zhai, A. L., and Yamins, D. Local aggregation for unsupervised learning of visual embeddings. In Proceedings of the IEEE International Conference on Computer Vision, pp. 6002\u20136012, 2019. A Simple Framework for Contrastive Learning of Visual Representations In our default pretraining setting (which is used to train our best models), we utilize random crop (with resize and random \ufb02ip), random color distortion, and random Gaussian blur as the data augmentations. The details of these three augmentations are provided below. Random crop and resize to 224x224 We use standard Inception-style random cropping . The crop of random size (uniform from 0.08 to 1.0 in area) of the original size and a random aspect ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. This crop is \ufb01nally resized to the original size", "234e4ff1-d575-4d3b-9513-1432130a2c7b": "When x is continuous, the Shannon entropy is known as the differential entropy.\n\nIf we have two separate probability distributions P(x) and Q (x) over the same random variable x, we can measure how different these two distributions are using the Kullback-Leibler (KL) divergence:  P(x) Q(x)  In the case of discrete variables, it is the extra amount of information (measured in bits if we use the base-2 logarithm, but in machine learning we usually use nats and the natural logarithm) needed to send a message containing symbols drawn from probability distribution P, when we use a code that was designed to minimize the length of messages drawn from probability distribution Q.  Dui (P||Q) = Exp tog | = Ba-pllog Pl) \u2014 log Q(@)). (3.50)  The KL divergence has many useful properties, most notably being non-negative. The KL divergence is 0 if and only if P and Q are the same distribution in the case of discrete variables, or equal \u201calmost everywhere\u201d in the case of continuous variables", "427679a6-8ae1-441f-8e30-0bf1c27c427e": "Maxima of many classes of random functions become exponentially rare in high-dimensional space, just as minima do. There may also be wide, flat regions of constant value. In these locations, the gradient and the Hessian are all zero. Such degenerate locations pose major problems for all numerical optimization algorithms. In a convex problem, a wide, flat region must consist entirely of global minima, but in a general optimization problem, such a region could correspond to a high value of the objective function.\n\n8.2.4 Cliffs and Exploding Gradients  NT awww] nw aterrnnlen weld we nwnee Laverne AL-Awn Laven nvrte nen nlee aban nanctnnn wanna Liin~  https://www.deeplearningbook.org/contents/optimization.html    ANCULAL LICLWULKS WILL iulatly layeLs VLLCLL LlAVE CAULLELUCLY SLCCp LEPIVLIS LOUSCLLULVILLLL  cliffs, as illustrated in figure 8.3. These result from the multiplication of severa large weights together", "ff51f572-041d-4387-93e0-da3e49256c07": "The joint distribution over latent and observed variables can be represented by the graphical model shown in Figure 14.7.\n\nThe complete-data log likelihood function then takes the form Exercise 14.13 The EM algorithm begins by \ufb01rst choosing an initial value \u03b8old for the model parameters. In the E step, these parameter values are then used to evaluate the posterior probabilities, or responsibilities, of each component k for every data point n given by The responsibilities are then used to determine the expectation, with respect to the posterior distribution p(Z|t, \u03b8old), of the complete-data log likelihood, which takes the form In the M step, we maximize the function Q(\u03b8, \u03b8old) with respect to \u03b8, keeping the \u03b3nk \ufb01xed. For the optimization with respect to the mixing coef\ufb01cients \u03c0k we need to take account of the constraint \ufffd k \u03c0k = 1, which can be done with the aid of a Lagrange multiplier, leading to an M-step re-estimation equation for \u03c0k in the form Exercise 14.14 Note that this has exactly the same form as the corresponding result for a simple mixture of unconditional Gaussians given by (9.22)", "376d8565-6281-42a5-8676-efe735c770f1": ", W} and W is the total number of weights and biases.\n\nThe Hessian plays an important role in many aspects of neural computing, including the following: 1. Several nonlinear optimization algorithms used for training neural networks are based on considerations of the second-order properties of the error surface, which are controlled by the Hessian matrix . 2. The Hessian forms the basis of a fast procedure for re-training a feed-forward network following a small change in the training data . 3. The inverse of the Hessian has been used to identify the least signi\ufb01cant weights in a network as part of network \u2018pruning\u2019 algorithms . 4. The Hessian plays a central role in the Laplace approximation for a Bayesian neural network (see Section 5.7). Its inverse is used to determine the predictive distribution for a trained network, its eigenvalues determine the values of hyperparameters, and its determinant is used to evaluate the model evidence. Various approximation schemes have been used to evaluate the Hessian matrix for a neural network", "39edb4cf-e2c2-4c4f-a6a0-0d3ed88b63bd": "Recurrent Networks as Directed Graphical Models  In the example recurrent network we have developed so far, the losses L\u00ae were  https://www.deeplearningbook.org/contents/rnn.html    cross-entropies between training targets 4 M) and outputs o\u201d) | As with a feedforward network, it is in principle possible to use almost any loss with a recurrent network. 380  CHAPTER 10. SEQUENCE MODELING: RECURRENT AND RECURSIVE NETS  The loss should be chosen based on the task.\n\nAs with a feedforward network, we usually wish to interpret the output of the RNN as a probability distribution, and we usually use the cross-entropy associated with that distribution to define the loss. Mean squared error is the cross-entropy loss associated with an output distribution chat is a unit Gaussian, for example, just as with a feedforward network. When we use a predictive log-likelihood training objective, such as equa- ion 10.12, we train the RNN to estimate the conditional distribution of the next sequence element y) given the past inputs", "03a872dc-55dc-40ca-aac0-e15fd61a8f02": "It is one of the primary mechanisms responsible for learning. The parameters, or weights, adjusted by learning algorithms correspond to synaptic e\ufb03cacies. As we detail below, modulation of synaptic plasticity via the neuromodulator dopamine is a plausible mechanism for how the brain might implement learning algorithms like many of those described in this book.\n\nLinks between neuroscience and computational reinforcement learning begin as parallels between signals in the brain and signals playing prominent roles in reinforcement learning theory and algorithms. In Chapter 3 we said that any problem of learning goal-directed behavior can be reduced to the three signals representing actions, states, and rewards. However, to explain links that have been made between neuroscience and reinforcement learning, we have to be less abstract than this and consider other reinforcement learning signals that correspond, in certain ways, to signals in the brain. In addition to reward signals, these include reinforcement signals (which we argue are di\u21b5erent from reward signals), value signals, and signals conveying prediction errors. When we label a signal by its function in this way, we are doing it in the context of reinforcement learning theory in which the signal corresponds to a term in an equation or an algorithm", "c63d9afa-8501-402b-989a-b80095f37d06": "Ideally, we would like to arrive at the global minimum, but this  might not be possible. This local minimum performs poorly and should be avoided}  x  Figure 4.3: Approximate minimization. Optimization algorithms may fail to find a global minimum when there are multiple local minima or plateaus present. In the context of deep learning, we generally accept such solutions even though they are not truly minimal, so long as they correspond to significantly low values of the cost function. critical points are points where every element of the gradient is equal to zero. The directional derivative in direction wu (a unit vector) is the slope of the function f in direction u. In other words, the directional derivative is the derivative of the function f(a + au) with respect to a, evaluated at q = 0. Using the chain  https://www.deeplearningbook.org/contents/numerical.html    rule, we can see that da f(a + au) evaluates to u' V# f(a) when a = 0.\n\nTo minimize f, we would like to find the direction in which f decreases the fastest", "f2e11f98-0586-41b5-b32b-459ef22a5472": "Stone, J. V. .\n\nIndependent Component Analysis: A Tutorial Introduction. MIT Press. Sung, K. K. and T. Poggio . Example-based learning for view-based human face detection. A.I. Memo 1521, MIT. Sutton, R. S. and A. G. Barto . Reinforcement Learning: An Introduction. MIT Press. Tarassenko, L. Novelty detection for the identi\ufb01cation of masses in mamograms. In Proceedings Fourth IEE International Conference on Arti\ufb01cial Neural Networks, Volume 4, pp. 442\u2013447. IEE. Tax, D. and R. Duin . Data domain description by support vectors. In M. Verleysen (Ed. ), Proceedings European Symposium on Arti\ufb01cial Neural Networks, ESANN, pp. 251\u2013256. D. Facto Press. Teh, Y", "e5bd1331-51d3-4dfa-b2e3-15f6b55b01dd": "where the elements of x could be discrete or continuous or a combination of these. Denote the mean and covariance of p(x|k) by \u00b5k and \u03a3k, respectively. Show that the mean and covariance of the mixture distribution are given by (9.49) and (9.50). 9.13 (\u22c6 \u22c6) Using the re-estimation equations for the EM algorithm, show that a mixture of Bernoulli distributions, with its parameters set to values corresponding to a maximum of the likelihood function, has the property that Hence show that if the parameters of this model are initialized such that all components have the same mean \u00b5k = \ufffd\u00b5 for k = 1, . , K, then the EM algorithm will converge after one iteration, for any choice of the initial mixing coef\ufb01cients, and that this solution has the property \u00b5k = x. Note that this represents a degenerate case of the mixture model in which all of the components are identical, and in practice we try to avoid such solutions by using an appropriate initialization", "2861e342-276c-4ff2-a3b2-5aad127104c5": "This corresponds to acting on the original forward-propagation and backpropagation equations with a differential operator vT\u2207. Pearlmutter  used the notation R{\u00b7} to denote the operator vT\u2207, and we shall follow this convention. The analysis is straightforward and makes use of the usual rules of differential calculus, together with the result R{w} = v. (5.97) The technique is best illustrated with a simple example, and again we choose a two-layer network of the form shown in Figure 5.1, with linear output units and a sum-of-squares error function. As before, we consider the contribution to the error function from one pattern in the data set.\n\nThe required vector is then obtained as usual by summing over the contributions from each of the patterns separately. For the two-layer network, the forward-propagation equations are given by We now act on these equations using the R{\u00b7} operator to obtain a set of forward propagation equations in the form where vji is the element of the vector v that corresponds to the weight wji", "4a49d80d-2a99-4934-b126-da2a6f623bed": "The joint distribution for a sequence of N observations under this model is given by From the d-separation property, we see that the conditional distribution for observaSection 8.2 tion xn, given all of the observations up to time n, is given by which is easily veri\ufb01ed by direct evaluation starting from (13.2) and using the product rule of probability. Thus if we use such a model to predict the next observation Exercise 13.1 in a sequence, the distribution of predictions will depend only on the value of the immediately preceding observation and will be independent of all earlier observations. In most applications of such models, the conditional distributions p(xn|xn\u22121) that de\ufb01ne the model will be constrained to be equal, corresponding to the assumption of a stationary time series. The model is then known as a homogeneous Markov chain. For instance, if the conditional distributions depend on adjustable parameters (whose values might be inferred from a set of training data), then all of the conditional distributions in the chain will share the same values of those parameters.\n\nAlthough this is more general than the independence model, it is still very restrictive. For many sequential observations, we anticipate that the trends in the data over several successive observations will provide important information in predicting the next value", "6d65143f-8714-4079-b32e-344adfd51d9d": "Haussler (Ed. ), Proceedings Fifth Annual Workshop on Computational Learning Theory (COLT), pp. 144\u2013152. ACM. Box, G. E. P., G. M. Jenkins, and G. C. Reinsel . Time Series Analysis. Prentice Hall. Box, G. E. P. and G. C. Tao . Bayesian Inference in Statistical Analysis. Wiley. Boyd, S. and L. Vandenberghe . Convex Optimization. Cambridge University Press. Boyen, X. and D. Koller . Tractable inference for complex stochastic processes. In G. F. Cooper and S. Moral (Eds. ), Proceedings 14th Annual Conference on Uncertainty in Arti\ufb01cial Intelligence (UAI), pp. 33\u201342. Morgan Kaufmann", "ceed3ed1-1c8c-4aa0-bc72-baafd7b11b8a": "The Journal of Machine Learning Research, 9999:3371\u20133408, 2010. See \ufb01gures 4 and 5 for visualisations of latent space and corresponding observed space of models learned with SGVB. The variational lower bound (the objective to be maximized) contains a KL term that can often be integrated analytically. Here we give the solution when both the prior p\u03b8(z) = N(0, I) and the posterior approximation q\u03c6(z|x(i)) are Gaussian. Let J be the dimensionality of z. Let \u00b5 and \u03c3 denote the variational mean and s.d. evaluated at datapoint i, and let \u00b5j and \u03c3j simply denote the j-th element of these vectors. Then: When using a recognition model q\u03c6(z|x) then \u00b5 and s.d.\n\n\u03c3 are simply functions of x and the variational parameters \u03c6, as exempli\ufb01ed in the text. In variational auto-encoders, neural networks are used as probabilistic encoders and decoders", "8e5e6fa4-f31d-4c10-9690-5a5c07e7be0f": "Furthermore, the splits in a decision tree are hard, so that each region of input space is associated with one, and only one, leaf node model. The last issue is particularly problematic in regression where we are typically aiming to model smooth functions, and yet the tree model produces piecewise-constant predictions with discontinuities at the split boundaries. We have seen that standard decision trees are restricted by hard, axis-aligned splits of the input space. These constraints can be relaxed, at the expense of interpretability, by allowing soft, probabilistic splits that can be functions of all of the input variables, not just one of them at a time. If we also give the leaf models a probabilistic interpretation, we arrive at a fully probabilistic tree-based model called the hierarchical mixture of experts, which we consider in Section 14.5.3.\n\nAn alternative way to motivate the hierarchical mixture of experts model is to start with a standard probabilistic mixtures of unconditional density models such as Gaussians and replace the component densities with conditional distributions. Here Chapter 9 we consider mixtures of linear regression models (Section 14.5.1) and mixtures of logistic regression models (Section 14.5.2). In the simplest case, the mixing coef\ufb01cients are independent of the input variables", "03d106af-7432-4ab0-85b5-dbbcbc7e8e34": "As a increases in size, however, the Hessian becomes dominated by the al diagonal,  Require: Initial parameter 0 Require: Training set of m examples while stopping criterion not met do Compute gradient: g + 1Ve yy, LF (x; 0), y) Compute Hessian: H < 2 V3 Lf (a; 0), y) Compute Hessian inverse: H ~! Compute update: A@ = \u2014H~'g Apply update: 0=0+A0 end while  308  https://www.deeplearningbook.org/contents/optimization.html    CHAPTER 8. OPTIMIZATION FOR TRAINING DEEP MODELS  and the direction chosen by Newton\u2019s method converges to the standard gradient divided by a. When strong negative curvature is present, a@ may need to be so large that Newton\u2019s method would make smaller steps than gradient descent with a properly chosen learning rate. Beyond the challenges created by certain features of the objective function, such as saddle points, the application of Newton\u2019s method for training large neural networks is limited by the significant computational burden it imposes", "a9f9b026-32aa-44ca-aa5e-1f78206e02f1": "We illustrate the result of running K-means to convergence, for any particular value of K, by re-drawing the image replacing each pixel vector with the {R, G, B} intensity triplet given by the centre \u00b5k to which that pixel has been assigned. Results for various values of K are shown in Figure 9.3. We see that for a given value of K, the algorithm is representing the image using a palette of only K colours. It should be emphasized that this use of K-means is not a particularly sophisticated approach to image segmentation, not least because it takes no account of the spatial proximity of different pixels.\n\nThe image segmentation problem is in general extremely dif\ufb01cult ing the initial images together with their K-means segmentations obtained using various values of K. This also illustrates of the use of vector quantization for data compression, in which smaller values of K give higher compression at the expense of poorer image quality. and remains the subject of active research and is introduced here simply to illustrate the behaviour of the K-means algorithm. We can also use the result of a clustering algorithm to perform data compression", "a2c60678-bb3c-46f9-a782-6c58f1f32bc7": "In this section we\u2019ll give an example where we use a neural network for the probabilistic encoder q\u03c6(z|x) (the approximation to the posterior of the generative model p\u03b8(x, z)) and where the parameters \u03c6 and \u03b8 are optimized jointly with the AEVB algorithm. Let the prior over the latent variables be the centered isotropic multivariate Gaussian p\u03b8(z) = N(z; 0, I). Note that in this case, the prior lacks parameters. We let p\u03b8(x|z) be a multivariate Gaussian (in case of real-valued data) or Bernoulli (in case of binary data) whose distribution parameters are computed from z with a MLP (a fully-connected neural network with a single hidden layer, see appendix C).\n\nNote the true posterior p\u03b8(z|x) is in this case intractable. While there is much freedom in the form q\u03c6(z|x), we\u2019ll assume the true (but intractable) posterior takes on a approximate Gaussian form with an approximately diagonal covariance", "4708ed08-1c9c-4570-b7a8-10e0604da7d0": "Deep learning algorithms are typically applied to extremely complicated domains such as images, audio sequences and text, for which the true generation process essentially involves simulating the entire universe. To some extent, we are always trying to fit a square peg (the data-generating process) into a round hole (our model family).\n\nWhat this means is that controlling the complexity of the model is not a simple matter of finding the model of the right size, with the right number of parameters. Instead, we might find\u2014and indeed in practical deep learning scenarios, we almost always do find\u2014that the best fitting model (in the sense of minimizing generalization error) is a large model that has been regularized appropriately. We now review several strategies for how to create such a large, deep regularized  td  https://www.deeplearningbook.org/contents/regularization.html    model. 225  CHAPTER 7. REGULARIZATION FOR DEEP LEARNING  7.1 Parameter Norm Penalties  Regularization has been used for decades prior to the advent of deep learning. Linear models such as linear regression and logistic regression allow simple, straightforward, and effective regularization strategies", "f6bbea4a-1a66-4362-84cd-5a548aeac702": "If we set the derivative of ln p(D|\u00b5) Section 2.4 with respect to \u00b5 equal to zero, we obtain the maximum likelihood estimator his time, including Boyle and Hooke in England.\n\nWhen he returned to Switzerland, he taught mechanics and became Professor of Mathematics at Basel in 1687. Unfortunately, rivalry between Jacob and his younger brother Johann turned an initially productive collaboration into a bitter and public dispute. Jacob\u2019s most signi\ufb01cant contributions to mathematics appeared in The Art of Conjecture published in 1713, eight years after his death, which deals with topics in probability theory including what has become known as the Bernoulli distribution. which is also known as the sample mean. If we denote the number of observations of x = 1 (heads) within this data set by m, then we can write (2.7) in the form so that the probability of landing heads is given, in this maximum likelihood framework, by the fraction of observations of heads in the data set. Now suppose we \ufb02ip a coin, say, 3 times and happen to observe 3 heads. Then N = m = 3 and \u00b5ML = 1. In this case, the maximum likelihood result would predict that all future observations should give heads", "6731ce39-46aa-4a4b-8ecf-dae80fe22102": "Perhaps 1 for RBM on a small image patch, or 5-50 for a more complicated model like a DBM.\n\nInitialize a set of m samples fx, ee x(\u2122)} to random values (e.g., from a uniform or normal distribution, or possibly a distribution with marginals matched to the model\u2019s marginals). wehtln 24 nner n wend AA  https://www.deeplearningbook.org/contents/partition.html    \u201cSample a yinibatch o examples {x x(\u2122) from the training set gi my i=l Pa TSe BOOP : for i = T to k do  for 7 = 1 tom do x) < gibbs_update(x)), end for end for ge g\u2014 2, Volog p(x; 6). 0+ 0+ eg. end while  burn-in for deep models. SML is able to train deep models efficiently. Marlin et al. compared SML to many other criteria presented in this chapter", "02e36df3-c183-4dd2-bce9-a83c21a38fa8": "DEEP GENERATIVE MODELS  unnormalized log conditional distribution:  1 Tt Tv  https://www.deeplearningbook.org/contents/generative_models.html    logN(v;Wh, 8 *)=\u20143(v\u2014-Wh)' 8(v\u2014 Wh) + f (8). (20.39)  Here f encapsulates all the terms that are a function only of the parameters and not the random variables in the model. We can discard f because its only role is to normalize the distribution, and the partition function of whatever energy function we choose will carry out that role.\n\nIf we include all the terms (with their sign flipped) involving v from equa- tion 20.39 in our energy function and do not add any other terms involving v, then our energy function will represent the desired conditional p(v | h). We have some freedom regarding the other conditional distribution, p(h | v). Note that equation 20.39 contains a term  sh! W' BWh. (20.40)  This term cannot be included in its entirety because it includes hjh; terms. These correspond to edges between the hidden units"}