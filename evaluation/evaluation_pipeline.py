from LMTutor.model.llm_langchain_tutor import LLMLangChainTutor
import re
import json
import pandas as pd
from tqdm import tqdm
import argparse
import openai

OPENAI_API_KEY = os.environ['OPENAI_API_KEY'] if 'ANTHROPIC_API_KEY' in os.environ else None
ANTHROPIC_API_KEY = os.environ['ANTHROPIC_API_KEY'] if 'ANTHROPIC_API_KEY' in os.environ else None

class LMTutorEvaluator:
    def __init__(self, evaluator) -> None:
        self.evaluator = evaluator

        if self.evaluator in ['chatgpt']:
            openai.api_key = OPENAI_API_KEY
            self.model = "gpt-4"

        if self.evaluator in ['claude']:
            assert ANTHROPIC_API_KEY is not None, "api_key is none!"

        self.system_prompt = "You are a helpful assistant."

    def evaluate(self, question, answer1, answer2, ground_truth):
        if self.evaluator == 'chatgpt':
            return self.evaluate_by_chatgpt(question, answer1, answer2, ground_truth)

    def evaluate_by_claude(self, question, answer1, answer2, ground_truth):

        template = ChatPromptTemplate.from_messages(
            [
                SystemMessage(
                    content=(
                        self.system_prompt
                    )
                ),
                HumanMessage(
                    content=(
                        f"Consider the following answers in response to the question:\n{question} \n\n Answer-1 (Vicuna):\n{answer1} \n\n Answer-2 (GPT-3.5):\n{answer2} \n\n Ground Truth:\n{ground_truth}\n\n Which answer is more accurate and better-written compared to the ground truth? Please answer with one of the following options and explaine why.\n\n Options:\n(A) Vicuna\n(B) GPT-3.5\n(C) Both answers are equally well-written and accurate.\n\n"
                    )
                ),
            ]
        )
        chat = ChatAnthropic(model='claude-2')
        response = chat(template)
        return response


    def evaluate_by_chatgpt(self, question, answer1, answer2, ground_truth):
        response = openai.ChatCompletion.create(
            model=self.model,
            messages=[
                {"role": "system", "content": self.system_prompt},
                {
                    "role": "user",
                    "content": f"Consider the following answers in response to the question:\n{question} \n\n Answer-1 (Vicuna):\n{answer1} \n\n Answer-2 (GPT-3.5):\n{answer2} \n\n Ground Truth:\n{ground_truth}\n\n Which answer is more accurate and better-written compared to the ground truth? Please answer with one of the following options and explaine why.\n\n Options:\n(A) Vicuna\n(B) GPT-3.5\n(C) Both answers are equally well-written and accurate.\n\n"
                },
            ],
            temperature=0,
            max_tokens=150,
        )

        # Extract and print the chosen option
        response_message = response['choices'][0]['message']['content'].strip()

            # Split the response to separate the option chosen from the reason
        split_response = response_message.split("\n\n")
        option_chosen = split_response[0].split(": ")[0]
        reason = split_response[1] if len(split_response) > 1 else "No detailed reason provided."

        # Collect the results in a dictionary
        result = {
            "question": question,
            "option_chosen": option_chosen,
            "reason": reason,
        }

        return response

    def evaluation_pipeline(self, questions_path, generated_answers_data_path, save_path):
        if generated_answers_data_path is None:
            lmtutor_eval = LMTutorAnswerGenerator()
            lmtutor_eval.load_questions(questions_path)
            lmtutor_eval.generate_answers()
            lmtutor_answers_df = pd.DataFrame({
                "lmtutor_answers": lmtutor_eval.answers,
                "questions": lmtutor_eval.questions,
                "ground_truth": lmtutor_eval.ground_truth
                }, index_col="question")

            gpt_eval = GPTAnswerGenerator()
            gpt_eval.load_questions(questions_path)
            gpt_eval.generate_answers()
            gpt_answers_df = pd.DataFrame({
                "gpt_answers": gpt_eval.answers,
                "questions": gpt_eval.questions
            }, index_col="question")

            # Save answers generated by LMTutor and GPT evaluators
            merged_answers_df = pd.concat([lmtutor_answers_df, gpt_answers_df])
            merged_answers_df.to_csv(os.path.join(save_path, "llm_answers.csv"))
        else:
            merged_answers_df = pd.read_csv(generated_answers_data_path)

        # Get Performance
        response = []
        for idx, row in merged_answers_df.iterrows():
            curr_response = self.evaluate(
                row["questions"],
                row["lmtutor_answers"],
                row["gpt_answers"],
                row["ground_truth"]
            )
            response.append(curr_response)

        df = pd.DataFrame(response)
        results = [i['option_chosen'] for i in response]
        accuracy = (df.get('results') == '(A) Vicuna').sum() / df.shape[0]

        print(f"LLM-as-a-judge performance: Evaluator = {self.evaluator}, LLMTutor Accuracy = {accuracy}")
        df.to_csv(f"LLM_as_a_judge_performance_{self.evaluator}.csv")

class AnswerGenerator:
    def __init__(self) -> None:
        pass

    def load_questions(self, question_path):
        self.questions = []
        self.ground_truth = []
        ### txt files
        if question_path.endswith('.txt'):
            with open(question_path, "r") as f:
                for each in f:
                    self.questions.append(re.sub(r"^\d+\.\s*", "", each))
        elif question_path.endswith('.csv'):
            q_file = pd.read_csv(question_path)
            for _, each in q_file.iterrows():
                self.questions.append(each['questions'])
                self.ground_truth.append(each['ground_truth'])

    def save_answers(self, save_path):
        with open(save_path, "w") as f:
            for each in self.answers:
                json.dump(each, f)
                f.write("\n")

    def similarity_search_statistics(self):
        raise NotImplementedError

    def generate_answers(self):
        raise NotImplementedError

class LMTutorAnswerGenerator(AnswerGenerator):
    def __init__(self) -> None:
        super().__init__()
        self.lmtutor = LLMLangChainTutor(
            embedding="instruct_embedding",
            llm="hf_lmsys/vicuna-7b-v1.3",
            embed_device="cuda:6",
            llm_device="cuda:7",
        )
        self.lmtutor.load_vector_store("/home/yuheng/LMTutor/data/DSC-291-vector-lec")
        self.lmtutor.initialize_hf_llm()

    def generate_answers(self):  # based on the questions
        self.answers = []
        for q in tqdm(self.questions, desc="answering questions"):
            self.lmtutor.first_conversation = True
            self.lmtutor.memory.clear()

            answer, context = self.generate(q)
            self.answers.append({"question": q, "lmtutor_answer": answer, "lmtutor_context": context})

    def generate(self, question):
        return self.lmtutor.conversational_qa(question)

    def get_context_similarity(self):
        for answer in tqdm(self.answers, desc="Generating context similarity"):
            context_sim = []
            for cxt in answer["lmtutor_context"]:
                pass

    def similarity_search_statistics(self):
        result = []
        for q in tqdm(self.questions, desc="processing questions"):
            result.append([each[1] for each in self.lmtutor.similarity_search_thres(query=q)])

        result = pd.DataFrame(result)

        return result

class GPTAnswerGenerator(AnswerGenerator):
    def __init__(self, api_key, gpt_model: str) -> None:
        super().__init__()

        openai.api_key = api_key
        self.model = gpt_model
        self.system_prompt = "You are a teaching assistant for the Advanced Data Mining course. You are asked to answer questions from students."

    def generate_answers(self):
        self.answers = []
        for q in tqdm(self.questions, desc="answering questions"):
            answer = self.generate(q)
            self.answers.append({"question": q, "gpt_answer": answer})

    def generate(self, question):
        if self.model == "gpt-3.5-turbo":
            for i in range(3):
                try:
                    response = openai.ChatCompletion.create(
                        model=self.model,
                        messages=[
                            {"role": "system", "content": self.system_prompt},
                            {
                                "role": "user",
                                "content": question,
                            },
                        ],
                        temperature=0,
                        max_tokens=512,
                    )

                    response = response['choices'][0]['message']['content']
                    break
                except:
                    print("retry openai api")

        elif self.model in ["text-davinci-003", 'gpt-4']:
            response = openai.Completion.create(
                model="text-davinci-003",
                prompt="Answer the following question:\n\n" + question,
                max_tokens=512,
                temperature=0
            )

            response = response['choices'][0]['text']

        else:
            raise NameError("incorrect GPT model name.")

        return response

if __name__ == '__main__':
    # Initialize parser
    parser = argparse.ArgumentParser()
    
    # Adding optional argument
    parser.add_argument("-s", "--save_path", help = "Path to save all intermediate files and logs", default = "")
    parser.add_argument("-q", "--questions_path", help = "Path to evaluation questions", default = "")
    parser.add_argument("-g", "--gpt_model_name", help = "Name of GPT model to use", default = "gpt-3.5-turbo")
    parser.add_argument("-l", "--lmtutor_model_name", help = "Name of LMTutor model to use", default = "hf_lmsys/vicuna-7b-v1.3")
    parser.add_argument("-v", "--vector_store_path", help = "Vector Store Path", default = "")
    parser.add_argument("-e", "--evaluator", help = "LLM Judge", default = "chatgpt")
    parser.add_argument("-d", "--generated_answers_data_path", default = "")
    
    # Read arguments from command line
    args = parser.parse_args()

    # Initialize evaluator
    evaluator = LMTutorEvaluator(args.evaluator)
    evaluator.evaluation_pipeline(args.questions_path, args.generated_answers_data_path, args.save_path)
    
